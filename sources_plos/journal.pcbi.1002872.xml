<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
        <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00610</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1002872</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Physics</subject>
          <subj-group>
            <subject>Biophysics</subject>
            <subj-group>
              <subject>Biophysics theory</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Statistical mechanics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Physics</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Dynamic Finite Size Effects in Spiking Neural Networks</article-title>
        <alt-title alt-title-type="running-head">Finite Size Effects in Spiking Neural Networks</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Buice</surname>
            <given-names>Michael A.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Chow</surname>
            <given-names>Carson C.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line>Laboratory of Biological Modeling, NIDDK, NIH, Bethesda, Maryland, United States of America</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Ermentrout</surname>
            <given-names>Bard</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>University of Pittsburgh, United States of America</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">mabuice@mail.clm.utexas.edu</email> (MAB); <email xlink:type="simple">carsonc@mail.nih.gov</email> (CCC)</corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: MAB CCC. Contributed reagents/materials/analysis tools: MAB CCC. Wrote the paper: MAB CCC.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <month>1</month>
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>24</day>
        <month>1</month>
        <year>2013</year>
      </pub-date>
      <volume>9</volume>
      <issue>1</issue>
      <elocation-id>e1002872</elocation-id>
      <history>
        <date date-type="received">
          <day>16</day>
          <month>4</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>21</day>
          <month>11</month>
          <year>2012</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2013</copyright-year>
        <license xlink:type="simple">
          <license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>We investigate the dynamics of a deterministic finite-sized network of synaptically coupled spiking neurons and present a formalism for computing the network statistics in a perturbative expansion. The small parameter for the expansion is the inverse number of neurons in the network. The network dynamics are fully characterized by a neuron population density that obeys a conservation law analogous to the Klimontovich equation in the kinetic theory of plasmas. The Klimontovich equation does not possess well-behaved solutions but can be recast in terms of a coupled system of well-behaved moment equations, known as a moment hierarchy. The moment hierarchy is impossible to solve but in the mean field limit of an infinite number of neurons, it reduces to a single well-behaved conservation law for the mean neuron density. For a large but finite system, the moment hierarchy can be truncated perturbatively with the inverse system size as a small parameter but the resulting set of reduced moment equations that are still very difficult to solve. However, the entire moment hierarchy can also be re-expressed in terms of a functional probability distribution of the neuron density. The moments can then be computed perturbatively using methods from statistical field theory. Here we derive the complete mean field theory and the lowest order second moment corrections for physiologically relevant quantities. Although we focus on finite-size corrections, our method can be used to compute perturbative expansions in any parameter.</p>
      </abstract>
      <abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>One avenue towards understanding how the brain functions is to create computational and mathematical models. However, a human brain has on the order of a hundred billion neurons with a quadrillion synaptic connections. Each neuron is a complex cell comprised of multiple compartments hosting a myriad of ions, proteins and other molecules. Even if computing power continues to increase exponentially, directly simulating all the processes in the brain on a computer is not feasible in the foreseeable future and even if this could be achieved, the resulting simulation may be no simpler to understand than the brain itself. Hence, the need for more tractable models. Historically, systems with many interacting bodies are easier to understand in the two opposite limits of a small number or an infinite number of elements and most of the theoretical efforts in understanding neural networks have been devoted to these two limits. There has been relatively little effort directed to the very relevant but difficult regime of large but finite networks. In this paper, we introduce a new formalism that borrows from the methods of many-body statistical physics to analyze finite size effects in spiking neural networks.</p>
      </abstract>
      <funding-group>
        <funding-statement>This work was funded by the Intramural Research Program of the NIH/NIDDK. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="21"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Realistic models of neural networks in the central nervous system are analytically and computationally intractable, presenting a challenge to our understanding of the highly complex spiking dynamics of neurons. Consequently, some degree of simplification is necessary for theoretical progress and there is a corresponding spectrum of models with a range of complexity. “Mean Field” models represent the highest degree of simplification and classically consider the evolution of an “activity” variable which is some average of the output of a population of neurons. Early examples of mean field models are those of Wilson-Cowan <xref ref-type="bibr" rid="pcbi.1002872-Wilson1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Wilson2">[2]</xref>, Cohen and Grossberg <xref ref-type="bibr" rid="pcbi.1002872-Cohen1">[3]</xref>, and Amari <xref ref-type="bibr" rid="pcbi.1002872-Amari1">[4]</xref>. These models have proven to be useful in studies of neural dynamics such as in pattern formation and visual hallucinations <xref ref-type="bibr" rid="pcbi.1002872-Coombes1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Bressloff1">[7]</xref>. However, because of the nature of the activity variables as averages, they necessarily neglect individual neuron dynamics as well as population level effects of phase information and synchrony. Additionally, it is not clear how the time scales in the equations of mean field models are related to the response properties of the constituent neurons <xref ref-type="bibr" rid="pcbi.1002872-Gerstner1">[8]</xref>.</p>
      <p>The next level of model complexity requires relating population level activity to single neuron dynamics. This is a question explored by Knight <xref ref-type="bibr" rid="pcbi.1002872-Knight1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Knight2">[10]</xref>, who noted in particular that although the population firing rate may track an external stimulus, the single neuron firing rate need not and generally does not. The important conceptual feature introduced was that a population of neurons, each of which has some potential variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e001" xlink:type="simple"/></inline-formula>, can be replaced with a density, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e002" xlink:type="simple"/></inline-formula>, which counts the fraction of neurons whose potential lies within the infinitesimal range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e003" xlink:type="simple"/></inline-formula>. The firing rate of the population is then the current density of the population at the threshold potential. In the limit of an infinite population of neurons, one can introduce a continuity equation derived from the single neuron dynamics, producing what can be called <italic>density mean field theory</italic>. The density mean field approach to analyzing coupled networks has been pursued by Desai and Zwanzig <xref ref-type="bibr" rid="pcbi.1002872-Desai1">[11]</xref>, Strogatz and Mirollo <xref ref-type="bibr" rid="pcbi.1002872-Mirollo1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Mirollo2">[14]</xref>, Treves <xref ref-type="bibr" rid="pcbi.1002872-Treves1">[15]</xref>, Abbott and van Vreeswijk <xref ref-type="bibr" rid="pcbi.1002872-Abbott1">[16]</xref> and others <xref ref-type="bibr" rid="pcbi.1002872-Brunel1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Nykamp2">[26]</xref>. The spike response formalism considers an integral formulation of the continuity equations <xref ref-type="bibr" rid="pcbi.1002872-Gerstner2">[27]</xref>. These density mean field approaches have been recently put on a mathematically rigorous footing using results from probability theory <xref ref-type="bibr" rid="pcbi.1002872-Faugeras1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Touboul2">[31]</xref>.</p>
      <p>Neuronal firing is inherently variable and the source of this variability has been subject to much study and debate <xref ref-type="bibr" rid="pcbi.1002872-Softky1">[32]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Shadlen1">[34]</xref>. Incorporating neuronal variability into theories is another level of complexity. Activity mean field models have been shown to exhibit complex dynamics with high variability when coupled with highly variable connectivity <xref ref-type="bibr" rid="pcbi.1002872-Vreeswijk1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Sussillo1">[38]</xref>, but this is independent of single neuron dynamics. It is not clear in the context of the density mean field approach how to quantify the fluctuations arising from the interactions of discrete neurons in a finite-sized network, where the fluctuations are not suppressed by averaging over an infinite pool of neurons. <italic>Ad hoc</italic> attempts at quantifying finite-size effects include driving the system with external noise <xref ref-type="bibr" rid="pcbi.1002872-Strogatz1">[13]</xref>, introducing a self-consistent noise from neural firing <xref ref-type="bibr" rid="pcbi.1002872-Amit1">[39]</xref>, or assuming Poisson firing rates of the neurons within the population <xref ref-type="bibr" rid="pcbi.1002872-Brunel1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Mattia1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Doiron1">[40]</xref>. However, a systematic means of handling fluctuations due to the finite size of a population of neurons remain lacking.</p>
      <p>Here, we present a systematic expansion around the density mean field behavior that quantifies the finite-size fluctuations and correlations of a population of neurons in terms of the interactions in the network. The expansion utilizes a kinetic theory approach adapted from plasma physics <xref ref-type="bibr" rid="pcbi.1002872-Hildebrand1">[41]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Liboff1">[46]</xref>. Because we are interested specifically in intrinsic fluctuations which arise across the population evolving via deterministic dynamics, we do not include any external “noise” or internal stochasticity. The network variability is thus entirely due to the fact that many possible neuron initial conditions and parameters are consistent for a given network, which implies that a given network is selected from an ensemble of networks. One should think of this ensemble as the ensemble of networks consistent with an initial experimental setup, or of those networks which are consistent with the experimentally accessible quantities in the network. In particular, we show that fluctuations and correlations and their effect on population behavior can be quantified in a fully deterministic dynamical system by considering the ensemble of system histories given a distribution of initial conditions and network parameters. In the finite size case, the density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e004" xlink:type="simple"/></inline-formula> will not represent the fraction of neurons in the network with potential in the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e005" xlink:type="simple"/></inline-formula> (as it is in the infinite neuron case), but will represent the fraction of networks in the ensemble for which there is a neuron within the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e006" xlink:type="simple"/></inline-formula>. In the cases we consider, there is a “typical” system in the large neuron limit, so that the two are nearly identical. To a given order in the network size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e007" xlink:type="simple"/></inline-formula>, one can derive a moment hierarchy of differential-integral equations for the statistical moments of the density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e008" xlink:type="simple"/></inline-formula>. The calculations are facilitated by transforming the moment hierarchy into a functional or path integral expression of the moment generating functional from which a perturbative expansion can be derived. We show this for two synaptically coupled neural networks in the <xref ref-type="sec" rid="s2">Results</xref> and provide some guidance on generalization to other models in the <xref ref-type="sec" rid="s3">Discussion</xref>.</p>
      <p>Our approach is thus in the spirit of Gibbs' view of statistical mechanics <xref ref-type="bibr" rid="pcbi.1002872-Gibbs1">[47]</xref>. Like Gibbs, we do not rely on ergodicity or make any claims about time averages of the dynamics. The systems we study do not obey detailed balance and thus there will not be a necessary correspondence between time averaging and the ensemble averages we study. Nonetheless, we obtain useful results for characterizing the fluctuations and correlations in a network. We consider a specific example with global coupling where these correlations will have well-defined expansions in terms of the inverse systems size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e009" xlink:type="simple"/></inline-formula> and we refer to them as “finite-size” effects. However, we wish to stress that our approach is not restricted to a finite-size expansion in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e010" xlink:type="simple"/></inline-formula> per se. Our main result is to provide a systematic framework to “average” over unknown or unessential degrees of freedom.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>The density description of neural networks</title>
        <p>We present a formalism to analyze finite-size effects in a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e011" xlink:type="simple"/></inline-formula> synaptically coupled spiking neurons. Under fairly generic conditions, such a system can be reduced to a set of phase variables with a set of ancillary variables (such as those representing synaptic input) <xref ref-type="bibr" rid="pcbi.1002872-Ermentrout2">[48]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Ermentrout3">[51]</xref>. We consider the phase dynamics of a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e012" xlink:type="simple"/></inline-formula> phase neurons obeying<disp-formula id="pcbi.1002872.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e013" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pcbi.1002872.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e014" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula id="pcbi.1002872.e015"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e015" xlink:type="simple"/><label>(3)</label></disp-formula>where each neuron has a phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e016" xlink:type="simple"/></inline-formula> that is indexed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e017" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e018" xlink:type="simple"/></inline-formula> is a global synaptic drive, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e019" xlink:type="simple"/></inline-formula> is the population firing rate of the network and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e020" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e021" xlink:type="simple"/></inline-formula>th firing time of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e022" xlink:type="simple"/></inline-formula> and a neuron fires when its phase crosses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e023" xlink:type="simple"/></inline-formula>. The frequency function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e024" xlink:type="simple"/></inline-formula> depends on all the phases <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e025" xlink:type="simple"/></inline-formula> and a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e026" xlink:type="simple"/></inline-formula> parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e027" xlink:type="simple"/></inline-formula>, that can be distinct for each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e028" xlink:type="simple"/></inline-formula>. The neuron can be in an oscillatory or excitable regime.</p>
        <p>We will develop our theory for a general frequency function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e029" xlink:type="simple"/></inline-formula> and apply it to the specific cases of a simple phase oscillator where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e030" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002872-Knight2">[10]</xref> and the theta model where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e031" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e032" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e033" xlink:type="simple"/></inline-formula> is an external input and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e034" xlink:type="simple"/></inline-formula> is a parameter that can be neuron dependent. The theta model is the normal form of a Type I neuron near the bifurcation to firing and is equivalent to a quadratic integrate-and-fire neuron <xref ref-type="bibr" rid="pcbi.1002872-Ermentrout4">[52]</xref>. For some neural networks, a phase reduction of this sort results in a phase coupled model, such as the Kuramoto model (e.g. Hansel and Golomb <xref ref-type="bibr" rid="pcbi.1002872-Golomb1">[53]</xref>), which we have previously analyzed <xref ref-type="bibr" rid="pcbi.1002872-Hildebrand1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Buice1">[42]</xref>. In the present paper, we consider all-to-all or global coupling through a synaptic drive variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e035" xlink:type="simple"/></inline-formula>. However, our basic approach is not restricted to global coupling.</p>
        <p>Our goal is to derive the fluctuation and correlation effects beyond mean field theory for the system. For global coupling, these effects arise from the finite number of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e036" xlink:type="simple"/></inline-formula> in the network. We calculate the effects of finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e037" xlink:type="simple"/></inline-formula> on the dynamics of the system as a perturbation expansion in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e038" xlink:type="simple"/></inline-formula> around the mean field limit of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e039" xlink:type="simple"/></inline-formula>. In particular, we will compute the fluctuations and correlations of the synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e040" xlink:type="simple"/></inline-formula> and network firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e041" xlink:type="simple"/></inline-formula>, defined as the variability over instances of the network given initial conditions as well as neuron and network parameters. We will do this through a probability density functional description of the neuron firing histories. Before we introduce our density functional approach, we describe the Klimontovich description of many-body systems. This description allows us to introduce the fundamental degrees of freedom in a straightforward manner without recourse to the statistical field theory formalism used in the density functional approach. While we focus on finite size effects in this paper, our method could also be used to generate perturbation expansions in other parameters.</p>
        <sec id="s2a1">
          <title>Klimontovich description</title>
          <p>We adapt the methods of the kinetic theory as applied to gas and plasma dynamics to create a probabilistic description of the network dynamics <xref ref-type="bibr" rid="pcbi.1002872-Ichimaru1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Liboff1">[46]</xref>. The approach will allow us to calculate the corrections to mean field theory due to correlations in the firing times of neurons. In particular, we employ a Klimontovich description, which considers the probability density of the phases of a population of neurons (i.e. the density of the <italic>empirical measure</italic>)<disp-formula id="pcbi.1002872.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e042" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e043" xlink:type="simple"/></inline-formula> is the Dirac delta functional, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e044" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e045" xlink:type="simple"/></inline-formula> are the solutions to system (1)–(3). The neuron density gives a count of the number of neurons with phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e046" xlink:type="simple"/></inline-formula> and parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e047" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e048" xlink:type="simple"/></inline-formula>. We have included the parameter vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e049" xlink:type="simple"/></inline-formula> in the neuron density. Hence, neurons are characterized by their phase and parameter values. For systems that obey exchange symmetry or exchangeability (i.e. the system remains unchanged statistically after a relabeling of the neurons), the neuron density in (4) gives a complete description of the system. In systems without exchangeability, the neuron density will still capture the complete dynamics of the system if it includes labels for the information attached to individual neurons. Using the fact that the Dirac delta functional in (3) can be expressed as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e050" xlink:type="simple"/></inline-formula> the population firing rate can be rewritten as<disp-formula id="pcbi.1002872.e051"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e051" xlink:type="simple"/><label>(5)</label></disp-formula>The neuron density formally obeys the conservation equation<disp-formula id="pcbi.1002872.e052"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e052" xlink:type="simple"/><label>(6)</label></disp-formula>which is known as the Klimontovich equation in kinetic theory and is only valid in the weak or distributional sense since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e053" xlink:type="simple"/></inline-formula> is not differentiable. The Klimontovich equation, the equation for the synaptic drive (2), and the firing rate expressed in terms of the neuron density (5), fully define the system. For the systems defined above, we expect that in the limit of a large number of neurons the ensemble of networks will converge to a “typical” network. In the infinite neuron limit, this will give the density equations of mean field theory, whereas for finite but large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e054" xlink:type="simple"/></inline-formula>, there will be some variation in systems around the mean field solution. For this reason, we consider taking expectations of the Klimontovich <xref ref-type="disp-formula" rid="pcbi.1002872.e052">equation (6)</xref> over initial conditions and neuron parameters, which produce smooth moment functions for the density. Because the interacting dynamics have a non-trivial effect on the distribution functions, computing this average is not always simple. In the next section, we will formally derive an expression for the measure or density functional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e055" xlink:type="simple"/></inline-formula> over which these averages are taken.</p>
          <p>Denoting averages over initial conditions and neuron parameters (i.e. those over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e056" xlink:type="simple"/></inline-formula>) by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e057" xlink:type="simple"/></inline-formula>, the average of (6) yields the equation<disp-formula id="pcbi.1002872.e058"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e058" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e059" xlink:type="simple"/></inline-formula> is the first moment of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e060" xlink:type="simple"/></inline-formula> and called the one-neuron distribution function, which will depend on higher order moments since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e061" xlink:type="simple"/></inline-formula> is a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e062" xlink:type="simple"/></inline-formula> and hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e063" xlink:type="simple"/></inline-formula>. Equations for the higher order moments can be constructed from (6) by multiplying by factors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e064" xlink:type="simple"/></inline-formula>. However, each moment will depend on yet higher moments, resulting in a system of coupled moment equations called the BBGKY hierarchy. Solving the entire BBGKY hierarchy is equivalent to solving the original system and thus provides no computational advantage. However, perturbative solutions in a small parameter such as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e065" xlink:type="simple"/></inline-formula> can be obtained by truncating the hierarchy and solving the truncated system. This has been the traditional approach in kinetic theory but is generally difficult to do. In the next section, we present a computational formalism where moments for the firing rate and synaptic drive are computed directly from a probability density functional of the neuron density.</p>
          <p>Mean field theory is obtained by neglecting all correlations and higher order cumulants. Thus, setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e066" xlink:type="simple"/></inline-formula> gives the self-consistent mean field system<disp-formula id="pcbi.1002872.e067"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e067" xlink:type="simple"/><label>(8)</label></disp-formula>Higher order moments (distribution functions) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e068" xlink:type="simple"/></inline-formula> are likewise defined. The second moment (2-neuron distribution function), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e069" xlink:type="simple"/></inline-formula>, is the fraction of networks in the ensemble for which there is a neuron of type <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e070" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e071" xlink:type="simple"/></inline-formula> and another of type <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e072" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e073" xlink:type="simple"/></inline-formula>. It is given by<disp-formula id="pcbi.1002872.e074"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e074" xlink:type="simple"/><label>(9)</label></disp-formula>We have implicitly defined the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e075" xlink:type="simple"/></inline-formula> using the fact that if the neurons are prepared identically and independently, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e076" xlink:type="simple"/></inline-formula>. We call <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e077" xlink:type="simple"/></inline-formula> the <italic>connected</italic> contribution and the product of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e078" xlink:type="simple"/></inline-formula>'s the <italic>disconnected</italic> contribution. These labels are equivalent to whether the contribution can be factored into products of lower moments. The two-neuron density function has connected, disconnected and finite-size (those with factors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e079" xlink:type="simple"/></inline-formula>) contributions. The finite-size contributions arise from the deviations in the ensemble average due to finite sample size. There are two types of finite size correction. There is a “sampling” correction because of the “diagonal” contribution where the indices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e080" xlink:type="simple"/></inline-formula> from the two factors of the neuron density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e081" xlink:type="simple"/></inline-formula> (4) coincide. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e082" xlink:type="simple"/></inline-formula> represents the joint probability density function of two neurons drawn from the population, there is a finite-size correction due to the fact that once a neuron has been drawn from the population, that neuron's phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e083" xlink:type="simple"/></inline-formula> is fixed and the probability density for that neuron is a point mass at that phase. Thus, the sampling finite size term consist of removing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e084" xlink:type="simple"/></inline-formula>th of the joint probability mass from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e085" xlink:type="simple"/></inline-formula> and adding it back as the one-neuron density multiplied by the Dirac delta functional. In the infinite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e086" xlink:type="simple"/></inline-formula> case, the probability of drawing a strictly identical neuron twice is zero.</p>
          <p>The second type of finite size effect is due to the coupling and is contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e087" xlink:type="simple"/></inline-formula> (it will be proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e088" xlink:type="simple"/></inline-formula>). For uncoupled neurons, if the neurons are not prepared such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e089" xlink:type="simple"/></inline-formula>, then no such correlations will be generated by the dynamics. Note that integration of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e090" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e091" xlink:type="simple"/></inline-formula> (or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e092" xlink:type="simple"/></inline-formula>) gives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e093" xlink:type="simple"/></inline-formula>. One can derive similar expressions for the higher moments, i.e. for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e094" xlink:type="simple"/></inline-formula>-neuron densities. There will be connected terms which cannot be factored into products of lower moments, there will be disconnected terms which can be so factored, and there will be finite size corrections given by the combinatorics of drawing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e095" xlink:type="simple"/></inline-formula> neurons from a population of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e096" xlink:type="simple"/></inline-formula>.</p>
        </sec>
      </sec>
      <sec id="s2b">
        <title>Density functional description</title>
        <p>We have shown that one tractable approach for incorporating fluctuations and correlations is to truncate the BBGKY hierarchy. However, solving such truncated systems for any model of reasonable complexity quickly becomes unwieldy. For this reason, we adapt the density functional formalism developed for statistical field theory to obtain a formal expression for the probability density functional of the neuron density and synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e097" xlink:type="simple"/></inline-formula>. The fundamental degrees of freedom in this approach reflect the moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e098" xlink:type="simple"/></inline-formula>, albeit in a more compact and manageable form. The measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e099" xlink:type="simple"/></inline-formula> is a distribution over the possible network realizations. The “variance” of this distribution (represented by the two-neuron distribution function) provides an indication of the extent to which different realizations of the network will differ from each other. For the systems we consider, the estimates of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e100" xlink:type="simple"/></inline-formula>-neuron distribution functions behave as a power of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e101" xlink:type="simple"/></inline-formula>. This has the side benefit of demonstrating that there is a limit in which the ensemble converges to a “typical” system described by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e102" xlink:type="simple"/></inline-formula>-neuron distribution function, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e103" xlink:type="simple"/></inline-formula>, i.e. the mean field theory. For the same reason, at large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e104" xlink:type="simple"/></inline-formula>, we can use the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e105" xlink:type="simple"/></inline-formula>-neuron distribution functions as estimates of the fluctuations in the density for a <italic>single</italic> system. Because these fluctuations vanish in the limit of large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e106" xlink:type="simple"/></inline-formula>, we term them “finite-size” effects. In the examples below, we concentrate on computing the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e107" xlink:type="simple"/></inline-formula>-neuron distribution function to lowest order in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e108" xlink:type="simple"/></inline-formula>, which gives estimates of fluctuations of the network coupling variables and the firing rate.</p>
        <p>In this section we present only final results, the complete derivation and description of the computational method can be found in the <xref ref-type="sec" rid="s4">Methods</xref>. The essential element of the field theoretic method is that the density functional be expressed in the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e109" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e110" xlink:type="simple"/></inline-formula> is called the <italic>action</italic>. Given this density functional, moments can be obtained by integrating over this density. For example, the second moment of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e111" xlink:type="simple"/></inline-formula> is given by a functional or “path” integral<disp-formula id="pcbi.1002872.e112"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e112" xlink:type="simple"/></disp-formula>where the measure in the integral is over <italic>functions</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e114" xlink:type="simple"/></inline-formula> in some appropriate functional space. A generating functional for all the moments or cumulants can be similarly defined (see <xref ref-type="sec" rid="s4">Methods</xref>). The strategy of field theory is to exploit the fact that Gaussian integrals have closed form expressions in an arbitrary (including infinite) number of dimensions. Hence, the path integrals can be performed using Laplace's method or the method of steepest descents to obtain an asymptotic series expression for the integrals in terms of a small parameter, which in this case will be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e115" xlink:type="simple"/></inline-formula>.</p>
        <p>In general, the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e116" xlink:type="simple"/></inline-formula> is not expressible in simple form. This is overcome by augmenting the system with an auxiliary set of imaginary <italic>response functions</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e118" xlink:type="simple"/></inline-formula> and defining an expanded action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e119" xlink:type="simple"/></inline-formula>. The action can then be Taylor expanded around a critical or saddle point where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e120" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e121" xlink:type="simple"/></inline-formula>), which produces an expansion of moments of a “Gaussian” distribution, in this case arising from the terms bilinear in the auxiliary variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e122" xlink:type="simple"/></inline-formula> and the configuration variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e123" xlink:type="simple"/></inline-formula>. A perturbation expansion can then be constructed by exploiting the fact that complex Gaussian integrals of the form<disp-formula id="pcbi.1002872.e124"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e124" xlink:type="simple"/></disp-formula>(for some variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e125" xlink:type="simple"/></inline-formula>) have closed form expressions in terms of linear response functions or propagators <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e126" xlink:type="simple"/></inline-formula> and are nonzero only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e127" xlink:type="simple"/></inline-formula>. This path integral identity can be used to formulate an explicit set of rules to obtain expressions for each term of the perturbation expansion. The computation is simplified by encapsulating the rules for constructing the terms in the expansion into diagrams (i.e. Feynman diagrams, see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
        <p>The variables in the action can be compared to those in a stochastic differential equation. The original variables (without a tilde, e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e128" xlink:type="simple"/></inline-formula>) denote the configuration variables, while the auxiliary variables (with a tilde, e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e129" xlink:type="simple"/></inline-formula>), denote stochastic or noise forcing terms although in our case the noise is imposed by the uncertainty in the initial conditions and heterogeneity in a fully deterministic network. Finally, the method does not compute the action directly in terms of the neuron density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e130" xlink:type="simple"/></inline-formula> but rather transforms it to a new set of neuron density variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e131" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e132" xlink:type="simple"/></inline-formula> through the transformation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e133" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e134" xlink:type="simple"/></inline-formula>. This transformation renders the action to be more amenable to analysis in a way that is similar in spirit to how the Cole-Hopf transformation reduces the nonlinear Burger's equation into the linear heat equation <xref ref-type="bibr" rid="pcbi.1002872-Gutkin1">[54]</xref>. Specifically it removes the Poisson-like counting noise from the definitions of the moments. As an example, whereas<disp-formula id="pcbi.1002872.e135"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e135" xlink:type="simple"/></disp-formula>the transformed variables have<disp-formula id="pcbi.1002872.e136"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e136" xlink:type="simple"/></disp-formula>As discussed in <xref ref-type="sec" rid="s4">Methods</xref>, the population level coupling implies that the desired quantities will have an expansion in powers of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e137" xlink:type="simple"/></inline-formula>. We describe basic results of this approach on two particular example networks: the phase model and the quadratic integrate-and-fire model. For each model, we describe mean field theory, the linear response of the population, and all the correlation functions involving the population and the synaptic drive. Each quantity is calculated to lowest non-trivial order.</p>
        <sec id="s2b1">
          <title>Phase model</title>
          <p>We first apply the formalism on the simple phase model defined by<disp-formula id="pcbi.1002872.e138"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e138" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e139" xlink:type="simple"/></inline-formula> is the magnitude of the coupling of a given neuron to the global activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e140" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e141" xlink:type="simple"/></inline-formula> indexes the input. (In analytical terms, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e142" xlink:type="simple"/></inline-formula> is an element of the sigma algebra representing the realizations of the inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e143" xlink:type="simple"/></inline-formula>, for example an instance of Brownian motion input).</p>
          <p>The action for the phase model as derived in the <xref ref-type="sec" rid="s4">Methods</xref> has the form<disp-formula id="pcbi.1002872.e144"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e144" xlink:type="simple"/><label>(11)</label></disp-formula>where <disp-formula id="pcbi.1002872.e145"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e145" xlink:type="simple"/></disp-formula>represents the contribution of the transformed neuron density to the action and <disp-formula id="pcbi.1002872.e146"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e146" xlink:type="simple"/></disp-formula>represents the global synaptic drive. The action (11) contains all the information about the statistics of the network. Given the action, mean field theory and a perturbative expansion around mean field theory can be derived using standard methods developed in field theory.</p>
          <p>The mean field equations, which are given by a critical point of the action, are given by (8), which for parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e147" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e148" xlink:type="simple"/></inline-formula> are rewritten as<disp-formula id="pcbi.1002872.e149"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e149" xlink:type="simple"/><label>(12)</label></disp-formula>For the phase model, we can solve (12) directly for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e150" xlink:type="simple"/></inline-formula> to obtain<disp-formula id="pcbi.1002872.e151"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e151" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e152" xlink:type="simple"/></inline-formula> is the initial distribution. In this case, the functional form given above is also the general (non-mean field) solution, upon replacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e153" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e154" xlink:type="simple"/></inline-formula>. Recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e155" xlink:type="simple"/></inline-formula> is the population distribution averaged over the ensemble of prepared networks. If the neurons are distributed uniformly in phase, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e156" xlink:type="simple"/></inline-formula>. In this case, the global activity does not affect the phase distribution. On the other hand, if the neurons are always prepared at the same phase, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e157" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e158" xlink:type="simple"/></inline-formula> is the prepared phase. In this case the neurons will remain in phase.</p>
          <p>Solving for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e159" xlink:type="simple"/></inline-formula> allows us to write a closed integro-differential equation for the synaptic drive<disp-formula id="pcbi.1002872.e160"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e160" xlink:type="simple"/></disp-formula>Note that as long as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e161" xlink:type="simple"/></inline-formula> is known, this mean field equation reduces the system from a partial differential equation to a two dimensional ODE, namely:<disp-formula id="pcbi.1002872.e162"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e162" xlink:type="simple"/></disp-formula>The population behavior is reduced to the synaptic drive dynamics along with the dynamics of a fictitious oscillator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e163" xlink:type="simple"/></inline-formula>. This is the result of the fact that the only important dynamical quantity is the overall phase shift of each neuron from its initial phase and that this quantity is the same for each neuron. Knowing the initial distribution of states is therefore enough to reduce the dimensionality of the system.</p>
          <p>The steepest descent expansions to the path integrals will be expressed in terms of the propagators or linear response functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e164" xlink:type="simple"/></inline-formula>, which appear as the inverses of the integral kernels of the bilinear terms in the actions. The linear response can be derived to order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e165" xlink:type="simple"/></inline-formula> by linearizing about the solutions of the mean field equation. Because there are two fields in the action (synaptic drive and density), there are four separate propagators:<disp-formula id="pcbi.1002872.e166"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e166" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e167" xlink:type="simple"/></inline-formula> describes the response in the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e168" xlink:type="simple"/></inline-formula> to a perturbation in the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e169" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e170" xlink:type="simple"/></inline-formula> denotes perturbations around the mean field solution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e171" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e172" xlink:type="simple"/></inline-formula>. The equations for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e173" xlink:type="simple"/></inline-formula> reflect a perturbation that consists of adding a single neuron to the population with the specified initial condition and parameters.</p>
          <p>If we assume a constant input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e174" xlink:type="simple"/></inline-formula> then in order to have a steady-state, the mean field must satisfy<disp-formula id="pcbi.1002872.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e175" xlink:type="simple"/><label>(14)</label></disp-formula>for a fixed parameter probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e176" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e177" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e178" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e179" xlink:type="simple"/></inline-formula> are the means of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e180" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e181" xlink:type="simple"/></inline-formula> under the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e182" xlink:type="simple"/></inline-formula>. The linear response around this solution is<disp-formula id="pcbi.1002872.e183"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e183" xlink:type="simple"/></disp-formula>which we can immediately solve in closed form to obtain<disp-formula id="pcbi.1002872.e184"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e184" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e185" xlink:type="simple"/></inline-formula> are the firing times of the fictitious oscillator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e186" xlink:type="simple"/></inline-formula> with initial condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e187" xlink:type="simple"/></inline-formula>, and is determined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e188" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e189" xlink:type="simple"/></inline-formula> is the expected form of the linear response upon perturbing the synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e190" xlink:type="simple"/></inline-formula>, i.e. exponential decay. The response of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e191" xlink:type="simple"/></inline-formula> to the population density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e192" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e193" xlink:type="simple"/></inline-formula>, is a series of exponential pulses at the firing times of the additional neuron, which is what we would expect if we added a single neuron at a given phase. The other propagators govern the response of the population. Since the distribution is uniform and the firing rate does not depend upon phase, perturbing the synaptic drive only makes the entire population fire faster, but does not change the relative phase, thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e194" xlink:type="simple"/></inline-formula>. On the other hand, adding a single neuron adjusts the population density by a single delta function at the location of the new neuron, hence the form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e195" xlink:type="simple"/></inline-formula>. The fact that single oscillator perturbations are not damped away by the linear response is an indication that the stationary state is marginally stable. We expect that finite size effects at the next order will stabilize these marginal modes assuming there is some degree of heterogeneity similar to what happens in the Kuramoto model <xref ref-type="bibr" rid="pcbi.1002872-Hildebrand1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Buice1">[42]</xref>.</p>
          <p>As described in <xref ref-type="sec" rid="s4">Methods</xref>, the expansion of any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e196" xlink:type="simple"/></inline-formula>-neuron correlation function in powers of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e197" xlink:type="simple"/></inline-formula> can be computed from the linear response and the “vertices” derived from the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e198" xlink:type="simple"/></inline-formula>. Here we give the lowest order contribution to the 2-neuron correlation functions. In addition, this will give us the firing rate fluctuations. For the fluctuations in the synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e199" xlink:type="simple"/></inline-formula> about an arbitrary mean field state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e200" xlink:type="simple"/></inline-formula>, the diagrams at tree level (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e201" xlink:type="simple"/></inline-formula>) give<disp-formula id="pcbi.1002872.e202"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e202" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e203" xlink:type="simple"/></inline-formula>. Inserting the expressions for the linear response in the stationary state (15) we obtain :<disp-formula id="pcbi.1002872.e204"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e204" xlink:type="simple"/><label>(17)</label></disp-formula>for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e205" xlink:type="simple"/></inline-formula>, where the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e206" xlink:type="simple"/></inline-formula> are determined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e207" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e208" xlink:type="simple"/></inline-formula> is the Kronecker delta. The reason for the Kronecker delta term is to account for the limiting process which defines the interaction vertex. Essentially only half the neurons within the vicinity of firing will contribute to the first cycle of firing (about half are above threshold, half under). On subsequent cycles, all neurons will contribute. This issue arises here because of an ambiguity of the continuum representation we are using. The vertex only measures those neurons which have passed threshold, whereas the linear response from (15) considers the limiting behavior of neurons initially configured in the neighborhood of some phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e209" xlink:type="simple"/></inline-formula> (consider the last equation in (15)). If the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e210" xlink:type="simple"/></inline-formula> is smooth, it is more convenient to compute the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e211" xlink:type="simple"/></inline-formula> convolved with the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e212" xlink:type="simple"/></inline-formula>.</p>
          <p>Performing the time integration gives<disp-formula id="pcbi.1002872.e213"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e213" xlink:type="simple"/><label>(18)</label></disp-formula>The equal time correlation function has a simpler form:<disp-formula id="pcbi.1002872.e214"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e214" xlink:type="simple"/><label>(19)</label></disp-formula>This correlation function quantifies the fluctuations in the global coupling variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e215" xlink:type="simple"/></inline-formula> as a function of time. Recall that we defined an initial state in which each neuron is statistically independent in phase and parameters. The time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e216" xlink:type="simple"/></inline-formula> is the interval elapsed since the network was in that initial state. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e217" xlink:type="simple"/></inline-formula> is a measure of the expected variance of the synaptic drive from the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e218" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e219" xlink:type="simple"/></inline-formula>. As mentioned above, due to the fact that higher moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e220" xlink:type="simple"/></inline-formula> will be suppressed by higher powers of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e221" xlink:type="simple"/></inline-formula>, this is also an estimate of the variance of the global coupling as a function of time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e222" xlink:type="simple"/></inline-formula> from a <italic>known mean field configuration</italic>. Because the linear response has a spectrum which includes the spectrum of the single neuron activity, we expect behavior characteristic of the time scales of single neuron dynamics to appear.</p>
          <p>We now turn to the correlations in the density variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e223" xlink:type="simple"/></inline-formula>. As discussed in the <xref ref-type="sec" rid="s4">Methods</xref> section, these are given by (let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e224" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e225" xlink:type="simple"/></inline-formula>)<disp-formula id="pcbi.1002872.e226"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e226" xlink:type="simple"/><label>(20)</label></disp-formula>The first term is given by expressions derived above. The second term is of the same form as the correlation of the synaptic drive variable.<disp-formula id="pcbi.1002872.e227"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e227" xlink:type="simple"/><label>(21)</label></disp-formula>The above is the general expression. For the fluctuations about steady state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e228" xlink:type="simple"/></inline-formula> from (15), giving the simple relation<disp-formula id="pcbi.1002872.e229"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e229" xlink:type="simple"/><label>(22)</label></disp-formula>which is just the negative of the product of the mean field steady state solutions at each argument <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e230" xlink:type="simple"/></inline-formula> times a factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e231" xlink:type="simple"/></inline-formula>. This term is due to the factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e232" xlink:type="simple"/></inline-formula> from the sampling correction in the two-neuron distribution function (see <xref ref-type="disp-formula" rid="pcbi.1002872.e074">equation (9)</xref> and below).</p>
          <p>The 2-neuron distribution function is given by<disp-formula id="pcbi.1002872.e233"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e233" xlink:type="simple"/><label>(23)</label></disp-formula>At equal times (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e234" xlink:type="simple"/></inline-formula>) we have<disp-formula id="pcbi.1002872.e235"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e235" xlink:type="simple"/><label>(24)</label></disp-formula>which shows that (22) is the correction term for the normalization of the two-neuron distribution function. So for the case of the simple phase model, the fluctuations in the density about steady state are given by the sampling fluctuations from the steady state distribution. Note that for large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e236" xlink:type="simple"/></inline-formula> this means that the variance of the number of neurons at firing (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e237" xlink:type="simple"/></inline-formula>) is equal to the mean times a factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e238" xlink:type="simple"/></inline-formula>, which is equivalent to the Poisson counting assumption of Brunel-Hakim <xref ref-type="bibr" rid="pcbi.1002872-Brunel1">[17]</xref>. As we will show in the next section, this will not hold in general. Note the form of the linear response (15) for the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e239" xlink:type="simple"/></inline-formula>. The fact that the linear response <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e240" xlink:type="simple"/></inline-formula>, eliminated the first term in (21), which is the contribution to the fluctuations from the coupling. Comparing to the general form of the linear response (13), we see that the equation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e241" xlink:type="simple"/></inline-formula> has a source term proportional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e242" xlink:type="simple"/></inline-formula>. Because the phase model has a uniform steady state, this source term is zero. For a model with a non-uniform steady state (such as the quadratic integrate-and-fire model, which we examine in the next section) this will not be the case, and there will be further corrections to the fluctuations in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e243" xlink:type="simple"/></inline-formula>. It occurs in the phase model because perturbations in the synaptic drive do not perturb the density in steady state. Thus the only fluctuations of the density in steady state are from the sampling fluctuations.</p>
          <p>The correlation function between the global coupling and the density is given by (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e244" xlink:type="simple"/></inline-formula>).<disp-formula id="pcbi.1002872.e245"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e245" xlink:type="simple"/><label>(25)</label></disp-formula>Again, the first term is composed of factors derived above. The remaining unique term is given by<disp-formula id="pcbi.1002872.e246"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e246" xlink:type="simple"/><label>(26)</label></disp-formula>In steady state, this term is<disp-formula id="pcbi.1002872.e247"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e247" xlink:type="simple"/><label>(27)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e248" xlink:type="simple"/></inline-formula>, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e249" xlink:type="simple"/></inline-formula> are defined such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e250" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e251" xlink:type="simple"/></inline-formula> is the largest <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e252" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e253" xlink:type="simple"/></inline-formula>.</p>
          <p>The firing rate of the population is given by<disp-formula id="pcbi.1002872.e254"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e254" xlink:type="simple"/><label>(28)</label></disp-formula>The mean field solution for this is<disp-formula id="pcbi.1002872.e255"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e255" xlink:type="simple"/><label>(29)</label></disp-formula>and in steady state we have<disp-formula id="pcbi.1002872.e256"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e256" xlink:type="simple"/><label>(30)</label></disp-formula>The second moment of the firing rate is given by<disp-formula id="pcbi.1002872.e257"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e257" xlink:type="simple"/><label>(31)</label></disp-formula>Using our expression for the variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e258" xlink:type="simple"/></inline-formula> in steady state, we have<disp-formula id="pcbi.1002872.e259"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e259" xlink:type="simple"/><label>(32)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e260" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e261" xlink:type="simple"/></inline-formula> are such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e262" xlink:type="simple"/></inline-formula>. At equal time we have the simple form<disp-formula id="pcbi.1002872.e263"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e263" xlink:type="simple"/><label>(33)</label></disp-formula>which is equivalent to the Poisson finite size ansatz. The delta function evaluated at zero is a singularity which arises upon attempting to isolate a counting process at a single point on the real line. This can be regularized by considering an estimate of this quantity in a time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e264" xlink:type="simple"/></inline-formula>. The variance in the counts will vary as<disp-formula id="pcbi.1002872.e265"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e265" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e266" xlink:type="simple"/></inline-formula>. This indicates that the population firing rate will appear as that from a population of independent Poisson neurons even though the individual neurons are regular. For intuition as to why this is the case, consider dividing up the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e267" xlink:type="simple"/></inline-formula> into bins of equal size and distributing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e268" xlink:type="simple"/></inline-formula> neurons into these bins. This is the initial state of the network when initialized in steady state. The distribution of the neuron counts in each bin will follow a hypergeometric distribution. In the limit of small bin size and large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e269" xlink:type="simple"/></inline-formula>, the number of neurons in each bin will approximate a Poisson distribution. The factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e270" xlink:type="simple"/></inline-formula> arises from normalizing the coupling by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e271" xlink:type="simple"/></inline-formula>. Recall that the absence of any other correction is an artifact of the uniformity of the steady state of the phase model. This will not be the case for the quadratic integrate-and-fire model.</p>
          <p><xref ref-type="fig" rid="pcbi-1002872-g001">Figure 1</xref> shows comparisons between our analytical predictions and numerical simulations. In (a) through (d), the network the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e272" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e273" xlink:type="simple"/></inline-formula> are constant and homogeneous (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e274" xlink:type="simple"/></inline-formula>). <xref ref-type="fig" rid="pcbi-1002872-g001">Figure 1 (a)–(c)</xref> shows examples of the variance of the synaptic drive as a function of time. As seen in the figures, the correlation function has contributions that appear at the firing times of the fictitious oscillator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e275" xlink:type="simple"/></inline-formula> (Recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e276" xlink:type="simple"/></inline-formula> is a function which parameterizes the linear response). Each such “firing event” produces a new positive transient response in the correlation function. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e277" xlink:type="simple"/></inline-formula>, each firing event produces ever smaller perturbations as the correlation approaches steady state. Note also in those figures that the analytic computation at order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e278" xlink:type="simple"/></inline-formula> becomes better as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e279" xlink:type="simple"/></inline-formula> grows larger, and that the overall magnitude scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e280" xlink:type="simple"/></inline-formula>. Deviations are observable for small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e281" xlink:type="simple"/></inline-formula>, particularly for the case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e282" xlink:type="simple"/></inline-formula>. Note also the firing rate of the fictitious oscillator increases as the population input increases. Comparison of numerical and analytic results for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e283" xlink:type="simple"/></inline-formula> is shown in <xref ref-type="fig" rid="pcbi-1002872-g001">Figure 1 (d)</xref>. We measured this quantity by binning the firing counts in a time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e284" xlink:type="simple"/></inline-formula> and have also subtracted the “Poisson” contribution. The analytic result is the first term from <xref ref-type="disp-formula" rid="pcbi.1002872.e263">equation (33)</xref>. <xref ref-type="fig" rid="pcbi-1002872-g001">Figure 1 (e)</xref> shows the two-time correlation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e285" xlink:type="simple"/></inline-formula>, where we have fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e286" xlink:type="simple"/></inline-formula>. As expected by our prediction in <xref ref-type="disp-formula" rid="pcbi.1002872.e213">equation (18)</xref>, the oscillations are much more pronounced. <xref ref-type="fig" rid="pcbi-1002872-g001">Figure 1 (f)</xref> shows the effects of heterogeneity on the synaptic drive. The drive distribution was chosen to be uniform, with inputs to each neuron chosen from the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e287" xlink:type="simple"/></inline-formula>. The oscillations in the synaptic drive are damped by the heterogeneity and there is an effective increase in the mean drive fluctuations as expected from the theory. In this case the heterogeneity clearly dominates as a contribution to the fluctuations, as can be seen by comparing <xref ref-type="fig" rid="pcbi-1002872-g001">figures 1 (a) and 1 (f)</xref>, which differ by close to a factor of four in steady state.</p>
          <fig id="pcbi-1002872-g001" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002872.g001</object-id>
            <label>Figure 1</label>
            <caption>
              <title>Phase model.</title>
              <p><bold>A</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e288" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e289" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e290" xlink:type="simple"/></inline-formula> (bottom) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e291" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e292" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e293" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e294" xlink:type="simple"/></inline-formula>. <bold>B</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e295" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e296" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e297" xlink:type="simple"/></inline-formula> (bottom) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e298" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e299" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e300" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e301" xlink:type="simple"/></inline-formula>. <bold>C</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e302" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e303" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e304" xlink:type="simple"/></inline-formula> (bottom) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e305" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e306" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e307" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e308" xlink:type="simple"/></inline-formula>. <bold>D</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e309" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e310" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e311" xlink:type="simple"/></inline-formula> (bottom) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e312" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e313" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e314" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e315" xlink:type="simple"/></inline-formula>, where the “Poisson” contribution has been subtracted. <bold>E</bold>. Two-time correlator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e316" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e317" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e318" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e319" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e320" xlink:type="simple"/></inline-formula>. <bold>F</bold>. Equal time correlators in a heterogeneous network; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e321" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e322" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e323" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e324" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e325" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e326" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e327" xlink:type="simple"/></inline-formula> is taken from the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e328" xlink:type="simple"/></inline-formula> for each neuron. Ensemble averages for all simulations are taken over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e329" xlink:type="simple"/></inline-formula> samples.</p>
            </caption>
            <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002872.g001" position="float" xlink:type="simple"/>
          </fig>
        </sec>
      </sec>
      <sec id="s2c">
        <title>The quadratic integrate-and-fire model</title>
        <p>The second model we analyze is the quadratic integrate-and-fire model, whose single neuron dynamics are given by<disp-formula id="pcbi.1002872.e330"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e330" xlink:type="simple"/><label>(34)</label></disp-formula>This model exhibits a finite-time blow-up that is considered to be “firing” at which point the neuron's membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e331" xlink:type="simple"/></inline-formula> is reset to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e332" xlink:type="simple"/></inline-formula>. We couple the neurons in the same manner as in the phase model with the synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e333" xlink:type="simple"/></inline-formula>. Ermentrout and Kopell mapped this model to an oscillator using the transformation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e334" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002872-Ermentrout5">[55]</xref> to obtain<disp-formula id="pcbi.1002872.e335"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e335" xlink:type="simple"/><label>(35)</label></disp-formula>This form of the model is often called the theta model <xref ref-type="bibr" rid="pcbi.1002872-Ermentrout5">[55]</xref>. Hence, the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e336" xlink:type="simple"/></inline-formula> is given by:<disp-formula id="pcbi.1002872.e337"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e337" xlink:type="simple"/><label>(36)</label></disp-formula>A convenient feature of this model is that neurons cross the firing phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e338" xlink:type="simple"/></inline-formula> at a constant rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e339" xlink:type="simple"/></inline-formula>.</p>
        <p>Defining the neuron density in the same way as before<disp-formula id="pcbi.1002872.e340"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e340" xlink:type="simple"/><label>(37)</label></disp-formula>the continuity equation is<disp-formula id="pcbi.1002872.e341"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e341" xlink:type="simple"/><label>(38)</label></disp-formula>The action, constructed according to the procedure outlined in the <xref ref-type="sec" rid="s4">Methods</xref> section, is<disp-formula id="pcbi.1002872.e342"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e342" xlink:type="simple"/><label>(39)</label></disp-formula>where the population part of the action is<disp-formula id="pcbi.1002872.e343"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e343" xlink:type="simple"/></disp-formula>and the part representing the synaptic drive is<disp-formula id="pcbi.1002872.e344"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e344" xlink:type="simple"/><label>(40)</label></disp-formula>Mean field theory is given by<disp-formula id="pcbi.1002872.e345"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e345" xlink:type="simple"/><label>(41)</label></disp-formula>Note that because the firing rate is constant at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e346" xlink:type="simple"/></inline-formula>, the input to the synaptic drive is only dependent upon <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e347" xlink:type="simple"/></inline-formula> and not directly on the synaptic drive itself.</p>
        <p>It is useful to examine the steady state of this model in some detail. For a constant drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e348" xlink:type="simple"/></inline-formula>, the steady state obeys<disp-formula id="pcbi.1002872.e349"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e349" xlink:type="simple"/><label>(42)</label></disp-formula>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e350" xlink:type="simple"/></inline-formula>, this solution is a unimodal distribution peaked at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e351" xlink:type="simple"/></inline-formula> whose width narrows in proportion to the size of the input. Conversely, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e352" xlink:type="simple"/></inline-formula>, the peak is at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e353" xlink:type="simple"/></inline-formula>. The higher the input, the more likely it is that any given neuron will be found near the firing phase, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e354" xlink:type="simple"/></inline-formula>. The synaptic drive variable must satisfy a consistency condition:<disp-formula id="pcbi.1002872.e355"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e355" xlink:type="simple"/><label>(43)</label></disp-formula>This equation can be viewed as the steady state solution to a Wilson-Cowan type rate equation. The firing rate for the quadratic integrate-and-fire model is given, in the mean field approximation, by<disp-formula id="pcbi.1002872.e356"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e356" xlink:type="simple"/><label>(44)</label></disp-formula>In steady-state we have<disp-formula id="pcbi.1002872.e357"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e357" xlink:type="simple"/><label>(45)</label></disp-formula>so that we can identify <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e358" xlink:type="simple"/></inline-formula> as the “gain” function for the neurons of type <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e359" xlink:type="simple"/></inline-formula>.</p>
        <p>The linear response for the coupled theta model is given by the equations:<disp-formula id="pcbi.1002872.e360"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e360" xlink:type="simple"/></disp-formula>where again <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e361" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e362" xlink:type="simple"/></inline-formula>.</p>
        <p>Consider the steady state and transform the angle variable for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e363" xlink:type="simple"/></inline-formula> with<disp-formula id="pcbi.1002872.e364"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e364" xlink:type="simple"/><label>(46)</label></disp-formula>Then we have<disp-formula id="pcbi.1002872.e365"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e365" xlink:type="simple"/><label>(47)</label></disp-formula>This change of variables makes the steady state uniform in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e366" xlink:type="simple"/></inline-formula> for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e367" xlink:type="simple"/></inline-formula>. The equations for the linear response in steady state in terms of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e368" xlink:type="simple"/></inline-formula> are<disp-formula id="pcbi.1002872.e369"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e369" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e370" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e371" xlink:type="simple"/></inline-formula> (note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e372" xlink:type="simple"/></inline-formula>).</p>
        <p>The linear response for the theta model is most easily expressed in terms of the Laplace variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e373" xlink:type="simple"/></inline-formula> and is given by<disp-formula id="pcbi.1002872.e374"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e374" xlink:type="simple"/><label>(48)</label></disp-formula>where<disp-formula id="pcbi.1002872.e375"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e375" xlink:type="simple"/></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e376" xlink:type="simple"/></inline-formula> is similar to the linear response of the synaptic drive in the phase model with the addition of the feedback response of the population through the filter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e377" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e378" xlink:type="simple"/></inline-formula> is the same as in the phase model with this transformation. It is a series of pulses with the pulse shape given by the linear response and the pulse times determined by the firing times of a fictitious oscillator driven at rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e379" xlink:type="simple"/></inline-formula>.</p>
        <p>We also have<disp-formula id="pcbi.1002872.e380"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e380" xlink:type="simple"/><label>(49)</label></disp-formula>These results produce the primary qualitative difference between the phase and the theta models. The first term in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e381" xlink:type="simple"/></inline-formula> is analogous to the phase model calculation. It represents a perturbation of adding a single oscillator with initial coordinate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e382" xlink:type="simple"/></inline-formula> evolving at rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e383" xlink:type="simple"/></inline-formula>. The second term and the non-zero value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e384" xlink:type="simple"/></inline-formula> arise from the non-uniform distribution of the steady state, which arises from the functional dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e385" xlink:type="simple"/></inline-formula> of the neural input function. This term produces deviations from the “Poisson” behavior of the firing rate fluctuations.</p>
        <p>We can use these expressions to compute the tree level correlations with:<disp-formula id="pcbi.1002872.e386"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e386" xlink:type="simple"/></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e387" xlink:type="simple"/></inline-formula>. The other correlation functions are given by<disp-formula id="pcbi.1002872.e388"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e388" xlink:type="simple"/></disp-formula>and<disp-formula id="pcbi.1002872.e389"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e389" xlink:type="simple"/></disp-formula>These are more difficult to put in closed form, other than in terms of the response function for the synaptic drive. Instead we show numerical results.</p>
        <p>We can use the linear response formulas above to compute analytic formula for steady state. Changing coordinates and using the steady state mean field values we have<disp-formula id="pcbi.1002872.e390"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e390" xlink:type="simple"/></disp-formula>where the Laplace transform of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e391" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1002872.e392"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e392" xlink:type="simple"/></disp-formula>and<disp-formula id="pcbi.1002872.e393"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e393" xlink:type="simple"/></disp-formula>is the firing rate of the population in steady state. The correlations in the synaptic drive variable has the same basic form as that of the phase model. Because of the structure of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e394" xlink:type="simple"/></inline-formula> it will also have the same pulse behavior at an interval defined by a fictitious oscillator evolving according to the population activity. The primary difference is the replacement of the response function for the synaptic drive with the response for the theta coupling and the firing rate with the theta model firing rate.</p>
        <p>The two-neuron density function, by contrast, is different by virtue of the non-uniform nature of the steady state. In this case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e395" xlink:type="simple"/></inline-formula> so there will be a contribution at first order in the perturbation expansion (i.e. tree level) to the density fluctuations. Similarly, there is an extra term for the correlation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e396" xlink:type="simple"/></inline-formula>. Each of these correlation functions is only computable in closed form in terms of the response functions, which we compute numerically.</p>
        <p>The firing rate fluctuations for the theta model are simpler than the phase model because the input for each neuron is the constant 2 at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e397" xlink:type="simple"/></inline-formula>. For the firing rate obeying<disp-formula id="pcbi.1002872.e398"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e398" xlink:type="simple"/><label>(50)</label></disp-formula>the second moment of the firing rate is<disp-formula id="pcbi.1002872.e399"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e399" xlink:type="simple"/><label>(51)</label></disp-formula>for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e400" xlink:type="simple"/></inline-formula>. The equal time second moment is given by<disp-formula id="pcbi.1002872.e401"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e401" xlink:type="simple"/><label>(52)</label></disp-formula>where the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e402" xlink:type="simple"/></inline-formula> term has the same meaning as in the phase model. In the phase model case, the analogous expression to the first term on the right hand side was zero, and the population firing rate appeared to be the firing rate of the average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e403" xlink:type="simple"/></inline-formula> Poisson firing neurons. In the theta model case, however, there is a correction of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e404" xlink:type="simple"/></inline-formula>. From (52), it is simple to show that the firing rate fluctuations in a bin of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e405" xlink:type="simple"/></inline-formula> obey<disp-formula id="pcbi.1002872.e406"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e406" xlink:type="simple"/><label>(53)</label></disp-formula>Comparisons between analytic and numerical results for the quadratic integrate-and-fire model are given in <xref ref-type="fig" rid="pcbi-1002872-g002">Figure 2</xref>. In (a) through (e), the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e407" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e408" xlink:type="simple"/></inline-formula> are constant and homogeneous. One can see the qualitative similarity between the phase and quadratic integrate-and-fire models in the behavior of the activity correlations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e409" xlink:type="simple"/></inline-formula>. Both share the same pulsatile behavior driven by the fictitious oscillator, i.e. both show the spectral characteristics inherited from the single neuron dynamics. The density fluctuations, however, have an effect on the fluctuations in the firing rate. These effects can be seen in <xref ref-type="fig" rid="pcbi-1002872-g002">Figures 2 (c), (d)</xref>. In addition to the nontrivial firing rate fluctuation dynamics, the quadratic integrate-and-fire model also shows near-critical behavior, owing to the phase transition between the “asynchronous state” and synchronous firing. For a population with no external drive, this transition occurs at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e410" xlink:type="simple"/></inline-formula>. With <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e411" xlink:type="simple"/></inline-formula>, as in <xref ref-type="fig" rid="pcbi-1002872-g002">Figure 2 (c,)</xref> this represents a configuration in which the system is usually not firing, but with the occasional neuron moving across threshold. The reader is encouraged to draw an analogy with “avalanche” dynamics, in which the population will briefly fire in bursts and then go silent. While there is a small but fixed average firing rate, the fluctuations are large owing to this transient behavior. Even a small drive will regularize the system, as in <xref ref-type="fig" rid="pcbi-1002872-g002">Figure 2 (d)</xref>. The finite size expansion is expected to break down near a phase transition, accordingly here it is expected to breakdown at the onset of synchrony. The breakdown of the expansion is evident in <xref ref-type="fig" rid="pcbi-1002872-g002">Figure 2 (c)</xref>, where one can see enormous discrepancy between the analytic and numerical computations. <xref ref-type="fig" rid="pcbi-1002872-g002">Figure 2 (e)</xref> shows the two-time correlation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e412" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e413" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1002872-g002">Figure 2 (f)</xref> shows the effects of heterogeneity on the synaptic drive, where the drive distribution was chosen to be uniform, with inputs to each neuron chosen from the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e414" xlink:type="simple"/></inline-formula>. The oscillations in the synaptic drive are damped and there is an effective increase in the mean drive fluctuations as expected from the theory. Again the heterogeneity is the dominant contribution to the fluctuations, as can be seen by comparing <xref ref-type="fig" rid="pcbi-1002872-g002">figures 2 (a) and 2 (f)</xref>, which differ by close to a factor of six in steady state. <xref ref-type="fig" rid="pcbi-1002872-g003">Figure 3</xref> shows a comparison of the firing rate fluctuations. In contrast to the Phase Model, there is non-trivial temporal behavior owing to the phase dependence of the neuron dynamics.</p>
        <fig id="pcbi-1002872-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002872.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Quadratic integrate-and-fire model.</title>
            <p><bold>A</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e415" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e416" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e417" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e418" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e419" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e420" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e421" xlink:type="simple"/></inline-formula> (bottom) neurons. <bold>B</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e422" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e423" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e424" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e425" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e426" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e427" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e428" xlink:type="simple"/></inline-formula> (bottom) neurons. <bold>C</bold>. Numerical computations (green line) and analytical predictions (black line) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e429" xlink:type="simple"/></inline-formula> (top) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e430" xlink:type="simple"/></inline-formula> (bottom) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e431" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e432" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e433" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e434" xlink:type="simple"/></inline-formula>. <bold>D</bold>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e435" xlink:type="simple"/></inline-formula> (top) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e436" xlink:type="simple"/></inline-formula> (bottom) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e437" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e438" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e439" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e440" xlink:type="simple"/></inline-formula>, where the Poisson contribution has been subtracted. <bold>E</bold>. Two-time correlator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e441" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e442" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e443" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e444" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e445" xlink:type="simple"/></inline-formula>. <bold>F</bold> Equal time correlators in a heterogeneous network; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e446" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e447" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e448" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e449" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e450" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e451" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e452" xlink:type="simple"/></inline-formula> is taken from the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e453" xlink:type="simple"/></inline-formula> for each neuron. Ensemble average for all simulations are taken over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e454" xlink:type="simple"/></inline-formula> samples.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002872.g002" position="float" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002872-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002872.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Numerical computations (green line) and analytical predictions (black line) of the firing rate fluctuations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e455" xlink:type="simple"/></inline-formula> for the quadratic integrate-and-fire model for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e456" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e457" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e458" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e459" xlink:type="simple"/></inline-formula> (top), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e460" xlink:type="simple"/></inline-formula> (middle), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e461" xlink:type="simple"/></inline-formula> (bottom) neurons with Poisson contribution subtracted.</title>
            <p>Ensemble average is taken over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e462" xlink:type="simple"/></inline-formula> samples.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002872.g003" position="float" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have constructed a system size expansion for the density formulation of spiking neural networks and computed the fluctuations and correlations of network variables to lowest order. In particular, we explicitly calculate two-neuron and higher order moments in the network. We have demonstrated our method in globally coupled networks with two different neuron types. We note that all the fluctuations and correlations are “finite-size” effects, i.e. they do not exist in mean field theory. There will also be finite-size effects on the mean firing rate and synaptic drive, which could also be calculated using our methods. However, in the systems we studied, the finite-size corrections to the mean field density in the steady state are necessarily zero by neuron conservation. The steady state is uniform and the fluctuation effects will not (for these models) break the symmetry.</p>
      <p>The method is based on the Klimontovich equation, which is an exact formal continuity equation for the finite-size neuron density. Solutions to the Klimontovich equation only exist in the weak or distributional sense because the neuron density is a collection of Dirac delta functionals and is not differentiable. In the limit of infinite system size, it can be shown that under some conditions, the neuron density becomes a smooth function that obeys a strong continuity equation called the Vlasov equation <xref ref-type="bibr" rid="pcbi.1002872-Ichimaru1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Liboff1">[46]</xref> that describes the mean field dynamics of the system. Previous work on large networks of coupled oscillators took the infinite system size limit immediately and started with the Vlasov equation <xref ref-type="bibr" rid="pcbi.1002872-Desai1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Treves1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Abbott1">[16]</xref>. If the oscillators are subjected to white noise, then the Vlasov equation becomes the McKean-Vlasov equation <xref ref-type="bibr" rid="pcbi.1002872-Mirollo1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Mirollo2">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Gutkin1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-McKean1">[56]</xref>, which has sometimes been erroneously called a nonlinear Fokker-Planck equation. Recent work has put these density mean field methods onto a rigorous mathematical footing <xref ref-type="bibr" rid="pcbi.1002872-Faugeras1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Touboul2">[31]</xref>. These authors prove that under reasonable assumptions, a network of stochastically coupled neurons under various conditions conditions will obey the McKean-Vlasov equation (Vlasov equation with diffusion) in the mean field limit. The network obeys the “propagation of chaos” property where neurons that are initially statistically independent will remain independent and the fluctuations are purely Gaussian. They also show that a self-consistent set of moment equations for the mean and variance when stochastically forced.</p>
      <p>Our approach is based on the traditional Gibbs picture of statistical mechanics, to wit: the variability in the dynamics (in the absence of externally supplied noise or explicitly probabilistic dynamics) is a reflection of the distribution of “microscopic dynamics” which are consistent with the “macroscopic dynamics”, population level variables such as the global coupling, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e463" xlink:type="simple"/></inline-formula>. The fluctuations in the firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e464" xlink:type="simple"/></inline-formula> arise from the variability across neuron distributions which are approximately consistent with the mean field value. Those variables which converge to well defined values as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e465" xlink:type="simple"/></inline-formula> define the set of “macroscopic” variables. In the examples we have shown, the global coupling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e466" xlink:type="simple"/></inline-formula> and the population density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e467" xlink:type="simple"/></inline-formula> are considered macroscopic. In a more general network, such as one with heterogeneous coupling, the identification of macroscopic variables is likely to be a more complex issue. Put another way, in our simple cases there is a clear sense of the “typical” system for large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e468" xlink:type="simple"/></inline-formula> to which all initial conditions and parameters approach. There is no general requirement that “typical” systems exist.</p>
      <p>The Gibbs picture is realized by taking the ensemble average of the Klimontovich equation, which leads to a moment hierarchy where lower ordered moments (or cumulants) of the neuron density depend on higher order moments. The moment hierarchy is an exact ensemble averaged description of the finite-size system. However, in general, solving the moment equations is as difficult if not more difficult than integrating the original system directly. For systems with a well defined large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e469" xlink:type="simple"/></inline-formula> limit, the moments, such as the two-neuron correlation function, represent the finite size effects. Estimates for the moments can be obtained by truncating the moment hierarchy and solving a reduced system of equations, wherein <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e470" xlink:type="simple"/></inline-formula> is a natural expansion parameter.</p>
      <p>A truncated moment hierarchy is still unwieldy to solve. Our approach is to compute the moments directly by constructing a formal expression for the probability density functional of this distribution. This density functional is a “doubly” infinite dimensional object since its elements are infinite dimensional functions. Its formal construction hinges on the fact that it is proportional to a point mass (in an infinite dimensional functional space) located at a population density function that obeys the Klimontovich equation. Intuitively, this can be thought of as a Dirac delta functional with the Klimontovich operator as an argument. This expression is rendered computationally useful by noting that a Dirac delta functional in infinite dimensions has a Laplace transform representation where the integration is over a space of functions or fields and a set of imaginary response fields corresponding to the Laplace transform variables. The exponent of the integrand is called the action and fully specifies the distribution over the neuron density and synaptic drive.</p>
      <p><xref ref-type="sec" rid="s4">Methods</xref> developed in quantum and statistical field theory are then employed to construct perturbative expansions for desired quantities such as moments. The expansions use the infinite dimensional analogue of the method of steepest descents. The action is expanded around a critical point at which the gradient is zero. The critical point condition yields mean field theory. The first order correction, or tree level, expands the action to quadratic order yielding an infinite dimensional Gaussian integral. The integral has a closed form expression in terms of the inverse of the Hessian matrix, which is analogous to the inverse of the covariance matrix of a finite dimensional normal distribution. Just as in a finite dimensional steepest descent expansion, the terms in the perturbative series will be in terms of the elements of the inverse of the Hessian matrix, which in our case correspond to the equations satisfied by the linear responses. Hence, the perturbative expansion of the time dependent moments of the coupled network will be in terms of the linear responses.</p>
      <p>Previously, we applied this strategy to the Kuramoto model where oscillators are coupled directly through their phase differences. The corresponding action is a function of the population density together with the response field. The linear response satisfies the linear Vlasov equation. The tree level expression for the second moment of the population density, which captures the fluctuations due to finite-size effects, is identical to a solution of the truncated moment hierarchy known as the Lenard-Balescu solution in plasma physics <xref ref-type="bibr" rid="pcbi.1002872-Ichimaru1">[45]</xref>. Here, we consider a network of neurons coupled via synapses that are triggered whenever a given neuron fires. Hence, the field theory now involves the density and synaptic drive fields with their auxiliary fields. There are now four linear response functions, which makes the computations more complex.</p>
      <p>Finite size effects were considered by Brunel and Hakim <xref ref-type="bibr" rid="pcbi.1002872-Brunel1">[17]</xref>. They assumed that the connections were sparse enough so that the arrival times of synaptic events at a given neuron would be uncorrelated. They then assumed that these inputs could be modeled by a Poisson process that was scaled by the number of inputs. We considered the opposite regime of a fully connected network. We find that for the phase model, the Poisson ansatz is essentially correct to order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e471" xlink:type="simple"/></inline-formula>. The theory of coupled diffusions in probability theory provides an explanation called “propagation of chaos” where the uncertainty in the initial conditions is propagated forward by the deterministic dynamics of the system <xref ref-type="bibr" rid="pcbi.1002872-Gutkin1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-McKean1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Baladron2">[57]</xref>.</p>
      <p>Our approach generates a natural explanation for Poisson like firing rates in a population of neurons. Indeed, it is a natural consequence of the neurons firing in a stable asynchronous state. The number of neurons firing is the number of neurons out of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e472" xlink:type="simple"/></inline-formula> randomly chosen that fall into a small bin of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e473" xlink:type="simple"/></inline-formula> around the firing threshold. In the limit of large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e474" xlink:type="simple"/></inline-formula>, this should follow a Poisson distribution. For this reason, Poisson firing of the population is a natural assumption. However, as we have shown, if the neurons have some phase dependence in their voltage evolution, this will produce fluctuations in the firing rate beyond the simple sampling induced Poisson fluctuations.</p>
      <p>The mean field theory for our system is comparable to a differential equation form of the spike response theory <xref ref-type="bibr" rid="pcbi.1002872-Gerstner2">[27]</xref>. The use of phase oscillators allows for a continuity equation without a jump condition at the boundaries in a threshold crossing integrate-and-fire neuron. It may be possible to perform a similar finite size expansion within the spike response theory by incorporating the boundary conditions. The mean field equations have Wilson-Cowan rate equation form in that all the inputs to population activity enters through the firing rate function. This arises because of our choice of the global synaptic drive dynamics where the synaptic inputs of the population are first summed and then “filtered”. If we had instead chosen synaptic dynamics such that the synaptic inputs are first filtered and then summed, we would arrive at the “Amari” formulation of the mean field equations in which the external inputs to the activity equation lie outside of the rate function.</p>
      <p>We considered the example of an all-to-all network. In this case, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e475" xlink:type="simple"/></inline-formula>-neuron joint distribution for the network obeys exchangeability, which means that the marginalization of the distribution over any set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e476" xlink:type="simple"/></inline-formula> neurons yields the same distribution. For such a system, the neuron density function is a complete description of the network. However, we can always write down a neuron density function for any network even if it does not posses an exchange symmetry. For such a situation, the density function still captures useful global dynamics of the network. In the case of heterogeneous neuron parameters, as we considered here, the network is exchangeable in the infinite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e477" xlink:type="simple"/></inline-formula> mean field limit and close to exchangeable for large but finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e478" xlink:type="simple"/></inline-formula>. Hence, our formalism is directly applicable in this case. Such networks are said to be “self-averaging” in that the large network can be divided into sub networks, whose average behavior mirrors the full network. However, the situation with heterogeneous connection weights is more complicated. In such a system, it is not certain that the network is self-averaging in the infinite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e479" xlink:type="simple"/></inline-formula> limit. If so then the mean field equations are not a useful description of the system. An analogy can be drawn to spin glasses, where depending on parameters, the system may or may not be self-averaging. The conditions under which a heterogeneous network of spiking neurons is self-averaging is a question that we wish to pursue in the future.</p>
      <p>However, even in the case of a heterogeneous network without self-averaging, we can still apply our formalism if we consider the network to be comprised of local populations which exhibit exchangeability <xref ref-type="bibr" rid="pcbi.1002872-Faugeras1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1002872-Touboul2">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002872-Buice2">[43]</xref>. In this case, each local population would be represented by its own neuron density, which are then coupled to other neuron densities. Each local density would obey its own Klimontovich equation and corresponding moment hierarchy. If the local populations are sufficiently large then the hierarchies can be truncated in a finite-size expansion as shown here. However, even if the local populations are not large or even consisting of a single neuron, our formalism could still be applied. A moment hierarchy or density functional for the entire system could still be constructed. Although a perturbation expansion cannot be constructed using the inverse system size as a small parameter, an expansion could still be constructed using some other small parameter such as the inverse of a slow synaptic time constant or the inverse of the number of connections. The mean field limit would consist of a network of coupled local activity fields. This could then be generalized to a network of coupled moment equations such as the activity and correlations. We had previously derived generalized activity equations for an abstract spike count model <xref ref-type="bibr" rid="pcbi.1002872-Buice2">[43]</xref>.</p>
      <p>There is always a tension in computational neuroscience between detailed realistic models versus simpler reduced models. The main purpose of this work is to build quantitative tools to bridge the gap between the two approaches. We have developed a principled method of coarse-graining a neural system that is relatable to experimentally accessible quantities. Even with the exponential increase in available data and computational power, detailed realistic modeling will still have limitations. For one, a large scale simulation of the brain may not necessarily be easier to understand than the brain itself. An exhaustive exploration of parameter space will be intractable even if Moore's law holds up for centuries. Thus, there will always be a role for theoretical analysis of simple models. However, one of the criticisms of reduced models is that they are <italic>ad hoc</italic> and cannot be easily linked to the underlying physiology. Hence, there is a need for methods to derive reduced models directly from detailed models. Additionally, one would also like to derive reduced models that can incorporate single neuron effects such as synchronization and correlated firing, which are lost in classical mean field models. This motivated our desire to derive generalized activity equations that include such discrete neuron effects. Applications for generalized activity equations include studying the effects of correlation-based learning rules as seen in spike-timing dependent plasticity, understanding the role of oscillations in motor and sensory processing, and probing the neurophysiological basis of cognitive disorders by analyzing how perturbations to neural parameters affect cortical circuit function.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Action and generating functionals</title>
        <p>The population statistics of the network is encoded in a hierarchy of moment functions of the population density, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e480" xlink:type="simple"/></inline-formula> and the synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e481" xlink:type="simple"/></inline-formula>. We now show that these moments can be systematically encoded into a generating functional specified by an action, from which each can be calculated via perturbation theory. The system is fully specified by <xref ref-type="disp-formula" rid="pcbi.1002872.e014">Equations (2)</xref>, <xref ref-type="disp-formula" rid="pcbi.1002872.e051">(5)</xref>, and <xref ref-type="disp-formula" rid="pcbi.1002872.e052">(6)</xref>, which we rewrite as<disp-formula id="pcbi.1002872.e482"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e482" xlink:type="simple"/><label>(2′)</label></disp-formula><disp-formula id="pcbi.1002872.e483"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e483" xlink:type="simple"/><label>(6)</label></disp-formula>where we have substituted (5) into (2) and the equations are subject to appropriate initial and boundary conditions.</p>
        <p>We wish to derive a probability density functional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e484" xlink:type="simple"/></inline-formula> for the dynamical variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e485" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e486" xlink:type="simple"/></inline-formula>, from which we can derive all statistical measures for the network dynamics. We can factorize the density functional into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e487" xlink:type="simple"/></inline-formula> and compute the probabilities separately. The density functionals are usually represented in terms of an action, which for the networks we consider are given by (11) and (39).</p>
        <p>The derivation is applicable to any dynamical system, so we derive it for a generic variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e488" xlink:type="simple"/></inline-formula> that is governed by the differential equation<disp-formula id="pcbi.1002872.e489"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e489" xlink:type="simple"/><label>(54)</label></disp-formula>with an initial probability density for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e490" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e491" xlink:type="simple"/></inline-formula>. The dynamical system is fully deterministic and the density functional will describe the ensemble of many such systems starting from different initial conditions. Given the probability density at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e492" xlink:type="simple"/></inline-formula>, the probability density at a later time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e493" xlink:type="simple"/></inline-formula> can be written as<disp-formula id="pcbi.1002872.e494"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e494" xlink:type="simple"/><label>(55)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e495" xlink:type="simple"/></inline-formula> is the solution of the dynamical system (54) with fixed initial condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e496" xlink:type="simple"/></inline-formula>.</p>
        <p>The generating function for the moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e497" xlink:type="simple"/></inline-formula> is given by the Laplace transform of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e498" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1002872.e499"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e499" xlink:type="simple"/><label>(56)</label></disp-formula>where the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e500" xlink:type="simple"/></inline-formula> is called the “response field”. The moments are obtained from the generating function by taking derivatives with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e501" xlink:type="simple"/></inline-formula> and setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e502" xlink:type="simple"/></inline-formula> to zero. The natural log of the generating function is called the cumulant or connected generating function. Derivatives of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e503" xlink:type="simple"/></inline-formula> generate cumulants, i.e. those contributions to the moments which cannot be factored into products of smaller moments. The nonlinear terms in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e504" xlink:type="simple"/></inline-formula> therefore represent the “noise” or correlations in the distribution being represented. For example, the connected generating function for a Gaussian with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e505" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e506" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e507" xlink:type="simple"/></inline-formula> and for a Poisson distribution with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e508" xlink:type="simple"/></inline-formula> it is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e509" xlink:type="simple"/></inline-formula>.</p>
        <p>Inserting (55) into (56) yields<disp-formula id="pcbi.1002872.e510"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e510" xlink:type="simple"/></disp-formula>where we have used the inverse Laplace transform for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e511" xlink:type="simple"/></inline-formula>. Setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e512" xlink:type="simple"/></inline-formula> and taking <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e513" xlink:type="simple"/></inline-formula> to be much smaller than any time scale in (54) allows us to write the solution as an Euler step<disp-formula id="pcbi.1002872.e514"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e514" xlink:type="simple"/><label>(57)</label></disp-formula>which leads to<disp-formula id="pcbi.1002872.e515"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e515" xlink:type="simple"/><label>(58)</label></disp-formula>Any given time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e516" xlink:type="simple"/></inline-formula> can be divided into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e517" xlink:type="simple"/></inline-formula> subintervals of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e518" xlink:type="simple"/></inline-formula>. Repeated application of (58) then expresses the generating functional at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e519" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1002872.e520"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e520" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e521" xlink:type="simple"/></inline-formula>. Taking the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e522" xlink:type="simple"/></inline-formula> limit gives the functional or path integral<disp-formula id="pcbi.1002872.e523"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e523" xlink:type="simple"/></disp-formula>where the measure is defined as<disp-formula id="pcbi.1002872.e524"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e524" xlink:type="simple"/></disp-formula>with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e525" xlink:type="simple"/></inline-formula> integrations following a contour parallel to the imaginary axis and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e526" xlink:type="simple"/></inline-formula> integrations following a contour parallel to the real axis. The action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e527" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1002872.e528"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e528" xlink:type="simple"/><label>(59)</label></disp-formula>where we have integrated by parts and expressed the initial generating functional in terms of the cumulant generating functional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e529" xlink:type="simple"/></inline-formula>. Note that the bracketed term is the left hand side of the differential <xref ref-type="disp-formula" rid="pcbi.1002872.e489">equation (54)</xref>. This property is generic and provides a short cut for deriving the action. Because the initial distribution is normalized we have<disp-formula id="pcbi.1002872.e530"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e530" xlink:type="simple"/><label>(60)</label></disp-formula>The path integral thus defines a normalized measure when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e531" xlink:type="simple"/></inline-formula>. The generating function for the synaptic drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e532" xlink:type="simple"/></inline-formula> will directly follow this prescription, where the initial probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e533" xlink:type="simple"/></inline-formula> is for similarly prepared networks. The action will have the form of (59), with (2′) replacing the ODE for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e534" xlink:type="simple"/></inline-formula>.</p>
        <p>For the population density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e535" xlink:type="simple"/></inline-formula>, the generating function becomes a generating functional and the expectation value which defines it is a functional integral over the possible values of the field <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e536" xlink:type="simple"/></inline-formula>. Again we introduce a response field <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e537" xlink:type="simple"/></inline-formula> in order to define<disp-formula id="pcbi.1002872.e538"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e538" xlink:type="simple"/><label>(61)</label></disp-formula>where the expectation value is taken over the ensemble of similarly prepared networks. The experimental preparation of the network is equivalent to choosing the initial network configuration from some ensemble distribution. We will address the exact form of this distribution below, but for now it will suffice to note that this implies an initial generating functional for the initial time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e539" xlink:type="simple"/></inline-formula>. We focus on the time evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e540" xlink:type="simple"/></inline-formula> here. The derivation above in terms of the single variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e541" xlink:type="simple"/></inline-formula> works equally well in the network case (consider the arguments to the field as indices for a configuration vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e542" xlink:type="simple"/></inline-formula>). The probability density functional of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e543" xlink:type="simple"/></inline-formula> at a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e544" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1002872.e545"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e545" xlink:type="simple"/><label>(62)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e546" xlink:type="simple"/></inline-formula> is the solution to the Klimontovich <xref ref-type="disp-formula" rid="pcbi.1002872.e052">equation (6)</xref>. This produces the probability density functional<disp-formula id="pcbi.1002872.e547"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e547" xlink:type="simple"/><label>(63)</label></disp-formula>with action<disp-formula id="pcbi.1002872.e548"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e548" xlink:type="simple"/><label>(64)</label></disp-formula>The action completely defines the system and all moments of the ensemble distribution can be computed from it. However, in general, closed form expressions will not be possible and thus perturbation theory is used. The appearance of the factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e549" xlink:type="simple"/></inline-formula> tells us immediately how to calculate finite size corrections to the infinite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e550" xlink:type="simple"/></inline-formula> network in terms of a perturbative expansion in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e551" xlink:type="simple"/></inline-formula> (cf. a steepest descent evaluation of a standard integral with integrand <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e552" xlink:type="simple"/></inline-formula> for large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e553" xlink:type="simple"/></inline-formula>).</p>
        <p>For the coupled system, we have both the synaptic variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e554" xlink:type="simple"/></inline-formula> as well as the population density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e555" xlink:type="simple"/></inline-formula>. The action for the coupled system is just the sum of the actions for the variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e556" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e557" xlink:type="simple"/></inline-formula>.<disp-formula id="pcbi.1002872.e558"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e558" xlink:type="simple"/><label>(65)</label></disp-formula>where coupling terms have been included implicitly in the dependence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e559" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e560" xlink:type="simple"/></inline-formula> upon each other.</p>
        <sec id="s4a1">
          <title>Initial distributions</title>
          <p>The initial values of the generating functions will be determined by the ensemble distribution of the initial state. In the simplest case, we will consider that the initial state of the synaptic drive variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e561" xlink:type="simple"/></inline-formula> is fixed; furthermore, we will choose it to be fixed at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e562" xlink:type="simple"/></inline-formula>. This means that<disp-formula id="pcbi.1002872.e563"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e563" xlink:type="simple"/></disp-formula>The initial state of the population density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e564" xlink:type="simple"/></inline-formula> is imposed by the N-neuron distribution of the initial state of the network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e565" xlink:type="simple"/></inline-formula> (note that we use the terminology of plasma physics where the ensemble distribution is equivalent to an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e566" xlink:type="simple"/></inline-formula> variable joint probability density function). In order to compute the initial state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e567" xlink:type="simple"/></inline-formula>, we must compute the following ensemble average over this distribution.<disp-formula id="pcbi.1002872.e568"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e568" xlink:type="simple"/><label>(66)</label></disp-formula>Using the definition of the population density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e569" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1002872.e042">equation (4)</xref>), we can write<disp-formula id="pcbi.1002872.e570"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e570" xlink:type="simple"/><label>(67)</label></disp-formula>where the index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e571" xlink:type="simple"/></inline-formula> runs over the neurons in the network. This means that the initial generating functional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e572" xlink:type="simple"/></inline-formula> is equivalent to<disp-formula id="pcbi.1002872.e573"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e573" xlink:type="simple"/><label>(68)</label></disp-formula>which is the generating function for the ensemble distribution specified by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e574" xlink:type="simple"/></inline-formula>. Consider an initial distribution that is independent for each neuron, which means that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e575" xlink:type="simple"/></inline-formula> factors into a product over all neurons in the network. Thus<disp-formula id="pcbi.1002872.e576"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e576" xlink:type="simple"/><label>(69)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e577" xlink:type="simple"/></inline-formula> is the one-neuron distribution function marginalized from the N-neuron distribution. We choose the notational convention <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e578" xlink:type="simple"/></inline-formula>. The first term of an expansion of the logarithm in (69) about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e579" xlink:type="simple"/></inline-formula> gives precisely the term which would appear in a Poisson distribution. The remaining terms account for the sampling corrections to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e580" xlink:type="simple"/></inline-formula>-neuron distributions due to a finite number of neurons.</p>
          <p>If the neurons are not prepared independently then the expressions for the connected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e581" xlink:type="simple"/></inline-formula>-neuron distributions (such as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e582" xlink:type="simple"/></inline-formula>) will appear as coefficients of powers of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e583" xlink:type="simple"/></inline-formula> in the exponent along with a combinatoric factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e584" xlink:type="simple"/></inline-formula>, e.g.<disp-formula id="pcbi.1002872.e585"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e585" xlink:type="simple"/><label>(70)</label></disp-formula>assuming the other connected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e586" xlink:type="simple"/></inline-formula>-neuron distributions are zero at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e587" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s4a2">
          <title>Doi-Peliti-Janssen transformation</title>
          <p>Just as the nonlinear terms  in the cumulant generating function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e588" xlink:type="simple"/></inline-formula> are the “noise” terms, the nonlinear terms in the response fields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e589" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e590" xlink:type="simple"/></inline-formula> in the actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e591" xlink:type="simple"/></inline-formula> determine the correlations in the fields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e592" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e593" xlink:type="simple"/></inline-formula>. Since the dynamics are deterministic, the initial distribution for the ensemble is the only part of the action which provides non-trivial correlations. This follows because the only introduction of “noise” per se has been through the the fact that the initial conditions and parameters of the network are drawn from a distribution. However, once those are decided, the dynamics are fixed and completely deterministic. It is difficult to compute the effects of fluctuations due to the initial state because the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e594" xlink:type="simple"/></inline-formula> that appears in the initial generating functional. This term is “linearized” by a transformation similar to a Cole-Hopf transformation, which we call the Doi-Peliti-Jannsen transformation <xref ref-type="bibr" rid="pcbi.1002872-Janssen1">[58]</xref>, given by<disp-formula id="pcbi.1002872.e595"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e595" xlink:type="simple"/><label>(71)</label></disp-formula>The form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e596" xlink:type="simple"/></inline-formula> is specified by the Poisson distribution, while the form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e597" xlink:type="simple"/></inline-formula> is derived by imposing the requirement that the transformation preserves bilinear derivative forms, i.e.<disp-formula id="pcbi.1002872.e598"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e598" xlink:type="simple"/><label>(72)</label></disp-formula>These boundary terms do not contribute to the moments and can be ignored. The Doi-Peliti-Jannsen transformation replaces the Poisson term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e599" xlink:type="simple"/></inline-formula> in the action with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e600" xlink:type="simple"/></inline-formula>. Hence, an action which is bilinear in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e601" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e602" xlink:type="simple"/></inline-formula> represents a Markov counting process whose solution is a Poisson distribution with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e603" xlink:type="simple"/></inline-formula>.</p>
          <p>In a more general case, the Doi-Peliti Janssen transformation provides an elegant means of expanding around Poisson solutions and is thus useful for models whose statistics should be near Poisson, such as population densities in networks, in which the statistics are essentially coupled counting processes, though not simple ones. The moments of the variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e604" xlink:type="simple"/></inline-formula> are the joint distributions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e605" xlink:type="simple"/></inline-formula> with the finite size sampling corrections removed. We call these moments factorial moments or <italic>normal ordered moments</italic>, borrowing the terminology from the field theory literature. The moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e606" xlink:type="simple"/></inline-formula> do not include the effects of coincident indices, which is to say they are moments from a distribution without replacement (i.e. there is no probability of drawing the same neuron twice). The distribution implied by the moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e607" xlink:type="simple"/></inline-formula> is derived from drawing with replacement.</p>
        </sec>
      </sec>
      <sec id="s4b">
        <title>Feynman Rules for neural models</title>
        <p>The neural models we describe have two different fields, one for the synaptic drive variable and one for the density variable (along with the response field counterparts). As above, the class of models we consider is given by<disp-formula id="pcbi.1002872.e608"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e608" xlink:type="simple"/><label>(2′)</label></disp-formula><disp-formula id="pcbi.1002872.e609"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e609" xlink:type="simple"/><label>(6)</label></disp-formula>Each term in the expansion of any given moment (such as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e610" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e611" xlink:type="simple"/></inline-formula>) can be represented in an economical fashion via the use of diagrams. The basic elements of these diagrams are completely determined by the action, as derived above. To begin, we expand each action about some solution of mean field theory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e612" xlink:type="simple"/></inline-formula>, i.e. shift the variables by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e613" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e614" xlink:type="simple"/></inline-formula>. This gives us <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e615" xlink:type="simple"/></inline-formula>, where<disp-formula id="pcbi.1002872.e616"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e616" xlink:type="simple"/><label>(73)</label></disp-formula>Each term in the action (post expansion) containing anything other than precisely one response field and one configuration field is called a “vertex” term because these terms constrain the types of vertices for our diagrams. The terms with one response field and one configuration field are linear responses and correspond to edges of the graphs. For our models, the linear responses are the solutions of<disp-formula id="pcbi.1002872.e617"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e617" xlink:type="simple"/></disp-formula>There are four linear response functions, and so four types of edges. Time is considered in diagrams to move from right to left. Edges are represented by a combination of solid and dashed lines. A completely solid line represents the linear response <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e618" xlink:type="simple"/></inline-formula>, i.e. a response in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e619" xlink:type="simple"/></inline-formula> due to a linear perturbation in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e620" xlink:type="simple"/></inline-formula>. Completely dashed lines represent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e621" xlink:type="simple"/></inline-formula>, i.e. a response in the density due to perturbations in the density. Mixed edges represent the “off diagonal” linear responses, with the perturbation on the rightward end of the edge and the configuration variable on the left edge.</p>
        <p>Graphs are constructed by connecting the vertices shown in <xref ref-type="fig" rid="pcbi-1002872-g004">Figure 4</xref> using the four edges defined by the linear response. The terms in the diagrams are constructed by multiplying each vertex factor shown in <xref ref-type="fig" rid="pcbi-1002872-g004">Figure 4</xref> by the factors of the linear response corresponding to each of the edges and integrating over the open variables indicated by each vertex. Moments are given by the sum of all diagrams with open edges corresponding to the variables in the moment, e.g. the moment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e622" xlink:type="simple"/></inline-formula> is given by the sum of all graphs with two leftward edges that end in solid lines. Finally, it can be shown <xref ref-type="bibr" rid="pcbi.1002872-ZinnJustin1">[59]</xref> that the order of each diagram in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e623" xlink:type="simple"/></inline-formula> is given by the number of “loops” in the topology of each graph, with higher moments having “tree level” graphs (those with no loops) of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e624" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e625" xlink:type="simple"/></inline-formula> is the order of the moment, i.e. the tree level graphs for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e626" xlink:type="simple"/></inline-formula> are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e627" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002872-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002872.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Vertices of the Feynman Rules for the neural models.</title>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002872.g004" position="float" xlink:type="simple"/>
        </fig>
        <p>The graphs for the two point correlations are shown in <xref ref-type="fig" rid="pcbi-1002872-g005">Figures 5</xref>. They correspond to the following terms:<disp-formula id="pcbi.1002872.e635"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e635" xlink:type="simple"/><label>(74)</label></disp-formula><disp-formula id="pcbi.1002872.e636"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e636" xlink:type="simple"/><label>(75)</label></disp-formula><disp-formula id="pcbi.1002872.e637"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e637" xlink:type="simple"/><label>(76)</label></disp-formula>and the variations in the density are given by<disp-formula id="pcbi.1002872.e638"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e638" xlink:type="simple"/><label>(77)</label></disp-formula>and<disp-formula id="pcbi.1002872.e639"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e639" xlink:type="simple"/><label>(78)</label></disp-formula>where we've assumed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e640" xlink:type="simple"/></inline-formula>. Higher moments can be constructed by considering higher order diagrams.</p>
        <fig id="pcbi-1002872-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002872.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Feynman diagrams for the connected two point correlation functions in the neural field models.</title>
            <p>By row they are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e641" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e642" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e643" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002872.g005" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s4c">
        <title>Reduction to ODEs</title>
        <p>In order to compute the linear response for the quadratic integrate-and-fire model, we use a reduction to a simple system of ODEs. We start with the propagators in steady state in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e644" xlink:type="simple"/></inline-formula> representation:<disp-formula id="pcbi.1002872.e645"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e645" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1002872.e646"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e646" xlink:type="simple"/><label>(79)</label></disp-formula>Define<disp-formula id="pcbi.1002872.e647"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e647" xlink:type="simple"/><label>(80)</label></disp-formula>Then we have<disp-formula id="pcbi.1002872.e648"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e648" xlink:type="simple"/><label>(81)</label></disp-formula>We are interested in solving for the value at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e649" xlink:type="simple"/></inline-formula>. Define<disp-formula id="pcbi.1002872.e650"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e650" xlink:type="simple"/><label>(82)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e651" xlink:type="simple"/></inline-formula>. Also define<disp-formula id="pcbi.1002872.e652"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e652" xlink:type="simple"/><label>(83)</label></disp-formula>The equations for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e653" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e654" xlink:type="simple"/></inline-formula> are<disp-formula id="pcbi.1002872.e655"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e655" xlink:type="simple"/><label>(84)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e656" xlink:type="simple"/></inline-formula> are defined such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e657" xlink:type="simple"/></inline-formula>.</p>
        <p>Let's derive equations for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e658" xlink:type="simple"/></inline-formula>'s. Taking the time derivative gives us<disp-formula id="pcbi.1002872.e659"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e659" xlink:type="simple"/><label>(85)</label></disp-formula>a second derivative gets us<disp-formula id="pcbi.1002872.e660"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e660" xlink:type="simple"/><label>(86)</label></disp-formula>So the pair of propagators involving <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e661" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1002872.e662"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e662" xlink:type="simple"/><label>(87)</label></disp-formula>The same procedure works for the other pair to give us<disp-formula id="pcbi.1002872.e663"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e663" xlink:type="simple"/><label>(88)</label></disp-formula></p>
        <sec id="s4c1">
          <title>Propagators convolved with initial conditions</title>
          <p>We start with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e664" xlink:type="simple"/></inline-formula>. Convolving the relevant pair of propagators with the initial (steady state) density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e665" xlink:type="simple"/></inline-formula> gives us<disp-formula id="pcbi.1002872.e666"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e666" xlink:type="simple"/><label>(89)</label></disp-formula>This reduces to the same set of equations as for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002872.e667" xlink:type="simple"/></inline-formula> with the addition of a constant driving term.<disp-formula id="pcbi.1002872.e668"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002872.e668" xlink:type="simple"/><label>(90)</label></disp-formula>All of the reduced equations were solved numerically with the midpoint method.</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002872-Wilson1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>J</given-names></name> (<year>1972</year>) <article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title>. <source>Biophysical Journal</source> <volume>12</volume>: <fpage>1</fpage>–<lpage>24</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Wilson2">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>J</given-names></name> (<year>1973</year>) <article-title>A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue</article-title>. <source>Biological Cybernetics</source> <volume>13</volume>: <fpage>55</fpage>–<lpage>80</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Cohen1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1983</year>) <article-title>Absolute Stability of Global Pattern Formation and Parallel Memory Storage by Competitive Neural Networks</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics</source> <volume>13</volume>: <fpage>815</fpage>–<lpage>826</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Amari1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>S</given-names></name> (<year>1977</year>) <article-title>Dynamics of pattern formation in lateral-inhibition type neural fields</article-title>. <source>Biological Cybernetics</source> <volume>27</volume>: <fpage>77</fpage>–<lpage>87</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Coombes1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coombes</surname><given-names>S</given-names></name> (<year>2005</year>) <article-title>Waves, bumps, and patterns in neural field theories</article-title>. <source>Biological Cybernetics</source> <volume>93</volume>: <fpage>91</fpage>–<lpage>108</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Ermentrout1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ermentrout</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>JD</given-names></name> (<year>1979</year>) <article-title>A mathematical theory of visual hallucination patterns</article-title>. <source>Biological Cybernetics</source> <volume>34</volume>: <fpage>137</fpage>–<lpage>150</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Bressloff1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bressloff</surname><given-names>PC</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Golubitsky</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Thomas</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Wiener</surname><given-names>MC</given-names></name> (<year>2001</year>) <article-title>Geometric visual hallucinations, Euclidean symmetry and the functional architecture of striate cortex</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>356</volume>: <fpage>299</fpage>–<lpage>330</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Gerstner1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2000</year>) <article-title>Population dynamics of spiking neurons: Fast transients, asynchronous states, and locking</article-title>. <source>Neural computation</source> <volume>12</volume>: <fpage>43</fpage>–<lpage>89</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Knight1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knight</surname><given-names>B</given-names></name> (<year>1972</year>) <article-title>The relationship between the firing rate of a single neuron and the level of activity in a population of neurons</article-title>. <source>The Journal of General Physiology</source> <volume>59</volume>: <fpage>767</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Knight2">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knight</surname><given-names>B</given-names></name> (<year>1972</year>) <article-title>Dynamics of encoding in a population of neurons</article-title>. <source>The Journal of General Physiology</source> <volume>59</volume>: <fpage>734</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Desai1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desai</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Zwanzig</surname><given-names>R</given-names></name> (<year>1978</year>) <article-title>Statistical mechanics of a nonlinear stochastic model</article-title>. <source>Journal of Statistical Physics</source> <volume>19</volume>: <fpage>1</fpage>–<lpage>24</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Mirollo1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mirollo</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Strogatz</surname><given-names>S</given-names></name> (<year>2007</year>) <article-title>The Spectrum of the Partially Locked State for the Kuramoto Model</article-title>. <source>Journal of Nonlinear Science</source> <volume>17</volume>: <fpage>309</fpage>–<lpage>347</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Strogatz1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strogatz</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Mirollo</surname><given-names>R</given-names></name> (<year>1991</year>) <article-title>Stability of incoherence in a population of coupled oscillators</article-title>. <source>Journal of Statistical Physics</source> <volume>63</volume>: <fpage>613</fpage>–<lpage>635</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Mirollo2">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mirollo</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Strogatz</surname><given-names>S</given-names></name> (<year>2005</year>) <article-title>The spectrum of the locked state for the Kuramoto model of coupled oscillators</article-title>. <source>Physica D: Nonlinear Phenomena</source> <volume>205</volume>: <fpage>249</fpage>–<lpage>266</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Treves1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name> (<year>1993</year>) <article-title>Mean-field analysis of neuronal spike dynamics</article-title>. <source>Network: Computation in Neural Systems</source> <volume>4</volume>: <fpage>259</fpage>–<lpage>284</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Abbott1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name>, <name name-style="western"><surname>van Vreeswijk</surname><given-names>C</given-names></name> (<year>1993</year>) <article-title>Asynchronous states in networks of pulse-coupled oscillators</article-title>. <source>Physical Review E</source> <volume>48</volume>: <fpage>1483</fpage>–<lpage>1490</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Brunel1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname><given-names>V</given-names></name> (<year>1999</year>) <article-title>Fast global oscillations in networks of integrate-and-fire neurons with low firing rates</article-title>. <source>Neural computation</source> <volume>11</volume>: <fpage>1621</fpage>–<lpage>1671</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Brunel2">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2000</year>) <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Journal of computational neuroscience</source> <volume>8</volume>: <fpage>183</fpage>–<lpage>208</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Fourcaud1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fourcaud</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2002</year>) <article-title>Dynamics of the firing probability of noisy integrate-and-fire neurons</article-title>. <source>Neural computation</source> <volume>14</volume>: <fpage>2057</fpage>–<lpage>2110</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Cai1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Shelley</surname><given-names>M</given-names></name>, <name name-style="western"><surname>McLaughlin</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>An Effective Kinetic Representation of Fluctuation-Driven Neuronal Networks with Application to Simple and Complex Cells in Visual Cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>101</volume>: <fpage>7757</fpage>–<lpage>7762</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Rangan1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rangan</surname><given-names>AV</given-names></name>, <name name-style="western"><surname>Cai</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Maximum-entropy closures for kinetic theories of neuronal network dynamics</article-title>. <source>Physical Review Letters</source> <volume>96</volume>: <fpage>178101</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Mattia1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mattia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Del Giudice</surname><given-names>P</given-names></name> (<year>2002</year>) <article-title>Population dynamics of interacting spiking neurons</article-title>. <source>Physical Review E</source> <volume>66</volume>: <fpage>051917</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Omurtag1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Omurtag</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Knight</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Sirovich</surname><given-names>L</given-names></name> (<year>2000</year>) <article-title>On the simulation of large populations of neurons</article-title>. <source>Journal of computational neuroscience</source> <volume>8</volume>: <fpage>51</fpage>–<lpage>63</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Omurtag2">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Omurtag</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Knight</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Sirovich</surname><given-names>L</given-names></name> (<year>2000</year>) <article-title>Dynamics of Neuronal Populations: The Equilibrium Solution</article-title>. <source>SIAM J Appl Math</source> <volume>60</volume>: <fpage>2009</fpage>–<lpage>2028</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Nykamp1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nykamp</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tranchina</surname><given-names>D</given-names></name> (<year>2000</year>) <article-title>A population density approach that facilitates large-scale modeling of neural networks: Analysis and an application to orientation tuning</article-title>. <source>Journal of computational neuroscience</source> <volume>8</volume>: <fpage>19</fpage>–<lpage>50</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Nykamp2">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nykamp</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tranchina</surname><given-names>D</given-names></name> (<year>2001</year>) <article-title>A population density approach that facilitates large-scale modeling of neural networks: extension to slow inhibitory synapses</article-title>. <source>Neural computation</source> <volume>13</volume>: <fpage>511</fpage>–<lpage>546</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Gerstner2">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>1995</year>) <article-title>Time structure of the activity in neural network models</article-title>. <source>Physical Review E</source> <volume>51</volume>: <fpage>738</fpage>–<lpage>758</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Faugeras1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faugeras</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Touboul</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Cessac</surname><given-names>B</given-names></name> (<year>2009</year>) <article-title>A constructive mean-field analysis of multi-population neural networks with random synaptic weights and stochastic inputs</article-title>. <source>Frontiers in computational neuroscience</source> <volume>3</volume>: <fpage>1</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Baladron1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baladron</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Fasoli</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Faugeras</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Touboul</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Mean-field description and propagation of chaos in networks of Hodgkin-Huxley and FitzHugh-Nagumo neurons</article-title>. <source>Journal of mathematical neuroscience</source> <volume>2</volume>: <fpage>10</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Touboul1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Touboul</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Mean-field equations for stochastic firing-rate neural fields with delays: Derivation and noise-induced transitions</article-title>. <source>Physica D: Nonlinear Phenomena</source> <volume>241</volume>: <fpage>1223</fpage>–<lpage>1244</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Touboul2">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Touboul</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hermann</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Faugeras</surname><given-names>O</given-names></name> (<year>2012</year>) <article-title>Noise-induced behaviors in neural mean field dynamics</article-title>. <source>SIAM J Appl Dynamical Syst</source> <volume>11</volume>: <fpage>49</fpage>–<lpage>81</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Softky1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Softky</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>1993</year>) <article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs</article-title>. <source>Journal of Neuroscience</source> <volume>13</volume>: <fpage>334</fpage>–<lpage>350</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Mainen1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mainen</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name> (<year>1995</year>) <article-title>Reliability of spike timing in neocortical neurons</article-title>. <source>Science</source> <volume>268</volume>: <fpage>1503</fpage>–<lpage>1506</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Shadlen1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name> (<year>1998</year>) <article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title>. <source>The Journal of neuroscience</source> <volume>18</volume>: <fpage>3870</fpage>–<lpage>3896</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Vreeswijk1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vreeswijk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1998</year>) <article-title>Chaotic balanced state in a model of cortical circuits</article-title>. <source>Neural computation</source> <volume>10</volume>: <fpage>1321</fpage>–<lpage>1371</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Vreeswijk2">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vreeswijk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>Chaos in Neuronal Networks with Balanced Excitatory and Inhibitory Activity</article-title>. <source>Science</source> <volume>274</volume>: <fpage>1724</fpage>–<lpage>1726</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Sompolinsky1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Crisanti</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sommers</surname><given-names>H</given-names></name> (<year>1988</year>) <article-title>Chaos in random neural networks</article-title>. <source>Physical Review Letters</source> <volume>61</volume>: <fpage>259</fpage>–<lpage>262</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Sussillo1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sussillo</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2009</year>) <article-title>Generating Coherent Patterns of Activity from Chaotic Neural Networks</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>544</fpage>–<lpage>557</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Amit1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>1997</year>) <article-title>Dynamics of a recurrent network of spiking neurons before and following learning</article-title>. <source>Network: Computation in Neural Systems</source> <volume>8</volume>: <fpage>373</fpage>–<lpage>404</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Doiron1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doiron</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Rinzel</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Reyes</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Stochastic synchronization in finite size spiking networks</article-title>. <source>Physical Review E</source> <volume>74</volume>: <fpage>030903</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Hildebrand1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hildebrand</surname><given-names>EJ</given-names></name>, <name name-style="western"><surname>Buice</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Chow</surname><given-names>CC</given-names></name> (<year>2007</year>) <article-title>Kinetic Theory of Coupled Oscillators</article-title>. <source>Physical Review Letters</source> <volume>98</volume>: <fpage>054101</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Buice1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buice</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Chow</surname><given-names>CC</given-names></name> (<year>2007</year>) <article-title>Correlations, fluctuations, and stability of a finite-size network of coupled oscillators</article-title>. <source>Physical Review E</source> <volume>76</volume>: <fpage>031118</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Buice2">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buice</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Chow</surname><given-names>CC</given-names></name> (<year>2010</year>) <article-title>Systematic fluctuation expansion for neural network activity equations</article-title>. <source>Neural Computation</source> <volume>22</volume>: <fpage>377</fpage>–<lpage>426</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Buice3">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buice</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Chow</surname><given-names>CC</given-names></name> (<year>2011</year>) <article-title>Effective stochastic behavior in dynamical systems with incomplete information</article-title>. <source>Physical Review E</source> <volume>84</volume>: <fpage>051120</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Ichimaru1">
        <label>45</label>
        <mixed-citation publication-type="other" xlink:type="simple">Ichimaru S (1973) Basic principles of plasma physics, a statistical approach. New York: W.A. Benjamin.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Liboff1">
        <label>46</label>
        <mixed-citation publication-type="other" xlink:type="simple">Liboff RL (2003) Kinetic Theory. New York: Springer.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Gibbs1">
        <label>47</label>
        <mixed-citation publication-type="other" xlink:type="simple">Gibbs JW (1981) Elementary principles in statistical mechanics developed with especial reference to the rational foundation of thermodynamics. Toronto: University of Toronto Libraries.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Ermentrout2">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ermentrout</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kopell</surname><given-names>N</given-names></name> (<year>1991</year>) <article-title>Multiple pulse interactions and averaging in systems of coupled neural oscillators</article-title>. <source>Journal of Mathematical Biology</source> <volume>29</volume>: <fpage>195</fpage>–<lpage>217</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Winfree1">
        <label>49</label>
        <mixed-citation publication-type="other" xlink:type="simple">Winfree AT (2001) The geometry of biological time. New York: Springer Verlag.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Izhikevich1">
        <label>50</label>
        <mixed-citation publication-type="other" xlink:type="simple">Izhikevich EM (2007) Dynamical systems in neuroscience. the geometry of excitability and bursting. Cambridge, MA: The MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Ermentrout3">
        <label>51</label>
        <mixed-citation publication-type="other" xlink:type="simple">Ermentrout GB, Terman D (2010) Mathematical Foundations of Neuroscience. New York: Springer Verlag.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Ermentrout4">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ermentrout</surname><given-names>B</given-names></name> (<year>1996</year>) <article-title>Type I membranes, phase resetting curves, and synchrony</article-title>. <source>Neural computation</source> <volume>8</volume>: <fpage>979</fpage>–<lpage>1001</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Golomb1">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Golomb</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Hansel</surname><given-names>D</given-names></name> (<year>2000</year>) <article-title>The number of synaptic inputs and the synchrony of large, sparse neuronal networks</article-title>. <source>Neural computation</source> <volume>12</volume>: <fpage>1095</fpage>–<lpage>1139</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Gutkin1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gutkin</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Kac</surname><given-names>M</given-names></name> (<year>1983</year>) <article-title>Propagation of chaos and the Burgers equation</article-title>. <source>SIAM Journal on Applied Mathematics</source> <volume>43</volume>: <fpage>971</fpage>–<lpage>980</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Ermentrout5">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ermentrout</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kopell</surname><given-names>N</given-names></name> (<year>1986</year>) <article-title>Parabolic bursting in an excitable system coupled with a slow oscillation</article-title>. <source>SIAM Journal on Applied Mathematics</source> <fpage>233</fpage>–<lpage>253</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-McKean1">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McKean</surname><given-names>H</given-names><suffix>Jr</suffix></name> (<year>1966</year>) <article-title>A class of Markov processes associated with nonlinear parabolic equations</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>56</volume>: <fpage>1907</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Baladron2">
        <label>57</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baladron</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Fasoli</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Faugeras</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Touboul</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Mean Field description of and propagation of chaos in recurrent multipopulation networks of Hodgkin-Huxley and Fitzhugh-Nagumo neurons</article-title>. <source>J Math Neurosci</source> <volume>2</volume>: <fpage>10</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-Janssen1">
        <label>58</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Janssen</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Täuber</surname><given-names>U</given-names></name> (<year>2005</year>) <article-title>The field theory approach to percolation processes</article-title>. <source>Annals of Physics</source> <volume>315</volume>: <fpage>147</fpage>–<lpage>192</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002872-ZinnJustin1">
        <label>59</label>
        <mixed-citation publication-type="other" xlink:type="simple">Zinn-Justin J (2002) Quantum field theory and critical phenomena. New York: Oxford University Press.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>