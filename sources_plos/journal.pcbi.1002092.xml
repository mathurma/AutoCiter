<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-10-00210</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002092</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Cognitive neuroscience</subject>
              <subj-group>
                <subject>Decision making</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Circuit models</subject>
                <subject>Coding mechanisms</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Developmental neuroscience</subject>
              <subj-group>
                <subject>Neural circuit formation</subject>
                <subject>Synaptic plasticity</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Animal cognition</subject>
              <subject>Behavioral neuroscience</subject>
              <subject>Learning and memory</subject>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Population biology</subject>
            <subj-group>
              <subject>Population modeling</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Spatio-Temporal Credit Assignment in Neuronal Population Learning</article-title><alt-title alt-title-type="running-head">Credit Assignment in Neuronal Population Learning</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Friedrich</surname>
            <given-names>Johannes</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Urbanczik</surname>
            <given-names>Robert</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Senn</surname>
            <given-names>Walter</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Department of Physiology, University of Bern, Bern, Switzerland</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Gutkin</surname>
            <given-names>Boris S.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">École Normale Supérieure, College de France, CNRS, France</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">senn@pyl.unibe.ch</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: RU WS. Performed the experiments: JF. Analyzed the data: JF. Contributed reagents/materials/analysis tools: JF RU. Wrote the paper: RU WS.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>6</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>30</day>
        <month>6</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>6</issue><elocation-id>e1002092</elocation-id><history>
        <date date-type="received">
          <day>10</day>
          <month>11</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>2</day>
          <month>5</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Friedrich et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>In learning from trial and error, animals need to relate behavioral decisions to environmental reinforcement even though it may be difficult to assign credit to a particular decision when outcomes are uncertain or subject to delays. When considering the biophysical basis of learning, the credit-assignment problem is compounded because the behavioral decisions themselves result from the spatio-temporal aggregation of many synaptic releases. We present a model of plasticity induction for reinforcement learning in a population of leaky integrate and fire neurons which is based on a cascade of synaptic memory traces. Each synaptic cascade correlates presynaptic input first with postsynaptic events, next with the behavioral decisions and finally with external reinforcement. For operant conditioning, learning succeeds even when reinforcement is delivered with a delay so large that temporal contiguity between decision and pertinent reward is lost due to intervening decisions which are themselves subject to delayed reinforcement. This shows that the model provides a viable mechanism for temporal credit assignment. Further, learning speeds up with increasing population size, so the plasticity cascade simultaneously addresses the spatial problem of assigning credit to synapses in different population neurons. Simulations on other tasks, such as sequential decision making, serve to contrast the performance of the proposed scheme to that of temporal difference-based learning. We argue that, due to their comparative robustness, synaptic plasticity cascades are attractive basic models of reinforcement learning in the brain.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>The key mechanisms supporting memory and learning in the brain rely on changing the strength of synapses which control the transmission of information between neurons. But how are appropriate changes determined when animals learn from trial and error? Information on success or failure is likely signaled to synapses by neurotransmitters like dopamine. But interpreting this reward signal is difficult because the number of synaptic transmissions occurring during behavioral decision making is huge and each transmission may have contributed differently to the decision, or perhaps not at all. Extrapolating from experimental evidence on synaptic plasticity, we suggest a computational model where each synapse collects information about its contributions to the decision process by means of a cascade of transient memory traces. The final trace then remodulates the reward signal when the persistent change of the synaptic strength is triggered. Simulation results show that with the suggested synaptic plasticity rule a simple neural network can learn even difficult tasks by trial and error, e.g., when the decision - reward sequence is scrambled due to large delays in reward delivery.</p>
      </abstract><funding-group><funding-statement>This work was supported by the Swiss National Science Foundation (SNSF, Sinergia grant CRSIKO-122697) and a grant from the Swiss SystemsX.ch initiative (evaluated by the SNSF). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="13"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Learning from reinforcement involves widely differing spatial and temporal scales both within the behavioral decision making process itself as well as when relating decisions to outcomes. Since they are adaptive, synapses may be viewed as the elementary decision making entities in the brain. But the presynaptic input of any single synapse will contain only very limited information about the task and, further, the millisecond duration of a synaptic release is much shorter than behaviorally relevant time scales. The behavioral decision results from a spatio-temporal aggregation of synaptic releases which is highly non-linear due to e.g. thresholding in the generation of action potentials. Hence the relationship between any single synaptic release and the behavioral decision is not only tenuous but also non-linear.</p>
      <p>In relating behavioral decisions to rewarding or unrewarding outcomes, problems arise which are analogous to the ones encountered when relating synaptic releases to decisions. In the “spatial” domain: The state of the world is only partially observable, and hence, what appears to be one and the same decision may sometimes be rewarded and sometimes not. Also, in social interactions, reward may depend on the decisions of other players. In the temporal domain: Whether a decision was appropriate or not may not be immediately obvious and reward may even change with time. Proverbially, short term gain may lead to long term pain (and vice versa).</p>
      <p>Hence the spatio-temporal credit assignment problem arises: How can a synapse adapt given that reward delivery is delayed and also depends on the releases of many other synapses as well as on external factors? As one basic mechanism for addressing the temporal problem, theories of reinforcement learning use the eligibility trace, a quantity, decaying exponentially in time, which memorizes the elementary decision up to the time when information about reward becomes available to trigger the persistent adaptive change <xref ref-type="bibr" rid="pcbi.1002092-Sutton1">[1]</xref>. Here we point out that a cascade of such synaptic memory traces can in fact provide an integrated solution to the spatio-temporal credit assignment problem by remodulating the presynaptic signal in view of information arising at different stages of the behavioral decision making.</p>
      <p>Evidence for synaptic eligibility traces comes from experiments on spike timing dependent plasticity (STDP) where a synaptic release leads to longterm potentiation (LTP) if the neuron emits an action potential shortly thereafter <xref ref-type="bibr" rid="pcbi.1002092-Markram1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Bi1">[3]</xref>. Importantly, the length of the LTP-induction time window (some <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e001" xlink:type="simple"/></inline-formula>) is on the order of the membrane time constant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e002" xlink:type="simple"/></inline-formula>), i.e. it reflects the time during which the synaptic release has influence on somatic action potential generation. The release itself lasts only for some <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e003" xlink:type="simple"/></inline-formula>, so this form of LTP is most easily accounted for by assuming a local synaptic quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e004" xlink:type="simple"/></inline-formula> providing, just like an eligibility trace, a memory of the release which decays with time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e005" xlink:type="simple"/></inline-formula>. When an action potential is generated, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e006" xlink:type="simple"/></inline-formula> is read-out to determine a quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e007" xlink:type="simple"/></inline-formula> which, in the simplest interpretation of the STDP findings, gives the change (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e008" xlink:type="simple"/></inline-formula>) of the synaptic strength <xref ref-type="bibr" rid="pcbi.1002092-Song1">[4]</xref>. Simply equating <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e009" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e010" xlink:type="simple"/></inline-formula>, however, may be hasty because many repeated pre/post pairings are required in the STDP-protocol to induce a noticeable change. So it seems more reasonable to view <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e011" xlink:type="simple"/></inline-formula> as a second synaptic eligibility trace, keeping a running record of recent pre/post pairings to modulate synaptic strength, perhaps even in a non-linear manner.</p>
      <p>As has been widely noted <xref ref-type="bibr" rid="pcbi.1002092-Baras1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1002092-Gavornik1">[11]</xref>, one can connect the STDP-findings with reinforcement learning by assuming that the transcription of the second eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e012" xlink:type="simple"/></inline-formula> into the synaptic change <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e013" xlink:type="simple"/></inline-formula> is modulated by neurotransmitters like dopamine which provide feedback about external reward (<xref ref-type="fig" rid="pcbi-1002092-g001">Fig. 1A</xref>). Such plasticity rules address the spatial credit assignment problem for synapses sharing a postsynaptic neuron since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e014" xlink:type="simple"/></inline-formula> captures the relevant correlations between a given synaptic release and the releases of other synapses when they contribute to postsynaptic firing in the neuron. But <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e015" xlink:type="simple"/></inline-formula> does not take into account the interaction in decision making between synapses which have different postsynaptic neurons. For temporal credit assignment, the memory length of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e016" xlink:type="simple"/></inline-formula> must correspond to the delay between a synaptic release and the delivery of pertinent reward feedback. This delay consists of the time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e017" xlink:type="simple"/></inline-formula> needed to reach a behavioral decision and the time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e018" xlink:type="simple"/></inline-formula> for this decision to be rewarded. A value on the order of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e019" xlink:type="simple"/></inline-formula> s seems reasonable for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e020" xlink:type="simple"/></inline-formula>, but <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e021" xlink:type="simple"/></inline-formula> can easily be much longer, as in a game where multiple decisions are needed to reach a rewarding state. In this case, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e022" xlink:type="simple"/></inline-formula> simply averages pre/post pairing over multiple decisions even if the firing of the particular neuron was important only for some of the decisions.</p>
      <fig id="pcbi-1002092-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002092.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Plasticity cascades and decision making.</title>
          <p>(A) Synaptic plasticity cascades for reinforcement learning in the single neuron approach and (B) in the proposed population level approach. The meaning of the symbols is the following. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e023" xlink:type="simple"/></inline-formula>: synaptic eligibility traces, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e024" xlink:type="simple"/></inline-formula>: change in synaptic strength, <bold>pre</bold>: synaptic input, <bold>post</bold>: feedback from the postsynaptic neuron, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e025" xlink:type="simple"/></inline-formula>: external reward feedback, <bold>Dec</bold>: feedback about the behavioral decision. The symbol denotes low pass filtering with the time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e026" xlink:type="simple"/></inline-formula> given next to the symbol. (C) Sketch of the studied population model for reinforcement learning: A stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e027" xlink:type="simple"/></inline-formula> is read by a population of neurons yielding a spatio-temporal activity pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e028" xlink:type="simple"/></inline-formula> which depends on the synaptic strength of the neurons. A decision making circuitry transforms the population response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e029" xlink:type="simple"/></inline-formula> into a behavioral decision. The synaptic strength of the neurons should adapt so that population responses lead to behavioral decisions which maximize an external reward signal.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.g001" xlink:type="simple"/>
      </fig>
      <p>Here we propose extending the eligibility trace cascade by a further trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e030" xlink:type="simple"/></inline-formula> which takes into account the behavioral decision making process (<xref ref-type="fig" rid="pcbi-1002092-g001">Fig. 1B</xref>). Now the time constant of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e031" xlink:type="simple"/></inline-formula> is simply <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e032" xlink:type="simple"/></inline-formula>, since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e033" xlink:type="simple"/></inline-formula> only needs to capture pre/post pairings upto the time when a decision is reached. The decision triggers a transcription of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e034" xlink:type="simple"/></inline-formula> into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e035" xlink:type="simple"/></inline-formula> which is modulated by a feedback signal from the decision making circuitry and a signal derived from the firings of the postsynaptic neuron during the decision period. So while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e036" xlink:type="simple"/></inline-formula> only captures the pre/post correlations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e037" xlink:type="simple"/></inline-formula> additionally captures the post/decision correlations. The time constant of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e038" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e039" xlink:type="simple"/></inline-formula>, and when reward feedback does become available, the reward together with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e040" xlink:type="simple"/></inline-formula> determines the synaptic change <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e041" xlink:type="simple"/></inline-formula>.</p>
      <p>In <xref ref-type="supplementary-material" rid="pcbi.1002092.s001">Text S1</xref> we show that, for a population of spiking neurons feeding into a decision making circuitry (<xref ref-type="fig" rid="pcbi-1002092-g001">Fig. 1C</xref>), such a synaptic cascade can be mathematically derived by calculating the gradient of the expected reward. The resulting gradient ascent rule, however, has a few biologically undesirable aspects. For instance, it requires that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e042" xlink:type="simple"/></inline-formula> averages pre/post correlations over each decision period. Synapses, however, are unlikely to know when decision periods start and end. For biological realism, we present a modified rule in the main text, where e.g. the averaging over the decision period is replaced by low pass filtering. Learning in a population of spiking neurons using this synaptic plasticity rule is illustrated by simulation results. These show that learning speeds up with increasing population size and that learning speed degrades gracefully when the delay period between decision and reinforcement is increased. In particular, perfect performance is approached even when in the delay period the network has to make further decisions which themselves give rise to delayed reinforcement.</p>
      <p>Eligibility traces memorize information about the decision making upto the time when reinforcement becomes available. In contrast, temporal difference (TD) learning, the other basic approach for temporal credit assignment in reinforcement learning, back-propagates reward to the time of the decision. For this, TD-learning estimates the value of states, or state-decision pairs, where, in the simplest case, a state corresponds to a stimulus. The value itself is the (discounted) expected future reward when being in the state, or when making a particular decision in the state. The value can then serve as an immediately available surrogate for the delayed reward signal. During Pavlovian learning, a backward shift in time is observed for the appetitive reaction from the delayed unconditioned stimulus to the conditioned stimulus, and the shift is found as well in the activity of midbrain dopaminergic neurons. The backward shift also occurs in the value estimation error computed by a TD-algorithm modeling the conditioning task, when a state of the algorithm corresponds to the time elapsed since the presentation of the conditioning stimulus <xref ref-type="bibr" rid="pcbi.1002092-Schultz1">[12]</xref>. Further to this observation, there has been a surge of interest in modeling dopaminergic activity in terms of TD-learning concepts, as reviewed in <xref ref-type="bibr" rid="pcbi.1002092-Dayan1">[13]</xref>.</p>
      <p>Temporal difference algorithms are based on the assumption that the information available for decision making is rich enough to make the learning problem Markovian. This means that the future is independent of past events, given the current state accessible to the TD-learner. In contrast, eligibility trace based approaches such as our population learning do not require such a completeness of available information. Hence, we present simulation results comparing the performance of the proposed approach to that of TD-learning on tasks, where the Markovian assumption may be violated.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>The model</title>
        <p>We consider a population of leaky integrate and fire neurons driven by a common presynaptic stimulus and read-out by a decision making circuitry. To facilitate exploration both the population neurons and the decision making are stochastic. As in forced choice tasks, the decision circuitry determines a behavioral choice <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e043" xlink:type="simple"/></inline-formula> at the end of stimulus presentation, based on its monitoring of the population activity for the duration of the stimulus. We focus on binary decision making and denote the two possible behavioral choices by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e044" xlink:type="simple"/></inline-formula>. Immediately, or at some later point in time, a behavioral decision may influence whether reward is delivered to the system, but the decision may also impact the environment, i.e. influence the sequence of stimuli presented to the population neurons. Due to the last point, our framework goes beyond operant conditioning and also includes sequential decision tasks.</p>
        <p>For the decision making circuitry itself, we use a very simple model, assuming that it only considers the number of population neurons which fire in response to the stimulus: For low population activity the likely decision is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e045" xlink:type="simple"/></inline-formula>, but the probability of generating the decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e046" xlink:type="simple"/></inline-formula> increases with the number of neurons that respond by spiking to the stimulus. Given this decision making circuitry, we present a plasticity rule for the synapses of the population neurons, which enables the system to optimize the received reward.</p>
        <p>In presenting the plasticity rule we focus on one synapse, with synaptic strength <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e047" xlink:type="simple"/></inline-formula>, of one of the population neurons. (In the simulations, of course, the rule is applied to all synapses of all population neurons.) Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e048" xlink:type="simple"/></inline-formula> be the set of spike times representing the presynaptic spike train impinging on the synapse upto time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e049" xlink:type="simple"/></inline-formula>. A presynaptic spike at some time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e050" xlink:type="simple"/></inline-formula> leads to a brief synaptic release with a time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e051" xlink:type="simple"/></inline-formula> on the order of a millisecond. The postsynaptic effect of the release will however linger for a while, decaying only with the membrane time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e052" xlink:type="simple"/></inline-formula> which is in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e053" xlink:type="simple"/></inline-formula> range. The first synaptic eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e054" xlink:type="simple"/></inline-formula> bridges the gap between the two time scales by low pass filtering (<xref ref-type="fig" rid="pcbi-1002092-g002">Fig. 2</xref>, column 1). It evolves as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e055" xlink:type="simple"/><label>(1)</label></disp-formula></p>
        <fig id="pcbi-1002092-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002092.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Examples for the modulatory signals and the resulting traces in the plasticity cascade of a synapse.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.g002" xlink:type="simple"/>
        </fig>
        <p>Correlations between synaptic and post-synaptic activity are captured by transcribing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e063" xlink:type="simple"/></inline-formula> into a second trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e064" xlink:type="simple"/></inline-formula> of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e065" xlink:type="simple"/><label>(2)</label></disp-formula>see <xref ref-type="fig" rid="pcbi-1002092-g002">Fig. 2</xref>, column 2. The postsynaptic modulation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e066" xlink:type="simple"/></inline-formula> depends on the postsynaptic spike times and on the time course <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e067" xlink:type="simple"/></inline-formula> of the neuron's membrane potential. Denoting by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e068" xlink:type="simple"/></inline-formula> the set of postsynaptic spike times, the specific form we use for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e069" xlink:type="simple"/></inline-formula> is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e070" xlink:type="simple"/></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e071" xlink:type="simple"/></inline-formula> is Dirac's delta-function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e072" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e073" xlink:type="simple"/></inline-formula> are parameters given in <xref ref-type="sec" rid="s4">Methods</xref>.</p>
        <p>As has been previously shown <xref ref-type="bibr" rid="pcbi.1002092-Pfister1">[14]</xref>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e074" xlink:type="simple"/></inline-formula> is a useful factor in plasticity rules due to the following properties:</p>
        <list list-type="bullet">
          <list-item>
            <p>A small synaptic change proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e075" xlink:type="simple"/></inline-formula> reinforces the observed neuronal response, i.e. it increases the likelihood that the neuron reproduces the observed postsynaptic spike train on a next presentation of the same stimulus.</p>
          </list-item>
          <list-item>
            <p>Conversely, a small synaptic change proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e076" xlink:type="simple"/></inline-formula> impedes the observed neuronal response. It encourages responding by a different spike train on a next presentation of the stimulus and thus facilitates exploration.</p>
          </list-item>
        </list>
        <p>Thanks to these properties, plasticity rules where synaptic change is driven by the product of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e077" xlink:type="simple"/></inline-formula> and reward have been widely used in reinforcement learning models <xref ref-type="bibr" rid="pcbi.1002092-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Castro1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1002092-Vasilaki1">[17]</xref>. Due to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e078" xlink:type="simple"/></inline-formula>, the neuronal quantities modulating plasticity in these rules are not just the pre- and post synaptic firing times but also the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e079" xlink:type="simple"/></inline-formula>. This further modulatory factor also arises in models matching STDP-experiments which measure plasticity induction by more than two spikes <xref ref-type="bibr" rid="pcbi.1002092-Clopath1">[18]</xref>.</p>
        <p>In our model, the time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e080" xlink:type="simple"/></inline-formula> in Eq. (2) should be matched to the decision time during which stimuli are presented and we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e081" xlink:type="simple"/></inline-formula>. Since the match may be imperfect in reality, we denote the actual stimulus duration by the symbol <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e082" xlink:type="simple"/></inline-formula>. To describe the stochastic decision making in this period, we introduce the population activity variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e083" xlink:type="simple"/></inline-formula> which is reset each time one decision is made and subsequently increased when a neuron spikes for the first time in response to the next presented stimulus (<xref ref-type="fig" rid="pcbi-1002092-g002">Fig. 2</xref>, column 3). A high (low) value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e084" xlink:type="simple"/></inline-formula> at the end of the decision period biases the next behavioral decision towards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e085" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e086" xlink:type="simple"/></inline-formula>). We do not model the temporal accumulation of population activity leading to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e087" xlink:type="simple"/></inline-formula> explicitly in neural terms, since this could be achieved along the lines previously suggested in <xref ref-type="bibr" rid="pcbi.1002092-Wang1">[19]</xref>.</p>
        <p>Since the decision circuitry is stochastic, even for a fairly high level of population activity the behavioral decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e088" xlink:type="simple"/></inline-formula> may be made by chance. In this case, by spiking, a population neuron in fact decreased the likelihood of the behavioral choice which was actually taken, whereas a neuron that stayed silent made the choice more likely. Hence, when the goal is to reinforce a behavioral decision, a sensible strategy is to reinforce a neuronal response when it is aligned with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e089" xlink:type="simple"/></inline-formula> (firing for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e090" xlink:type="simple"/></inline-formula>, not firing for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e091" xlink:type="simple"/></inline-formula>) and to impede it when it is not aligned. To this end, the third eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e092" xlink:type="simple"/></inline-formula> captures the interactions between single neuron activity, population activity and behavioral decision. It evolves as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e093" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e094" xlink:type="simple"/></inline-formula> is a feedback signal, based on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e095" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e096" xlink:type="simple"/></inline-formula>, generated by the decision making circuitry and, further, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e097" xlink:type="simple"/></inline-formula> is determined by the postsynaptic activity of the neuron. Mathematically, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e098" xlink:type="simple"/></inline-formula> should reflect how the neuron contributed to the decision and equal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e099" xlink:type="simple"/></inline-formula> according to whether or not the neuron fired in response to the decision stimulus. The feedback signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e100" xlink:type="simple"/></inline-formula> should consist of pulses generated at the times when a decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e101" xlink:type="simple"/></inline-formula> is made. The value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e102" xlink:type="simple"/></inline-formula> should have the same sign as the corresponding decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e103" xlink:type="simple"/></inline-formula> and be modulated by the population activity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e104" xlink:type="simple"/></inline-formula> which gave rise to the decision. In particular, the magnitude of the pulse is large when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e105" xlink:type="simple"/></inline-formula> is close to the stochastic decision threshold, increasing synaptic plasticity in the cases where the decision making is still very explorative.</p>
        <p>Since the post-stimulus value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e106" xlink:type="simple"/></inline-formula> has the same sign as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e107" xlink:type="simple"/></inline-formula>, the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e108" xlink:type="simple"/></inline-formula> in Eq. (3) is positive when the neuronal response is aligned with the decision - otherwise it is negative. Because this term remodulates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e109" xlink:type="simple"/></inline-formula> during the transcription and in view of the above characterization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e110" xlink:type="simple"/></inline-formula>, the eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e111" xlink:type="simple"/></inline-formula> has the following property:</p>
        <list list-type="bullet">
          <list-item>
            <p>A small synaptic change proportional to the post-stimulus value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e112" xlink:type="simple"/></inline-formula> reinforces the neurons response when the response is aligned with the behavioral decision but, in the not aligned case, the response is impeded.</p>
          </list-item>
        </list>
        <p>Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e113" xlink:type="simple"/></inline-formula> encodes the correlations between the releases of the synapse and the behavioral decision, the final stage of the cascade becomes very simple (<xref ref-type="fig" rid="pcbi-1002092-g002">Fig. 2</xref>, column 4). It just remodulates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e114" xlink:type="simple"/></inline-formula> by reward to yield the synaptic change:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e115" xlink:type="simple"/><label>(4)</label></disp-formula>Mathematically, the reward function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e116" xlink:type="simple"/></inline-formula> should be made up of pulses at the times when external reinforcement information becomes available, with the height of each pulse proportional to the reward received at that time.</p>
        <p>The above description uses some mathematical idealizations which biologically are not quite realistic. We envisage that the reinforcement and decision feedback is delivered to the synapses by changes in levels of neurotransmitters such as dopamine, acetylcholine or norepinephrine <xref ref-type="bibr" rid="pcbi.1002092-Foehring1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1002092-Seol1">[22]</xref>. Then, in contrast to the pulses assumed above, the feedback read-out by the synapses should change only quite slowly. In our simulations, this is addressed by low pass filtering the above feedback pulses when obtaining the signals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e118" xlink:type="simple"/></inline-formula>. Further, we assumed above that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e119" xlink:type="simple"/></inline-formula> in Eq. (3) encodes whether the neuron fired in response to the decision stimulus. But it seems unrealistic, that a population neuron knows when a stimulus starts and ends. In the simulations we use low pass filtering to compute a version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e120" xlink:type="simple"/></inline-formula> which just encodes whether the neuron spiked recently, on a time scale given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e121" xlink:type="simple"/></inline-formula> (<xref ref-type="sec" rid="s4">Methods</xref>). Such a delayed feedback about postsynaptic activity could realistically be provided by calcium related signaling.</p>
      </sec>
      <sec id="s2b">
        <title>Learning stimulus-response associations with delayed reinforcement</title>
        <p>To study the proposed plasticity rule, we first consider an operant conditioning like task, where for each of the stimuli presented to the network, one of the two possible behavioral decisions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e122" xlink:type="simple"/></inline-formula> is correct. A correct decision is rewarded, whereas an incorrect one is penalized, but in both cases the delivery of reinforcement is delayed for some time. While operant conditioning with delayed reward has been widely considered in the context of temporal discounting <xref ref-type="bibr" rid="pcbi.1002092-Daw1">[23]</xref>, here, we are interested in a quite different issue. We do not wish to assume that little of relevance happens in the delay period between the decision and the corresponding reinforcement since this seems artificial in many real life settings. In the task we consider, during the delay period, other decisions need to be made which are themselves again subject to delayed reinforcement (<xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3A</xref>). Then temporal contiguity between decision and reward is no longer a proxy for causation. So the issue is not how to trade small immediate reward against a larger but later reward, but how to at all learn the association between decision and reward.</p>
        <fig id="pcbi-1002092-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002092.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Stimulus-response association with delayed reinforcement.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.g003" xlink:type="simple"/>
        </fig>
        <p>In the simulations, a stimulus is represented by a fixed spike pattern made up of 80 Poisson spike trains, each having a duration of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e148" xlink:type="simple"/></inline-formula> and a mean firing rate of 6 Hz. To allow for some variability, on each presentation of the stimulus, the spike times in the pattern are jittered by a zero mean Gaussian with a standard deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e149" xlink:type="simple"/></inline-formula>. This stimulus representation is used throughout the paper. In the present task, we use 10 stimuli and, for each, one of the two possible decisions is randomly assigned as the correct one. Stimuli are presented in random order and right after the decision on one stimulus has been made, the next stimulus is presented.</p>
        <p><xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3B</xref> shows learning curves for tasks where there is a fixed delay <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e150" xlink:type="simple"/></inline-formula> between each decision and the delivery of the reinforcement pertinent to that decision. Perfect performance is eventually approached, even for the largest value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e151" xlink:type="simple"/></inline-formula> considered. For this value, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e152" xlink:type="simple"/></inline-formula>, two other decisions are made in the delay period. Learning time increases in a stepwise manner when extending the delay, with a step occurring each time a further intervening decision has to be made in the delay period (<xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3B</xref> inset).</p>
        <p>To demonstrate that the proposed plasticity rule addresses the spatial credit assignment problem as well, we studied learning performance as function of the number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e153" xlink:type="simple"/></inline-formula> of population neurons. The results in <xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3C</xref> show that learning speeds up with increasing population size. In a larger population there are more synapses and the speedup indicates that the plasticity rule is capable of recruiting the additional synapses to enhance learning.</p>
        <p>To gauge robustness, we used the same synaptic plasticity parameters for all simulations in Panels B and C. In particular <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e154" xlink:type="simple"/></inline-formula> was always set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e155" xlink:type="simple"/></inline-formula> even though the actual delay <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e156" xlink:type="simple"/></inline-formula> in reward delivery is varied substantially in Panel B. To further highlight robustness, <xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3D</xref> shows the performance for different values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e157" xlink:type="simple"/></inline-formula> when the actual delay in reward delivery is fixed at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e158" xlink:type="simple"/></inline-formula>.</p>
        <p>In the above simulations the delay between decision and reward did not change from trial to trial. But the proposed plasticity rule does not rely on this for learning and also works with variable delays. This is shown in <xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3E</xref>, where a different, randomly chosen, delay <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e159" xlink:type="simple"/></inline-formula> was used on each trial.</p>
      </sec>
      <sec id="s2c">
        <title>Two armed bandit with intermittent reward</title>
        <p>To achieve near perfect performance in the above operant conditioning task, our network had to learn to make close to deterministic decisions. Here we show that, when appropriate, the architecture can also support stochastic decision making. For this we consider a two armed bandit where one of the two targets delivers a fixed reward of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e160" xlink:type="simple"/></inline-formula> when chosen. The second choice target (which we call intermittent) will deliver a reward of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e161" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e162" xlink:type="simple"/></inline-formula> depending on whether or not the target is baited. Baiting occurs on a variable interval schedule: Once the reward of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e163" xlink:type="simple"/></inline-formula> has been collected, the target becomes un-baited. It stays un-baited for between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e164" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e165" xlink:type="simple"/></inline-formula> time steps (randomly chosen) and is then baited again. Once baited, the target stays in this state until it is chosen. As a consequence, always choosing the intermittent target yields an average reward equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e166" xlink:type="simple"/></inline-formula>. This does not improve on choosing the fixed reward target and, hence, a better policy is to pick the intermittent target less frequently.</p>
        <p>We assume that our network does not have access to the past decisions it has made. Hence on every trial one and the same stimulus is presented to the network (with the same spike pattern statistics as in the previous subsection). The evolution of the average reward collected by the network is shown in <xref ref-type="fig" rid="pcbi-1002092-g004">Fig. 4A</xref>. Due to learning, average reward increases, reaching a value which is within <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e167" xlink:type="simple"/></inline-formula> of the reward achievable by the optimal stochastic policy. The probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e168" xlink:type="simple"/></inline-formula> of choosing the intermittent target decreases from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e169" xlink:type="simple"/></inline-formula> to around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e170" xlink:type="simple"/></inline-formula> as shown in <xref ref-type="fig" rid="pcbi-1002092-g004">Fig. 4B</xref>. This panel also plots the evolution of the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e171" xlink:type="simple"/></inline-formula> of choosing the intermittent target. The value being the expected reward collected from choosing the intermittent target assuming that the policy is to pick this target with a probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e172" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002092-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002092.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Two armed bandit with intermittent reward.</title>
            <p>Panels (A) and (B) plot the results for learning with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e173" xlink:type="simple"/></inline-formula> population neurons and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e174" xlink:type="simple"/></inline-formula>. The evolution of average reward per decision is shown in (A) and compared to the reward achievable by the optimal stochastic policy (dashed line). The latter was determined by Monte Carlo simulation. The probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e175" xlink:type="simple"/></inline-formula> of choosing the intermittent target is shown in (B) as well as the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e176" xlink:type="simple"/></inline-formula>, i.e the average reward obtained when choosing the intermittent target with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e177" xlink:type="simple"/></inline-formula>. Panels (C) and (D) show the asymptotic performance of TD-learning (reached after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e178" xlink:type="simple"/></inline-formula> trials) for different values of the inverse temperature <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e179" xlink:type="simple"/></inline-formula>. The red empty circles in panel (D) show the estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e180" xlink:type="simple"/></inline-formula> computed by the TD-algorithm. The full red circles give the exact value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e181" xlink:type="simple"/></inline-formula> for the choice probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e182" xlink:type="simple"/></inline-formula> used by the TD-algorithm (blue curve).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.g004" xlink:type="simple"/>
        </fig>
        <p>Asymptotically <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e183" xlink:type="simple"/></inline-formula> approaches a value around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e184" xlink:type="simple"/></inline-formula>. So choosing the intermittent target is much more rewarding on average than choosing the fixed target (which has a value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e185" xlink:type="simple"/></inline-formula>). Nevertheless, the intermittent target is chosen less frequently than the fixed target. This amounts to a strong deviation from matching or melioration theory <xref ref-type="bibr" rid="pcbi.1002092-Mazur1">[24]</xref> which stipulates that choice frequencies adjust up to the point where the value of the two choices becomes the same - this would lead to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e186" xlink:type="simple"/></inline-formula> in the present task. On a task similar to ours, deviations from matching and melioration, favoring a more global optimization of reward, have also been observed in a behavioral experiment with rats <xref ref-type="bibr" rid="pcbi.1002092-Baum1">[25]</xref>.</p>
        <p>Our plasticity rule, of course, does not explicitly value choices but directly adapts the choice policy to optimize overall reward. This is in contrast to temporal-difference (TD) based approaches to learning, where estimating the value of choices (or, more generally, the value of state-action pairs) is the key part of the learning procedure. Hence it is of interest to compare the above results to those obtainable with TD-learning.</p>
        <p>The two most common strategies in TD-learning for making decisions based on the valuation of choices are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e187" xlink:type="simple"/></inline-formula>-greedy and softmax. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e188" xlink:type="simple"/></inline-formula>-greedy the choice with the highest estimated value is taken with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e189" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e190" xlink:type="simple"/></inline-formula> is typically a small positive parameter. This does not allow for a fine grained control of the level of stochasticity in the decision making, so we will only consider softmax here. For softmax, a decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e191" xlink:type="simple"/></inline-formula> is made with a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e192" xlink:type="simple"/></inline-formula> related to its value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e193" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e194" xlink:type="simple"/></inline-formula>. Here the positive parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e195" xlink:type="simple"/></inline-formula>, called inverse temperature, modulates the level of stochasticity in the decision making. TD-theory does not give a prescription for choosing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e196" xlink:type="simple"/></inline-formula> and, hence, we will consider a large range of values for the inverse temperature. The results in panels 4C and 4D plot the asymptotic performance as function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e197" xlink:type="simple"/></inline-formula>. Panel 4c shows that the average reward achieved by the TD-learner decreases with increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e198" xlink:type="simple"/></inline-formula>. So best performance is obtained for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e199" xlink:type="simple"/></inline-formula>, i.e. when the choice valuations estimated during learning are irrelevant. The probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e200" xlink:type="simple"/></inline-formula> of choosing the intermittent target increases with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e201" xlink:type="simple"/></inline-formula>, Panel 4D. The panel also shows that the estimates of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e202" xlink:type="simple"/></inline-formula> computed by the TD-algorithm are in excellent agreement to the true values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e203" xlink:type="simple"/></inline-formula> for the policy characterized by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e204" xlink:type="simple"/></inline-formula>. Hence, the TD-learner fails to optimize reward not because the valuation of the decisions is wrong, but it fails because softmax is a poor strategy for transforming valuations into decisions in the present task.</p>
        <p>The root cause for the failure of TD-learning is that our decision task is not Markovian. Due to the variable interval schedule, the probability that the intermittent target is baited depends on the previous decisions made by the TD-learner. But as in the simulation on population learning, we have assumed that previous decisions are not memorized and the TD-learner is in the same state in each trial. Hence, even given the state accessible to the TD-learner, past events are nevertheless predictive of future ones because the information about the present encoded in the state is incomplete. This violates the Markovian assumption on which TD-learning theory is based. To rectify this, one needs to assume that decisions are made in view of previous decisions and outcomes. Given that the intermittent target can stay un-baited for a maximum of 12 steps, this requires a TD-learner which memorizes decisions and outcomes (reward/no reward) for the last 12 time steps. Hence, we simulated a TD-learner with the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e205" xlink:type="simple"/></inline-formula> states needed to represent the task history in sufficient detail to render the decision problem Markovian. We found that the algorithm after learning (with softmax and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e206" xlink:type="simple"/></inline-formula>) achieved an average reward of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e207" xlink:type="simple"/></inline-formula> per decision. The algorithm learned to employ sophisticated policies such as not choosing the intermittent target for 8 time steps after it had delivered reward - but polling it frequently thereafter until the intermittent target again delivered reward. Obviously such policies are beyond the scope of the simple memoryless stochastic decision making considered above.</p>
      </sec>
      <sec id="s2d">
        <title>Sequential decision making</title>
        <p>We next studied population learning in a sequential decision making task, where reward delivery is contingent on making a sequence of correct decisions. For this, a simple path finding task on a linear track was used (<xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5A</xref>). We imagine an owner who is tired of having to take his dog for a walk and wants to teach the animal to exercise all by itself. The dog is put in front of the door (position 1 on the track), can move left or right, and may be rewarded on coming home (position 0). But since the point is to exercise the dog, reward (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e208" xlink:type="simple"/></inline-formula>) is only delivered when the dog has reached position 3 at least once while moving on the track. If the dog comes home early without visiting the required position 3, the learning episode simply ends with neither reward or punishment. The episode ends in the same way if position 5 is ever reached (the dog should not run away).</p>
        <fig id="pcbi-1002092-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002092.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Sequential decision making.</title>
            <p>(A) Top row, sketch of the path finding task. Bottom row, example stochastic policy learned by the population when decisions are based on just the current position, arrow thickness represents probability of transition. (B) Evolution of the average reward per episode (blue) and the average number of steps per episode (red) for population learning with decisions based on current position. (C) Same as in (B), but for population learning with decisions based on the current and previous position. The above population simulations used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e209" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e210" xlink:type="simple"/></inline-formula>. (D) TD-learning with decisions based on the current and previous position. Average reward per episode (solid blue curve) and reward per episode in a typical single run (dotted blue). For this run, the green curve shows the evolution of the value assigned by the TD-learner to making a shortcut, i.e. to the state action pair (<sub>1</sub>2, <italic>left</italic>). Error bars show 1 SEM of the mean.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.g005" xlink:type="simple"/>
        </fig>
        <p>In an initial simulation, we assumed that decisions have to be made based just on the current position on the track. So the stimuli presented to the population just encode this position (using the same spike pattern statistics as in the previous tasks). Given such stimuli, our population model is faced with a non-Markovian decision problem because, the appropriateness of a decision may depend not just on the current stimulus but also on the stimuli which were previously encountered. For instance, whether one should go left or right in position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e211" xlink:type="simple"/></inline-formula> depends on whether position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e212" xlink:type="simple"/></inline-formula> has been visited already. In fact the learning problem is even more dire. When the basis of decision making is just the current position, complete failure will result for any deterministic policy which must lead to one of the following three outcomes: (i) direct exit from position 1 to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e213" xlink:type="simple"/></inline-formula>, (ii) exit at position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e214" xlink:type="simple"/></inline-formula>, (iii) an infinite cycle. This is not to say that nothing can be learned. As the result in the bottom row of <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5A</xref> shows, it is possible to increase the odds that an episode will end with reward delivery by adapting a stochastic policy. Initially the network was almost equally likely to go left or right in any position but after learning this has changed. In position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e215" xlink:type="simple"/></inline-formula> for instance left is much more likely than right, whereas, in position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e216" xlink:type="simple"/></inline-formula>, left is just a little bit more likely than right. After learning, the average number of steps per episode is lower than initially (<xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5B</xref>, red curve). So in terms of average reward per step taken, there is even more improvement through learning than suggested by the blue curve in <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5B</xref>. In the simulations we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e217" xlink:type="simple"/></inline-formula>. This is somewhat longer than the minimal time of 2.5 s (5 steps of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e218" xlink:type="simple"/></inline-formula> duration) needed from position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e219" xlink:type="simple"/></inline-formula> to reward delivery.</p>
        <p>Thanks to working memory, a real dog is of course entirely capable to collect reward by simply running from position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e220" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e221" xlink:type="simple"/></inline-formula> and then back to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e222" xlink:type="simple"/></inline-formula>. So for describing the behavior of an animal with a highly developed nervous system, the above model is woefully inadequate. Nevertheless, it may usefully account for behavior in the presence of working memory impairments. To allow for working memory, in a next set of simulations we switched to stimuli encoding not just the current but also the immediately preceeding position on the track. Of the 80 spike trains in a stimulus presented to the network, 50 were used to encode the current and 30 to encode the preceeding position (<xref ref-type="sec" rid="s4">Methods</xref>). Now, learning with the proposed plasticity rule converges towards perfect performance with the reward per episode approaching <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e223" xlink:type="simple"/></inline-formula> and the number of decision steps per episode approaching <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e224" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5C</xref>).</p>
        <p>It is worthwhile noting, that even with a working memory reaching one step back, the decision task is non-Markovian: For instance, knowing that coming from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e225" xlink:type="simple"/></inline-formula> we are now in position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e226" xlink:type="simple"/></inline-formula> does not allow us to tell whether moving left leads to reward. For this we would need to know if we have been in position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e227" xlink:type="simple"/></inline-formula>, say, two steps back. Technically, when remembering the sequence of past positions, the memory depth required to make the decision problem Markovian is infinite because any finite memory can be exhausted by cycling many times between positions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e228" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e229" xlink:type="simple"/></inline-formula>. The non-Markovian nature of the task is highlighted by <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5D</xref>, which shows simulation result for TD-learning. The specific algorithm used is SARSA with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e230" xlink:type="simple"/></inline-formula>-greedy decision making (see <xref ref-type="bibr" rid="pcbi.1002092-Sutton1">[1]</xref> and <xref ref-type="sec" rid="s4">Methods</xref>). Similarly to <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5C</xref>, we assumed that the states upon which the TD-learner bases decisions represents the current and the immediately preceeding position on the track. The solid blue curve in <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5D</xref>, computed by averaging performance over multiple runs of the algorithm, demonstrates that TD-learning does not converge towards perfect performance. The dotted blue curve, giving results for a typical single run, shows that in fact TD-learning leads to large irregular oscillations in performance, which are averaged away in the solid curve. While optimal performance is approached initially in the single run, the algorithm is not stable and at some point performance breaks down, initiating a new cycle in the oscillation.</p>
        <p>To understand the instability in more detail, we denote the states of the TD-learner by notation such as <sub>2</sub>1, meaning that coming from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e231" xlink:type="simple"/></inline-formula> the current position is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e232" xlink:type="simple"/></inline-formula>. The TD-learner assigns values to state-decision pairs, which we write as e.g. (<sub>2</sub>1, <italic>left</italic>), by estimating discounted future reward. Now consider the single run of the TD-learner (dotted blue curve, <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5D</xref>) after some 1500 episodes. The strategy then is close to optimal, so in most episodes when we are in state <sub>2</sub>1, i.e. on the inbound leg of the tour, position 3 will have previously been visited. Then <italic>left</italic> in <sub>2</sub>1 leads to immediate reward delivery, so the state-action pair (<sub>2</sub>1, left) has a high value. Next assume that we are on the outbound leg in state <sub>1</sub>2. Since the policy is close to optimal, in most episodes the next move is <italic>right</italic>, in order to visit position 3. But, due to exploration, the TD-learner will occasionally try the shortcut of going left in state <sub>1</sub>2, testing the state-action pair (<sub>1</sub>2, <italic>left</italic>). This leads to state <sub>2</sub>1 and then most likely to the high value decision <italic>left</italic>, terminating the episode without reward because the shortcut was taken. But the TD-learner updates the value of the tested state-action pair (<sub>1</sub>2, <italic>left</italic>) based not on the failure at the very end of the episode but based on the value of the subsequent state-action pair, in this case (<sub>2</sub>1, <italic>left</italic>). As noted above, the latter pair has high value, so the update increases the value of the shortcut (<sub>1</sub>2, <italic>left</italic>) even-though the shortcut resulted in failure (green curve in <xref ref-type="fig" rid="pcbi-1002092-g005">Fig. 5D</xref>). This happens most of the times when the shortcut is tested for exploration, leading to further increases in the green curve, upto the point where the value of (<sub>1</sub>2, <italic>left</italic>) is so high that making a shortcut becomes the dominant policy. This causes the observed breakdown in performance. In summary, a central idea in temporal difference learning is to handle non-immediate reward by back-propagating it in time via the valuations of intermediate state-decision pairs. This is mathematically justified in the Markovian case, but may lead to unexpected results for general sequential decision making tasks.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have presented a model of reinforcement learning in a population of spiking neurons read out by a decision making circuitry where plasticity induction is controlled by a cascade of synaptic memory traces. In each synapse of the population neurons, the presynaptic trace is in stages remodulated by somatic feedback, by feedback about the behavioral decision making and by an external reward signal before being consolidated into a persistent change of the synaptic strength. Simulation results show that this leads to robust learning performance in a variety of reinforcement tasks.</p>
      <p>Our model builds on, but goes beyond, the classical STDP findings <xref ref-type="bibr" rid="pcbi.1002092-Markram1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1002092-Bi1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1002092-Song2">[26]</xref>. On the neuronal level, we assume that plasticity does not only depend on the timings in a pre- and postsynaptic spike pair but that there is a further modulation by postsynaptic subthreshold activity. Such a modulation also arises when modeling the plasticity findings obtained when the standard STDP-protocol is extended to allow multi spike interactions <xref ref-type="bibr" rid="pcbi.1002092-Clopath1">[18]</xref>. For reinforcement learning, plasticity cannot be blind to activity-related downstream information. This matches with experimental observations revealing that the polarity and magnitude of STDP can in fact be regulated by neuromodulators such as dopamine, acetylcholine or noradrenaline which may even revert the sign of the synaptic change <xref ref-type="bibr" rid="pcbi.1002092-Zhang1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Matsuda1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Seol1">[22]</xref>, e.g. by entering after the mGluR signaling pathways <xref ref-type="bibr" rid="pcbi.1002092-Lynch1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1002092-Abraham1">[29]</xref>. Some recent research has further highlighted astrocytes as local communication elements which are capable of modulating synaptic plasticity <xref ref-type="bibr" rid="pcbi.1002092-Volterra1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Henneberger1">[31]</xref>. Research on synaptic tagging has revealed the astonishingly large time span during which the consolidation of early-LTP into long lasting synaptic change can be dependent on behavioral reinforcement <xref ref-type="bibr" rid="pcbi.1002092-Frey1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-AlmaguerMelian1">[33]</xref>. The present work provides a phenomenological model showing how the multi-stage processes observed in the induction of long-term synaptic plasticity can be bound into a functional whole.</p>
      <p>Previous modeling of population learning has already considered the modulation of plasticity by feedback from the decision circuitry <xref ref-type="bibr" rid="pcbi.1002092-Urbanczik1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Friedrich1">[34]</xref>. However, in these works the cascade was shortcut, with decision and reward feedback interacting directly in the modulation of plasticity. As a consequence the previous plasticity rule was capable of handling delays between decision and reward feedback only when these where very small, namely a fraction of typical stimulus duration. The present rule achieves a far more general solution to the temporal credit assignment problem by using a further stage in the synaptic cascade to decouple decision from reward feedback. Further, the rule is now based directly on optimizing the average reward rate (<xref ref-type="supplementary-material" rid="pcbi.1002092.s001">Text S1</xref>) and not just, as previously, a related objective function. This puts the present approach squarely into the field of policy gradient methods <xref ref-type="bibr" rid="pcbi.1002092-Williams1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1002092-Baxter2">[37]</xref>. Within this field, our main contribution is to show how the spatial credit assignment problem of distributing the learning between the population neurons can be solved in a biophysically plausible way. As the results in the section on learning stimulus-response association demonstrate, our plasticity rule leads to a learning performance which scales well to large population sizes (a more detailed scaling analysis has been given in <xref ref-type="bibr" rid="pcbi.1002092-Friedrich1">[34]</xref>). This is in contrast to the straightforward policy gradient approach of treating the neurons as independent agents which results in a rapid deterioration of learning performance with increasing population size <xref ref-type="bibr" rid="pcbi.1002092-Urbanczik1">[16]</xref>.</p>
      <p>Crucially in our population model neurons need to cooperate in order to receive reward and hence during learning a difficult spatial credit assignment problem arises. The appropriateness of any single neuron response cannot be determined without taking the responses of the other neurons into account and hence synapses in different neurons need to co-adapt in optimizing reward. This is in contrast to previous work <xref ref-type="bibr" rid="pcbi.1002092-Legenstein2">[38]</xref> modeling a biofeedback experiment in monkeys <xref ref-type="bibr" rid="pcbi.1002092-Fetz1">[39]</xref> where reward delivery was contingent on the firings of a single target neuron. In the model <xref ref-type="bibr" rid="pcbi.1002092-Legenstein2">[38]</xref> background activity was high, so that reinforcement could be increased by simply strengthening the synapses of the target neuron without any need for coordinated adaptation by the other neurons in the system.</p>
      <p>Some parameters in our plasticity scheme are related to properties of the learning task. For instance the time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e233" xlink:type="simple"/></inline-formula> in the last stage of the cascade represents a guess at the typical delay between decision and reinforcement. Our simulation results indicate that learning is not overly sensitive to the choice of the synaptic parameters (see e.g. <xref ref-type="fig" rid="pcbi-1002092-g003">Fig. 3D</xref>). Nevertheless, learning does of course deteriorate once the mismatch between synaptic and actual task parameters becomes too large. An intriguing possibility for further increasing robustness could be an inhomogeneous population of neurons. After all, a key point in population coding is to provide redundancy <xref ref-type="bibr" rid="pcbi.1002092-Pouget1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002092-Averbeck1">[41]</xref>. This is borne out by findings in <xref ref-type="bibr" rid="pcbi.1002092-Urbanczik1">[16]</xref> where, with increasing population size, decision performance improves but the correlation between single neuron performance and decision decreases. Hence it is of interest to study learning when different population neurons have different synaptic parameters. Then the neurons with parameters best matched to the task at hand, are expected to learn best. Thanks to their resulting correlated activity, they should be able to carry the population decision because the contributions from the badly learning mismatched neurons should be uncorrelated and thus tend to cancel. Unfortunately, meaningfully testing whether neuronal variability increases robustness in this manner, requires the simulation of population sizes which are an order of magnitude larger than what is currently within our computational reach.</p>
      <p>With regard to the temporal credit assignment problem, we think it is important to note that delayed interaction between decision making and reward delivery can arise in diverse manners:</p>
      <list list-type="roman-lower">
        <list-item>
          <p><italic>Delays in causation.</italic> Sometimes it just takes a while till the effect of decisions and actions becomes apparent - as when taking a pill against headache.</p>
        </list-item>
        <list-item>
          <p><italic>Incomplete information.</italic> The stimulus on which the decision is based does not encode all of the decision relevant information. Then previous stimuli and decisions can be of importance to the current decision because they induce a bias on the missing information. A case in point is the two armed bandit task, where previous decisions influence the odds that the intermittent target is baited. If, in contrast, the decision stimulus where to encode whether or not the intermittent target is baited, optimal decision making would be possible based just on the current stimulus.</p>
        </list-item>
        <list-item>
          <p><italic>Moving towards a rewarding state.</italic> Appropriate decisions or actions are needed to navigate through a set of intermediate non-rewarding states towards a rewarding goal - as when first going to the kitchen, then opening the fridge in order to finally get a beer. In contrast, for the sequential decision making task we considered above, reward is not just contingent on reaching the home state but also on the path taken.</p>
        </list-item>
      </list>
      <p>Policy gradient methods work in all of the above settings. Of course, missing information can be detrimental to the performance which is achievable at all. But, given this constraint, policy gradient methods will nevertheless optimize the performance. Temporal difference (TD) methods, however, by design handle only problems of type <italic>iii</italic>. In the first two cases TD-learning only applies when the state which serves as basis for the decision making represents the recent task history to the extent that the problem becomes Markovian. Formally, this maps the first two kinds of delays onto the third kind.</p>
      <p>Representing the recent task history is what working memory is good for - and working memory is well known to enter into decision making as in delayed match to sample tasks. On the other hand, transforming a non-Markovian into a Markovian decision problem can pose daunting demands on the working memory capacity needed to adequately represent the states in the TD-algorithm. With insufficient working memory the algorithm can fail in two distinct ways. The estimates for the value of some state-action pairs may be wrong (as demonstrated in the sequential decision making task), or, even when the estimates are correct, preferentially choosing the available action with highest estimated value may lead to a suboptimal policy (as in the two armed bandit).</p>
      <p>Policy gradient methods such as our population learning rule seem attractive as basic biological models of reinforcement learning because they work in a very general setting. Arguably, this generality is also a drawback. Precisely because the Markovian property is restrictive, exploiting it in the cases where it does apply, can substantially speed up learning. Hence, it is of interest that policy gradient methods can easily be combined with TD-state valuations in the framework of actor-critic methods. This amounts to simply replacing the direct reward signal in the policy gradient plasticity rule with a signal generated by the TD-valuation circuitry. The TD-signal can either be the estimated value of the current state <xref ref-type="bibr" rid="pcbi.1002092-Sutton2">[42]</xref> or the value prediction error <xref ref-type="bibr" rid="pcbi.1002092-Castro1">[15]</xref>. Combining policy gradient with TD-valuations in this way, again brings about the Markovian restriction. Hence, if reinforcement learning is to be both robust and fast, issues of metaplasticity arise: How does brain learn how to learn when?</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Population neurons</title>
        <p>The model neurons in our population are escape noise neurons <xref ref-type="bibr" rid="pcbi.1002092-Pfister1">[14]</xref>, i.e. leaky integrate and fire neurons where action potentials are generated with an instantaneous firing rate which depends on the membrane potential. Focusing on one of the population neurons, we denote by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e234" xlink:type="simple"/></inline-formula> its input which is a spike pattern made up of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e235" xlink:type="simple"/></inline-formula> spike trains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e236" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e237" xlink:type="simple"/></inline-formula>. Each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e238" xlink:type="simple"/></inline-formula> is a list of the input spike times in afferent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e239" xlink:type="simple"/></inline-formula>. We use the symbol <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e240" xlink:type="simple"/></inline-formula> to refer to the postsynaptic spike train produced by the neuron, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e241" xlink:type="simple"/></inline-formula> is also a list of spike times. If the neuron, with synaptic vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e242" xlink:type="simple"/></inline-formula>, produces the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e243" xlink:type="simple"/></inline-formula> in response to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e244" xlink:type="simple"/></inline-formula>, its membrane potential is determined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e245" xlink:type="simple"/><label>(5)</label></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e246" xlink:type="simple"/></inline-formula> is the unit step function and, further, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e247" xlink:type="simple"/></inline-formula> is Dirac's delta function, leading to immediate hyperpolarization after a postsynaptic spike. For the resting potential, denoted above by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e248" xlink:type="simple"/></inline-formula>, we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e249" xlink:type="simple"/></inline-formula> (arbitrary units). Further, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e250" xlink:type="simple"/></inline-formula> is used for the membrane time constant and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e251" xlink:type="simple"/></inline-formula> for the synaptic time constant.</p>
        <p>By integrating the differential equation, the membrane potential can be written in spike response form as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e252" xlink:type="simple"/><label>(6)</label></disp-formula>The postsynaptic kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e253" xlink:type="simple"/></inline-formula> and the reset kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e254" xlink:type="simple"/></inline-formula> vanish for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e255" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e256" xlink:type="simple"/></inline-formula> they are given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e257" xlink:type="simple"/></disp-formula>Note that the first eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e258" xlink:type="simple"/></inline-formula> of synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e259" xlink:type="simple"/></inline-formula> can be expressed in terms of the postsynaptic kernel as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e260" xlink:type="simple"/></inline-formula>.</p>
        <p>Action potential generation is controlled by an instantaneous firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e261" xlink:type="simple"/></inline-formula> which increases with the membrane potential. So, at each point <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e262" xlink:type="simple"/></inline-formula> in time, the neuron fires with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e263" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e264" xlink:type="simple"/></inline-formula> represents an infinitesimal time window (we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e265" xlink:type="simple"/></inline-formula> in the simulations). Our firing rate function is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e266" xlink:type="simple"/></disp-formula>with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e267" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e268" xlink:type="simple"/></inline-formula>. (In the limit of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e269" xlink:type="simple"/></inline-formula> one would recover a deterministic neuron with a spiking threshold <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e270" xlink:type="simple"/></inline-formula>.)</p>
        <p>As shown in <xref ref-type="bibr" rid="pcbi.1002092-Pfister1">[14]</xref>, the probability density, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e271" xlink:type="simple"/></inline-formula>, that the neuron actually produces the output spike train <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e272" xlink:type="simple"/></inline-formula> in response to the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e273" xlink:type="simple"/></inline-formula> during a decision period lasting from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e274" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e275" xlink:type="simple"/></inline-formula> satisfies:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e276" xlink:type="simple"/><label>(7)</label></disp-formula>The derivative of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e277" xlink:type="simple"/></inline-formula> with respect to the strength of synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e278" xlink:type="simple"/></inline-formula> is known as characteristic eligibility in reinforcement learning <xref ref-type="bibr" rid="pcbi.1002092-Williams1">[35]</xref>. For our choice of the firing rate function one obtains<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e279" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e280" xlink:type="simple"/></inline-formula> is the first eligibility trace of the synapse (Eq. 1) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e281" xlink:type="simple"/></inline-formula> the postsynaptic signal of the neuron given right below Eq. (2). Note that (8) is similar to our second eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e282" xlink:type="simple"/></inline-formula>, see Eq. (2), except that we have replaced the integration over the decision period by low pass filtering with a time constant matched to the stimulus duration. The reason for this is that it seems un-biological to assume that the synapses of the population neurons know when decision periods start and end.</p>
      </sec>
      <sec id="s4b">
        <title>Architecture and decision making</title>
        <p>We use the superscript <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e283" xlink:type="simple"/></inline-formula>, running from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e284" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e285" xlink:type="simple"/></inline-formula>, to index the population neurons. For instance, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e286" xlink:type="simple"/></inline-formula> is the postsynaptic spike train produced by neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e287" xlink:type="simple"/></inline-formula> in response to its input spike pattern <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e288" xlink:type="simple"/></inline-formula>. As suggested by the notation, the population neurons have different inputs, but their inputs are highly correlated because the neurons are randomly connected to a common input layer which present the stimulus to the network. In particular, we assume that each population neuron synapses onto a site in the input layer with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e289" xlink:type="simple"/></inline-formula>, leading to many shared input spike trains between the neurons.</p>
        <p>The population response is read out by the decision making circuitry based on a spike/no-spike code. For notational convenience we introduce the coding function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e290" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e291" xlink:type="simple"/></inline-formula>, if the there is no spike in the postsynaptic response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e292" xlink:type="simple"/></inline-formula>, otherwise, if neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e293" xlink:type="simple"/></inline-formula> produce at least one spike in response to the stimulus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e294" xlink:type="simple"/></inline-formula>. In term of this coding function the population activity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e295" xlink:type="simple"/></inline-formula> being read out by the decision making circuitry can be written as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e296" xlink:type="simple"/></disp-formula>Using this activity reading, the behavioral decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e297" xlink:type="simple"/></inline-formula> is made probabilistically, the likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e298" xlink:type="simple"/></inline-formula> of producing the decision is given by the logistic function<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e299" xlink:type="simple"/><label>(9)</label></disp-formula>Note that due to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e300" xlink:type="simple"/></inline-formula> normalization in the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e301" xlink:type="simple"/></inline-formula>, the magnitude of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e302" xlink:type="simple"/></inline-formula> can be as large as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e303" xlink:type="simple"/></inline-formula>. This is why, decisions based on the activity of a large population can be close to deterministic, despite of the noisy decision making circuitry.</p>
      </sec>
      <sec id="s4c">
        <title>Feedback signals and the postsynaptic trace</title>
        <p>We start with the reward feedback <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e304" xlink:type="simple"/></inline-formula>, modulating synaptic plasticity in Eq. (4). This feedback is encoded by means of a concentration variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e305" xlink:type="simple"/></inline-formula>, representing ambient levels of a neurotransmitter, e.g. dopamine. In the absence of reward information, the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e306" xlink:type="simple"/></inline-formula> approaches a homeostatic level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e307" xlink:type="simple"/></inline-formula> with a time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e308" xlink:type="simple"/></inline-formula>. For any point in time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e309" xlink:type="simple"/></inline-formula> when external reward information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e310" xlink:type="simple"/></inline-formula> is available, this reinforcement leads to a change in the production rate of the neurotransmitter. The change is proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e311" xlink:type="simple"/></inline-formula> and lasts for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e312" xlink:type="simple"/></inline-formula>. So up to the point in time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e313" xlink:type="simple"/></inline-formula> when further reinforcement becomes available, the concentration variable evolves as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e314" xlink:type="simple"/></disp-formula>Here the step function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e315" xlink:type="simple"/></inline-formula> equals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e316" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e317" xlink:type="simple"/></inline-formula>, otherwise the function value is zero. The reward feedback read-out at a synapse is determined by the deviation of the current neurotransmitter level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e318" xlink:type="simple"/></inline-formula> from its homeostatic value and equals<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e319" xlink:type="simple"/></disp-formula>Here the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e320" xlink:type="simple"/></inline-formula> is the positive learning rate which, for notational convenience, we absorb into the reward signal.</p>
        <p>The decision feedback <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e321" xlink:type="simple"/></inline-formula> used in Eq. (3) is encoded in the concentration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e322" xlink:type="simple"/></inline-formula> of a second neurotransmitter. As for reward feedback, this is achieved by a temporary change in the production rate of the encoding neurotransmitter. For describing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e323" xlink:type="simple"/></inline-formula>, we assume a stimulus that ended at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e324" xlink:type="simple"/></inline-formula>, evoking the population activity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e325" xlink:type="simple"/></inline-formula> and behavioral decision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e326" xlink:type="simple"/></inline-formula>. As shown in <xref ref-type="supplementary-material" rid="pcbi.1002092.s001">Text S1</xref>, the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e327" xlink:type="simple"/></inline-formula> should then be determined by the derivative of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e328" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e329" xlink:type="simple"/></inline-formula> and, in view of Eq. (9), this derivative is simply <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e330" xlink:type="simple"/></inline-formula>. Hence we use<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e331" xlink:type="simple"/></disp-formula>for the temporal evolution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e332" xlink:type="simple"/></inline-formula>. Parameter values in the simulations are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e333" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e334" xlink:type="simple"/></inline-formula>. The above equation holds up to time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e335" xlink:type="simple"/></inline-formula> when the subsequent stimulus presentation ends, at which point the decision variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e336" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e337" xlink:type="simple"/></inline-formula> are replaced by their values for the latter stimulus. The decision feedback <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e338" xlink:type="simple"/></inline-formula> is simply<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e339" xlink:type="simple"/></disp-formula></p>
        <p>For the postsynaptic trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e340" xlink:type="simple"/></inline-formula> in Eq. (3), we assume a concentration variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e341" xlink:type="simple"/></inline-formula> which reflects the spiking of the neuron. Each time there is a postsynaptic spike, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e342" xlink:type="simple"/></inline-formula> is set to 1; at other times, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e343" xlink:type="simple"/></inline-formula> decays as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e344" xlink:type="simple"/></inline-formula>. The value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e345" xlink:type="simple"/></inline-formula> should reflect whether or not the neuron spiked in response to the decision stimulus. So, as for the eligibility trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e346" xlink:type="simple"/></inline-formula> (see Eq. 2), the relevant time scale is the decision period and this is why the same time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e347" xlink:type="simple"/></inline-formula> is used in both cases. The trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e348" xlink:type="simple"/></inline-formula> is obtained as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e349" xlink:type="simple"/></disp-formula>comparing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e350" xlink:type="simple"/></inline-formula> to an appropriate threshold <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e351" xlink:type="simple"/></inline-formula>. In the simulation we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e352" xlink:type="simple"/></inline-formula>. For the reasoning behind this choice, consider a stimulus ending at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e353" xlink:type="simple"/></inline-formula> of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e354" xlink:type="simple"/></inline-formula>. The value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e355" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e356" xlink:type="simple"/></inline-formula> will accurately reflect whether or not the decision stimulus elicited a postsynaptic spike, if we choose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e357" xlink:type="simple"/></inline-formula>. But since decision feedback is not instantaneous, the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e358" xlink:type="simple"/></inline-formula> is mainly read-out at times later than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e359" xlink:type="simple"/></inline-formula>. This is why the smaller value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e360" xlink:type="simple"/></inline-formula> seemed a somewhat better choice.</p>
      </sec>
      <sec id="s4d">
        <title>TD-learning</title>
        <p>For TD-learning we used the SARSA control algorithm <xref ref-type="bibr" rid="pcbi.1002092-Sutton1">[1]</xref> which estimates the values of state-action pairs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e361" xlink:type="simple"/></inline-formula>. At each point in time, the value estimates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e362" xlink:type="simple"/></inline-formula> are updated according to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e363" xlink:type="simple"/></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e364" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e365" xlink:type="simple"/></inline-formula> have values between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e366" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e367" xlink:type="simple"/></inline-formula>. The parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e368" xlink:type="simple"/></inline-formula> is similar to a learning rate and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e369" xlink:type="simple"/></inline-formula> controls the temporal discounting. The above update is done after every transition from a nonterminal state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e370" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e371" xlink:type="simple"/></inline-formula> is terminal, then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e372" xlink:type="simple"/></inline-formula> is defined as zero. When in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e373" xlink:type="simple"/></inline-formula>, the next action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e374" xlink:type="simple"/></inline-formula> is chosen using either <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e375" xlink:type="simple"/></inline-formula>-greedy or softmax. In both cases only the values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e376" xlink:type="simple"/></inline-formula> pertinent to the current state enter into the decision making.</p>
        <p>For memoryless TD-learning in the two armed bandit we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e377" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e378" xlink:type="simple"/></inline-formula>. A positive discount factor would not qualitatively change the result. For each of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e379" xlink:type="simple"/></inline-formula> runs per chosen value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e380" xlink:type="simple"/></inline-formula>, we simulated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e381" xlink:type="simple"/></inline-formula> trials. After <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e382" xlink:type="simple"/></inline-formula> trials learning had converged and the reported asymptotic quantities are the average over the next <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e383" xlink:type="simple"/></inline-formula> trials. For learning with memory we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e384" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e385" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e386" xlink:type="simple"/></inline-formula>.</p>
        <p>For the sequential decision making task decision selection used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e387" xlink:type="simple"/></inline-formula>-greedy with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e388" xlink:type="simple"/></inline-formula>. The discount factor was set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e389" xlink:type="simple"/></inline-formula> and the step-size parameter to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e390" xlink:type="simple"/></inline-formula>.</p>
        <p>With regard to the failure of TD-learning in the sequential decision making task, we note that there are also eligibility trace based versions, SARSA<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e391" xlink:type="simple"/></inline-formula>, of the algorithm with the above version corresponding to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e392" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e393" xlink:type="simple"/></inline-formula>, the value update takes into account not just the next state-action pair but the value of all subsequent state-action pairs. Importantly, for the special case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e394" xlink:type="simple"/></inline-formula> the subsequent values occurring in the update cancel, and the value update is in effect driven directly by the reward signal <xref ref-type="bibr" rid="pcbi.1002092-Sutton1">[1]</xref>. So SARSA<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e395" xlink:type="simple"/></inline-formula> is just a complicated way of doing basic Monte Carlo estimation of the values. It hence does not assume that the process is Markovian and SARSA<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e396" xlink:type="simple"/></inline-formula> does reliably converge towards optimal performance in our task. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e397" xlink:type="simple"/></inline-formula> the procedure interpolates between the two extremes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e398" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e399" xlink:type="simple"/></inline-formula>. Consequently the valuation of some state-action pairs (e.g. the shortcut <sub>1</sub>2, <italic>left</italic>) will then be wrong but the error will be smaller than for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e400" xlink:type="simple"/></inline-formula>. If action selection is based on softmax the incorrect valuation will nevertheless be detrimental to decision making. However, this need not always be the case for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e401" xlink:type="simple"/></inline-formula>-greedy, due to the thresholding inherent in this decision procedure. In particular, there is a positive critical value for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e402" xlink:type="simple"/></inline-formula> (which depends mainly on the discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e403" xlink:type="simple"/></inline-formula>) above which the valuation error will no longer affect the decision making. In this parameter regime, SARSA<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e404" xlink:type="simple"/></inline-formula> will reliably learn the optimal policy (upto the exploration determined by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e405" xlink:type="simple"/></inline-formula>).</p>
      </sec>
      <sec id="s4e">
        <title>Miscellaneous simulation details</title>
        <p>In all the simulations initial values for the synaptic strength were picked from a Gaussian distribution with mean zero and standard deviation equal to 4, independently for each afferent and each neuron.</p>
        <p>A learning rate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e406" xlink:type="simple"/></inline-formula> was used in all simulations, except for the 2-armed bandit task where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e407" xlink:type="simple"/></inline-formula> was used.</p>
        <p>In the sequential decision making task with working memory, the population is presented stimuli encoding not just the current but also the immediately preceeding position. For this, each location on the track is assigned to a fixed spike pattern made up of 50 spike trains representing the location in the case that it is the current position and, further, to a second spike pattern with 30 spike trains for the case that it is the immediately preceeding position. The stimulus for the network is then obtained by concatenating the 50 spike trains corresponding to the current position with the 30 spike trains for the preceeding position.</p>
        <p>The curves showing the evolution of performance were obtained by calculating an exponentially weighted moving average in each run and then averaging over multiple runs. For the sequential decision making task reward per episode was considered and the smoothing factor in the exponentially weighted moving average was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e408" xlink:type="simple"/></inline-formula>. In the other task, where performance per trial was considered, the smoothing factor was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e409" xlink:type="simple"/></inline-formula>. For each run a new set of initial synaptic strength and a new set of stimuli was generated. The number of runs was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002092.e410" xlink:type="simple"/></inline-formula>, except in the two armed bandit where we averaged over 40 runs.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002092.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002092.s001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>We show how the plasticity rule presented in the main text is based on a gradient ascent procedure maximizing the average reward rate.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Michael Herzog and Thomas Nevian for helpful discussions on the learning task paradigms and on possible molecular implementations of the synaptic plasticity rule.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002092-Sutton1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name><name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name></person-group>             <year>1998</year>             <source>Reinforcement Learning: An Introduction</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Markram1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Lübke</surname><given-names>J</given-names></name><name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name><name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name></person-group>             <year>1997</year>             <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>213</fpage>             <lpage>215</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Bi1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bi</surname><given-names>G</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M</given-names></name></person-group>             <year>1998</year>             <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type.</article-title>             <source>J Neurosci</source>             <volume>18</volume>             <fpage>10464</fpage>             <lpage>10472</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Song1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2001</year>             <article-title>Cortical development and remapping through spike timing-dependent plasticity.</article-title>             <source>Neuron</source>             <volume>32</volume>             <fpage>339</fpage>             <lpage>350</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Baras1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baras</surname><given-names>D</given-names></name><name name-style="western"><surname>Meir</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Reinforcement learning, spike-time-dependent plasticity, and the BCM rule.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2245</fpage>             <lpage>2279</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Florian1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Florian</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>1468</fpage>             <lpage>1502</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Izhikevich1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Izhikevich</surname><given-names>E</given-names></name></person-group>             <year>2007</year>             <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>2443</fpage>             <lpage>2452</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Legenstein1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name><name name-style="western"><surname>Naeger</surname><given-names>C</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>             <year>2005</year>             <article-title>What can a neuron learn with spike-timing-dependent plasticity?</article-title>             <source>Neural Comput</source>             <volume>17</volume>             <fpage>2337</fpage>             <lpage>2382</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Pawlak1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pawlak</surname><given-names>V</given-names></name><name name-style="western"><surname>Kerr</surname><given-names>JND</given-names></name></person-group>             <year>2008</year>             <article-title>Dopamine receptor activation is required for corticostriatal spiketiming-dependent plasticity.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>2435</fpage>             <lpage>2446</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Zhang1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name><name name-style="western"><surname>Lau</surname><given-names>P</given-names></name><name name-style="western"><surname>Bi</surname><given-names>G</given-names></name></person-group>             <year>2009</year>             <article-title>Gain in sensitivity and loss in temporal contrast of STDP by dopaminergic modulation of hippocampal synapses.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>106</volume>             <fpage>13028</fpage>             <lpage>13033</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Gavornik1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gavornik</surname><given-names>JP</given-names></name><name name-style="western"><surname>Shuler</surname><given-names>MGH</given-names></name><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name><name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name><name name-style="western"><surname>Shouval</surname><given-names>HZ</given-names></name></person-group>             <year>2009</year>             <article-title>Learning reward timing in cortex through reward dependent expression of synaptic plasticity.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>106</volume>             <fpage>6826</fpage>             <lpage>6831</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Schultz1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name></person-group>             <year>1997</year>             <article-title>A neural substrate of prediction and reward.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>1593</fpage>             <lpage>1599</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Dayan1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name></person-group>             <year>2008</year>             <article-title>Reinforcement learning: the good, the bad and the ugly.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>18</volume>             <fpage>185</fpage>             <lpage>196</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Pfister1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pfister</surname><given-names>J</given-names></name><name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name><name name-style="western"><surname>Barber</surname><given-names>D</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name></person-group>             <year>2006</year>             <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning.</article-title>             <source>Neural Comput</source>             <volume>18</volume>             <fpage>1318</fpage>             <lpage>1348</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Castro1">
        <label>15</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Castro</surname><given-names>D</given-names></name><name name-style="western"><surname>Volkinshtein</surname><given-names>S</given-names></name><name name-style="western"><surname>Meir</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Temporal difference based actor critic learning – convergence and neural implementation.</article-title>             <source>Advances in neural information processing systems 21</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>385</fpage>             <lpage>392</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Urbanczik1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name><name name-style="western"><surname>Senn</surname><given-names>W</given-names></name></person-group>             <year>2009</year>             <article-title>Reinforcement learning in populations of spiking neurons.</article-title>             <source>Nat Neurosci</source>             <volume>12</volume>             <fpage>250</fpage>             <lpage>252</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Vasilaki1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name><name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name><name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name><name name-style="western"><surname>Senn</surname><given-names>W</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name></person-group>             <year>2009</year>             <article-title>Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000586</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Clopath1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name><name name-style="western"><surname>Büsing</surname><given-names>L</given-names></name><name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name></person-group>             <year>2010</year>             <article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis.</article-title>             <source>Nat Neurosci</source>             <volume>13</volume>             <fpage>344</fpage>             <lpage>352</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Wang1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name></person-group>             <year>2002</year>             <article-title>Probabilistic decision making by slow reverberation in cortical circuits.</article-title>             <source>Neuron</source>             <volume>36</volume>             <fpage>955</fpage>             <lpage>968</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Foehring1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Foehring</surname><given-names>R</given-names></name><name name-style="western"><surname>Lorenzon</surname><given-names>N</given-names></name></person-group>             <year>1999</year>             <article-title>Neuromodulation, development and synaptic plasticity.</article-title>             <source>Can J Exp Psychol</source>             <volume>53</volume>             <fpage>45</fpage>             <lpage>61</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Matsuda1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Matsuda</surname><given-names>Y</given-names></name><name name-style="western"><surname>Marzo</surname><given-names>A</given-names></name><name name-style="western"><surname>Otani</surname><given-names>S</given-names></name></person-group>             <year>2006</year>             <article-title>The presence of background dopamine signal converts longterm synaptic depression to potentiation in rat prefrontal cortex.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>4803</fpage>             <lpage>4810</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Seol1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Seol</surname><given-names>G</given-names></name><name name-style="western"><surname>Ziburkus</surname><given-names>J</given-names></name><name name-style="western"><surname>Huang</surname><given-names>S</given-names></name><name name-style="western"><surname>Song</surname><given-names>L</given-names></name><name name-style="western"><surname>Kim</surname><given-names>I</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Neuromodulators control the polarity of spike-timing-dependent synaptic plasticity.</article-title>             <source>Neuron</source>             <volume>55</volume>             <fpage>919</fpage>             <lpage>929</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Daw1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name></person-group>             <year>2006</year>             <article-title>The computational neurobiology of learning and reward.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>16</volume>             <fpage>199</fpage>             <lpage>204</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Mazur1">
        <label>24</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mazur</surname><given-names>J</given-names></name></person-group>             <year>2002</year>             <source>Learning and Behavior</source>             <publisher-loc>Upper Saddle River, NJ</publisher-loc>             <publisher-name>Prentice Hall</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Baum1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baum</surname><given-names>W</given-names></name><name name-style="western"><surname>Aparicio</surname><given-names>C</given-names></name></person-group>             <year>1999</year>             <article-title>Optimality And Concurrent Variable-interval Variable-ratio Schedules.</article-title>             <source>J Exp Anal Behav</source>             <volume>71</volume>             <fpage>75</fpage>             <lpage>89</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Song2">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Miller</surname><given-names>K</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name></person-group>             <year>2000</year>             <article-title>Competitive Hebbian learning through spike-timing dependent synaptic plasticity.</article-title>             <source>Nat Neurosci</source>             <volume>3</volume>             <fpage>919</fpage>             <lpage>926</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Lynch1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lynch</surname><given-names>MA</given-names></name></person-group>             <year>2004</year>             <article-title>Long-term potentiation and memory.</article-title>             <source>Physiol Rev</source>             <volume>84</volume>             <fpage>87</fpage>             <lpage>136</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Nevian1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nevian</surname><given-names>T</given-names></name><name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name></person-group>             <year>2006</year>             <article-title>Spine Ca<sup>2+</sup> signaling in spike-timing-dependent plasticity.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>11001</fpage>             <lpage>11013</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Abraham1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abraham</surname><given-names>W</given-names></name></person-group>             <year>2008</year>             <article-title>Metaplasticity: tuning synapses and networks for plasticity.</article-title>             <source>Nat Neurosci</source>             <volume>9</volume>             <fpage>387</fpage>             <lpage>399</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Volterra1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Volterra</surname><given-names>A</given-names></name><name name-style="western"><surname>Meldolesi</surname><given-names>J</given-names></name></person-group>             <year>2005</year>             <article-title>Astrocytes, from brain glue to communication elements: the revolution continues.</article-title>             <source>Nat Rev Neurosci</source>             <volume>6</volume>             <fpage>626</fpage>             <lpage>640</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Henneberger1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Henneberger</surname><given-names>C</given-names></name><name name-style="western"><surname>Papouin</surname><given-names>T</given-names></name><name name-style="western"><surname>Oliet</surname><given-names>SH</given-names></name><name name-style="western"><surname>Rusakov</surname><given-names>DA</given-names></name></person-group>             <year>2010</year>             <article-title>Long-term potentiation depends on release of D-serine from astrocytes.</article-title>             <source>Nature</source>             <volume>463</volume>             <fpage>232</fpage>             <lpage>236</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Frey1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Frey</surname><given-names>S</given-names></name><name name-style="western"><surname>Frey</surname><given-names>JU</given-names></name></person-group>             <year>2008</year>             <article-title>‘Synaptic tagging’ and ‘cross-tagging’ and related associative reinforcement processes of functional plasticity as the cellular basis for memory formation.</article-title>             <source>Prog Brain Res</source>             <volume>169</volume>             <fpage>117</fpage>             <lpage>143</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-AlmaguerMelian1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Almaguer-Melian</surname><given-names>W</given-names></name><name name-style="western"><surname>Bergado</surname><given-names>JA</given-names></name><name name-style="western"><surname>Lopez-Rojas</surname><given-names>J</given-names></name><name name-style="western"><surname>Frey</surname><given-names>S</given-names></name><name name-style="western"><surname>Frey</surname><given-names>JU</given-names></name></person-group>             <year>2010</year>             <article-title>Differential effects of electrical stimulation patterns, motivational-behavioral stimuli and their order of application on functional plasticity processes within one input in the dentate gyrus of freely moving rats in vivo.</article-title>             <source>Neuroscience</source>             <volume>165</volume>             <fpage>1546</fpage>             <lpage>1558</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Friedrich1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friedrich</surname><given-names>J</given-names></name><name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name><name name-style="western"><surname>Senn</surname><given-names>W</given-names></name></person-group>             <year>2010</year>             <article-title>Learning spike-based population codes by reward and population feedback.</article-title>             <source>Neural Comput</source>             <volume>22</volume>             <fpage>1698</fpage>             <lpage>1717</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Williams1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>R</given-names></name></person-group>             <year>1992</year>             <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning.</article-title>             <source>Mach Learn</source>             <volume>8</volume>             <fpage>229</fpage>             <lpage>256</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Baxter1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baxter</surname><given-names>J</given-names></name><name name-style="western"><surname>Bartlett</surname><given-names>P</given-names></name></person-group>             <year>2001</year>             <article-title>Infinite-horizon policy-gradient estimation.</article-title>             <source>J Artif Intell Res</source>             <volume>15</volume>             <fpage>319</fpage>             <lpage>350</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Baxter2">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baxter</surname><given-names>J</given-names></name><name name-style="western"><surname>Bartlett</surname><given-names>P</given-names></name><name name-style="western"><surname>Weaver</surname><given-names>L</given-names></name></person-group>             <year>2001</year>             <article-title>Experiments with infinite-horizon, policy-gradient estimation.</article-title>             <source>J Artif Intell Res</source>             <volume>15</volume>             <fpage>351</fpage>             <lpage>381</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Legenstein2">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name><name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>             <year>2008</year>             <article-title>A learning theory for reward-modulated spike-timingdependent plasticity with application to biofeedback.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000180</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Fetz1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fetz</surname><given-names>EE</given-names></name><name name-style="western"><surname>Baker</surname><given-names>MA</given-names></name></person-group>             <year>1973</year>             <article-title>Operantly conditioned patterns on precentral unit activity and correlated responses in adjacent cells and contralateral muscles.</article-title>             <source>J Neurophysiol</source>             <volume>36</volume>             <fpage>179</fpage>             <lpage>204</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Pouget1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Zemel</surname><given-names>R</given-names></name></person-group>             <year>2000</year>             <article-title>Information processing with population codes.</article-title>             <source>Nat Rev Neurosci</source>             <volume>1</volume>             <fpage>125</fpage>             <lpage>132</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Averbeck1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Averbeck</surname><given-names>B</given-names></name><name name-style="western"><surname>Latham</surname><given-names>P</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2006</year>             <article-title>Neural correlations, population coding and computation.</article-title>             <source>Nat Rev Neurosci</source>             <volume>7</volume>             <fpage>358</fpage>             <lpage>366</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002092-Sutton2">
        <label>42</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sutton</surname><given-names>R</given-names></name><name name-style="western"><surname>McAllester</surname><given-names>D</given-names></name><name name-style="western"><surname>Singh</surname><given-names>S</given-names></name><name name-style="western"><surname>Mansour</surname><given-names>Y</given-names></name></person-group>             <year>2002</year>             <article-title>Policy gradient methods for reinforcement learning with function approximation.</article-title>             <source>Advances in neural information processing systems 12</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>1057</fpage>             <lpage>1063</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>