<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006370</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01892</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Chemical compounds</subject><subj-group><subject>Organic compounds</subject><subj-group><subject>Amines</subject><subj-group><subject>Catecholamines</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Organic chemistry</subject><subj-group><subject>Organic compounds</subject><subj-group><subject>Amines</subject><subj-group><subject>Catecholamines</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurotransmitters</subject><subj-group><subject>Biogenic amines</subject><subj-group><subject>Catecholamines</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurotransmitters</subject><subj-group><subject>Biogenic amines</subject><subj-group><subject>Catecholamines</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Hormones</subject><subj-group><subject>Catecholamines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Behavioral conditioning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Brainstem</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Brainstem</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Dorsal anterior cingulate-brainstem ensemble as a reinforcement meta-learner</article-title>
<alt-title alt-title-type="running-head">dACC-brainstem as a meta-learner</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2925-0615</contrib-id>
<name name-style="western">
<surname>Silvetti</surname>
<given-names>Massimo</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5761-6813</contrib-id>
<name name-style="western">
<surname>Vassena</surname>
<given-names>Eliana</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Abrahamse</surname>
<given-names>Elger</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7783-4754</contrib-id>
<name name-style="western">
<surname>Verguts</surname>
<given-names>Tom</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Experimental Psychology, Ghent University, Ghent, Belgium</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>‎Donders Institute for Brain, Cognition and Behaviour, Radboud University, Nijmegen, The Netherlands</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Basque Center on Cognition, Brain and Language, San Sebastián, Spain</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>IKERBASQUE, Basque Foundation for Science, Bilbao, Spain</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>O'Reilly</surname>
<given-names>Jill</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Oxford University, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">massimo.silvetti@istc.cnr.it</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>8</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>8</issue>
<elocation-id>e1006370</elocation-id>
<history>
<date date-type="received">
<day>13</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>17</day>
<month>7</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Silvetti et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006370"/>
<abstract>
<p>Optimal decision-making is based on integrating information from several dimensions of decisional space (e.g., reward expectation, cost estimation, effort exertion). Despite considerable empirical and theoretical efforts, the computational and neural bases of such multidimensional integration have remained largely elusive. Here we propose that the current theoretical stalemate may be broken by considering the computational properties of a cortical-subcortical circuit involving the dorsal anterior cingulate cortex (dACC) and the brainstem neuromodulatory nuclei: ventral tegmental area (VTA) and locus coeruleus (LC). From this perspective, the dACC optimizes decisions about stimuli and actions, and using the same computational machinery, it also modulates cortical functions (meta-learning), via neuromodulatory control (VTA and LC). We implemented this theory in a novel neuro-computational model–the Reinforcement Meta Learner (RML). We outline how the RML captures critical empirical findings from an unprecedented range of theoretical domains, and parsimoniously integrates various previous proposals on dACC functioning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>A major challenge for all organisms is selecting optimal behaviour to obtain resources while minimizing energetic and other expenses. Evolution provided mammals with exceptional decision-making capabilities to face this challenge. Even though neuroscientists have identified a heterogeneous and distributed set of brain structures to be involved, a comprehensive theory about the biological and computational basis of such decision-making is yet to be formulated. We propose that the interaction between the medial prefrontal cortex (a part of the frontal lobes) and the subcortical nuclei releasing catecholaminergic neuromodulators will be key to such a theory. We argue that this interaction allows both the selection of optimal behaviour and, more importantly, the optimal modulation of the very brain circuits that drive such behavioral selection (i.e., meta-learning). We implemented this theory in a novel neuro-computational model, the Reinforcement Meta-Learner (RML). By means of computer simulations we showed that the RML provides a biological and computational account for a set of neuroscientific data with unprecedented scope, thereby suggesting a critical mechanism of decision-making in the mammalian brain.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010665</institution-id>
<institution>H2020 Marie Skłodowska-Curie Actions</institution>
</institution-wrap>
</funding-source>
<award-id>795919</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2925-0615</contrib-id>
<name name-style="western">
<surname>Silvetti</surname>
<given-names>Massimo</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010665</institution-id>
<institution>H2020 Marie Skłodowska-Curie Actions</institution>
</institution-wrap>
</funding-source>
<award-id>705630</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5761-6813</contrib-id>
<name name-style="western">
<surname>Vassena</surname>
<given-names>Eliana</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003130</institution-id>
<institution>Fonds Wetenschappelijk Onderzoek</institution>
</institution-wrap>
</funding-source>
<award-id>12C4715N</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Abrahamse</surname>
<given-names>Elger</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>MS was funded from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie Grant Agreement No. 795919. EV was funded from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie Grant Agreement No. 705630. EA was supported by Research Foundation Flanders under contract number 12C4715N. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="1"/>
<page-count count="32"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-09-06</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files. The software implementing the discrete version of the model is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AL458/RML.git" xlink:type="simple">https://github.com/AL458/RML.git</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Making the right decisions in uncertain and changing environments is at the heart of intelligent behavior [<xref ref-type="bibr" rid="pcbi.1006370.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref002">2</xref>]. To this purpose, the mammalian brain needs to integrate information from various dimensions of decisional space (e.g., reward expectation, costs estimation and effort exertion). Involvement of the medial prefrontal cortex (MFC), and in particular the dorsal anterior cingulate cortex (dACC), seems to be ubiquitous in experimental studies aimed at investigating this multidimensional integration [<xref ref-type="bibr" rid="pcbi.1006370.ref001">1</xref>]. Computational models suggested that many signals recorded in the dACC (e.g., error detection, error likelihood estimation and uncertainty) can be accounted for in terms of Reinforcement Learning (RL) operations [<xref ref-type="bibr" rid="pcbi.1006370.ref003">3</xref>]. Yet, dACC is also linked to adaptive control of (neuro)cognitive functions, like controlling physical or cognitive effort exertion, or regulating the right amount of neural plasticity as a function of environmental changes [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref007">7</xref>]. How these insights combine has so far remained elusive [<xref ref-type="bibr" rid="pcbi.1006370.ref008">8</xref>], resulting in an ongoing debate on the dACC functions and the neurobiogical basis of decision-making [<xref ref-type="bibr" rid="pcbi.1006370.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref009">9</xref>], (for a review see [<xref ref-type="bibr" rid="pcbi.1006370.ref010">10</xref>]).</p>
<p>Here we present a theoretical proposal on how the mammalian brain can optimize behaviour by simultaneously taking into account several dimensions involved in decision-making, and which role the dACC plays in this process. We start from the assumption, inherited from the RL domain, that decision-making is an optimization problem aimed at maximizing reward on the long term [<xref ref-type="bibr" rid="pcbi.1006370.ref011">11</xref>]. To pursue this optimization process, the mammalian brain needs to engage in <italic>meta-learning</italic>: it needs not only to optimally control ongoing behaviour (e.g. deciding whether or not to start chasing a prey), but also to learn how to control its own internal states that, in their turn, influence behavioural selection (e.g., deciding how much effort to invest in a chase). We propose that such meta-learning is carried out by a specific cortical-subcortical macrocircuit including the dACC, the brainstem catecholamine nuclei, ventral tegmental area (VTA) and locus coeruleus (LC), and their demonstrated bidirectional connections [<xref ref-type="bibr" rid="pcbi.1006370.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref018">18</xref>]. In this macrocircuit, RL principles are exploited to select appropriate behavioural responses and to modulate its own internal states via dopamine (DA, synthetized by VTA) and norepinephrine (NE, synthetized by LC) neuromodulation. We implemented this theoretical proposal in a novel computational model coined the Reinforcement Meta Learner (RML), modeling the dACC, the VTA, and locus coeruleus LC. Like in earlier RL models, the dACC in the RML computes the values of specific stimuli and actions to achieve adaptive behavior. However–and unlike in earlier models–dACC internal dynamics is modulated by catecholamines via recurrent interaction between the dACC itself and the brainstem nuclei.</p>
<p>It is worth stressing that a single fixed parameter set is used in the RML to simulate empirical findings from an unprecedented range of theoretical domains. This demonstrates that the RML provides a viable model for dACC functioning and a first potential step toward theoretical unification across these domains, inspiring new perspectives on the biological and computational foundations of decision-making in the mammalian brain.</p>
<sec id="sec002">
<title>Paper structure</title>
<p>In the next two subsections of the Introduction we describe qualitatively the computational principles of the RML (The RML: General description) and the main novelties introduced by the model (The RML: Innovations). In the subsequent Results section, we describe the experimental paradigms we used to test the RML and the results, together with domain-specific discussion paragraphs. Next, in the domain-general Discussion section we broadly frame and connect the results, comparing our model with other models from recent literature (Relationships to Other Models). We also propose future experimental paradigm to test RML predictions (Experimental Predictions), including possible applications to translational research, and we describe some limitations of our work (Limitations). Finally, in the Methods section we provide the full mathematical description of the RML.</p>
</sec>
<sec id="sec003">
<title>The RML: General description</title>
<p>At the basis of our model is the idea that a macrocircuit involving dACC-VTA-LC represents a core computational unit for optimizing both behaviour and internal states that modulate behaviour itself (meta-learning). <xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref> represents an overview of the RML architecture. The RML dynamics is based on two inter-related loops connecting four computational modules: dACC<sub>Boost</sub>, dACC<sub>Act</sub>, VTA, and LC. An <italic>external loop</italic> represents the interaction between the dACC modules and the environment, while an <italic>internal loop</italic> covers the interaction between the dACC modules and the brainstem nuclei (VTA and LC; orange and red bidirectional arrows in <xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref>). This double loop structure is aimed at optimizing performance (i.e., maximizing reward) while minimizing two different types of costs: the costs of motor actions (external loop; e.g. the metabolic cost of climbing a stair), and the boosting costs of neuromodulators release (internal loop; e.g. the cost of neurotransmitters depletion). Connectivity and functional studies corroborate the hypothesis underlying this architecture, because they show that there is an anatomical overlap between the midfrontal sub-region related to the meta-learning processes discussed above, and the midfrontal sub-region maximally connected with both LC and VTA nuclei [<xref ref-type="bibr" rid="pcbi.1006370.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref021">21</xref>], both located within the dACC area.</p>
<fig id="pcbi.1006370.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g001</object-id>
<label>Fig 1</label>
<caption>
<title>RML overview with neuroanatomical mapping.</title>
<p>The RML consists of a state-action selection dual system (dACC<sub>Act</sub> and dACC<sub>Boost</sub>), based on RL algorithms, and a parameter modulation system via catecholamine release (VTA and LC) that are in constant interaction. Finally, the RML can be connected to an external neural model (e.g. a fronto-parietal network) and part of the LC output (NE) can be used to modulate its activity while the entire system (RML + external model) is interacting with the environment.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g001" xlink:type="simple"/>
</fig>
<p>In the RML, the dACC plays the role of a performance monitoring system, which compares expectations about environmental states and executed actions with environmental outcomes (cf. [<xref ref-type="bibr" rid="pcbi.1006370.ref022">22</xref>]). Discrepancies between expectations and outcomes generate prediction error (PE) signals (Figure G in <xref ref-type="supplementary-material" rid="pcbi.1006370.s002">S2 File</xref>), which are used to update the expectations themselves [<xref ref-type="bibr" rid="pcbi.1006370.ref003">3</xref>]. This monitoring process is at the basis of two dACC modules, and in substantial agreement with the experimental literature (see [<xref ref-type="bibr" rid="pcbi.1006370.ref003">3</xref>] for a review). One dACC module (dACC<sub>Act</sub> in <xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref>) receives environmental <italic>states</italic> and selects <italic>actions</italic> directed toward the external environment (part of the <italic>external loop</italic>). Although value-based action selection involves also other subcortical and cortical structures (e.g. the dorsolateral prefrontal cortex, DLPFC), here we frame both value estimation and action selection within the dACC for both modeling parsimony and because also the MFC, with its motor components, plays an important role in action selection (see [<xref ref-type="bibr" rid="pcbi.1006370.ref003">3</xref>] for a review).</p>
<p>A second dACC module (dACC<sub>Boost</sub> in <xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref>) receives environmental states and consequently modulates (that is, <italic>boosts</italic>) the release of catecholamines from the brainstem nuclei LC and VTA (part of the <italic>internal loop</italic>). Catecholamines, in turn, control the internal dynamics of the dACC in real time (i.e. while the RML is interacting with the environment), by modulating the magnitude of <italic>reward</italic> signals (by VTA module) and the amount of <italic>effort</italic> (by LC module) that the RML exerts to execute a task. Although the dACC<sub>Boost</sub> module is the main responsible for catecholaminergic modulation, the dACC<sub>Act</sub> module, too, is in recurrent interaction with the brainstem nuclei, providing the VTA with a reward prediction signal. The latter is used by the VTA to compute <italic>non-primary rewards</italic>, which are sent back to the dACC<sub>Act</sub> (like in a TD-learning algorithm; [<xref ref-type="bibr" rid="pcbi.1006370.ref023">23</xref>]) allowing the system to learn complex tasks without the immediate availability of primary rewards (higher-order conditioning).</p>
<p>Importantly, both dACC modules have dynamic <italic>learning rates</italic> (λ), ensuring that knowledge is updated only when there are relevant environmental changes (volatility). Learning rate adaptation emerges from the interaction between the LC and both dACC modules. Each dACC module feeds the LC with reward prediction and PE signals, while the LC analyzes these “raw data” from the cortex (approximating a Bayesian learner), estimating volatility and adjusting the modules’ learning rate as a consequence.</p>
<p>Finally, the RML can be connected to other neural models (e.g. a visuo-spatial working memory model, see Simulation 2c). This allows the effort-related signal from the LC to modulate processing in other brain areas for performance optimization (<xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref>, orange arrows; see <xref ref-type="sec" rid="sec038">Methods</xref> for details).</p>
</sec>
<sec id="sec004">
<title>The RML: Innovations</title>
<p>In this section we briefly introduce the main theoretical novelties of the RML. For a more detailed analysis we address the reader to the Discussion section, where we also relate the RML in detail to previous models, describe explicit experimental predictions that derive from the model, and speculate on the potential application of the RML to translational research.</p>
<p>The RML is an autonomous agent able to near-optimally adapt to a diverse range of environments and tasks, with no need of task-specific parameters setting: across all the reported simulations the RML autonomously controlled its internal dynamics as a function of the environmental challenges, with no offline parameters optimization or human intervention (i.e. one parameter set was used for all the simulations). From here, four major novelties can be identified.</p>
<sec id="sec005">
<title>Meta-learning via recurrence with brainstem neuromodulators</title>
<p>The RML internal loop (dACC-VTA-LC) allows meta-learning of optimal modulation over learning rate, reward, and effort. Due to this interaction, the RML achieves autonomous flexibility to manage changing demands in cognitive control. To the best of our knowledge, this is the first computational theory on how neuromodulators are controlled during task execution and how these can influence behavioural performance. This also implies that several decision problems that in earlier work were tackled via hierarchical models (e.g. hierarchical models for effort modulation [<xref ref-type="bibr" rid="pcbi.1006370.ref024">24</xref>] or hierarchical Bayesian models [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>]) are solved here by the RML loops that have no intrinsic hierarchical structure itself (cf. simulations 1 and 2a-c, and “Relationships to Other Models” section). Our proposal adds a novel theoretical perspective with the aim of being complementary rather than alternative to hierarchical models.</p>
<p>Control on other neural circuits. The external loop of the RML can be used not only to drive optimal external behavior, but also to drive optimization of brain networks beyond (thus external to) the dACC-VTA-LC circuit. Specifically, the LC module (under the influence of the dACC) generates control signals (based on task demands) that can modulate (e.g. gain modulation) the activity of other neural modules. This simulates how the dACC can exert cognitive control over other brain areas, via catecholaminergic modulation. For example, we simulated how the fronto-parietal network in working memory tasks can be optimally modulated by LC NE release, thanks to the dialogue between the dACC and the brainstem nuclei (cf. Simulation 2c). Thus, the RML can generate cognitive control signals for improving the performance of different, independently designed and published models.</p>
</sec>
<sec id="sec006">
<title>Comprehensive understanding of DA dynamics</title>
<p>In the RML, the typical DA dynamics as recorded from the VTA during conditioning tasks (e.g. PE coding and DA shifting from reward onset to cue onset) emerges from the interaction between dACC and VTA–in contrast with the classical view representing the VTA itself as the main source of PE and temporal difference (TD) signals (e.g. [<xref ref-type="bibr" rid="pcbi.1006370.ref025">25</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref026">26</xref>]). This mechanism (see also [<xref ref-type="bibr" rid="pcbi.1006370.ref022">22</xref>]) is based on a large amount of empirical data identifying the dACC as a major source of PE signals (see [<xref ref-type="bibr" rid="pcbi.1006370.ref003">3</xref>] for a review); it is here integrated within a comprehensive theory on the cortical origin of the DA dynamics and what could be its computational role in decision-making.</p>
</sec>
<sec id="sec007">
<title>Mechanism underlying intrinsic motivation</title>
<p>As the dopaminergic reward signals from midbrain to dACC are modulated by the dACC itself (based on task demand), the RML implements a computational hypothesis about the mechanisms behind intrinsic motivation [<xref ref-type="bibr" rid="pcbi.1006370.ref027">27</xref>], i.e. on how the mammalian brain can energize behavior in a way that is independent from the immediate availability of primary rewards (cf. Simulations 3a-b).</p>
</sec>
</sec>
</sec>
<sec id="sec008" sec-type="results">
<title>Results</title>
<p>Here we present the results on both neural and behavioural dynamics of the RML in six key experimental paradigms selected from lower and higher cognitive decision-making domains. We show how the RML can provide a unified framework to explain experimental data from a set of decision-making contexts to which dACC and catecholamines are often related, namely optimal decision-making in uncertain and volatile conditions (Simulation 1), and optimal control of both physical and cognitive effort exertion (Simulations 2a-c). Finally, we generalize our findings to a domain where the dACC is typically not discussed, yet very important in decision-making, i.e. modulating intrinsic motivation to learn complex tasks without the immediate availability of primary rewards (higher-order conditioning; Simulations 3a-b). As the RML represents a generalization of previous RL models of MFC functions (the RVPM [<xref ref-type="bibr" rid="pcbi.1006370.ref022">22</xref>], and the PRO model [<xref ref-type="bibr" rid="pcbi.1006370.ref028">28</xref>]), it can reproduce also all the experimental findings simulated by those models (e.g. error detection and error likelihood estimation). This extends the RML results to an even wider domain of experimental paradigms.</p>
<p>To mimic standard experimental paradigms as closely as possible, we repeated each simulation only 12 times (i.e., 12 simulated subjects). This verified that the model can generate a large effect size of results. Obviously, p-values (but not effect sizes) improved when running more simulated subjects. Further details on simulations methods can be found in the Supporting Information in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>.</p>
<sec id="sec009">
<title>Simulation 1: Learning rate optimization</title>
<p>Adaptive control of learning rate is a fundamental aspect of cognition. Humans can solve the tradeoff between stability and plasticity in a (near) Bayesian fashion [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref029">29</xref>], distinguishing between variability due to noise versus variability due to actual changes of the environment; thus they can increase the learning rate only when volatility is detected [<xref ref-type="bibr" rid="pcbi.1006370.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref031">31</xref>]. At the neural level, a currently unexplained dissociation exists between dACC and LC activity, recorded during decision-making tasks where uncertainty due to noise and uncertainty due to volatility were systematically manipulated. The LC activity (and thus NE release) has been shown to track specifically volatility [<xref ref-type="bibr" rid="pcbi.1006370.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref033">33</xref>], while the results about the dACC role in volatility estimation are less consistent. Indeed, while in the seminal study by Behrens et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>], the dACC was hypothesized to track volatility, more recent study suggested that dACC activity in volatile environments are driven rather by PE coding, rather than specifically by volatility estimation [<xref ref-type="bibr" rid="pcbi.1006370.ref021">21</xref>]. These empirical findings seem to attribute different roles to LC and dACC in uncertainty coding, without providing a computational rationale for their functional specialization.</p>
<p>In this simulation, we will investigate to what extent the model accounts for human adaptive control of learning rate at both behavioural and neural levels, and whether it can explain the dACC/LC dissociation.</p>
<sec id="sec010">
<title>Simulation methods</title>
<p>We administered to the RML a 2-armed bandit task in three different stochastic environments (<xref ref-type="fig" rid="pcbi.1006370.g002">Fig 2A and 2B</xref>). The three environments were: stationary environment (Stat, where the links between reward probabilities and options were stable over time, either 70 or 30%), stationary with high uncertainty (Stat2, also stable reward probabilities, but all the options led to a reward in 60% of times), and volatile (Vol, where the links between reward probabilities and options randomly changed over time). We administered a total of 432 trials equally distributed between the three statistical environments. We assigned higher reward magnitudes to choices with lower reward probability, to promote switching between choices and to make the task more challenging (cf. [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>]). Nonetheless, the value of each choice (probability × magnitude) remained higher for higher reward probability (see Table B in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>), meaning that reward probability was the relevant variable to be tracked. A second experiment, where we manipulated reward magnitude instead of reward probability led to very similar results (see Simulations S1 and S3 in <xref ref-type="supplementary-material" rid="pcbi.1006370.s002">S2 File</xref>).</p>
<fig id="pcbi.1006370.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Simulation 1: Methods and results.</title>
<p><bold>a)</bold> The task (2-armed bandit) is represented like a binary choice task (blue or red squares), where the model decisions are represented as joystick movements. After each choice, the model received either a reward (sun) or not (cross). <bold>b)</bold> Example of task design with time line of statistical environments (order of presentation of different environments was randomized across simulations). The plot shows reward probability linked to each option (blue or red) as a function of trial number. In this case the model executed the task first in a stationary environment (Stat), then in a stationary environment with high uncertainty (Stat2), and finally in a volatile (Vol) environment. <bold>c)</bold> Learning rate (<italic>λ</italic>) time course (average across simulations ± s.e.m.). As the order of statistical environments was randomized across simulations, each simulation time course was sorted as Stat-Stat2-Vol. <bold>d, e)</bold> Average ∠ (across time and simulations) as a function of environmental volatility (± s.e.m.) in the RML (d) and humans (<bold>e;</bold> modified from: [<xref ref-type="bibr" rid="pcbi.1006370.ref030">30</xref>]). <bold>f)</bold> human pupil size (proxy of LC activity [<xref ref-type="bibr" rid="pcbi.1006370.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref036">36</xref>]) during the same task.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>Simulation results and discussion</title>
<p>The RML performance in terms of optimal choice percentages was: Stat = 66.5% (± 4% s.e.m.), Vol = 63.6% (± 1.4% s.e.m.). For Stat2 condition there was no optimal choice, as both options led to reward in 60% of all trials. Importantly, the model successfully distinguished not only between stationary (Stat) and volatile (Vol) environments, but also between stationary-uncertain (Stat2) and Vol, increasing the learning rate (computed in the LC module) exclusively in the latter (<xref ref-type="fig" rid="pcbi.1006370.g002">Fig 2C and 2D</xref>). Indeed, there was a main effect of volatility on learning rate <italic>λ</italic> (F(2,11) = 29, p &lt; 0.0001). Post-hoc analysis showed that stationary conditions did not differ (Stat2 &gt; Stat, t(11) = 1.65, p = 0.13), while in the volatile condition learning rate was higher than in stationary conditions (Vol &gt; Stat2, t(11) = 5.54, p &lt; 0.0001; Vol &gt; Stat, t(11) = 5.76, p &lt; 0.0001). Hence, the interaction between dACC and LC allows disentangling uncertainty due to noise from uncertainty due to actual environmental changes [<xref ref-type="bibr" rid="pcbi.1006370.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref031">31</xref>] promoting plasticity (high learning rate) when new information must be acquired (condition Vol), and stability (low learning rate) when acquired information must be protected from noise (conditions Stat and Stat2). This mechanism controls learning rates in both the dACC<sub>Act</sub> and the dACC<sub>Boost</sub> modules, thus influencing the entire RML dynamics.</p>
<p>Differently from the LC, dACC<sub>Act</sub> showed a maximal activation in the Stat2 environment (uncertain) rather than in the volatile environment (<xref ref-type="fig" rid="pcbi.1006370.g003">Fig 3B</xref>). This dissociation is due to the different roles played by dACC and LC in learning rate control (see Eq 5A in <xref ref-type="sec" rid="sec038">Methods</xref>). Indeed, while dACC modules compute reward expectation and PE, the LC performs approximate Bayesian analysis on those signals to compute optimal learning rate. For this reason, the dACC is more responsive to overall environmental uncertainty (expressed by average PE), while LC selectively responds to volatility. As mentioned before, this dissociation between the LC and the dACC dynamics simulated by the RML were found also in humans. Indeed, in the same task, humans increased both learning rate and LC activity only in Vol environments [<xref ref-type="bibr" rid="pcbi.1006370.ref030">30</xref>] (<xref ref-type="fig" rid="pcbi.1006370.g002">Fig 2E and 2F</xref>). Moreover, during a RL task executed in the same three statistical environments used in this simulation, the human dACC activity peaked for the Stat2 environment, suggesting that activity of human dACC is dominated by PE rather than by explicit estimation of environmental volatility [<xref ref-type="bibr" rid="pcbi.1006370.ref021">21</xref>] (<xref ref-type="fig" rid="pcbi.1006370.g003">Fig 3A</xref>).</p>
<fig id="pcbi.1006370.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Simulation 1: Results comparison with fMRI data.</title>
<p><bold>a)</bold> Outcome-locked activation of human dACC (with 90% CI, extracted from the ROI indicated by the blue sphere; MNI: [<xref ref-type="bibr" rid="pcbi.1006370.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref044">44</xref>]) in a RL task executed during fMRI scanning. Data extracted by WebPlotDigitizer from Fig 4 in ref. [<xref ref-type="bibr" rid="pcbi.1006370.ref021">21</xref>]. The ROI is a local maximum within the cluster with the highest z value. The task was performed in the same three environments we used in our simulations. dACC activity peaked in Stat2 and not in Vol condition (Stat2 &gt; Vol, p &lt; 0.05), indicating responsiveness to overall uncertainty (i.e. PE) rather than to volatility (see ref. [<xref ref-type="bibr" rid="pcbi.1006370.ref021">21</xref>] for further details) <bold>b)</bold> dACC<sub>Act</sub> average activity (sum of PE units activity ± s.e.m.; see <xref ref-type="disp-formula" rid="pcbi.1006370.e001">Eq 1</xref> and Equations S3-S4 in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>) as a function of environmental uncertainty. Differently from the LC, the dACC<sub>Act</sub> is maximally active in stationary uncertain environments (Stat2), indicating that due to PE computation, dACC<sub>Act</sub> (like the human dACC) codes for overall uncertainty rather than for volatility.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g003" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec012">
<title>Simulation 2: Controlling physical and cognitive effort</title>
<p>A long list of experimental results indicates that DA and NE neuromodulators are not only crucial for learning environmental regularities, but also for exerting cognitive control [<xref ref-type="bibr" rid="pcbi.1006370.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref041">41</xref>]. Although these mechanisms have been widely studied, little is known about how the brainstem catecholamine output is controlled to maximize performance [<xref ref-type="bibr" rid="pcbi.1006370.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref042">42</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref043">43</xref>], and how the dACC is involved in such a process. In this section, we describe how the dACC<sub>Boost</sub> module learns to regulate LC and VTA activity to control effort exertion, at both cognitive and physical level [<xref ref-type="bibr" rid="pcbi.1006370.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref045">45</xref>]. In Simulation 2a, we test the cortical-subcortical dynamics in experimental paradigms involving decision-making in physically effortful tasks, where cost/benefit trade off must be optimized [<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref048">48</xref>]. In Simulation 2b, we show how the LC can provide a NE signal to external neural modules to optimize cognitive effort [<xref ref-type="bibr" rid="pcbi.1006370.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref020">20</xref>] allocation and thus behavioural performance in a visuo-spatial working memory (WM) task. In both simulations, we also test the RML dynamics and behaviour after cortical and subcortical lesions.</p>
</sec>
<sec id="sec013">
<title>Simulation 2a: Physical effort control and decision-making in challenging cost/benefit trade off conditions</title>
<p>Deciding how much effort to invest to obtain a reward is crucial for human and non-human animals. Animals can choose high effort-high reward options when reward is sufficiently high [<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>]. The impairment of the mesolimbic DA system strongly disrupts such decision-making [<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>]. Besides the VTA, experimental data indicate also the dACC as having a pivotal role in decision-making in this domain [<xref ref-type="bibr" rid="pcbi.1006370.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref048">48</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref050">50</xref>] (see also[<xref ref-type="bibr" rid="pcbi.1006370.ref051">51</xref>] for a review). In this simulation, we show how cortical-subcortical interactions between the dACC, VTA and LC can drive optimal decision-making when effortful choices leading to large rewards compete with low effort choices leading to smaller rewards. We thus test whether the RML can account for both behavioral and physiological experimental data from humans and nonhuman animals. Moreover, we test whether simulated ACC lesion or DA depletion can replicate the disruption of optimal decision-making, and, finally, how effective behaviour can be restored. Simulation results will be compared with behavioural data from rodents ([<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>], see also Simulation 2a in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>), and with physiological data from nonhuman primates [<xref ref-type="bibr" rid="pcbi.1006370.ref035">35</xref>] and humans [<xref ref-type="bibr" rid="pcbi.1006370.ref044">44</xref>]. Rodent data from Walton et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>] were chosen for comparison to study how the cost-benefit trade-off could be affected by ACC damage and by DA lesion and how behavioural performance could be partially recovered with environmental intervention (Simulation 2b). We express the caveat that DA depletion studies in the literature we cited ([<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>], to compare with RML performance) either deplete DA systemically, or are focused more on the mesolimbic-accumbens path than on DA afferents to the medial prefrontal cortex. Our assumption that mesolimbic DA lesion affects dACC functioning is neurophysiologically sound, because functional and anatomical connectivity indicates strong nucleus accumbens (NAc)—dACC connectivity [<xref ref-type="bibr" rid="pcbi.1006370.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref052">52</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref053">53</xref>], probably contributing to convey reward-related information to the dACC. For this reason, lesioning the NAc may also disrupt the information flow from VTA to the dACC. Moreover, our simulations lead to the experimental prediction that DA lesion to dACC generates effects similar to mesolimbic DA lesions.</p>
<sec id="sec014">
<title>Simulation methods</title>
<p>We administered to the RML a 2-armed bandit task with one option requiring high effort to obtain a large reward, and one option requiring low effort to obtain a small reward (here called Effort task [<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>]; <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4A</xref>). We also administered to the model a task where both options implied a low effort (called No Effort task; <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4D</xref>). The tasks were also administered to a dACC-lesioned and to a DA-lesioned RML (simulated, respectively, by reducing all neural activations in both the dACC modules and by reducing all VTA outputs; further details about RML simulations and the experimental data from rodents can be found in Simulation 2a in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>).</p>
<fig id="pcbi.1006370.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Simulation 2a: Methods and Results.</title>
<p><bold>a)</bold> Effort task, where a high effort choice (thick arrow from joystick) leading to high reward (HR, large sun) was in competition with a low effort choice (thin arrow) leading to low reward (LR, small sun). <bold>b)</bold> Behavioural results (average HR/(LR+HR) ratio ±s.e.m., and average Stay/(LR+HR+Stay) choices ratio percentage ±s.e.m.) from RML and <bold>c)</bold> empirical data from rodents [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>], in controls (blue), DA lesioned (red) and ACC lesioned (green) subjects. <bold>d)</bold> No Effort task, same as <bold>a)</bold> but with both options implying a low effort (thin black arrows). <bold>e)</bold> dACC<sub>Boost</sub> efferent signal (boosting level <italic>b</italic>) time course over trials (average across simulations ± s.e.m.). <bold>f)</bold> dACC<sub>Boost</sub> efferent signal (<italic>b;</italic> average across time and simulations) as a function of task type (Effort or No Effort task) and DA lesion. The boosting value is higher in the Effort task (main effect of task), but there is also a task x lesion interaction indicating the dACC<sub>Boost</sub> attempts to compensate the loss of DA in the No Effort task (see main text). Results from the dACC lesion are not reported, as the simulated lesion targeted the dACC itself, leading to an obvious reduction of dACC<sub>Boost</sub> activity. <bold>g)</bold> LC activity as a function of physical effort in the rhesus monkey [<xref ref-type="bibr" rid="pcbi.1006370.ref035">35</xref>]. Like in the RML (panel f, blue plot), the LC activity (controlled by the dACC<sub>Boost</sub>) is higher for high effort condition. <bold>h)</bold> RML net subjective value computed in both dACC modules (sum of net values from both dACC modules, Equation S18 in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>) for the HR choice as a function of effort. <bold>i)</bold> Like in the human brain [<xref ref-type="bibr" rid="pcbi.1006370.ref044">44</xref>] the RML dACC computes also the net value (i.e. the value discounted by the expected cost) of choices.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g004" xlink:type="simple"/>
</fig>
<p>Before the execution of the Effort task, the RML learned the reward values in a task where both options implied low effort (No Effort task). Besides the high effort and low effort choices, the model could choose to execute no action if it evaluated that no action was worth the reward (“Stay” option). Animal data for comparison are from [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>] (see Simulation 2a in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>).</p>
</sec>
<sec id="sec015">
<title>Simulation results and discussion</title>
<p>At the behavioural level (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B</xref>, blue), the RML, like animal subjects [<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>] (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4C</xref>, blue), prefers choosing the high-effort-high-reward option (HR) during the Effort task (t(11) = 4.71, p = 0.0042). Again in agreement with rodent data [<xref ref-type="bibr" rid="pcbi.1006370.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>], both DA and dACC lesions (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B and 4C</xref>, red and green) change this behaviour in a similar manner. Compared with controls, DA lesion increases both the number of choices for low-effort-low-reward (LR) option (t(11) = 3.71, p = 0.0034) and how often the model refuses to engage in the task (“Stay”; t(11) = 18.2, p &lt; 0.0001). dACC lesion leads to the same pattern, with both an increase of LR preference (t(11) = 13.6, p &lt; 0.0001) and of Stay options (t(11) = 11.6, p &lt; 0.0001).</p>
<p>At the neural level, the dACC<sub>Boost</sub> increased the boosting level (<italic>b</italic>) in the Effort task (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4F</xref>; main effect of task, F(1,11) = 231.73, p &lt; 0.0001) enhancing both LC and VTA output. Also nonhuman primates show the same LC effort-related modulation, with a higher LC activation for high effort choices [<xref ref-type="bibr" rid="pcbi.1006370.ref035">35</xref>] (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4G</xref>).</p>
<p>The plot in <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4E</xref> shows how the RML learns over trials to boost catecholamine release, representing the trial-by-trial optimization process to find the best intensity of modulation over both VTA and LC. Increased <italic>NE</italic> influences decision-making in the dACC<sub>Act</sub> (effect of <italic>NE</italic> on action cost estimation in decision-making process, Eq 2 in <xref ref-type="sec" rid="sec038">Methods</xref>), facilitating effortful actions, while increased DA affects learning in the dACC<sub>Act</sub> (Eqs 1 and 7A in <xref ref-type="sec" rid="sec038">Methods</xref>), increasing the reward signal related to effortful actions. At the same time, boosting catecholamines has a cost (Eq 6B in <xref ref-type="sec" rid="sec038">Methods</xref>), so that the higher <italic>b</italic>, the higher was the reward discount for the dACC<sub>Boost</sub> module. The result of these two opposite forces (maximizing performance by catecholamines boosting and minimizing the cost of boosting itself) converges to the optimal value of <italic>b</italic> and therefore of catecholamines release by VTA and LC (<xref ref-type="fig" rid="pcbi.1006370.g005">Fig 5A</xref>). After DA lesion, the dACC<sub>Boost</sub> decreased boosting output during the Effort task, while it increased the boosting output during the No Effort task (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4F</xref>, red; task x lesion interaction F(1,11) = 249.26, p &lt; 0.0001). Decreased boosting derives from decreased DA signal to dACC<sub>Boost</sub> module (<xref ref-type="fig" rid="pcbi.1006370.g005">Fig 5B</xref>). Increased boosting <italic>b</italic> in No Effort task can be interpreted as a compensatory mechanism ensuring the minimal catecholamines level to achieve the large reward (HR option) when just a low effort is necessary (<xref ref-type="fig" rid="pcbi.1006370.g005">Fig 5D</xref>). In other words, when the incentive is high (high reward available) and the effort required to obtain the reward is low, the RML predicts that the DA lesioned animal would choose to exert some effort (boosting up the remaining catecholamines) to promote task engagement versus “Stay” option.</p>
<fig id="pcbi.1006370.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Cost-benefits plots and optimal control of <italic>b</italic> in the dACC<sub>Boost</sub> module.</title>
<p>To obtain these plots we systematically clamped <italic>b</italic> at several values (from 1 to 10, x axis of each plot) and then we administered the same paradigms of <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B and 4D</xref> (all the combinations Effort x DA lesion). In all the plots, y axis represents simultaneously performance in terms of average reward signal to dACC<sub>Boost</sub> (blue plots), boosting cost (red plots) and net value (performance–boost cost, described by <xref ref-type="disp-formula" rid="pcbi.1006370.e012">Eq 6B</xref>). <bold>a)</bold> Effort task, no lesion. Plot showing RML behavioural performance as a function of <italic>b</italic> (blue plot), boosting cost (red plot, Eq 6B in <xref ref-type="sec" rid="sec038">Methods</xref>) and net value for the dACC<sub>Boost</sub> module (green plot, resulting from <xref ref-type="disp-formula" rid="pcbi.1006370.e012">Eq 6B</xref>). Red dotted circles highlight the optimal <italic>b</italic> value which maximizes the final net reward signal received by the dACC<sub>Boost</sub> module. (maximum of green plot) <bold>b)</bold> Effort task, DA lesion. Same as a), but in this case the RML was DA lesioned. Due to lower average reward signal (blue plot), the net value (green) decreases monotonically, because the cost of boosting (red plot) did not change. Red dotted circle highlights the optimal <italic>b</italic> value, which is lower than in a). It must be considered that, although the optimal <italic>b</italic> value is 1, the average <italic>b</italic> (as shown in figures 5b and s11b) is biased toward higher values, as it is selected by a stochastic process (<xref ref-type="disp-formula" rid="pcbi.1006370.e004">Eq 4</xref>) and values lower than 1 are not possible (asymmetric distribution). <bold>c)</bold> No Effort task, no lesion. In this case, being the task easy, the RML reaches a maximal performance without high values of <italic>b</italic> (blue plot is flat), therefore the optimal <italic>b</italic> value is low also in this case. <bold>d)</bold> No Effort task, DA lesion. As shown also in <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B</xref>, in this case the optimal <italic>b</italic> value (dotted circle), is higher than in c), because a certain amount of boosting is necessary to avoid the preference for “Stay” option, which has no costs but also provides no reward. This ensures a minimal behavioural energization to prevent apathy and get a large reward paying a minimal cost (as it is a No Effort task). Plots are average on 40 simulations, error shadows mean s.e.m.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g005" xlink:type="simple"/>
</fig>
<p>Finally, human dACC activity is known to covary not only with effort exertion, but also with net subjective value in effortful tasks (i.e. the expected value of an action discounted by its associated expected effort) [<xref ref-type="bibr" rid="pcbi.1006370.ref044">44</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref054">54</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref055">55</xref>]. In <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4H</xref>, we show how the combined signal from both dACC modules (Equation S18 in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>) codes also for the net subjective value, in comparison with human fMRI from [<xref ref-type="bibr" rid="pcbi.1006370.ref044">44</xref>] (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4I</xref>).</p>
</sec>
</sec>
<sec id="sec016">
<title>Simulation 2b: Performance recovery after DA lesion, in cost/benefit trade off conditions</title>
<p>In DA lesioned subjects, the preference for HR option can be restored by removing the difference in effort between the two options [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>], that is, by removing the critical trade-off between costs and benefits. In Simulation 2b, we show how the RML can recover a preference toward HR options, as demonstrated empirically in experimental paradigms used in rats. We focused specifically on recovery after DA lesion. Our choice was aimed at investigating the consequences of DA lesion at cortical-subcortical level and how these can be modulated by the environment, to open a view on future translational scenarios about DA-related neuropsychiatric disorders. We elaborate on the latter topic in the Experimental Predictions section.</p>
<sec id="sec017">
<title>Simulation methods</title>
<p>The same DA lesioned subjects of Simulation 2a were exposed to either a No Effort task (where both the option required low effort) or a Double Effort task (where both the options required a high effort) (<xref ref-type="fig" rid="pcbi.1006370.g006">Fig 6A</xref>). All other experimental settings were identical to those of Simulation 2a. Animal data for comparison are from [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>] (see also Simulation 2b in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>).</p>
<fig id="pcbi.1006370.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Recovery of HR option preference after DA lesion.</title>
<p><bold>a)</bold> Double Effort task, where both options implied high effort. <bold>b)</bold> Recovery of the preference for HR option (HR/(HR+LR)) when a No Effort task is administered after an Effort task session (Effort → No Effort, blue plot), in both RML and <bold>c)</bold> animals [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>] (mean percentage ± s.e.m.). Same phenomenon when a Double Effort session follows an Effort one (Effort → Double Effort, red plot). Note that in this case the number of “Stay” choices (Stay/number of trials) increased, simulating the emergence of apathic behaviour.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec018">
<title>Simulation results and discussion</title>
<p>DA-lesioned RML performance recovers immediately when a No Effort task is administered after the Effort task (<xref ref-type="fig" rid="pcbi.1006370.g006">Fig 6B</xref>, blue), in agreement with animal data ([<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>]; <xref ref-type="fig" rid="pcbi.1006370.g006">Fig 6C</xref>, blue). This result shows that performance impairment after DA lesion in the model is not due to a learning deficit (although partial learning impairment must occur due to the role of DA in learning), but rather to down-regulation of catecholamines boosting, driven by dACC<sub>Boost</sub>. A task where both options require a low effort does not need a strong behavioural energization, therefore the information about the high reward location is sufficient for an optimal execution.</p>
<p>The same performance recovery occurs also in a task where both options are effortful (Double Effort task, <xref ref-type="fig" rid="pcbi.1006370.g006">Fig 6B</xref>, red), again in agreement with experimental data (<xref ref-type="fig" rid="pcbi.1006370.g006">Fig 6C</xref>, red). Also in this case, when there is no trade-off between costs and benefits (both options are the same in terms of effort), the information about high reward location is sufficient to execute the task optimally, although there is a reduced catecholamine boosting. Nonetheless, differently from the previous scenario, apathy emerges here (percentage of “Stay”). Indeed, the RML often refuse to engage in the task; rather than working hard to get the high reward (whose position is well known) it prefers to remain still. Apathic behaviour in this experiment is more evident than in <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B and 4C</xref>, because both RML and animals are forced to make an effort to get a reward, while in Simulation 2a (<xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B</xref>) they could opt for the low effort-low reward choice.</p>
</sec>
</sec>
<sec id="sec019">
<title>Simulation 2c: Adapting cognitive effort in a working memory (WM) task</title>
<p>NE neuromodulation also plays a crucial role in WM, improving signal-to-noise ratio by gain modulation mediated by <italic>α</italic>2-A adrenoceptors [<xref ref-type="bibr" rid="pcbi.1006370.ref037">37</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref056">56</xref>]. A low level of NE transmission leads to WM impairment [<xref ref-type="bibr" rid="pcbi.1006370.ref057">57</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref058">58</xref>]. At the same time, as described above, NE is a major biological marker of effort exertion [<xref ref-type="bibr" rid="pcbi.1006370.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref059">59</xref>]. Besides NE release by the LC, experimental findings showed that also dACC activity increases as a function of effort in WM tasks [<xref ref-type="bibr" rid="pcbi.1006370.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref060">60</xref>]. Here we show that the same machinery that allows optimal physical effort exertion (Simulation 2a) may be responsible for optimal catecholamine management to control the activity of other brain areas, thus rooting physical and cognitive effort exertion in a common decision-making mechanism. This is possible because the design of the RML allows easy interfacing with external modules (<xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref> and Methods).</p>
<sec id="sec020">
<title>Simulation methods</title>
<p>We connected the RML to a WM model (FROST model; Ashby et al. 2005; see "FROST model description" section in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>). Information was exchanged between the two models through the state/action channels in the dACC<sub>Act</sub> module and the external LC output. The FROST model was chosen for convenience only; no theoretical assumptions prompted us to use this model specifically. FROST is a dynamical recurrent neural network simulating a macro-circuit involving the DLPFC, the parietal cortex and the basal ganglia. This model simulates behavioural and neurophysiological data in several visuo-spatial WM tasks. FROST dynamics simulates the effect of memory loads on information coding, with a decrement of coding precision proportional to memory load (i.e. the number of spatial locations to be maintained in memory). This feature allows to simulate the increment of behavioural errors when memory load increases [<xref ref-type="bibr" rid="pcbi.1006370.ref061">61</xref>]. In this simulation, the external LC output improves the signal gain in the FROST DLPFC neurons, increasing the coding precision of spatial locations retained in memory (Equation S22 in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>), thus improving behavioural performance. We administered to the RML-FROST circuit a delayed matching-to-sample task with different memory loads (a template of 1, 4 or 6 items to be retained; <xref ref-type="fig" rid="pcbi.1006370.g007">Fig 7A</xref>). We used a block design, where we administered three blocks of 70 trials, each with one specific memory load (1, 4, or 6). In 50% of all trials, the probe fell within the template. The statistical analysis was conducted by a repeated measure 3 × 2 ANOVA (memory load by DA lesion).</p>
<fig id="pcbi.1006370.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Simulation 2c: Methods and results.</title>
<p><bold>a)</bold> Delayed Matching-to-sample task: events occurring in one trial. <bold>b)</bold> RML behavioural performance as a function of memory load and DA lesion (±s.e.m.). <bold>c)</bold> dACC<sub>Boost</sub> output as a function of memory load and DA lesion (±s.e.m). <bold>d)</bold> Local maxima in which human dACC activity covaries with cognitive effort (left) and dACC activity as a function of memory load in a WM task ref (right, from blue coordinates).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec021">
<title>Simulation results and discussion</title>
<p>At the behavioural level, the FROST-RML system maintain a good performance also for high memory loads (<xref ref-type="fig" rid="pcbi.1006370.g007">Fig 7B</xref>, blue plot). At the neural level, the dACC<sub>Boost</sub> module dynamically modulates catecholamine release as a function of memory load, in order to optimize performance (<xref ref-type="fig" rid="pcbi.1006370.g007">Fig 7C</xref>, blue plot; main effect of memory load on dACC<sub>Boost</sub> output: F(2,22) = 16.74, p &lt; 0.0001). The computational mechanisms involved in this effect are the same as described in Simulation 2a: The dACC<sub>Boost</sub> enhances both VTA and LC output, balancing the performance benefits of catecholamines boosting versus the intrinsic cost of boosting. For this reason, when the task is easy (low memory load), catecholamines are low. There is no need to boost in this case: Boosting would be just a cost. In contrast, when the task becomes harder (higher memory loads), catecholamines release increases to keep performance (and reward) high (same mechanism depicted in <xref ref-type="fig" rid="pcbi.1006370.g005">Fig 5</xref>). RML-like dACC activity was found also in healthy humans [<xref ref-type="bibr" rid="pcbi.1006370.ref019">19</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref020">20</xref>] during WM and mental arithmetic tasks (<xref ref-type="fig" rid="pcbi.1006370.g007">Fig 7D</xref>).</p>
<p>In case of DA lesion, at behavioural level, this results in poor performance in particular for high memory loads, when a high level of <italic>NE</italic> is necessary (<xref ref-type="fig" rid="pcbi.1006370.g007">Fig 7B</xref>, red plot; lesion × memory-load interaction: F(2,22) = 8.6, p = 0.0017). This behavioural pattern is due to the consequent disruption of VTA-dACC-LC interaction, leading to a devaluation of boosting and the consequent decision (by the dACC<sub>Boost</sub> module) of downregulating LC activity (<xref ref-type="fig" rid="pcbi.1006370.g007">Fig 7C</xref>, red plot; main effect of DA lesion on LC output: F(1,11) = 24.88, p &lt; 0.0001). This happened especially for high memory loads (lesion × memory-load interaction: F(2,22) = 7.1, p = 0.0042).</p>
</sec>
</sec>
<sec id="sec022">
<title>Simulation 3: Reinforcement learning, meta-learning and higher-order conditioning</title>
<p>Animal behavior in the real world is seldom motivated by conditioned stimuli directly leading to primary rewards. Instead, behavior is guided by higher-order conditioning, bridging the gap between reward and behavior. However, a unifying account explaining behavioral results and underlying neurophysiological dynamics of higher-order conditioning is currently lacking. First, at the behavioral level, literature suggests a sharp distinction between higher-order conditioning in classical versus instrumental paradigms. Indeed, although it is possible to train animals to execute complex chains of actions to obtain a reward (instrumental higher-order conditioning, [<xref ref-type="bibr" rid="pcbi.1006370.ref062">62</xref>]), it is impossible to install a third- or higher-order level of classical conditioning (i.e. when no action is required to get a reward [<xref ref-type="bibr" rid="pcbi.1006370.ref063">63</xref>]). Although the discrepancy has been well known for decades, its reason has not been resolved. Second, a number of models have considered how TD signals can support conditioning and learning more generally [<xref ref-type="bibr" rid="pcbi.1006370.ref064">64</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref065">65</xref>]. However, no model addressing DA temporal dynamics also simulated higher-order conditioning at behavioural level.</p>
<p>Here we use the RML to provide a unified theory to account for learning in classical and instrumental conditioning. We show how the RML can closely simulate the DA shifting in classical conditioning (Simulation S2 and Fig F in <xref ref-type="supplementary-material" rid="pcbi.1006370.s002">S2 File</xref>). We also describe how the VTA-dACC interaction allows the model to emancipate itself from primary rewards (higher-order conditioning). Finally, we investigate how the synergy between the VTA-dACC<sub>Boost</sub> and LC-dACC<sub>Boost</sub> (the catecholamines boosting dynamics) is necessary for obtaining higher-order instrumental conditioning and how this process could be considered one of the foundations of <italic>intrinsic motivation</italic>. This provides a mechanistic theory on why higher-order conditioning is possible only in instrumental and not in classical conditioning.</p>
</sec>
<sec id="sec023">
<title>Simulation 3a: Higher-order classical conditioning</title>
<p>As VTA can vigorously respond to conditioned stimuli, it is natural to wonder whether a conditioned stimulus can work as a reward itself, allowing to build a chain of progressively higher-order conditioning (i.e. not directly dependent on primary reward). However, for unknown reasons, classical higher-order conditioning is probably impossible to obtain in animal paradigms [<xref ref-type="bibr" rid="pcbi.1006370.ref063">63</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref066">66</xref>]. We thus investigate what happens in the model in such a paradigm.</p>
<sec id="sec024">
<title>Simulation methods</title>
<p>We first administered a first-order classical conditioning paradigm. We then conditioned a second cue by using the first CS as a non-primary reward. The same procedure was repeated up to third-order conditioning. Each cue was presented for 2s followed by the successive cue or by a primary reward. All cue transitions were deterministic and the reward rate after the third cue was 100%.</p>
</sec>
<sec id="sec025">
<title>Simulation results and discussion</title>
<p>In <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8A</xref> we show the VTA response locked to the onset of each conditioned stimulus. Surprisingly, but in agreement with experimental animal data, the conditioned cue-locked DA release is strongly blunted at the 2<sup>nd</sup> order, and disappeared almost completely at the 3<sup>rd</sup> order. This aspect of VTA module dynamics is because, at each order of conditioning, the cue-locked DA signal is computed as the temporal derivative of reward prediction activity from dACC<sub>Action</sub> (Equation S5b in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>). This mechanism implies a steep decay of the conditioning effectiveness of non-primary rewards, because the reinforcing property of cues becomes lower at each order of conditioning. From an ethological viewpoint, it makes sense that the weaker is the link between a cue and a primary reward, the weaker should be its conditioning effectiveness. Nonetheless, as we describe in the following paragraph, this phenomenon is partially counteracted in instrumental conditioning, making higher-order conditioning effective.</p>
<fig id="pcbi.1006370.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Simulation 3a-b: Methods and results.</title>
<p><bold>a)</bold> Experimental paradigm for higher-order classical conditioning (lower row) and cue-locked VTA response (upper row). The task consisted of a sequence of conditioned stimuli (colored disks) followed by primary reward (sun). Already at the second conditioning order, VTA activity results almost absent. <bold>b)</bold> During a higher-order instrumental conditioning (lower row), the VTA response (upper row) remains sustained up to the third order. <bold>c)</bold> Average dACC<sub>Boost</sub> efferent signal (<italic>b</italic>± s.e.m.) in classical and instrumental paradigms. In instrumental paradigm the efferent boosting signal is higher, enhancing the VTA activity over different conditioning orders.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g008" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec026">
<title>Simulation 3b: Higher-order instrumental conditioning</title>
<p>Differently from classical conditioning paradigms, animal learning studies report that in instrumental conditioning it is possible to train complex action chains using conditioned stimuli (environmental cues) as reward proxies, delivering primary reward only at the end of the task [<xref ref-type="bibr" rid="pcbi.1006370.ref062">62</xref>].</p>
<sec id="sec027">
<title>Simulation methods</title>
<p>We administered to the RML a maze-like problem, structured as a series of binary choices before the achievement of a final reward (Figure E in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>). Each choice led to an environmental change (encoded by a colored disk, like in <xref ref-type="fig" rid="pcbi.1006370.g002">Fig 2</xref>). The training procedure was the same as for higher-order classical conditioning. We first administered a first-order instrumental conditioning (2-armed bandit task). Then, we used the conditioned environmental cue as non-primary reward to train the RML for second-order conditioning. The procedure was repeated up to third-order conditioning. State-to-state transitions were deterministic and primary reward rate was 100% for correct choices and 0% for wrong choices.</p>
</sec>
<sec id="sec028">
<title>Simulation results and discussion</title>
<p>At the end of training, the system was able to perform three sequential choices before getting a final reward, with an average accuracy of 77.3% (90% C.I. = ±13%) for the first choice (furthest away from primary reward; purple disk, <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8B</xref>); 95.8% (90% C.I. = [4.2, 5.6]%) for the second; and 98% (90% C.I. = ±0.4%) for the third choice (the one potentially leading to primary reward; orange disk, <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8B</xref>). <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8B</xref> shows the cue-locked VTA activity during a correct sequence of choices. Differently from classical conditioning, the DA signal amplitude persists over several orders of conditioning, making colored disks (also far away from final reward) effective non-primary rewards, which are able to shape behaviour. It is worth noting that in this simulation the RML self-enhances DA levels to energize behaviour also when primary reward is not available, i.e. it implements intrinsic motivation.</p>
<p>The reason for this difference between classical and instrumental conditioning, is in the role played by the dACC<sub>Boost</sub> module, and is based on the very same mechanisms underlying optimal control on effort exertion (Simulations 2a-c). <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8C</xref> compares average boosting levels <italic>b</italic> (efferent signal of dACC<sub>Boost</sub>) in classical and instrumental conditioning. The dACC<sub>Boost</sub> learned that boosting catecholamines was useful in instrumental conditioning; furthermore it learned that it was not useful in classical conditioning (t(11) = 5.64, p &lt; 0.0001). This decision amplified DA release during task execution only in instrumental conditioning (compare <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8A</xref> and <xref ref-type="fig" rid="pcbi.1006370.g008">Fig 8B</xref>). Enhanced VTA activity during the presentation of conditioned stimuli (the colored lights indicating a change in the problem space) means more effective higher-order conditioning, therefore a more efficient behaviour. Conversely, in classical conditioning, the model does not need to make any motor decision, as the task consists exclusively of passive observation of incoming cues (colored lights). Therefore, boosting NE and/or DA does not affect performance (reward amount), as this is completely decided by the environment. In this case, boosting would only be a cost (<xref ref-type="disp-formula" rid="pcbi.1006370.e012">Eq 6B</xref>), and the dACC<sub>Boost</sub> module learned not to boost, with a low DA levels for conditioned stimuli. This explains the strong limitations in establishing higher-order classical conditioning, shows how effort control is involved in higher-order conditioning, and how optimal effort regulation can motivate behaviour also when there is no immediate primary reward available (intrinsic motivation).</p>
</sec>
</sec>
</sec>
<sec id="sec029" sec-type="conclusions">
<title>Discussion</title>
<p>We proposed a novel perspective on the neurobiology of decision-making, showing that the recurrent interaction between the dACC and the catecholaminergic brainstem nuclei can generate meta-learning processes, which optimize neural parameters and therefore decision-making in interaction with a wide range of different environments and problems. The RML, the neuro-computational model implementing this novel perspective explains a wide array of heterogeneous empirical findings, including learning rate optimization, effort exertion in physical and cognitive tasks, and higher-order conditioning in classical and instrumental paradigms.</p>
<p>The first meta-learning process we analyzed concerned learning rate (Simulation 1). The RML provides an explicit theory and neuro-computational architecture of how autonomous control of learning rate can emerge from dACC-LC interaction. We propose that the dACC provides RL signals to the LC, about the statistical structure of the environment; in turn, the LC processes those signals to select optimal learning rate by approximating a Bayesian learner. This explains why both structures are necessary for optimal control of flexibility [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref033">33</xref>], and why empirical findings indicate that dACC and LC activity are respectively related to RL computation [<xref ref-type="bibr" rid="pcbi.1006370.ref021">21</xref>] and volatility estimation [<xref ref-type="bibr" rid="pcbi.1006370.ref030">30</xref>].</p>
<p>The second meta-learning process concerned effort exertion, and optimal allocation of both cognitive and physical effort to achieve a goal (Simulations 2a-c). We proposed that investing (cognitive or physical) effort and controlling associated costs is based on the same computational mechanisms involved in action selection, with one difference: in effort optimization, decision-making is not about actions toward the environment, but concerns the amount of catecholamines that must be released. Moreover, the RML generalizes this mechanism to virtually any cognitive domain, showing how the dACC-brainstem ensemble can work as a provider of optimal control signals (catecholamines) to other brain areas to maximize success while minimizing costs. Finally, effort control is itself modulated by the same mechanisms optimizing learning rate for action selection. This aspect provides near optimal meta-flexibility to cognitive control, a novelty that merges cognitive control with Bayesian learning.</p>
<p>The third meta-learning process that we simulated concerned intrinsic motivation via control over reward signals (both primary and non-primary). Thus, the (primary or nonprimary) reward signal does not depend exclusively on an environmental variable (the reinforcer), but instead can be proactively modulated to increase the value of effortful actions (thus energizing behaviour; <xref ref-type="disp-formula" rid="pcbi.1006370.e011">Eq 6A</xref>, simulations 2a-c) or to increase the value of non-primary rewards (simulations 3a-b). The latter mechanism allowed explaining why higher-order conditioning is possible in instrumental but not in classical paradigms. Moreover, as VTA activity is modulated by the same signal modulating <italic>NE</italic> release (<italic>b</italic> from dACC<sub>Boost</sub>), this feature provides a unified theoretical view on optimal effort allocation and control over motivational and learning aspects.</p>
<p>Although we described them separately, in the RML, learning rate, effort estimation and reward-related processes are integrated and mutually dependent. For example, dynamic control of learning rate (<italic>λ</italic>) is based on RL signals from dACC modules. Learning rate modulation influences both decision-making for action selection and for boosting control (<italic>b</italic>). Boosting control modulates in parallel both LC and VTA, modulating both performance (<italic>NE</italic>) and learning (<italic>DA</italic>). Catecholamine modulation changes behavioural performance, influencing action selection and environmental feedback, thus influencing LC control over learning rate.</p>
<sec id="sec030">
<title>Relationships to other models</title>
<sec id="sec031">
<title>RL models</title>
<p>The RML belongs to a set of computational models suggesting RL as main function of mammalian dACC [<xref ref-type="bibr" rid="pcbi.1006370.ref067">67</xref>]. For example, the main idea that dACC is a state-action-outcome predictor is inherited from previous RL neural models (the RVPM and PRO) that already tried to provide a unified view on dACC function (see [<xref ref-type="bibr" rid="pcbi.1006370.ref003">3</xref>] for a review). The RVPM, in particular, is a subcomponent of the RML model (Model description: dynamical form, in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>). This implies that the RML can also simulate the results obtained by the RVPM (e.g., congruency effects, error likelihood estimation), extending even further the amount of empirical data that can be explained by this framework. Although the RML goes beyond these earlier works, by implementing meta-learning and higher-order conditioning, it shares with them the hypothesis that PE plays a core role for learning and decision-making. Indeed, we hypothesize that PE is a ubiquitous computational mechanism, which allows both dACC operations (Eqs <xref ref-type="disp-formula" rid="pcbi.1006370.e001">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006370.e003">3</xref>) and the approximation of optimal learning rate in the LC (Eq <xref ref-type="disp-formula" rid="pcbi.1006370.e006">5A</xref>–<xref ref-type="disp-formula" rid="pcbi.1006370.e010">5D</xref>).</p>
</sec>
<sec id="sec032">
<title>Hierarchical RL models</title>
<p>Recent computational neuroscience of RL and decision-making focused on hierarchical architectures. For instance, Alexander and Brown [<xref ref-type="bibr" rid="pcbi.1006370.ref068">68</xref>] proposed a hierarchical RL model (based on their previous PRO model), where hierarchical design is implemented within the dACC, unfolding in parallel with a hierarchical model of the DLPFC. In this model, PE afferents from hierarchically lower dACC layers work as an outcome proxy to train higher layers; at the same time, error predictions formulated at higher layers of DLPFC modulate outcome predictions at lower ones. DLPFC-dACC communication is horizontal (i.e. between layers sharing the same hierarchical level), consisting in PE afferents from dACC to DLPFC, to update predictions. This architecture successfully learned tasks where information is structured at different abstraction levels (like the 1-2AX task), exploring the RL basis of autonomous control of information access to WM.</p>
<p>Also Holroyd and McClure [<xref ref-type="bibr" rid="pcbi.1006370.ref024">24</xref>] proposed a model exploiting hierarchical RL architecture (the HRL), where the dorsal striatum played a role of action selector, the dACC of task selector and the prelimbic cortex (in rodents) of context selector (where and when to execute a task). Moreover, each hierarchical layer implements a PE-based cognitive control signal that attenuates the costs of action (or task) selection on the lower hierarchical level. This model can explain a wide variety of data about task selection and decision-making in cognitive and physical effort regulation.</p>
<p>The RML differs from these two models for the following reasons. First, it can provide a theoretical account for a broad range of domains (from effort modulation to higher-order conditioning), while having a lower complexity (number of fixed parameters). Second, the RML lacks a genuine hierarchical structure. Its dynamics is emergent from the interaction between cortical and subcortical circuits, allowing meta-learning. This means that the RML provides a recurrent rather than hierarchical theory on the generation of cognitive control signals, without ruling out the relevance of hierarchical mechanisms like those implemented in the HER and HRL.</p>
</sec>
<sec id="sec033">
<title>Adaptive effort allocation models</title>
<p>The RML represents cognitive control as dynamic selection of effort exertion, a mechanism that has been recently studied also by Verguts et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref006">6</xref>], where effort allocation was framed as a decision-making problem. In this model, effort exertion was dynamically optimized by the dACC as a process of RL-based decision-making, so that effort levels were selected to maximize long-term reward. This solution successfully simulated many experimental results from cognitive control and effort investment. The RML makes a step forward, by introducing a mechanism that regulates flexibility of cognitive control itself. Indeed, the interaction between dACC and LC ensures near optimal control of learning rate also in the dACC<sub>Boost</sub> module. This makes possible to modulate the plasticity of decision-making about effort exertion, while the model is interacting with the environment. Moreover, the RML extends the modeling of cognitive control also to learning and motivation (VTA modulation), describing how LC and VTA influence each other while optimizing behaviour.</p>
<p>A second model by Verguts [<xref ref-type="bibr" rid="pcbi.1006370.ref069">69</xref>] described how dACC could implement cognitive control by functionally binding two or more brain areas by theta-frequency-locked activation bursts; the theta-wave amplitude would be proportional to the level of control. This theory describes how but not when (and neither how much) control should be exerted. The mechanisms proposed in the RML are complementary to this theory, hypothesizing how, when, and to what extent the dACC itself can decide to modulate theta bursts amplitude.</p>
<p>Le Bouc et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref070">70</xref>] recently proposed an interesting model-based behavioural analysis on Parkinson disease (PD) patients on and off medication, while executing a physical effort task. Their model aimed at choosing a force exertion level to maximize the expected net value during an effort-based decision-making task. They found that off medication patients had a reduced willingness for exerting effort (apathy) and a slower effort output when this was produced (motor impairment). The authors found that this behavioural pattern was captured by two different free parameters of the model. Apathy was captured by the free parameter coding for reward sensitivity, while motor impairment by the free parameter coding for the rate of motor activation. The RML provided similar results about apathy and reward sensitivity (DA lesion in Simulations 2a-b), with the advantages of ranging its explanatory power across different domains and of being explicitly defined from the neurophysiological point of view, producing in parallel both behavioral and neural dynamics.</p>
<p>In a recent work, the PRO model provided an alternative interpretation of effort-related dACC activation [<xref ref-type="bibr" rid="pcbi.1006370.ref009">9</xref>], where dACC activation is due to effort intensity prediction (and prediction error) and not to value of exerting effort or to any effort-related control signal. Although this theory is notable for parsimony, it provides no explanation about autonomous control of effort exertion, as it assumes that effort-related effects in the dACC are a byproduct of comparisons between predicted and experienced reward and effort levels. Moreover, it leaves unexplained the causal effects of both DA and dACC lesions and manipulations on effort control itself [<xref ref-type="bibr" rid="pcbi.1006370.ref047">47</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref071">71</xref>].</p>
</sec>
<sec id="sec034">
<title>Meta-learning in Bayesian and RL models</title>
<p>Khamassi et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref072">72</xref>] also hypothesized a role for dACC in meta-learning. The authors proposed a neural model (embodied in a humanoid robotic platform) where the temperature of the action selection process (i.e. the parameter controlling the trade-off between exploration and exploitation) was dynamically regulated as a function of PE signals. Like in the RML, dACC plays both a role in reward-based decision-making and in autonomous control of parameters involved in decision-making itself. Differently from the RML, this model provided a more classical view on PE origin, which were generated by the VTA and not by the dACC like in the RML. Moreover, the mechanism proposed for temperature control was modulated by overall environmental variance (PE), failing to disentangle noise from volatility.</p>
<p>Concerning control of learning rate, earlier <italic>Bayesian models</italic> also adapted their learning rates [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref073">73</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref074">74</xref>], proposing a computational account of behavioural adaptation. The main limitations of those models are their loose anatomo-functional characterization, the fact that they are computationally hard (in particular for optimal Bayesian solutions, e.g. [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>]), the need for ad hoc forward models of environment statistical structure and the presence of fixed parameters providing the model with explicit information about environmental volatility itself [<xref ref-type="bibr" rid="pcbi.1006370.ref073">73</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref074">74</xref>]. Ad hoc forward models are hierarchically organized, and at the top of this hierarchy, the experimenter defines a priori crucial characteristics (not updatable) about volatility (like the precision of the probability function describing environmental volatility [<xref ref-type="bibr" rid="pcbi.1006370.ref074">74</xref>]). To the best of our knowledge, the only Bayesian model able to estimate volatility without the need of specifying fixed parameters is the one by Behrens et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref004">4</xref>], which works only for binary outcomes.</p>
<p>In contrast, the RML provides an explicit neurophysiological theory on how near-optimal control emerges from the dialogue between dACC and brainstem, and it does not rely on fixed parameters providing information about environmental volatility itself. Indeed, we used one hyper-parameter (α in Eq <xref ref-type="disp-formula" rid="pcbi.1006370.e009">5C</xref>–<xref ref-type="disp-formula" rid="pcbi.1006370.e010">5D</xref>, Methods) representing the minimal assumption that noise variance occurs at higher frequencies than process variance; in other words that environmental changes are slower than fluctuations due to noise. This means that the RML infers environmental volatility in a completely autonomous manner. Moreover, the RML can adapt learning rate in any kind of problem (e.g., binary, continuous), and finally, it integrates approximate Bayesian optimization with other cognitive functions, like effort control and higher-order conditioning.</p>
<p>Interestingly, also Wilson et al. [<xref ref-type="bibr" rid="pcbi.1006370.ref075">75</xref>] proposed an approximate Bayesian estimator that is based on PE, without the need of specifying a forward model of environmental statistical structure. However, the authors provided a solution for one subclass of volatility estimation problems (the change-point problems) and also in this case, an a priori (fixed) parameter providing information about volatility (the process variance) was needed.</p>
</sec>
</sec>
<sec id="sec035">
<title>Experimental predictions</title>
<p>The flexibility of RML, and the explicit neurophysiological hypotheses on which it is based, allow several experimental predictions. In this paper we aimed at presenting the general potential and the theoretical value of the RML, comparing, in a qualitative fashion, the results from our simulations with experimental data from many different domains. A larger use of quantitative approaches to test the experimental predictions derivable from the RML (e.g. model-based data analysis) will be necessary in future work.</p>
<p>Here we list some potential experiments deriving from RML predictions. The first three are sufficiently specific to potentially falsify the model (at least in its neurophysiological interpretation), the others are currently formulated as working hypotheses.</p>
<p>First, the RML architecture suggests that PE signals are generated by the dACC and then converge toward the brainstem nuclei. This hypothesis implies that dACC lesion disrupts DA dynamics in higher-order conditioning, with a consequent impairment in higher-order instrumental conditioning; further, dACC lesion should disrupt LC dynamics related to learning rate control, with a consequent impairment of behavioural flexibility optimization.</p>
<p>A second prediction concerns the mechanisms subtending higher-order conditioning and the difference between classical and instrumental paradigms. In the RML, higher-order conditioning is possible only when the agent plays an active role in learning (i.e., instrumental conditioning). We predict that hijacking the dACC decision of boosting catecholamines (e.g., via optogenetic intervention) would make possible higher-order conditioning in classical conditioning paradigms (ref. simulations 3a-b).</p>
<p>Third, the DA-lesioned RML shows stronger dACC activation during an easy task (without effort) in the presence of a high reward (see Simulation 2a, <xref ref-type="fig" rid="pcbi.1006370.g004">Fig 4B</xref>). This finding can be interpreted as a compensatory phenomenon allowing to avoid apathy (i.e. refusal to engage in the task) if a small effort can make available a big reward. This is an explicit experimental prediction that could be tested both in animal paradigms and in mesolimbic DA impaired humans [<xref ref-type="bibr" rid="pcbi.1006370.ref076">76</xref>], or in patients with Parkinson’s disease on and off medication [<xref ref-type="bibr" rid="pcbi.1006370.ref051">51</xref>], therefore providing also possible translational implications.</p>
<p>Fourth, as shown above, the model provides a promising platform for investigating the pathogenesis of several psychiatric disorders. In a previous computational work, we proposed how motivational and decision-making problems in attention-deficit/hyperactivity disorder (ADHD) could originate from disrupted DA signals to the dACC [<xref ref-type="bibr" rid="pcbi.1006370.ref077">77</xref>]. In the current paper, we also simulated a deficit related to cognitive effort (Simulation 2c) in case of DA deficit. Together, these findings suggest how DA deficit can cause both motivational and cognitive impairment in ADHD, with an explicit prediction on how DA deficit can impair also NE dynamics [<xref ref-type="bibr" rid="pcbi.1006370.ref078">78</xref>] in ADHD. This prediction could be tested by measuring performance and LC activation during decision-making or working memory tasks, while specifically modulating DA transmission in both patients (via pharmacological manipulation) and RML.</p>
<p>Fifth, another clinical application concerns a recent theory on autism spectrum disorder (ASD) pathogenesis. Recent studies [<xref ref-type="bibr" rid="pcbi.1006370.ref079">79</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref080">80</xref>] proposed that a substantial number of ASD symptoms could be explained by dysfunctional control of learning rate and overestimation of environment volatility. This qualitative hypothesis could be easily implemented and explored quantitatively by altering meta-learning mechanisms in the RML leading to chronically high learning rate and LC activation.</p>
</sec>
<sec id="sec036">
<title>Limitations</title>
<p>The RML framework has three main limitations. First, in the RML DA plays a role only in learning. As with any other neuromodulator, experimental results suggest a less clear-cut picture, with DA being involved also in performance directly (e.g. attention and WM via DLPFC modulation) [<xref ref-type="bibr" rid="pcbi.1006370.ref039">39</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref081">81</xref>–<xref ref-type="bibr" rid="pcbi.1006370.ref083">83</xref>]. The goal of our simplified characterization of DA function was to elucidate how the two neuromodulators can influence each other for learning (DA) and performance (NE). Moreover, other theories stress the importance of direct (and hierarchically organized) interaction between the medial prefrontal cortex and the DLPFC in cognitive control [<xref ref-type="bibr" rid="pcbi.1006370.ref084">84</xref>] and WM function [<xref ref-type="bibr" rid="pcbi.1006370.ref068">68</xref>]. From this perspective, reduced DA signal to the dACC could directly disrupt the dACC-DLPFC interaction, impairing cognitive control and WM without the involvement of the NE modulation. dACC-DLPFC interaction is a neglected aspect in our model that should be investigated in future works (see next section).</p>
<p>The second limitation is the separation of the LC functions of learning rate modulation (<italic>λ</italic>) and cognitive control exertion. The cost of this separation between these two functions is outweighed by stable approximate optimal control of learning rate and catecholamines boosting policy. It must be stressed that the ACC<sub>Boost</sub> module receives the LC signal <italic>λ</italic> related to learning rate in any case, making the boosting policy adaptive to environmental changes.</p>
<p>Third, the RML reacts to environmental changes by learning rate modulation, while human and nonhuman primates can use specific events that occurred (episodic control [<xref ref-type="bibr" rid="pcbi.1006370.ref085">85</xref>]), to trigger policy change for adapting to novel situations. There is also converging evidence that primate dACC (and most likely its homologous area in rats) is critical to perform this type of higher-order inference (see [<xref ref-type="bibr" rid="pcbi.1006370.ref007">7</xref>] for a short review), and that LC bursts could work as circuit breakers to reset ongoing neural representations and trigger behavioural adaptation driven by episodic control [<xref ref-type="bibr" rid="pcbi.1006370.ref086">86</xref>]. The lack of contribution by episodic knowledge in behavioural optimization is clearly a limitation of our model, especially if we consider that episodic control can also optimize motivational signals to modulate cognitive effort [<xref ref-type="bibr" rid="pcbi.1006370.ref084">84</xref>]. We believe that these two adaptive processes (i.e. learning rate control and episodic control) are complementary and run in parallel and that their integration (a possibly arbitration on influencing behaviour) should receive future theoretical investigation.</p>
</sec>
<sec id="sec037">
<title>Future perspectives</title>
<p>The RML shows how meta-learning involving three interconnected neuro-cognitive domains can account for the flexibility of the mammalian brain. However, our model is not meant to cover all aspects of meta-learning. Many other decision-making dimensions may be optimized by meta-learned too. One obvious candidate is the stochasticity (temperature) of the decision process [<xref ref-type="bibr" rid="pcbi.1006370.ref087">87</xref>], which arbitrates the exploration/exploitation trade-off. We recently proposed that this parameter is similarly meta-learned trading off effort costs versus rewards [<xref ref-type="bibr" rid="pcbi.1006370.ref006">6</xref>]. It must be noted that experimental findings indicated a link between LC activation and the arbitration on exploration/exploitation trade-off [<xref ref-type="bibr" rid="pcbi.1006370.ref088">88</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref089">89</xref>], suggesting that the same mechanism used for learning rate optimization could be extended also to this domain. Other aspects from the classical RL modeling framework include discounting rate or eligibility traces [<xref ref-type="bibr" rid="pcbi.1006370.ref090">90</xref>]; future work should investigate the computational and biological underpinnings of their optimization. Moreover, considering the strong empirical evidence attributing to the dACC a prominent role in foraging (e.g. [<xref ref-type="bibr" rid="pcbi.1006370.ref091">91</xref>]), future work should focus on how the RML can also face this class of problems, where it is studied not only how mammals optimize choices within a task, but also how they decide when it is convenient to switch to another task, to maximize reward in the long run.</p>
<p>Given the exceptionally extended dACC connectivity [<xref ref-type="bibr" rid="pcbi.1006370.ref012">12</xref>], other brain areas are likely relevant for the implementation of decision making in more complex settings. For example, we only considered model-free dynamics in RL and decision-making. However, both humans and nonhuman animals can rely also on complex environment models to improve learning and decision making (e.g. spatial maps for navigation or declarative rules about environment features). In this respect, future work should particularly focus on dACC-DLPFC-hippocampus interactions [<xref ref-type="bibr" rid="pcbi.1006370.ref092">92</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref093">93</xref>], in order to investigate how environment models can modulate reward expectations, how the nervous system can represent and learn decision tree navigation [<xref ref-type="bibr" rid="pcbi.1006370.ref094">94</xref>] and how reward expectations can modulate goal-directed DLPFC representations [<xref ref-type="bibr" rid="pcbi.1006370.ref084">84</xref>].</p>
<p>Another anatomo-functional aspect that could be investigated concerns the anatomical segregation of the twofold dACC function we described here (dACC<sub>Act</sub> and dACC<sub>Boost</sub>). Although we remain agnostic about this question, it would be interesting to investigate whether the neural units performing these two types of decision-making operations are overlapping, intermixed, or even segregated in different dACC sectors.</p>
<p>Finally, the RML can work in continuous time and in the presence of noise. These features are crucial to make a model survive outside the simplified environment of trial-level simulations, and allow simulating behaviour in the real world, like, for example, in robotic platforms. RML embodiment into robotic platforms could be useful for both neuroscience and robotics. Indeed, testing our model outside the simplified environment of computer simulations could reveal model weaknesses that are otherwise hidden. Moreover, closing the loop between decision-making, body and environment [<xref ref-type="bibr" rid="pcbi.1006370.ref095">95</xref>] is important to have a complete theory on the biological and computational basis of decision-making. At the same time, the RML could suggest new perspectives on natural-like flexibility in machine learning, helping, for example, in optimizing plasticity as a function of environmental changes.</p>
</sec>
</sec>
<sec id="sec038" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec039">
<title>Model description</title>
<p>RML architecture was implemented in two versions: a discrete model (simulating inter-trial dynamics) and a dynamical model (a dynamical system simulating also intra-trial dynamics). Both implementations share the same architecture displayed in <xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1</xref>, and follow the same computational principles. All the results reported above were obtained with the dynamical model. Here we introduce the mathematical form of the discrete model, which provides a clearer and more compact RML description. All the simulations (with exception of Simulation 2c, which requires intra-trial dynamics) were replicated with the discrete model (Figures S9-S12 in <xref ref-type="supplementary-material" rid="pcbi.1006370.s002">S2 File</xref>), demonstrating that the computational principles founding the RML are independent from specific implementations. We used a single set of parameters across all simulations both for the discrete model (<xref ref-type="table" rid="pcbi.1006370.t001">Table 1</xref>) and for the dynamical model (Table A in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>). Parameters were hand-tuned to ensure acceptable performance in a simple 2-armed bandit task and second-order conditioning task.</p>
<table-wrap id="pcbi.1006370.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.t001</object-id>
<label>Table 1</label> <caption><title>Parameters list and values for discrete model.</title></caption>
<alternatives>
<graphic id="pcbi.1006370.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Parameter</th>
<th align="center">Value</th>
<th align="center">Meaning</th>
<th align="center">Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>ρ</italic></td>
<td align="center">0.2</td>
<td align="center">TD-learning signal decay</td>
<td align="center">6a</td>
</tr>
<tr>
<td align="center"><italic>μ</italic></td>
<td align="center">0.1</td>
<td align="center">DA dynamics</td>
<td align="center">6a</td>
</tr>
<tr>
<td align="center"><italic>τ</italic></td>
<td align="center">0.6</td>
<td align="center">Softmax temperature</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center"><italic>α</italic></td>
<td align="center">0.3</td>
<td align="center">Kalman filtering meta-parameter</td>
<td align="center">5c-d</td>
</tr>
<tr>
<td align="center"><italic>β</italic></td>
<td align="center">0.2</td>
<td align="center">Learning rate lower bound</td>
<td align="center">5a</td>
</tr>
<tr>
<td align="center"><italic>ω</italic></td>
<td align="center">0.15</td>
<td align="center">Boosting cost</td>
<td align="center">6b</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The mathematical description of the dynamical model can be found in the <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>.</p>
<p>We designed the model such that communication with the external environment is based on 9 channels (<xref ref-type="fig" rid="pcbi.1006370.g009">Fig 9A</xref>). Six channels represent environmental states (<italic>s</italic>) and RML actions (<italic>a</italic>) (3 states and 3 actions). The first two actions are aimed at changing the environmental state (e.g. turning right or left), while the 3<sup>rd</sup> action means “Stay”, i.e. refusing to engage in the task. There are two other input channels, one dedicated to reward from environment (<italic>RW</italic>) and the other to signal costs of motor actions (<italic>C</italic>). Finally, there is one output channel conveying norepinephrine (<italic>NE</italic>) signals to other brain areas. The RML is scalable by design, i.e. there is no theoretical limit to the number of state/action channels, and neither the number of parameters nor their values changes as a function of task type/complexity.</p>
<fig id="pcbi.1006370.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006370.g009</object-id>
<label>Fig 9</label>
<caption>
<title>RML overview with equations.</title>
<p><bold>a)</bold> The RML-environment interaction happens through nine channels of information exchange (black arrows) (input = empty bars; output = filled bars). The input channels consist of one channel encoding action costs (<italic>C</italic>), three channels encoding environmental states (<italic>s</italic>), and one channel encoding primary rewards (<italic>RW</italic>). The output consists of three channels coding each for one specific action (<italic>a</italic>), plus one channel conveying LC signals to other brain areas (<italic>NE</italic>). The entire model is composed of four reciprocally connected modules (each in a different color). The upper modules (blue and green) simulate the dACC, while the lower modules (red and orange) simulate the brainstem catecholamine nuclei (VTA and LC). dACC<sub>Act</sub> selects actions directed toward the environment and learns through first and higher-order conditioning, while dACC<sub>Boost</sub> modulates catecholamine nuclei output. The VTA module provides <italic>DA</italic> training signals to both dACC modules. The LC controls learning rate (<italic>λ</italic>; yellow bidirectional arrow) in both dACC modules, and effort exertion (promoting effortful actions) in the dACC<sub>Act</sub> module (orange arrow), influencing their decisions. Finally, the LC signal controlling effort in the dACC<sub>Act</sub> can be directed also toward other cognitive modules for neuro-modulation. <bold>b)</bold> Model overview with equations embedded. The equations are reported in their discrete form. Communication between modules is represented by arrows, with corresponding variables near each arrow. Variables δ and δ<sub><italic>B</italic></sub> represent the prediction errors from respectively Eqs <xref ref-type="disp-formula" rid="pcbi.1006370.e001">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006370.e003">3</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.g009" xlink:type="simple"/>
</fig>
<sec id="sec040">
<title>dACC<sub>Act</sub></title>
<p>The dACC<sub>Act</sub> module consists in a Q-learning algorithm augmented by meta-learning functions (<xref ref-type="fig" rid="pcbi.1006370.g009">Fig 9A and 9B</xref>, blue box). Here we refer to the performance monitoring part of the dACC<sub>Act</sub> module as “Critic”, while to the action selection part as “Actor”. The Critic is a performance evaluator and computes reward expectation and PE for either primary or non-primary rewards (higher-order conditioning), learning to associate stimuli and actions to environmental outcomes. The Actor selects motor actions (based on Critic expectation) to maximize long-term reward.</p>
<p>The central equation in this module governs Critic state/action value updates:
<disp-formula id="pcbi.1006370.e001">
<alternatives>
<graphic id="pcbi.1006370.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>v</italic>(<italic>s</italic>,<italic>a</italic>) indicates the value (outcome prediction) of a specific action <italic>a</italic> given a state <italic>s</italic>. <xref ref-type="disp-formula" rid="pcbi.1006370.e001">Eq 1</xref> ensures that <italic>v</italic> comes to resemble the environmental outcome encoded by dopaminergic signal (<italic>DA</italic>), which is generated by the VTA module (<xref ref-type="fig" rid="pcbi.1006370.g009">Fig 9B</xref>; Eq 6). It entails that the update of <italic>v</italic> at trial <italic>t</italic> is based on the difference between prediction (<italic>v</italic>) and outcome (<italic>DA</italic>), which defines the concept of PE. The latter is weighted by learning rate <italic>λ</italic> (called also step-size parameter in RL terminology), making the update more (high <italic>λ</italic>) or less (low <italic>λ</italic>) dependent on recent events. We propose that <italic>λ</italic> itself is modulated by the LC based on <italic>v</italic> and PE signals from the dACC<sub>Act</sub> (<xref ref-type="disp-formula" rid="pcbi.1006370.e006">Eq 5A</xref>).</p>
<p>The <italic>DA</italic> signal, afferent from the VTA, conveys either primary or non-primary reward (higher-order conditioning) and is modulated by the dACC<sub>Boost</sub> module via parameter <italic>b</italic> (<xref ref-type="disp-formula" rid="pcbi.1006370.e011">Eq 6A</xref>). It is worth noting that the rate of value changing described in <xref ref-type="disp-formula" rid="pcbi.1006370.e001">Eq 1</xref> depends obviously on <italic>λ</italic>, but also on PE. The latter depends on <italic>DA</italic> magnitude, and thence on the modulation that dACC<sub>Boost</sub> exerts over the VTA module. For this reason, we can say that the overall rate of learning (i.e. <italic>Δv</italic>) depends on both NE (controlling <italic>λ</italic>) and DA (determining PE) modulations.</p>
<p>Action <italic>a</italic> is selected by the Actor subsystem, which implements action selection (by softmax selection function, with temperature τ) based on state/action values discounted by state/action costs <italic>C</italic>:
<disp-formula id="pcbi.1006370.e002">
<alternatives>
<graphic id="pcbi.1006370.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where we define softmax(<italic>x</italic><sub><italic>i</italic></sub>,<italic>τ</italic>) = exp(<italic>x</italic><sub><italic>i</italic></sub>/<italic>τ</italic>)/∑exp(<italic>x</italic><sub><italic>i</italic></sub>/<italic>τ</italic>). Function <italic>C</italic> assigns a cost to each state/action couple, for example energy depletion consequent to climbing an obstacle. <italic>C</italic> is modulated by norepinephrine afferents from LC (<italic>NE</italic>), which is itself controlled by the dACC<sub>Boost</sub> module, via parameter <italic>b</italic> (cf. also Holroyd and McClure, 2015) <italic>NE</italic> levels discount <italic>C</italic>, lowering the perceived costs and energizing behaviour. We remind the reader that the RML can choose not to engage in the task (“Stay”); this option has <italic>C</italic> = 0. In this way, a high level of <italic>NE</italic> energizes behaviour, promoting both high cost actions and reducing the probability that the RML chooses to “Stay”.</p>
<p>The dynamical form of these equations is described in the <italic>dACC</italic><sub><italic>Act</italic></sub><italic>-VTA system</italic> paragraph in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>.</p>
</sec>
<sec id="sec041">
<title>dACC<sub>Boost</sub></title>
<p>The dACC<sub>Boost</sub> module is an Actor-Critic system that learns only from primary rewards (<xref ref-type="fig" rid="pcbi.1006370.g009">Fig 9</xref>, green box). This module controls the parameters for cost and reward signals in Eqs <xref ref-type="disp-formula" rid="pcbi.1006370.e001">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006370.e002">2</xref> (dACC<sub>Act</sub>), via modulation of VTA and LC activity (boosting catecholamines). In other words, whereas the dACC<sub>Act</sub> decides on actions toward the external environment, the dACC<sub>Boost</sub> decides on actions toward the internal environment: It modulates brainstem nuclei (VTA and LC), given a specific environmental state. This is implemented by selecting the modulatory signal <italic>b</italic> (<italic>boost signal</italic>), by RL-based decision-making. In our model, <italic>b</italic> is a discrete signal that can assume ten different values (integers 1–10), each corresponding to one action selectable by the dACC<sub>Boost</sub>. The Critic submodule inside the dACC<sub>Boost</sub> updates the boost values <italic>v</italic><sub><italic>B</italic></sub>(<italic>s</italic>, <italic>b</italic>), via the equation:
<disp-formula id="pcbi.1006370.e003">
<alternatives>
<graphic id="pcbi.1006370.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
<xref ref-type="disp-formula" rid="pcbi.1006370.e003">Eq 3</xref> represents the value update of boosting level <italic>b</italic> in the environmental state <italic>s</italic>. The dACC<sub>Boost</sub> module receives dopaminergic outcome signals (<italic>DA</italic><sub><italic>B</italic></sub>) from the VTA module. As described in <xref ref-type="disp-formula" rid="pcbi.1006370.e012">Eq 6B</xref>, <italic>DA</italic><sub><italic>B</italic></sub> represent the reward signal discounted by the cost of boosting catecholamines [<xref ref-type="bibr" rid="pcbi.1006370.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref096">96</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref097">97</xref>]. Also in <xref ref-type="disp-formula" rid="pcbi.1006370.e003">Eq 3</xref> there is a dynamic learning rate (<italic>λ</italic><sub><italic>B</italic></sub>), estimated by <xref ref-type="disp-formula" rid="pcbi.1006370.e006">Eq 5A</xref> in the LC. The Actor submodule selects boosting actions based on expected values <italic>v</italic><sub><italic>B</italic></sub> and temperature <italic>τ</italic>:
<disp-formula id="pcbi.1006370.e004">
<alternatives>
<graphic id="pcbi.1006370.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>Referring to <xref ref-type="disp-formula" rid="pcbi.1006370.e001">Eq 1</xref>, the dACC<sub>Boost</sub> modulates the reward signal by changing the <italic>DA</italic> signal coded in VTA (<xref ref-type="disp-formula" rid="pcbi.1006370.e011">Eq 6A</xref>). Furthermore, dACC<sub>Boost</sub> also modulates the cost signal by changing parameter <italic>NE</italic> (via LC module, see paragraph below) in the function representing action cost <italic>C</italic> (<xref ref-type="disp-formula" rid="pcbi.1006370.e002">Eq 2</xref>; represented in the Actor within the dACC<sub>Act</sub>). The dynamical form of these equations is described in the <italic>dACC</italic><sub><italic>Boost</italic></sub><italic>-LC-VTA system</italic> paragraph in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>.</p>
</sec>
<sec id="sec042">
<title>LC: Control over effort exertion and behavioural activation</title>
<p>The LC module plays a double role (<xref ref-type="fig" rid="pcbi.1006370.g009">Fig 9</xref>, orange box). First it controls cost via parameter <italic>Ne</italic>, as a function of boosting value <italic>b</italic> selected by the dACC<sub>Boost</sub> module. For sake of simplicity, we assumed <italic>NE</italic> = <italic>b</italic>; any monotonic function would have played a similar role. The <italic>NE</italic> signal is also directed toward external brain areas as a performance modulation signal (<xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1A</xref>; Simulation 2c).</p>
</sec>
<sec id="sec043">
<title>LC: Control over learning rate</title>
<p>The LC module also optimizes learning rate in the two dACC modules (∠ and <italic>λ</italic><sub><italic>B</italic></sub>). Approximate optimization of <italic>λ</italic> solves the trade-off between stability and plasticity, increasing learning speed when the environment changes and lowering it when the environment is simply noisy. In this way, the RML updates its knowledge when needed (plasticity), protecting it from random fluctuations. This function is performed by means of recurrent connections between the dACC (both modules) and the LC module, which controls learning rate based on the signals afferent from the dACC. The resulting algorithm approximates Kalman filtering [<xref ref-type="bibr" rid="pcbi.1006370.ref073">73</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref098">98</xref>], which is a recursive Bayesian estimator. In its simplest formulation, Kalman filter computes expectations (posteriors) from current estimates (priors) plus PE weighted by an adaptive learning rate (called Kalman gain). If we define process variance as the outcome variance due to volatility of the environment, Kalman filter computes the Kalman gain as the ratio between process variance and total variance (i.e. the sum of process and noise variance). From the Bayesian perspective, the Kalman gain reflects the confidence about priors, so that high values reflect low confidence in priors and more influence of evidence on posteriors estimation.</p>
<p>The main limitation of this and similar methods is that one must know a priori the model describing the environment statistical properties (noise and process variance). This information is typically inaccessible by biological or artificial agents, which perceive only the current state and outcome signals from the environment. Our LC module bypasses this problem by an approximation based on the information afferent from the dACC, without knowing a priori neither process nor noise variance. To do that, the LC modulates <italic>λ</italic> (or <italic>λ</italic><sub><italic>B</italic></sub>) as a function of the ratio between the estimated variance of state/action-value <inline-formula id="pcbi.1006370.e005"><alternatives><graphic id="pcbi.1006370.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mover accent="true"><mml:mi mathvariant="normal">a</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> over the estimated squared PE (<italic>δ</italic><sup>2</sup>):
<disp-formula id="pcbi.1006370.e006">
<alternatives>
<graphic id="pcbi.1006370.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mover accent="true"><mml:mi mathvariant="normal">a</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">r</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(5A)</label>
</disp-formula>
with <italic>β ≤ λ ≤</italic> 1 (<italic>β</italic> is a free parameter indicating the minimal learning rate), to ensure numerical stability.</p>
<p>The process variance is given by:
<disp-formula id="pcbi.1006370.e007">
<alternatives>
<graphic id="pcbi.1006370.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mover accent="true"><mml:mi mathvariant="normal">a</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(5B)</label>
</disp-formula>
where <inline-formula id="pcbi.1006370.e008"><alternatives><graphic id="pcbi.1006370.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the estimate of <italic>v</italic>, obtained by low-pass filtering tuned by meta-parameter <italic>α</italic>:
<disp-formula id="pcbi.1006370.e009">
<alternatives>
<graphic id="pcbi.1006370.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(5C)</label>
</disp-formula>
The same low-pass filter is applied to the PE signal (<italic>δ</italic>) to obtain a running estimation of total variance <italic>δ</italic><sup>2</sup>, which corresponds to the squared estimate of unsigned PE:
<disp-formula id="pcbi.1006370.e010">
<alternatives>
<graphic id="pcbi.1006370.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(5D)</label>
</disp-formula></p>
<p>In summary, in Eqs <xref ref-type="disp-formula" rid="pcbi.1006370.e006">5A</xref>–<xref ref-type="disp-formula" rid="pcbi.1006370.e010">5D</xref> Kalman gain is approximated using 3 components: reward expectation (<italic>v</italic>), PE signals (<italic>δ</italic>) (both afferent from the dACC modules) and a meta-parameter (<italic>α</italic>), defining the low-pass filter to estimate process and total variance. The meta-parameter <italic>α</italic> represents the minimal assumption that noise-related variability occurs at a faster time scale than volatility-related variability. Eqs <xref ref-type="disp-formula" rid="pcbi.1006370.e006">5A</xref>–<xref ref-type="disp-formula" rid="pcbi.1006370.e010">5D</xref> are implemented independently for each of the two dACC modules, so that each Critic interacts with the LC to modulate its own learning rate. The dACC modules and the LC play complementary roles in controlling <italic>λ</italic>: The dACC modules provide the LC with the time course of expectations and PEs occurring during a task, while the LC integrates them to compute <xref ref-type="disp-formula" rid="pcbi.1006370.e006">Eq 5A</xref>.</p>
<p>The dynamical form of these equations is described in the <italic>dACC-LC system</italic> paragraph in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>.</p>
</sec>
<sec id="sec044">
<title>VTA</title>
<p>The VTA provides training signal <italic>DA</italic> to both dACC modules, either for action selection directed toward the environment (by dACC<sub>Act</sub>) or for boosting-level selection (by dACC<sub>Boost</sub>) directed to the brainstem catecholamine nuclei (<xref ref-type="fig" rid="pcbi.1006370.g009">Fig 9</xref>, red box). The VTA module also learns to link dopamine signals to arbitrary environmental stimuli (non-primary rewards) to allow higher-order conditioning. We hypothesize that this mechanism is based on DA shifting from primary reward onset to conditioned stimulus (<italic>s</italic>, <italic>a</italic>, or both) onset [<xref ref-type="bibr" rid="pcbi.1006370.ref099">99</xref>].
<disp-formula id="pcbi.1006370.e011">
<alternatives>
<graphic id="pcbi.1006370.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>ρ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(6A)</label>
</disp-formula>
<xref ref-type="disp-formula" rid="pcbi.1006370.e011">Eq 6A</xref> represents the modulated (by <italic>b</italic>) reward signal. Here, <italic>r</italic> is a binary variable indicating the presence of reward signal, and <italic>R</italic> is a real number variable indicating reward magnitude. Parameter <italic>ρ</italic> is the TD discount factor, while parameter <italic>μ</italic> is a scaling factor distributing the modulation <italic>b</italic> between primary (first term of the equation) and non-primary (second term) reward. It is worth noting that when <italic>μ</italic> = 0, <xref ref-type="disp-formula" rid="pcbi.1006370.e011">Eq 6A</xref> simplifies to a Q-learning reward signal.</p>
<p>The VTA signal directed toward the dACC<sub>Boost</sub> is described by the following equation:
<disp-formula id="pcbi.1006370.e012">
<alternatives>
<graphic id="pcbi.1006370.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006370.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>ω</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(6B)</label>
</disp-formula>
where <italic>ω</italic> is a parameter defining the cost of catecholamine boosting [<xref ref-type="bibr" rid="pcbi.1006370.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref096">96</xref>,<xref ref-type="bibr" rid="pcbi.1006370.ref097">97</xref>]. In summary, boosting up DA by <italic>b</italic> (<xref ref-type="disp-formula" rid="pcbi.1006370.e011">Eq 6A</xref>), can improve behavioural performance (as shown in simulations below) but it also represents a cost (<xref ref-type="disp-formula" rid="pcbi.1006370.e012">Eq 6B</xref>). The dACC<sub>Boost</sub> module finds the optimal solution for this trade-off, choosing the optimal DA level to maximize performance while minimizing costs (for a formal analysis about this optimization process we refer to Verguts et al., 2015). The dynamical form of these equations is described in the <italic>dACC</italic><sub><italic>Act</italic></sub><italic>-VTA</italic> and <italic>dACC</italic><sub><italic>Boost</italic></sub><italic>-VTA</italic> paragraphs in <xref ref-type="supplementary-material" rid="pcbi.1006370.s001">S1 File</xref>.</p>
</sec>
<sec id="sec045">
<title>Control over other brain areas</title>
<p>Finally, the RML can optimize performance of other brain areas via its plug-in loop. It does so via the LC-based control signal (<italic>NE</italic>), which is the same signal that modulates effort (<xref ref-type="disp-formula" rid="pcbi.1006370.e002">Eq 2</xref>; <xref ref-type="fig" rid="pcbi.1006370.g001">Fig 1A</xref>). Indeed, the Actor-Critic function of the dACC<sub>Act</sub> module is domain-independent (i.e. the state/action channels can come from any brain area outside dACC), and this allows a dialogue with other areas. Moreover, because optimization of any brain area improves behavioural performance, the dACC<sub>Boost</sub> can modulate (via LC signals) any cortical area to improve performance (see Simulation 2c).</p>
</sec>
</sec>
</sec>
<sec id="sec046">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006370.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Supplementary methods.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006370.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006370.s002" xlink:type="simple">
<label>S2 File</label>
<caption>
<title>Supplementary results.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>Thanks are due to Tim Behrens, Clay Holroyd, Gianluca Baldassarre, Daniele Caligiore, Giovanni Pezzulo, and Domenico Maisto for useful comments on this project.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006370.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rushworth</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>. <article-title>Choice, uncertainty and value in prefrontal and cingulate cortex</article-title>. <source>Nat Neurosci</source>. <year>2008</year>;<volume>11</volume>: <fpage>389</fpage>–<lpage>397</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn2066" xlink:type="simple">10.1038/nn2066</ext-link></comment> <object-id pub-id-type="pmid">18368045</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Seeberger</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>R C</given-names></name>. <article-title>By carrot or by stick: cognitive reinforcement learning in parkinsonism</article-title>. <source>Science</source> (80-). <year>2004</year>;<volume>306</volume>: <fpage>1940</fpage>–<lpage>1943</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Alexander</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>JW</given-names></name>. <article-title>From conflict management to reward-based decision making: Actors and critics in primate medial frontal cortex</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2014</year>;<volume>46</volume>: <fpage>44</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2013.11.003" xlink:type="simple">10.1016/j.neubiorev.2013.11.003</ext-link></comment> <object-id pub-id-type="pmid">24239852</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MF</given-names></name>. <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>1214</fpage>–<lpage>1221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1954" xlink:type="simple">10.1038/nn1954</ext-link></comment> <object-id pub-id-type="pmid">17676057</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shenhav</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>The expected value of control: an integrative theory of anterior cingulate cortex function</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>: <fpage>217</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.07.007" xlink:type="simple">10.1016/j.neuron.2013.07.007</ext-link></comment> <object-id pub-id-type="pmid">23889930</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Vassena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>. <article-title>Adaptive effort investment in cognitive and physical tasks: a neurocomputational model</article-title>. <source>Front Behav Neurosci. Frontiers Media SA</source>; <year>2015</year>;<volume>9</volume>: <fpage>57</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbeh.2015.00057" xlink:type="simple">10.3389/fnbeh.2015.00057</ext-link></comment> <object-id pub-id-type="pmid">25805978</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kolling</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wittmann</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Boorman</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Mars</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>. <article-title>Value, search, persistence and model updating in anterior cingulate cortex</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>: <fpage>1280</fpage>–<lpage>1285</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4382" xlink:type="simple">10.1038/nn.4382</ext-link></comment> <object-id pub-id-type="pmid">27669988</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ebitz</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Hayden</surname> <given-names>BY</given-names></name>. <article-title>Dorsal anterior cingulate: a Rorschach test for cognitive neuroscience</article-title>. <source>Nat Neurosci</source>. Nature Publishing Group; <year>2016</year>;<volume>19</volume>: <fpage>1278</fpage>–<lpage>1279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4387" xlink:type="simple">10.1038/nn.4387</ext-link></comment> <object-id pub-id-type="pmid">27669987</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vassena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Deraeve</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Alexander</surname> <given-names>W</given-names></name>. <article-title>Predicting motivation: computational models of PFC can explain neural coding of motivation and effort-based decision-making in health and disease</article-title>. <source>J Cogn</source>. <year>2017</year>;</mixed-citation></ref>
<ref id="pcbi.1006370.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vassena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Holroyd</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Alexander</surname> <given-names>WH</given-names></name>. <article-title>Computational models of anterior cingulate cortex: At the crossroads between prediction and effort</article-title>. <source>Front Neurosci</source>. <year>2017</year>;<volume>11</volume>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>Reinforcement learning: an introduction</chapter-title>. <publisher-loc>Cambridge (MA)</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Devinsky</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Morrell</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Vogt</surname> <given-names>BA</given-names></name>. <article-title>Contributions of anterior cingulate cortex to behaviour</article-title>. <source>Brain</source>. <year>1995</year>;<volume>118</volume> (<issue>Pt 1</issue>: <fpage>279</fpage>–<lpage>306</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Margulies</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Kelly</surname> <given-names>AMC</given-names></name>, <name name-style="western"><surname>Uddin</surname> <given-names>LQ</given-names></name>, <name name-style="western"><surname>Biswal</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Castellanos</surname> <given-names>FX</given-names></name>, <name name-style="western"><surname>Milham</surname> <given-names>MP</given-names></name>. <article-title>Mapping the functional connectivity of anterior cingulate cortex</article-title>. <source>Neuroimage</source>. <year>2007</year>;<volume>37</volume>: <fpage>579</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2007.05.019" xlink:type="simple">10.1016/j.neuroimage.2007.05.019</ext-link></comment> <object-id pub-id-type="pmid">17604651</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gariano</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Groves</surname> <given-names>PM</given-names></name>. <article-title>Burst firing induced in midbrain dopamine neurons by stimulation of the medial prefrontal and anterior cingulate cortices</article-title>. <source>Brain Res</source>. <year>1988</year>;<volume>462</volume>: <fpage>194</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">3179734</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samuels</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Szabadi</surname> <given-names>E</given-names></name>. <article-title>Functional neuroanatomy of the noradrenergic locus coeruleus: its roles in the regulation of arousal and autonomic function part I: principles of functional organisation</article-title>. <source>Curr Neuropharmacol</source>. <year>2008</year>;<volume>6</volume>: <fpage>235</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2174/157015908785777229" xlink:type="simple">10.2174/157015908785777229</ext-link></comment> <object-id pub-id-type="pmid">19506723</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jodo</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Chiang</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Aston-Jones</surname> <given-names>G</given-names></name>. <article-title>Potent excitatory influence of prefrontal cortex activity on noradrenergic locus coeruleus neurons</article-title>. <source>Neuroscience</source>. <year>1998</year>;<volume>83</volume>: <fpage>63</fpage>–<lpage>79</lpage>. <object-id pub-id-type="pmid">9466399</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnsten</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Selective prefrontal cortical projections to the region of the locus coeruleus and raphe nuclei in the rhesus monkey</article-title>. <source>Brain Res</source>. <year>1984</year>;<volume>306</volume>: <fpage>9</fpage>–<lpage>18</lpage>. <object-id pub-id-type="pmid">6466989</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Köhler</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bär</surname> <given-names>K-J</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>G</given-names></name>. <article-title>Differential involvement of brainstem noradrenergic and midbrain dopaminergic nuclei in cognitive control</article-title>. <source>Hum Brain Mapp</source>. <year>2016</year>;<volume>37</volume>: <fpage>2305</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.23173" xlink:type="simple">10.1002/hbm.23173</ext-link></comment> <object-id pub-id-type="pmid">26970351</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vassena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Boehler</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Achten</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Fias</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>Overlapping Neural Systems Represent Cognitive Effort and Reward Anticipation</article-title>. <name name-style="western"><surname>Maurits</surname> <given-names>NM</given-names></name>, editor. <source>PLoS One</source>. Public Library of Science; <year>2014</year>;<volume>9</volume>: <fpage>e91008</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0091008" xlink:type="simple">10.1371/journal.pone.0091008</ext-link></comment> <object-id pub-id-type="pmid">24608867</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engström</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Landtblom</surname> <given-names>A-M</given-names></name>, <name name-style="western"><surname>Karlsson</surname> <given-names>T</given-names></name>. <article-title>Brain and effort: brain activation and effort-related working memory in healthy participants and patients with working memory deficits</article-title>. <source>Front Hum Neurosci</source>. <year>2013</year>;<volume>7</volume>: <fpage>140</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2013.00140" xlink:type="simple">10.3389/fnhum.2013.00140</ext-link></comment> <object-id pub-id-type="pmid">23616756</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Seurinck</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>Value and prediction error estimation account for volatility effects in ACC: A model-based fMRI study</article-title>. <source>Cortex</source>. <year>2013</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2012.05.008" xlink:type="simple">10.1016/j.cortex.2012.05.008</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006370.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Seurinck</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>Value and prediction error in medial frontal cortex: integrating the single-unit and systems levels of analysis</article-title>. <source>Front Hum Neurosci</source>. <year>2011</year>;<volume>5</volume>: <fpage>75</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2011.00075" xlink:type="simple">10.3389/fnhum.2011.00075</ext-link></comment> <object-id pub-id-type="pmid">21886616</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>. <article-title>Learning to Predict by the Method of Temporal Differences</article-title>. <source>Mach Learn</source>. <year>1988</year>;<volume>3</volume>: <fpage>9</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holroyd</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>McClure</surname> <given-names>SM</given-names></name>. <article-title>Hierarchical control over effortful behavior by rodent medial frontal cortex: A computational model</article-title>. <source>Psychol Rev</source>. <year>2015</year>;<volume>122</volume>: <fpage>54</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0038339" xlink:type="simple">10.1037/a0038339</ext-link></comment> <object-id pub-id-type="pmid">25437491</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source> (80-). <year>1997</year>;<volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pan</surname> <given-names>W-X</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Hyland</surname> <given-names>BI</given-names></name>. <article-title>Dopamine cells respond to predicted events during classical conditioning: evidence for eligibility traces in the reward-learning network</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>6235</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1478-05.2005" xlink:type="simple">10.1523/JNEUROSCI.1478-05.2005</ext-link></comment> <object-id pub-id-type="pmid">15987953</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mirolli</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Baldassarre</surname> <given-names>G</given-names></name>. <article-title>Intrinsically motivated learning in natural and artificial systems</article-title>. <source>Intrinsically Motiv Learn Nat Artif</source>. <year>2013</year>;</mixed-citation></ref>
<ref id="pcbi.1006370.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alexander</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>JW</given-names></name>. <article-title>Medial prefrontal cortex as an action-outcome predictor</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>: <fpage>1338</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2921" xlink:type="simple">10.1038/nn.2921</ext-link></comment> <object-id pub-id-type="pmid">21926982</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>AJ</given-names></name>. <article-title>Adaptive Behavior: Humans Act as Bayesian Learners</article-title>. <source>Curr Biol</source>. <year>2007</year>;<volume>17</volume>: <fpage>R977</fpage>–<lpage>R980</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2007.09.007" xlink:type="simple">10.1016/j.cub.2007.09.007</ext-link></comment> <object-id pub-id-type="pmid">18029257</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Seurinck</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>van Bochove</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>The influence of the noradrenergic system on optimal control of neural plasticity</article-title>. <source>Front Behav Neurosci</source>. <year>2013</year>;in press: 160. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbeh.2013.00160" xlink:type="simple">10.3389/fnbeh.2013.00160</ext-link></comment> <object-id pub-id-type="pmid">24312028</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty, neuromodulation, and attention</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>: <fpage>681</fpage>–<lpage>692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.04.026" xlink:type="simple">10.1016/j.neuron.2005.04.026</ext-link></comment> <object-id pub-id-type="pmid">15944135</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Rumsey</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Parikh</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Heasly</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>1040</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3130" xlink:type="simple">10.1038/nn.3130</ext-link></comment> <object-id pub-id-type="pmid">22660479</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jepma</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Rangel-Gomez</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Meeter</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nieuwenhuis</surname> <given-names>S</given-names></name>. <article-title>Catecholaminergic Regulation of Learning Rate in a Dynamic Environment</article-title>. <name name-style="western"><surname>O’Reilly</surname> <given-names>JX</given-names></name>, editor. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>: <fpage>e1005171</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005171" xlink:type="simple">10.1371/journal.pcbi.1005171</ext-link></comment> <object-id pub-id-type="pmid">27792728</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joshi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kalwani</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>Relationships between Pupil Diameter and Neuronal Activity in the Locus Coeruleus, Colliculi, and Cingulate Cortex</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>: <fpage>221</fpage>–<lpage>234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.11.028" xlink:type="simple">10.1016/j.neuron.2015.11.028</ext-link></comment> <object-id pub-id-type="pmid">26711118</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Varazzani</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>San-Galli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gilardeau</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bouret</surname> <given-names>S</given-names></name>. <article-title>Noradrenaline and dopamine neurons in the reward/effort trade-off: a direct electrophysiological comparison in behaving monkeys</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>7866</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0454-15.2015" xlink:type="simple">10.1523/JNEUROSCI.0454-15.2015</ext-link></comment> <object-id pub-id-type="pmid">25995472</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aston-Jones</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>Adaptive gain and the role of the locus coeruleus-norepinephrine system in optimal performance</article-title>. <source>J Comp Neurol</source>. <year>2005</year>;<volume>493</volume>: <fpage>99</fpage>–<lpage>110</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/cne.20723" xlink:type="simple">10.1002/cne.20723</ext-link></comment> <object-id pub-id-type="pmid">16254995</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aston-Jones</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance</article-title>. <source>Annu Rev Neurosci</source>. <year>2005</year>;<volume>28</volume>: <fpage>403</fpage>–<lpage>450</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.28.061604.135709" xlink:type="simple">10.1146/annurev.neuro.28.061604.135709</ext-link></comment> <object-id pub-id-type="pmid">16022602</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sara</surname> <given-names>SJ</given-names></name>. <article-title>The locus coeruleus and noradrenergic modulation of cognition</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year>;<volume>10</volume>: <fpage>211</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2573" xlink:type="simple">10.1038/nrn2573</ext-link></comment> <object-id pub-id-type="pmid">19190638</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vijayraghavan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Birnbaum</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>G V</given-names></name>, <name name-style="western"><surname>Arnsten</surname> <given-names>AF</given-names></name>. <article-title>Inverted-U dopamine D1 receptor actions on prefrontal neurons engaged in working memory</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>376</fpage>–<lpage>384</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1846" xlink:type="simple">10.1038/nn1846</ext-link></comment> <object-id pub-id-type="pmid">17277774</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langner</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Eickhoff</surname> <given-names>SB</given-names></name>. <article-title>Sustaining attention to simple tasks: a meta-analytic review of the neural mechanisms of vigilant attention</article-title>. <source>Psychol Bull</source>. <year>2013</year>;<volume>139</volume>: <fpage>870</fpage>–<lpage>900</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0030694" xlink:type="simple">10.1037/a0030694</ext-link></comment> <object-id pub-id-type="pmid">23163491</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>D’Esposito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Postle</surname> <given-names>BR</given-names></name>. <article-title>The Cognitive Neuroscience of Working Memory</article-title>. <source>Annu Rev Psychol</source>. <year>2015</year>;<volume>66</volume>: <fpage>115</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-psych-010814-015031" xlink:type="simple">10.1146/annurev-psych-010814-015031</ext-link></comment> <object-id pub-id-type="pmid">25251486</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Metalearning and neuromodulation</article-title>. <source>Neural Netw</source>. <year>2002</year>;<volume>15</volume>: <fpage>495</fpage>–<lpage>506</lpage>. <object-id pub-id-type="pmid">12371507</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Tonic dopamine: opportunity costs and the control of response vigor</article-title>. <source>Psychopharmacology (Berl)</source>. Springer-Verlag; <year>2007</year>;<volume>191</volume>: <fpage>507</fpage>–<lpage>520</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00213-006-0502-4" xlink:type="simple">10.1007/s00213-006-0502-4</ext-link></comment> <object-id pub-id-type="pmid">17031711</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chong</surname> <given-names>TT-J</given-names></name>, <name name-style="western"><surname>Apps</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Giehl</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Sillence</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Grima</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Husain</surname> <given-names>M</given-names></name>. <article-title>Neurocomputational mechanisms underlying subjective valuation of effort costs. Seymour B, editor</article-title>. <source>PLOS Biol</source>. Public Library of Science; <year>2017</year>;<volume>15</volume>: <fpage>e1002598</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002598" xlink:type="simple">10.1371/journal.pbio.1002598</ext-link></comment> <object-id pub-id-type="pmid">28234892</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurniawan</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Guitart-Masip</surname> <given-names>M</given-names></name>, <article-title>… PD-J of, 2013 U. Effort and valuation in the brain: the effects of anticipation and execution</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>6160</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4777-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4777-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23554497</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salamone</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Cousins</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Bucher</surname> <given-names>S</given-names></name>. <article-title>Anhedonia or anergia? Effects of haloperidol and nucleus accumbens dopamine depletion on instrumental response selection in a T-maze cost/benefit procedure</article-title>. <source>Behav Brain Res</source>. <year>1994</year>;<volume>65</volume>: <fpage>221</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">7718155</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Groves</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Jennings</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Croxson</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Sharp</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>, <etal>et al</etal>. <article-title>Comparing the role of the anterior cingulate cortex and 6-hydroxydopamine nucleus accumbens lesions on operant effort-based decision making</article-title>. <source>Eur J Neurosci</source>. <year>2009</year>;<volume>29</volume>: <fpage>1678</fpage>–<lpage>1691</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2009.06726.x" xlink:type="simple">10.1111/j.1460-9568.2009.06726.x</ext-link></comment> <object-id pub-id-type="pmid">19385990</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skvortsova</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Palminteri</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pessiglione</surname> <given-names>M</given-names></name>. <article-title>Learning To Minimize Efforts versus Maximizing Rewards: Computational Principles and Neural Correlates</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>15621</fpage>–<lpage>15630</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1350-14.2014" xlink:type="simple">10.1523/JNEUROSCI.1350-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25411490</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kennerley</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>. <article-title>Double dissociation of value computations in orbitofrontal and anterior cingulate neurons</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>: <fpage>1581</fpage>–<lpage>1589</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2961" xlink:type="simple">10.1038/nn.2961</ext-link></comment> <object-id pub-id-type="pmid">22037498</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Apps</surname> <given-names>MAJ</given-names></name>, <name name-style="western"><surname>Ramnani</surname> <given-names>N</given-names></name>. <article-title>The Anterior Cingulate Gyrus Signals the Net Value of Others’ Rewards</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>6190</fpage>–<lpage>6200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2701-13.2014" xlink:type="simple">10.1523/JNEUROSCI.2701-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24790190</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pessiglione</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vinckier</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bouret</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Le Bouc</surname> <given-names>R</given-names></name>. <article-title>Why not try harder? Computational approach to motivation deficits in neuro-psychiatric diseases</article-title>. <source>Brain</source>. <year>2018</year>;<volume>141</volume>: <fpage>629</fpage>–<lpage>650</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/aw" xlink:type="simple">10.1093/brain/aw</ext-link></comment>x278</mixed-citation></ref>
<ref id="pcbi.1006370.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hauber</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>S</given-names></name>. <article-title>Prefrontostriatal Circuitry Regulates Effort-Related Decision Making</article-title>. <source>Cereb Cortex</source>. Oxford University Press; <year>2009</year>;<volume>19</volume>: <fpage>2240</fpage>–<lpage>2247</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhn241" xlink:type="simple">10.1093/cercor/bhn241</ext-link></comment> <object-id pub-id-type="pmid">19131436</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parkinson</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Willoughby</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Everitt</surname> <given-names>BJ</given-names></name>. <article-title>Disconnection of the anterior cingulate cortex and nucleus accumbens core impairs Pavlovian approach behaviour</article-title>. <source>Behav Neurosci</source>. <year>2000</year>;<volume>114</volume>: <fpage>42</fpage>–<lpage>63</lpage>. <object-id pub-id-type="pmid">10718261</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Croxson</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>JX</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>. <article-title>Effort-based cost-benefit valuation and the human brain</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>: <fpage>4531</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4515-08.2009" xlink:type="simple">10.1523/JNEUROSCI.4515-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19357278</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klein-Flugge</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Kennerley</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bestmann</surname> <given-names>S</given-names></name>. <article-title>Neural Signatures of Value Comparison in Human Cingulate Cortex during Decisions Requiring an Effort-Reward Trade-off</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>10002</fpage>–<lpage>10015</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0292-16.2016" xlink:type="simple">10.1523/JNEUROSCI.0292-16.2016</ext-link></comment> <object-id pub-id-type="pmid">27683898</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ramos</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Paspalas</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Shu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Simen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Duque</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>α2A-Adrenoceptors Strengthen Working Memory Networks by Inhibiting cAMP-HCN Channel Signaling in Prefrontal Cortex</article-title>. <source>Cell</source>. <year>2007</year>;<volume>129</volume>: <fpage>397</fpage>–<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2007.03.015" xlink:type="simple">10.1016/j.cell.2007.03.015</ext-link></comment> <object-id pub-id-type="pmid">17448997</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Mao</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mei</surname> <given-names>ZT</given-names></name>. <article-title>Alpha-2 adrenergic modulation of prefrontal cortical neuronal activity related to spatial working memory in monkeys</article-title>. <source>Neuropsychopharmacology</source>. <year>1999</year>;<volume>21</volume>: <fpage>601</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-133X(99)00070-6" xlink:type="simple">10.1016/S0893-133X(99)00070-6</ext-link></comment> <object-id pub-id-type="pmid">10516956</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Mei</surname> <given-names>ZT</given-names></name>. <article-title>Delayed-response deficit induced by local injection of the alpha 2-adrenergic antagonist yohimbine into the dorsolateral prefrontal cortex in young adult monkeys</article-title>. <source>Behav Neural Biol</source>. <year>1994</year>;<volume>62</volume>: <fpage>134</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">7993303</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref059"><label>59</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name>. <chapter-title>Attention and effort</chapter-title>. <publisher-name>Prentice-Hall</publisher-name>; <year>1973</year>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borst</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>JR</given-names></name>. <article-title>Using model-based functional MRI to locate working memory updates and declarative memory retrievals in the fronto-parietal network</article-title>. <source>Proc Natl Acad Sci</source>. <year>2013</year>;<volume>110</volume>: <fpage>1628</fpage>–<lpage>1633</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1221572110" xlink:type="simple">10.1073/pnas.1221572110</ext-link></comment> <object-id pub-id-type="pmid">23319628</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name>, <name name-style="western"><surname>Ell</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Valentin</surname> <given-names>V V</given-names></name>., MB. <article-title>FROST: A Distributed Neurocomputational Model of Working Memory Maintenance</article-title>. <source>J Cogn Neurosci</source>. <year>2005</year>;<volume>17</volume>: <fpage>1728</fpage>–<lpage>1743</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892905774589271" xlink:type="simple">10.1162/089892905774589271</ext-link></comment> <object-id pub-id-type="pmid">16269109</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref062"><label>62</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Pierce</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Cheney</surname> <given-names>D</given-names></name>. <chapter-title>Behavior Analysis and Learning</chapter-title> <publisher-loc>New Jersey</publisher-loc>: <publisher-name>Laurence Erlbaum Associates</publisher-name>. <year>2004</year>;</mixed-citation></ref>
<ref id="pcbi.1006370.ref063"><label>63</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Denny</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ratner</surname> <given-names>S</given-names></name>. <chapter-title>Comparative psychology: Research in animal behavior</chapter-title>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Dorsey Press</publisher-name>; <year>1970</year>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holroyd</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>Coles</surname> <given-names>MG</given-names></name>. <article-title>The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity</article-title>. <source>Psychol Rev</source>. <year>2002</year>;<volume>109</volume>: <fpage>679</fpage>–<lpage>709</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.109.4.679" xlink:type="simple">10.1037/0033-295X.109.4.679</ext-link></comment> <object-id pub-id-type="pmid">12374324</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Dopamine, learning, and impulsivity: a biological account of attention-deficit/hyperactivity disorder</article-title>. <source>J Child Adolesc Psychopharmacol</source>. <year>2005</year>;<volume>15</volume>: <fpage>160</fpage>–<lpage>169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1089/cap.2005.15.160" xlink:type="simple">10.1089/cap.2005.15.160</ext-link></comment> <object-id pub-id-type="pmid">15910202</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Hazy</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Watz</surname> <given-names>B</given-names></name>. <article-title>PVLV: the primary value and learned value Pavlovian learning algorithm</article-title>. <source>Behav Neurosci</source>. <year>2007</year>;<volume>121</volume>: <fpage>31</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0735-7044.121.1.31" xlink:type="simple">10.1037/0735-7044.121.1.31</ext-link></comment> <object-id pub-id-type="pmid">17324049</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vassena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Holroyd</surname> <given-names>CCB</given-names></name>, <name name-style="western"><surname>Alexander</surname> <given-names>WH</given-names></name>. <article-title>Computational models of anterior cingulate cortex: At the crossroads between prediction and effort</article-title>. <source>Front Neurosci</source>. <year>2017</year>;<volume>11</volume>: <fpage>316</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2017.00316" xlink:type="simple">10.3389/fnins.2017.00316</ext-link></comment> <object-id pub-id-type="pmid">28634438</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alexander</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>JW</given-names></name>. <article-title>Hierarchical Error Representation: A Computational Model of Anterior Cingulate and Dorsolateral Prefrontal Cortex</article-title>. <source>Neural Comput</source>. <year>2015</year>;<volume>27</volume>: <fpage>2354</fpage>–<lpage>2410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00779" xlink:type="simple">10.1162/NECO_a_00779</ext-link></comment> <object-id pub-id-type="pmid">26378874</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>Binding by Random Bursts: A Computational Model of Cognitive Control</article-title>. <source>J Cogn Neurosci</source>. MIT PressOne Rogers Street, Cambridge, MA <email xlink:type="simple">02142-1209USAjournals-info@mit.edu</email>; <year>2017</year>;<volume>29</volume>: <fpage>1103</fpage>–<lpage>1118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn_a_01117" xlink:type="simple">10.1162/jocn_a_01117</ext-link></comment> <object-id pub-id-type="pmid">28253078</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Le Bouc</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rigoux</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Degos</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Welter</surname> <given-names>M-L</given-names></name>, <name name-style="western"><surname>Vidailhet</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Computational Dissection of Dopamine Motor and Motivational Functions in Humans</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>6623</fpage>–<lpage>6633</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3078-15.2016" xlink:type="simple">10.1523/JNEUROSCI.3078-15.2016</ext-link></comment> <object-id pub-id-type="pmid">27335396</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parvizi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rangarajan</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Shirer</surname> <given-names>WR</given-names></name>, <name name-style="western"><surname>Desai</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Greicius</surname> <given-names>MD</given-names></name>. <article-title>The will to persevere induced by electrical stimulation of the human cingulate gyrus</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>: <fpage>1359</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.10.057" xlink:type="simple">10.1016/j.neuron.2013.10.057</ext-link></comment> <object-id pub-id-type="pmid">24316296</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lallée</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Enel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Procyk</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Dominey</surname> <given-names>PF</given-names></name>. <article-title>Robot cognitive control with a neurophysiologically inspired reinforcement learning model</article-title>. <source>Front Neurorobot</source>. <year>2011</year>;<volume>5</volume>: <fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbot.2011.00001" xlink:type="simple">10.3389/fnbot.2011.00001</ext-link></comment> <object-id pub-id-type="pmid">21808619</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalman</surname> <given-names>R</given-names></name>. <article-title>A new approach to linear filtering and prediction problems</article-title>. <source>J basic Eng</source>. <year>1960</year>;</mixed-citation></ref>
<ref id="pcbi.1006370.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>. <article-title>A Bayesian foundation for individual learning under uncertainty</article-title>. <source>Front Hum Neurosci. Frontiers</source>; <year>2011</year>;<volume>5</volume>: <fpage>39</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2011.00039" xlink:type="simple">10.3389/fnhum.2011.00039</ext-link></comment> <object-id pub-id-type="pmid">21629826</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>A Mixture of Delta-Rules Approximation to Bayesian Inference in Change-Point Problems</article-title>. <name name-style="western"><surname>Behrens</surname> <given-names>T</given-names></name>, editor. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003150</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003150" xlink:type="simple">10.1371/journal.pcbi.1003150</ext-link></comment> <object-id pub-id-type="pmid">23935472</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Marco</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Venneri</surname> <given-names>A</given-names></name>. <article-title>Volume and Connectivity of the Ventral Tegmental Area are Linked to Neurocognitive Signatures of Alzheimer’s Disease in Humans</article-title>. <source>J Alzheimer’s Dis</source>. <year>2018</year>;<volume>63</volume>: <fpage>167</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3233/JAD-171018" xlink:type="simple">10.3233/JAD-171018</ext-link></comment> <object-id pub-id-type="pmid">29578486</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wiersema</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Sonuga-Barke</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>Deficient reinforcement learning in medial frontal cortex as a model of dopamine-related motivational deficits in ADHD</article-title>. <source>Neural Netw</source>. <year>2013</year>;<volume>46</volume>: <fpage>199</fpage>–<lpage>209</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2013.05.008" xlink:type="simple">10.1016/j.neunet.2013.05.008</ext-link></comment> <object-id pub-id-type="pmid">23811383</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hauser</surname> <given-names>TU</given-names></name>, <name name-style="western"><surname>Fiore</surname> <given-names>VG</given-names></name>, <name name-style="western"><surname>Moutoussis</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Computational Psychiatry of ADHD: Neural Gain Impairments across Marrian Levels of Analysis</article-title>. <source>Trends Neurosci</source>. <year>2016</year>;<volume>39</volume>: <fpage>63</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2015.12.009" xlink:type="simple">10.1016/j.tins.2015.12.009</ext-link></comment> <object-id pub-id-type="pmid">26787097</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van de Cruys</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Evers</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Van der Hallen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Van Eylen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Boets</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>de-Wit</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Precise minds in uncertain worlds: Predictive coding in autism</article-title>. <source>Psychol Rev</source>. <year>2014</year>;<volume>121</volume>: <fpage>649</fpage>–<lpage>675</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0037665" xlink:type="simple">10.1037/a0037665</ext-link></comment> <object-id pub-id-type="pmid">25347312</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lawson</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>. <article-title>Adults with autism overestimate the volatility of the sensory environment</article-title>. <source>Nat Neurosci</source>. <year>2017</year>;<volume>20</volume>: <fpage>1293</fpage>–<lpage>1299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4615" xlink:type="simple">10.1038/nn.4615</ext-link></comment> <object-id pub-id-type="pmid">28758996</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shiner</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wunderlich</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hill</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bhatia</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Dopamine and performance in a reinforcement learning task: evidence from Parkinson’s disease</article-title>. <source>Brain</source>. <year>2012</year>;<volume>135</volume>: <fpage>1871</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/aws083" xlink:type="simple">10.1093/brain/aws083</ext-link></comment> <object-id pub-id-type="pmid">22508958</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vijayraghavan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Selective D2 receptor actions on the functional circuitry of working memory</article-title>. <source>Science</source> (80-). <year>2004</year>;<volume>303</volume>: <fpage>853</fpage>–<lpage>856</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006370.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Opstal</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Van Laeken</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>van Dijck</surname> <given-names>J-P</given-names></name>, <name name-style="western"><surname>De Vos</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Goethals</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Correlation between individual differences in striatal dopamine and in visual consciousness</article-title>. <source>Curr Biol</source>. <year>2014</year>;<volume>24</volume>: <fpage>R265</fpage>–<lpage>R266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.02.001" xlink:type="simple">10.1016/j.cub.2014.02.001</ext-link></comment> <object-id pub-id-type="pmid">24698371</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kouneiher</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Charron</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Koechlin</surname> <given-names>E</given-names></name>. <article-title>Motivation and cognitive control in the human prefrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>: <fpage>939</fpage>–<lpage>945</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2321" xlink:type="simple">10.1038/nn.2321</ext-link></comment> <object-id pub-id-type="pmid">19503087</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koechlin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ody</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kouneiher</surname> <given-names>F</given-names></name>. <article-title>The Architecture of Cognitive Control in the Human Prefrontal Cortex</article-title>. <source>Science</source> (80-). <year>2003</year>;<volume>302</volume>: <fpage>1181</fpage>–<lpage>1185</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1088545" xlink:type="simple">10.1126/science.1088545</ext-link></comment> <object-id pub-id-type="pmid">14615530</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bouret</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sara</surname> <given-names>SJ</given-names></name>. <article-title>Network reset: a simplified overarching theory of locus coeruleus noradrenaline function</article-title>. <source>Trends Neurosci</source>. <year>2005</year>;<volume>28</volume>: <fpage>574</fpage>–<lpage>582</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2005.09.002" xlink:type="simple">10.1016/j.tins.2005.09.002</ext-link></comment> <object-id pub-id-type="pmid">16165227</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Quilodran</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Enel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dominey</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Procyk</surname> <given-names>E</given-names></name>. <article-title>Behavioral Regulation and the Modulation of Information Coding in the Lateral Prefrontal and Cingulate Cortex</article-title>. <source>Cereb Cortex</source>. <year>2015</year>;<volume>25</volume>: <fpage>3197</fpage>–<lpage>218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhu114" xlink:type="simple">10.1093/cercor/bhu114</ext-link></comment> <object-id pub-id-type="pmid">24904073</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jepma</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nieuwenhuis</surname> <given-names>S</given-names></name>. <article-title>Pupil Diameter Predicts Changes in the Exploration-Exploitation Tradeoff: Evidence for the Adaptive Gain Theory</article-title>. <source>J Cogn Neurosci</source>. <year>2011</year>;<volume>23</volume>: <fpage>1587</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2010.21548" xlink:type="simple">10.1162/jocn.2010.21548</ext-link></comment> <object-id pub-id-type="pmid">20666595</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tervo</surname> <given-names>DGR</given-names></name>, <name name-style="western"><surname>Proskurin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Manakov</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kabra</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vollmer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Branson</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Behavioral Variability through Stochastic Choice and Its Gating by Anterior Cingulate Cortex</article-title>. <source>Cell</source>. <year>2014</year>;<volume>159</volume>: <fpage>21</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2014.08.037" xlink:type="simple">10.1016/j.cell.2014.08.037</ext-link></comment> <object-id pub-id-type="pmid">25259917</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schweighofer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Meta-learning in reinforcement learning</article-title>. <source>Neural Netw</source>. <year>2003</year>;<volume>16</volume>: <fpage>5</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">12576101</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kolling</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Mars</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>. <article-title>Neural mechanisms of foraging</article-title>. <source>Science</source>. <year>2012</year>;<volume>336</volume>: <fpage>95</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1216930" xlink:type="simple">10.1126/science.1216930</ext-link></comment> <object-id pub-id-type="pmid">22491854</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stoll</surname> <given-names>FM</given-names></name>, <name name-style="western"><surname>Fontanier</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Procyk</surname> <given-names>E</given-names></name>. <article-title>Specific frontal neural dynamics contribute to decisions to check</article-title>. <source>Nat Commun</source>. <year>2016</year>;<volume>7</volume>: <fpage>11990</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms11990" xlink:type="simple">10.1038/ncomms11990</ext-link></comment> <object-id pub-id-type="pmid">27319361</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Womelsdorf</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ardid</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Everling</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Valiante</surname> <given-names>TA</given-names></name>. <article-title>Burst firing synchronizes prefrontal and anterior cingulate cortex during attentional control</article-title>. <source>Curr Biol</source>. <year>2014</year>;<volume>24</volume>: <fpage>2613</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.09.046" xlink:type="simple">10.1016/j.cub.2014.09.046</ext-link></comment> <object-id pub-id-type="pmid">25308081</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfeiffer</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name>. <article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title>. <source>Nature</source>. <year>2013</year>;<volume>497</volume>: <fpage>74</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12112" xlink:type="simple">10.1038/nature12112</ext-link></comment> <object-id pub-id-type="pmid">23594744</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Barsalou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cangelosi</surname> <given-names>A</given-names></name>. <article-title>The mechanics of embodiment: A dialog on embodiment and computational modeling</article-title>. <source>Embodied and</source>. <year>2011</year>;</mixed-citation></ref>
<ref id="pcbi.1006370.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>. <article-title>The intrinsic cost of cognitive control</article-title>. <source>Behav Brain Sci</source>. <year>2013</year>;<volume>36</volume>: <fpage>661</fpage>–<lpage>698</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X12003196" xlink:type="simple">10.1017/S0140525X12003196</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006370.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>McGuire</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Rosen</surname> <given-names>ZB</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Decision making and the avoidance of cognitive demand</article-title>. <source>J Exp Psychol Gen</source>. <year>2010</year>;<volume>139</volume>: <fpage>665</fpage>–<lpage>682</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0020198" xlink:type="simple">10.1037/a0020198</ext-link></comment> <object-id pub-id-type="pmid">20853993</object-id></mixed-citation></ref>
<ref id="pcbi.1006370.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Welch</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bishop</surname> <given-names>G</given-names></name>. <source>An introduction to the Kalman filter</source>. <year>1995</year>;</mixed-citation></ref>
<ref id="pcbi.1006370.ref099"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ljungberg</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Apicella</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Responses of monkey dopamine neurons during learning of behavioral reactions</article-title>. <source>J Neurophysiol</source>. <year>1992</year>;<volume>67</volume>: <fpage>145</fpage>–<lpage>163</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1992.67.1.145" xlink:type="simple">10.1152/jn.1992.67.1.145</ext-link></comment> <object-id pub-id-type="pmid">1552316</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>