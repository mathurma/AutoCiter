<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00354</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002211</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
      </article-categories><title-group><article-title>Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons</article-title><alt-title alt-title-type="running-head">Neural Dynamics as Sampling</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Buesing</surname>
            <given-names>Lars</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="fn" rid="fn1">
            <sup>¤</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Bill</surname>
            <given-names>Johannes</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Nessler</surname>
            <given-names>Bernhard</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Maass</surname>
            <given-names>Wolfgang</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Institute for Theoretical Computer Science, Graz University of Technology, Graz, Austria</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">lars@gatsby.ucl.ac.uk</email></corresp>
        <fn fn-type="current-aff" id="fn1">
          <label>¤</label>
          <p>Current address: Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: LB JB WM. Performed the experiments: JB. Analyzed the data: LB JB. Contributed reagents/materials/analysis tools: LB JB BN WM. Wrote the paper: LB JB BN WM. Designed network model and its theoretical analysis: LB.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>11</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>3</day>
        <month>11</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>11</issue><elocation-id>e1002211</elocation-id><history>
        <date date-type="received">
          <day>16</day>
          <month>3</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>10</day>
          <month>8</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Buesing et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>It is well-known that neurons communicate with short electric pulses, called action potentials or spikes. But how can spiking networks implement complex computations? Attempts to relate spiking network activity to results of deterministic computation steps, like the output bits of a processor in a digital computer, are conflicting with findings from cognitive science and neuroscience, the latter indicating the neural spike output in identical experiments changes from trial to trial, i.e., neurons are “unreliable”. Therefore, it has been recently proposed that neural activity should rather be regarded as <italic>samples</italic> from an underlying <italic>probability distribution</italic> over many variables which, e.g., represent a model of the external world incorporating prior knowledge, memories as well as sensory input. This hypothesis assumes that networks of stochastically spiking neurons are able to emulate powerful algorithms for reasoning in the face of uncertainty, i.e., to carry out probabilistic inference. In this work we propose a detailed neural network model that indeed fulfills these computational requirements and we relate the spiking dynamics of the network to concrete probabilistic computations. Our model suggests that neural systems are suitable to carry out probabilistic inference by using stochastic, rather than deterministic, computing elements.</p>
      </abstract><funding-group><funding-statement>This paper was written under partial support by the European Union project #FP7-237955 (FACETS-ITN), project #FP7-269921 (BrainScaleS), project #FP7-216593 (SECO), project #FP7-506778 (PASCAL2) and project #FP7-243914 (BRAIN-I-NETS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="22"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Attempts to understand the organization of computations in the brain from the perspective of traditional, mostly deterministic, models of computation, such as attractor neural networks or Turing machines, have run into problems: Experimental data suggests that neurons, synapses, and neural systems are inherently <italic>stochastic</italic> <xref ref-type="bibr" rid="pcbi.1002211-Rolls1">[1]</xref>, especially in vivo, and therefore seem less suitable for implementing deterministic computations. This holds for ion channels of neurons <xref ref-type="bibr" rid="pcbi.1002211-Cannon1">[2]</xref>, synaptic release <xref ref-type="bibr" rid="pcbi.1002211-Flight1">[3]</xref>, neural response to stimuli (trial-to-trial variability) <xref ref-type="bibr" rid="pcbi.1002211-Azouz1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-GerstnerW1">[5]</xref>, and perception <xref ref-type="bibr" rid="pcbi.1002211-Brascamp1">[6]</xref>. In fact, several experimental studies arrive at the conclusion that external stimuli only modulate the highly stochastic spontaneous firing activity of cortical networks of neurons <xref ref-type="bibr" rid="pcbi.1002211-Fiser1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Ringach1">[8]</xref>. Furthermore, traditional models for neural computation have been challenged by the fact that typical sensory data from the environment is often noisy and ambiguous, hence requiring neural systems to take <italic>uncertainty</italic> about external inputs into account. Therefore many researchers have suggested that information processing in the brain carries out probabilistic, rather than logical, inference for making decisions and choosing actions <xref ref-type="bibr" rid="pcbi.1002211-Geman1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Sadaghiani1">[22]</xref>. Probabilistic inference has emerged in the 1960’s <xref ref-type="bibr" rid="pcbi.1002211-Pearl1">[23]</xref>, as a principled mathematical framework for reasoning in the face of uncertainty with regard to observations, knowledge, and causal relationships, which is characteristic for real-world inference tasks. This framework has become tremendously successful in real-world applications of artificial intelligence and machine learning. A typical computation that needs to be carried out for probabilistic inference on a high-dimensional joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e001" xlink:type="simple"/></inline-formula> is the evaluation of the conditional distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e002" xlink:type="simple"/></inline-formula> (or marginals thereof) over some variables of interest, say <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e003" xlink:type="simple"/></inline-formula>, given variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e004" xlink:type="simple"/></inline-formula>. In the following, we will call the set of variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e005" xlink:type="simple"/></inline-formula>, which we condition on, the <italic>observed</italic> variables and denote it by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e006" xlink:type="simple"/></inline-formula>.</p>
      <p>Numerous studies in different areas of neuroscience and cognitive science have suggested that probabilistic inference could explain a variety of computational processes taking place in neural systems (see <xref ref-type="bibr" rid="pcbi.1002211-Rao1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Doya1">[11]</xref>). In models of perception the observed variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e007" xlink:type="simple"/></inline-formula> are interpreted as the sensory input to the central nervous system (or its early representation by the firing response of neurons, e.g., in the LGN in the case of vision), and the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e008" xlink:type="simple"/></inline-formula> model the interpretation of the sensory input, e.g., the texture and position of objects in the case of vision, which might be encoded in the response of neurons in various higher cortical areas <xref ref-type="bibr" rid="pcbi.1002211-Lee1">[15]</xref>. Furthermore, in models for motor control the observed variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e009" xlink:type="simple"/></inline-formula> often consist not only of sensory and proprioceptive inputs to the brain, but also of specific goals and constraints for a planned movement <xref ref-type="bibr" rid="pcbi.1002211-Friston1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Toussaint2">[26]</xref>, whereas inference is carried out over the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e010" xlink:type="simple"/></inline-formula> representing a motor plan or motor commands to muscles. Recent publications show that human reasoning and learning can also be cast into the form of probabilistic inference problems <xref ref-type="bibr" rid="pcbi.1002211-Tenenbaum1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Oaksford1">[29]</xref>. In these models learning of concepts, ranging from concrete to more abstract ones, is interpreted as inference in lower and successively higher levels of hierarchical probabilistic models, giving a consistent description of inductive learning within and across domains of knowledge.</p>
      <p>In spite of this active research on the functional level of neural processing, it turned out to be surprisingly hard to relate the computational machinery required for probabilistic inference to experimental data on neurons, synapses, and neural systems. There are mainly two different approaches for implementing the computational machinery for probabilistic inference in “neural hardware”. The first class of approaches builds on deterministic methods for evaluating exactly or approximately the desired conditional and/or marginal distributions, whereas the second class relies on sampling from the probability distributions in question. Multiple models in the class of deterministic approaches implement algorithms from machine learning called message passing or belief propagation <xref ref-type="bibr" rid="pcbi.1002211-Rao2">[30]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Litvak1">[33]</xref>. By clever reordering of sum and product operators occurring in the evaluation of the desired probabilities, the total number of computation steps are drastically reduced. The results of subcomputations are propagated as "messages" or "beliefs" that are sent to other parts of the computational network. Other deterministic approaches for representing distributions and performing inference are probabilistic population code (PPC) models <xref ref-type="bibr" rid="pcbi.1002211-Sahani1">[34]</xref>. Although deterministic approaches provide a theoretically sound hypothesis about how complex computations can possibly be embedded in neural networks and explain aspects of experimental data, it seems difficult (though not impossible) to conciliate them with other aspects of experimental evidence, such as stochasticity of spiking neurons, spontaneous firing, trial-to-trial variability, and perceptual multistability.</p>
      <p>Therefore other researchers (e.g., <xref ref-type="bibr" rid="pcbi.1002211-Hoyer1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Fiser2">[35]</xref>) have proposed to model computations in neural systems as probabilistic inference based on a different class of algorithms, which requires stochastic, rather than deterministic, computational units. This approach, commonly referred to as sampling, focuses on drawing <italic>samples</italic>, i.e., concrete values for the random variables that are distributed according to the desired probability distribution. Sampling can naturally capture the effect of apparent stochasticity in neural responses and seems to be furthermore consistent with multiple experimental effects reported in cognitive science literature <xref ref-type="bibr" rid="pcbi.1002211-Sundareswara1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref>. On the conceptual side, it has proved to be difficult to implement learning in message passing and PPC network models. In contrast, following the lines of <xref ref-type="bibr" rid="pcbi.1002211-Ackley1">[36]</xref>, the sampling approach might be well suited to incorporate learning.</p>
      <p>Previous network models that implement sampling in neural networks are mostly based on a special sampling algorithm called Gibbs (or general Metropolis-Hastings) sampling <xref ref-type="bibr" rid="pcbi.1002211-Geman1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Sundareswara1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Hinton1">[37]</xref>. The dynamics that arise from this approach, the so-called Glauber dynamics, however are only superficially similar to spiking neural dynamics observed in experiments, rendering these models rather abstract. Building on and extending previous models, we propose here a family of network models, that can be shown to exactly sample from any arbitrary member of a well-defined class of probability distributions via their inherent network dynamics. These dynamics incorporate refractory effects and finite durations of postsynaptic potentials (PSPs), and are therefore more biologically realistic than existing approaches. Formally speaking, our model implements Markov chain Monte Carlo (MCMC) sampling in a spiking neural network. In contrast to prior approaches however, our model incorporates irreversible dynamics (i.e., no detailed balance) allowing for finite time PSPs and refractory mechanisms. Furthermore, we also present a continuous time version of our network model. The resulting stochastic dynamical system can be shown to sample from the correct distribution. In general, continuous time models arguably provide a higher amount of biological realism compared to discrete time models.</p>
      <p>The paper is structured in the following way. First we provide a brief introduction to MCMC sampling. We then define the neural network model whose neural activity samples from a given class of probability distributions. The model will be first presented in discrete time together with some illustrative simulations. An extension of the model to networks of more detailed spiking neuron models which feature a relative refractory mechanism is presented. Furthermore, it is shown how the neural network model can also be formulated in continuous time. Finally, as a concrete simulation example we present a simple network model for perceptual multistability.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Recapitulation of MCMC sampling</title>
        <p>In machine learning, sampling is often considered the “gold standard” of inference methods, since, assuming that we can sample from the distribution in question, and assuming enough computational resources, any inference task can be carried out with arbitrary precision (in contrast to some deterministic approximate inference methods such as variational inference). However sampling from an arbitrary distribution can be a difficult problem in itself, as, e.g., many distributions can only be evaluated modulo a global constant (the partition function). In order to circumvent these problems, elaborate MCMC sampling techniques have been developed in machine learning and statistics <xref ref-type="bibr" rid="pcbi.1002211-Andrieu1">[38]</xref>. MCMC algorithms are based on the following idea: instead of producing an ad-hoc sample, a process that is heuristically comparable to a global search over the whole state space of the random variables, MCMC methods produce a new sample via a “local search” around a point in the state space that is already (approximately) a sample from the distribution.</p>
        <p>More formally, a Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e011" xlink:type="simple"/></inline-formula> (in discrete time) is defined by a set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e012" xlink:type="simple"/></inline-formula> of states (we consider for discrete time only the case where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e013" xlink:type="simple"/></inline-formula> has a finite size, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e014" xlink:type="simple"/></inline-formula>) together with a transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e015" xlink:type="simple"/></inline-formula>. The operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e016" xlink:type="simple"/></inline-formula> is a conditional probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e017" xlink:type="simple"/></inline-formula> over the next state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e018" xlink:type="simple"/></inline-formula> given a preceding state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e019" xlink:type="simple"/></inline-formula>. The Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e020" xlink:type="simple"/></inline-formula> is started in some initial state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e021" xlink:type="simple"/></inline-formula>, and moves through a trajectory of states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e022" xlink:type="simple"/></inline-formula> via iterated application of the stochastic transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e023" xlink:type="simple"/></inline-formula>. More precisely, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e024" xlink:type="simple"/></inline-formula> is the state at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e025" xlink:type="simple"/></inline-formula>, then the next state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e026" xlink:type="simple"/></inline-formula> is drawn from the conditional probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e027" xlink:type="simple"/></inline-formula>. An important theorem from probability theory (see, e.g., p. 232 in <xref ref-type="bibr" rid="pcbi.1002211-Grimmett1">[39]</xref>) states that if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e028" xlink:type="simple"/></inline-formula> is irreducible (i.e., any state in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e029" xlink:type="simple"/></inline-formula> can be reached from any other state in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e030" xlink:type="simple"/></inline-formula> in finitely many steps with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e031" xlink:type="simple"/></inline-formula>) and aperiodic (i.e., its state transitions cannot be trapped in deterministic cycles), then the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e032" xlink:type="simple"/></inline-formula> converges for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e033" xlink:type="simple"/></inline-formula> to a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e034" xlink:type="simple"/></inline-formula> that does not depend on the initial state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e035" xlink:type="simple"/></inline-formula>. This state distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e036" xlink:type="simple"/></inline-formula> is called the invariant distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e037" xlink:type="simple"/></inline-formula>. The irreducibility of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e038" xlink:type="simple"/></inline-formula> implies that it is the only distribution over the states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e039" xlink:type="simple"/></inline-formula> that is invariant under its transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e040" xlink:type="simple"/></inline-formula>, i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e041" xlink:type="simple"/><label>(1)</label></disp-formula></p>
        <p>Thus, in order to carry out probabilistic inference for a given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e042" xlink:type="simple"/></inline-formula>, it suffices to construct an irreducible and aperiodic Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e043" xlink:type="simple"/></inline-formula> that leaves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e044" xlink:type="simple"/></inline-formula> invariant, i.e., satisfies equation (1). Then one can answer numerous probabilistic inference questions regarding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e045" xlink:type="simple"/></inline-formula> without any numerical computations of probabilities. Rather, one plugs in the observed values for some of the random variables (RVs) and simply collects samples from the conditional distribution over the other RVs of interest when the Markov chain approaches its invariant distribution.</p>
        <p>A convenient and popular method for the construction of an operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e046" xlink:type="simple"/></inline-formula> for a given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e047" xlink:type="simple"/></inline-formula> is looking for operators <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e048" xlink:type="simple"/></inline-formula> that satisfy the following detailed balance condition,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e049" xlink:type="simple"/><label>(2)</label></disp-formula>for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e050" xlink:type="simple"/></inline-formula>. A Markov chain that satisfies (2) is said to be reversible. In particular, the Gibbs and Metropolis-Hastings algorithms employ reversible Markov chains. A very useful property of (2) is that it implies the invariance property (1), and this is in fact the standard method for proving (1). However, as our approach makes use of irreversible Markov chains as explained below, we will have to prove (1) directly.</p>
      </sec>
      <sec id="s2b">
        <title>Neural sampling</title>
        <p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e051" xlink:type="simple"/></inline-formula> be some arbitrary joint distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e052" xlink:type="simple"/></inline-formula> binary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e053" xlink:type="simple"/></inline-formula> that only takes on values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e054" xlink:type="simple"/></inline-formula>. We will show that under a certain computability assumption on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e055" xlink:type="simple"/></inline-formula> a network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e056" xlink:type="simple"/></inline-formula> consisting of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e057" xlink:type="simple"/></inline-formula> spiking neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e058" xlink:type="simple"/></inline-formula> can sample from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e059" xlink:type="simple"/></inline-formula> using its inherent stochastic dynamics. More precisely, we show that the stochastic firing activity of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e060" xlink:type="simple"/></inline-formula> can be viewed as a non-reversible Markov chain that samples from the given probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e061" xlink:type="simple"/></inline-formula>. If a subset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e062" xlink:type="simple"/></inline-formula> of the variables are observed, modelled as the corresponding neurons being “clamped” to the observed values, the remaining network samples from the conditional distribution of the remaining variables given the observables. Hence, this approach offers a quite natural implementation of probabilistic inference. It is similar to sampling approaches which have already been applied extensively, e.g., in Boltzmann machines, however our model is more biologically realistic as it incorporates aspects of the inherent temporal dynamics and spike-based communication of a network of spiking neurons. We call this approach <italic>neural sampling</italic> in the remainder of the paper.</p>
        <p>In order to enable a network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e063" xlink:type="simple"/></inline-formula> of spiking neurons to sample from a distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e064" xlink:type="simple"/></inline-formula> of binary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e065" xlink:type="simple"/></inline-formula>, one needs to specify how an assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e066" xlink:type="simple"/></inline-formula> of values to these binary variables can be represented by the spiking activity of the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e067" xlink:type="simple"/></inline-formula> and vice versa. A spike, or action potential, of a biological neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e068" xlink:type="simple"/></inline-formula> has a short duration of roughly <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e069" xlink:type="simple"/></inline-formula>. But the effect of such spike, both on the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e070" xlink:type="simple"/></inline-formula> itself (in the form of refractory processes) and on the membrane potential of other neurons (in the form of postsynaptic potentials) lasts substantially longer, on the order of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e071" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e072" xlink:type="simple"/></inline-formula>. In order to capture this temporally extended effect of each spike, we fix some parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e073" xlink:type="simple"/></inline-formula> that models the average duration of these temporally extended processes caused by a spike. We say that a binary vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e074" xlink:type="simple"/></inline-formula> is represented by the firing activity of the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e075" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e076" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e077" xlink:type="simple"/></inline-formula> iff:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e078" xlink:type="simple"/><label>(3)</label></disp-formula></p>
        <p>In other words, any spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e079" xlink:type="simple"/></inline-formula> sets the value of the associated binary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e080" xlink:type="simple"/></inline-formula> to 1 for a duration of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e081" xlink:type="simple"/></inline-formula>.</p>
        <p>An obvious consequence of this definition is that the binary vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e082" xlink:type="simple"/></inline-formula> that is defined by the activity of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e083" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e084" xlink:type="simple"/></inline-formula> does not fully capture the internal state of this stochastic system. Rather, one needs to take into account additional non-binary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e085" xlink:type="simple"/></inline-formula>, where the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e086" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e087" xlink:type="simple"/></inline-formula> specifies <italic>when</italic> within the time interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e088" xlink:type="simple"/></inline-formula> the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e089" xlink:type="simple"/></inline-formula> has fired (if it has fired within this time interval, thereby causing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e090" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e091" xlink:type="simple"/></inline-formula>). The neural sampling process has the Markov property only with regard to these more informative auxiliary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e092" xlink:type="simple"/></inline-formula>. Therefore our analysis of neural sampling will focus on the temporal evolution of these auxiliary variables. We adopt the convention that each spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e093" xlink:type="simple"/></inline-formula> sets the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e094" xlink:type="simple"/></inline-formula> to its maximal value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e095" xlink:type="simple"/></inline-formula>, from which it linearly decays back to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e096" xlink:type="simple"/></inline-formula> during the subsequent time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e097" xlink:type="simple"/></inline-formula>.</p>
        <p>For the construction of the sampling network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e098" xlink:type="simple"/></inline-formula>, we assume that the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e099" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e100" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e101" xlink:type="simple"/></inline-formula> equals the log-odds of the corresponding variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e102" xlink:type="simple"/></inline-formula> to be active, and refer to this property as <italic>neural computability condition</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e103" xlink:type="simple"/><label>(4)</label></disp-formula>where we write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e104" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e106" xlink:type="simple"/></inline-formula> for the current values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e107" xlink:type="simple"/></inline-formula> of all other variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e108" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e109" xlink:type="simple"/></inline-formula>. Under the assumption we make in equation (4), i.e., that the neural membrane potential reflects the log-odds of the corresponding variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e110" xlink:type="simple"/></inline-formula>, it is required that each single neuron in the network can actually compute the right-hand side of equation (4), i.e., that it fulfills the neural computability condition.</p>
        <p>A concrete class of probability distributions, that we will use as an example in the remainder, are Boltzmann distributions:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e111" xlink:type="simple"/><label>(5)</label></disp-formula>with arbitrary real valued parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e112" xlink:type="simple"/></inline-formula> which satisfy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e114" xlink:type="simple"/></inline-formula> (the constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e115" xlink:type="simple"/></inline-formula> ensures the normalization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e116" xlink:type="simple"/></inline-formula>). For the Boltzmann distribution, condition (4) is satisfied by neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e117" xlink:type="simple"/></inline-formula> with the standard membrane potential<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e118" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e119" xlink:type="simple"/></inline-formula> is the bias of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e120" xlink:type="simple"/></inline-formula> (which regulates its excitability), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e121" xlink:type="simple"/></inline-formula> is the strength of the synaptic connection from neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e122" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e123" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e124" xlink:type="simple"/></inline-formula> approximates the time course of the postsynaptic potential in neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e125" xlink:type="simple"/></inline-formula> caused by a firing of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e126" xlink:type="simple"/></inline-formula> with a constant signal of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e127" xlink:type="simple"/></inline-formula> (i.e., a square pulse). As we will describe below, spikes of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e128" xlink:type="simple"/></inline-formula> are evoked stochastically depending on the current membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e129" xlink:type="simple"/></inline-formula> and the auxiliary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e130" xlink:type="simple"/></inline-formula>.</p>
        <p>The neural computability condition (4) links classes of probability distributions to neuron and synapse models in a network of spiking neurons. As shown above, Boltzmann distributions satisfy the condition if one considers point neuron models which compute a linear weighted sum of the presynaptic inputs. The class of distributions can be extended to include more complex distributions using a method proposed in <xref ref-type="bibr" rid="pcbi.1002211-Nessler1">[40]</xref> which is based on the following idea. Neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e131" xlink:type="simple"/></inline-formula> representing the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e132" xlink:type="simple"/></inline-formula> is not directly influenced by the activities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e133" xlink:type="simple"/></inline-formula> of the presynaptic neurons, but via intermediate nonlinear preprocessing elements. This preprocessing might be implemented by dendrites or other (inter-) neurons and is assumed to compute nonlinear combinations of the presynaptic activities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e134" xlink:type="simple"/></inline-formula> (similar to a kernel). This allows the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e135" xlink:type="simple"/></inline-formula>, and therefore the log-odds ratio on the right-hand side of (4), to represent a more complex function of the activities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e136" xlink:type="simple"/></inline-formula>, giving rise to more complex joint distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e137" xlink:type="simple"/></inline-formula>. The concrete implementation of non-trivial directed and undirected graphical models with the help of preprocessing elements in the neural sampling framework is subject of current research. For the examples given in this study, we focus on the standard form of the membrane potential (6) of point neurons. As shown below, these spiking network models can emulate any Boltzmann machine (BM) <xref ref-type="bibr" rid="pcbi.1002211-Ackley1">[36]</xref>.</p>
        <p>A substantial amount of preceding studies has demonstrated that BMs are very powerful, and that the application of suitable learning algorithms for setting the weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e138" xlink:type="simple"/></inline-formula> makes it possible to learn and represent complex sensory processing tasks by such distributions <xref ref-type="bibr" rid="pcbi.1002211-Hinton1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Hinton2">[41]</xref>. In applications in statistics and machine learning using such Boltzmann distributions, sampling is typically implemented by Gibbs sampling or more general <italic>reversible</italic> MCMC methods. However, it is difficult to model some neural processes, such as an absolute refractory period or a postsynaptic potential (PSP) of fixed duration, using a reversible Markov chain, but they are more conveniently modelled using an irreversible one. As we wish to keep the computational power of BMs and at the same time to augment the sampling procedure with aspects of neural dynamics (such as PSPs with fixed durations, refractory mechanisms) to increase biological realism, we focus in the following on irreversible MCMC methods (keeping in mind that this might not be the only possible way to achieve these goals).</p>
      </sec>
      <sec id="s2c">
        <title>Neural sampling in discrete time</title>
        <p>Here we describe neural dynamics in discrete time with an absolute refractory period <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e139" xlink:type="simple"/></inline-formula>. We interpret one step of the Markov chain as a time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e140" xlink:type="simple"/></inline-formula> in biological real time. The dynamics of the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e141" xlink:type="simple"/></inline-formula>, that describes the time course of the effect of a spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e142" xlink:type="simple"/></inline-formula>, are defined in the following way. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e143" xlink:type="simple"/></inline-formula> is set to the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e144" xlink:type="simple"/></inline-formula> when neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e145" xlink:type="simple"/></inline-formula> fires, and decays by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e146" xlink:type="simple"/></inline-formula> at each subsequent discrete time step. The parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e147" xlink:type="simple"/></inline-formula> is chosen to be some integer, so that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e148" xlink:type="simple"/></inline-formula> decays back to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e149" xlink:type="simple"/></inline-formula> in exactly <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e150" xlink:type="simple"/></inline-formula> time steps. The neuron can only spike (with a probability that is a function of its current membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e151" xlink:type="simple"/></inline-formula>) if its variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e152" xlink:type="simple"/></inline-formula>. If however, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e153" xlink:type="simple"/></inline-formula>, the neuron is considered refractory and it cannot spike, but its <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e154" xlink:type="simple"/></inline-formula> is reduced by 1 per time step. To show that these simple dynamics do indeed sample from the given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e155" xlink:type="simple"/></inline-formula>, we proceed in the following way. We define a joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e156" xlink:type="simple"/></inline-formula> which has the desired marginal distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e157" xlink:type="simple"/></inline-formula>. Further we formalize the dynamics informally described above as a transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e158" xlink:type="simple"/></inline-formula> operating on the state vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e159" xlink:type="simple"/></inline-formula>. Finally, in the Methods section, we show that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e160" xlink:type="simple"/></inline-formula> is the unique invariant distribution of this operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e161" xlink:type="simple"/></inline-formula>, i.e., that the dynamics described by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e162" xlink:type="simple"/></inline-formula> produce samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e163" xlink:type="simple"/></inline-formula> from the desired distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e164" xlink:type="simple"/></inline-formula>. We refer to sampling through networks with this stochastic spiking mechanism as <italic>neural sampling with absolute refractory period</italic> due to the persistent refractory process.</p>
        <p>Given the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e165" xlink:type="simple"/></inline-formula> that we want to sample from, we define the following joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e166" xlink:type="simple"/></inline-formula> over the neural variables:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e167" xlink:type="simple"/><label>(7)</label></disp-formula></p>
        <p>This definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e168" xlink:type="simple"/></inline-formula> simply expresses that if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e169" xlink:type="simple"/></inline-formula>, then the auxiliary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e170" xlink:type="simple"/></inline-formula> can assume any value in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e171" xlink:type="simple"/></inline-formula> with equal probability. On the other hand <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e172" xlink:type="simple"/></inline-formula> necessarily assumes the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e173" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e174" xlink:type="simple"/></inline-formula> (i.e., when the neuron is in its resting state).</p>
        <p>The state transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e175" xlink:type="simple"/></inline-formula> can be defined in a transparent manner as a composition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e176" xlink:type="simple"/></inline-formula> transition operators, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e177" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e178" xlink:type="simple"/></inline-formula> only updates the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e179" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e180" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e181" xlink:type="simple"/></inline-formula>, i.e., the neurons are updated sequentially in the same order (this severe restriction will become obsolete in the case of continuous time discussed below). We define the composition as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e182" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e183" xlink:type="simple"/></inline-formula> is applied prior to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e184" xlink:type="simple"/></inline-formula>. The new values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e185" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e186" xlink:type="simple"/></inline-formula> only depend on the previous value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e187" xlink:type="simple"/></inline-formula> and on the current membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e188" xlink:type="simple"/></inline-formula>. The interesting dynamics take place in the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e189" xlink:type="simple"/></inline-formula>. They are illustrated in <xref ref-type="fig" rid="pcbi-1002211-g001">Figure 1</xref> where the arrows represent transition probabilities greater than 0.</p>
        <fig id="pcbi-1002211-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Neuron model with absolute refractory mechanism.</title>
            <p>The figure shows a schematic of the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e190" xlink:type="simple"/></inline-formula> for the internal state variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e191" xlink:type="simple"/></inline-formula> of a spiking neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e192" xlink:type="simple"/></inline-formula> with an absolute refractory period. The neuron can fire in the resting state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e193" xlink:type="simple"/></inline-formula> and in the last refractory state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e194" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g001" xlink:type="simple"/>
        </fig>
        <p>If the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e195" xlink:type="simple"/></inline-formula> is not refractory, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e196" xlink:type="simple"/></inline-formula>, it can spike (i.e., a transition from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e197" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e198" xlink:type="simple"/></inline-formula>) with probability<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e199" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e200" xlink:type="simple"/></inline-formula> is the standard sigmoidal activation function and the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e201" xlink:type="simple"/></inline-formula> denotes the natural logarithm. The term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e202" xlink:type="simple"/></inline-formula> is the current membrane potential, which depends on the current values of the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e203" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e204" xlink:type="simple"/></inline-formula>. The term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e205" xlink:type="simple"/></inline-formula> in (8) reflects the granularity of a chosen discrete time scale. If it is very fine (say one step equals one microsecond), then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e206" xlink:type="simple"/></inline-formula> is large, and the firing probability at each specific discrete time step is therefore reduced. If the neuron in a state with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e207" xlink:type="simple"/></inline-formula> does not spike, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e208" xlink:type="simple"/></inline-formula> relaxes into the resting state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e209" xlink:type="simple"/></inline-formula> corresponding to a non-refractory neuron.</p>
        <p>If the neuron is in a refractory state, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e210" xlink:type="simple"/></inline-formula>, its new variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e211" xlink:type="simple"/></inline-formula> assumes deterministically the next lower value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e212" xlink:type="simple"/></inline-formula>, reflecting the inherent temporal process:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e213" xlink:type="simple"/><label>(9)</label></disp-formula></p>
        <p>After the transition of the auxiliary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e214" xlink:type="simple"/></inline-formula>, the binary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e215" xlink:type="simple"/></inline-formula> is deterministically set to a consistent state, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e216" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e217" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e218" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e219" xlink:type="simple"/></inline-formula>.</p>
        <p>It can be shown that each of these stochastic state transition operators <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e220" xlink:type="simple"/></inline-formula> leaves the given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e221" xlink:type="simple"/></inline-formula> invariant, i.e., satisfies equation (1). This implies that any composition or mixture of these operators <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e222" xlink:type="simple"/></inline-formula> also leaves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e223" xlink:type="simple"/></inline-formula> invariant, see, e.g., <xref ref-type="bibr" rid="pcbi.1002211-Andrieu1">[38]</xref>. In particular, the composition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e224" xlink:type="simple"/></inline-formula> of these operators <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e225" xlink:type="simple"/></inline-formula> leaves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e226" xlink:type="simple"/></inline-formula> invariant, which has a quite natural interpretation as firing dynamics of the spiking neural network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e227" xlink:type="simple"/></inline-formula>: At each discrete time step the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e228" xlink:type="simple"/></inline-formula> are updated for all neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e229" xlink:type="simple"/></inline-formula>, where the update of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e230" xlink:type="simple"/></inline-formula> takes preceding updates for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e231" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e232" xlink:type="simple"/></inline-formula> into account. Alternatively, one could also choose at each discrete time step a different order for updates according to <xref ref-type="bibr" rid="pcbi.1002211-Andrieu1">[38]</xref>. The assumption of a well-regulated updating policy will be overcome in the continuous-time limit, i.e., in case where the neural dynamics are described as a Markov jump process. In the <xref ref-type="sec" rid="s4">methods</xref> section we prove the following central theorem:</p>
        <sec id="s2c1">
          <title>Theorem 1</title>
          <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e233" xlink:type="simple"/></inline-formula> <italic>is the unique invariant distribution of operator</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e234" xlink:type="simple"/></inline-formula>, <italic>i.e.,</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e235" xlink:type="simple"/></inline-formula> <italic>is aperiodic and irreducible and satisfies</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e236" xlink:type="simple"/><label>(10)</label></disp-formula></p>
          <p>The proof of this Theorem is provided by Lemmata 1 – 3 in the <xref ref-type="sec" rid="s4">Methods</xref> section. The statement that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e237" xlink:type="simple"/></inline-formula> (which is composed of the operators <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e238" xlink:type="simple"/></inline-formula>) is irreducible and aperiodic ensures that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e239" xlink:type="simple"/></inline-formula> is the <italic>unique</italic> invariant distribution of the Markov chain defined by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e240" xlink:type="simple"/></inline-formula>, i.e., that irrespective of the initial network state the successive application of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e241" xlink:type="simple"/></inline-formula> explores the whole state space in a non-periodic manner.</p>
          <p>This theorem guarantees that after a sufficient “burn-in” time (more precisely in the limit of an infinite “burn-in” time), the dynamics of the network, which are given by the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e242" xlink:type="simple"/></inline-formula>, produce samples from the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e243" xlink:type="simple"/></inline-formula>. As by construction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e244" xlink:type="simple"/></inline-formula>, the Markov chain provides samples from the given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e245" xlink:type="simple"/></inline-formula>. Furthermore, the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e246" xlink:type="simple"/></inline-formula> can carry out probabilistic inference for this distribution. For example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e247" xlink:type="simple"/></inline-formula> can be used to sample from the posterior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e248" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e249" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e250" xlink:type="simple"/></inline-formula>. One just needs to clamp those neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e251" xlink:type="simple"/></inline-formula> to the corresponding observed values. This could be implemented by injecting a strong positive (negative) current into the units with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e252" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e253" xlink:type="simple"/></inline-formula>). Then, as soon as the stochastic dynamics of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e254" xlink:type="simple"/></inline-formula> has converged to its invariant distribution, the averaged firing rate of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e255" xlink:type="simple"/></inline-formula> is proportional to the following desired marginal probability<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e256" xlink:type="simple"/></disp-formula></p>
          <p>In a biological neural system this result of probabilistic inference could for example be read out by an integrator neuron that counts spikes from this neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e257" xlink:type="simple"/></inline-formula> within a behaviorally relevant time window of a few hundred milliseconds, similarly as the experimentally reported integrator neurons in area LIP of monkey cortex <xref ref-type="bibr" rid="pcbi.1002211-Yang1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Gold1">[21]</xref>. Another readout neuron that receives spike input from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e258" xlink:type="simple"/></inline-formula> could at the same time estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e259" xlink:type="simple"/></inline-formula> for another RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e260" xlink:type="simple"/></inline-formula>. But valuable information for probabilistic inference is not only provided by firing rates or spike counts, but also by spike correlations of the neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e261" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e262" xlink:type="simple"/></inline-formula>. For example, the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e263" xlink:type="simple"/></inline-formula> can be estimated by a readout neuron that responds to superpositions of EPSPs caused by near-coincident firing of neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e264" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e265" xlink:type="simple"/></inline-formula> within a time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e266" xlink:type="simple"/></inline-formula>. Thus, a large number of different probabilistic inferences can be carried out efficiently in parallel by readout neurons that receive spike input from different subsets of neurons in the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e267" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s2c2">
          <title>Variation of the discrete time model with a relative refractory mechanism</title>
          <p>For the previously described simple neuron model, the refractory process was assumed to last for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e268" xlink:type="simple"/></inline-formula> time steps, exactly as long as the postsynaptic potentials caused by each spike. In this section we relax this assumption by introducing a more complex and biologically more realistic neuron model, where the duration of the refractory process is decoupled from the duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e269" xlink:type="simple"/></inline-formula> of a postsynaptic potential. Thus, this model can for example also fire bursts of spikes with an interspike interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e270" xlink:type="simple"/></inline-formula>. The introduction of this more complex neuron model comes at the price that one can no longer prove that a network of such neurons samples from the desired distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e271" xlink:type="simple"/></inline-formula>. Nevertheless, if the sigmoidal activation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e272" xlink:type="simple"/></inline-formula> is replaced by a different activation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e273" xlink:type="simple"/></inline-formula>, one can still prove that the sampling is “locally correct”, as specified in equation (12) below. Furthermore, our computer simulations suggest that also globally the error introduced by the more complex neuron model is not functionally significant, i.e. that statistical dependencies between the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e274" xlink:type="simple"/></inline-formula> are still faithfully captured.</p>
          <p>The neuron model with a relative refractory period is defined in the following way. Consider some arbitrary refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e275" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e276" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e277" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e278" xlink:type="simple"/></inline-formula>. The idea is that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e279" xlink:type="simple"/></inline-formula> models the readiness of the neuron to fire in its state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e280" xlink:type="simple"/></inline-formula>. This readiness has value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e281" xlink:type="simple"/></inline-formula> when the neuron has fired at the preceding time step (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e282" xlink:type="simple"/></inline-formula>), and assumes the resting state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e283" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e284" xlink:type="simple"/></inline-formula> has dropped to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e285" xlink:type="simple"/></inline-formula>. In between, the readiness may take on any non-negative value according to the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e286" xlink:type="simple"/></inline-formula>. The function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e287" xlink:type="simple"/></inline-formula> does not need to be monotonic, allowing for example that it increases to high values in between, yielding a preferred interspike interval of a oscillatory neuron. The firing probability of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e288" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e289" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e290" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e291" xlink:type="simple"/></inline-formula> is an appropriate function of the membrane potential as described below. Thus this function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e292" xlink:type="simple"/></inline-formula> is closely related to the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e293" xlink:type="simple"/></inline-formula> (called afterpotential) in the spike response model <xref ref-type="bibr" rid="pcbi.1002211-GerstnerW1">[5]</xref> as well as to the self-excitation kernel in Generalized Linear Models <xref ref-type="bibr" rid="pcbi.1002211-Pillow1">[42]</xref>. In general, different neurons in the network may have different refractory profiles, which can be modeled by a different refractory function for each neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e294" xlink:type="simple"/></inline-formula>. However for the sake of notational simplicity we assume a single refractory function in the following.</p>
          <p>In the presence of this refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e295" xlink:type="simple"/></inline-formula> one needs to replace the sigmoidal activation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e296" xlink:type="simple"/></inline-formula> by a suitable function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e297" xlink:type="simple"/></inline-formula> that satisfies the condition<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e298" xlink:type="simple"/><label>(11)</label></disp-formula>for all real numbers <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e299" xlink:type="simple"/></inline-formula>. This equation can be derived (see <xref ref-type="sec" rid="s4">Methods</xref> section Lemma 0) if one requires each neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e300" xlink:type="simple"/></inline-formula> to represent the correct distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e301" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e302" xlink:type="simple"/></inline-formula> conditioned the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e303" xlink:type="simple"/></inline-formula>. One can show that, for any <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e304" xlink:type="simple"/></inline-formula> as above, there always exists a continuous, monotonic function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e305" xlink:type="simple"/></inline-formula> which satisfies this equation (see Lemma 0 in <xref ref-type="sec" rid="s4">Methods</xref>). Unfortunately (11) cannot be solved analytically for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e306" xlink:type="simple"/></inline-formula> in general. Hence, for simulations we approximate the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e307" xlink:type="simple"/></inline-formula> for a given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e308" xlink:type="simple"/></inline-formula> by numerically solving (11) on a grid and interpolating between the grid points with a constant function. Examples for several functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e309" xlink:type="simple"/></inline-formula> and the associated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e310" xlink:type="simple"/></inline-formula> are shown in <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2B</xref> and <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2C</xref> respectively. Furthermore, spike trains emitted by single neurons with these refractory functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e311" xlink:type="simple"/></inline-formula> and the corresponding functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e312" xlink:type="simple"/></inline-formula> are shown in <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2D</xref> for the case of piecewise constant membrane potentials. This figure indicates, that functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e313" xlink:type="simple"/></inline-formula> that define a shorter refractory effect lead to higher firing rates and more irregular firing. It is worth noticing that the standard activation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e314" xlink:type="simple"/></inline-formula> is the solution of equation (11) for the absolute refractory function, i.e., for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e315" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e316" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e317" xlink:type="simple"/></inline-formula>.</p>
          <fig id="pcbi-1002211-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Neuron model with relative refractory mechanism.</title>
              <p>The figure shows the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e318" xlink:type="simple"/></inline-formula>, refractory functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e319" xlink:type="simple"/></inline-formula> and activation functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e320" xlink:type="simple"/></inline-formula> for the neuron model with relative refractory mechanism. (A) Transition probabilities of the internal variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e321" xlink:type="simple"/></inline-formula> given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e322" xlink:type="simple"/></inline-formula>. (B) Three examples of possible refractory functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e323" xlink:type="simple"/></inline-formula>. They assume value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e324" xlink:type="simple"/></inline-formula> when the neuron cannot spike, and return to value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e325" xlink:type="simple"/></inline-formula> (full readiness to fire again) with different time courses. The value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e326" xlink:type="simple"/></inline-formula> at intermediate time points regulates the current probability of firing of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e327" xlink:type="simple"/></inline-formula> (see A). The x-axis is equivalent to the number of time steps since last spike (running from 0 to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e328" xlink:type="simple"/></inline-formula> from left to right). (C) Associated activation functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e329" xlink:type="simple"/></inline-formula> according to (11). (D) Spike trains produced by the resulting three different neuron models with (hypothetical) membrane potentials that jump at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e330" xlink:type="simple"/></inline-formula> from a constant low value to a constant high value. Black horizontal bars indicate spikes, and the active states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e331" xlink:type="simple"/></inline-formula> are indicated by gray shaded areas of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e332" xlink:type="simple"/></inline-formula> after each spike. It can be seen from this example that different refractory mechanisms give rise to different spiking dynamics.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g002" xlink:type="simple"/>
          </fig>
          <p>The transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e333" xlink:type="simple"/></inline-formula> is defined for this model in a very similar way as before. However, for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e334" xlink:type="simple"/></inline-formula>, when the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e335" xlink:type="simple"/></inline-formula> was deterministically reduced by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e336" xlink:type="simple"/></inline-formula> in the simpler model (yielding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e337" xlink:type="simple"/></inline-formula>), this reduction occurs now only with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e338" xlink:type="simple"/></inline-formula>. With probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e339" xlink:type="simple"/></inline-formula> the operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e340" xlink:type="simple"/></inline-formula> sets <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e341" xlink:type="simple"/></inline-formula>, modeling the firing of another spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e342" xlink:type="simple"/></inline-formula> at this time point. The neural computability condition (4) remains unchanged, e.g., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e343" xlink:type="simple"/></inline-formula> for a Boltzmann distribution. A schema of the stochastic dynamics of this local state transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e344" xlink:type="simple"/></inline-formula> is shown in <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2A</xref>.</p>
          <p>This transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e345" xlink:type="simple"/></inline-formula> has the following properties. In Lemma 0 in <xref ref-type="sec" rid="s4">Methods</xref> it is proven that the unique invariant distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e346" xlink:type="simple"/></inline-formula>, denoted as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e347" xlink:type="simple"/></inline-formula>, gives rise to the correct marginal distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e348" xlink:type="simple"/></inline-formula>, i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e349" xlink:type="simple"/></disp-formula></p>
          <p>This means that a neuron whose dynamics is described by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e350" xlink:type="simple"/></inline-formula> samples from the correct distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e351" xlink:type="simple"/></inline-formula> if it receives a static input from the other neurons in the network, i.e., as long as its membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e352" xlink:type="simple"/></inline-formula> is constant. Hence the “local” computation performed by such neuron can be considered as correct. If however, several neurons in the network change their states in a short interval of time, the joint distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e353" xlink:type="simple"/></inline-formula> is in general not the desired one, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e354" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e355" xlink:type="simple"/></inline-formula> denotes the invariant distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e356" xlink:type="simple"/></inline-formula>. In the <xref ref-type="sec" rid="s4">Methods</xref> section, we present simulation results that indicate that the error of the approximation to the desired Boltzmann distributions introduced by neural sampling with relative refractory mechanism is rather minute. It is shown that the neural sampling approximation error is orders of magnitudes below the one introduced by a fully factorized distribution (which amounts to assuming correct marginal distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e357" xlink:type="simple"/></inline-formula> and independent neurons).</p>
          <p>To illustrate the sampling process with the relative refractory mechanism, we examine a network of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e358" xlink:type="simple"/></inline-formula> neurons. We aim to sample from a Boltzmann distribution (5) with parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e359" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e360" xlink:type="simple"/></inline-formula> being randomly drawn from normal distributions. For the neuron model, we use the relative refractory mechanism shown in the mid row of <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2B</xref>. A detailed description of the simulation and the parameters used is given in the <xref ref-type="sec" rid="s4">Methods</xref> section. A spike pattern of the resulting sampling network is shown in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3A</xref>. The network features a sparse, irregular spike response with average firing rate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e361" xlink:type="simple"/></inline-formula>. For one neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e362" xlink:type="simple"/></inline-formula>, indicated with orange spikes, the internal dynamics are shown in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3B</xref>. After each action potential the neuron’s refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e363" xlink:type="simple"/></inline-formula> drops to zero and reduces the probability of spiking again in a short time interval. The influence of the remaining network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e364" xlink:type="simple"/></inline-formula> is transmitted to neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e365" xlink:type="simple"/></inline-formula> via PSPs of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e366" xlink:type="simple"/></inline-formula> and sums up to the fluctuating membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e367" xlink:type="simple"/></inline-formula>. As reflected in the highly variable membrane potential even this small network exhibits rich interactions. To represent the correct distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e368" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e369" xlink:type="simple"/></inline-formula> conditioned on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e370" xlink:type="simple"/></inline-formula>, the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e371" xlink:type="simple"/></inline-formula> continuously adapts its instantaneous firing rate. To quantify the precision with which the spiking network draws samples from the target distribution (5), <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3C</xref> shows the joint distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e372" xlink:type="simple"/></inline-formula> neurons. For comparison we accompany the distribution of sampled network states with the result obtained from the standard Gibbs sampling algorithm (considered as the ground truth). Since the number of possible states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e373" xlink:type="simple"/></inline-formula> grows exponentially in the number of neurons, we restrict ourselves for visualization purposes to the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e374" xlink:type="simple"/></inline-formula> of the gray shaded units and marginalize over the remaining network. The probabilities are estimated from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e375" xlink:type="simple"/></inline-formula> samples, i.e., from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e376" xlink:type="simple"/></inline-formula> successive states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e377" xlink:type="simple"/></inline-formula> of the Markov chain. Stochastic deviations of the estimated probabilities due to the finite number of samples are quite small (typical errors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e378" xlink:type="simple"/></inline-formula>) and are comparable to systematic deviations due to the only locally correct computation of neurons with relative refractory mechanism. In the <xref ref-type="sec" rid="s4">Methods</xref> section, we present further simulation results showing that the proposed networks consisting of neurons with relative refractory mechanism approximate the desired target distributions faithfully over a large range of distribution parameters.</p>
          <fig id="pcbi-1002211-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Sampling from a Boltzmann distribution by spiking neurons with relative refractory mechanism.</title>
              <p>(A) Spike raster of the network. (B) Traces of internal state variables of a neuron (# 26, indicated by orange spikes in A). The rich interaction of the network gives rise to rapidly changing membrane potentials and instantaneous firing rates. (C) Joint distribution of 5 neurons (gray shaded area in A) obtained by the spiking neural network and Gibbs sampling from the same distribution. Active states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e379" xlink:type="simple"/></inline-formula> are indicated by a black dot, using one row for each neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e380" xlink:type="simple"/></inline-formula>, the columns list all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e381" xlink:type="simple"/></inline-formula> possible states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e382" xlink:type="simple"/></inline-formula> of these <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e383" xlink:type="simple"/></inline-formula> neurons. The tight match between both distributions suggests that the spiking network represents the target probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e384" xlink:type="simple"/></inline-formula> with high accuracy.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g003" xlink:type="simple"/>
          </fig>
          <p>In order to illustrate that the proposed sampling networks feature biologically quite realistic spiking dynamics, we present in the <xref ref-type="sec" rid="s4">Methods</xref> section several neural firing statistics (e.g., the inter-spike interval histogram) of the network model. In general, the statistics computed from the model match experimentally observed statistics well. The proposed network models are based on the assumption of rectangular-shaped, renewal PSPs. More precisely, we define renewal (or non-additive) PSPs in the following way. Renewal PSPs evoked by a single synapse do not add up but are merely prolonged in their duration (according to equation (6)); renewal PSPs elicited at different synapses nevertheless add up in the normal way. In <xref ref-type="sec" rid="s4">Methods</xref> we investigate the impact of replacing the theoretically ideal rectangular-shaped, renewal PSPs with biologically more realistic alpha-shaped, additive PSPs. Simulation results suggest that the network model with alpha-shaped PSPs does not capture the target distribution as accurately as with the theoretically ideal PSP shapes, statistical dependencies between the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e385" xlink:type="simple"/></inline-formula> are however still approximated reasonably well.</p>
        </sec>
      </sec>
      <sec id="s2d">
        <title>Neural sampling in continuous time</title>
        <p>The neural sampling model proposed above was formulated in discrete time of step size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e386" xlink:type="simple"/></inline-formula>, inspired by the discrete time nature of MCMC techniques in statistics and machine learning as well as to make simulations possible on digital computers. However, models in continuous time (e.g., ordinary differential equations) are arguably more natural and “realistic” descriptions of temporally varying biological processes. This gives rise to the question whether one can find a sensible limit of the discrete time model in the limit <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e387" xlink:type="simple"/></inline-formula>, yielding a sampling network model in continuous time. Another motivation for considering continuous time models for neural sampling is the fact that many mathematical models for recurrent networks are formulated in continuous time <xref ref-type="bibr" rid="pcbi.1002211-GerstnerW1">[5]</xref>, and a comparison to these existing models would be facilitated. Here we propose a stochastically spiking neural network model in continuous time, whose states still represent correct samples from the desired probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e388" xlink:type="simple"/></inline-formula> at any time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e389" xlink:type="simple"/></inline-formula>. These types of models are usually referred to as Markov jump processes. It can be shown that discretizing this continuous time model yields the discrete time model defined earlier, which thus can be regarded as a version suitable for simulations on a digital computer.</p>
        <p>We define the continuous time model in the following way. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e390" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e391" xlink:type="simple"/></inline-formula>, denote the firing times of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e392" xlink:type="simple"/></inline-formula>. The refractory process of this neuron, in analogy to <xref ref-type="fig" rid="pcbi-1002211-g001">Figure 1</xref> and equation (8)-(9) for the case of discrete time, is described by the following differential equation for the auxiliary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e393" xlink:type="simple"/></inline-formula>, which may now assume any nonnegative real number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e394" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e395" xlink:type="simple"/><label>(12)</label></disp-formula></p>
        <p>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e396" xlink:type="simple"/></inline-formula> denotes Dirac’s Delta centered at the spike time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e397" xlink:type="simple"/></inline-formula>. This differential equation describes the following simple dynamics. The auxiliary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e398" xlink:type="simple"/></inline-formula> decays linearly with time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e399" xlink:type="simple"/></inline-formula> when the neuron is refractory, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e400" xlink:type="simple"/></inline-formula>. Once <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e401" xlink:type="simple"/></inline-formula> arrives at its resting state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e402" xlink:type="simple"/></inline-formula> it remains there, corresponding to the neuron being ready to spike again (more precisely, in order to avoid point measures we set it to a random value in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e403" xlink:type="simple"/></inline-formula>, see <xref ref-type="sec" rid="s4">Methods</xref>). In the resting state, the neuron has the probability density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e404" xlink:type="simple"/></inline-formula> to fire at every time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e405" xlink:type="simple"/></inline-formula>. If it fires at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e406" xlink:type="simple"/></inline-formula>, this results in setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e407" xlink:type="simple"/></inline-formula>, which is formalized in equation (12) by the sum of Dirac Delta’s <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e408" xlink:type="simple"/></inline-formula>. Here the current membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e409" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e410" xlink:type="simple"/></inline-formula> is defined as in the discrete time case, e.g., by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e411" xlink:type="simple"/></inline-formula> for the case of a Boltzmann distribution (5). The binary variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e412" xlink:type="simple"/></inline-formula> is defined to be 1 if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e413" xlink:type="simple"/></inline-formula> and 0 if the neuron is in the resting state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e414" xlink:type="simple"/></inline-formula>. Biologically, the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e415" xlink:type="simple"/></inline-formula> can again be interpreted as the value at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e416" xlink:type="simple"/></inline-formula> of a rectangular-shaped PSP (with a duration of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e417" xlink:type="simple"/></inline-formula>) that neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e418" xlink:type="simple"/></inline-formula> evokes in neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e419" xlink:type="simple"/></inline-formula>. As the spikes are discrete events in continuous time, the probability of two or more neurons spiking at the same time is zero. This allows for updating all neurons in parallel using a differential equation.</p>
        <p>In analogy to the discrete time case, the neural network in continuous time can be shown to sample from the desired distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e420" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e421" xlink:type="simple"/></inline-formula> is an invariant distribution of the network dynamics defined above. However, to establish this fact, one has to rely on a different mathematical framework. The probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e422" xlink:type="simple"/></inline-formula> of the auxiliary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e423" xlink:type="simple"/></inline-formula> as a function of time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e424" xlink:type="simple"/></inline-formula>, which describes the evolution of the network, obeys a partial differential equation, the so-called Differential-Chapman-Kolmogorov equation (see <xref ref-type="bibr" rid="pcbi.1002211-Gardiner1">[43]</xref>):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e425" xlink:type="simple"/><label>(13)</label></disp-formula>where the operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e426" xlink:type="simple"/></inline-formula>, which captures the dynamics of the network, is implicitly defined by the differential equations (12) and the spiking probabilities. This operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e427" xlink:type="simple"/></inline-formula> is the continuous time equivalent to the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e428" xlink:type="simple"/></inline-formula> in the discrete time case. The operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e429" xlink:type="simple"/></inline-formula> consists here of two components. The <italic>drift term</italic> captures the deterministic decay process of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e430" xlink:type="simple"/></inline-formula>, stemming from the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e431" xlink:type="simple"/></inline-formula> in equation (12). The <italic>jump term</italic> describes the non-continuous aspects of the path <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e432" xlink:type="simple"/></inline-formula> associated with “jumping” from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e433" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e434" xlink:type="simple"/></inline-formula> at the time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e435" xlink:type="simple"/></inline-formula> when the neuron fires.</p>
        <p>In the <xref ref-type="sec" rid="s4">Methods</xref> section we prove that the resulting time invariant distribution, i.e., the distribution that solves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e436" xlink:type="simple"/></inline-formula>, now denoted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e437" xlink:type="simple"/></inline-formula> as it is not a function of time, gives rise to the desired marginal distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e438" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e439" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e440" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e441" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e442" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e443" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e444" xlink:type="simple"/></inline-formula> otherwise. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e445" xlink:type="simple"/></inline-formula> denotes Kronecker’s Delta with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e446" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e447" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e448" xlink:type="simple"/></inline-formula> otherwise. Thus, the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e449" xlink:type="simple"/></inline-formula> simply reflects the definition that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e450" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e451" xlink:type="simple"/></inline-formula> and 0 otherwise. For an explicit definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e452" xlink:type="simple"/></inline-formula>, a proof of the above statement, and some additional comments see the <xref ref-type="sec" rid="s4">Methods</xref> section.</p>
        <p>The neural samplers in discrete and continuous time are closely related. The model in discrete time provides an increasingly more precise description of the inherent spike dynamics when the duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e453" xlink:type="simple"/></inline-formula> of the discrete time step is reduced, causing an increase of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e454" xlink:type="simple"/></inline-formula> (such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e455" xlink:type="simple"/></inline-formula> is constant) and therefore a reduced firing probability of each neuron at any discrete time step (see the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e456" xlink:type="simple"/></inline-formula> in equation (8)). In the limit of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e457" xlink:type="simple"/></inline-formula> approaching <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e458" xlink:type="simple"/></inline-formula>, the probability that two or more neurons will fire at the same time approaches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e459" xlink:type="simple"/></inline-formula>, and the discrete time sampler becomes equal to the continuous time system defined above, which updates all units in parallel.</p>
        <p>It is also possible to formulate a continuous time version of the neural sampler based on neuron models with relative refractory mechanisms. In the <xref ref-type="sec" rid="s4">Methods</xref> section the resulting continuous time neuron model with a relative refractory mechanism is defined. Theoretical results similar to the discrete time case can be derived for this sampler (see Lemmata 9 and 10 in <xref ref-type="sec" rid="s4">Methods</xref>): It is shown that each neuron “locally” performs the correct computation under the assumption of static input from the remaining neurons. However one can no longer prove in general that the global network samples from the target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e460" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2e">
        <title>Demonstration of probabilistic inference with recurrent networks of spiking neurons in an application to perceptual multistability</title>
        <p>In the following we present a network model for perceptual multistability based on the neural sampling framework introduced above. This simulation study is aimed at showing that the proposed network can indeed sample from a desired distribution and also perform inference, i.e., sample from the correct corresponding posterior distribution. It is not meant to be a highly realistic or exhaustive model of perceptual multistability nor of biologically plausible learning mechanisms. Such models would naturally require considerably more modelling work.</p>
        <p>Perceptual multistability evoked by ambiguous sensory input, such as a 2D drawing (e.g., Necker cube) that allows for different consistent 3D interpretations, has become a frequently studied perceptual phenomenon. The most important finding is that the perceptual system of humans and nonhuman primates does not produce a superposition of different possible percepts of an ambiguous stimulus, but rather switches between different self-consistent global percepts in a spontaneous manner. Binocular rivalry, where different images are presented to the left and right eye, has become a standard experimental paradigm for studying this effect <xref ref-type="bibr" rid="pcbi.1002211-Leopold1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Bartels1">[47]</xref>. A typical pair of stimuli are the two images shown in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4A</xref>. Here the percepts of humans and nonhuman primates switch (seemingly stochastically) between the two presented orientations. <xref ref-type="bibr" rid="pcbi.1002211-Hoyer1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref> propose that several aspects of experimental data on perceptual multistability can be explained if one assumes that percepts correspond to samples from the conditional distribution over interpretations (e.g., different 3D shapes) given the visual input (e.g., the 2D drawing). Furthermore, the experimentally observed fact that percepts tend to be stable on the time scale of seconds suggests that perception can be interpreted as probabilistic inference that is carried out by MCMC sampling which produces successively correlated samples. In <xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref> it is shown that this MCMC interpretation is also able to qualitatively reproduce the experimentally observed distribution of dominance durations, i.e., the distribution of time intervals between perceptual switches. However, in lack of an adequate model for sampling by a recurrent network of spiking neurons, theses studies could describe this approach only on a rather abstract level, and pointed out the open problem to relate this algorithmic approach to neural processes. We have demonstrated in a computer simulation that the previously described model for neural sampling could in principle fill this gap, providing a modelling framework that is on the one hand consistent with the dynamics of networks of spiking neurons, and which can on the other hand also be clearly understood from the perspective of probabilistic inference through MCMC sampling.</p>
        <fig id="pcbi-1002211-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Modeling perceptual multistability as probabilistic inference with neural sampling.</title>
            <p>(A) Typical visual stimuli for the left and right eye in binocular rivalry experiments. (B) Tuning curve of a neuron with preferred orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e461" xlink:type="simple"/></inline-formula>. (C) Distribution of dominance durations in the trained network under ambiguous input. The red curve shows the Gamma distribution with maximum likelihood on the data. (D) 2-dimensional projection (via population vector) of the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e462" xlink:type="simple"/></inline-formula> encoded in the spiking network showing that it strongly favors coherent global states of arbitrary orientation to incoherent ones (corresponding to population vectors of small magnitude). (E) 2-dimensional projection of the bimodal posterior distribution under an ambiguous input consisting of two different orientations reminiscent of the stimuli shown in A. The black trace shows the temporal evolution of the network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e463" xlink:type="simple"/></inline-formula> for 500 ms around a perceptual switch. (F) Network states at 3 time points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e464" xlink:type="simple"/></inline-formula> marked in E. Neurons that fired in the preceding 20 ms (see gray bar in G) are plotted in the color of their preferred orientation. Inactive neurons are shown in white. While states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e465" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e466" xlink:type="simple"/></inline-formula> represent rather coherent orientations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e467" xlink:type="simple"/></inline-formula> shows an incoherent state corresponding to a perceptual switch. Clamped neurons (which the posterior is condition on) are marked by a black dot. (G) Spike raster of the unclamped neurons during a 500 ms epoch marked by the black trace in E. Gray bars indicate the 20 ms time intervals that define the network states shown in F. Altogether this figure shows that a theoretically rigorous probabilistic inference process can be carried out by a network of spiking neurons with a spike raster that is similar to generic recorded data.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g004" xlink:type="simple"/>
        </fig>
        <p>In the following we model some essential aspects of an experimental setup for binocular rivalry with grating stimuli (see <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4A</xref>) in a recurrent network of spiking neurons with the previously described relative refractory mechanism. We assigned to each of the 217 neurons in the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e468" xlink:type="simple"/></inline-formula> a tuning curve <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e469" xlink:type="simple"/></inline-formula>, centered around its preferred orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e470" xlink:type="simple"/></inline-formula> as shown in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4B</xref>. The preferred orientations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e471" xlink:type="simple"/></inline-formula> of the neurons were chosen to cover the entire interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e472" xlink:type="simple"/></inline-formula> of possible orientations and were randomly assigned to the neurons. The neurons were arranged on a hexagonal grid as depicted in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4F</xref>. Any two neurons with distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e473" xlink:type="simple"/></inline-formula> were synaptically connected (neighboring units had distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e474" xlink:type="simple"/></inline-formula>). We assume that these neurons represent neurons in the visual system that have roughly the same or neighboring receptive field, and that each neuron receives visual input from either the left or the right eye. The network connections were chosen such that neurons that have similar (very different) preferred orientations are connected with positive (negative) weights (for details see <xref ref-type="sec" rid="s4">Methods</xref> section).</p>
        <p>We examined the resulting distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e475" xlink:type="simple"/></inline-formula> over the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e476" xlink:type="simple"/></inline-formula> dimensional network states. To provide an intuitive visualization of these high dimensional network states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e477" xlink:type="simple"/></inline-formula>, we resort to a 2-dimensional projection, the population vector of a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e478" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref> for details of the applied population vector decoding scheme). Only the endpoints of the population vectors are drawn (as colored points) in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4D,E</xref>. The orientation of the population vector is assumed to correspond to the dominant orientation of the percept, and its distance from the origin encodes the strength of this percept. We also, somewhat informally, call the strength of a percept its coherence and a network state which represents a coherent percept a coherent network state. A coherent network state hence results in a population vector of large magnitude. Each direction of a population vector is color coded in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4D,E</xref>, using the color code for directions shown on the right hand side of <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4F</xref>. In <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4D</xref> the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e479" xlink:type="simple"/></inline-formula> of the network is illustrated by sampling of the network for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e480" xlink:type="simple"/></inline-formula>, with samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e481" xlink:type="simple"/></inline-formula> taken every millisecond. Each dot equals a sampled network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e482" xlink:type="simple"/></inline-formula>. In a biological interpretation the spike response of the freely evolving network reflects spontaneous activity, since no observations, i.e., no external input, was added to the system. <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4D</xref> shows that the spontaneous activity of this simple network of spiking neurons moves preferably through coherent network states for all possible orientations due to the chosen recurrent network connections (being positive for neurons with similar preferred orientation and negative otherwise). This can directly be seen from the rare occurrence of population vectors with small magnitude (vectors close to the “center”) in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4D</xref>.</p>
        <p>To study percepts elicited by ambiguous stimuli, where inputs like in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4A</xref> are shown simultaneously to the left and right eye during a binocular rivalry experiment, we provided ambiguous input to the network. Two cells with preferred orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e483" xlink:type="simple"/></inline-formula> and two cells with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e484" xlink:type="simple"/></inline-formula> were clamped to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e485" xlink:type="simple"/></inline-formula>. Additionally four neurons with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e486" xlink:type="simple"/></inline-formula> resp. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e487" xlink:type="simple"/></inline-formula> were muted by clamping to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e488" xlink:type="simple"/></inline-formula>. This ambiguous input is incompatible with a coherent percept, as it corresponds to two orthogonal orientations presented at the same time. The resulting distribution over the state of the 209 remaining neurons is shown for a time span of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e489" xlink:type="simple"/></inline-formula> of simulated biological time (with samples taken every millisecond) in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4E</xref>. One clearly sees that the network spends most of the time in network states that correspond to one of the two simultaneously presented input orientations (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e490" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e491" xlink:type="simple"/></inline-formula>), and virtually no time on orientations in between. This implements a sampling process from a bimodal conditional distribution. The black line marks a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e492" xlink:type="simple"/></inline-formula> trace of network states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e493" xlink:type="simple"/></inline-formula> around a perceptual switch: The network remained in one mode of high probability – corresponding to one percept – for some period of time, and then quickly traversed the state space to another mode – corresponding to a different percept.</p>
        <p>Three of the states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e494" xlink:type="simple"/></inline-formula> around this perceptual switch (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e495" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e496" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e497" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4E</xref>) are explicitly shown in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4F</xref>. Neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e498" xlink:type="simple"/></inline-formula> that fired during the preceding interval of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e499" xlink:type="simple"/></inline-formula> ms (marked in gray in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4G</xref>) are drawn in the respective color of their preferred orientation. Inactive neurons are drawn in white, and clamped neurons are marked by a black dot (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e500" xlink:type="simple"/></inline-formula>).</p>
        <p><xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4G</xref> shows the action potentials of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e501" xlink:type="simple"/></inline-formula> non-clamped neurons during the same <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e502" xlink:type="simple"/></inline-formula> trace around the perceptual switch. One sees that the sampling process is expressed in this neural network model by a sparse, asynchronous and irregular spike response. It is worth mentioning that the average firing rate when sampling from the posterior distribution is only slightly higher than the average firing rate of spontaneous activity (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e503" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e504" xlink:type="simple"/></inline-formula> respectively), which is reminiscent of related experimental data <xref ref-type="bibr" rid="pcbi.1002211-Fiser1">[7]</xref>. Thus on the basis of the overall network activity it is indistinguishable whether the network carries out an inference task or freely samples from its prior distribution. It is furthermore notable, that a focus of the network activity on the two orientations that are given by the external input can be achieved in this model, in spite of the fact that only two of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e505" xlink:type="simple"/></inline-formula> neurons were clamped for each of them. This numerical relationship is reminiscent of standard data on the weak input from LGN to V1 that is provided in the brain <xref ref-type="bibr" rid="pcbi.1002211-Binzegger1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Binzegger2">[49]</xref>, and raises the question whether the proposed neural sampling model could provide a possible mechanism (under the modelling assumptions made above) for cortical processing of such numerically weak external inputs.</p>
        <p>The distribution of the resulting dominance durations, i.e., the time between perceptual switches, for the previously described setup with ambiguous input is shown for a continuous run of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e506" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4C</xref> (a similar method as in <xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref> was used to measure dominance durations, see <xref ref-type="sec" rid="s4">Methods</xref>). This distribution can be approximated quite well by a Gamma distribution, which also provides a good fit to experimental data (see the discussion in <xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref>). We expect that also other features of the more abstract MCMC model for biological vision of <xref ref-type="bibr" rid="pcbi.1002211-Sundareswara1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref>, such as contextual biases and traveling waves, will emerge in larger and more detailed implementations of the MCMC approach through the proposed neural sampling method in networks of spiking neurons.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have presented a spiking neural network that samples from a given probability distribution via its inherent network dynamics. In particular the network is able to carry out probabilistic inference through sampling. The model, based on assumptions about the underlying probability distribution (formalized by the neural computability condition) as well as on certain assumptions regarding the underlying MCMC model, provides one possible neural implementation of the “inference-by-sampling paradigm” emerging in computational neuroscience.</p>
      <p>During inference the observations (i.e., the variables which we wish to condition on) are modeled in this study by clamping the corresponding neurons by strong external input to the observed binary value. Units which receive no input or input with vanishing contrast (stimulus intensity) are treated as unobserved. Using this admittedly quite simplistic model of the input, we observed in simulations that our network model exhibits the following property: The onset of a sensory stimulus reduces the variability of the firing activity, which represents (after stimulus onset) a conditional distribution, rather than the prior distribution (see the difference between panels <bold>D</bold> and <bold>E</bold> of <xref ref-type="fig" rid="pcbi-1002211-g005">Figure 5</xref>. It is tempting to compare these results to the experimental finding of reduced firing rate variability after stimulus onset observed in several cortical areas <xref ref-type="bibr" rid="pcbi.1002211-Churchland1">[50]</xref>. We wish to point out however, that a consistent treatment of zero contrast stimuli requires more thorough modelling efforts (e.g., by explicitly adding a random variable for the stimulus intensity <xref ref-type="bibr" rid="pcbi.1002211-Fiser2">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Berkes1">[51]</xref>), which is not the focus of the presented work.</p>
      <fig id="pcbi-1002211-g005" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g005</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Firing statistics of neural sampling networks.</title>
          <p>(A) Shown is the membrane potential histogram of a typical neuron during sampling. The data is that of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e507" xlink:type="simple"/></inline-formula> from the simulation shown in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref> (the membrane potential and spike trace of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e508" xlink:type="simple"/></inline-formula> are highlighted in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>). (B) The plot shows the ISI distribution of a typical neuron (again <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e509" xlink:type="simple"/></inline-formula> from <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>) during sampling. The distribution is roughly gamma-shaped, reminiscent of experimentally observed ISI distributions. (C) A scatter plot of the coefficient of variation (CV) versus the average interspike interval (ISI) of each neuron taken from the simulation shown in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>. The value of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e510" xlink:type="simple"/></inline-formula> from <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref> is marked by a cross. The simulated data is in accordance with experimentally observed data.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g005" xlink:type="simple"/>
      </fig>
      <p>Virtually all high-level computational tasks that a brain has to solve can be formalized as optimization problems, that take into account a (possibly large) number of soft or hard constraints. In typical applications of probabilistic inference in science and engineering (see e.g. <xref ref-type="bibr" rid="pcbi.1002211-Bishop1">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Koller1">[53]</xref>) such constraints are encoded in e.g., conditional probability tables or factors. In a biological setup they could possibly be encoded through the synaptic weights of a recurrent network of spiking neurons. The solution of such optimizations problems in a probabilistic framework via sampling, as implemented in our model, provides an alternative to deterministic solutions, as traditionally implemented in neural networks (see, e.g., <xref ref-type="bibr" rid="pcbi.1002211-Hopfield1">[54]</xref> for the case of constraint satisfaction problems). Whereas an attractor neural network converges to <italic>one</italic> (possibly approximate) solution of the problem, a stochastic network may alternate between different approximate solutions and stay the longest at those approximate solutions that provide the best fit. This might be advantageous, as given more time a stochastic network can explore more of the state space and avoid shallow local minima. Responses to ambiguous sensory stimuli <xref ref-type="bibr" rid="pcbi.1002211-Leopold1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Bartels1">[47]</xref> might be interpreted as an optimization with soft constraints. The interpretation of human thinking as sampling process solving an inference task, recently proposed in cognitive science <xref ref-type="bibr" rid="pcbi.1002211-Griffiths2">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Vul1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Denison1">[56]</xref>, further emphasizes that considering neural activity as an inferential process via sampling promises to be a fruitful approach.</p>
      <p>Our approach builds on, and extends, previous work where recurrent networks of non-spiking stochastic neurons (commonly considered in artificial neural networks) were shown to be able to carry out probabilistic inference through Gibbs sampling <xref ref-type="bibr" rid="pcbi.1002211-Ackley1">[36]</xref>. In <xref ref-type="bibr" rid="pcbi.1002211-Hinton3">[57]</xref> a first extension of this approach to a network of recurrently connected spiking neurons had been presented. The dynamics of the recurrently connected spiking neurons are described as stepwise sampling from the posterior of a temporal Restricted Boltzmann Machine (tRBM) by introducing a clever interpretation of the temporal spike code as time varying parameters of a multivariate Gaussian distribution. Drawing one sample from the posterior of a RBM is, by construction, a trivial one-step task. In contrast to our model, the model of <xref ref-type="bibr" rid="pcbi.1002211-Hinton3">[57]</xref> does not produce multiple samples from a fixed posterior distribution, given the fixed input, but produces exactly one sample consisting of the temporal sequence of the hidden nodes, given a temporal input sequence. Similar temporal models, sometimes called Bayesian filtering, also underlie the important contributions of <xref ref-type="bibr" rid="pcbi.1002211-Zemel1">[58]</xref> and <xref ref-type="bibr" rid="pcbi.1002211-Deneve1">[32]</xref>. In <xref ref-type="bibr" rid="pcbi.1002211-Deneve1">[32]</xref> every single neuron is described as hidden Markov Model (HMM) with two states. Instead of drawing samples from the instantaneous posterior distribution using stochastic spikes, <xref ref-type="bibr" rid="pcbi.1002211-Deneve1">[32]</xref> presents a deterministic spike generation with the intention to convey the analog probability value rather than discrete samples. The approach presented here can be interpreted as a biologically more realistic version of Gibbs sampling for a specific class of probability distributions by taking into account a spike-based communication, finite duration PSPs and refractory mechanisms. Other implementations based on different distributions (e.g., directed graphical models) and different sampling methods (e.g., reversible MCMC methods) are of course conceivable and worth exploring.</p>
      <p>In a computer experiment (see <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4</xref>, we used our proposed network to model aspects of biological vision as probabilistic inference along the lines of argumentation put forward in <xref ref-type="bibr" rid="pcbi.1002211-Hoyer1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Gershman1">[18]</xref>. Our model was chosen to be quite simplistic, just to demonstrate that a number of experimental data on the dynamics of spontaneous activity <xref ref-type="bibr" rid="pcbi.1002211-Berkes1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Kenet1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Fox1">[60]</xref> and binocular rivalry <xref ref-type="bibr" rid="pcbi.1002211-Leopold1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1002211-Bartels1">[47]</xref> can in principle be captured by this approach. The main point of the modelling study is to show that rather realistic neural dynamics can support computational functions rigorously formalized as inference via sampling.</p>
      <p>We have also presented a model of spiking dynamics in continuous time that performs sampling from a given probability distribution. Although computer simulations of biological networks of neurons often actually use discrete time, it is desirable to also have a sound approach for understanding and describing the network sampling dynamics in continuous time, as the latter is arguable a natural framework for describing temporal processes in biology. Furthermore comparison to many existing continuous time neuron and network models of neurons is facilitated.</p>
      <p>We have made various simplifying assumption regarding neural processes, e.g., simple symbolic postsynaptic potentials in the form of step-functions (reminiscent of plateau potentials caused by dendritic NMDA spikes <xref ref-type="bibr" rid="pcbi.1002211-Antic1">[61]</xref>). More accurate models for neurons have to integrate a multitude of time constants that represent different temporal processes on the physical, molecular, and genetic level. Hence the open problem arises, to which extent this multitude of time constants and other complex dynamics can be integrated into theoretical models of neural sampling. We have gone one first step in this direction by showing that in computer simulations the two temporal processes that we have considered (refractory processes and postsynaptic potentials) can approximately be decoupled. Furthermore, we have presented simulation results suggesting that more realistic alpha-shaped, additive EPSPs are compatible with the functionality of the proposed network model.</p>
      <p>Finally, we want to point out that the prospect of using networks of spiking neurons for probabilistic inference via sampling suggests new applications for energy-efficient spike-based and massively parallel electronic hardware that is currently under development <xref ref-type="bibr" rid="pcbi.1002211-Merolla1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Bruederle1">[63]</xref>.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <p>We first provide details and proofs for the neural sampling models, followed by details for the computer simulations. Then we investigate typical firing statistics of individual neurons during neural sampling and examine the approximation quality of neural sampling with different neuron and synapse models.</p>
      <sec id="s4a">
        <title>Mathematical details</title>
        <sec id="s4a1">
          <title>Notation</title>
          <p>To keep the derivations in a compact form, we introduce the following notations. We define the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e511" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e512" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e513" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e514" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e515" xlink:type="simple"/></inline-formula> otherwise. Analogously we define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e516" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e517" xlink:type="simple"/></inline-formula> denote Kronecker’s Delta, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e518" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e519" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e520" xlink:type="simple"/></inline-formula> whereas <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e521" xlink:type="simple"/></inline-formula> denotes Dirac’s Delta, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e522" xlink:type="simple"/></inline-formula>. Furthermore <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e523" xlink:type="simple"/></inline-formula> is the indicator function of the set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e524" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e525" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e526" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e527" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e528" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s4a2">
          <title>Details to neural sampling with absolute refractory period in discrete time</title>
          <p>The following Lemmata 1 – 3 provide a proof of Theorem 1. For completeness we begin this paragraph with a recapitulation of the definitions stated in Results. We then identify some central properties of the joint probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e529" xlink:type="simple"/></inline-formula> and proof that the proposed network samples from the desired invariant distribution.</p>
          <p>For a given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e530" xlink:type="simple"/></inline-formula> over the binary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e531" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e532" xlink:type="simple"/></inline-formula>, the joint distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e533" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e534" xlink:type="simple"/></inline-formula> is defined in the following way (see equation 7):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e535" xlink:type="simple"/></disp-formula></p>
          <p>The assumption <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e536" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e537" xlink:type="simple"/></inline-formula> is required to show the irreducibility of the Markov chain, a prerequisite to ensure the uniqueness of the invariant distribution of the MCMC dynamics. Furthermore, for the given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e538" xlink:type="simple"/></inline-formula> we define the functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e539" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e540" xlink:type="simple"/></inline-formula> which map <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e541" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e542" xlink:type="simple"/></disp-formula></p>
          <p>Instead of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e543" xlink:type="simple"/></inline-formula> we simply write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e544" xlink:type="simple"/></inline-formula> in the following.</p>
          <p>
            <italic>Lemma 1. The distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e545" xlink:type="simple"/></inline-formula>
            <italic> has conditional distributions of the following form:</italic>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e546" xlink:type="simple"/></disp-formula>
          </p>
          <p><italic>These results can also be written more compactly in the following form:</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e547" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e548" xlink:type="simple"/></inline-formula>.</p>
          <p><italic>Proof.</italic> Here we use the fact that the logistic function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e549" xlink:type="simple"/></inline-formula> is the inverse of the logit function, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e550" xlink:type="simple"/></inline-formula>.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e551" xlink:type="simple"/></disp-formula></p>
          <p>This also shows that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e552" xlink:type="simple"/></inline-formula> is independent from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e553" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e554" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e555" xlink:type="simple"/></inline-formula>. Now we show the second relation using Bayes’ rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e556" xlink:type="simple"/></disp-formula></p>
          <p>In order to facilitate the verification of the next two Lemmata, we first restate the definition of the operators <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e557" xlink:type="simple"/></inline-formula> in a more concise way:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e558" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e559" xlink:type="simple"/></inline-formula>.</p>
          <p>
            <italic>Lemma 2. For all </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e560" xlink:type="simple"/></inline-formula>
            <italic> the operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e561" xlink:type="simple"/></inline-formula>
            <italic> leaves the conditional distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e562" xlink:type="simple"/></inline-formula>
            <italic> invariant.</italic>
          </p>
          <p><italic>Proof.</italic> For sake of simplicity, denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e563" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e564" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e565" xlink:type="simple"/></inline-formula>. We have to show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e566" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e567" xlink:type="simple"/></inline-formula>.</p>
          <p>First we show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e568" xlink:type="simple"/></inline-formula> using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e569" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e570" xlink:type="simple"/></inline-formula> (which results from Lemma 1):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e571" xlink:type="simple"/></disp-formula></p>
          <p>Here we used the definition of the logistic function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e572" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e573" xlink:type="simple"/></inline-formula>.</p>
          <p>Now we show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e574" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e575" xlink:type="simple"/></disp-formula></p>
          <p>Here we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e576" xlink:type="simple"/></inline-formula>.</p>
          <p>It is trivial to show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e577" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e578" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e579" xlink:type="simple"/></inline-formula>. Here we used the facts that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e580" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e581" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e582" xlink:type="simple"/></inline-formula> by definition.</p>
          <p>
            <italic>Lemma 3. For all </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e583" xlink:type="simple"/></inline-formula>
            <italic> the operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e584" xlink:type="simple"/></inline-formula>
            <italic> leaves the distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e585" xlink:type="simple"/></inline-formula>
            <italic> invariant.</italic>
          </p>
          <p><italic>Proof.</italic> We start from Lemma 2, which states that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e586" xlink:type="simple"/></inline-formula> leaves the conditional distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e587" xlink:type="simple"/></inline-formula> invariant:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e588" xlink:type="simple"/></disp-formula></p>
          <p>Here we used the relations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e589" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e590" xlink:type="simple"/></inline-formula> as well as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e591" xlink:type="simple"/></inline-formula> which directly follow from the definitions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e592" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e593" xlink:type="simple"/></inline-formula>.</p>
          <p>Finally, we can verify that the composed operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e594" xlink:type="simple"/></inline-formula> samples from the given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e595" xlink:type="simple"/></inline-formula>.</p>
          <p>
            <italic>Theorem 1. </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e596" xlink:type="simple"/></inline-formula>
            <italic> is the unique invariant distribution of operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e597" xlink:type="simple"/></inline-formula>
            <italic>.</italic>
          </p>
          <p><italic>Proof.</italic> As all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e598" xlink:type="simple"/></inline-formula> leave <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e599" xlink:type="simple"/></inline-formula> invariant, so does the concatenation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e600" xlink:type="simple"/></inline-formula>. To ensure that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e601" xlink:type="simple"/></inline-formula> is the <italic>unique</italic> invariant distribution, we have to show that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e602" xlink:type="simple"/></inline-formula> is irreducible and aperiodic. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e603" xlink:type="simple"/></inline-formula> is aperiodic as the transition probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e604" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e605" xlink:type="simple"/></inline-formula> (this follows from the assumption <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e606" xlink:type="simple"/></inline-formula> made above).</p>
          <p>The operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e607" xlink:type="simple"/></inline-formula> is also irreducible for the following reason. First we see that from any state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e608" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e609" xlink:type="simple"/></inline-formula> steps we can get to the zero-state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e610" xlink:type="simple"/></inline-formula> (and stay there) with non-zero probability, as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e611" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e612" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e613" xlink:type="simple"/></inline-formula>. Furthermore, it can be seen that any state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e614" xlink:type="simple"/></inline-formula> can be reached from the zero-state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e615" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e616" xlink:type="simple"/></inline-formula> steps since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e617" xlink:type="simple"/></inline-formula> for any value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e618" xlink:type="simple"/></inline-formula>. Hence every final state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e619" xlink:type="simple"/></inline-formula> can be reached from every starting state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e620" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e621" xlink:type="simple"/></inline-formula> steps with non-vanishing probability.</p>
        </sec>
        <sec id="s4a3">
          <title>Details to neural sampling with a relative refractory period in discrete time</title>
          <p>We augment the neuron model with a relative refractory period described by a function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e622" xlink:type="simple"/></inline-formula>. We first ensure existence of the corresponding function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e623" xlink:type="simple"/></inline-formula>. Based on these functions we then introduce the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e624" xlink:type="simple"/></inline-formula> of the Markov chain. This operator is shown to entail correct “local” computations.</p>
          <p>
            <italic>Lemma 4. Let </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e625" xlink:type="simple"/></inline-formula>
            <italic> be a tuple of non-negative real numbers, with </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e626" xlink:type="simple"/></inline-formula>
            <italic> and at least one element </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e627" xlink:type="simple"/></inline-formula>
            <italic>. This defines the refractory function via </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e628" xlink:type="simple"/></inline-formula>
            <italic>. There exists a unique </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e629" xlink:type="simple"/></inline-formula>
            <italic> function </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e630" xlink:type="simple"/></inline-formula>
            <italic> with the following property </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e631" xlink:type="simple"/></inline-formula>
            <italic>:</italic>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e632" xlink:type="simple"/><label>(15)</label></disp-formula>
          </p>
          <p>
            <italic>Furthermore, the function </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e633" xlink:type="simple"/></inline-formula>
            <italic> has the property:</italic>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e634" xlink:type="simple"/></disp-formula>
          </p>
          <p><italic>Proof.</italic> Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e635" xlink:type="simple"/></inline-formula>; we know that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e636" xlink:type="simple"/></inline-formula>. We define the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e637" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e638" xlink:type="simple"/></disp-formula></p>
          <p>We can see that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e639" xlink:type="simple"/></inline-formula> is a positive <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e640" xlink:type="simple"/></inline-formula> function on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e641" xlink:type="simple"/></inline-formula>. Furthermore, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e642" xlink:type="simple"/></inline-formula> is defined as a sum of functions of the form <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e643" xlink:type="simple"/></inline-formula>. Each factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e644" xlink:type="simple"/></inline-formula> is positive and strictly monotonous. Therefore, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e645" xlink:type="simple"/></inline-formula> is strictly monotonous on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e646" xlink:type="simple"/></inline-formula> with the limits:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e647" xlink:type="simple"/></disp-formula></p>
          <p>Hence the equation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e648" xlink:type="simple"/></inline-formula> has a unique solution for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e649" xlink:type="simple"/></inline-formula> called <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e650" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e651" xlink:type="simple"/></inline-formula>. From applying the implicit function theorem to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e652" xlink:type="simple"/></inline-formula> it follows that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e653" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e654" xlink:type="simple"/></inline-formula>.</p>
          <p>From here on, with the letter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e655" xlink:type="simple"/></inline-formula> we will denote the function characterized by the above Lemma for the given tuple <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e656" xlink:type="simple"/></inline-formula> (which denotes the chosen refractory function).</p>
          <p><italic>Definition 1. Define </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e657" xlink:type="simple"/></inline-formula><italic>. The transition operator </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e658" xlink:type="simple"/></inline-formula><italic> is defined in the following way for all</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e659" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e660" xlink:type="simple"/></disp-formula><italic>with</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e661" xlink:type="simple"/></inline-formula>.</p>
          <p>
            <italic>Lemma 5. For all </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e662" xlink:type="simple"/></inline-formula>
            <italic> the unique invariant distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e663" xlink:type="simple"/></inline-formula>
            <italic> of the operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e664" xlink:type="simple"/></inline-formula>
            <italic> fulfills </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e665" xlink:type="simple"/></inline-formula>
            <italic>. This means, for a constant configuration </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e666" xlink:type="simple"/></inline-formula>
            <italic>, the operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e667" xlink:type="simple"/></inline-formula>
            <italic> produces samples </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e668" xlink:type="simple"/></inline-formula>
            <italic> from the correct conditional distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e669" xlink:type="simple"/></inline-formula>
            <italic>.</italic>
          </p>
          <p><italic>Proof.</italic> We define:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e670" xlink:type="simple"/></disp-formula>where the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e671" xlink:type="simple"/></inline-formula> is defined as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e672" xlink:type="simple"/></disp-formula></p>
          <p>It is trivial to see that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e673" xlink:type="simple"/></inline-formula> has the correct marginal distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e674" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e675" xlink:type="simple"/></disp-formula></p>
          <p>We now show that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e676" xlink:type="simple"/></inline-formula> is the unique invariant distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e677" xlink:type="simple"/></inline-formula>. Because of the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e678" xlink:type="simple"/></inline-formula>, we only have to show that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e679" xlink:type="simple"/></inline-formula> is the unique invariant distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e680" xlink:type="simple"/></inline-formula>. We denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e681" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e682" xlink:type="simple"/></inline-formula>, i.e., we have to show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e683" xlink:type="simple"/></inline-formula>.</p>
          <p>It is trivial to show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e684" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e685" xlink:type="simple"/></inline-formula>, as there is only one non-vanishing element of transition operator, namely <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e686" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e687" xlink:type="simple"/></disp-formula></p>
          <p>Here we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e688" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e689" xlink:type="simple"/></inline-formula> and the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e690" xlink:type="simple"/></inline-formula>.</p>
          <p>Now we show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e691" xlink:type="simple"/></inline-formula> starting from equation (15) and additionally using the relations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e692" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e693" xlink:type="simple"/></inline-formula> as well as the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e694" xlink:type="simple"/></inline-formula>. We define for the sake of simplicity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e695" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e696" xlink:type="simple"/></disp-formula></p>
          <p>We finally show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e697" xlink:type="simple"/></inline-formula>, using the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e698" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e699" xlink:type="simple"/></disp-formula></p>
          <p>The argument that the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e700" xlink:type="simple"/></inline-formula> is aperiodic and irreducible is similar to the one presented in Lemma 1.</p>
        </sec>
        <sec id="s4a4">
          <title>Details to neural sampling with an absolute refractory period in continuous time</title>
          <p>In contrast to the discrete time model we define the state space of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e701" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e702" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e703" xlink:type="simple"/></inline-formula>, i.e., as the union of the positive real numbers and a small interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e704" xlink:type="simple"/></inline-formula>. We will define the sampling operator in such a way that after neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e705" xlink:type="simple"/></inline-formula> was refractory for exactly its refractory period <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e706" xlink:type="simple"/></inline-formula>, its refractory variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e707" xlink:type="simple"/></inline-formula> is uniformly placed in the small interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e708" xlink:type="simple"/></inline-formula>, which represents now the resting state and replaces <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e709" xlink:type="simple"/></inline-formula>. This avoids point measures (Dirac’s Delta) on the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e710" xlink:type="simple"/></inline-formula>. This system is still exactly equivalent to the system discussed in the main paper, as all spike-transition probabilities of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e711" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e712" xlink:type="simple"/></inline-formula> are constant. Hence, it does not matter which values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e713" xlink:type="simple"/></inline-formula> assumes with respect to the spike mechanism during its non-refractory period as long as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e714" xlink:type="simple"/></inline-formula>.</p>
          <p><italic>Definition 2. </italic>For a given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e715" xlink:type="simple"/></inline-formula> over the binary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e716" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e717" xlink:type="simple"/></inline-formula>, we define a joint distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e718" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e719" xlink:type="simple"/></inline-formula> in the following way:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e720" xlink:type="simple"/></disp-formula><italic>where </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e721" xlink:type="simple"/></inline-formula><italic> is the refractory resting state interval. In accordance with this definition we can also write </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e722" xlink:type="simple"/></inline-formula><italic>.</italic></p>
          <p><italic>Lemma 6. The distribution </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e723" xlink:type="simple"/></inline-formula><italic> has the following marginal distribution</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e724" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e725" xlink:type="simple"/></inline-formula>.</p>
          <p><italic>Definition 3. For </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e726" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e727" xlink:type="simple"/></inline-formula><italic> the operator </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e728" xlink:type="simple"/></inline-formula><italic> is defined in the following way for a function </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e729" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e730" xlink:type="simple"/></disp-formula><italic>where the functional </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e731" xlink:type="simple"/></inline-formula><italic> is defined as the one-sided limit from above at 0:</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e732" xlink:type="simple"/></disp-formula></p>
          <p>
            <italic>The operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e733" xlink:type="simple"/></inline-formula>
            <italic> is defined in the following way for a probability distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e734" xlink:type="simple"/></inline-formula>
            <italic> on </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e735" xlink:type="simple"/></inline-formula>
            <italic>:</italic>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e736" xlink:type="simple"/></disp-formula>
            <italic>where </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e737" xlink:type="simple"/></inline-formula>
            <italic> denotes the function </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e738" xlink:type="simple"/></inline-formula>
            <italic> of </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e739" xlink:type="simple"/></inline-formula>
            <italic> where </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e740" xlink:type="simple"/></inline-formula>
            <italic> is held constant and </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e741" xlink:type="simple"/></inline-formula>
            <italic>.</italic>
          </p>
          <p>The transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e742" xlink:type="simple"/></inline-formula> defines the following Fokker-Planck equation for a time-dependent distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e743" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e744" xlink:type="simple"/></disp-formula></p>
          <p>The jump and drift functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e745" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e746" xlink:type="simple"/></inline-formula> associated to the operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e747" xlink:type="simple"/></inline-formula> are given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e748" xlink:type="simple"/></disp-formula></p>
          <p>
            <italic>Lemma 7. The operator </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e749" xlink:type="simple"/></inline-formula>
            <italic> leaves the conditional distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e750" xlink:type="simple"/></inline-formula>
            <italic> invariant with </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e751" xlink:type="simple"/></inline-formula>
            <italic>, i.e.:</italic>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e752" xlink:type="simple"/></disp-formula>
          </p>
          <p><italic>Proof.</italic> This is easy to proof using calculus and the relations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e753" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e754" xlink:type="simple"/></inline-formula>.</p>
          <p><italic>Lemma 8.</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e755" xlink:type="simple"/></inline-formula> <italic>is an invariant distribution of </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e756" xlink:type="simple"/></inline-formula><italic>, i.e., it is a solution to the invariant Fokker-Planck equation:</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e757" xlink:type="simple"/></disp-formula></p>
          <p><italic>Proof.</italic> We observe that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e758" xlink:type="simple"/></inline-formula> for a constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e759" xlink:type="simple"/></inline-formula> (which is not a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e760" xlink:type="simple"/></inline-formula>). Hence:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e761" xlink:type="simple"/></disp-formula></p>
          <p>The Lemma follows then from the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e762" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s4a5">
          <title>Details to neural sampling with a relative refractory period in continuous time</title>
          <p>As already assumed in the case of the absolute refractory sampler in continuous time, we define the state space of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e763" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e764" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e765" xlink:type="simple"/></inline-formula>.</p>
          <p>
            <italic>Lemma 9. Let </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e766" xlink:type="simple"/></inline-formula>
            <italic> be a continuous, non-negative function </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e767" xlink:type="simple"/></inline-formula>
            <italic> with </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e768" xlink:type="simple"/></inline-formula>
            <italic> for </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e769" xlink:type="simple"/></inline-formula>
            <italic>. There exists a unique </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e770" xlink:type="simple"/></inline-formula>
            <italic> function </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e771" xlink:type="simple"/></inline-formula>
            <italic> with the following property </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e772" xlink:type="simple"/></inline-formula>
            <italic>:</italic>
            <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e773" xlink:type="simple"/><label>(16)</label></disp-formula>
          </p>
          <p><italic>Proof.</italic> We define the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e774" xlink:type="simple"/></inline-formula> in the following way:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e775" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e776" xlink:type="simple"/></inline-formula>. From <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e777" xlink:type="simple"/></inline-formula> we can follow that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e778" xlink:type="simple"/></inline-formula> is non-negative. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e779" xlink:type="simple"/></inline-formula> is differentiable with the derivative:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e780" xlink:type="simple"/></disp-formula></p>
          <p>Hence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e781" xlink:type="simple"/></inline-formula> is strictly monotonously increasing. Furthermore, the following relations hold:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e782" xlink:type="simple"/></disp-formula></p>
          <p>Therefore the equation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e783" xlink:type="simple"/></disp-formula>has exactly one solution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e784" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e785" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e786" xlink:type="simple"/></inline-formula>. From applying the implicit function theorem to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e787" xlink:type="simple"/></inline-formula> it follows that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e788" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e789" xlink:type="simple"/></inline-formula>.</p>
          <p><italic>Definition 4. For all </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e790" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e791" xlink:type="simple"/></inline-formula><italic> the operator </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e792" xlink:type="simple"/></inline-formula><italic> is defined in the following way for a function</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e793" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e794" xlink:type="simple"/></disp-formula></p>
          <p>The transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e795" xlink:type="simple"/></inline-formula> defines the following Fokker-Planck equation for a time-dependent distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e796" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e797" xlink:type="simple"/></disp-formula></p>
          <p>The jump and drift functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e798" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e799" xlink:type="simple"/></inline-formula> associated to the operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e800" xlink:type="simple"/></inline-formula> are given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e801" xlink:type="simple"/></disp-formula></p>
          <p><italic>Lemma 10. For all </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e802" xlink:type="simple"/></inline-formula><italic> the invariant distribution </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e803" xlink:type="simple"/></inline-formula><italic> of the operator </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e804" xlink:type="simple"/></inline-formula><italic> fulfills </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e805" xlink:type="simple"/></inline-formula>.</p>
          <p><italic>Proof.</italic> We define the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e806" xlink:type="simple"/></inline-formula> as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e807" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e808" xlink:type="simple"/></inline-formula>. By applying the operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e809" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e810" xlink:type="simple"/></inline-formula> one can verify that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e811" xlink:type="simple"/></inline-formula> holds using the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e812" xlink:type="simple"/></inline-formula> given in (16). Furthermore we can compute the ratio:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e813" xlink:type="simple"/></disp-formula></p>
        </sec>
      </sec>
      <sec id="s4b">
        <title>Details to the computer simulations</title>
        <p>The simulation results shown in <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2</xref>, <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref> and <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4</xref> used the biologically more realistic neuron model with the relative refractory mechanism. During all experiments the first second of simulated time was discarded as burn-in time. The full list of parameters defining the experimental setup is given in <xref ref-type="table" rid="pcbi-1002211-t001">Table 1</xref>. All occurring joint probability distributions are Boltzmann distributions of the form given in equation (5). Example Python <xref ref-type="bibr" rid="pcbi.1002211-Python1">[64]</xref> scripts for neural sampling from Boltzmann distributions are available on request and will be provided on our webpage. The example code comprises networks with both absolute and relative refractory mechanism. It requires standard Python packages only and is readily executable.</p>
        <table-wrap id="pcbi-1002211-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.t001</object-id><label>Table 1</label><caption>
            <title>List of parameters of the computer simulations.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002211-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1">Description</td>
                <td align="left" colspan="1" rowspan="1">Variable</td>
                <td align="left" colspan="1" rowspan="1">Value</td>
                <td align="left" colspan="1" rowspan="1">Figure</td>
                <td align="left" colspan="1" rowspan="1">Comment</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="5" rowspan="1">
                  <italic>Simulation Time</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Simulation step size</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e814" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e815" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2–7</td>
                <td align="left" colspan="1" rowspan="1">interpretation of an MCMC step</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Burn-in time</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e816" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e817" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2–7</td>
                <td align="left" colspan="1" rowspan="1">before recording spikes</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Simulation time</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e818" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e819" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e820" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">3,5–7</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e821" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e822" xlink:type="simple"/></inline-formula> for <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3C</xref></td>
              </tr>
              <tr>
                <td align="left" colspan="5" rowspan="1">
                  <italic>Network</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Number of neurons</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e823" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">3</td>
                <td align="left" colspan="1" rowspan="1">2</td>
                <td align="left" colspan="1" rowspan="1">unconnected</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">40</td>
                <td align="left" colspan="1" rowspan="1">3,5,6</td>
                <td align="left" colspan="1" rowspan="1">randomly connected</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">217</td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e824" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">7</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e825" xlink:type="simple"/></inline-formula> networks</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Connection radius</td>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e826" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e827" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">3,5–7</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e828" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Recurrent weights</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e829" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e830" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">3,5–7</td>
                <td align="left" colspan="1" rowspan="1">from Gaussian distribution</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Falling edge</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e831" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <xref ref-type="bibr" rid="pcbi.1002211-Yang1">[20]</xref>
                  <italic>ms</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">6,7</td>
                <td align="left" colspan="1" rowspan="1">for realistic PSP shapes</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Rising edge</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e832" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <xref ref-type="bibr" rid="pcbi.1002211-Flight1">[3]</xref>
                  <italic>ms</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">6,7</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Scaling factor</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e833" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">20/17</td>
                <td align="left" colspan="1" rowspan="1">6,7</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="5" rowspan="1">
                  <italic>Neuron Model</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Number recovery steps</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e834" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e835" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2–7</td>
                <td align="left" colspan="1" rowspan="1">PSP duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e836" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Refractory function</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e837" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e838" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e839" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">normalized to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e840" xlink:type="simple"/></inline-formula>,</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e841" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2–7</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e842" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e843" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">2<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e844" xlink:type="simple"/></inline-formula>,7</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Excitability</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e845" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e846" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e847" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">2</td>
                <td align="left" colspan="1" rowspan="1">defines membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e848" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e849" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">3,5–7</td>
                <td align="left" colspan="1" rowspan="1">from Gaussian distribution</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e850" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1">initial value</td>
              </tr>
              <tr>
                <td align="left" colspan="5" rowspan="1">
                  <italic>Tuning Function, Training and Inference (</italic>
                  <xref ref-type="fig" rid="pcbi-1002211-g004">
                    <italic>Figure 4</italic>
                  </xref>
                  <italic>)</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Peakedness</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e851" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e852" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1">measured: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e853" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Base sensitivity</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e854" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e855" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1">measured: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e856" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Sensitivity contrast</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e857" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e858" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1">measured: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e859" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Training samples</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e860" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e861" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Decorrelation steps</td>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e862" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1">for contrastive divergence</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Learning rate</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e863" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e864" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">Number of neurons clamped on/off</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e865" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">4</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <sec id="s4b1">
          <title>Details to <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2:</xref> Neuron model with relative refractory mechanism</title>
          <p>The three refractory functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e866" xlink:type="simple"/></inline-formula> of panel (B) as well as all other simulation parameters are listed in <xref ref-type="table" rid="pcbi-1002211-t001">Table 1</xref>. Panel (C) shows the corresponding functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e867" xlink:type="simple"/></inline-formula>, which result from numerically solving equation (11). The spike patterns in panel (D) show the response of the neurons when the membrane potential is low (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e868" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e869" xlink:type="simple"/></inline-formula>) or high (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e870" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e871" xlink:type="simple"/></inline-formula>). These membrane potentials encode <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e872" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e873" xlink:type="simple"/></inline-formula>, respectively according to (3) and (4). The binary state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e874" xlink:type="simple"/></inline-formula> is indicated by gray shaded areas of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e875" xlink:type="simple"/></inline-formula> after each spike.</p>
        </sec>
        <sec id="s4b2">
          <title>Details to <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3:</xref> Sampling from a Boltzmann distribution by spiking neurons with relative refractory mechanism</title>
          <p>We examined the spike response of a network of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e876" xlink:type="simple"/></inline-formula> randomly connected neurons which sampled from a Boltzmann distribution. The excitabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e877" xlink:type="simple"/></inline-formula> as well as the synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e878" xlink:type="simple"/></inline-formula> were drawn from Gaussian distributions (with diagonal elements <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e879" xlink:type="simple"/></inline-formula>). For the full list of parameters please refer to <xref ref-type="table" rid="pcbi-1002211-t001">Table 1</xref>. One second of the arising spike pattern is shown in panel (A). The average firing rate of the network was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e880" xlink:type="simple"/></inline-formula>. To highlight the internal dynamics of the neuron model, the values of the refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e881" xlink:type="simple"/></inline-formula>, the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e882" xlink:type="simple"/></inline-formula> and the instantaneous firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e883" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e884" xlink:type="simple"/></inline-formula> (indicated with red spikes) are shown in panel (B). Here, the instantaneous firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e885" xlink:type="simple"/></inline-formula> is defined for the discrete time Markov chain as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e886" xlink:type="simple"/><label>(17)</label></disp-formula></p>
          <p>As stated before, the neuron model with relative refractory mechanism <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e887" xlink:type="simple"/></inline-formula> does not entail the correct overall invariant distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e888" xlink:type="simple"/></inline-formula>. To estimate the impact of this approximation on the joint network dynamics, we compared the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e889" xlink:type="simple"/></inline-formula> over five neurons (indicated by gray background in A) in the spiking network with the correct distribution obtained from Gibbs sampling. The probabilities were estimated from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e890" xlink:type="simple"/></inline-formula> samples. A more quantitative analysis of the approximation quality of neural sampling with a relative refractory mechanism is provided below.</p>
        </sec>
        <sec id="s4b3">
          <title>Details to <xref ref-type="fig" rid="pcbi-1002211-g004">Figure 4:</xref> Modeling perceptual multistability as probabilistic inference with neural sampling</title>
          <p>We demonstrate probabilistic inference and learning in a network of orientation selective neurons. As a simple model we consider a network of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e891" xlink:type="simple"/></inline-formula> neurons on a hexagonal grid as shown in panel (F). Any two neurons with distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e892" xlink:type="simple"/></inline-formula> were synaptically connected (neighboring units had distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e893" xlink:type="simple"/></inline-formula>). For the remaining parameters of the network and neuron model please refer to <xref ref-type="table" rid="pcbi-1002211-t001">Table 1</xref>. Each neuron featured a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e894" xlink:type="simple"/></inline-formula>-periodic tuning curve as depicted in panel (B):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e895" xlink:type="simple"/><label>(18)</label></disp-formula>with base sensitivity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e896" xlink:type="simple"/></inline-formula>, contrast <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e897" xlink:type="simple"/></inline-formula>, peakedness <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e898" xlink:type="simple"/></inline-formula> and preferred orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e899" xlink:type="simple"/></inline-formula>. The preferred orientations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e900" xlink:type="simple"/></inline-formula> of the neurons were chosen to cover the entire interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e901" xlink:type="simple"/></inline-formula> of possible orientations with equal spacing and were randomly assigned to the neurons.</p>
          <p>For simplicity we did not incorporate the input dynamics in our probabilistic model, but rather trained the network directly like a fully visible Boltzmann machine. We used for this purpose a standard Boltzmann machine learning rule known as contrastive divergence <xref ref-type="bibr" rid="pcbi.1002211-Hinton2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002211-Hinton4">[65]</xref>. This learning rule requires posterior samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e902" xlink:type="simple"/></inline-formula>, i.e., network states under the influence of the present input, and approximate prior samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e903" xlink:type="simple"/></inline-formula>, which reflect the probability distribution of the network in the absence of stimuli. The update rules for synaptic weights and neuronal excitabilities read:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e904" xlink:type="simple"/><label>(19)</label></disp-formula></p>
          <p>While more elaborate policies can speed up convergence, we simply used a global learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e905" xlink:type="simple"/></inline-formula> which was constant in time. The values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e906" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e907" xlink:type="simple"/></inline-formula> were initialized at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e908" xlink:type="simple"/></inline-formula>. We generated binary training patterns in the following way:</p>
          <list list-type="order">
            <list-item>
              <p>A global orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e909" xlink:type="simple"/></inline-formula> was drawn uniformly from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e910" xlink:type="simple"/></inline-formula>,</p>
            </list-item>
            <list-item>
              <p>each neuron was independently set to be active with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e911" xlink:type="simple"/></inline-formula>,</p>
            </list-item>
            <list-item>
              <p>the resulting network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e912" xlink:type="simple"/></inline-formula> was taken as posterior sample.</p>
            </list-item>
          </list>
          <p>To obtain an approximate prior sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e913" xlink:type="simple"/></inline-formula> we let the network run for a short time freely starting from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e914" xlink:type="simple"/></inline-formula>. The variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e915" xlink:type="simple"/></inline-formula> were also assumed to be observed with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e916" xlink:type="simple"/></inline-formula> iid. uniformly in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e917" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e918" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e919" xlink:type="simple"/></inline-formula> otherwise. After evolving freely for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e920" xlink:type="simple"/></inline-formula> time steps, the resulting network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e921" xlink:type="simple"/></inline-formula> was taken as approximate prior sample and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e922" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e923" xlink:type="simple"/></inline-formula> were updated according to (19). This process was repeated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e924" xlink:type="simple"/></inline-formula> times. As a result, neurons with similar preferred orientations featured excitatory synaptic connections (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e925" xlink:type="simple"/></inline-formula>  =  mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e926" xlink:type="simple"/></inline-formula> standard deviation of weight distribution), those with dissimilar orientations maintained inhibitory synapses (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e927" xlink:type="simple"/></inline-formula>). Here, preferred orientations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e928" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e929" xlink:type="simple"/></inline-formula> are defined as similar if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e930" xlink:type="simple"/></inline-formula>, otherwise they are dissimilar. Neuronal biases converged to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e931" xlink:type="simple"/></inline-formula>.</p>
          <p>We illustrate the learned prior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e932" xlink:type="simple"/></inline-formula> of the network through sampled states when the network evolved freely. As seen in panel (D), the population vector – a 2-dimensional projection of the high dimensional network state – typically reflected an arbitrary, yet coherent, orientation (for the definition of the population vector see below). Each dot represents a sampled network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e933" xlink:type="simple"/></inline-formula>.</p>
          <p>To apply an ambiguous cue, we clamped <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e934" xlink:type="simple"/></inline-formula> out of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e935" xlink:type="simple"/></inline-formula> neurons: Two units with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e936" xlink:type="simple"/></inline-formula> and two with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e937" xlink:type="simple"/></inline-formula> were set active, two units with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e938" xlink:type="simple"/></inline-formula> and two with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e939" xlink:type="simple"/></inline-formula> were set inactive. This led to a bimodal posterior distribution as shown in panel (E). The sampling network represented this distribution by encoding either global perception separately: The trace of network states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e940" xlink:type="simple"/></inline-formula> roamed in one mode for multiple steps before quickly crossing the state space towards the opposite percept.</p>
          <p>We define the population vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e941" xlink:type="simple"/></inline-formula> of a network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e942" xlink:type="simple"/></inline-formula> as a function of the preferred orientations of all active units:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e943" xlink:type="simple"/><label>(20)</label></disp-formula></p>
          <p>This definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e944" xlink:type="simple"/></inline-formula> is not based on the preferred orientations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e945" xlink:type="simple"/></inline-formula> which are used for generating external input to the network from a given stimulus with orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e946" xlink:type="simple"/></inline-formula>. It is rather based on the preferred orientations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e947" xlink:type="simple"/></inline-formula> measured from the network response. We used population vector decoding based on the measured values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e948" xlink:type="simple"/></inline-formula>, as they are conceptually closer to experimentally measurable preferred orientations, and this decoding hence does not require knowledge of the (unobservable) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e949" xlink:type="simple"/></inline-formula>. For every neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e950" xlink:type="simple"/></inline-formula> the preferred orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e951" xlink:type="simple"/></inline-formula> was measured in the following way. We estimated a tuning curve <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e952" xlink:type="simple"/></inline-formula> by a van-Mises fit (of the form (18)) to data from stimulation trials in which neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e953" xlink:type="simple"/></inline-formula> was not clamped, i.e., where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e954" xlink:type="simple"/></inline-formula> was only stimulated by recurrent input (feedforward input was modeled by clamping 8 out of 217 neurons as a function of stimulus orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e955" xlink:type="simple"/></inline-formula> as before). Due to the structured recurrent weights, the experimentally measured tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e956" xlink:type="simple"/></inline-formula> were found to be reasonably close to the tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e957" xlink:type="simple"/></inline-formula> used for external stimulation. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e958" xlink:type="simple"/></inline-formula> was set to the preferred orientation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e959" xlink:type="simple"/></inline-formula> (localization parameter of the van-Mises fit). The measured values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e960" xlink:type="simple"/></inline-formula> turned out to be consistent with the preferred orientations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e961" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e962" xlink:type="simple"/></inline-formula> averaged over all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e963" xlink:type="simple"/></inline-formula> neurons). The mean and standard deviation of the remaining parameter values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e964" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e965" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e966" xlink:type="simple"/></inline-formula> of the fitted tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e967" xlink:type="simple"/></inline-formula> are listed in <xref ref-type="table" rid="pcbi-1002211-t001">Table 1</xref> next to the ones used for stimulation.</p>
          <p>The population vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e968" xlink:type="simple"/></inline-formula> was defined in (20) with the argument <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e969" xlink:type="simple"/></inline-formula> (instead of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e970" xlink:type="simple"/></inline-formula>) as orthogonal orientations should cancel each other and neighborhood relations should be respected. For example neurons with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e971" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e972" xlink:type="simple"/></inline-formula> contribute similarly to the population vector for small <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e973" xlink:type="simple"/></inline-formula>. But counter to intuition the population vector of a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e974" xlink:type="simple"/></inline-formula> with dominant orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e975" xlink:type="simple"/></inline-formula> will point into direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e976" xlink:type="simple"/></inline-formula>. For visualization in panel (D) and (E) we therefore rescaled the population vector: If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e977" xlink:type="simple"/></inline-formula> in polar coordinates, then the dot is located at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e978" xlink:type="simple"/></inline-formula> in accord with intuition. The black semicircles equal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e979" xlink:type="simple"/></inline-formula>.</p>
          <p>The population vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e980" xlink:type="simple"/></inline-formula> was also used for measuring the dominance durations shown in panel (C). To this <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e981" xlink:type="simple"/></inline-formula> was divided into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e982" xlink:type="simple"/></inline-formula> areas: (a) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e983" xlink:type="simple"/></inline-formula>, (b) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e984" xlink:type="simple"/></inline-formula>, (c) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e985" xlink:type="simple"/></inline-formula>. We detected a perceptual switch when the network state entered area (a) or (c) while the previous perception was (c) or (a), respectively.</p>
          <p>In panel (F) neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e986" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e987" xlink:type="simple"/></inline-formula> are plotted with their preferred orientation color code, inactive neurons are displayed in white. Cells marked by a dot (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e988" xlink:type="simple"/></inline-formula>) were part of the observed variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e989" xlink:type="simple"/></inline-formula>. The three network states correspond to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e990" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e991" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e992" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e993" xlink:type="simple"/></inline-formula> in the spike pattern in panel (G). The spike pattern shows the response of the freely evolving units around a perceptual switch during sampling from the posterior distribution. The corresponding trace of the population vector is drawn as black line in panel (E). The width of the light-gray shaded areas in the spike pattern equals the PSP duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e994" xlink:type="simple"/></inline-formula>, i.e., neurons that spiked in these intervals were active in the corresponding state in (F).</p>
        </sec>
      </sec>
      <sec id="s4c">
        <title>Firing statistics of neural sampling networks</title>
        <p>In previous sections it was shown that a spiking neural network can draw samples from a given joint distribution which is in a well-defined class of probability distributions (see the neural computability condition (4)). Here, we examine some statistics of individual neurons in a sampling network which are commonly used to analyze experimental data from recordings. The spike trains and membrane potential data are taken from the simulation presented in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>.</p>
        <p><xref ref-type="fig" rid="pcbi-1002211-g005">Figure 5A,B</xref> exemplarily show the distribution of the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e995" xlink:type="simple"/></inline-formula> and the interspike interval (ISI) histogram of a single neuron, namely neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e996" xlink:type="simple"/></inline-formula> which was already considered in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3B</xref>. The responses of other neurons yield qualitatively similar statistics. The bell-shaped distribution of the membrane potential is commonly observed in neurons embedded in an active network <xref ref-type="bibr" rid="pcbi.1002211-Pospischil1">[66]</xref>. The ISI histogram reflects the reduced spiking probability immediately after an action potential due the refractory mechanism. Interspike intervals larger than the refractory time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e997" xlink:type="simple"/></inline-formula> roughly follow an exponential distribution. Similar ISI distributions were observed during in-vivo recordings in awake, behaving monkeys <xref ref-type="bibr" rid="pcbi.1002211-Shinomoto1">[67]</xref>.</p>
        <p><xref ref-type="fig" rid="pcbi-1002211-g005">Figure 5C</xref> shows a scatterplot of the coefficient of variation (CV) of the ISIs versus the average ISI for each neuron in the network. The neurons exhibited a variety of average firing rates between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e998" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e999" xlink:type="simple"/></inline-formula>. Most of the neurons responded in a highly irregular manner with a CV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1000" xlink:type="simple"/></inline-formula>. Neurons with high firing rates had a slightly lower CV due to the increased influence of the refractory mechanism The dashed line marks the CV of a Poisson process, i.e., a memoryless spiking behavior. The CV of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1001" xlink:type="simple"/></inline-formula> is marked by a cross. The structure of this plot resembles, e.g., data from recordings in behaving macaque monkeys <xref ref-type="bibr" rid="pcbi.1002211-Softky1">[68]</xref> (but note the lower average firing rate).</p>
      </sec>
      <sec id="s4d">
        <title>Approximation quality of neural sampling with different neuron and synapse models</title>
        <p>The theory of the neuron model with absolute refractory mechanism guarantees sampling form the correct distribution. In contrast, the theory for the neuron model with a relative refractory mechanism only shows that the sampling process is “locally correct”, i.e., that it would yield correct conditional distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1002" xlink:type="simple"/></inline-formula> for each individual neuron if the state of the remaining network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1003" xlink:type="simple"/></inline-formula> stayed constant. Therefore, the stationary distribution of the sampling process with relative refractory mechanism only provides an approximation to the target distribution. In the following we examine the approximation quality and robustness of sampling networks with different refractory mechanisms for target Boltzmann distributions with parameters randomly drawn from different distributions. Furthermore, we investigate the effect of additive PSP shapes with more realistic time courses.</p>
        <p>We generated target Boltzmann distributions with randomly drawn weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1004" xlink:type="simple"/></inline-formula> and biases (excitabilities) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1005" xlink:type="simple"/></inline-formula> and computed the similarity between these reference distributions and the corresponding neural sampling approximations. The setup of these simulations is the same as for the simulation presented in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>. As we aimed to compare the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1006" xlink:type="simple"/></inline-formula> sampled by the network with the exact Boltzmann distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1007" xlink:type="simple"/></inline-formula>, we reduced the number of neurons per network to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1008" xlink:type="simple"/></inline-formula>. This resulted in a state space of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1009" xlink:type="simple"/></inline-formula> possible network states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1010" xlink:type="simple"/></inline-formula> for which the normalization constant for the target Boltzmann distribution could be computed exactly. The weight matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1011" xlink:type="simple"/></inline-formula> was constraint to be symmetric with vanishing diagonal. Off-diagonal elements were drawn from zero-mean normal distributions with three different standard deviations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1012" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1013" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1014" xlink:type="simple"/></inline-formula>, whereas the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1015" xlink:type="simple"/></inline-formula> were sampled from the same distribution as in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>. For every value of the hyperparameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1016" xlink:type="simple"/></inline-formula> we generated 100 random distributions. For Boltzmann distributions with small weights (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1017" xlink:type="simple"/></inline-formula>), the RVs are nearly independent, whereas distributions with intermediate weights (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1018" xlink:type="simple"/></inline-formula>) show substantial statistical dependencies between RVs. For very large weights (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1019" xlink:type="simple"/></inline-formula>), the probability mass of the distributions is concentrated on very few states (usually 90% on less than 10 out of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1020" xlink:type="simple"/></inline-formula> states). Hence, the range of the hyperparameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1021" xlink:type="simple"/></inline-formula> considered here covers a range a very different distributions.</p>
        <p>The approximation quality of the sampled distribution was measured in terms of the Kullback-Leibler divergence between the target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1022" xlink:type="simple"/></inline-formula> and the neural approximation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1023" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1024" xlink:type="simple"/><label>(21)</label></disp-formula></p>
        <p>We estimated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1025" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1026" xlink:type="simple"/></inline-formula> samples for each simulation trial using a Laplace estimator, i.e., we added a priori <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1027" xlink:type="simple"/></inline-formula> to the number of occurrences of each state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1028" xlink:type="simple"/></inline-formula>.</p>
        <p><xref ref-type="table" rid="pcbi-1002211-t002">Table 2</xref> shows the means and the standard deviations of the Kullback-Leibler divergences between the target Boltzmann distributions and the estimated approximations stemming from neural sampling networks with three different neuron and synapse models: the exact model with absolute refractory mechanism and two models with different relative refractory mechanisms shown in the bottom and middle row in <xref ref-type="fig" rid="pcbi-1002211-g002">Figure 2B</xref>. Additionally, as a reference, we provide the (analytically calculated) Kullback-Leibler divergences for fully factorized distributions, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1029" xlink:type="simple"/></inline-formula> with correct marginals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1030" xlink:type="simple"/></inline-formula> but independent variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1031" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1032" xlink:type="simple"/></inline-formula>.</p>
        <table-wrap id="pcbi-1002211-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.t002</object-id><label>Table 2</label><caption>
            <title>Approximation quality of networks with different refractory mechanisms.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002211-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.t002" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1033" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">Absolute refractory</td>
                <td align="left" colspan="1" rowspan="1">Rel. late recovery</td>
                <td align="left" colspan="1" rowspan="1">Rel. moderate recovery</td>
                <td align="left" colspan="1" rowspan="1">Prod. of marginals</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">0.03</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1034" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1035" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1036" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1037" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">0.3</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1038" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1039" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1040" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1041" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">3.0</td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1042" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1043" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1044" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1045" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p>Mean and standard deviation of the Kullback-Leibler divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1046" xlink:type="simple"/></inline-formula> between reference Boltzmann distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1047" xlink:type="simple"/></inline-formula> and neural sampling approximations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1048" xlink:type="simple"/></inline-formula> for three different neuron models (corresponding to columns) and three different values for the reference distribution hyperparameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1049" xlink:type="simple"/></inline-formula> (corresponding to rows). The parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1050" xlink:type="simple"/></inline-formula> controls the standard deviation of the weights of the reference distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1051" xlink:type="simple"/></inline-formula>. In case of very strong synaptic interactions (leading to sharply peaked distributions, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1052" xlink:type="simple"/></inline-formula>) the approximation quality of the spiking network degrades, if the neurons feature a relative refractory mechanism. The data was computed from 100 randomly generated Boltzmann distributions and their neural approximations for each value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1053" xlink:type="simple"/></inline-formula>.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>The absolute refractory model provides the best results as we expected due to the theoretical guarantee to sample from the correct distribution (the non-zero Kullback-Leibler divergence is caused by the estimation from a finite number of samples). The models with relative refractory mechanism provide faithful approximations for all values of the hyperparameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1054" xlink:type="simple"/></inline-formula> considered here. These relative refractory models are characterized by the theory to be “locally correct” and turn out to be much more accurate approximations than fully factorized distributions if substantial statistical dependencies between the RVs are present (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1055" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1056" xlink:type="simple"/></inline-formula>). As expected, a late recovery of the refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1057" xlink:type="simple"/></inline-formula> is beneficial for the approximation quality of the model as it is closer to an absolute refractory mechanism. <xref ref-type="fig" rid="pcbi-1002211-g006">Figure 6</xref> shows the full histograms of the Kullback-Leibler divergences for the intermediate weights group (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1058" xlink:type="simple"/></inline-formula>). Systematic deviations due to the relative refractory mechanism are on the same order as the effect of estimating from finite samples (as can be seen, e.g., from a comparison with the absolute refractory model which has 0 systematic error). For completeness, we mention that the divergences of the fully factorized distributions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1059" xlink:type="simple"/></inline-formula> out of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1060" xlink:type="simple"/></inline-formula> networks with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1061" xlink:type="simple"/></inline-formula> are not shown in the plot.</p>
        <fig id="pcbi-1002211-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Comparison of neural sampling with different neuron and synapse models.</title>
            <p>The figure shows a histogram of the Kullback-Leibler divergence between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1062" xlink:type="simple"/></inline-formula> different Boltzmann distributions over K = 10 variables (with parameters randomly drawn, see setup of <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3</xref>) and approximations stemming from different neural sampling networks. Networks with absolute refractory mechanism provide the best approximation (as expected from theoretical guarantees). Networks consisting of neurons with relative refractory mechanisms, with only “locally” correct sampling, also provide a close fit to the true distribution (see inset) compared to a fully factorized approximation (assuming correct marginals and independent variables). Furthermore, it can be seen that sampling networks with more realistic, alpha-shaped, additive PSPs still fit the true distribution reasonably well.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g006" xlink:type="simple"/>
        </fig>
        <p>The theorems presented in this article assumed renewed (i.e., non-additive), rectangular PSPs. In the following we examine the effect of additive PSPs with more realistic time courses. We define additive, alpha-shaped PSPs in the following way. The influence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1063" xlink:type="simple"/></inline-formula> of each presynaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1064" xlink:type="simple"/></inline-formula> on the postsynaptic membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1065" xlink:type="simple"/></inline-formula> is modeled by convolving the input spikes with a kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1066" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1067" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1068" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1069" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1070" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1071" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1072" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1073" xlink:type="simple"/></inline-formula> are the spike times of the presynaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1074" xlink:type="simple"/></inline-formula>. The time constant governing the rising edge of the PSPs was set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1075" xlink:type="simple"/></inline-formula>. The time constant controlling the falling edge was chosen equal to the duration of rectangular PSPs, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1076" xlink:type="simple"/></inline-formula>. The scaling parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1077" xlink:type="simple"/></inline-formula> was set such that the time integral over a single PSP matches the time integral over the theoretically optimal rectangular PSP, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1078" xlink:type="simple"/></inline-formula>. These parameters display a simple and reasonable choice for the purpose of this study (an optimization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1079" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1080" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1081" xlink:type="simple"/></inline-formula> is likely to yield an improved approximation quality). <xref ref-type="fig" rid="pcbi-1002211-g007">Figure 7A</xref> shows the resulting shape of the non-rectangular PSP. Furthermore the time course of the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1082" xlink:type="simple"/></inline-formula> caused by a single spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1083" xlink:type="simple"/></inline-formula> is shown in order to illustrate that the time constants of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1084" xlink:type="simple"/></inline-formula> and of a PSP are closely related due to the assumption <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1085" xlink:type="simple"/></inline-formula> made above. Preliminary and non-exhaustive simulations seem to suggest that the choice <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1086" xlink:type="simple"/></inline-formula> yields better approximation quality than setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1087" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1088" xlink:type="simple"/></inline-formula>; however it is very well possible that a mismatch between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1089" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1090" xlink:type="simple"/></inline-formula> can be compensated for by adapting other parameters, e.g., the PSP magnitude or a specific choice of the refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1091" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1002211-g007">Figure 7B</xref> shows the results of an experiment, similar to the one presented in <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3C</xref> , with additive, alpha-shaped PSPs and relative refractory mechanism. While differences to Gibbs sampling results are visible, the spiking network still captures dependencies between the binary random variables quite well.</p>
        <fig id="pcbi-1002211-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002211.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Sampling from a Boltzmann distribution with more realistic PSP shapes.</title>
            <p>(A) The upper panel shows the shape of a single PSP elicited at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1092" xlink:type="simple"/></inline-formula>. The lower panel shows the time course of the refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1093" xlink:type="simple"/></inline-formula> caused by a single spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1094" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1095" xlink:type="simple"/></inline-formula>. The grey-shaded area of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1096" xlink:type="simple"/></inline-formula> indicates the interval of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1097" xlink:type="simple"/></inline-formula> being active (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1098" xlink:type="simple"/></inline-formula>) due to a single spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1099" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1100" xlink:type="simple"/></inline-formula>. (B) Shown is the probability distribution of 5 out of 40 neurons. The plot is similar to <xref ref-type="fig" rid="pcbi-1002211-g003">Figure 3C</xref>, however it is generated with a sampling network that features alpha-shaped, additive PSPs. It can be seen that the network still produces a reasonable approximation to the true Boltzmann distribution (determined by Gibbs sampling).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002211.g007" xlink:type="simple"/>
        </fig>
        <p>For a quantitative analysis of the approximation quality, we repeated the experiment of <xref ref-type="fig" rid="pcbi-1002211-g006">Figure 6</xref> with additive, alpha-shaped PSPs (shown as green bars). The Kullback-Leibler divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1101" xlink:type="simple"/></inline-formula> to the true distribution is clearly higher compared to the case of renewed, rectangular PSPs. Still networks with this more realistic synapse model account for dependencies between the random variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1102" xlink:type="simple"/></inline-formula> and yield a better approximation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002211.e1103" xlink:type="simple"/></inline-formula> than fully factorized distributions.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank Mihai Petrovici, Robert Legenstein and Samuel Gershman for helpful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002211-Rolls1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name><name name-style="western"><surname>Deco</surname><given-names>G</given-names></name></person-group>             <year>2010</year>             <article-title>The Noisy Brain: Stochastic Dynamics as a Principle of Brain Function.</article-title>             <publisher-name>Oxford University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Cannon1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cannon</surname><given-names>R</given-names></name><name name-style="western"><surname>O’Donnell</surname><given-names>C</given-names></name><name name-style="western"><surname>Nolan</surname><given-names>M</given-names></name></person-group>             <year>2010</year>             <article-title>Stochastic ion channel gating in dendritic neurons: morphology dependence and probabilistic synaptic activation of dendritic spikes.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <fpage>e1000886</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Flight1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Flight</surname><given-names>M</given-names></name></person-group>             <year>2010</year>             <article-title>Synaptic transmission: On the probability of release.</article-title>             <source>Nat Rev Neurosci</source>             <volume>9</volume>             <fpage>736</fpage>             <lpage>737</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Azouz1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Azouz</surname><given-names>R</given-names></name><name name-style="western"><surname>Gray</surname><given-names>CM</given-names></name></person-group>             <year>1999</year>             <article-title>Cellular mechanisms contributing to response variability of cortical neurons in vivo.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>2209</fpage>             <lpage>2223</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-GerstnerW1">
        <label>5</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>GerstnerW</surname></name><name name-style="western"><surname>KistlerWM</surname></name></person-group>             <year>2002</year>             <article-title>Spiking Neuron Models.</article-title>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Brascamp1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brascamp</surname><given-names>JW</given-names></name><name name-style="western"><surname>van Ee</surname><given-names>R</given-names></name><name name-style="western"><surname>Noest</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Jacobs</surname><given-names>RHAH</given-names></name><name name-style="western"><surname>van den Berg</surname><given-names>AV</given-names></name></person-group>             <year>2006</year>             <article-title>The time course of binocular rivalry reveals a fundamental role of noise.</article-title>             <source>J Vis</source>             <volume>6</volume>             <fpage>1244</fpage>             <lpage>1256</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Fiser1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name><name name-style="western"><surname>Chiu</surname><given-names>C</given-names></name><name name-style="western"><surname>Weliky</surname><given-names>M</given-names></name></person-group>             <year>2004</year>             <article-title>Small modulation of ongoing cortical dynamics by sensory input during natural vision.</article-title>             <source>Nature</source>             <volume>431</volume>             <fpage>573</fpage>             <lpage>583</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Ringach1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ringach</surname><given-names>DL</given-names></name></person-group>             <year>2009</year>             <article-title>Spontaneous and driven cortical activity: implications for computation.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>19</volume>             <fpage>1</fpage>             <lpage>6</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Geman1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Geman</surname><given-names>S</given-names></name><name name-style="western"><surname>Geman</surname><given-names>D</given-names></name></person-group>             <year>1984</year>             <article-title>Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.</article-title>             <source>IEEE Trans Pattern Anal Mach Intell</source>             <volume>6</volume>             <fpage>721</fpage>             <lpage>741</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Rao1">
        <label>10</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <article-title>Probabilistic Models of the Brain.</article-title>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Doya1">
        <label>11</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>S</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <year>2007</year>             <article-title>Bayesian Brain: Probabilistic Approaches to Neural Coding.</article-title>             <publisher-name>MIT-Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Krding1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name><name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name></person-group>             <year>2004</year>             <article-title>Bayesian integration in sensorimotor learning.</article-title>             <source>Nature</source>             <volume>427</volume>             <fpage>244</fpage>             <lpage>247</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Kersten1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kersten</surname><given-names>D</given-names></name><name name-style="western"><surname>Mamassian</surname><given-names>P</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A</given-names></name></person-group>             <year>2004</year>             <article-title>Object perception as Bayesian inference.</article-title>             <source>Annu Rev Psychol</source>             <volume>55</volume>             <fpage>271</fpage>             <lpage>304</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Gopnik1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gopnik</surname><given-names>A</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group>             <year>2007</year>             <article-title>Bayesian special section: Introduction; Bayesian networks, Bayesian learning and cognitive development.</article-title>             <source>Dev Sci</source>             <volume>10</volume>             <fpage>281</fpage>             <lpage>287</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Lee1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name><name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>Hierarchical Bayesian inference in the visual cortex.</article-title>             <source>J Opt Soc Am A</source>             <volume>20</volume>             <fpage>1434</fpage>             <lpage>1448</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Hoyer1">
        <label>16</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hoyer</surname><given-names>P</given-names></name><name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Interpreting neural response variability as Monte Carlo sampling of the posterior.</article-title>             <source>Proceedings of the 16th Conference on Advances in Neural Information Processing Systems; December 2002</source>             <publisher-loc>Vancouver, Canada</publisher-loc>             <comment>NIPS 2002</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Sundareswara1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sundareswara</surname><given-names>R</given-names></name><name name-style="western"><surname>Schrater</surname><given-names>PR</given-names></name></person-group>             <year>2008</year>             <article-title>Perceptual multistability predicted by search model for bayesian decisions.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>1</fpage>             <lpage>19</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Gershman1">
        <label>18</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Vul</surname><given-names>E</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>Perceptual multistability as Markov chain Monte Carlo inference.</article-title>             <source>Proceedings of the 22nd Conference on Advances in Neural Information Processing Systems; December 2008</source>             <publisher-loc>Vancouver, Canada</publisher-loc>             <comment>NIPS 2008</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Griffiths1">
        <label>19</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group>             <year>2008</year>             <article-title>Bayesian models of cognition.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Sun</surname><given-names>R</given-names></name></person-group>             <source>Handbook of Computational Cognitive Modeling</source>             <publisher-name>Cambridge University Press</publisher-name>             <fpage>59</fpage>             <lpage>100</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Yang1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>T</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2007</year>             <article-title>Probabilistic reasoning by neurons.</article-title>             <source>Nature</source>             <volume>447</volume>             <fpage>1075</fpage>             <lpage>1080</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Gold1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2007</year>             <article-title>The neural basis of decision making.</article-title>             <source>Annu Rev Neurosci</source>             <volume>30</volume>             <fpage>535</fpage>             <lpage>574</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Sadaghiani1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sadaghiani</surname><given-names>S</given-names></name><name name-style="western"><surname>Hesselmann</surname><given-names>G</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Kleinschmidt</surname><given-names>A</given-names></name></person-group>             <year>2010</year>             <article-title>The relation of ongoing brain activity, evoked neural responses, and cognition.</article-title>             <source>Front Syst Neurosci 4: Artikel</source>             <volume>20</volume>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Pearl1">
        <label>23</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J</given-names></name></person-group>             <year>1988</year>             <article-title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</article-title>             <publisher-name>Morgan Kaufmann</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Friston1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group>             <year>2010</year>             <article-title>Action and behavior: a free-energy formulation.</article-title>             <source>Biol Cybern</source>             <volume>102</volume>             <fpage>227</fpage>             <lpage>260</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Toussaint1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Toussaint</surname><given-names>M</given-names></name></person-group>             <year>2009</year>             <article-title>Probabilistic inference as a model of planned behavior.</article-title>             <source>Künstliche Intelligenz</source>             <volume>3</volume>             <fpage>23</fpage>             <lpage>29</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Toussaint2">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Toussaint</surname><given-names>M</given-names></name><name name-style="western"><surname>Goerick</surname><given-names>C</given-names></name></person-group>             <year>2010</year>             <article-title>A Bayesian view on motor control and planning.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Sigaud</surname><given-names>O</given-names></name><name name-style="western"><surname>Peters</surname><given-names>J</given-names></name></person-group>             <source>From motor to interaction learning in robots</source>             <publisher-name>Studies in Computational Intelligence. Springer</publisher-name>             <fpage>227</fpage>             <lpage>252</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Tenenbaum1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name></person-group>             <year>2006</year>             <article-title>Theory-based bayesian models of inductive learning and reasoning.</article-title>             <source>Trends Cogn Sci</source>             <volume>10</volume>             <fpage>309</fpage>             <lpage>318</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Griffiths2">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group>             <year>2006</year>             <article-title>Optimal predictions in everyday cognition.</article-title>             <source>Psychol Sci</source>             <volume>17</volume>             <fpage>767</fpage>             <lpage>773</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Oaksford1">
        <label>29</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Oaksford</surname><given-names>M</given-names></name><name name-style="western"><surname>Chater</surname><given-names>N</given-names></name></person-group>             <year>2007</year>             <article-title>Bayesian Rationality: The Probabilistic Approach to Human Reasoning.</article-title>             <publisher-name>Oxford University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Rao2">
        <label>30</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <year>2007</year>             <article-title>Neural models of Bayesian belief propagation.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>S</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <source>Bayesian Brain</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT-Press</publisher-name>             <fpage>239</fpage>             <lpage>267</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Steimer1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Steimer</surname><given-names>A</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Belief-propagation in networks of spiking neurons.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>2502</fpage>             <lpage>2523</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Deneve1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name></person-group>             <year>2008</year>             <article-title>Bayesian spiking neurons I: Inference.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>91</fpage>             <lpage>117</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Litvak1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Litvak</surname><given-names>S</given-names></name><name name-style="western"><surname>Ullman</surname><given-names>S</given-names></name></person-group>             <year>2009</year>             <article-title>Cortical circuitry implementing graphical models.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>1</fpage>             <lpage>47</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Sahani1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name></person-group>             <year>2003</year>             <article-title>Doubly distributional population codes: Simultaneous representation of uncertainty and multiplicity.</article-title>             <source>Neural Comput</source>             <volume>15</volume>             <fpage>2255</fpage>             <lpage>2279</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Fiser2">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name><name name-style="western"><surname>Orbn</surname><given-names>G</given-names></name><name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name></person-group>             <year>2010</year>             <article-title>Statistically optimal perception and learning: from behavior to neural representations.</article-title>             <source>Trends Cogn Sci</source>             <volume>14</volume>             <fpage>119</fpage>             <lpage>130</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Ackley1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ackley</surname><given-names>DH</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>1985</year>             <article-title>A learning algorithm for boltzmann machines.</article-title>             <source>Cogn Sci</source>             <volume>9</volume>             <fpage>147</fpage>             <lpage>169</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Hinton1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Osindero</surname><given-names>S</given-names></name><name name-style="western"><surname>Teh</surname><given-names>YW</given-names></name></person-group>             <year>2006</year>             <article-title>A fast learning algorithm for deep belief nets.</article-title>             <source>Neural Comput</source>             <volume>18</volume>             <fpage>1527</fpage>             <lpage>1554</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Andrieu1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Andrieu</surname><given-names>C</given-names></name><name name-style="western"><surname>Freitas</surname><given-names>ND</given-names></name><name name-style="western"><surname>Doucet</surname><given-names>A</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name></person-group>             <year>2003</year>             <article-title>An introduction to MCMC for machine learning.</article-title>             <source>Mach Learn</source>             <volume>50</volume>             <fpage>5</fpage>             <lpage>43</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Grimmett1">
        <label>39</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Grimmett</surname><given-names>GR</given-names></name><name name-style="western"><surname>Stirzaker</surname><given-names>DR</given-names></name></person-group>             <year>2001</year>             <article-title>Probability and Random Processes.</article-title>             <publisher-name>Oxford University Press, 3rd edition</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Nessler1">
        <label>40</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name><name name-style="western"><surname>Pfeiffer</surname><given-names>M</given-names></name><name name-style="western"><surname>MaassW</surname></name></person-group>             <year>2009</year>             <article-title>Hebbian learning of Bayes optimal decisions.</article-title>             <source>Proceedings of the 21th Conference on Advances in Neural Information Processing Systems; December 2008</source>             <publisher-loc>Vancouver, Canada</publisher-loc>             <comment>NIPS 2008</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Hinton2">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group>             <year>2010</year>             <article-title>Learning to represent visual input.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>365</volume>             <fpage>177</fpage>             <lpage>184</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Pillow1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name><name name-style="western"><surname>Sher</surname><given-names>A</given-names></name><name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population.</article-title>             <source>Nature</source>             <volume>454</volume>             <fpage>995</fpage>             <lpage>999</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Gardiner1">
        <label>43</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gardiner</surname><given-names>C</given-names></name></person-group>             <year>2004</year>             <article-title>Handbook of Stochastic Methods.</article-title>             <publisher-name>3rd ed. Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Leopold1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Leopold</surname><given-names>DA</given-names></name><name name-style="western"><surname>Wilke</surname><given-names>M</given-names></name><name name-style="western"><surname>Maier</surname><given-names>A</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2002</year>             <article-title>Stable perception of visually ambiguous patterns.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <fpage>605</fpage>             <lpage>609</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Blake1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Blake</surname><given-names>R</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2002</year>             <article-title>Visual competition.</article-title>             <source>Nat Rev Neurosci</source>             <volume>3</volume>             <fpage>13</fpage>             <lpage>21</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Alais1">
        <label>46</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alais</surname><given-names>D</given-names></name><name name-style="western"><surname>Blake</surname><given-names>R</given-names></name></person-group>             <year>2005</year>             <article-title>Binocular Rivalry.</article-title>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Bartels1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bartels</surname><given-names>A</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2010</year>             <article-title>Binocular rivalry: a time dependence of eye and stimulus contributions.</article-title>             <source>J Vis</source>             <volume>10</volume>             <fpage>3</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Binzegger1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Binzegger</surname><given-names>T</given-names></name><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name></person-group>             <year>2004</year>             <article-title>A quantitative map of the circuit of cat primary visual cortex.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>8441</fpage>             <lpage>8453</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Binzegger2">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Binzegger</surname><given-names>T</given-names></name><name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name><name name-style="western"><surname>Martin</surname><given-names>K</given-names></name></person-group>             <year>2009</year>             <article-title>Topology and dynamics of the canonical circuit of cat V1.</article-title>             <source>Neural Netw</source>             <volume>22</volume>             <fpage>1071</fpage>             <lpage>1078</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Churchland1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Churchland</surname><given-names>MM</given-names></name><name name-style="western"><surname>Yu</surname><given-names>BM</given-names></name><name name-style="western"><surname>Cunningham</surname><given-names>JP</given-names></name><name name-style="western"><surname>Sugrue</surname><given-names>LP</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>MR</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon.</article-title>             <source>Nat Neurosci</source>             <volume>13</volume>             <fpage>369</fpage>             <lpage>378</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Berkes1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name><name name-style="western"><surname>Orbán</surname><given-names>G</given-names></name><name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name></person-group>             <year>2011</year>             <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment.</article-title>             <source>Science</source>             <volume>331</volume>             <fpage>83</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Bishop1">
        <label>52</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>CM</given-names></name></person-group>             <year>2006</year>             <article-title>Pattern Recognition and Machine Learning.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Koller1">
        <label>53</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koller</surname><given-names>D</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name></person-group>             <year>2009</year>             <article-title>Probabilistic Graphical Models: Principles and Techniques.</article-title>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Hopfield1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name></person-group>             <year>1985</year>             <article-title>“Neural” computation of decisions in optimization problems.</article-title>             <source>Biol Cybern</source>             <volume>52</volume>             <fpage>141</fpage>             <lpage>152</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Vul1">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vul</surname><given-names>E</given-names></name><name name-style="western"><surname>Pashler</surname><given-names>H</given-names></name></person-group>             <year>2008</year>             <article-title>Measuring the crowd within: Probabilistic representations within individuals.</article-title>             <source>Psychol Sci</source>             <volume>19</volume>             <fpage>645</fpage>             <lpage>647</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Denison1">
        <label>56</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Denison</surname><given-names>S</given-names></name><name name-style="western"><surname>Bonawitz</surname><given-names>E</given-names></name><name name-style="western"><surname>Gopnik</surname><given-names>A</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name></person-group>             <year>2009</year>             <article-title>Preschoolers sample from probability distributions.</article-title>             <source>Proceedings of the 32nd Annual Conference of the Cognitive Science Society; 29 July - 1 August 2009</source>             <publisher-loc>Amsterdam, Netherlands</publisher-loc>             <comment>CogSci 2009</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Hinton3">
        <label>57</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name><name name-style="western"><surname>Brown</surname><given-names>A</given-names></name></person-group>             <year>2000</year>             <article-title>Spiking Boltzmann machines.</article-title>             <source>Proceedings of the 13th Conference on Advances in Neural Information Processing Systems; December 1999</source>             <publisher-loc>Vancouver, Canada</publisher-loc>             <publisher-name>NIPS 1999</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Zemel1">
        <label>58</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zemel</surname><given-names>R</given-names></name><name name-style="western"><surname>Huys</surname><given-names>QJM</given-names></name><name name-style="western"><surname>Natarajan</surname><given-names>R</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name></person-group>             <year>2005</year>             <article-title>Probabilistic computation in spiking populations.</article-title>             <source>Proceedings of the 17th Conference on Advances in Neural Information Processing Systems; December 2004</source>             <publisher-loc>Vancouver, Canada</publisher-loc>             <publisher-name>NIPS 2004</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Kenet1">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name><name name-style="western"><surname>Bibitchkov</surname><given-names>D</given-names></name><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name><name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name><name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Spontaneously emerging cortical representations of visual attributes.</article-title>             <source>Nature</source>             <volume>425</volume>             <fpage>954</fpage>             <lpage>956</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Fox1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>MD</given-names></name><name name-style="western"><surname>Raichle</surname><given-names>ME</given-names></name></person-group>             <year>2007</year>             <article-title>Spontaneous fluctuations in brain activity observed with functional magnetic resonance imaging.</article-title>             <source>Nat Rev Neurosci</source>             <volume>8</volume>             <fpage>700</fpage>             <lpage>711</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Antic1">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Antic</surname><given-names>SD</given-names></name><name name-style="western"><surname>ZhouWL</surname></name><name name-style="western"><surname>Moore</surname><given-names>AR</given-names></name><name name-style="western"><surname>Short</surname><given-names>SM</given-names></name><name name-style="western"><surname>Ikonomu</surname><given-names>KD</given-names></name></person-group>             <year>2010</year>             <article-title>The decade of the dendritic NMDA spike.</article-title>             <source>J Neurosci Res</source>             <volume>88</volume>             <fpage>2991</fpage>             <lpage>3001</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Merolla1">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Merolla</surname><given-names>P</given-names></name><name name-style="western"><surname>Arthur</surname><given-names>J</given-names></name><name name-style="western"><surname>Shi</surname><given-names>BE</given-names></name><name name-style="western"><surname>Boahen</surname><given-names>K</given-names></name></person-group>             <year>2007</year>             <article-title>Expandable networks for neuromorphic chips.</article-title>             <source>IEEE Trans Circuits Syst I Regul Pap</source>             <volume>54</volume>             <fpage>301</fpage>             <lpage>311</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Bruederle1">
        <label>63</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bruederle</surname><given-names>D</given-names></name><name name-style="western"><surname>Bill</surname><given-names>J</given-names></name><name name-style="western"><surname>Kaplan</surname><given-names>B</given-names></name><name name-style="western"><surname>Kremkow</surname><given-names>J</given-names></name><name name-style="western"><surname>Meier</surname><given-names>K</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Live demonstration: Simulatorlike exploration of cortical network architectures with a mixed-signal VLSI system.</article-title>             <source>Proceedings of the IEEE International Symposium on Circuits and Systems; 30May - 2 June 2010</source>             <publisher-loc>Paris, France</publisher-loc>             <publisher-name>ISCAS 2010</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Python1">
        <label>64</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Python</surname></name></person-group>             <year>2011</year>             <comment>The python language reference. Available: <ext-link ext-link-type="uri" xlink:href="http://docs.python.org/reference/" xlink:type="simple">http://docs.python.org/reference/</ext-link>.</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Hinton4">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group>             <year>2002</year>             <article-title>Training products of experts by minimizing contrastive divergence.</article-title>             <source>Neural Comput</source>             <volume>14</volume>             <fpage>1771</fpage>             <lpage>1800</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Pospischil1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pospischil</surname><given-names>M</given-names></name><name name-style="western"><surname>Piwkowska</surname><given-names>Z</given-names></name><name name-style="western"><surname>Bal</surname><given-names>T</given-names></name><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name></person-group>             <year>2009</year>             <article-title>Characterizing neuronal activity by describing the membrane potential as a stochastic process.</article-title>             <source>J Physiol Paris</source>             <volume>103</volume>             <fpage>98</fpage>             <lpage>106</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Shinomoto1">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shinomoto</surname><given-names>S</given-names></name><name name-style="western"><surname>Kim</surname><given-names>H</given-names></name><name name-style="western"><surname>Shimokawa</surname><given-names>T</given-names></name><name name-style="western"><surname>Matsuno</surname><given-names>N</given-names></name><name name-style="western"><surname>Funahashi</surname><given-names>S</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>Relating neuronal firing patterns to functional differentiation of cerebral cortex.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000433</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002211-Softky1">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Softky</surname><given-names>W</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name></person-group>             <year>1993</year>             <article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random epsps.</article-title>             <source>J Neurosci</source>             <volume>13</volume>             <fpage>334</fpage>             <lpage>350</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>