<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
    <front>
        <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
                <publisher-name>Public Library of Science</publisher-name>
                <publisher-loc>San Francisco, USA</publisher-loc>
            </publisher></journal-meta>
        <article-meta><article-id pub-id-type="publisher-id">08-PLCB-RA-0513R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000211</article-id><article-categories>
                <subj-group subj-group-type="heading">
                    <subject>Research Article</subject>
                </subj-group>
                <subj-group subj-group-type="Discipline">
                    <subject>Mathematics/Statistics</subject>
                    <subject>Neuroscience</subject>
                    <subject>Neuroscience/Theoretical Neuroscience</subject>
                </subj-group>
            </article-categories><title-group><article-title>Hierarchical Models in the Brain</article-title><alt-title alt-title-type="running-head">Hierarchical Models</alt-title></title-group><contrib-group>
                <contrib contrib-type="author" xlink:type="simple">
                    <name name-style="western">
                        <surname>Friston</surname>
                        <given-names>Karl</given-names>
                    </name>
                    <xref ref-type="aff" rid="aff1"/>
                    <xref ref-type="corresp" rid="cor1">
                        <sup>*</sup>
                    </xref>
                </contrib>
            </contrib-group><aff id="aff1">
                <addr-line>The Wellcome Trust Centre of Neuroimaging, University College London,
                    London, United Kingdom</addr-line>
            </aff><contrib-group>
                <contrib contrib-type="editor" xlink:type="simple">
                    <name name-style="western">
                        <surname>Sporns</surname>
                        <given-names>Olaf</given-names>
                    </name>
                    <role>Editor</role>
                    <xref ref-type="aff" rid="edit1"/>
                </contrib>
            </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
                <corresp id="cor1">* E-mail: <email xlink:type="simple">k.friston@fil.ion.ucl.ac.uk</email></corresp>
                <fn fn-type="con">
                    <p>Conceived and designed the experiments: KJF. Performed the experiments: KJF.
                        Analyzed the data: KJF. Contributed reagents/materials/analysis tools: KJF.
                        Wrote the paper: KJF.</p>
                </fn>
            <fn fn-type="conflict">
                <p>The author has declared that no competing interests exist.</p>
            </fn></author-notes><pub-date pub-type="collection">
                <month>11</month>
                <year>2008</year>
            </pub-date><pub-date pub-type="epub">
                <day>7</day>
                <month>11</month>
                <year>2008</year>
            </pub-date><volume>4</volume><issue>11</issue><elocation-id>e1000211</elocation-id><history>
                <date date-type="received">
                    <day>30</day>
                    <month>6</month>
                    <year>2008</year>
                </date>
                <date date-type="accepted">
                    <day>19</day>
                    <month>9</month>
                    <year>2008</year>
                </date>
            </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Karl Friston</copyright-holder><license><license-p>This is an open-access article distributed under the
                terms of the Creative Commons Attribution License, which permits unrestricted use,
                distribution, and reproduction in any medium, provided the original author and
                source are credited.</license-p></license></permissions><abstract>
                <p>This paper describes a general model that subsumes many parametric models for
                    continuous data. The model comprises hidden layers of state-space or dynamic
                    causal models, arranged so that the output of one provides input to another. The
                    ensuing hierarchy furnishes a model for many types of data, of arbitrary
                    complexity. Special cases range from the general linear model for static data to
                    generalised convolution models, with system noise, for nonlinear time-series
                    analysis. Crucially, all of these models can be inverted using exactly the same
                    scheme, namely, dynamic expectation maximization. This means that a single model
                    and optimisation scheme can be used to invert a wide range of models. We present
                    the model and a brief review of its inversion to disclose the relationships
                    among, apparently, diverse generative models of empirical data. We then show
                    that this inversion can be formulated as a simple neural network and may provide
                    a useful metaphor for inference and learning in the brain.</p>
            </abstract><abstract abstract-type="summary">
                <title>Author Summary</title>
                <p>Models are essential to make sense of scientific data, but they may also play a
                    central role in how we assimilate sensory information. In this paper, we
                    introduce a general model that generates or predicts diverse sorts of data. As
                    such, it subsumes many common models used in data analysis and statistical
                    testing. We show that this model can be fitted to data using a single and
                    generic procedure, which means we can place a large array of data analysis
                    procedures within the same unifying framework. Critically, we then show that the
                    brain has, in principle, the machinery to implement this scheme. This suggests
                    that the brain has the capacity to analyse sensory input using the most
                    sophisticated algorithms currently employed by scientists and possibly models
                    that are even more elaborate. The implications of this work are that we can
                    understand the structure and function of the brain as an inference machine.
                    Furthermore, we can ascribe various aspects of brain anatomy and physiology to
                    specific computational quantities, which may help understand both normal brain
                    function and how aberrant inferences result from pathological processes
                    associated with psychiatric disorders.</p>
            </abstract><funding-group><funding-statement>This work was supported by the Wellcome Trust.</funding-statement></funding-group><counts>
                <page-count count="24"/>
            </counts></article-meta>
    </front>
    <body>
        <sec id="s1">
            <title>Introduction</title>
            <p>This paper describes hierarchical dynamic models (HDMs) and reviews a generic
                variational scheme for their inversion. We then show that the brain has evolved the
                necessary anatomical and physiological equipment to implement this inversion, given
                sensory data. These models are general in the sense that they subsume simpler
                variants, such as those used in independent component analysis, through to
                generalised nonlinear convolution models. The generality of HDMs renders the
                inversion scheme a useful framework that covers procedures ranging from variance
                component estimation, in classical linear observation models, to blind
                deconvolution, using exactly the same formalism and operational equations.
                Critically, the nature of the inversion lends itself to a relatively simple neural
                network implementation that shares many formal similarities with real cortical
                hierarchies in the brain.</p>
            <p>Recently, we introduced a variational scheme for model inversion (i.e., inference on
                models and their parameters given data) that considers hidden states in generalised
                coordinates of motion. This enabled us to derive estimation procedures that go
                beyond conventional approaches to time-series analysis, like Kalman or particle
                filtering. We have described two versions; variational filtering <xref ref-type="bibr" rid="pcbi.1000211-Friston1">[1]</xref> and
                dynamic expectation maximisation (<bold>DEM</bold>; <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref>) that use free and
                fixed-form approximations to the posterior or conditional density respectively. In
                these papers, we used hierarchical dynamic models to illustrate how the schemes
                worked in practice. In this paper, we focus on the model <italic>per se</italic> and
                the relationships among its special cases. We will use <bold>DEM</bold> to show how
                their inversion relates to conventional treatments of these special cases.</p>
            <p>A key aspect of <bold>DEM</bold> is that it was developed with neuronal
                implementation in mind. This constraint can be viewed as formulating a neuronally
                inspired estimation and inference framework or conversely, as providing heuristics
                that may inform our understanding of neuronal processing. The basic ideas have
                already been described, in the context of static models, in a series of papers <xref ref-type="bibr" rid="pcbi.1000211-Friston3">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1000211-Friston5">[5]</xref> that entertain the
                notion that the brain may use empirical Bayes for inference about its sensory input,
                given the hierarchical organisation of cortical systems. In this paper, we
                generalise this idea to cover hierarchical dynamical systems and consider how neural
                networks could be configured to invert HDMs and deconvolve sensory causes from
                sensory input.</p>
            <p>This paper comprises five sections. In the first, we introduce hierarchical dynamic
                models. These cover many observation or generative models encountered in the
                estimation and inference literature. An important aspect of these models is their
                formulation in generalised coordinates of motion; this lends them a hierarchal form
                in both structure and dynamics. These hierarchies induce empirical priors that
                provide structural and dynamic constraints, which can be exploited during inversion.
                In the second and third sections, we consider model inversion in general terms and
                then specifically, using dynamic expectation maximisation (<bold>DEM</bold>). This
                reprises the material in Friston et al. <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref> with a special focus on
                HDMs. <bold>DEM</bold> is effectively a variational or ensemble learning scheme that
                optimises the conditional density on model states (<bold>D</bold>-step), parameters
                    (<bold>E</bold>-step) and hyperparameters (<bold>M</bold>-step). It can also be
                regarded as a generalisation of expectation maximisation (<bold>EM</bold>), which
                entails the introduction of a deconvolution or <bold>D</bold>-step to estimate
                time-dependent states. In the fourth section, we review a series of HDMs that
                correspond to established models used for estimation, system identification and
                learning. Their inversion is illustrated with worked-examples using
                <bold>DEM</bold>. In the final section, we revisit the <bold>DEM</bold> steps and
                show how they can be formulated as a simple gradient ascent using neural networks
                and consider how evoked brain responses might be understood in terms of inference
                under hierarchical dynamic models of sensory input.</p>
            <sec id="s1a">
                <title>Notation</title>
                <p>To simplify notation we will use
                            <italic>f<sub>x</sub></italic>: = <italic>f<sub>x</sub></italic>(<italic>x</italic>) = ∂<italic><sub>x</sub>f</italic> = ∂<italic>f</italic>/∂<italic>x</italic>
                    to denote the partial derivative of the function, <italic>f</italic>, with
                    respect to the variable <italic>x</italic>. We also use <italic>x</italic>
                            ˙ = ∂<italic><sub>t</sub>x</italic>
                    for temporal derivatives. Furthermore, we will be dealing with variables in
                    generalised coordinates of motion, which will be denoted by a tilde;
                        <italic>x̃</italic>: = [<italic>x</italic>,<italic>x</italic>′,<italic>x</italic>″,…]<italic>
                        <sup>T</sup>
                    </italic> = [<italic>x</italic><sup>[0]</sup>,<italic>x</italic><sup>[1]</sup>,<italic>x</italic><sup>[2]</sup>,…]<italic>
                        <sup>T</sup>
                    </italic>, where
                    <italic>x</italic><sup>[<italic>i</italic>]</sup> denotes
                        <italic>i</italic>th order motion. A point in generalised coordinates can be
                    regarded as encoding the instantaneous trajectory of a variable, in the sense it
                    prescribes its location, velocity, acceleration etc.</p>
            </sec>
        </sec>
        <sec id="s2">
            <title>Materials and Methods</title>
            <sec id="s2a">
                <title>Hierarchical Dynamic Models</title>
                <p>In this section, we cover hierarchal models for dynamic systems. We start with
                    the basic model and how generalised motion furnishes empirical priors on the
                    dynamics of the model's hidden states. We then consider hierarchical
                    forms and see how these induce empirical priors in a structural sense. We will
                    try to relate these perspectives to established treatments of empirical priors
                    in static and state-space models.</p>
                <sec id="s2a1">
                    <title>Hierarchical dynamic causal models</title>
                    <p>Dynamic causal models are probabilistic generative models
                            <italic>p</italic>(<italic>y</italic>,<italic>ϑ</italic>)
                        based on state-space models. As such, they entail the likelihood,
                            <italic>p</italic>(<italic>y</italic>|<italic>ϑ</italic>) of
                        getting some data, <italic>y</italic>, given some parameters
                            <italic>ϑ</italic> = {<italic>x</italic>,<italic>v</italic>,<italic>θ</italic>,<italic>λ</italic>}
                        and priors on those parameters,
                        <italic>p</italic>(<italic>ϑ</italic>). We will see that the
                        parameters subsume different quantities, some of which change with time and
                        some which do not. These models are causal in a control-theory sense because
                        they are state-space models, formulated in continuous time.</p>
                </sec>
                <sec id="s2a2">
                    <title>State-pace models in generalised coordinates</title>
                    <p>A dynamic input-state-output model can be written as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e001" xlink:type="simple"/><label>(1)</label></disp-formula>The continuous nonlinear functions <italic>f</italic> and
                            <italic>g</italic> of the states are parameterised by
                        <italic>θ</italic>. The states
                        <italic>v</italic>(<italic>t</italic>) can be deterministic, stochastic, or
                        both. They are variously referred to as inputs, sources or causes. The
                        states <italic>x</italic>(<italic>t</italic>) meditate the influence of the
                        input on the output and endow the system with memory. They are often
                        referred to as hidden states because they are seldom observed directly. We
                        assume the stochastic terms (i.e., observation noise)
                            <italic>z</italic>(<italic>t</italic>) are analytic, such that the
                        covariance of
                            <italic>z̃</italic> = [<italic>z</italic>,<italic>z</italic>′,<italic>z</italic>″,…]<italic>
                            <sup>T</sup>
                        </italic> is well defined; similarly for the system or state noise,
                            <italic>w</italic>(<italic>t</italic>), which represents random
                        fluctuations on the motion of the hidden states. Under local linearity
                        assumptions (i.e., ignoring high-order derivatives of the generative model
                        functions), the generalised output or response
                            <italic>ỹ</italic> = [<italic>y</italic>,<italic>y</italic>′,<italic>y</italic>″,…]<italic>
                            <sup>T</sup>
                        </italic> obtains from recursive differentiation with respect to time using
                        the chain rule<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e002" xlink:type="simple"/><label>(2)</label></disp-formula>Note that the derivatives are evaluated at each point in time
                        and the linear approximation is local to the current state. The first
                        (observer) equation show that the generalised states
                            <italic>u</italic> = [<italic>ṽ</italic>,<italic>x̃</italic>,]<italic>
                            <sup>T</sup>
                        </italic> are needed to generate a generalised response that encodes a path
                        or trajectory. The second (state) equations enforce a coupling between
                        neighbouring orders of motion of the hidden states and confer memory on the
                        system.</p>
                    <p>At this point, readers familiar with standard state-space models may be
                        wondering where all the extra equations in Equation 2 come from and, in
                        particular, what the generalised motions; <italic>w</italic>′,
                            <italic>w</italic>″, … represent. These terms always
                        exist but are ignored in standard treatments based on the theory of
                        Markovian processes <xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>. This is because standard Markovian (c.f.,
                        Wiener) processes have generalised motion that has infinite variance and are
                        infinitely ‘jagged’ or rough. This means
                        <italic>w</italic>′, <italic>w</italic>″, … and
                            <italic>x</italic>″, <italic>x</italic>‴,
                        … have no precision (inverse variance) and can be ignored with
                        impunity. It is important to realise that this approximation is not
                        appropriate for real or actual fluctuations, as noted at the inception of
                        the standard theory; “a certain care must be taken in replacing an
                        actual process by Markov process, since Markov processes have many special
                        features, and, in particular, differ from the processes encountered in radio
                        engineering by their lack of smoothness… any random process
                        actually encountered in radio engineering is analytic, and all its
                        derivative are finite with probability one” (<xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, pp
                        122–124). So why have standard state-space models, and their
                        attending inversion schemes like Kalman filtering, dominated the literature
                        over the past half-century? Partly because it is convenient to ignore
                        generalised motion and partly because they furnish reasonable approximations
                        to fluctuations over time-scales that exceed the correlation time of the
                        random processes: “Thus the results obtained by applying the
                        techniques of Markov process theory are valuable only to the extent to which
                        they characterise just these ‘large-scale’
                        fluctuations” (<xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, p 123).
                        However, standard models fail at short time-scales. This is especially
                        relevant in this paper because the brain has to model continuous sensory
                        signals on a fast time-scale.</p>
                    <p>Having said this, it is possible to convert the generalised state-space model
                        in Equation 2 into a standard form by expressing the components of
                        generalised motion in terms of a standard [uncorrelated]
                        Markovian process, <italic>ς</italic>(<italic>t</italic>):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e003" xlink:type="simple"/><label>(3)</label></disp-formula>The first line encodes the autocorrelation function or
                        spectral density of the fluctuations <italic>w</italic>(<italic>t</italic>)
                        in term of smoothness parameters,
                                <italic>γ</italic><sub>1</sub>,…<italic>γ<sub>n</sub></italic>,
                        where <italic>n</italic> is the order of generalised motion. These
                        parameters can be regarded as the coefficients of a polynomial expansion
                                <italic>P<sub>n</sub></italic>(∂<italic>
                            <sub>t</sub>
                        </italic>)<italic>w</italic> = <italic>ς</italic>(<italic>t</italic>)
                        (see <xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, Equation 4.288 and below). The second
                        line obtains by substituting Equation 2 into the first and prescribes a
                        standard state-space model, whose states cover generalised motion;
                            <italic>x</italic><sup>[0]</sup>,…,<italic>x</italic><sup>[<italic>n</italic>]</sup>.
                        When <italic>n</italic> = 0 we recover the
                        state equation in Equation 1, namely, <italic>x</italic>
                            ˙ = <italic>f</italic>(<italic>x</italic>,<italic>v</italic>)+<italic>ς</italic>.
                        This corresponds to the standard Markovian approximation because the random
                        fluctuations are uncorrelated and
                            <italic>w</italic> = <italic>ς</italic>;
                        from Equation 3. When
                            <italic>n</italic> = 1⇒<italic>w</italic>+<italic>γ</italic><sub>1</sub><italic>w</italic>′ = <italic>ς</italic>,
                        the fluctuations <italic>w</italic>(<italic>t</italic>) correspond to an
                        exponentially correlated process, with a decay time of
                            <italic>γ</italic><sub>1</sub> (<xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, p 121).
                        However, generally
                        <italic>n</italic> = ∞:
                        “Therefore we cannot describe an actual process within the
                        framework of Markov process theory, and the more accurately we wish to
                        approximate such a process by a Markov process, the more components the
                        latter must have.” (<xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, p 165).
                        See also <xref ref-type="bibr" rid="pcbi.1000211-Jazwinski1">[7]</xref> (pp 122–125) for a related
                        treatment.</p>
                    <p>If there is a formal equivalence between standard and generalised state-space
                        models, why not use the standard formulation, with a suitably high-order
                        approximation? The answer is that we do not need to; by retaining an
                        explicit formulation in generalised coordinates we can devise a simple
                        inversion scheme (Equation 23) that outperforms standard Markovian
                        techniques like Kalman filtering. This simplicity is important because we
                        want to understand how the brain inverts dynamic models. This requires a
                        relatively simple neuronal implementation that could have emerged through
                        natural selection. From now on, we will reserve ‘state-space
                        models’ (SSM) for standard
                        <italic>n</italic> = 0 models that discount
                        generalised motion and, implicitly, serial correlations among the random
                        terms. This means we can treat SSMs as special cases of generalised
                        state-space models, in which the precision of generalised motion on the
                        states noise is zero.</p>
                </sec>
                <sec id="s2a3">
                    <title>Probabilistic dynamic models</title>
                    <p>Given the form of generalised state-space models we now consider what they
                        entail as probabilistic models of observed signals. We can write Equation 2
                        compactly as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e004" xlink:type="simple"/><label>(4)</label></disp-formula>Where the predicted response
                            <italic>g̃</italic> = [<italic>g</italic>,<italic>g</italic>′,<italic>g</italic>″,…]<italic>
                            <sup>T</sup>
                        </italic> and motion
                            <italic>f̃</italic> = [<italic>f</italic>,<italic>f</italic>′,<italic>f</italic>″,…]<italic>
                            <sup>T</sup>
                        </italic> in the absence of random fluctuations are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e005" xlink:type="simple"/></disp-formula>and <italic>D</italic> is a block-matrix derivative operator,
                        whose first leading-diagonal contains identity matrices. This operator
                        simply shifts the vectors of generalised motion so
                                <italic>x</italic><sup>[<italic>i</italic>]</sup>
                        that is replaced by
                            <italic>x</italic><sup>[<italic>i</italic>+1]</sup>.</p>
                    <p>Gaussian assumptions about the fluctuations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e006" xlink:type="simple"/></inline-formula> provide the likelihood,
                            <italic>p</italic>(<italic>ỹ</italic>|<italic>x̃</italic>,<italic>ṽ</italic>).
                        Similarly, Gaussian assumptions about state-noise <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e007" xlink:type="simple"/></inline-formula> furnish empirical priors,
                            <italic>p</italic>(<italic>x̃</italic>|<italic>ṽ</italic>)
                        in terms of predicted motion<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e008" xlink:type="simple"/><label>(5)</label></disp-formula>We will assume Gaussian priors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e009" xlink:type="simple"/></inline-formula> on the generalised causes, with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e010" xlink:type="simple"/></inline-formula> and covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e011" xlink:type="simple"/></inline-formula>. The density on the hidden states
                            <italic>p</italic>(<italic>x̃</italic>|<italic>ṽ</italic>)
                        is part of the prior on quantities needed to evaluate the likelihood of the
                        response or output. This prior means that low-order motion constrains
                        high-order motion (and <italic>vice versa</italic>). These constraints are
                        discounted in standard state-space models because the precision on the
                        generalised motion of a standard Markovian process is zero. This means the
                        only constraint is mediated by the prior
                        <italic>p</italic>(<italic>x</italic>
                        ˙|<italic>x</italic>,<italic>v</italic>). However, it is clear from
                        Equation 5 that high-order terms contribute. In this work, we exploit these
                        constraints by adopting more plausible models of noise, which are encoded by
                        their covariances <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e012" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e013" xlink:type="simple"/></inline-formula> (or precisions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e014" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e015" xlink:type="simple"/></inline-formula>). These are functions of unknown hyperparameters,
                            <italic>λ</italic> which control the amplitude and smoothness of
                        the random fluctuations.</p>
                    <p><xref ref-type="fig" rid="pcbi-1000211-g001">Figure 1 (left)</xref> shows the
                        directed graph depicting the conditional dependencies implied by this model.
                        Next, we consider hierarchal models that provide another form of
                        hierarchical constraint. It is useful to note that hierarchical models are
                        special cases of Equation 1, in the sense that they are formed by
                        introducing conditional independencies (i.e., removing edges in Bayesian
                        dependency graphs).</p>
                    <fig id="pcbi-1000211-g001" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g001</object-id>
                        <label>Figure 1</label>
                        <caption>
                            <title>Conditional dependencies of dynamic (right) and hierarchical
                                (left) models, shown as directed Bayesian graphs.</title>
                            <p>The nodes of these graphs correspond to quantities in the model and
                                the responses they generate. The arrows or edges indicate
                                conditional dependencies between these quantities. The form of the
                                models is provided, both in terms of their state-space equations
                                (above) and in terms of the prior and conditional probabilities
                                (below). The hierarchal structure of these models induces empirical
                                priors; dynamical priors are mediated by the equations of
                                generalised motion and structural priors by the hierarchical form,
                                under which states in higher levels provide constraints on the level
                                below.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g001" xlink:type="simple"/>
                    </fig>
                </sec>
                <sec id="s2a4">
                    <title>Hierarchical forms</title>
                    <p>HDMs have the following form, which generalises the
                        (<italic>m</italic> = 1) model above<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e016" xlink:type="simple"/><label>(6)</label></disp-formula>Again, <italic>f</italic><sup>(<italic>i</italic>)</sup>
                            : = <italic>f</italic>(<italic>x</italic><sup>(<italic>i</italic>)</sup>,<italic>v</italic><sup>(<italic>i</italic>)</sup>)
                        and <italic>g</italic><sup>(<italic>i</italic>)</sup>
                            : = <italic>g</italic>(<italic>x</italic><sup>(<italic>i</italic>)</sup>,<italic>v</italic><sup>(<italic>i</italic>)</sup>)
                        are continuous nonlinear functions of the states. The processes
                                <italic>z</italic><sup>(<italic>i</italic>)</sup> and
                                <italic>w</italic><sup>(<italic>i</italic>)</sup> are conditionally
                        independent fluctuations that enter each level of the hierarchy. These play
                        the role of observation error or noise at the first level and induce random
                        fluctuations in the states at higher levels. The causes
                            <italic>v</italic> = [<italic>v</italic><sup>(1)</sup>,…,<italic>v</italic><sup>(<italic>m</italic>)</sup>]<italic>
                            <sup>T</sup>
                        </italic> link levels, whereas the hidden states
                            <italic>x</italic> = [<italic>x</italic><sup>(1)</sup>,…,<italic>x</italic><sup>(<italic>m</italic>)</sup>]<italic>
                            <sup>T</sup>
                        </italic> link dynamics over time. The corresponding directed graphical
                        model is shown in <xref ref-type="fig" rid="pcbi-1000211-g001">Figure 1
                            (right)</xref>. In hierarchical form, the output of one level acts as an
                        input to the next. When the state-equations are linear, the hierarchy
                        performs successive convolutions of the highest level input, with random
                        fluctuations entering at each level. However, inputs from higher levels can
                        also enter nonlinearly into the state equations and can be regarded as
                        changing its control parameters to produce quite complicated generalised
                        convolutions with ‘deep’ (i.e., hierarchical) structure.</p>
                    <p>The conditional independence of the fluctuations at different hierarchical
                        levels means that the HDM has a Markov property over levels, which
                        simplifies attending inference schemes. See <xref ref-type="bibr" rid="pcbi.1000211-Kass1">[8]</xref> for a discussion
                        of approximate Bayesian inference in conditionally independent hierarchical
                        models of static data. Consider the empirical prior implied by Equation 6<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e017" xlink:type="simple"/><label>(7)</label></disp-formula>where the full prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e018" xlink:type="simple"/></inline-formula> is now restricted to the last level. Equation 7 is similar
                        in form to the prior in Equation 5 but now factorises over levels; where
                        higher causes place empirical priors on the dynamics of the level below. The
                        factorisation in Equation 7 is important because one can appeal to empirical
                        Bayes to interpret the conditional dependences. In empirical Bayes <xref ref-type="bibr" rid="pcbi.1000211-Efron1">[9]</xref>,
                        factorisations of the likelihood create empirical priors that share
                        properties of both the likelihood and priors. For example, the prediction
                            <italic>g̃</italic><sup>(<italic>i</italic>)</sup> = <italic>g̃</italic>(<italic>x̃</italic><sup>(<italic>i</italic>)</sup>,<italic>ṽ</italic><sup>(<italic>i</italic>)</sup>)
                        plays the role of a prior expectation on
                                <italic>ṽ</italic><sup>(<italic>i</italic>−1)</sup>,
                        yet it has to be estimated in terms of
                                <italic>x̃</italic><sup>(<italic>i</italic>)</sup>,<italic>ṽ</italic><sup>(<italic>i</italic>)</sup>.
                        In short, a hierarchical form endows models with the ability to construct
                        their own priors. These formal or structural priors are central to many
                        inference and estimation procedures, ranging from mixed-effects analyses in
                        classical covariance component analysis to automatic relevance determination
                        in machine learning. The hierarchical form and generalised motion in HDMs
                        furnishes them with both structural and dynamic empirical priors
                        respectively.</p>
                </sec>
                <sec id="s2a5">
                    <title>The precisions and temporal smoothness</title>
                    <p>In generalised coordinates, the precision, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e019" xlink:type="simple"/></inline-formula> is the Kronecker tensor product of a temporal precision
                        matrix, <italic>S</italic>(<italic>γ</italic>) and the precision
                        over random fluctuations, which has a block diagonal form in hierarchical
                        models; similarly for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e020" xlink:type="simple"/></inline-formula>. The temporal precision encodes temporal dependencies
                        among the random fluctuations and can be expressed as a function of their autocorrelations<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e021" xlink:type="simple"/><label>(8)</label></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e022" xlink:type="simple"/></inline-formula> is the second derivative of the autocorrelation function
                        evaluated at zero. This is a ubiquitous measure of roughness in the theory
                        of stochastic processes <xref ref-type="bibr" rid="pcbi.1000211-Cox1">[10]</xref>. Note that when the random fluctuations
                        are uncorrelated, the curvature (and higher derivatives) of the
                        autocorrelation are infinite. In this instance, the precision of high-order
                        motion falls to zero. This is the limiting case assumed by state-space
                        models; it corresponds to the assumption that incremental fluctuations are
                        independent (c.f., a Wiener process or random walk). Although, this is a
                        convenient assumption that is exploited in conventional Bayesian filtering
                        schemes and appropriate for physical systems with Brownian processes, it is
                        less plausible for biological and other systems, where random fluctuations
                        are themselves generated by dynamical systems (<xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, p 81).</p>
                    <p><italic>S</italic>(<italic>γ</italic>) can be evaluated for any
                        analytic autocorrelation function. For convenience, we assume that the
                        temporal correlations have the same Gaussian form. This gives<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e023" xlink:type="simple"/><label>(9)</label></disp-formula>Here, <italic>γ</italic> is the precision parameter
                        of a Gaussian autocorrelation function. Typically,
                        <italic>γ</italic>&gt;1, which ensures the precisions of
                        high-order motion converge quickly. This is important because it enables us
                        to truncate the representation of an infinite number of generalised
                        coordinates to a relatively small number; because high-order prediction
                        errors have a vanishingly small precision. An order of
                        <italic>n</italic> = 6 is sufficient in
                        most cases <xref ref-type="bibr" rid="pcbi.1000211-Friston1">[1]</xref>. A typical example is shown in <xref ref-type="fig" rid="pcbi-1000211-g002">Figure 2</xref>, in generalised
                        coordinates and after projection onto the time-bins (using a Taylor
                        expansion, whose coefficients comprise the matrix
                        <italic>Ẽ</italic>). It can be seen that the precision falls
                        quickly with order and, in this case, we can consider just six orders of
                        motion, with no loss of precision.</p>
                    <fig id="pcbi-1000211-g002" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g002</object-id>
                        <label>Figure 2</label>
                        <caption>
                            <title>Image representations of the precision matrices encoding temporal
                                dependencies among the generalised motion of random fluctuations.</title>
                            <p>The precision in generalised coordinates (left) and over discrete
                                samples in time (right) are shown for a roughness of
                                    <italic>γ</italic> = 4
                                and seventeen observations (with an order of
                                <italic>n</italic> = 16). This
                                corresponds to an autocorrelation function whose width is half a
                                time bin. With this degree of temporal correlation only a few (i.e.,
                                five or six) discrete local observations are specified with any
                                precision.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g002" xlink:type="simple"/>
                    </fig>
                    <p>When dealing with discrete time-series it is necessary to map the trajectory
                        implicit in the generalised motion of the response onto discrete samples,
                            [<italic>y</italic>(<italic>t</italic><sub>1</sub>),…,<italic>y</italic>(<italic>t<sub>N</sub></italic>)]<italic>
                            <sup>T</sup>
                        </italic> = <italic>Ẽ</italic><italic>ỹ</italic>(<italic>t</italic>)
                        (note that this is not necessary with continuous data such as sensory data
                        sampled by the brain). After this projection, the precision falls quickly
                        over time-bins (<xref ref-type="fig" rid="pcbi-1000211-g002">Figure 2,
                        right</xref>). This means samples in the remote past or future do not
                        contribute to the likelihood and the inversion of discrete time-series data
                        can proceed using local samples around the current time bin; i.e., it can
                        operate ‘on-line’.</p>
                </sec>
                <sec id="s2a6">
                    <title>Energy functions</title>
                    <p>We can now write down the exact form of the generative model. For dynamic
                        models, under Gaussian assumptions about the random terms, we have a simple
                        quadratic form (ignoring constants)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e024" xlink:type="simple"/><label>(10)</label></disp-formula>The auxiliary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e025" xlink:type="simple"/></inline-formula> comprise prediction errors for the generalised response
                        and motion of hidden states, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e122" xlink:type="simple"/></inline-formula> and
                         
                        <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e123" xlink:type="simple"/></inline-formula> are the respective
                        predictions, whose precision is encoded by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e026" xlink:type="simple"/></inline-formula>. The use of prediction errors simplifies exposition and
                        may be used in neurobiological implementations (i.e., encoded explicitly in
                        the brain; see last section and <xref ref-type="bibr" rid="pcbi.1000211-Friston4">[4]</xref>). For
                        hierarchical models, the prediction error on the response is supplemented
                        with prediction errors on the causes<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e027" xlink:type="simple"/><label>(11)</label></disp-formula>Note that the data and priors enter the prediction error at
                        the lowest and highest level respectively. At intermediate levels the
                        prediction errors,
                            <italic>v</italic><sup>(<italic>i</italic>−1)</sup>−<italic>g</italic><sup>(<italic>i</italic>)</sup>
                        mediate empirical priors on the causes. In the next section, we will use a
                        variational inversion of the HDM, which entails message passing between
                        hierarchical levels. These messages are the prediction errors and their
                        influence rests on the derivatives of the prediction error with respect to
                        the unknown states<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e028" xlink:type="simple"/><label>(12)</label></disp-formula>This form highlights the role of causes in linking successive
                        hierarchical levels (the <italic>D<sup>T</sup></italic> matrix) and the role
                        of hidden states in linking successive temporal derivatives (the
                        <italic>D</italic> matrix). The <italic>D<sup>T</sup></italic> in the
                        upper-left block reflects the fact that that the prediction error on the
                        causes depends on causes at that level and the lower level being predicted;
                            <italic>ε</italic><sup>(<italic>i</italic>)<italic>v</italic></sup> = <italic>v</italic><sup>(<italic>i</italic>−1)</sup>−<italic>g</italic>(<italic>x</italic><sup>(<italic>i</italic>)</sup>,<italic>v</italic><sup>(<italic>i</italic>)</sup>).
                        The <italic>D</italic> in the lower-right block plays a homologous role, in
                        that the prediction error on the motion of hidden states depends on motion
                        at that order and the higher order;
                                <italic>ε</italic><sup>[<italic>i</italic>]<italic>x</italic></sup> = <italic>x</italic><sup>[<italic>i</italic>+1]</sup>−<italic>f</italic>(<italic>x</italic><sup>[<italic>i</italic>]</sup>,<italic>v</italic><sup>[<italic>i</italic>]</sup>).</p>
                    <p>These constraints on the structural and dynamic form of the system are
                        specified by the functions
                            <italic>g</italic> = [<italic>g</italic><sup>(1)</sup>,…,<italic>g</italic><sup>(<italic>m</italic>)</sup>]<italic>
                            <sup>T</sup>
                        </italic> and
                            <italic>f</italic> = [<italic>f</italic><sup>(1)</sup>,…,<italic>f</italic><sup>(<italic>m</italic>)</sup>]<italic>
                            <sup>T</sup>
                        </italic>, respectively. The partial derivatives of these functions have a
                        block diagonal form, reflecting the model's hierarchical separability<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e029" xlink:type="simple"/><label>(13)</label></disp-formula>Note that the partial derivatives of
                            <italic>g</italic>(<italic>x</italic>,<italic>v</italic>) have an extra
                        row to accommodate the top level. To complete model specification we need
                        priors on the parameters and hyperparameters. We will assume these are
                        Gaussian, where (ignoring constants)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e030" xlink:type="simple"/><label>(14)</label></disp-formula></p>
                </sec>
                <sec id="s2a7">
                    <title>Summary</title>
                    <p>In this section, we have introduced hierarchical dynamic models in
                        generalised coordinates of motion. These models are about as complicated as
                        one could imagine; they comprise causes and hidden states, whose dynamics
                        can be coupled with arbitrary (analytic) nonlinear functions. Furthermore,
                        these states can have random fluctuations with unknown amplitude and
                        arbitrary (analytic) autocorrelation functions. A key aspect of the model is
                        its hierarchical form, which induces empirical priors on the causes. These
                        recapitulate the constraints on hidden states, furnished by the hierarchy
                        implicit in generalised motion. We now consider how these models are
                        inverted.</p>
                </sec>
            </sec>
            <sec id="s2b">
                <title>Model Inversion</title>
                <p>This section considers variational inversion of models under mean-field and
                    Laplace approximations, with a special focus on HDMs. This treatment provides a
                    heuristic summary of the material in <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref>. Variational Bayes
                    is a generic approach to model inversion that approximates the conditional
                    density
                        <italic>p</italic>(<italic>ϑ</italic>|<italic>y</italic>,<italic>m</italic>)
                    on some model parameters, <italic>ϑ</italic>, given a model
                    <italic>m</italic> and data <italic>y</italic>. This is achieved by optimising
                    the sufficient statistics (e.g., mean and variance) of an approximate
                    conditional density <italic>q</italic>(<italic>ϑ</italic>)with
                    respect to a lower bound on the evidence (marginal or integrated likelihood)
                        <italic>p</italic>(<italic>y</italic>|<italic>m</italic>) of the model
                    itself. These two quantities are used for inference on the parameters of any
                    given model and on the model <italic>per se</italic>. <xref ref-type="bibr" rid="pcbi.1000211-Feynman1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1000211-Friston6">[15]</xref>.
                    The log-evidence for any parametric model can be expressed in terms of a
                    free-energy
                    <italic>F</italic>(<italic>ỹ</italic>,<italic>q</italic>) and a
                    divergence term, for any density
                    <italic>q</italic>(<italic>ϑ</italic>) on the unknown quantities<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e031" xlink:type="simple"/><label>(15)</label></disp-formula>The free-energy comprises the internal energy,
                        <italic>U</italic>(<italic>y</italic>,<italic>ϑ</italic>) = ln
                        <italic>p</italic>(<italic>y</italic>,<italic>ϑ</italic>)
                    expected under <italic>q</italic>(<italic>ϑ</italic>) and an entropy
                    term, which is a measure of its uncertainty. In this paper, energies are the
                    negative of the corresponding quantities in physics; this ensures the
                    free-energy increases with log-evidence. Equation 15 indicates that
                        <italic>F</italic>(<italic>ỹ</italic>,<italic>q</italic>) is a
                    lower-bound on the log-evidence because the cross-entropy or divergence term is
                    always positive.</p>
                <p>The objective is to optimise <italic>q</italic>(<italic>ϑ</italic>) by
                    maximising the free-energy and then use <italic>F</italic>≈ln
                        <italic>p</italic>(<italic>ỹ</italic>|<italic>m</italic>) as a
                    lower-bound approximation to the log-evidence for model comparison or averaging.
                    Maximising the free-energy minimises the divergence, rendering
                        <italic>q</italic>(<italic>ϑ</italic>)≈<italic>p</italic>(<italic>ϑ</italic>|<italic>y</italic>,<italic>m</italic>)
                    an approximate posterior, which is exact for simple (e.g., linear) systems. This
                    can then be used for inference on the parameters of the model selected.</p>
                <p>Invoking an arbitrary density, <italic>q</italic>(<italic>ϑ</italic>)
                    converts a difficult integration problem (inherent in computing the evidence;
                    see <xref ref-type="sec" rid="s4">discussion</xref>) into an easier optimisation
                    problem. This rests on inducing a bound that can be optimised with respect to
                        <italic>q</italic>(<italic>ϑ</italic>). To finesse optimisation,
                    one usually assumes <italic>q</italic>(<italic>ϑ</italic>) factorises
                    over a partition of the parameters<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e032" xlink:type="simple"/><label>(16)</label></disp-formula>In statistical physics this is called a mean-field approximation.
                    This factorisation means that one assumes the dependencies between different
                    sorts of parameters can be ignored. It is a ubiquitous assumption in statistics
                    and machine learning. Perhaps the most common example is a partition into
                    parameters coupling causes to responses and hyperparameters controlling the
                    amplitude or variance of random effects. This partition greatly simplifies the
                    calculation of things like <italic>t</italic>-tests and implies that, having
                    seen some data, knowing their variance does not tell you anything more about
                    their mean. Under our hierarchical dynamic model we will appeal to separation of
                    temporal scales and assume,
                        <italic>q</italic>(<italic>ϑ</italic>) = <italic>q</italic>(<italic>u</italic>(<italic>t</italic>))<italic>q</italic>(<italic>θ</italic>)<italic>q</italic>(<italic>λ</italic>),
                    where
                        <italic>u</italic> = [<italic>ṽ</italic>,<italic>x̃</italic>,]<italic>
                        <sup>T</sup>
                    </italic> are generalised states. This means that, in addition to the partition
                    into parameters and hyperparameters, we assume conditional independence between
                    quantities that change (states) and quantities that do not (parameters and
                    hyperparameters).</p>
                <p>In this dynamic setting
                    <italic>q</italic>(<italic>u</italic>(<italic>t</italic>)) and the free-energy
                    become functionals of time. By analogy with Lagrangian mechanics, this calls on
                    the notion of <italic>action</italic>. Action is the anti-derivative or
                    path-integral of energy. We will denote the action associated with the free
                    energy by <italic>F̅</italic>, such that
                        ∂<italic><sub>t</sub>F̅</italic> = <italic>F</italic>.
                    We now seek <italic>q</italic>(<italic>ϑ<sup>i</sup></italic>) that
                    maximise the action. It is fairly easy to show <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref> that the solution
                    for the states is a function of their instantaneous energy,
                        <italic>U</italic>(<italic>t</italic>): = <italic>U</italic>(<italic>u</italic>|<italic>θ</italic>,<italic>λ</italic>) = ln
                        <italic>p</italic>(<italic>ỹ</italic>,<italic>u</italic>|<italic>θ</italic>,<italic>λ</italic>)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e033" xlink:type="simple"/><label>(17)</label></disp-formula>where
                            <italic>V</italic>(<italic>t</italic>) = ∂<italic><sub>t</sub>V̅</italic><italic>
                        <sup>u</sup>
                    </italic> is their variational energy. The variational energy of the states is
                    simply their instantaneous energy averaged over their Markov blanket (i.e.,
                    averaged over the conditional density of the parameters and hyperparameters).
                    Because the states are time-varying quantities, their conditional density is a
                    function of time-dependent energy. In contrast, the conditional density of the
                    parameters and hyperparameters are functions of their variational action, which
                    are fixed for a given period of observation.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e034" xlink:type="simple"/><label>(18)</label></disp-formula>Where
                    <italic>U<sup>θ</sup></italic> = ln
                        <italic>p</italic>(<italic>θ</italic>) and
                        <italic>U<sup>λ</sup></italic> = ln
                        <italic>p</italic>(<italic>λ</italic>) are the prior energies of the
                    parameters and hyperparameters respectively and play the role of integration
                    constants in the corresponding variational actions; <italic>V̅</italic><italic>
                        <sup>θ</sup>
                    </italic> and <italic>V̅</italic><italic>
                        <sup>λ</sup>
                    </italic>.</p>
                <p>These equations provide closed-form expressions for the conditional or
                    variational density in terms of the internal energy defined by our model;
                    Equation 10. They are intuitively sensible, because the conditional density of
                    the states should reflect the instantaneous energy; Equation 17. Whereas the
                    conditional density of the parameters can only be determined after all the data
                    have been observed; Equation 18. In other words, the variational energy involves
                    the prior energy and an integral of time-dependent energy. In the absence of
                    data, when the integrals are zero, the conditional density reduces to the prior
                    density.</p>
                <p>If the analytic forms of Equations 17 and 18 were tractable (e.g., through the
                    use of conjugate priors),
                    <italic>q</italic>(<italic>ϑ<sup>i</sup></italic>) could be optimised
                    directly by iterating these self-consistent nonlinear equations. This is known
                    as variational Bayes; see <xref ref-type="bibr" rid="pcbi.1000211-Beal1">[16]</xref> for an excellent treatment of static
                    conjugate-exponential models. However, we will take a simpler approach that does
                    not require bespoke update equations. This is based on a fixed-form
                    approximation to the variational density.</p>
                <sec id="s2b1">
                    <title>The Laplace approximation</title>
                    <p>Under the Laplace approximation, the marginals of the conditional density
                        assume a Gaussian form
                            <italic>q</italic>(<italic>ϑ<sup>i</sup></italic>) = <italic>N</italic>(<italic>ϑ<sup>i</sup></italic>:
                                <italic>µ<sup>i</sup></italic>,<italic>C<sup>i</sup></italic>)
                        with sufficient statistics <italic>µ<sup>i</sup></italic> and
                                <italic>C<sup>i</sup></italic>, corresponding to the conditional
                        mean and covariance of the <italic>i</italic>th marginal. For consistency,
                        we will use <italic>µ<sup>i</sup></italic> for the conditional
                        means or modes and <italic>η<sup>i</sup></italic> for prior means.
                        Similarly, we will use Σ<italic>
                            <sup>i</sup>
                        </italic> and <italic>C<sup>i</sup></italic> for the prior and conditional
                        covariances and Π<italic>
                            <sup>i</sup>
                        </italic> and <italic>P<sup>i</sup></italic> for the corresponding inverses
                        (i.e., precisions).</p>
                    <p>The advantage of the Laplace assumption is that the conditional covariance is
                        a simple function of the modes. Under the Laplace assumption, the internal
                        and variational actions are (ignoring constants)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e035" xlink:type="simple"/><label>(19)</label></disp-formula><italic>C<sup>u</sup></italic>
                            : = <italic>C</italic>(<italic>t</italic>)<italic>
                            <sup>u</sup>
                        </italic> is the conditional covariance of the states at time
                            <italic>t</italic>∈[0,<italic>N</italic>].
                        The quantities <italic>W</italic>(<italic>t</italic>)<italic>
                            <sup>i</sup>
                        </italic> represent the contribution to the variational action from other
                        marginals and mediate the effect of the uncertainty they encode on each
                        other. We will refer to these as mean-field terms.</p>
                </sec>
                <sec id="s2b2">
                    <title>Conditional precisions</title>
                    <p>By differentiating Equation 19 with respect to the covariances and solving
                        for zero, it is easy to show that the conditional precisions are the
                        negative curvatures of the internal action <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref>. Unless stated
                        otherwise, all gradients and curvatures are evaluated at the mode or mean.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e036" xlink:type="simple"/><label>(20)</label></disp-formula>Notice that the precisions of the parameters and
                        hyperparameters increase with observation time, as one would expect. For our
                        HDM the gradients and curvatures of the internal energy are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e037" xlink:type="simple"/><label>(21)</label></disp-formula>where the covariance, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e038" xlink:type="simple"/></inline-formula> is the inverse of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e039" xlink:type="simple"/></inline-formula>. The <italic>i</italic>th element of the energy gradient; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e040" xlink:type="simple"/></inline-formula> is the derivative with respect to the <italic>i</italic>th
                        hyperparameter (similarly for the curvatures). We have assumed that the
                        precision of the random fluctuations is linear in the hyperparameters, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e041" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e042" xlink:type="simple"/></inline-formula>. The derivatives of the generalised prediction error with
                        respect to the generalised states are provided in Equation 12. The
                        corresponding derivatives with respect to each parameter, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e043" xlink:type="simple"/></inline-formula> rest on second derivatives of the model's
                        functions that mediate interactions between each parameter and the states<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e044" xlink:type="simple"/><label>(22)</label></disp-formula>These also quantify how the states and parameters affect each
                        other through mean-field effects (see below).</p>
                </sec>
                <sec id="s2b3">
                    <title>Summary</title>
                    <p>The Laplace approximation gives a compact and simple form for the conditional
                        precisions; and reduces the problem of inversion to finding the conditional
                        modes. This generally proceeds in a series of iterated steps, in which the
                        mode of each parameter set is updated. These updates optimise the
                        variational actions in Equation 19 with respect to
                            <italic>µ<sup>i</sup></italic>, using the sufficient
                        statistics (conditional mean and covariance) of the other sets. We have
                        discussed static cases of this fixed-form scheme previously and how it
                        reduces to expectation maximisation (<bold>EM</bold>; <xref ref-type="bibr" rid="pcbi.1000211-Dempster1">[17]</xref>) and
                        restricted maximum likelihood (<bold>ReML</bold>; <xref ref-type="bibr" rid="pcbi.1000211-Harville1">[18]</xref>) for linear
                        models <xref ref-type="bibr" rid="pcbi.1000211-Friston6">[15]</xref>. We now consider each of the steps
                        entailed by our mean-field partition.</p>
                </sec>
            </sec>
            <sec id="s2c">
                <title>Dynamic Expectation Maximisation</title>
                <p>As with conventional variational schemes, we can update the modes of our three
                    parameter sets in three distinct steps. However, the step dealing with the state
                        (<bold>D</bold>-step) must integrate its conditional mode <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e045" xlink:type="simple"/></inline-formula> over time to accumulate the quantities necessary for updating
                    the parameters (<bold>E</bold>-step) and hyperparameters (<bold>M</bold>-step).
                    We now consider optimising the modes or conditional means in each of these
                    steps.</p>
                <sec id="s2c1">
                    <title>The D-step</title>
                    <p>In static systems, the mode of the conditional density maximises variational
                        energy, such that
                        ∂<italic><sub>u</sub>V</italic>(<italic>t</italic>) = 0;
                        this is the solution to a gradient ascent scheme; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e046" xlink:type="simple"/></inline-formula>. In dynamic systems, we also require the path of the mode
                        to be the mode of the path; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e047" xlink:type="simple"/></inline-formula>. These two conditions are satisfied by the solution to the ansatz<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e048" xlink:type="simple"/><label>(23)</label></disp-formula>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e049" xlink:type="simple"/></inline-formula> can be regarded as motion in a frame of reference that
                        moves along the trajectory encoded in generalised coordinates. Critically,
                        the stationary solution in this moving frame of reference maximises
                        variational action. This can be seen easily by noting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e050" xlink:type="simple"/></inline-formula> means the gradient of the variational energy is zero and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e051" xlink:type="simple"/><label>(24)</label></disp-formula>This is sufficient for the mode to maximise variational
                        action. In other words, changes in variational action, <italic>V̅</italic><italic>
                            <sup>u</sup>
                        </italic>, with respect to variations of the path of the mode are zero
                        (c.f., Hamilton's principle of stationary action). Intuitively,
                        this means tiny perturbations to its path do not change the variational
                        energy and it has the greatest variational action (i.e., path-integral of
                        variational energy) of all possible paths.</p>
                    <p>Another way of looking at this is to consider the problem of finding the path
                        of the conditional mode. However, the mode is in generalised coordinates and
                        already encodes its path. This means we have to optimise the path of the
                        mode subject to the constraint that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e052" xlink:type="simple"/></inline-formula>, which ensures the path of the mode and the mode of the
                        path are the same. The solution to Equation 23 ensures that variational
                        energy is maximised and the path is self-consistent. Note that this is a
                        very different (and simpler) construction in relation to incremental schemes
                        such as Bayesian filtering.</p>
                    <p>Equation 23 prescribes the trajectory of the conditional mode, which can be
                        realised with a local linearization <xref ref-type="bibr" rid="pcbi.1000211-Ozaki1">[19]</xref> by integrating
                        over Δ<italic>t</italic> to recover its evolution over discrete intervals<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e053" xlink:type="simple"/><label>(25)</label></disp-formula>For simplicity, we have suppressed the dependency of
                            <italic>V</italic>(<italic>u</italic>,<italic>t</italic>) on the data.
                        However, it is necessary to augment Equation 25 with any time-varying
                        quantities that affect the variational energy. The form of the ensuing
                        Jacobian ℑ(<italic>t</italic>) is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e054" xlink:type="simple"/><label>(26)</label></disp-formula>Here. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e055" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e056" xlink:type="simple"/></inline-formula> where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e057" xlink:type="simple"/><label>(27)</label></disp-formula>These forms reflect the fact that data and priors only affect
                        the prediction error at the first and last levels respectively. The only
                        remaining quantities we require are the gradients and curvatures of the
                        variational energy, which are simply<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e058" xlink:type="simple"/><label>(28)</label></disp-formula>The mean-field term, <italic>W</italic>(<italic>t</italic>)<italic>
                            <sup>λ</sup>
                        </italic> does not contribute to the <bold>D</bold>-step because it is not a
                        function of the states. This means uncertainly about the hyperparameters
                        does not affect the update for the states. This is because we assumed the
                        precision was linear in the hyperparameters. The updates in Equation 25
                        provide the conditional trajectory <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e059" xlink:type="simple"/></inline-formula> at each time point. Usually, Δ<italic>t</italic>
                        is the time between observations but could be smaller, if nonlinearities in
                        the model render local linearity assumptions untenable.</p>
                </sec>
                <sec id="s2c2">
                    <title>The E- and M-steps</title>
                    <p>Exactly the same update procedure can be used for the <bold>E</bold>- and
                            <bold>M</bold>-steps. However, in this instance there are no generalised
                        coordinates to consider. Furthermore, we can set the interval between
                        updates to be arbitrarily long because the parameters are updated after the
                        time-series has been integrated. If
                        Δ<italic>t</italic>→∞ is sufficiently large, the
                        matrix exponential in Equation 25 disappears (because the curvature of the
                        Jacobian is negative definite) giving<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e060" xlink:type="simple"/><label>(29)</label></disp-formula>Equation 29 is a conventional Gauss-Newton update scheme. In
                        this sense, the <bold>D</bold>-Step can be regarded as a generalization of
                        classical ascent schemes to generalised coordinates that cover dynamic
                        systems. For our HDM, the requisite gradients and curvatures of variational
                        action for the <bold>E</bold>-step are<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e061" xlink:type="simple"/><label>(30)</label></disp-formula>Similarly, for the hyperparameters<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e062" xlink:type="simple"/><label>(31)</label></disp-formula>Although uncertainty about the hyperparameters does not
                        affect the states and parameters, uncertainty about both the states and
                        parameters affect the hyperparameter update.</p>
                    <p>These steps represent a full variational scheme. A simplified version, which
                        discounts uncertainty about the parameters and states in the <bold>D</bold>
                        and <bold>E</bold>-steps, would be the analogue of an <bold>EM</bold>
                        scheme. This simplification is easy to implement by removing
                            <italic>W</italic>(<italic>t</italic>)<italic>
                            <sup>θ</sup>
                        </italic> and <italic>W</italic>(<italic>t</italic>)<italic>
                            <sup>u</sup>
                        </italic> from the <bold>D-</bold> and <bold>E</bold>-steps respectively. We
                        will pursue this in the context of neurobiological implementations in the
                        last section.</p>
                </sec>
                <sec id="s2c3">
                    <title>Summary</title>
                    <p>These updates furnish a variational scheme under the Laplace approximation.
                        To further simplify things, we will assume
                        Δ<italic>t</italic> = 1, such that
                        sampling intervals serve as units of time. With these simplifications, the
                            <bold>DEM</bold> scheme can be summarised as iterating until
                    convergence</p>
                </sec>
                <sec id="s2c4">
                    <title><bold>D-</bold>step <bold>(states)</bold></title>
                    <p><italic>for t</italic> = 1: <italic>N</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e063" xlink:type="simple"/></disp-formula></p>
                    <p>
                        <italic>end</italic>
                    </p>
                </sec>
                <sec id="s2c5">
                    <title><bold>E-</bold>step <bold>(parameters)</bold></title>
                    <p>
                        <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e064" xlink:type="simple"/></disp-formula>
                    </p>
                </sec>
                <sec id="s2c6">
                    <title><bold>M-</bold>step <bold>(hyperparameters)</bold></title>
                    <p>
                        <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e065" xlink:type="simple"/><label>(32)</label></disp-formula>
                    </p>
                    <p>In this section, we have seen how the inversion of dynamic models can be
                        formulated as an optimization of action. This action is the anti-derivative
                        or path-integral of free-energy associated with changing states and a
                        constant (of integration) corresponding to the prior energy of
                        time-invariant parameters. By assuming a fixed-form (Laplace) approximation
                        to the conditional density, one can reduce optimisation to finding the
                        conditional modes of unknown quantities, because their conditional
                        covariance is simply the curvature of the internal action (evaluated at the
                        mode). The conditional modes of (mean-field) marginals optimise variational
                        action, which can be framed in terms of gradient ascent. For the states,
                        this entails finding a path or trajectory with stationary variational
                        action. This can be formulated as a gradient ascent in a frame of reference
                        that moves along the path encoded in generalised coordinates.</p>
                </sec>
            </sec>
        </sec>
        <sec id="s3">
            <title>Results</title>
            <p>In this section, we review the model and inversion scheme of the previous section in
                light of established procedures for supervised and self-supervised learning. This
                section considers HDMs from the pragmatic point of view of statistics and machine
                learning, where the data are empirical and arrive as discrete data sequences. In the
                next section, we revisit these models and their inversion from the point of view of
                the brain, where the data are sensory and continuous. This section aims to establish
                the generality of HDMs by showing that many well-known approaches to data can be
                cast as an inverting a HDM under simplifying assumptions. It recapitulates the
                unifying perspective of Roweis and Ghahramani <xref ref-type="bibr" rid="pcbi.1000211-Roweis1">[20]</xref> with a special focus on
                hierarchical models and the triple estimation problems <bold>DEM</bold> can solve.
                We start with supervised learning and then move to unsupervised schemes. Supervised
                schemes are called for when causes are known but the parameters are not. Conversely,
                the parameters may be known and we may want to estimate causes or hidden states.
                This leads to a distinction between <italic>identification</italic> of a
                model's parameters and <italic>estimation</italic> of its states. When
                neither the states nor parameters are known, the learning is unsupervised. We will
                consider models in which the parameters are unknown, the states are unknown or both
                are unknown. Within each class, we will start with static models and then consider
                dynamic models.</p>
            <p>All the schemes described in this paper are available in Matlab code as academic
                freeware (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). The simulation figures in this paper
                can be reproduced from a graphical user interface called from the DEM toolbox.</p>
            <sec id="s3a">
                <title>Models with Unknown Parameters</title>
                <p>In these models the causes are known and enter as priors
                    <italic>η</italic> with infinite precision; Σ<italic>
                        <sup>v</sup>
                    </italic> = 0. Furthermore, if the model is
                    static or, more generally when
                    <italic>g<sub>x</sub></italic> = 0, we can
                    ignore hidden states and dispense with the <bold>D</bold>-step.</p>
                <sec id="s3a1">
                    <title>Static models and neural networks</title>
                    <p>Usually, supervised learning entails learning the parameters of static
                        nonlinear generative models with known causes. This corresponds to a HDM
                        with infinitely precise priors at the last level, any number of subordinate
                        levels (with no hidden states)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e066" xlink:type="simple"/><label>(33)</label></disp-formula>One could regard this model as a neural network with
                            <italic>m</italic> hidden layers. From the neural network perspective,
                        the objective is to optimise the parameters of a nonlinear mapping from data
                            <italic>y</italic> to the desired output <italic>η</italic>,
                        using back propagation of errors or related approaches <xref ref-type="bibr" rid="pcbi.1000211-Rumelhart1">[21]</xref>. This
                        mapping corresponds to inversion of the generative model that maps causes to
                        data; <italic>g</italic><sup>(<italic>i</italic>)</sup>:
                            <italic>η</italic>→<italic>y</italic>. This inverse
                        problem is solved by <bold>DEM</bold>. However, unlike back propagation of
                        errors or universal approximation in neural networks <xref ref-type="bibr" rid="pcbi.1000211-Chen1">[22]</xref>, <bold>DEM</bold>
                        is not simply a nonlinear function approximation device. This is because the
                        network connections parameterise a generative model as opposed to its
                        inverse; <italic>h</italic>:
                        <italic>y</italic>→<italic>η</italic> (i.e., recognition
                        model). This means that the parameters specify how states cause data and can
                        therefore be used to generate data. Furthermore, unlike many neural network
                        or <bold>PDP</bold> (parallel distributed processing) schemes,
                        <bold>DEM</bold> enables Bayesian inference through an explicit
                        parameterisation of the conditional densities of the parameters.</p>
                </sec>
                <sec id="s3a2">
                    <title>Nonlinear system identification</title>
                    <p>In nonlinear optimisation, we want to identify the parameters of a static,
                        nonlinear function that maps known causes to responses. This is a trivial
                        case of the static model above that obtains when the hierarchical order
                        reduces to <italic>m</italic> = 1<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e067" xlink:type="simple"/><label>(34)</label></disp-formula>The conditional estimates of
                            <italic>θ</italic><sup>(1)</sup> optimise the mapping
                            <italic>g</italic><sup>(1)</sup>:
                            <italic>η</italic>→<italic>y</italic> for any
                        specified form of generating function. Because there are no dynamics, the
                        generalised motion of the response is zero, rendering the
                        <bold>D</bold>-step and generalised coordinates redundant. Therefore,
                        identification or inversion of these models reduces to conventional
                        expectation-maximisation (<bold>EM</bold>), in which the parameters and
                        hyperparameters are optimised recursively, through a coordinate ascent on
                        the variational energy implicit in the <bold>E</bold> and
                        <bold>M</bold>-steps. Expectation-maximisation has itself some ubiquitous
                        special cases, when applied to simple linear models:</p>
                </sec>
                <sec id="s3a3">
                    <title>The general linear model</title>
                    <p>Consider the linear model, with a response that has been elicited using known
                        causes,
                            <italic>y</italic> = <italic>θ</italic><sup>(1)</sup><italic>η</italic>+<italic>z</italic><sup>(1)</sup>.
                        If we start with an initial estimate of the parameters,
                            <italic>θ</italic><sup>(1)</sup> = 0,
                        the <bold>E</bold>-step reduces to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e068" xlink:type="simple"/><label>(35)</label></disp-formula>These are the standard results for the conditional
                        expectation and covariance of a general linear model, under parametric
                        (i.e., Gaussian error) assumptions. From this perspective, the known causes
                                <italic>η<sup>T</sup></italic> play the role of explanatory
                        variables that are referred to collectively in classical statistics as a
                        design matrix. This can be seen more easily by considering the transpose of
                        the linear model in Equation 34;
                                <italic>y<sup>T</sup></italic> = <italic>η<sup>T</sup>θ</italic><sup>(1)<italic>T</italic></sup>+<italic>z</italic><sup>(1)<italic>T</italic></sup>.
                        In this form, the causes are referred to as explanatory or independent
                        variables and the data as response or dependent variables. A significant
                        association between these two sets of variables is usually established by
                        testing the null hypothesis that
                        <italic>θ</italic><sup>(1)</sup> = 0.
                        This proceeds either by comparing the evidence for (full or alternate)
                        models with and (reduced or null) models without the appropriate explanatory
                        variables or using the conditional density of the parameters, under the full
                        model.</p>
                    <p>If we have flat priors on the parameters, Π<italic>
                            <sup>θ</sup>
                        </italic> = 0, the conditional moments in
                        Equation (35) become maximum likelihood (<bold>ML</bold>) estimators.
                        Finally, under i.i.d. (identically and independently distributed)
                        assumptions about the errors, the dependency on the hyperparameters
                        disappears (because the precisions cancel) and we obtain ordinary least
                        squares (<bold>OLS</bold>) estimates;
                            <italic>µ<sup>θ</sup></italic> = <italic>η</italic><sup>−</sup><italic>y<sup>T</sup></italic>,
                        where
                                <italic>η</italic><sup>−</sup> = (<italic>ηη<sup>T</sup></italic>)<sup>−1</sup><italic>η</italic>
                        is the generalised inverse.</p>
                    <p>It is interesting to note that transposing the general linear model is
                        equivalent to the switching the roles of the causes and parameters;
                            <italic>θ</italic><sup>(1)<italic>T</italic></sup> ↔
                            <italic>η</italic>. Under this transposition, one could replace
                        the <bold>D</bold>-step with the <bold>E</bold>-step. This gives exactly the
                        same results because the two updates are formally identical for static
                        models, under which<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e069" xlink:type="simple"/><label>(36)</label></disp-formula>The exponential term disappears because the update is
                        integrated until convergence; i.e.,
                        Δ<italic>t</italic> = ∞.
                        At this point, generalised motion is zero and an embedding order of
                            <italic>n</italic> = 0⇒<italic>D</italic> = 0
                        is sufficient. This is a useful perspective because it suggests that static
                        models can be regarded as models of steady-state or equilibrium responses,
                        for systems with fixed point attractors.</p>
                </sec>
                <sec id="s3a4">
                    <title>Identifying dynamic systems</title>
                    <p>In the identification of nonlinear dynamic systems, one tries to characterise
                        the architecture that transforms known inputs into measured outputs. This
                        transformation is generally modelled as a generalised convolution <xref ref-type="bibr" rid="pcbi.1000211-Fliess1">[23]</xref>. When then inputs are known deterministic
                        quantities the following
                        <italic>m</italic> = 1 dynamic model applies<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e070" xlink:type="simple"/><label>(37)</label></disp-formula>Here <italic>η</italic> and <italic>y</italic> play
                        the role of inputs (priors) and outputs (responses) respectively. Note that
                        there is no state-noise; i.e., Σ<italic>
                            <sup>w</sup>
                        </italic> = 0 because the states are known.
                        In this context, the hidden states become a deterministic nonlinear
                        convolution of the causes <xref ref-type="bibr" rid="pcbi.1000211-Fliess1">[23]</xref>. This means there is no conditional
                        uncertainty about the states (given the parameters) and the
                        <bold>D</bold>-step reduces to integrating the state-equation to produce
                        deterministic outputs. The <bold>E</bold>-Step updates the conditional
                        parameters, based on the resulting prediction error and the
                        <bold>M</bold>-Step estimates the precision of the observation error. The
                        ensuing scheme is described in detail in <xref ref-type="bibr" rid="pcbi.1000211-Friston7">[24]</xref>, where it is
                        applied to nonlinear hemodynamic models of fMRI time-series. This is an
                            <bold>EM</bold> scheme that has been used widely to invert deterministic
                        dynamic causal models of biological time-series. In part, the motivation to
                        develop <bold>DEM</bold> was to generalise <bold>EM</bold> to handle
                        state-noise or random fluctuations in hidden states. The extension of
                            <bold>EM</bold> schemes into generalised coordinates had not yet been
                        fully explored and represents a potentially interesting way of harnessing
                        serial correlations in observation noise to optimise the estimates of a
                        system's parameters. This extension is trivial to implement with
                            <bold>DEM</bold> by specifying very high precisions on the causes and
                        state-noise.</p>
                </sec>
            </sec>
            <sec id="s3b">
                <title>Models with Unknown States</title>
                <p>In these models, the parameters are known and enter as priors
                            <italic>η<sup>θ</sup></italic> with infinite
                    precision, Σ<italic>
                        <sup>θ</sup>
                    </italic> = 0. This renders the
                    <bold>E</bold>-Step redundant. We will review estimation under static models and
                    then consider Bayesian deconvolution and filtering with dynamic models. Static
                    models imply the generalised motion of causal states is zero and therefore it is
                    sufficient to represent conditional uncertainty on their amplitude; i.e.,
                        <italic>n</italic> = 0⇒<italic>D</italic> = 0.
                    As noted above the <bold>D</bold>-step for static models is integrated until
                    convergence to a fixed point, which entails setting
                    Δ<italic>t</italic> = ∞; see
                        <xref ref-type="bibr" rid="pcbi.1000211-Friston6">[15]</xref>. Note that making
                    <italic>n</italic> = 0 renders the roughness
                    parameter irrelevant because this only affects the precision of generalised
                    motion.</p>
                <sec id="s3b1">
                    <title>Estimation with static models</title>
                    <p>In static systems, the problem reduces to estimating the causes of inputs
                        after they are passed through some linear or nonlinear mapping to generate
                        observed responses. For simple nonlinear estimation, in the absence of prior
                        expectations about the causes, we have the nonlinear hierarchal model<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e071" xlink:type="simple"/><label>(38)</label></disp-formula>This is the same as Equation 33 but with unknown causes.
                        Here, the <bold>D</bold>-Step performs a nonlinear optimisation of the
                        states to estimate their most likely values and the <bold>M</bold>-Step
                        estimates the variance components at each level. As mentioned above, for
                        static systems,
                        Δ<italic>t</italic> = ∞
                        and <italic>n</italic> = 0. This renders it
                        a classical Gauss-Newton scheme for nonlinear model estimation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e072" xlink:type="simple"/><label>(39)</label></disp-formula>Empirical priors are embedded in the scheme through the
                        hierarchical construction of the prediction errors,
                        <italic>ε</italic> and their precision Π, in the usual way;
                        see Equation 11 and <xref ref-type="bibr" rid="pcbi.1000211-Friston6">[15]</xref> for more details.</p>
                </sec>
                <sec id="s3b2">
                    <title>Linear models and parametric empirical Bayes</title>
                    <p>When the model above is linear, we have the ubiquitous hierarchical linear
                        observation model used in Parametric Empirical Bayes (<bold>PEB</bold>;
                            <xref ref-type="bibr" rid="pcbi.1000211-Kass1">[8]</xref>) and mixed-effects analysis of covariance
                            (<bold>ANCOVA</bold>) analyses.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e073" xlink:type="simple"/><label>(40)</label></disp-formula>Here the <bold>D</bold>-Step converges after a single
                        iteration because the linearity of this model renders the Laplace assumption
                        exact. In this context, the <bold>M</bold>-Step becomes a classical
                        restricted maximum likelihood (<bold>ReML</bold>) estimation of the
                        hierarchical covariance components,
                                Σ<sup>(<italic>i</italic>)<italic>z</italic></sup>. It is
                        interesting to note that the <bold>ReML</bold> objective function and the
                        variational energy are formally identical under this model <xref ref-type="bibr" rid="pcbi.1000211-Friston6">[15]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Harville1">[18]</xref>. <xref ref-type="fig" rid="pcbi-1000211-g003">Figure 3</xref> shows a
                        comparative evaluation of <bold>ReML</bold> and <bold>DEM</bold> using the
                        same data. The estimates are similar but not identical. This is because
                            <bold>DEM</bold> hyperparameterises the covariance as a linear mixture
                        of precisions, whereas the <bold>ReML</bold> scheme used a linear mixture of
                        covariance components.</p>
                    <fig id="pcbi-1000211-g003" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g003</object-id>
                        <label>Figure 3</label>
                        <caption>
                            <title>Example of estimation under a mixed-effects or hierarchical
                                linear model.</title>
                            <p>The inversion was cross-validated with expectation maximization (EM),
                                where the M-step corresponds to restricted maximum likelihood
                                (ReML). This example used a simple two-level model that embodies
                                empirical shrinkage priors on the first-level parameters. These
                                models are also known as parametric empirical Bayes (PEB) models
                                (left). Causes were sampled from the unit normal density to generate
                                a response, which was used to recover the causes, given the
                                parameters. Slight differences in the hyperparameter estimates
                                (upper right), due to a different hyperparameterisation, have little
                                effect on the conditional means of the unknown causes (lower right),
                                which are almost indistinguishable.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g003" xlink:type="simple"/>
                    </fig>
                </sec>
                <sec id="s3b3">
                    <title>Covariance component estimation and Gaussian process models</title>
                    <p>When there are many more causes then observations, a common device is to
                        eliminate the causes in Equation 40 by recursive substitution to give a
                        model that generates sample covariances and is formulated in terms of
                        covariance components (i.e., hyperparameters).<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e074" xlink:type="simple"/><label>(41)</label></disp-formula>Inversion then reduces to iterating the <bold>M</bold>-step.
                        The causes can then be recovered from the hyperparameters using Equation 39
                        and the matrix inversion lemma. This can be useful when inverting ill-posed
                        linear models (e.g., the electromagnetic inversion problem; <xref ref-type="bibr" rid="pcbi.1000211-Mattout1">[25]</xref>). Furthermore, by using shrinkage hyperpriors
                        one gets a behaviour known as automatic relevance determination
                        (<bold>ARD</bold>), where irrelevant components are essentially switched off
                            <xref ref-type="bibr" rid="pcbi.1000211-Tipping1">[26]</xref>. This leads to sparse models of the data that
                        are optimised automatically.</p>
                    <p>The model in Equation 41 is also referred to as a Gaussian process model
                            <xref ref-type="bibr" rid="pcbi.1000211-Ripley1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1000211-Kim1">[29]</xref>. The basic idea
                        behind Gaussian process modelling is to replace priors
                            <italic>p</italic>(<italic>v</italic>) on the parameters of the mapping,
                            <italic>g</italic>(<italic>v</italic>):
                            <italic>v</italic>→<italic>y</italic> with a prior on the space
                        of mappings; <italic>p</italic>(<italic>g</italic>(<italic>v</italic>)). The
                        simplest is a Gaussian process prior (<bold>GPP</bold>), specified by a
                        Gaussian covariance function of the response,
                            Σ(<italic>y</italic>|<italic>λ</italic>). The form of
                        this <bold>GPP</bold> is furnished by the hierarchical structure of the
                    HDM.</p>
                </sec>
                <sec id="s3b4">
                    <title>Deconvolution and dynamic models</title>
                    <p>In deconvolution problems, the objective is to estimate the inputs to a
                        dynamic system given its response and parameters.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e075" xlink:type="simple"/><label>(42)</label></disp-formula>This model is similar to Equation 37 but now we have random
                        fluctuations on the unknown states. Estimation of the states proceeds in the
                            <bold>D</bold>-Step. Recall the <bold>E</bold>-Step is redundant because
                        the parameters are known. When Σ<sup>(1)</sup> is known, the
                        <bold>M</bold>-Step is also unnecessary and <bold>DEM</bold> reduces to
                        deconvolution. This is related to Bayesian deconvolution or filtering under
                        state-space models:</p>
                </sec>
                <sec id="s3b5">
                    <title>State-space models and filtering</title>
                    <p>State-space models have the following form in discrete time and rest on a
                        vector autoregressive (<bold>VAR</bold>) formulation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e076" xlink:type="simple"/><label>(43)</label></disp-formula>where <italic>w<sub>t</sub></italic> is a standard noise
                        term. These models are parameterised by a system matrix <italic>A</italic>,
                        an input matrix <italic>B</italic>, and an observation matrix
                                <italic>g<sub>x</sub></italic>. State-space models are special cases
                        of linear HDMs, where the system-noise can be treated as a cause with random fluctuations<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e077" xlink:type="simple"/><label>(44)</label></disp-formula>Notice that we have had to suppress state-noise in the HDM to
                        make a simple state-space model. These models are adopted by conventional
                        approaches for inference on hidden states in dynamic models:</p>
                    <p>Deconvolution under HDMs is related to Bayesian approaches to inference on
                        states using Bayesian belief update procedures (i.e., incremental or
                        recursive Bayesian filters). The conventional approach to online Bayesian
                        tracking of nonlinear or non-Gaussian systems employs extended Kalman
                        filtering <xref ref-type="bibr" rid="pcbi.1000211-Kalman1">[30]</xref> or sequential Monte Carlo methods such as
                        particle filtering. These Bayesian filters try to find the posterior
                        densities of the hidden states in a recursive and computationally expedient
                        fashion, assuming that the parameters and hyperparameters of the system are
                        known. The extended Kalman filter is a generalisation of the Kalman filter
                        in which the linear operators, of the state-space equations, are replaced by
                        their partial derivatives evaluated at the current conditional mean. See
                        also Wang and Titterington <xref ref-type="bibr" rid="pcbi.1000211-Wang1">[31]</xref> for a careful analysis of variational
                        Bayes for continuous linear dynamical systems and <xref ref-type="bibr" rid="pcbi.1000211-Srensen1">[32]</xref> for a review
                        of the statistical literature on continuous nonlinear dynamical systems.
                        These treatments belong to the standard class of schemes that assume Wiener
                        or diffusion processes for state-noise and, unlike HDM, do not consider
                        generalised motion.</p>
                    <p>In terms of establishing the generality of the HDM, it is sufficient to note
                        that Bayesian filters simply estimate the conditional density on the hidden
                        states of a HDM. As intimated in the <xref ref-type="sec" rid="s1">introduction</xref>, their underlying state-space models assume that
                                <italic>z<sub>t</sub></italic> and <italic>w<sub>t</sub></italic>
                        are serially independent to induce a Markov property over sequential
                        observations. This pragmatic but questionable assumption means the
                        generalised motion of the random terms have zero precision and there is no
                        point in representing generalised states. We have presented a fairly
                        thorough comparative evaluation of <bold>DEM</bold> and extended Kalman
                        filtering (and particle filtering) in <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref>.
                        <bold>DEM</bold> is consistently more accurate because it harvests empirical
                        priors in generalised coordinates of motion. Furthermore, <bold>DEM</bold>
                        can be used for both inference on hidden states and the random fluctuations
                        driving them, because it uses an explicit conditional density
                            <italic>q</italic>(<italic>x̃</italic>,<italic>ṽ</italic>)
                        over both.</p>
                </sec>
            </sec>
            <sec id="s3c">
                <title>Models with Unknown States and Parameters</title>
                <p>In all the examples below, both the parameters and states are unknown. This
                    entails a dual or triple estimation problem, depending on whether the
                    hyperparameters are known. We will start with simple static models and work
                    towards more complicated dynamic variants. See <xref ref-type="bibr" rid="pcbi.1000211-Ghahramani1">[33]</xref> for a
                    comprehensive review of unsupervised learning for many of the models in this
                    section. This class of models is often discussed under the rhetoric of blind
                    source separation (BSS), because the inversion is blind to the parameters that
                    control the mapping from sources or causes to observed signals.</p>
                <sec id="s3c1">
                    <title>Principal components analysis</title>
                    <p>The Principal Components Analysis (<bold>PCA</bold>) model assumes that
                        uncorrelated causes are mixed linearly to form a static observation. This is
                        a <italic>m</italic> = 1 model with no
                        observation noise; i.e., Σ<sup>(1)<italic>z</italic></sup> = 0.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e078" xlink:type="simple"/><label>(45)</label></disp-formula>where priors on
                            <italic>v</italic><sup>(1)</sup> = <italic>z</italic><sup>(2)</sup>
                        render them orthonormal Σ<italic>
                            <sup>v</sup>
                        </italic> = <italic>I</italic>. There is no
                            <bold>M</bold>-Step here because there are no hyperparameters to
                        estimate. The <bold>D</bold>-Step estimates the causes under the unitary
                        shrinkage priors on their amplitude and the <bold>E</bold>-Step updates the
                        parameters to account for the data. Clearly, there are more efficient ways
                        of inverting this model than using <bold>DEM</bold>; for example, using the
                        eigenvectors of the sample covariance of the data. However, our point is
                        that <bold>PCA</bold> is a special case of an HDM and that any optimal
                        solution will optimise variational action or energy. Nonlinear
                        <bold>PCA</bold> is exactly the same but allowing for a nonlinear generating function.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e079" xlink:type="simple"/><label>(46)</label></disp-formula>See <xref ref-type="bibr" rid="pcbi.1000211-Friston8">[34]</xref> for an example of nonlinear
                        <bold>PCA</bold> with a bilinear model applied to neuroimaging data to
                        disclose interactions among modes of brain activity.</p>
                </sec>
                <sec id="s3c2">
                    <title>Factor analysis and probabilistic PCA</title>
                    <p>The model for factor analysis is exactly the same as for <bold>PCA</bold> but
                        allowing for observation error<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e080" xlink:type="simple"/><label>(47)</label></disp-formula>When the covariance of the observation error is spherical;
                        e.g.,
                            Σ<sup>(1)<italic>z</italic></sup> = <italic>λ</italic><sup>(1)<italic>z</italic></sup><italic>I</italic>,
                        this is also known as a probabilistic <bold>PCA</bold> model <xref ref-type="bibr" rid="pcbi.1000211-Tipping2">[35]</xref>. The critical distinction, from the point of
                        view of the HDM, is that the <bold>M</bold>-Step is now required to estimate
                        the error variance. See <xref ref-type="fig" rid="pcbi-1000211-g004">Figure
                            4</xref> for a simple example of factor analysis using <bold>DEM</bold>.
                        Nonlinear variants of factor analysis obtain by analogy with Equation 46.</p>
                    <fig id="pcbi-1000211-g004" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g004</object-id>
                        <label>Figure 4</label>
                        <caption>
                            <title>Example of Factor Analysis using a hierarchical model, in which
                                the causes have deterministic and stochastic components.</title>
                            <p>Parameters and causes were sampled from the unit normal density to
                                generate a response, which was then used for their estimation. The
                                aim was to recover the causes without knowing the parameters, which
                                is effected with reasonable accuracy (upper). The conditional
                                estimates of the causes and parameters are shown in lower panels,
                                along with the increase in free-energy or log-evidence, with the
                                number of DEM iterations (lower left). Note that there is an
                                arbitrary affine mapping between the conditional means of the causes
                                and their true values, which we estimated, <italic>post hoc</italic>
                                to show the correspondence in the upper panel.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g004" xlink:type="simple"/>
                    </fig>
                </sec>
                <sec id="s3c3">
                    <title>Independent component analysis</title>
                    <p>Independent component analysis (<bold>ICA</bold>) decomposes the observed
                        response into a linear mixture of non-Gaussian causes <xref ref-type="bibr" rid="pcbi.1000211-Bell1">[36]</xref>. Non-Gaussian
                        causal states are implemented simply in
                        <italic>m</italic> = 2 hierarchical models
                        with a nonlinear transformation at higher levels. <bold>ICA</bold>
                        corresponds to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e081" xlink:type="simple"/><label>(48)</label></disp-formula>Where, as for <bold>PCA</bold>, Σ<italic>
                            <sup>v</sup>
                        </italic> = <italic>I</italic>. The
                        nonlinear function <italic>g</italic><sup>(2)</sup> transforms a Gaussian
                        cause, specified by the priors at the third level, into a non-Gaussian cause
                        and plays the role of a probability integral transform. Note that there are
                        no hyperparameters to estimate and consequently there is no
                        <bold>M</bold>-Step. It is interesting to examine the relationship between
                        nonlinear <bold>PCA</bold> and <bold>ICA</bold>; the key difference is that
                        the nonlinearity is in the first level in <bold>PCA</bold>, as opposed to
                        the second in <bold>ICA</bold>. Usually, in <bold>ICA</bold> the probability
                        integral transform is pre-specified to render the second-level causes
                        supra-Gaussian. From the point of view of a HDM this corresponds to
                        specifying precise priors on the second-level parameters. However,
                        <bold>DEM</bold> can fit unknown distributions by providing conditional
                        estimates of both the mixing matrix
                        <italic>θ</italic><sup>(1)</sup> and the probability integral
                        transform implicit in
                            <italic>g</italic>(<italic>v</italic><sup>(2)</sup>,<italic>θ</italic><sup>(2)</sup>).</p>
                </sec>
                <sec id="s3c4">
                    <title>Sparse coding</title>
                    <p>In the same way that factor analysis is a generalisation of <bold>PCA</bold>
                        to non-Gaussian causes, <bold>ICA</bold> can be extended to form
                        sparse-coding models of the sort proposed by Olshausen and Fields <xref ref-type="bibr" rid="pcbi.1000211-Olshausen1">[37]</xref> by allowing observation error.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e082" xlink:type="simple"/><label>(49)</label></disp-formula>This is exactly the same as the <bold>ICA</bold> model but
                        with the addition of observation error. By choosing
                            <italic>g</italic><sup>(2)</sup> to create heavy-tailed (supra-Gaussian)
                        second-level causes, sparse encoding is assured in the sense that the causes
                        will have small values on most occasions and large values on only a few.
                        Note the <bold>M</bold>-Step comes into play again for these models. All the
                        models considered so far are for static data. We now turn to BSS in dynamic
                        systems.</p>
                </sec>
                <sec id="s3c5">
                    <title>Blind deconvolution</title>
                    <p>Blind deconvolution tries to estimate the causes of an observed response
                        without knowing the parameters of the dynamical system producing it. This
                        represents the least constrained problem we consider and calls upon the same
                        HDM used for system identification. An empirical example of triple
                        estimation of states, parameters and hyperparameters can be found in <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref>. This example uses functional magnetic
                        resonance imaging time-series from a brain region to estimate not only the
                        underlying neuronal and hemodynamic states causing signals but the
                        parameters coupling experimental manipulations to neuronal activity. See
                        Friston et al. <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref> for further examples, ranging from the
                        simple convolution model considered next, through to systems showing
                        autonomous dynamics and deterministic chaos. Here we conclude with a simple
                            <italic>m</italic> = 2 linear
                        convolution model (Equation 42), as specified in <xref ref-type="table" rid="pcbi-1000211-t001">Table 1</xref>.</p>
                    <table-wrap id="pcbi-1000211-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.t001</object-id><label>Table 1</label><caption>
                            <title>Specification of a linear convolution model.</title>
                        </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000211-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.t001" xlink:type="simple"/><table>
                            <colgroup span="1">
                                <col align="left" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                                <col align="center" span="1"/>
                            </colgroup>
                            <thead>
                                <tr>
                                    <td align="left" colspan="1" rowspan="1">Level</td>
                                    <td align="left" colspan="1" rowspan="1"><italic>g</italic>(<italic>x</italic>,<italic>v</italic>)</td>
                                    <td align="left" colspan="1" rowspan="1"><italic>f</italic>(<italic>x</italic>,<italic>v</italic>)</td>
                                    <td align="left" colspan="1" rowspan="1">Π<italic>
                                            <sup>z</sup>
                                        </italic></td>
                                    <td align="left" colspan="1" rowspan="1">Π<italic>
                                            <sup>w</sup>
                                        </italic></td>
                                    <td align="left" colspan="1" rowspan="1"><italic>η</italic>(<italic>t</italic>)</td>
                                    <td align="left" colspan="1" rowspan="1">
                                        <italic>η<sup>θ</sup></italic>
                                    </td>
                                    <td align="left" colspan="1" rowspan="1">Π<italic>
                                            <sup>θ</sup>
                                        </italic></td>
                                    <td align="left" colspan="1" rowspan="1">
                                        <italic>η<sup>λ</sup></italic>
                                    </td>
                                    <td align="left" colspan="1" rowspan="1">Π<italic>
                                            <sup>λ</sup>
                                        </italic></td>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td align="left" colspan="1" rowspan="1"><italic>m</italic> = 1</td>
                                    <td align="left" colspan="1" rowspan="1">
                                        <italic>θ</italic>
                                        <sub>1</sub>
                                        <italic>x</italic>
                                    </td>
                                    <td align="left" colspan="1" rowspan="1"><italic>θ</italic><sub>2</sub><italic>x</italic>+<italic>θ</italic><sub>3</sub><italic>v</italic></td>
                                    <td align="left" colspan="1" rowspan="1">exp(<italic>λ<sup>z</sup></italic>)</td>
                                    <td align="left" colspan="1" rowspan="1">exp(<italic>λ<sup>w</sup></italic>)</td>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1">0</td>
                                    <td align="left" colspan="1" rowspan="1">
                                        <italic>e</italic>
                                        <sup>−8</sup>
                                    </td>
                                    <td align="left" colspan="1" rowspan="1">0</td>
                                    <td align="left" colspan="1" rowspan="1">
                                        <italic>e</italic>
                                        <sup>−16</sup>
                                    </td>
                                </tr>
                                <tr>
                                    <td align="left" colspan="1" rowspan="1"><italic>m</italic> = 2</td>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1">1</td>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1">0</td>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1"/>
                                    <td align="left" colspan="1" rowspan="1"/>
                                </tr>
                            </tbody>
                        </table></alternatives></table-wrap>
                    <p>In this model, causes or inputs perturb the hidden states, which decay
                        exponentially to produce an output that is a linear mixture of hidden
                        states. Our example used a single input, two hidden states and four outputs.
                        To generate data, we used a deterministic Gaussian bump function input
                            <italic>v</italic><sup>(1)</sup> = exp(<sup>1</sup>/<sub>4</sub>(<italic>t</italic>−12)<sup>2</sup>)
                        and the following parameters<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e083" xlink:type="simple"/><label>(50)</label></disp-formula>During inversion, the cause is unknown and was subject to
                        mildly informative (zero mean and unit precision) shrinkage priors. We also
                        treated two of the parameters as unknown; one parameter from the observation
                        function (the first) and one from the state equation (the second). These
                        parameters had true values of 0.125 and −0.5, respectively, and
                        uninformative shrinkage priors. The priors on the hyperparameters, sometimes
                        referred to as hyperpriors were similarly uninformative. These Gaussian
                        hyperpriors effectively place lognormal hyperpriors on the precisions
                        (strictly speaking, this invalidates the assumption of a linear
                        hyperparameterisation but the effects are numerically small), because the
                        precisions scale as exp(<italic>λ<sup>z</sup></italic>) and
                                exp(<italic>λ<sup>w</sup></italic>). <xref ref-type="fig" rid="pcbi-1000211-g005">Figure 5</xref> shows a schematic of the
                        generative model and the implicit recognition scheme based on prediction
                        errors. This scheme can be regarded as a message passing scheme that is
                        considered in more depth in the next section.</p>
                    <fig id="pcbi-1000211-g005" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g005</object-id>
                        <label>Figure 5</label>
                        <caption>
                            <title>This schematic shows the linear convolution model used in the
                                subsequent figure in terms of a directed Bayesian graph.</title>
                            <p>In this model, a simple Gaussian ‘bump’ function
                                acts as a cause to perturb two coupled hidden states. Their dynamics
                                are then projected to four response variables, whose time-courses
                                are cartooned on the left. This figure also summarises the
                                architecture of the implicit inversion scheme (right), in which
                                precision-weighted prediction errors drive the conditional modes to
                                optimise variational action. Critically, the prediction errors
                                propagate their effects up the hierarchy (c.f., Bayesian belief
                                propagation or message passing), whereas the predictions are passed
                                down the hierarchy. This sort of scheme can be implemented easily in
                                neural networks (see last section and <xref ref-type="bibr" rid="pcbi.1000211-Friston5">[5]</xref> for a
                                neurobiological treatment). This generative model uses a single
                                cause <italic>v</italic><sup>(1)</sup>, two dynamic states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e084" xlink:type="simple"/></inline-formula> and four outputs
                                    <italic>y</italic><sub>1</sub>,…,<italic>y</italic><sub>4</sub>.
                                The lines denote the dependencies of the variables on each other,
                                summarised by the equations (in this example both the equations were
                                simple linear mappings). This is effectively a linear convolution
                                model, mapping one cause to four outputs, which form the inputs to
                                the recognition model (solid arrow). The inputs to the four data or
                                sensory channels are also shown as an image in the insert.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g005" xlink:type="simple"/>
                    </fig>
                    <p><xref ref-type="fig" rid="pcbi-1000211-g006">Figure 6</xref> summarises the
                        results after convergence of <bold>DEM</bold> (about sixteen iterations
                        using an embedding order of
                        <italic>n</italic> = 6, with a roughness
                        hyperparameter,
                        <italic>γ</italic> = 4). Each row
                        corresponds to a level in the model, with causes on the left and hidden
                        states on the right. The first (upper left) panel shows the predicted
                        response and the error on this response. For the hidden states (upper right)
                        and causes (lower left) the conditional mode is depicted by a coloured line
                        and the 90% conditional confidence intervals by the grey area. It
                        can be seen that there is a pleasing correspondence between the conditional
                        mean and veridical states (grey lines). Furthermore, the true values lie
                        largely within the 90% confidence intervals; similarly for the
                        parameters. This example illustrates the recovery of states, parameters and
                        hyperparameters from observed time-series, given just the form of a model.</p>
                    <fig id="pcbi-1000211-g006" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g006</object-id>
                        <label>Figure 6</label>
                        <caption>
                            <title>The predictions and conditional densities on the states and
                                parameters of the linear convolution model of the previous figure.</title>
                            <p>Each row corresponds to a level, with causes on the left and hidden
                                states on the right. In this case, the model has just two levels.
                                The first (upper left) panel shows the predicted response and the
                                error on this response (their sum corresponds to the observed data).
                                For the hidden states (upper right) and causes (lower left) the
                                conditional mode is depicted by a coloured line and the
                                90% conditional confidence intervals by the grey area.
                                These are sometimes referred to as “tubes”.
                                Finally, the grey lines depict the true values used to generate the
                                response. Here, we estimated the hyperparameters, parameters and the
                                states. This is an example of triple estimation, where we are trying
                                to infer the states of the system as well as the parameters
                                governing its causal architecture. The hyperparameters correspond to
                                the precision of random fluctuations in the response and the hidden
                                states. The free parameters correspond to a single parameter from
                                the state equation and one from the observer equation that govern
                                the dynamics of the hidden states and response, respectively. It can
                                be seen that the true value of the causal state lies within the
                                90% confidence interval and that we could infer with
                                substantial confidence that the cause was non-zero, when it occurs.
                                Similarly, the true parameter values lie within fairly tight
                                confidence intervals (red bars in the lower right).</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g006" xlink:type="simple"/>
                    </fig>
                </sec>
                <sec id="s3c6">
                    <title>Summary</title>
                    <p>This section has tried to show that the HDM encompasses many standard static
                        and dynamic observation models. It is further evident than many of these
                        models could be extended easily within the hierarchical framework. <xref ref-type="fig" rid="pcbi-1000211-g007">Figure 7</xref> illustrates this
                        by providing a ontology of models that rests on the various constraints
                        under which HDMs are specified. This partial list suggests that only a
                        proportion of potential models have been covered in this section.</p>
                    <fig id="pcbi-1000211-g007" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g007</object-id>
                        <label>Figure 7</label>
                        <caption>
                            <title>Ontology of models starting with a simple general linear model
                                with two levels (the PCA model).</title>
                            <p>This ontology is one of many that could be constructed and is based
                                on the fact that hierarchical dynamic models have several attributes
                                that can be combined to create an infinite number of models; some of
                                which are shown in the figure. These attributes include; (i) the
                                number of levels or depth; (ii) for each level, linear or nonlinear
                                output functions; (iii) with or without random fluctuations; (iii)
                                static or dynamic (iv), for dynamic levels, linear or nonlinear
                                equations of motion; (v) with or without state noise and, finally,
                                (vi) with or without generalised coordinates.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g007" xlink:type="simple"/>
                    </fig>
                    <p>In summary, we have seen that endowing dynamical models with a hierarchical
                        architecture provides a general framework that covers many models used for
                        estimation, identification and unsupervised learning. A hierarchical
                        structure, in conjunction with nonlinearities, can emulate non-Gaussian
                        behaviours, even when random effects are Gaussian. In a dynamic context, the
                        level at which the random effects enter controls whether the system is
                        deterministic or stochastic and nonlinearities determine whether their
                        effects are additive or multiplicative. <bold>DEM</bold> was devised to find
                        the conditional moments of the unknown quantities in these nonlinear,
                        hierarchical and dynamic models. As such it emulates procedures as diverse
                        as independent components analysis and Bayesian filtering, using a single
                        scheme. In the final section, we show that a <bold>DEM</bold>-like scheme
                        might be implemented in the brain. If this is true, the brain could, in
                        principle, employ any of the models considered in this section to make
                        inferences about the sensory data it harvests.</p>
                </sec>
            </sec>
            <sec id="s3d">
                <title>Neuronal Implementation</title>
                <p>In this final section, we revisit <bold>DEM</bold> and show that it can be
                    formulated as a relatively simple neuronal network that bears many similarities
                    to real networks in the brain. We have made the analogy between the
                    <bold>DEM</bold> and perception in previous communications; here we focus on the
                    nature of recognition in generalised coordinates. In brief, deconvolution of
                    hidden states and causes from sensory data (<bold>D</bold>-step) may correspond
                    to perceptual inference; optimising the parameters of the model
                    (<bold>E</bold>-step) may correspond to perceptual learning through changes in
                    synaptic efficacy and optimising the precision hyperparameters
                    (<bold>M</bold>-step) may correspond to encoding perceptual salience and
                    uncertainty, through neuromodulatory mechanisms.</p>
                <sec id="s3d1">
                    <title>Hierarchical models in the brain</title>
                    <p>A key architectural principle of the brain is its hierarchical organisation
                            <xref ref-type="bibr" rid="pcbi.1000211-Maunsell1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1000211-Mesulam1">[41]</xref>. This has been
                        established most thoroughly in the visual system, where lower (primary)
                        areas receive sensory input and higher areas adopt a multimodal or
                        associational role. The neurobiological notion of a hierarchy rests upon the
                        distinction between forward and backward connections <xref ref-type="bibr" rid="pcbi.1000211-Rockland1">[42]</xref>–<xref ref-type="bibr" rid="pcbi.1000211-Angelucci1">[45]</xref>. This
                        distinction is based upon the specificity of cortical layers that are the
                        predominant sources and origins of extrinsic connections (extrinsic
                        connections couple remote cortical regions, whereas intrinsic connections
                        are confined to the cortical sheet). Forward connections arise largely in
                        superficial pyramidal cells, in supra-granular layers and terminate on spiny
                        stellate cells of layer four in higher cortical areas <xref ref-type="bibr" rid="pcbi.1000211-Felleman1">[40]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-DeFelipe1">[46]</xref>. Conversely, backward connections arise
                        largely from deep pyramidal cells in infra-granular layers and target cells
                        in the infra and supra-granular layers of lower cortical areas. Intrinsic
                        connections mediate lateral interactions between neurons that are a few
                        millimetres away. There is a key functional asymmetry between forward and
                        backward connections that renders backward connections more modulatory or
                        nonlinear in their effects on neuronal responses (e.g., <xref ref-type="bibr" rid="pcbi.1000211-Sherman1">[44]</xref>; see also Hupe et al. <xref ref-type="bibr" rid="pcbi.1000211-Hupe1">[47]</xref>). This is
                        consistent with the deployment of voltage-sensitive NMDA receptors in the
                        supra-granular layers that are targeted by backward connections <xref ref-type="bibr" rid="pcbi.1000211-Rosier1">[48]</xref>. Typically, the synaptic dynamics of backward
                        connections have slower time constants. This has led to the notion that
                        forward connections are driving and illicit an obligatory response in higher
                        levels, whereas backward connections have both driving and modulatory
                        effects and operate over larger spatial and temporal scales.</p>
                    <p>The hierarchical structure of the brain speaks to hierarchical models of
                        sensory input. We now consider how this functional architecture can be
                        understood under the inversion of HDMs by the brain. We first consider
                        inference on states or perception.</p>
                </sec>
                <sec id="s3d2">
                    <title>Perceptual inference</title>
                    <p>If we assume that the activity of neurons encode the conditional mode of
                        states, then the <bold>D</bold>-step specifies the neuronal dynamics
                        entailed by perception or recognizing states of the world from sensory data.
                        Furthermore, if we ignore mean-field terms; i.e., discount the effects of
                        conditional uncertainty about the parameters when optimising the states,
                        Equation 23 prescribes very simple recognition dynamics<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e085" xlink:type="simple"/><label>(51)</label></disp-formula>Where, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e086" xlink:type="simple"/></inline-formula> is prediction error multiplied by its precision, which we
                        have re-parameterised in terms of a covariance component, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e087" xlink:type="simple"/></inline-formula>. Here, the matrix Λ can be thought of as lateral
                        connections among error-units. Equation 51 is an ordinary differential
                        equation that describes how neuronal states self-organise, when exposed to
                        sensory input. The form of Equation 51 is quite revealing, it suggests two
                        distinct populations of neurons; <italic>state-units</italic> whose activity
                        encodes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e088" xlink:type="simple"/></inline-formula> and <italic>error-units</italic> encoding
                            <italic>ξ</italic>(<italic>t</italic>), with one error-unit for
                        each state. Furthermore, the activities of error-units are a function of the
                        states and the dynamics of state-units are a function of prediction error.
                        This means the two populations pass messages to each other and to
                        themselves. The messages passed among the states, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e089" xlink:type="simple"/></inline-formula> mediate empirical priors on their motion, while the
                        lateral connections among the error-units,
                            −Λ<italic>ξ</italic> weight prediction errors
                        in proportion to their precision.</p>
                </sec>
                <sec id="s3d3">
                    <title>Hierarchical message passing</title>
                    <p>If we unpack these equations we can see the hierarchical nature of this
                        message passing (see <xref ref-type="fig" rid="pcbi-1000211-g008">Figure 8</xref>).<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e090" xlink:type="simple"/><label>(52)</label></disp-formula>This shows that error-units receive messages from the states
                        in the same level and the level above, whereas states are driven by
                        error-units in the same level and the level below. Critically, inference
                        requires only the prediction error from the lower level
                                <italic>ξ</italic><sup>(<italic>i</italic>)</sup> and the
                        level in question,
                            <italic>ξ</italic><sup>(<italic>i</italic>+1)</sup>.
                        These constitute bottom-up and lateral messages that drive conditional means <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e091" xlink:type="simple"/></inline-formula> towards a better prediction, to explain away the
                        prediction error in the level below. These top-down and lateral predictions
                        correspond to <italic>g̃</italic><sup>(<italic>i</italic>)</sup>
                        and <italic>f̃</italic><sup>(<italic>i</italic>)</sup>. This is
                        the essence of recurrent message passing between hierarchical levels to
                        optimise free-energy or suppress prediction error; i.e., recognition
                        dynamics.</p>
                    <fig id="pcbi-1000211-g008" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g008</object-id>
                        <label>Figure 8</label>
                        <caption>
                            <title>Schematic detailing the neuronal architectures that encode an
                                ensemble density on the states and parameters of one level in a
                                hierarchical model.</title>
                            <p>This schematic shows the speculative cells of origin of forward
                                driving connections that convey prediction error from a lower area
                                to a higher area and the backward connections that are used to
                                construct predictions. These predictions try to explain away input
                                from lower areas by suppressing prediction error. In this scheme,
                                the sources of forward connections are the superficial pyramidal
                                cell population and the sources of backward connections are the deep
                                pyramidal cell population. The differential equations relate to the
                                optimisation scheme detailed in the main text and their constituent
                                terms are placed alongside the corresponding connections. The
                                state-units and their efferents are in black and the error-units in
                                red, with causes on the left and hidden states on the right. For
                                simplicity, we have assumed the output of each level is a function
                                of, and only of, the hidden states. This induces a hierarchy over
                                levels and, within each level, a hierarchical relationship between
                                states, where hidden states predict causes.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g008" xlink:type="simple"/>
                    </fig>
                    <p>The connections from error to state-units have a simple form that depends on
                        the gradients of the model's functions; from Equation 12<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e092" xlink:type="simple"/><label>(53)</label></disp-formula>These pass prediction errors forward to state-units in the
                        higher level and laterally to state-units at the same level. The reciprocal
                        influences of the state on the error-units are mediated by backward
                        connections and lateral interactions. In summary, all connections between
                        error and state-units are reciprocal, where the only connections that link
                        levels are forward connections conveying prediction error to state-units and
                        reciprocal backward connections that mediate predictions (see <xref ref-type="fig" rid="pcbi-1000211-g008">Figure 8</xref>).</p>
                    <p>We can identify error-units with superficial pyramidal cells, because the
                        only messages that pass up the hierarchy are prediction errors and
                        superficial pyramidal cells originate forward connections in the brain. This
                        is useful because it is these cells that are primarily responsible for
                        electroencephalographic (EEG) signals that can be measured non-invasively.
                        Similarly the only messages that are passed down the hierarchy are the
                        predictions from state-units that are necessary to form prediction errors in
                        lower levels. The sources of extrinsic backward connections are largely the
                        deep pyramidal cells and one might deduce that these encode the expected
                        causes of sensory states (see <xref ref-type="bibr" rid="pcbi.1000211-Mumford1">[49]</xref> and <xref ref-type="fig" rid="pcbi-1000211-g009">Figure 9</xref>). Critically, the
                        motion of each state-unit is a linear mixture of bottom-up prediction error;
                        see Equation 52. This is exactly what is observed physiologically; in that
                        bottom-up driving inputs elicit obligatory responses that do not depend on
                        other bottom-up inputs. The prediction error itself is formed by predictions
                        conveyed by backward and lateral connections. These influences embody the
                        nonlinearities implicit in
                            <italic>g̃</italic><sup>(<italic>i</italic>)</sup> and
                            <italic>f̃</italic><sup>(<italic>i</italic>)</sup>. Again,
                        this is entirely consistent with the nonlinear or modulatory characteristics
                        of backward connections.</p>
                    <fig id="pcbi-1000211-g009" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g009</object-id>
                        <label>Figure 9</label>
                        <caption>
                            <title>Schematic detailing the neuronal architectures that encode an
                                ensemble density on the states and parameters of hierarchical
                                models.</title>
                            <p>This schematic shows how the neuronal populations of the previous
                                figure may be deployed hierarchically within three cortical areas
                                (or macro-columns). Within each area the cells are shown in relation
                                to the laminar structure of the cortex that includes supra-granular
                                (SG) granular (L4) and infra-granular (IG) layers.</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g009" xlink:type="simple"/>
                    </fig>
                </sec>
                <sec id="s3d4">
                    <title>Encoding generalised motion</title>
                    <p>Equation 51 is cast in terms of generalised states. This suggests that the
                        brain has an explicit representation of generalised motion. In other words,
                        there are separable neuronal codes for different orders of motion. This is
                        perfectly consistent with empirical evidence for distinct populations of
                        neurons encoding elemental visual features and their motion (e.g.,
                        motion-sensitive area V5; <xref ref-type="bibr" rid="pcbi.1000211-Zeki1">[39]</xref>). The analysis in this paper suggests
                        that acceleration and higher-order motion are also encoded; each order
                        providing constraints on a lower order, through <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e093" xlink:type="simple"/></inline-formula>. Here, <italic>D</italic> represents a fixed connectivity
                        matrix that mediates these temporal constraints. Notice that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e094" xlink:type="simple"/></inline-formula> only when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e095" xlink:type="simple"/></inline-formula>. This means it is perfectly possible to represent the
                        motion of a state that is inconsistent with the state of motion. The motion
                        after-effect is a nice example of this, where a motion percept coexists with
                        no change in the perceived location of visual stimuli. The encoding of
                        generalised motion may mean that we represent paths or trajectories of
                        sensory dynamics over short periods of time and that there is no perceptual
                        instant (c.f., the remembered present; <xref ref-type="bibr" rid="pcbi.1000211-Edelman1">[50]</xref>). One could
                        speculate that the encoding of different orders of motion may involve rate
                        codes in distinct neuronal populations or multiplexed temporal codes in the
                        same populations (e.g., in different frequency bands). See <xref ref-type="bibr" rid="pcbi.1000211-Grossberg1">[51]</xref> for a neurobiologically realistic treatment
                        of temporal dynamics in decision-making during motion perception and <xref ref-type="bibr" rid="pcbi.1000211-Grossberg2">[52]</xref> for a discussion of synchrony and attentive
                        learning in laminar thalamocortical circuits.</p>
                    <p>When dealing with empirical data-sequences one has to contend with sparse and
                        discrete sampling. Analogue systems, like the brain can sample generalised
                        motion directly. When sampling sensory data, one can imagine easily how
                        receptors generate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e096" xlink:type="simple"/></inline-formula>. Indeed, it would be surprising to find any sensory system
                        that did not respond to a high-order derivative of changing sensory fields
                        (e.g., acoustic edge detection; offset units in the visual system,
                            <italic>etc</italic>; <xref ref-type="bibr" rid="pcbi.1000211-Chait1">[53]</xref>). Note that sampling high-order
                        derivatives is formally equivalent to high-pass filtering sensory data. A
                        simple consequence of encoding generalised motion is, in
                        electrophysiological terms, the emergence of spatiotemporal receptive fields
                        that belie selectivity to particular sensory trajectories.</p>
                </sec>
                <sec id="s3d5">
                    <title>Perceptual learning and plasticity</title>
                    <p>The conditional expectations of the parameters,
                                <italic>µ<sup>θ</sup></italic> control the
                        construction of prediction error through backward and lateral connections.
                        This suggests that they are encoded in the strength of extrinsic and
                        intrinsic connections. If we define effective connectivity as the rate of
                        change of a unit's response with respect to its inputs, Equation 51
                        suggests an interesting antisymmetry in the effective connectivity between
                        the state and error-units. The effective connectivity from the states to the
                        error-units is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e097" xlink:type="simple"/></inline-formula>. This is simply the negative transpose of the effective
                        connectivity that mediates recognition dynamics; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e098" xlink:type="simple"/></inline-formula>. In other words, the effective connection from any state
                        to any error-unit has the same strength (but opposite sign) of the
                        reciprocal connection from the error to the state-unit. This means we would
                        expect to see connections reciprocated in the brain, which is generally the
                        case <xref ref-type="bibr" rid="pcbi.1000211-Zeki1">[39]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Felleman1">[40]</xref>. Furthermore,
                        we would not expect to see positive feedback loops; c.f., <xref ref-type="bibr" rid="pcbi.1000211-Crick1">[54]</xref>.
                        We now consider the synaptic efficacies underlying effective connectivity.</p>
                    <p>If synaptic efficacy encodes the parameter estimates, we can cast parameter
                        optimisation as changing synaptic connections. These changes have a
                        relatively simple form that is recognisable as associative plasticity. To
                        show this, we will make the simplifying but plausible assumption that the
                        brain's generative model is based on nonlinear functions
                        <italic>a</italic> of linear mixtures of states<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e099" xlink:type="simple"/><label>(54)</label></disp-formula>Under this assumption <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e100" xlink:type="simple"/></inline-formula> correspond to matrices of synaptic strengths or weights
                        and <italic>a</italic> can be understood as a neuronal activation function
                        that models nonlinear summation of presynaptic inputs over the dendritic
                        tree <xref ref-type="bibr" rid="pcbi.1000211-London1">[55]</xref>. This means that the synaptic connection to
                        the <italic>i</italic>th error from the <italic>j</italic>th state depends
                        on only one parameter, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e101" xlink:type="simple"/></inline-formula> which changes according to Equation 29<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e102" xlink:type="simple"/><label>(55)</label></disp-formula>This suggests that plasticity comprises an associative term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e103" xlink:type="simple"/></inline-formula> and a decay term mediating priors on the parameters. The
                        dynamics of the associative term are given by Equation 21 (and exploiting
                        the Kronecker form of Equation 22). The integral of this associative term is
                        simply the covariance between presynaptic input and postsynaptic prediction
                        error, summed over orders of motion. In short, it mediates associative or
                        Hebbian plasticity. The product of pre and postsynaptic signals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e104" xlink:type="simple"/></inline-formula> is modulated by an activity-dependent term, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e105" xlink:type="simple"/></inline-formula>, which is the gradient of the activation function at its
                        current level of input (and is constant for linear models). Critically,
                        updating the conditional estimates of the parameters, through synaptic
                        efficacies, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e106" xlink:type="simple"/></inline-formula>, uses local information that is available at each
                        error-unit. Furthermore, the same information is available at the synaptic
                        terminal of the reciprocal connection, where the <italic>i</italic>th
                        error-unit delivers presynaptic inputs to the <italic>j</italic>th state. In
                        principle, this enables reciprocal connections to change in tandem. Finally,
                        because plasticity is governed by two coupled ordinary differential
                        equations (Equation 55), connection strengths should change more slowly than
                        the neuronal activity they mediate. These theoretical predictions are
                        entirely consistent with empirical and computational characterisations of
                        plasticity <xref ref-type="bibr" rid="pcbi.1000211-Buonomano1">[56]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Martin1">[57]</xref>.</p>
                </sec>
                <sec id="s3d6">
                    <title>Perceptual salience and uncertainty</title>
                    <p>Equation 51 shows that the influence of prediction error is scaled by its
                        precision <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e107" xlink:type="simple"/></inline-formula> or covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e108" xlink:type="simple"/></inline-formula> that is a function of
                            <italic>µ<sup>λ</sup></italic>. This means that the
                        relative influence of bottom-up, lateral and top-down effects are modulated
                        by the conditional expectation of the hyperparameters. This selective
                        modulation of afferents mirrors the gain-control mechanisms invoked for
                        attention; e.g., <xref ref-type="bibr" rid="pcbi.1000211-Treue1">[58]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-MartinezTrujillo1">[59]</xref>.
                        Furthermore, they enact the sorts of mechanisms implicated in biased
                        competition models of spatial and object-based attention mediating visual
                        search <xref ref-type="bibr" rid="pcbi.1000211-Chelazzi1">[60]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Desimone1">[61]</xref>.</p>
                    <p>Equation 51 formulates this bias or gain-control in terms of lateral
                        connections, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e109" xlink:type="simple"/></inline-formula> among error-units. This means hyperparameter optimisation
                        would be realised, in the brain, as neuromodulation or plasticity of lateral
                        interactions among error-units. If we assume that the covariance is a linear
                        mixture of covariance components, <italic>R<sub>i</sub></italic> among
                        non-overlapping subsets of error-units, then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e110" xlink:type="simple"/><label>(56)</label></disp-formula>Where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e111" xlink:type="simple"/></inline-formula>. Under this hyperparameterisation, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e112" xlink:type="simple"/></inline-formula> modulates subsets of connections to encode a partition of
                        the covariance. Because each set of connections is a function of only one
                        hyperparameter, their plasticity is prescribed simply by Equation 31<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e113" xlink:type="simple"/><label>(57)</label></disp-formula>The quantities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e114" xlink:type="simple"/></inline-formula> might correspond to specialised (e.g., noradrenergic or
                        cholinergic) systems in the brain that broadcast their effects to the
                            <italic>i</italic>th subset of error-units to modulate their
                        responsiveness to each other. The activities of these units change
                        relatively slowly, in proportion to an associative term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e115" xlink:type="simple"/></inline-formula> and decay that mediates hyperpriors. The associative term
                        is basically the difference between the sample covariance of
                        precision-weighted prediction errors and the precision expected, under the
                        current value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e116" xlink:type="simple"/></inline-formula>.</p>
                    <p>As above, changes in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e117" xlink:type="simple"/></inline-formula> occur more slowly than the fast dynamics of the states
                        because they are driven by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e118" xlink:type="simple"/></inline-formula>, which accumulates energy gradients to optimise
                        variational action. One could think of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e119" xlink:type="simple"/></inline-formula> as the synaptic efficacy of lateral or intrinsic
                        connections that depend upon classical neuromodulatory inputs and other
                        slower synaptic dynamics (e.g., after-hyperpolarisation potentials and
                        molecular signalling). The physiological aspects of these dynamics provide
                        an interesting substrate for attentional mechanisms in the brain (see
                        Schroeder et al., <xref ref-type="bibr" rid="pcbi.1000211-Schroeder1">[62]</xref> for review) and are not unrelated to the
                        ideas in <xref ref-type="bibr" rid="pcbi.1000211-Yu1">[63]</xref>. These authors posit a role for acetylcholine
                        (an ascending modulatory neurotransmitter) in mediating expected
                        uncertainty. This is entirely consistent with the dynamics of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e120" xlink:type="simple"/></inline-formula> that are driven by the amplitude of prediction errors
                        encoding the relative precision of sensory signals and empirical priors.
                        Modulatory neurotransmitters have, characteristically, much slower time
                        constants, in terms of their synaptic effects, than glutamatergic
                        neurotransmission that is employed by cortico-cortical extrinsic
                        connections.</p>
                </sec>
                <sec id="s3d7">
                    <title>The mean-field partition</title>
                    <p>The mean-field approximation
                            <italic>q</italic>(<italic>ϑ</italic>) = <italic>q</italic>(<italic>u</italic>(<italic>t</italic>))<italic>q</italic>(<italic>θ</italic>)<italic>q</italic>(<italic>λ</italic>)
                        enables inference about perceptual states, causal regularities and context,
                        without representing the joint distribution explicitly; c.f., <xref ref-type="bibr" rid="pcbi.1000211-Rao1">[64]</xref>.
                        However, the optimisation of one set of sufficient statistics is a function
                        of the others. This has a fundamental implication for optimisation in the
                        brain (see <xref ref-type="fig" rid="pcbi-1000211-g010">Figure 10</xref>).
                        For example, ‘activity-dependent plasticity’ and
                        ‘functional segregation’ speak to reciprocal influences
                        between changes in states and connections; in that changes in connections
                        depend upon activity and changes in activity depend upon connections. Things
                        get more interesting when we consider three sets, because quantities
                        encoding precision must be affected by and affect neuronal activity and
                        plasticity. This places strong constraints on the neurobiological candidates
                        for these hyperparameters. Happily, the ascending neuromodulatory
                        neurotransmitter systems, such as dopaminergic and cholinergic projections,
                        have exactly the right characteristics: they are driven by activity in
                        presynaptic connections and can affect activity though classical
                        neuromodulatory effects at the post-synaptic membrane <xref ref-type="bibr" rid="pcbi.1000211-Tseng1">[65]</xref>, while also
                        enabling potentiation of connection strengths <xref ref-type="bibr" rid="pcbi.1000211-Brocher1">[66]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Gu1">[67]</xref>.
                        Furthermore, it is exactly these systems that have been implicated in
                        value-learning <xref ref-type="bibr" rid="pcbi.1000211-Friston9">[68]</xref>–<xref ref-type="bibr" rid="pcbi.1000211-Schultz1">[70]</xref>, attention and
                        the encoding of uncertainty <xref ref-type="bibr" rid="pcbi.1000211-Yu1">[63]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Niv1">[71]</xref>.</p>
                    <fig id="pcbi-1000211-g010" position="float">
                        <object-id pub-id-type="doi">10.1371/journal.pcbi.1000211.g010</object-id>
                        <label>Figure 10</label>
                        <caption>
                            <title>The ensemble density and its mean-field partition.</title>
                            <p><italic>q</italic>(<italic>ϑ</italic>) is the ensemble
                                density and is encoded in terms of the sufficient statistics of its
                                marginals. These statistics or variational parameters (e.g., mean or
                                expectation) change to extremise free-energy to render the ensemble
                                density an approximate conditional density on the causes of sensory
                                input. The mean-field partition corresponds to a factorization over
                                the sets comprising the partition. Here, we have used three sets
                                (neural activity, modulation and connectivity). Critically, the
                                optimisation of the parameters of any one set depends on the
                                parameters of the other sets. In this figure, we have focused on
                                means or expectations <italic>µ<sup>i</sup></italic> of
                                the marginal densities,
                                        <italic>q</italic>(<italic>ϑ<sup>i</sup></italic>) = <italic>N</italic>(<italic>ϑ<sup>i</sup></italic>:
                                        <italic>µ<sup>i</sup></italic>,<italic>C<sup>i</sup></italic>).</p>
                        </caption>
                        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.g010" xlink:type="simple"/>
                    </fig>
                </sec>
                <sec id="s3d8">
                    <title>Summary</title>
                    <p>We have seen that the brain has, in principle, the infrastructure needed to
                        invert hierarchical dynamic models of the sort considered in previous
                        sections. It is perhaps remarkable that such a comprehensive treatment of
                        generative models can be reduced to recognition dynamics that are as simple
                        as Equation 51. Having said this, the notion that the brain inverts
                        hierarchical models, using a <bold>DEM</bold>-like scheme, speaks to a range
                        of empirical facts about the brain:</p>
                    <list list-type="bullet">
                        <list-item>
                            <p>The hierarchical organisation of cortical areas (c.f., <xref ref-type="bibr" rid="pcbi.1000211-Zeki1">[39]</xref>)</p>
                        </list-item>
                        <list-item>
                            <p>Each area comprises distinct neuronal subpopulations, encoding
                                expected states of the world and prediction error (c.f., <xref ref-type="bibr" rid="pcbi.1000211-Kawato1">[72]</xref>).</p>
                        </list-item>
                        <list-item>
                            <p>Extrinsic forward connections convey prediction error (from
                                superficial pyramidal cells) and backward connections mediate
                                predictions, based on hidden and causal states (from deep pyramidal
                                cells) <xref ref-type="bibr" rid="pcbi.1000211-Mumford1">[49]</xref>.</p>
                        </list-item>
                        <list-item>
                            <p>Recurrent dynamics are intrinsically stable because they are trying
                                to suppress prediction error <xref ref-type="bibr" rid="pcbi.1000211-Crick1">[54]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Rao1">[64]</xref>.</p>
                        </list-item>
                        <list-item>
                            <p>Functional asymmetries in forwards (linear) and backwards (nonlinear)
                                connections may reflect their distinct roles in recognition (c.f.,
                                    <xref ref-type="bibr" rid="pcbi.1000211-Sherman1">[44]</xref>).</p>
                        </list-item>
                        <list-item>
                            <p>Principal cells elaborating predictions (e.g., deep pyramidal cells)
                                may show distinct (low-pass) dynamics, relative to those encoding
                                error (e.g., superficial pyramidal cells)</p>
                        </list-item>
                        <list-item>
                            <p>Lateral interactions may encode the relative precision of prediction
                                errors and change in a way that is consistent with classical
                                neuromodulation (c.f., <xref ref-type="bibr" rid="pcbi.1000211-Yu1">[63]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Niv1">[71]</xref>).</p>
                        </list-item>
                        <list-item>
                            <p>The rescaling of prediction errors by recurrent connections, in
                                proportion to their precision, affords a form of cortical bias or
                                gain control <xref ref-type="bibr" rid="pcbi.1000211-Desimone2">[73]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Abbott1">[74]</xref>.</p>
                        </list-item>
                        <list-item>
                            <p>The dynamics of plasticity and modulation of lateral interactions
                                encoding precision or uncertainty (which optimise a path-integral of
                                variational energy) must be slower than the dynamics of neuronal
                                activity (which optimise variational energy <italic>per
                            se</italic>)</p>
                        </list-item>
                        <list-item>
                            <p>Neuronal activity, synaptic efficacy and neuromodulation must all
                                affect each other; activity-dependent plasticity and neuromodulation
                                shape neuronal responses and:</p>
                        </list-item>
                        <list-item>
                            <p>Neuromodulatory factors play a dual role in modulating postsynaptic
                                responsiveness (e.g., through modulating in after-hyperpolarising
                                currents) and synaptic plasticity <xref ref-type="bibr" rid="pcbi.1000211-Brocher1">[66]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Gu1">[67]</xref>.</p>
                        </list-item>
                    </list>
                    <p>These observations pertain to the anatomy and physiology of neuronal
                        architectures; see Friston et al. <xref ref-type="bibr" rid="pcbi.1000211-Friston5">[5]</xref> for a
                        discussion of operational and cognitive issues, under a free-energy
                        principle for the brain.</p>
                    <p>We have tried to establish the generality of HDMs as a model that may be used
                        by the brain. However, there are many alternative formulations that could be
                        considered. Perhaps the work of Archambeau et al. <xref ref-type="bibr" rid="pcbi.1000211-Archambeau1">[75]</xref> is formally
                        the closest to one presented in this paper. These authors propose an
                        approach that is very similar to DEM but is framed in terms of SDEs. A
                        related formulation, with particle annihilation and symmetry breaking, has
                        been proposed <xref ref-type="bibr" rid="pcbi.1000211-Kappen1">[76]</xref> as a mechanism for learning. This work
                        adopts a path integral approach to optimal control theory and reinforcement
                        learning. Cortical processing as the statistical result of the activity of
                        neural ensembles is an established and important idea (e.g., <xref ref-type="bibr" rid="pcbi.1000211-John1">[77]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Freeman1">[78]</xref>). Although,
                        computationally intensive (see <xref ref-type="sec" rid="s4">Discussion</xref>), particle filtering can be efficient <xref ref-type="bibr" rid="pcbi.1000211-Beskos1">[79]</xref>; Furthermore, it can be combined with local
                        linearised sequential methods (e.g., the Ensemble Kalman Filter; <xref ref-type="bibr" rid="pcbi.1000211-Evensen1">[80]</xref>) to provide data assimilation methods for
                        huge data sets. In fact, Schiff and Sauer <xref ref-type="bibr" rid="pcbi.1000211-Schiff1">[81]</xref> have recently
                        proposed an Ensemble Kalman Filter for the control of cortical dynamics that
                        could have biological and engineering significance. Finally, <xref ref-type="bibr" rid="pcbi.1000211-Restrepo1">[82]</xref> proposes a path integral approach to particle
                        filtering for data assimilation. The common theme here is the use of
                        ensembles to represent more realistic and complicated conditional densities.
                        Although the biological relevance of these exciting developments remains to
                        be established they may provide insights into neuronal computations. They
                        also speak to ensembles of HDMs, under the Laplace assumption, to
                        approximate the conditional density with a mixture of Gaussians (Nelson
                        Trujillo-Barreto – personal communication).</p>
                    <p>Clearly, the theoretical treatment of this section calls for an enormous
                        amount of empirical verification and hypothesis testing, not least to
                        disambiguate among alternative theories and architectures. We have laid out
                        the neurobiological and psychophysical motivation for the neuronal
                        implementation of <bold>DEM</bold> in <xref ref-type="bibr" rid="pcbi.1000211-Friston3">[3]</xref> and <xref ref-type="bibr" rid="pcbi.1000211-Friston4">[4]</xref>. These papers deal with inference in the brain
                        and motivate an overarching free-energy principle, using the notion of
                        equilibrium densities and active agents. In <xref ref-type="bibr" rid="pcbi.1000211-Friston10">[83]</xref> we address
                        the face validity of the neuronal scheme described in this section, using
                        synthetic birds and the perceptual categorisation of birdsong. These papers
                        try to emulate empirical LFP and EEG studies to establish the sorts of
                        electrophysiological responses one would expect to see in paradigms, such as
                        those used to elicit the mismatch negativity <xref ref-type="bibr" rid="pcbi.1000211-Henson1">[84]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Ntnen1">[85]</xref>.</p>
                </sec>
            </sec>
        </sec>
        <sec id="s4">
            <title>Discussion</title>
            <p>We are now in a position to revisit some of the basic choices behind the
                <bold>DEM</bold> scheme, in light of its neuronal implementation. Some of these
                choices are generic and some are specific to neuronal inversion. All can be framed
                in terms of assumptions about the existence and form of the approximating
                conditional density, <italic>q</italic>(<italic>ϑ</italic>). The first
                choice was to optimise a bound on the log-evidence, as opposed to the evidence
                itself. This choice is mandated by the fact that the evidence entails an integral<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000211.e121" xlink:type="simple"/><label>(58)</label></disp-formula>that is not generally tractable. In other words, this integral has no
                general analytic solution, particularly when the generative model is nonlinear. This
                means the brain is obliged to induce a bound approximation, through
                    <italic>q</italic>(<italic>ϑ</italic>).</p>
            <p>The second choice was to use a fixed-form for
                <italic>q</italic>(<italic>ϑ</italic>), as opposed to a free-form. This
                is a more pragmatic choice that is dictated by implementational constraints. A
                fixed-form approximation allows one to represent the density in terms of a small
                number of quantities; its sufficient statistics. A free-form approximation would
                require an infinite number of quantities encoding the density over its support.
                Clearly, a fixed-form is assumption is imperative for the brain and predominates in
                most practical applications. In engineering and machine learning, free-form
                densities are usually approximated by the sample density of a large number of
                ‘particles’ that populate state-space. For dynamic systems in
                generalised coordinates, the ensuing scheme is known as variational filtering <xref ref-type="bibr" rid="pcbi.1000211-Friston1">[1]</xref>. For
                standard SSMs, which ignore the high-order motion of random fluctuations, particle
                filtering is the most common approach. However, the dimensionality of the
                representational problems entailed by neuronal computations probably precludes
                particle-based (i.e., free-form) representations: consider face recognition, a
                paradigm example in perceptual inference. Faces can be represented in a perceptual
                space of about thirty dimensions (i.e., faces have about thirty discriminable
                attributes). To populate a thirty-dimensional space we would need at least
                2<sup>30</sup> particles, where each particle could correspond to the activity of
                thirty neurons (note that the conditional mean can be encoded with a single
                particle). The brain has about 2<sup>11</sup> neurons at its disposal. Arguments
                like this suggest that free-form approximations and their attending sampling schemes
                are not really viable in a neuronal context (although they have been considered; see
                    <xref ref-type="bibr" rid="pcbi.1000211-Lee1">[86]</xref> and
                above)</p>
            <p>The third choice was a mean-field approximation;
                    <italic>q</italic>(<italic>ϑ</italic>) = <italic>q</italic>(<italic>u</italic>(<italic>t</italic>))<italic>q</italic>(<italic>θ</italic>)<italic>q</italic>(<italic>λ</italic>).
                This allowed us to separate the optimisation of states from parameters, using
                separation of temporal scales. This allowed us to optimise the states online, while
                learning the parameters offline. The motivation here was more didactic; in that
                special cases of the ensuing scheme are formally equivalent to established analyses
                of discrete data sequences (e.g., expectation maximisation and restricted maximum
                likelihood). However, the mean-field factorisation is less critical in neuronal
                implementations because the brain optimises both states and parameters online. We
                portrayed the neuronal implementation as a <bold>DEM</bold> scheme in which
                conditional uncertainty about the parameters was ignored when optimising the states
                and <italic>vice versa</italic> (i.e., mean field-effects were ignored).
                Alternatively, we could have relaxed the mean-field assumption and treated the
                solutions to Equations 51 and 52 as optimising the mean of
                    <italic>q</italic>(<italic>u</italic>(<italic>t</italic>),<italic>θ</italic>)
                simultaneously. In this case, mean-field effects coupling states and parameters are
                no longer required.</p>
            <p>The fourth assumption was that the fixed-form of
                    <italic>q</italic>(<italic>ϑ</italic>) was Gaussian. This Laplace
                assumption affords an important simplification that may be relevant for recognition
                schemes that deal with large amounts of data. Under the Laplace assumption, only the
                conditional mean has to be optimised (because the conditional covariance is a
                function of the mean). The resulting recognition dynamics (Equation 51) are simple
                and neuronally plausible. The Laplace assumption enforces a unimodal approximation
                but does not require the density of underlying causes to be Gaussian. This is
                because nonlinearities in HDMs can implement probability integral transforms. A
                common example is the use of log-normal densities for non-negative scale parameters.
                These are simple to implement under the Laplace assumption with a log-transform,
                    <italic>ϑ</italic> = ln
                    <italic>β</italic>, which endows <italic>β</italic> with a
                log-normal density (we use this for precision hyperparameters; see the triple
                estimation example above). The unimodal constraint may seem restrictive; however, we
                know of no psychophysical or electrophysiological evidence for multimodal
                representations in the brain. In fact, the psychophysics of ambiguous stimuli and
                related bistable perceptual phenomena suggest that we can only represent one
                conditional cause or percept at a time.</p>
            <p>The final choice was to include generalised motion under
                    <italic>q</italic>(<italic>ϑ</italic>). The alternative would have
                been to assume the precisions of
                    <italic>z</italic>′,<italic>z</italic>″,…<italic>w</italic>′,<italic>w</italic>″,…;
                the generalised motion of the random fluctuations, were zero (i.e., assume a
                serially uncorrelated process). It is important to appreciate that generalised
                motion always exists; the choice is whether to ignore it or not. Variational
                filtering and <bold>DEM</bold> assume high-order motion exists to infinite order.
                This is because random fluctuations in biophysical systems are almost invariably the
                product of dynamical systems, which renders their serial correlations analytic
                    (<xref ref-type="bibr" rid="pcbi.1000211-Stratonovich1">[6]</xref>, p 83; <xref ref-type="bibr" rid="pcbi.1000211-Fliess1">[23]</xref>). The resulting
                optimisation scheme is very simple (Equation 23) and is basically a restatement of
                Hamilton's principle of stationary action. If one ignores serial
                correlations, one could recourse to Extended Kalman filtering (EKF) or related
                Bayesian assimilation procedures for standard SSMs. From the perspective of
                    <bold>DEM</bold>, these conventional procedures have an unduly complicated
                construction and deal only with a special
                (<italic>n</italic> = 0) case of dynamic models. In
                    <xref ref-type="bibr" rid="pcbi.1000211-Friston2">[2]</xref>,
                we show that <bold>DEM</bold> and EKF give numerically identical results, when
                serial correlations are suppressed.</p>
            <p>It is interesting to consider <bold>DEM</bold> in relation to common distinctions
                among inversion schemes: sequential data assimilation (SDA) vs. path integral
                approaches or integration <italic>vs.</italic> solution of differential equations.
                    <bold>DEM</bold> blurs these distinctions somewhat: on the one hand,
                <bold>DEM</bold> is a path integral approach because the unknown quantities optimise
                action (the path integral of energy). On the other hand, it operates online and
                assimilates data with a differential equation (23), whose solution has stationary
                action. Furthermore, this equation can be integrated over time; indeed this is the
                mechanism suggested for neuronal schemes. However, when using <bold>DEM</bold> to
                analyse discrete data (e.g., the examples in the third section), this differential
                equation is solved over sampling intervals, using local linearization; c.f., <xref ref-type="bibr" rid="pcbi.1000211-Ozaki1">[19]</xref>.</p>
            <sec id="s4a">
                <title>Summary</title>
                <p>In summary, any generic inversion scheme needs to induce a lower-bound on the
                    log-evidence by invoking an approximating conditional density
                        <italic>q</italic>(<italic>ϑ</italic>) that, for dynamic systems,
                    covers generalised motion. Physical constraints on the representation of
                        <italic>q</italic>(<italic>ϑ</italic>) enforce a fixed
                    parameterised form so that is can be encoded in terms of its parameters or
                    sufficient statistics. The Laplace or Gaussian assumption about this fixed-form
                    affords a substantial simplification of recognition dynamics at the price of
                    restricting recognition to unimodal probabilistic representations; a price that
                    evolution may well have paid to optimise neuronal schemes. The mean-field
                    approximation is ubiquitous in the statistics but may not be necessary in an
                    online or neuronal setting.</p>
            </sec>
            <sec id="s4b">
                <title>Conclusion</title>
                <p>In conclusion, we have seen how the inversion of a fairly generic hierarchical
                    and dynamical model of sensory inputs can be transcribed onto neuronal
                    quantities that optimise a variational bound on the evidence for that model This
                    optimisation corresponds, under some simplifying assumptions, to suppression of
                    prediction error at all levels in a cortical hierarchy. This suppression rests
                    upon a balance between bottom-up (prediction error) influences and top-down
                    (empirical prior) influences that are balanced by representations of their
                    precision (uncertainty). These representations may be mediated by classical
                    neuromodulatory effects and slow postsynaptic cellular processes that are driven
                    by overall levels of prediction error.</p>
                <p>The ideas presented in this paper have a long history, starting with the notion
                    of neuronal energy <xref ref-type="bibr" rid="pcbi.1000211-Helmholtz1">[87]</xref>; covering ideas like efficient coding and
                    analysis by synthesis <xref ref-type="bibr" rid="pcbi.1000211-Barlow1">[88]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Neisser1">[89]</xref> to more recent
                    formulations in terms of Bayesian inversion and predictive coding (e.g., <xref ref-type="bibr" rid="pcbi.1000211-Ballard1">[90]</xref>,<xref ref-type="bibr" rid="pcbi.1000211-Dayan1">[91]</xref>). The specific
                    contribution of this work is to establish the generality of models that may, at
                    least in principle, be entertained by the brain.</p>
            </sec>
        </sec>
    </body>
    <back>
        <ack>
            <p>I would like to thank my colleagues for invaluable help in formulating thee ideas,
                Pedro Valdes-Sosa in for guidance on the relationship between standard and
                generalised state-space models and the three reviewers for helpful advice and
                challenging comments.</p>
        </ack>
        <ref-list>
            <title>References</title>
            <ref id="pcbi.1000211-Friston1">
                <label>1</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>Variational filtering.</article-title>
                    <source>Neuroimage</source>
                    <volume>41(3)</volume>
                    <fpage>747</fpage>
                    <lpage>766</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston2">
                <label>2</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Trujillo-Barreto</surname>
                            <given-names>N</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Daunizeau</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>DEM: a variational treatment of dynamic systems.</article-title>
                    <source>Neuroimage</source>
                    <volume>41(3)</volume>
                    <fpage>849</fpage>
                    <lpage>885</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston3">
                <label>3</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                    </person-group>
                    <year>2003</year>
                    <article-title>Learning and inference in the brain.</article-title>
                    <source>Neural Netw</source>
                    <volume>16</volume>
                    <fpage>1325</fpage>
                    <lpage>1352</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston4">
                <label>4</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                    </person-group>
                    <year>2005</year>
                    <article-title>A theory of cortical responses.</article-title>
                    <source>Philos Trans R Soc Lond B Biol Sci</source>
                    <volume>360</volume>
                    <fpage>815</fpage>
                    <lpage>836</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston5">
                <label>5</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>K</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Kilner</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Harrison</surname>
                            <given-names>L</given-names>
                        </name>
                    </person-group>
                    <year>2006</year>
                    <article-title>A free energy principle for the brain.</article-title>
                    <source>J Physiol Paris</source>
                    <volume>100(1–3)</volume>
                    <fpage>70</fpage>
                    <lpage>87</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Stratonovich1">
                <label>6</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Stratonovich</surname>
                            <given-names>RL</given-names>
                        </name>
                    </person-group>
                    <year>1967</year>
                    <source>Topics in the Theory of Random Noise</source>
                    <publisher-loc>New York</publisher-loc>
                    <publisher-name>Gordon and Breach</publisher-name>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Jazwinski1">
                <label>7</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Jazwinski</surname>
                            <given-names>AH</given-names>
                        </name>
                    </person-group>
                    <year>1970</year>
                    <source>Stochastic Processes and Filtering Theory</source>
                    <publisher-loc>San Diego</publisher-loc>
                    <publisher-name>Academic Press</publisher-name>
                    <fpage>122</fpage>
                    <lpage>125</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Kass1">
                <label>8</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Kass</surname>
                            <given-names>RE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Steffey</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <year>1989</year>
                    <article-title>Approximate Bayesian inference in conditionally independent
                        hierarchical models (parametric empirical Bayes models).</article-title>
                    <source>J Am Stat Assoc</source>
                    <volume>407</volume>
                    <fpage>717</fpage>
                    <lpage>726</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Efron1">
                <label>9</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Efron</surname>
                            <given-names>B</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Morris</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <year>1973</year>
                    <article-title>Stein's estimation rule and its competitors –
                        an empirical Bayes approach.</article-title>
                    <source>J Am Stats Assoc</source>
                    <volume>68</volume>
                    <fpage>117</fpage>
                    <lpage>130</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Cox1">
                <label>10</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Cox</surname>
                            <given-names>DR</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Miller</surname>
                            <given-names>HD</given-names>
                        </name>
                    </person-group>
                    <year>1965</year>
                    <article-title>The theory of stochastic processes.</article-title>
                    <comment>Methuen. London</comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Feynman1">
                <label>11</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Feynman</surname>
                            <given-names>RP</given-names>
                        </name>
                    </person-group>
                    <year>1972</year>
                    <source>Statistical mechanics</source>
                    <publisher-loc>Reading (Massachusetts)</publisher-loc>
                    <publisher-name>Benjamin</publisher-name>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Hinton1">
                <label>12</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Hinton</surname>
                            <given-names>GE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>von Cramp</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <year>1993</year>
                    <article-title>Keeping neural networks simple by minimising the description
                        length of weights.</article-title>
                    <fpage>5</fpage>
                    <lpage>13</lpage>
                    <comment>In: Proceedings of COLT-93</comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-MacKay1">
                <label>13</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>MacKay</surname>
                            <given-names>DJC</given-names>
                        </name>
                    </person-group>
                    <year>1995</year>
                    <article-title>Free-energy minimisation algorithm for decoding and
                        cryptoanalysis.</article-title>
                    <source>Electron Lett</source>
                    <volume>31</volume>
                    <fpage>445</fpage>
                    <lpage>447</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Neal1">
                <label>14</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Neal</surname>
                            <given-names>RM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Hinton</surname>
                            <given-names>GE</given-names>
                        </name>
                    </person-group>
                    <year>1998</year>
                    <article-title>A view of the EM algorithm that justifies incremental sparse and
                        other variants.</article-title>
                    <person-group person-group-type="editor">
                        <name name-style="western">
                            <surname>Jordan</surname>
                            <given-names>MI</given-names>
                        </name>
                    </person-group>
                    <source>Learning in Graphical Models.</source>
                    <publisher-loc>Dordrecht, The Netherlands</publisher-loc>
                    <publisher-name>Kluwer Academic</publisher-name>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston6">
                <label>15</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>K</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Mattout</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Trujillo-Barreto</surname>
                            <given-names>N</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Ashburner</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Penny</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <year>2007</year>
                    <article-title>Variational Bayes and the Laplace approximation.</article-title>
                    <source>Neuroimage</source>
                    <volume>34</volume>
                    <fpage>220</fpage>
                    <lpage>234</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Beal1">
                <label>16</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Beal</surname>
                            <given-names>MJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Ghahramani</surname>
                            <given-names>Z</given-names>
                        </name>
                    </person-group>
                    <year>2003</year>
                    <article-title>The variational Bayesian EM algorithm for incomplete Data: with
                        application to scoring graphical model structures.</article-title>
                    <source>Bayesian Statistics, Chapter 7.</source>
                    <person-group person-group-type="editor">
                        
                        <name name-style="western">
                            <surname>Bernardo</surname>
                            <given-names>JM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Bayarri</surname>
                            <given-names>MJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Berger</surname>
                            <given-names>JO</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Dawid</surname>
                            <given-names>AP</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Heckerman</surname>
                            <given-names>D</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Smith</surname>
                            <given-names>AFM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>West</surname>
                            <given-names>M</given-names>
                        </name>
                    </person-group>
                    
                    <publisher-loc>Oxford, UK</publisher-loc>
                    <publisher-name>Oxford University Press</publisher-name>
                    
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Dempster1">
                <label>17</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Dempster</surname>
                            <given-names>AP</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Laird</surname>
                            <given-names>NM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Rubin</surname>
                            <given-names>DB</given-names>
                        </name>
                    </person-group>
                    <year>1977</year>
                    <article-title>Maximum likelihood from incomplete data via the EM algorithm.</article-title>
                    <source>J R Stat Soc Ser B</source>
                    <volume>39</volume>
                    <fpage>1</fpage>
                    <lpage>38</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Harville1">
                <label>18</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Harville</surname>
                            <given-names>DA</given-names>
                        </name>
                    </person-group>
                    <year>1977</year>
                    <article-title>Maximum likelihood approaches to variance component estimation
                        and to related problems.</article-title>
                    <source>J Am Stat Assoc</source>
                    <volume>72</volume>
                    <fpage>320</fpage>
                    <lpage>338</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Ozaki1">
                <label>19</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Ozaki</surname>
                            <given-names>T</given-names>
                        </name>
                    </person-group>
                    <year>1992</year>
                    <article-title>A bridge between nonlinear time-series models and nonlinear
                        stochastic dynamical systems: A local linearization approach.</article-title>
                    <source>Stat Sin</source>
                    <volume>2</volume>
                    <fpage>113</fpage>
                    <lpage>135</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Roweis1">
                <label>20</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Roweis</surname>
                            <given-names>S</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Ghahramani</surname>
                            <given-names>Z</given-names>
                        </name>
                    </person-group>
                    <year>1999</year>
                    <article-title>A unifying review of linear Gaussian models.</article-title>
                    <source>Neural Comput</source>
                    <volume>11(2)</volume>
                    <fpage>305</fpage>
                    <lpage>345</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Rumelhart1">
                <label>21</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <source>Parallel Distributed Processing: Explorations in the Microstructures of
                        Cognition.</source>
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Rumelhart</surname>
                            <given-names>DE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Hinton</surname>
                            <given-names>GE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Williams</surname>
                            <given-names>RJ</given-names>
                        </name>
                    
                    </person-group>
                    <year>1986</year>
                    <article-title>Learning internal representations by error propagations.</article-title>
                    <person-group person-group-type="editor">
                        <name name-style="western">
                            <surname>Rumelhart</surname>
                            <given-names>DE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>McClelland</surname>
                            <given-names>JL</given-names>
                        </name>
                    </person-group>
                    
                    <publisher-loc>Cambridge (Massachusetts)</publisher-loc>
                    <publisher-name>MIT Press</publisher-name>
                    <volume>Vol. 1</volume>
                    <fpage>318</fpage>
                    <lpage>362</lpage>
                    
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Chen1">
                <label>22</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Chen</surname>
                            <given-names>T</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Chen</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <year>1995</year>
                    <article-title>Universal approximation to nonlinear operators by neural networks
                        with arbitrary activation functions and its application to dynamical
                        systems.</article-title>
                    <source>IEEE Trans Neural Netw</source>
                    <volume>6(4)</volume>
                    <fpage>918</fpage>
                    <lpage>928</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Fliess1">
                <label>23</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Fliess</surname>
                            <given-names>M</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Lamnabhi</surname>
                            <given-names>M</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Lamnabhi-Lagarrigue</surname>
                            <given-names>F</given-names>
                        </name>
                    </person-group>
                    <year>1983</year>
                    <article-title>An algebraic approach to nonlinear functional expansions.</article-title>
                    <source>IEEE Trans Circuits Syst</source>
                    <volume>30</volume>
                    <fpage>554</fpage>
                    <lpage>570</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston7">
                <label>24</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                    </person-group>
                    <year>2002</year>
                    <article-title>Bayesian estimation of dynamical systems: an application to fMRI.</article-title>
                    <source>Neuroimage</source>
                    <volume>16(2)</volume>
                    <fpage>513</fpage>
                    <lpage>530</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Mattout1">
                <label>25</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Mattout</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Phillips</surname>
                            <given-names>C</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Penny</surname>
                            <given-names>WD</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Rugg</surname>
                            <given-names>MD</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                    </person-group>
                    <year>2006</year>
                    <article-title>MEG source localization under multiple constraints: an extended
                        Bayesian framework.</article-title>
                    <source>Neuroimage</source>
                    <volume>30</volume>
                    <fpage>753</fpage>
                    <lpage>767</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Tipping1">
                <label>26</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Tipping</surname>
                            <given-names>ME</given-names>
                        </name>
                    </person-group>
                    <year>2001</year>
                    <article-title>Sparse Bayesian learning and the Relevance Vector Machine.</article-title>
                    <source>J Mach Learn Res</source>
                    <volume>1</volume>
                    <fpage>211</fpage>
                    <lpage>244</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Ripley1">
                <label>27</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Ripley</surname>
                            <given-names>BD</given-names>
                        </name>
                    </person-group>
                    <year>1994</year>
                    <article-title>Flexible Nonlinear Approaches to Classification.</article-title>
                    <source>From Statistics to Neural Networks.</source>
                    <person-group person-group-type="editor">
                        <name name-style="western">
                            <surname>Cherkassy</surname>
                            <given-names>V</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Friedman</surname>
                            <given-names>JH</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Wechsler</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    
                    <publisher-loc>New York</publisher-loc>
                    <publisher-name>Springer</publisher-name>
                    <fpage>105</fpage>
                    <lpage>126</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Rasmussen1">
                <label>28</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Rasmussen</surname>
                            <given-names>CE</given-names>
                        </name>
                    </person-group>
                    <year>1996</year>
                    <article-title>Evaluation of Gaussian Processes and Other Methods for Nonlinear
                        Regression [PhD thesis]. Toronto, Canada: Department of
                        Computer Science, University of Toronto.</article-title>
                    <comment>
                        <ext-link ext-link-type="uri" xlink:href="http://www.cs.utoronto.ca/~carl" xlink:type="simple">http://www.cs.utoronto.ca/~carl</ext-link>
                    </comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Kim1">
                <label>29</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Kim</surname>
                            <given-names>H-C</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Ghahramani</surname>
                            <given-names>Z</given-names>
                        </name>
                    </person-group>
                    <year>2006</year>
                    <article-title>Bayesian Gaussian process classification with the EM-EP
                        algorithm.</article-title>
                    <source>IEEE Trans Pattern Anal Mach Intell</source>
                    <volume>28(12)</volume>
                    <fpage>1948</fpage>
                    <lpage>1959</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Kalman1">
                <label>30</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Kalman</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <year>1960</year>
                    <article-title>A new approach to linear filtering and prediction problems.</article-title>
                    <source>ASME Trans J Basic Eng</source>
                    <volume>82(1)</volume>
                    <fpage>35</fpage>
                    <lpage>45</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Wang1">
                <label>31</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Wang</surname>
                            <given-names>B</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Titterington</surname>
                            <given-names>DM</given-names>
                        </name>
                    </person-group>
                    <year>2004</year>
                    <article-title>Variational Bayesian inference for partially observed diffusions.
                        Technical Report 04-4, University of Glasgow.</article-title>
                    <comment>
                        <ext-link ext-link-type="uri" xlink:href="http://www.stats.gla.ac.uk/Research/-TechRep2003/04-4.pdf" xlink:type="simple">http://www.stats.gla.ac.uk/Research/-TechRep2003/04-4.pdf</ext-link>
                    </comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Srensen1">
                <label>32</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Sørensen</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <year>2004</year>
                    <article-title>Parametric inference for diffusion processes observed at discrete
                        points in time: a survey.</article-title>
                    <source>Int Stat Rev</source>
                    <volume>72(3)</volume>
                    <fpage>337</fpage>
                    <lpage>354</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Ghahramani1">
                <label>33</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Ghahramani</surname>
                            <given-names>Z</given-names>
                        </name>
                    </person-group>
                    <year>2004</year>
                    <article-title>Unsupervised Learning.</article-title>
                    <person-group person-group-type="editor">
                        <name name-style="western">
                            <surname>Bousquet</surname>
                            <given-names>O</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Raetsch</surname>
                            <given-names>G</given-names>
                        </name>
                        <name name-style="western">
                            <surname>von Luxburg</surname>
                            <given-names>U</given-names>
                        </name>
                    </person-group>
                    <source>Advanced Lectures on Machine Learning LNAI 3176.</source>
                    <publisher-loc>Berlin, Germany</publisher-loc>
                    <publisher-name>Springer-Verlag</publisher-name>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston8">
                <label>34</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>K</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Phillips</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Chawla</surname>
                            <given-names>D</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Büchel</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <year>2000</year>
                    <article-title>Nonlinear PCA: characterizing interactions between modes of brain
                        activity.</article-title>
                    <source>Philos Trans R Soc Lond B Biol Sci</source>
                    <volume>355(1393)</volume>
                    <fpage>135</fpage>
                    <lpage>46</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Tipping2">
                <label>35</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Tipping</surname>
                            <given-names>ME</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Bishop</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <year>1999</year>
                    <article-title>Probabilistic principal component analysis.</article-title>
                    <source>J R Stat Soc Ser B</source>
                    <volume>61(3)</volume>
                    <fpage>611</fpage>
                    <lpage>622</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Bell1">
                <label>36</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Bell</surname>
                            <given-names>AJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Sejnowski</surname>
                            <given-names>TJ</given-names>
                        </name>
                    </person-group>
                    <year>1995</year>
                    <article-title>An information maximisation approach to blind separation and
                        blind de-convolution.</article-title>
                    <source>Neural Comput</source>
                    <volume>7</volume>
                    <fpage>1129</fpage>
                    <lpage>1159</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Olshausen1">
                <label>37</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Olshausen</surname>
                            <given-names>BA</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Field</surname>
                            <given-names>DJ</given-names>
                        </name>
                    </person-group>
                    <year>1996</year>
                    <article-title>Emergence of simple-cell receptive field properties by learning a
                        sparse code for natural images.</article-title>
                    <source>Nature</source>
                    <volume>381</volume>
                    <fpage>607</fpage>
                    <lpage>609</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Maunsell1">
                <label>38</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Maunsell</surname>
                            <given-names>JH</given-names>
                        </name>
                        <name name-style="western">
                            <surname>van Essen</surname>
                            <given-names>DC</given-names>
                        </name>
                    </person-group>
                    <year>1983</year>
                    <article-title>The connections of the middle temporal visual area (MT) and their
                        relationship to a cortical hierarchy in the macaque monkey.</article-title>
                    <source>J Neurosci</source>
                    <volume>3</volume>
                    <fpage>2563</fpage>
                    <lpage>2586</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Zeki1">
                <label>39</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Zeki</surname>
                            <given-names>S</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Shipp</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <year>1988</year>
                    <article-title>The functional logic of cortical connections.</article-title>
                    <source>Nature</source>
                    <volume>335</volume>
                    <fpage>311</fpage>
                    <lpage>31</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Felleman1">
                <label>40</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Felleman</surname>
                            <given-names>DJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Van Essen</surname>
                            <given-names>DC</given-names>
                        </name>
                    </person-group>
                    <year>1991</year>
                    <article-title>Distributed hierarchical processing in the primate cerebral
                        cortex.</article-title>
                    <source>Cereb Cortex</source>
                    <volume>1</volume>
                    <fpage>1</fpage>
                    <lpage>47</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Mesulam1">
                <label>41</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Mesulam</surname>
                            <given-names>MM</given-names>
                        </name>
                    </person-group>
                    <year>1998</year>
                    <article-title>From sensation to cognition.</article-title>
                    <source>Brain</source>
                    <volume>121</volume>
                    <fpage>1013</fpage>
                    <lpage>1052</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Rockland1">
                <label>42</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Rockland</surname>
                            <given-names>KS</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Pandya</surname>
                            <given-names>DN</given-names>
                        </name>
                    </person-group>
                    <year>1979</year>
                    <article-title>Laminar origins and terminations of cortical connections of the
                        occipital lobe in the rhesus monkey.</article-title>
                    <source>Brain Res</source>
                    <volume>179</volume>
                    <fpage>3</fpage>
                    <lpage>20</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Murphy1">
                <label>43</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Murphy</surname>
                            <given-names>PC</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Sillito</surname>
                            <given-names>AM</given-names>
                        </name>
                    </person-group>
                    <year>1987</year>
                    <article-title>Corticofugal feedback influences the generation of length tuning
                        in the visual pathway.</article-title>
                    <source>Nature</source>
                    <volume>329</volume>
                    <fpage>727</fpage>
                    <lpage>729</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Sherman1">
                <label>44</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Sherman</surname>
                            <given-names>SM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Guillery</surname>
                            <given-names>RW</given-names>
                        </name>
                    </person-group>
                    <year>1998</year>
                    <article-title>On the actions that one nerve cell can have on another:
                        distinguishing “drivers” from
                        “modulators”.</article-title>
                    <source>Proc Natl Acad Sci U S A</source>
                    <volume>95</volume>
                    <fpage>7121</fpage>
                    <lpage>7126</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Angelucci1">
                <label>45</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Angelucci</surname>
                            <given-names>A</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Levitt</surname>
                            <given-names>JB</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Walton</surname>
                            <given-names>EJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Hupe</surname>
                            <given-names>JM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Bullier</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Lund</surname>
                            <given-names>JS</given-names>
                        </name>
                    </person-group>
                    <year>2002</year>
                    <article-title>Circuits for local and global signal integration in primary
                        visual cortex.</article-title>
                    <source>J Neurosci</source>
                    <volume>22</volume>
                    <fpage>8633</fpage>
                    <lpage>8646</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-DeFelipe1">
                <label>46</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>DeFelipe</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Alonso-Nanclares</surname>
                            <given-names>L</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Arellano</surname>
                            <given-names>JI</given-names>
                        </name>
                    </person-group>
                    <year>2002</year>
                    <article-title>Microstructure of the neocortex: comparative aspects.</article-title>
                    <source>J Neurocytol</source>
                    <volume>31</volume>
                    <fpage>299</fpage>
                    <lpage>316</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Hupe1">
                <label>47</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Hupe</surname>
                            <given-names>JM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>James</surname>
                            <given-names>AC</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Payne</surname>
                            <given-names>BR</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Lomber</surname>
                            <given-names>SG</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Girard</surname>
                            <given-names>P</given-names>
                        </name>
                        <etal/>
                    </person-group>
                    <year>1998</year>
                    <article-title>Cortical feedback improves discrimination between figure and
                        background by V1, V2 and V3 neurons.</article-title>
                    <source>Nature</source>
                    <volume>394</volume>
                    <fpage>784</fpage>
                    <lpage>787</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Rosier1">
                <label>48</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Rosier</surname>
                            <given-names>AM</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Arckens</surname>
                            <given-names>L</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Orban</surname>
                            <given-names>GA</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Vandesande</surname>
                            <given-names>F</given-names>
                        </name>
                    </person-group>
                    <year>1993</year>
                    <article-title>Laminar distribution of NMDA receptors in cat and monkey visual
                        cortex visualized by [3H]-MK-801 binding.</article-title>
                    <source>J Comp Neurol</source>
                    <volume>335</volume>
                    <fpage>369</fpage>
                    <lpage>380</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Mumford1">
                <label>49</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Mumford</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <year>1992</year>
                    <article-title>On the computational architecture of the neocortex. II. The role
                        of cortico-cortical loops.</article-title>
                    <source>Biol Cybern</source>
                    <volume>66</volume>
                    <fpage>241</fpage>
                    <lpage>251</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Edelman1">
                <label>50</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Edelman</surname>
                            <given-names>GM</given-names>
                        </name>
                    </person-group>
                    <year>1993</year>
                    <article-title>Neural Darwinism: selection and reentrant signaling in higher
                        brain function.</article-title>
                    <source>Neuron</source>
                    <volume>10</volume>
                    <fpage>115</fpage>
                    <lpage>125</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Grossberg1">
                <label>51</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Grossberg</surname>
                            <given-names>S</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Pilly</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>Temporal dynamics of decision-making during motion perception in
                        the visual cortex.</article-title>
                    <source>Vis Res</source>
                    <volume>48</volume>
                    <fpage>1345</fpage>
                    <lpage>1373</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Grossberg2">
                <label>52</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Grossberg</surname>
                            <given-names>S</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Versace</surname>
                            <given-names>M</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>Spikes, synchrony, and attentive learning by laminar
                        thalamocortical circuits.</article-title>
                    <source>Brain Res</source>
                    <volume>1218</volume>
                    <fpage>278</fpage>
                    <lpage>312</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Chait1">
                <label>53</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Chait</surname>
                            <given-names>M</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Poeppel</surname>
                            <given-names>D</given-names>
                        </name>
                        <name name-style="western">
                            <surname>de Cheveigné</surname>
                            <given-names>A</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Simon</surname>
                            <given-names>JZ</given-names>
                        </name>
                    </person-group>
                    <year>2007</year>
                    <article-title>Processing asymmetry of transitions between order and disorder in
                        human auditory cortex.</article-title>
                    <source>J Neurosci</source>
                    <volume>27(19)</volume>
                    <fpage>5207</fpage>
                    <lpage>5214</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Crick1">
                <label>54</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Crick</surname>
                            <given-names>F</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Koch</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <year>1998</year>
                    <article-title>Constraints on cortical and thalamic projections: the
                        no-strong-loops hypothesis.</article-title>
                    <source>Nature</source>
                    <volume>391(6664)</volume>
                    <fpage>245</fpage>
                    <lpage>250</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-London1">
                <label>55</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>London</surname>
                            <given-names>M</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Häusser</surname>
                            <given-names>M</given-names>
                        </name>
                    </person-group>
                    <year>2005</year>
                    <article-title>Dendritic computation.</article-title>
                    <source>Annu Rev Neurosci</source>
                    <volume>28</volume>
                    <fpage>503</fpage>
                    <lpage>532</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Buonomano1">
                <label>56</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Buonomano</surname>
                            <given-names>DV</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Merzenich</surname>
                            <given-names>MM</given-names>
                        </name>
                    </person-group>
                    <year>1998</year>
                    <article-title>Cortical plasticity: from synapses to maps.</article-title>
                    <source>Annu Rev Neurosci</source>
                    <volume>21</volume>
                    <fpage>149</fpage>
                    <lpage>186</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Martin1">
                <label>57</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Martin</surname>
                            <given-names>SJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Grimwood</surname>
                            <given-names>PD</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Morris</surname>
                            <given-names>RG</given-names>
                        </name>
                    </person-group>
                    <year>2000</year>
                    <article-title>Synaptic plasticity and memory: an evaluation of the hypothesis.</article-title>
                    <source>Annu Rev Neurosci</source>
                    <volume>23</volume>
                    <fpage>649</fpage>
                    <lpage>711</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Treue1">
                <label>58</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Treue</surname>
                            <given-names>S</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Maunsell</surname>
                            <given-names>HR</given-names>
                        </name>
                    </person-group>
                    <year>1996</year>
                    <article-title>Attentional modulation of visual motion processing in cortical
                        areas MT and MST.</article-title>
                    <source>Nature</source>
                    <volume>382</volume>
                    <fpage>539</fpage>
                    <lpage>541</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-MartinezTrujillo1">
                <label>59</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Martinez-Trujillo</surname>
                            <given-names>JC</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Treue</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <year>2004</year>
                    <article-title>Feature-based attention increases the selectivity of population
                        responses in primate visual cortex.</article-title>
                    <source>Curr Biol</source>
                    <volume>14</volume>
                    <fpage>744</fpage>
                    <lpage>751</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Chelazzi1">
                <label>60</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Chelazzi</surname>
                            <given-names>L</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Miller</surname>
                            <given-names>E</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Duncan</surname>
                            <given-names>J</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Desimone</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <year>1993</year>
                    <article-title>A neural basis for visual search in inferior temporal cortex.</article-title>
                    <source>Nature</source>
                    <volume>363</volume>
                    <fpage>345</fpage>
                    <lpage>347</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Desimone1">
                <label>61</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Desimone</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <year>1996</year>
                    <article-title>Neural mechanisms for visual memory and their role in attention.</article-title>
                    <source>Proc Natl Acad Sci U S A</source>
                    <volume>93(24)</volume>
                    <fpage>13494</fpage>
                    <lpage>13499</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Schroeder1">
                <label>62</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Schroeder</surname>
                            <given-names>CE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Mehta</surname>
                            <given-names>AD</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Foxe</surname>
                            <given-names>JJ</given-names>
                        </name>
                    </person-group>
                    <year>2001</year>
                    <article-title>Determinants and mechanisms of attentional modulation of neural
                        processing.</article-title>
                    <source>Front Biosci</source>
                    <volume>6</volume>
                    <fpage>D672</fpage>
                    <lpage>D684</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Yu1">
                <label>63</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Yu</surname>
                            <given-names>AJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Dayan</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <year>2005</year>
                    <article-title>Uncertainty, neuromodulation and attention.</article-title>
                    <source>Neuron</source>
                    <volume>46</volume>
                    <fpage>681</fpage>
                    <lpage>692</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Rao1">
                <label>64</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Rao</surname>
                            <given-names>RP</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Ballard</surname>
                            <given-names>DH</given-names>
                        </name>
                    </person-group>
                    <year>1998</year>
                    <article-title>Predictive coding in the visual cortex: a functional
                        interpretation of some extra-classical receptive field effects.</article-title>
                    <source>Nat Neurosci</source>
                    <volume>2</volume>
                    <fpage>79</fpage>
                    <lpage>87</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Tseng1">
                <label>65</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Tseng</surname>
                            <given-names>KY</given-names>
                        </name>
                        <name name-style="western">
                            <surname>O'Donnell</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <year>2004</year>
                    <article-title>Dopamine-glutamate interactions controlling prefrontal cortical
                        pyramidal cell excitability involve multiple signaling mechanisms.</article-title>
                    <source>J Neurosci</source>
                    <volume>24</volume>
                    <fpage>5131</fpage>
                    <lpage>5139</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Brocher1">
                <label>66</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Brocher</surname>
                            <given-names>S</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Artola</surname>
                            <given-names>A</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Singer</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <year>1992</year>
                    <article-title>Agonists of cholinergic and noradrenergic receptors facilitate
                        synergistically the induction of long-term potentiation in slices of rat
                        visual cortex.</article-title>
                    <source>Brain Res</source>
                    <volume>573</volume>
                    <fpage>27</fpage>
                    <lpage>36</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Gu1">
                <label>67</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Gu</surname>
                            <given-names>Q</given-names>
                        </name>
                    </person-group>
                    <year>2002</year>
                    <article-title>Neuromodulatory transmitter systems in the cortex and their role
                        in cortical plasticity.</article-title>
                    <source>Neuroscience</source>
                    <volume>111</volume>
                    <fpage>815</fpage>
                    <lpage>835</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston9">
                <label>68</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Tononi</surname>
                            <given-names>G</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Reeke</surname>
                            <given-names>GN</given-names>
                            <suffix>Jr</suffix>
                        </name>
                        <name name-style="western">
                            <surname>Sporns</surname>
                            <given-names>O</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Edelman</surname>
                            <given-names>GM</given-names>
                        </name>
                    </person-group>
                    <year>1994</year>
                    <article-title>Value-dependent selection in the brain: simulation in a synthetic
                        neural model.</article-title>
                    <source>Neuroscience</source>
                    <volume>59(2)</volume>
                    <fpage>229</fpage>
                    <lpage>243</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Montague1">
                <label>69</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Montague</surname>
                            <given-names>PR</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Dayan</surname>
                            <given-names>P</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Person</surname>
                            <given-names>C</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Sejnowski</surname>
                            <given-names>TJ</given-names>
                        </name>
                    </person-group>
                    <year>1995</year>
                    <article-title>Bee foraging in uncertain environments using predictive Hebbian
                        learning.</article-title>
                    <source>Nature</source>
                    <volume>377(6551)</volume>
                    <fpage>725</fpage>
                    <lpage>728</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Schultz1">
                <label>70</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Schultz</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <year>2007</year>
                    <article-title>Multiple dopamine functions at different time courses.</article-title>
                    <source>Annu Rev Neurosci</source>
                    <volume>30</volume>
                    <fpage>259</fpage>
                    <lpage>288</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Niv1">
                <label>71</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Niv</surname>
                            <given-names>Y</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Duff</surname>
                            <given-names>MO</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Dayan</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <year>2005</year>
                    <article-title>Dopamine, uncertainty and TD learning.</article-title>
                    <source>Behav Brain Funct</source>
                    <volume>4</volume>
                    <fpage>1</fpage>
                    <lpage>6</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Kawato1">
                <label>72</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Kawato</surname>
                            <given-names>M</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Hayakawa</surname>
                            <given-names>H</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Inui</surname>
                            <given-names>T</given-names>
                        </name>
                    </person-group>
                    <year>1993</year>
                    <article-title>A forward-inverse optics model of reciprocal connections between
                        visual cortical areas.</article-title>
                    <source>Network</source>
                    <volume>4</volume>
                    <fpage>415</fpage>
                    <lpage>422</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Desimone2">
                <label>73</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Desimone</surname>
                            <given-names>R</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Duncan</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <year>1995</year>
                    <article-title>Neural mechanisms of selective visual attention.</article-title>
                    <source>Annu Rev Neurosci</source>
                    <volume>18</volume>
                    <fpage>193</fpage>
                    <lpage>222</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Abbott1">
                <label>74</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Abbott</surname>
                            <given-names>LF</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Varela</surname>
                            <given-names>JA</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Sen</surname>
                            <given-names>K</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Nelson</surname>
                            <given-names>SB</given-names>
                        </name>
                    </person-group>
                    <year>1997</year>
                    <article-title>Synaptic depression and cortical gain control.</article-title>
                    <source>Science</source>
                    <volume>275(5297)</volume>
                    <fpage>220</fpage>
                    <lpage>224</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Archambeau1">
                <label>75</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Archambeau</surname>
                            <given-names>C</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Cornford</surname>
                            <given-names>D</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Opper</surname>
                            <given-names>M</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Shawe-Taylor</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <year>2007</year>
                    <article-title>Gaussian process approximations of stochastic differential
                        equations.</article-title>
                    <source>In: JMLR: Workshop and Conference Proceedings</source>
                    <fpage>1</fpage>
                    <lpage>16</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Kappen1">
                <label>76</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Kappen</surname>
                            <given-names>HJ</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>An introduction to stochastic control theory, path integrals and
                        reinforcement learning.</article-title>
                    <comment>
                        <ext-link ext-link-type="uri" xlink:href="http://www.snn.ru.nl/~bertk/kappen_granada2006.pdf" xlink:type="simple">http://www.snn.ru.nl/~bertk/kappen_granada2006.pdf</ext-link>
                    </comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-John1">
                <label>77</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>John</surname>
                            <given-names>ER</given-names>
                        </name>
                    </person-group>
                    <year>1972</year>
                    <article-title>Switchboard versus statistical theories of learning and memory.</article-title>
                    <source>Science</source>
                    <volume>177(4052)</volume>
                    <fpage>850</fpage>
                    <lpage>864</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Freeman1">
                <label>78</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Freeman</surname>
                            <given-names>WJ</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>A pseudo-equilibrium thermodynamic model of information
                        processing in nonlinear brain dynamics.</article-title>
                    <source>Neural Netw</source>
                    <volume>21(2–3)</volume>
                    <fpage>257</fpage>
                    <lpage>265</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Beskos1">
                <label>79</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Beskos</surname>
                            <given-names>A</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Papaspiliopoulos</surname>
                            <given-names>O</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Roberts</surname>
                            <given-names>GO</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Fearnhead</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <year>2006</year>
                    <article-title>Exact and computationally efficient likelihood-based estimation
                        for discretely observed diffusion processes (with discussion).</article-title>
                    <source>J R Stat Soc Ser B</source>
                    <volume>68</volume>
                    <fpage>333</fpage>
                    <lpage>361</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Evensen1">
                <label>80</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Evensen</surname>
                            <given-names>G</given-names>
                        </name>
                        <name name-style="western">
                            <surname>van Leeuwen</surname>
                            <given-names>PJ</given-names>
                        </name>
                    </person-group>
                    <year>2000</year>
                    <article-title>An ensemble Kalman smoother for nonlinear dynamics.</article-title>
                    <source>Mon Weather Rev</source>
                    <volume>128(6)</volume>
                    <fpage>1852</fpage>
                    <lpage>1867</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Schiff1">
                <label>81</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Schiff</surname>
                            <given-names>SJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Sauer</surname>
                            <given-names>T</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>Kalman filter control of a model of spatiotemporal cortical
                        dynamics.</article-title>
                    <source>J Neural Eng</source>
                    <volume>5(1)</volume>
                    <fpage>1</fpage>
                    <lpage>8</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Restrepo1">
                <label>82</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Restrepo</surname>
                            <given-names>JM</given-names>
                        </name>
                    </person-group>
                    <year>2008</year>
                    <article-title>A path integral method for data assimilation.</article-title>
                    <source>Physica D</source>
                    <volume>237(1)</volume>
                    <fpage>14</fpage>
                    <lpage>27</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Friston10">
                <label>83</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Friston</surname>
                            <given-names>KJ</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Kiebel</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <year>2009</year>
                    <article-title>Predictive coding under the free energy principle.</article-title>
                    <comment>Philos Trans R Soc Lond. Under review</comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Henson1">
                <label>84</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Henson</surname>
                            <given-names>R</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Shallice</surname>
                            <given-names>T</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Dolan</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <year>2000</year>
                    <article-title>Neuroimaging evidence for dissociable forms of repetition
                        priming.</article-title>
                    <source>Science</source>
                    <volume>287</volume>
                    <fpage>1269</fpage>
                    <lpage>1272</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Ntnen1">
                <label>85</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Näätänen</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <year>2003</year>
                    <article-title>Mismatch negativity: clinical research and possible applications.</article-title>
                    <source>Int J Psychophysiol</source>
                    <volume>48</volume>
                    <fpage>179</fpage>
                    <lpage>188</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Lee1">
                <label>86</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Lee</surname>
                            <given-names>TS</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Mumford</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <year>2003</year>
                    <article-title>Hierarchical Bayesian inference in the visual cortex.</article-title>
                    <source>J Opt Soc Am A</source>
                    <volume>20</volume>
                    <fpage>1434</fpage>
                    <lpage>1448</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Helmholtz1">
                <label>87</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Helmholtz</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <year>1860/1962</year>
                    <article-title>Handbuch der Physiologischen Optik. English translation.</article-title>
                    <person-group person-group-type="editor">
                        <name name-style="western">
                            <surname>Southall</surname>
                            <given-names>JPC</given-names>
                        </name>
                    </person-group>
                    <publisher-loc>Dover</publisher-loc>
                    <publisher-name>New York</publisher-name>
                    <comment>Vol. 3</comment>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Barlow1">
                <label>88</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Barlow</surname>
                            <given-names>HB</given-names>
                        </name>
                    </person-group>
                    <year>1961</year>
                    <article-title>Possible principles underlying the transformation of sensory
                        messages.</article-title>
                    <person-group person-group-type="editor">
                        <name name-style="western">
                            <surname>Rosenblith</surname>
                            <given-names>WA</given-names>
                        </name>
                    </person-group>
                    <source>Sensory Communication.</source>
                    <publisher-loc>Cambridge (Massachusetts)</publisher-loc>
                    <publisher-name>MIT Press</publisher-name>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Neisser1">
                <label>89</label>
                <element-citation publication-type="other" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Neisser</surname>
                            <given-names>U</given-names>
                        </name>
                    </person-group>
                    <year>1967</year>
                    <source>Cognitive psychology</source>
                    <publisher-loc>New York</publisher-loc>
                    <publisher-name>Appleton-Century-Crofts</publisher-name>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Ballard1">
                <label>90</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Ballard</surname>
                            <given-names>DH</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Hinton</surname>
                            <given-names>GE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Sejnowski</surname>
                            <given-names>TJ</given-names>
                        </name>
                    </person-group>
                    <year>1983</year>
                    <article-title>Parallel visual computation.</article-title>
                    <source>Nature</source>
                    <volume>306</volume>
                    <fpage>21</fpage>
                    <lpage>26</lpage>
                </element-citation>
            </ref>
            <ref id="pcbi.1000211-Dayan1">
                <label>91</label>
                <element-citation publication-type="journal" xlink:type="simple">
                    <person-group person-group-type="author">
                        <name name-style="western">
                            <surname>Dayan</surname>
                            <given-names>P</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Hinton</surname>
                            <given-names>GE</given-names>
                        </name>
                        <name name-style="western">
                            <surname>Neal</surname>
                            <given-names>RM</given-names>
                        </name>
                    </person-group>
                    <year>1995</year>
                    <article-title>The Helmholtz machine.</article-title>
                    <source>Neural Comput</source>
                    <volume>7</volume>
                    <fpage>889</fpage>
                    <lpage>904</lpage>
                </element-citation>
            </ref>
        </ref-list>
        
    </back>
</article>