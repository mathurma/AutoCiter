<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01679</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005497</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Ellipses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Algebra</subject><subj-group><subject>Linear algebra</subject><subj-group><subject>Eigenvalues</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical noise</subject><subj-group><subject>Gaussian noise</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Robust information propagation through noisy neural circuits</article-title>
<alt-title alt-title-type="running-head">Robust information propagation through noisy neural circuits</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8208-5698</contrib-id>
<name name-style="western">
<surname>Zylberberg</surname> <given-names>Joel</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Pouget</surname> <given-names>Alexandre</given-names></name>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8713-9328</contrib-id>
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Shea-Brown</surname> <given-names>Eric</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff007"><sup>7</sup></xref>
<xref ref-type="aff" rid="aff008"><sup>8</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Physiology and Biophysics, Center for Neuroscience, and Computational Bioscience Program, University of Colorado School of Medicine, Aurora, Colorado, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Applied Mathematics, University of Colorado, Boulder, Colorado, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Applied Mathematics, University of Washington, Seattle, Washington, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Learning in Machines and Brains Program, Canadian Institute For Advanced Research, Toronto, Ontario, Canada</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Basic Neuroscience, University of Geneva, Geneva, Switzerland</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</addr-line>
</aff>
<aff id="aff007">
<label>7</label>
<addr-line>Department of Physiology and Biophysics, Program in Neuroscience, University of Washington Institute for Neuroengineering, and Center for Sensorimotor Neural Engineering, University of Washington, Seattle, Washington, United States of America</addr-line>
</aff>
<aff id="aff008">
<label>8</label>
<addr-line>Allen Institute for Brain Science, Seattle, Washington, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Brunel</surname> <given-names>Nicolas</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>The University of Chicago, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> JZ AP PEL ESB.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> JZ PEL ESB.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> JZ AP PEL ESB.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> JZ PEL ESB.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> JZ AP PEL ESB.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> JZ AP PEL ESB.</p>
</list-item>
</list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">joel.zylberberg@ucdenver.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>18</day>
<month>4</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>4</issue>
<elocation-id>e1005497</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>10</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>4</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Zylberberg et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005497"/>
<abstract>
<p>Sensory neurons give highly variable responses to stimulation, which can limit the amount of stimulus information available to downstream circuits. Much work has investigated the factors that affect the amount of information encoded in these population responses, leading to insights about the role of covariability among neurons, tuning curve shape, etc. However, the informativeness of neural responses is not the only relevant feature of population codes; of potentially equal importance is how robustly that information propagates to downstream structures. For instance, to quantify the retina’s performance, one must consider not only the informativeness of the optic nerve responses, but also the amount of information that survives the spike-generating nonlinearity and noise corruption in the next stage of processing, the lateral geniculate nucleus. Our study identifies the set of covariance structures for the upstream cells that optimize the ability of information to propagate through noisy, nonlinear circuits. Within this optimal family are covariances with “differential correlations”, which are known to reduce the information encoded in neural population activities. Thus, covariance structures that maximize information in neural population codes, and those that maximize the ability of this information to propagate, can be very different. Moreover, redundancy is neither necessary nor sufficient to make population codes robust against corruption by noise: redundant codes can be very fragile, and synergistic codes can—in some cases—optimize robustness against noise.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Information about the outside world, which originates in sensory neurons, propagates through multiple stages of processing before reaching the neural structures that control behavior. While much work in neuroscience has investigated the factors that affect the amount of information contained in peripheral sensory areas, very little work has asked how much of that information makes it through subsequent processing stages. That’s the focus of this paper, and it’s an important issue because information that fails to propagate cannot be used to affect decision-making. We find a tradeoff between information content and information transmission: neural codes which contain a large amount of information can transmit that information poorly to subsequent processing stages. Thus, the problem of robust information propagation—which has largely been overlooked in previous research—may be critical for determining how our sensory organs communicate with our brains. We identify the conditions under which information propagates well—or poorly—through multiple stages of neural processing.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Canadian Institute for Advanced Research (CA)</institution>
</funding-source>
<principal-award-recipient>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8208-5698</contrib-id>
<name name-style="western">
<surname>Zylberberg</surname> <given-names>Joel</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>CRCNS-1208027</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Shea-Brown</surname> <given-names>Eric</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Gatsby Charitable Foundation (GB)</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>Simons Foundation (US)</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Shea-Brown</surname> <given-names>Eric</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000952</institution-id>
<institution>Paul G. Allen Family Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Shea-Brown</surname> <given-names>Eric</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>JZ’s contribution to this work was partially supported by new faculty start-up funds from the University of Colorado, and by an Azrieli Global Scholar Award from the Canadian Institute For Advanced Research (CIFAR, <ext-link ext-link-type="uri" xlink:href="http://www.cifar.ca" xlink:type="simple">www.cifar.ca</ext-link>). AP was supported by a grant from the Swiss National Science Foundation, <ext-link ext-link-type="uri" xlink:href="http://www.snf.ch" xlink:type="simple">www.snf.ch</ext-link>, (grant #31003A_143707) and by the Simons Collaboration for the Global Brain, <ext-link ext-link-type="uri" xlink:href="http://www.simonsfoundation.org" xlink:type="simple">www.simonsfoundation.org</ext-link>. PEL was supported by the Gatsby Charitable Foundation, <ext-link ext-link-type="uri" xlink:href="http://www.gatsby.org.uk" xlink:type="simple">www.gatsby.org.uk</ext-link>. ESB acknowledges the support of National Science Foundation (NSF, <ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link>) Grant CRCNS-1208027 and a Simons Fellowship in Mathematics, and thanks the Allen Institute founders, Paul G. Allen and Jody Allen, for their vision, encouragement and support (<ext-link ext-link-type="uri" xlink:href="http://www.alleninstitute.org" xlink:type="simple">www.alleninstitute.org</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="35"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-05-02</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Neurons in sensory systems gather information about the environment, and transmit that information to other parts of the nervous system. This information is encoded in the activity of neural populations, and that activity is variable: repeated presentations of the same stimulus lead to different neuronal responses [<xref ref-type="bibr" rid="pcbi.1005497.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref007">7</xref>]. This variability can degrade the ability of neural populations to encode information about stimuli, leading to the question: which features of population codes help to combat—or exacerbate—information loss?</p>
<p>This question is typically addressed by assessing the amount of information that is encoded in the periphery as a function of the covariance structure [<xref ref-type="bibr" rid="pcbi.1005497.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref024">24</xref>], the shapes of the tuning curves [<xref ref-type="bibr" rid="pcbi.1005497.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref026">26</xref>], or both [<xref ref-type="bibr" rid="pcbi.1005497.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref028">28</xref>]. However, the informativeness of the population responses at the periphery is not the only relevant quantity for understanding sensory coding; of potentially equal importance is the amount of information that propagates through the neural circuit to downstream structures [<xref ref-type="bibr" rid="pcbi.1005497.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref030">30</xref>].</p>
<p>To illustrate the ideas, consider the case of retinal ganglion cells transmitting information about visual stimuli to the cortex via the thalamus, as shown in <xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1</xref>. To quantify the performance of the retina, one must consider not only the informativeness of the optic nerve responses (<italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) in <xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1A</xref>), but also how much of that information is transmitted by the lateral geniculate nucleus (LGN) to the cortex (<italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>) in <xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1A</xref>) [<xref ref-type="bibr" rid="pcbi.1005497.ref031">31</xref>]. The two may be very different, as only information that survives the LGN’s spike-generating nonlinearity and noise corruption will propagate to downstream cortical structures.</p>
<fig id="pcbi.1005497.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The information propagation problem.</title>
<p>This problem is illustrated with the visual periphery, but the information propagation problem is general: it arises whenever information is transmitted from one area to another, and also when information is combined to carry out computations. (<bold>A</bold>) The retina transmits information about visual stimuli, <italic>s</italic>, to the visual cortex. The information does not propagate directly from retina to cortex; it is transmitted via an intermediary structure, the lateral geniculate nucleus (LGN). Consequently, the information about the stimuli that is available to the cortex, denoted <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), is not the same as the information that retina transmits, denoted <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>). Here, we ask what properties of neural activities in the periphery maximize the information that propagates to the deeper neural structures. (<bold>B</bold>) Illustration of our model. Neural activity in the periphery, <bold>x</bold>, is generated by passing the stimulus, <italic>s</italic>, through a set of neural tuning curves, <bold>f</bold>(<italic>s</italic>), and then adding zero-mean noise, <bold><italic>ξ</italic></bold>, which may be correlated between cells. This activity then propagates via feed-forward connectivity, described by the matrix <bold>W</bold>, to the next layer. The activity at the next layer, <bold>y</bold>, is generated by passing the inputs, <bold>W</bold> · <bold>x</bold>, through a nonlinearity <italic>g</italic>(⋅), and then adding zero-mean noise, <bold><italic>η</italic></bold>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g001" xlink:type="simple"/>
</fig>
<p>Despite its importance, the ability of information to propagate through neural circuits remains relatively unexplored [<xref ref-type="bibr" rid="pcbi.1005497.ref031">31</xref>]. One notable exception is the literature on how synchrony among the spikes of different cells affects responses in downstream populations [<xref ref-type="bibr" rid="pcbi.1005497.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref036">36</xref>]. This is, however, distinct from the information propagation question we consider here, as there is no guarantee that those downstream spikes will be informative. Other work [<xref ref-type="bibr" rid="pcbi.1005497.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref038">38</xref>] investigated the question of optimal network properties (tuning curves and connection matrices) for information propagation in the presence of noise.</p>
<p>No prior work, however, has isolated the impact of correlations on the ability of population-coded information to propagate. Given the frequent observations of correlations in the sensory periphery [<xref ref-type="bibr" rid="pcbi.1005497.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref039">39</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref045">45</xref>], and the importance of the information propagation problem, this is a significant gap in our knowledge. To fill that gap, we consider a model (<xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1B</xref>; described in more detail below), in which there are two layers (retina and LGN, for example). The first layer contains a fixed amount of information, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>), which is encoded in the noisy, stimulus-dependent responses of the cells in that layer. The information is passed to the second layer via feedforward connections followed by a nonlinearity, with noise added along the way. We ask how the covariance structure of the trial-to-trial variability in the first layer affects the amount of information in the second.</p>
<p>Although we focus on information propagation, the problem we consider applies to more general scenarios. In essence, we are asking: how does the noise in the input to a network interact with noise added to the output? Because we consider linear feedforward weights followed by a nonlinearity, the possible transformations from input to output, and thus the computations the network could perform, is quite broad [<xref ref-type="bibr" rid="pcbi.1005497.ref046">46</xref>]. Thus, the conclusions we draw apply not just to information propagation, but also to many computations. Moreover, it may be possible to extend our analysis to recurrent, time-dependent neural networks. That is, however, beyond the scope of this work.</p>
<p>Our results indicate that the amount of information that successfully propagates to the second layer depends strongly on the structure of correlated responses in the first. For linear neural gain functions, and some classes of nonlinear ones, we identify analytically the covariance structures that optimize information propagation through noisy downstream circuits. Within the optimal family of covariance structures, we find variability with so-called differential correlations [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>]—correlations that are proven to minimize the information in neural population activity. Thus, covariance structures that maximize the information content of neural population codes, and those that maximize the ability of this information to propagate, can be very different. Importantly, we also find that redundancy is neither necessary nor sufficient for the population code to be robust against corruption by noise. Consequently, to understand how correlated neural activity affects the function of neural systems, we must not only consider the impact of those correlations on information, but also the ability of the encoded information to propagate robustly through multi-layer circuits.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Problem formulation: Information propagation in the presence of corrupting noise</title>
<p>We consider a model in which a vector of “peripheral” neural population responses, <bold>x</bold>, is determined by two components. The first is the set of tuning curves, <bold>f</bold>(<italic>s</italic>), which define the cells’ mean responses to any particular stimulus (typical tuning curves are shown in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2A</xref>). Here we consider a one dimensional stimulus, denoted <italic>s</italic>, which may represent, for example, the direction of motion of a visual object. In that case, a natural interpretation of our model is that it describes the transmission of motion information by direction selective retinal ganglion cells to the visual cortex (<xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1</xref>) [<xref ref-type="bibr" rid="pcbi.1005497.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref047">47</xref>]. Extension to multi-dimensional stimuli is straightforward. The second component of the neural population responses, <bold><italic>ξ</italic></bold>, represents the trial-to-trial variability. This results in the usual “tuning curve plus noise” model,
<disp-formula id="pcbi.1005497.e001"><alternatives><graphic id="pcbi.1005497.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">f</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <bold><italic>ξ</italic></bold> is a zero mean random variable with covariance <bold>Σ</bold><sub><italic>ξ</italic></sub>.</p>
<fig id="pcbi.1005497.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Not all population codes are equally robust against corruption by noise.</title>
<p>We constructed two model populations, each with the same 100 tuning curves for the first layer of cells but with different covariance structures, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> (see text, especially <xref ref-type="disp-formula" rid="pcbi.1005497.e008">Eq (4)</xref>). The covariance structures were chosen so that the two populations convey identical amounts of information <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) about the stimulus. (<bold>A</bold>) 20 randomly-chosen tuning curves from the 100 cell population. (<bold>B</bold>) We corrupted the responses of each neural population by additional Gaussian noise (independently and identically distributed for all cells) of variance <italic>σ</italic><sup>2</sup>, to mimic corruption that might arise as the signals propagate through a multi-layered neural circuit, and computed the “output” information <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>) that these further-corrupted responses convey about the stimulus (blue and green curves). The population shown in green forms a relatively fragile code wherein modest amounts of noise strongly reduce the information, whereas the population shown in blue is more robust. (<bold>C</bold>) Input information <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) in the two model populations (left; “correlated”) and information that would be conveyed by the model populations if they had their same tuning curves and levels of trial-to-trial variability, but no correlations between cells (right; “trial-shuffled”). For panels B and C, we computed the information for each of 100 equally spaced stimulus values, and averaged the information over those stimuli. See <xref ref-type="sec" rid="sec009">Methods</xref> for additional details (section titled “Details for Numerical Examples”).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g002" xlink:type="simple"/>
</fig>
<p>The neural activity, <bold>x</bold>, propagates to the second layer via feed-forward weights, <bold>W</bold>, as in the model of [<xref ref-type="bibr" rid="pcbi.1005497.ref038">38</xref>]. The activity in the second layer is given by passing the input, <bold>W</bold> · <bold>x</bold>, through a nonlinearity, <italic>g</italic>(⋅), and then corrupting it with noise, <bold><italic>η</italic></bold> (<xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1B</xref>),
<disp-formula id="pcbi.1005497.e002"><alternatives><graphic id="pcbi.1005497.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">η</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where the nonlinearity is taken component by component, and <bold><italic>η</italic></bold> is zero mean noise with covariance matrix <bold>Σ</bold><sub><italic>η</italic></sub>. The function <italic>g</italic>(⋅) need not be invertible, so this model can include spike generation.</p>
<p>While we have, in <xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1</xref>, given one explicit interpretation of our model, the model itself is quite general. This means that our results apply more broadly than just to circuits in the peripheral visual system. Moreover, while our analysis (below) focuses on information loss between layers, this should not be taken to mean that there is no meaningful computation happening within the circuit: because we have considered arbitrary nonlinear transformations between layers, the same model can describe a wide range of possible computations [<xref ref-type="bibr" rid="pcbi.1005497.ref046">46</xref>]. Our results apply to information loss during those computations.</p>
<p>In the standard fashion [<xref ref-type="bibr" rid="pcbi.1005497.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>], we quantify the information in the neural responses using the linear Fisher information. This measure quantifies the precision (inverse of the mean squared error) with which a locally optimal linear estimator can recover the stimulus from the neural responses [<xref ref-type="bibr" rid="pcbi.1005497.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref049">49</xref>]. The linear Fisher information in the first and second layers, denoted <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) and <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), respectively, is given by
<disp-formula id="pcbi.1005497.e003"><alternatives><graphic id="pcbi.1005497.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(3a)</label></disp-formula> <disp-formula id="pcbi.1005497.e004"><alternatives><graphic id="pcbi.1005497.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(3b)</label></disp-formula>
where a prime denotes a derivative. Here <bold>W</bold><sub>eff</sub> are the effective weights—basically, the weights, <bold>W</bold>, multiplied by the average slope of the gain function, <italic>g</italic>(⋅)—and <bold>Σ</bold><sub>eff,<italic>η</italic></sub> includes contributions from the noise in the second layer, <bold><italic>η</italic></bold>, and, if <italic>g</italic>(⋅) is nonlinear, from the noise in the first layer. (If <italic>g</italic> is linear, <bold>Σ</bold><sub>eff,<italic>η</italic></sub> = <bold>Σ</bold><sub><italic>η</italic></sub>, so in this case <bold>Σ</bold><sub>eff,<italic>η</italic></sub> depends only on the noise in the second layer). This expression is valid if <inline-formula id="pcbi.1005497.e005"><alternatives><graphic id="pcbi.1005497.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is invertible; so long as there are more cells in the second layer than the first, this is typically the case. See <xref ref-type="sec" rid="sec009">Methods</xref> for details (section titled “Information in the output layer”).</p>
<p>
<xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref> is somewhat intuitive, at least at a gross level: both large effective noise (<bold>Σ</bold><sub>eff,<bold><italic>η</italic></bold></sub>) and small effective weights (<bold>W</bold><sub>eff</sub>) reduce the amount of information at the second layer. At a finer level, the relationship between the two covariance structures—corresponding to the first and second terms in brackets in <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>—can have a large effect on <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), as we will see shortly.</p>
</sec>
<sec id="sec004">
<title>Information content and information propagation put different constraints on neural population codes</title>
<p>We begin with an example to highlight the difference between the information contained in neural population codes and the information that propagates through subsequent layers. Here, we consider two different neuronal populations with identical tuning curves (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2A</xref>), nearly-identical levels of trial-to-trial neural variability, and identical amounts of stimulus information encoded in their firing-rate responses; the populations’ correlational structures, however, differ. We then corrupt these two populations’ response patterns with noise, to mimic corruption that might arise in subsequent processing stages, and ask how much of the stimulus information remains. Surprisingly, the two population codes can show very different amounts of information after corruption by even modest amounts of noise (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2B</xref>).</p>
<p>In more detail, there are 100 neurons in the first layer; those neurons encode an angle, denoted <italic>s</italic>, via their randomly-shaped and located tuning curves (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2A</xref>). We consider two separate model populations. Both have the same tuning curves, but different covariance matrices. For reasons we discuss below, those covariance matrices, denoted <inline-formula id="pcbi.1005497.e006"><alternatives><graphic id="pcbi.1005497.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mtext>blue</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005497.e007"><alternatives><graphic id="pcbi.1005497.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mtext>green</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> (blue and green correspond to the colors in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2B and 2C</xref>), are given by
<disp-formula id="pcbi.1005497.e008"><alternatives><graphic id="pcbi.1005497.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mtext>blue</mml:mtext></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle> <mml:mo>)</mml:mo></mml:mrow> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>s</mml:mi></mml:mstyle> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(4a)</label></disp-formula> <disp-formula id="pcbi.1005497.e009"><alternatives><graphic id="pcbi.1005497.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mtext>green</mml:mtext></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(4b)</label></disp-formula>
where <bold>Σ</bold><sub>0</sub> is a diagonal matrix with elements equal to the mean response,
<disp-formula id="pcbi.1005497.e010"><alternatives><graphic id="pcbi.1005497.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>Here <italic>δ</italic><sub><italic>ij</italic></sub> is the Kronecker delta (<italic>δ</italic><sub><italic>ij</italic></sub> = 1 if <italic>i</italic> = <italic>j</italic> and 0 otherwise), and we use the convention that two adjacent vectors denote an outer product; for instance, the <italic>ij</italic><sup>th</sup> element if <bold>uu</bold> is <italic>u</italic><sub><italic>i</italic></sub> <italic>u</italic><sub><italic>j</italic></sub>. The vector <bold>u</bold> has the same magnitude as <bold>f</bold>′, but points in a slightly different direction (it makes an angle <italic>θ</italic><sub><italic>u</italic></sub> with <bold>f</bold>′), and <italic>ϵ</italic> and <italic>ϵ</italic><sub><italic>u</italic></sub> are chosen so that the information in the two populations, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>), is the same (<italic>ϵ</italic><sub><italic>u</italic></sub> also depends on <italic>s</italic>; we suppress that dependence for clarity).</p>
<p>In our simulations, both <italic>ϵ</italic> and <italic>ϵ</italic><sub><italic>u</italic></sub> are small (on the order of 10<sup>−3</sup>; see <xref ref-type="sec" rid="sec009">Methods</xref>), so the variance of the <italic>i</italic><sup>th</sup> neuron is approximately equal to its mean. This makes the variability Poisson-like, as is typically observed when counting neural spikes in finite time windows [<xref ref-type="bibr" rid="pcbi.1005497.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref006">6</xref>]. (More precisely, the average Fano factors—averaged over neurons and stimuli—were 1.01 for the “blue” population and 1.04 for the “green” one.) Both model populations also have the same average correlation coefficients, which are near-zero (see <xref ref-type="sec" rid="sec009">Methods</xref>, section titled “Details for Numerical Examples”).</p>
<p>To determine how much of the information in the two populations propagates to the second layer, we computed <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>) for both populations using <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>. For simplicity, we used the identity matrix for the feed-forward weights, <bold>W</bold>, a linear gain function, <italic>g</italic>(⋅), and independently and identically distributed (<italic>iid</italic>) noise with variance <italic>σ</italic><sup>2</sup>. Later we consider the more general case: arbitrary feedforward weights, nonlinear gain functions, and arbitrary covariance for the second layer noise. Those complications don’t, however, change the basic story.</p>
<p>
<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2B</xref> shows the information in the output layer versus the level of output noise, <italic>σ</italic><sup>2</sup>, for the two populations. Blue and green curves correspond to the different covariance structures. Although the two populations have identical tuning curves, nearly-identical levels of trial-to-trial neural variability, and contain identical amounts of information about the stimulus, they differ markedly in the robustness of that information to corruption by noise in the second layer. Thus, quantifying the information content of neural population codes is not sufficient to characterize them: recordings from the first-layer cells of the two example populations in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref> would yield identical information about the stimulus, but the blue population has a greater ability to propagate that information downstream.</p>
<p>One possible explanation for the difference in robustness is that the information in the green population relies heavily on correlations, which are destroyed by a small amount of noise. To check this, we compared the information of the correlated neural populations to the information that would be obtained with the same tuning curves and levels of single neuron trial-to-trial variability, but no inter-neuronal correlations [<xref ref-type="bibr" rid="pcbi.1005497.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref051">51</xref>] (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2C</xref>). We find that removing the correlations actually <italic>increases</italic> the information in both populations (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2C</xref>; “Trial-Shuffled”), and by about the same amount, so this possible explanation cannot account for the difference in robustness. We also considered the case where the correlated responses carry more information than would be obtained from independent cells. We again found (similar to <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2C</xref>) that there could be substantial differences in the amount of information propagated by equally informative population codes (see <xref ref-type="sec" rid="sec009">Methods</xref>, section titled “Details for numerical examples”, and the figure therein).</p>
<p>These examples illustrate that merely knowing the amount of information in a population, or how that information depends on correlations in neural responses, doesn’t tell us how much of that information will propagate to the next layer. In the remainder of this paper, we provide a theoretical explanation of this observation, and identify the covariance structures at the first layer that maximize robustness to information loss during propagation through downstream circuits.</p>
</sec>
<sec id="sec005">
<title>Geometry of robust versus fragile population codes</title>
<p>To understand, from a geometrical point of view, why some population codes are more sensitive to noise than others, we need to consider the relationship between the noise covariance ellipse and the “signal direction,” <bold>f</bold>′(<italic>s</italic>)—the direction the mean neural response changes when the stimulus <italic>s</italic> changes by a small amount. <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3A and 3B</xref> show this relationship for two different populations. The noise distribution in the first layer is indicated by the magenta ellipses, and the signal direction by the green arrows. The uncertainty in the stimulus after observing the neural response is indicated by the overlap of the green line with the magenta ellipse. Because the overlap is the same for the two populations, they have the same amount of stimulus uncertainty, and thus the same amount of information—at least in the first layer.</p>
<fig id="pcbi.1005497.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Geometry of robust versus fragile population codes.</title>
<p>Cartoons showing the interaction of signal and noise for two populations with the same information in the input layer. The dimension of the space is equal to the number of cells in the population; we show a two dimensional projection. Within this space, when the stimulus changes by an amount Δ<italic>s</italic> (with Δ<italic>s</italic> small), the average neural response changes by <bold>f</bold>′(<italic>s</italic>)Δ<italic>s</italic>. Thus, <bold>f</bold>′(<italic>s</italic>) is the “signal direction” (green arrows). Trial-by-trial fluctuations in the neural responses in the first layer are described by the ellipses; these correspond to 1 standard-deviation probability contours of the conditional response distributions. The impact of the neural variability on the encoding of stimulus <italic>s</italic> is determined by the projection of the response distributions onto the signal direction (magenta double-headed arrows). By construction, these are identical in the first layer. Accordingly, an observer of the neural activity in the first layer of either population would have the same level of uncertainty about the stimulus, and so both populations encode the same amount of stimulus information. When additional <italic>iid</italic> noise is added to the neural responses, the response distributions grow; the dashed ellipses show the resultant response distributions at the second layer. Even though the same amount of <italic>iid</italic> noise is added to both populations, the one in panel A shows greater stimulus uncertainty after the addition of noise than does the one in panel B. Consequently, the information encoded by the population in panel B is more robust against corruption by noise.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g003" xlink:type="simple"/>
</fig>
<p>Although the two populations have the same amount of information, the covariance ellipses are very different: one long and skinny but slightly tilted relative to the signal direction (<xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3A</xref>), the other shorter and fatter and parallel to the signal direction (<xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3B</xref>). Consequently, when <italic>iid</italic> noise is added, as indicated by the dashed lines, stimulus uncertainty increases by very different amounts: there’s a much larger increase for the long skinny ellipse than for the short fat one. This makes the population code in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3A</xref> much more sensitive to added noise than the one in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3B</xref>.</p>
<p>To more rigorously support this intuition, in Methods, section titled “Analysis behind the geometry of information loss”, we derive explicit expressions for the stimulus uncertainty in the first and second layers as a function of the angle between the long axis of the covariance ellipse and the signal direction. Those expressions corroborate the phenomenon shown in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>.</p>
</sec>
<sec id="sec006">
<title>A family of optimal noise structures</title>
<p>The geometrical picture in the previous section tells us that a code is robust against added noise if the covariance ellipse lines up with the signal direction. Taken to its extreme, this suggests that when all the noise is concentrated along the <bold>f</bold>′(<italic>s</italic>) direction, so that the covariance matrix is given by
<disp-formula id="pcbi.1005497.e011"><alternatives><graphic id="pcbi.1005497.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∝</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
the resulting code should be optimally robust. While this may be intuitively appealing, the arguments that led to it were based on several assumptions: <italic>iid</italic> noise added in the second layer, feedforward weights, <bold>W</bold>, set to the identity matrix, and a linear neural response function <italic>g</italic>(⋅). In real neural circuits, none of these assumptions hold. It turns out, though, that the only one that matters is the linearity of <italic>g</italic>(⋅). In this section we demonstrate that the covariance matrix given by <xref ref-type="disp-formula" rid="pcbi.1005497.e011">Eq (6)</xref> optimizes information transmission for neurons with linear gain functions (although we find, perhaps surprisingly, that this optimum is not unique). In the next section we consider nonlinear gain functions; for that case the covariance matrix given by <xref ref-type="disp-formula" rid="pcbi.1005497.e011">Eq (6)</xref> can be, but is not always guaranteed to be, optimal.</p>
<p>To determine what covariance structures maximize information propagation, we simply maximize information in the second layer, <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), with respect to the noise covariance matrix in the first layer, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>, with the information in the first layer held fixed. When the gain function, <italic>g</italic>(⋅), is linear (the focus of this section), this is relatively straightforward. Details of the calculation are given in Methods, section titled “Identifying the family of optimal covariance matrices”; here we summarize the results.</p>
<p>The main finding is that there exists a family of first-layer covariance matrices <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>, not just one, that maximizes the information in the second layer. That family, parameterized by <italic>α</italic>, is given by
<disp-formula id="pcbi.1005497.e012"><alternatives><graphic id="pcbi.1005497.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi></mml:mrow> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where <bold>Σ</bold><sub><italic>y</italic></sub> is the effective covariance matrix in the second layer,
<disp-formula id="pcbi.1005497.e013"><alternatives><graphic id="pcbi.1005497.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>≡</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
and <italic>I</italic><sub><italic>η</italic></sub>(<italic>s</italic>) is the information the second layer would have if there were no noise in the first layer,
<disp-formula id="pcbi.1005497.e014"><alternatives><graphic id="pcbi.1005497.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
(see in particular <xref ref-type="sec" rid="sec009">Methods</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref>). For this whole family of distributions—that is, for any value of <italic>α</italic> for which <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> is positive semi-definite—the output information, <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), has exactly the same value,
<disp-formula id="pcbi.1005497.e015"><alternatives><graphic id="pcbi.1005497.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
(see <xref ref-type="sec" rid="sec009">Methods</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e120">Eq (76)</xref>). This is the maximum possible output information given the input information, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>).</p>
<p>Two members of this family are of particular interest. One is <italic>α</italic> = 0, for which the covariance matrix corresponds to differential correlations (<xref ref-type="disp-formula" rid="pcbi.1005497.e011">Eq (6)</xref>); that covariance matrix is illustrated in <xref ref-type="fig" rid="pcbi.1005497.g004">Fig 4A</xref>. This covariance matrix aligns the noise direction with the signal direction. Accordingly, as for the geometrical picture in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>, it makes the encoded information maximally robust.</p>
<fig id="pcbi.1005497.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Family of optimal covariance matrices.</title>
<p>For all panels, green arrows indicate the signal direction, <bold>f</bold>′(<italic>s</italic>). Magenta ellipses indicate the noise in the first layer (with corresponding covariance matrix <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>), and grey ellipses indicate the effective noise in the second layer (with corresponding covariance matrix <bold>Σ</bold><sub><italic>y</italic></sub>). (<bold>A</bold>) The covariance ellipse in the first layer has its long axis aligned with the signal direction; this configuration (which corresponds to differential correlations) optimizes information robustness for any distribution of second layer noise. (<bold>B</bold>) The covariance ellipse in the first layer does not have its long axis aligned with the signal direction. However, the covariance ellipse of the effective noise in the second layer, <bold>Σ</bold><sub><italic>y</italic></sub>, has the same shape as the covariance ellipse in the first. In this case, the blue “good” projection—which is aligned both with a low-variance direction of the first-layer distribution (magenta), and with the signal curve (green), and thus is relatively informative about the stimulus (see text)—is corrupted by relatively little noise at the second layer. This “matched” noise configuration is among those that optimize robustness to noise. The optimal family of covariance matrices interpolates between the configurations shown in panels A and B. (<bold>C</bold>) Again the covariance ellipse in the first layer does not have its long axis aligned with the signal direction. But now the “good” projection is heavily corrupted by noise at the second layer. In this configuration, all projections are substantially corrupted by noise at some point in the circuit, and thus relatively little information can propagate.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g004" xlink:type="simple"/>
</fig>
<p>The other family member we highlight is <italic>α</italic> = 1, for which <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> ∝ <bold>Σ</bold><sub><italic>y</italic></sub>. For this case, the covariance matrix in the first layer matches the effective covariance matrix in the second layer; we thus refer to this as “matched covariance”. To understand why this covariance optimizes information in the second layer, we start with the observation that the population activities can be decomposed into their principal components: each principal component corresponds to a different axis along with the population activities can be projected. The information contained in each such projection (principal component) adds up to give the total Fisher information (see <xref ref-type="sec" rid="sec009">Methods</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e109">Eq 71</xref>). The most informative of these projections are those that have low noise variance, and which align somewhat with the signal curve—like the blue line in <xref ref-type="fig" rid="pcbi.1005497.g004">Fig 4B</xref>. When <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> ∝ <bold>Σ</bold><sub><italic>y</italic></sub>, the projections that are most informative in the first layer are corrupted by relatively little noise in the second layer. Consequently, this configuration enables robust information propagation. In contrast, when the covariance structures in the first and second layers are less well matched, all projections are heavily corrupted by noise at some point (i.e., either in the first or the second layer), and hence very little information propagates (<xref ref-type="fig" rid="pcbi.1005497.g004">Fig 4C</xref>).</p>
<p>The family of optima interpolates between the two configurations shown in <xref ref-type="fig" rid="pcbi.1005497.g004">Fig 4A and 4B</xref> (see also <xref ref-type="disp-formula" rid="pcbi.1005497.e012">Eq (7)</xref>). Almost all members of this optimal covariance family depend on the details of the downstream circuit: for <italic>α</italic> ≠ 0 in <xref ref-type="disp-formula" rid="pcbi.1005497.e012">Eq (7)</xref>, the optimal noise covariance at the first layer depends on the feed-forward weights, <bold>W</bold>, and the structure of the downstream noise. The one exception to this is the covariance matrix given by <xref ref-type="disp-formula" rid="pcbi.1005497.e011">Eq (6)</xref>: that one is optimal regardless of the downstream circuit. These are so-called “differential correlations”—the only correlations that lead to information saturation in large populations [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>], and the correlations that minimize information in general (see <xref ref-type="sec" rid="sec009">Methods</xref>, section titled “Minimum information”, for proof). The fact that correlations can minimize information content and at the same time maximize robustness highlights the fact that optimizing the amount of information in a population code versus optimizing the ability of that information to be transmitted put very different constraints on neural population codes.</p>
<p>The existence of an optimum where the covariance matrices are matched across layers emphasizes that not all optimally robust population codes are necessarily redundant. (By redundant we mean the population encodes less information than would be encoded by a population of independent cells with the same tuning curves and levels of single neuron trial-to-trial variability [<xref ref-type="bibr" rid="pcbi.1005497.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref021">21</xref>]; see <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>). Notably, if the effective second layer covariance matrix, <bold>Σ</bold><sub><italic>y</italic></sub>, admits a synergistic population code—wherein more information is encoded in the correlated population versus an uncorrelated one with the same tuning curves and levels of trial-to-trial response variability—then the matched case, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> ∝ <bold>Σ</bold><sub><italic>y</italic></sub>, will also admit a synergistic population code, and be optimally robust.</p>
<p>Optimally robust, however, does not necessarily mean the majority of the information is transmitted; for that we need another condition. We show in the Methods section titled “Variances of neural responses, and robustness to added noise, for different coding strategies” that for non-redundant codes, a large fraction of the information is transmitted only if there are many more neurons in the second layer than in the first. This is typically the case in the periphery. For differential correlations, that condition is not necessary—so long as there are a large number of neurons in both the input and output layers, most of the information is transmitted.</p>
</sec>
<sec id="sec007">
<title>Nonlinear gain functions</title>
<p>So far we have focused on linear gain functions <italic>g</italic>(⋅); here we consider nonlinear ones. This case is much harder to analyze, as the effective covariance structure in the second layer, <bold>Σ</bold><sub>eff,<italic>η</italic></sub>, depends on the noise in the first layer (see <xref ref-type="sec" rid="sec009">Methods</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e034">Eq (22)</xref>). We therefore leave the analysis to Methods (section titled “Nonlinear gain functions”); here we briefly summarize the main results. After that we consider two examples of nonlinear gain functions—both involving a thresholding nonlinearity to mimic spike generation.</p>
<p>For linear gain functions we were able to find a whole family of optimal covariance structures, for nonlinear ones we did not even try. Instead, we asked: under what circumstances are differential correlations optimal? Even for this simplified question a definitive answer does not appear to exist. Nevertheless, we can make progress in special cases. When there is no added noise in the second layer (e.g., <bold><italic>η</italic></bold> = 0 for the model in <xref ref-type="fig" rid="pcbi.1005497.g001">Fig 1B</xref>), differential correlations maximize the amount of information that propagates through the nonlinearity, so long as the tuning curves are sufficiently dense relative to the steepness of the tuning curves (meaning that whenever the stimulus changes, the average stimulus-evoked response of at least one neuron also changes; see <xref ref-type="sec" rid="sec009">Methods</xref>). If there is added noise at the second layer, differential correlations tend to be optimal in cases where the addition of noise at the first layer, <italic>ξ</italic>, causes reductions in information, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>). (This means that, so long as there are no <italic>stochastic resonance</italic> effects causing added noise to increase information, then differential correlations are optimal.)</p>
<p>We first check, with simulations, the prediction that differential correlations are optimal if there is no added noise. For that we use a thresholding nonlinearity, chosen for two reasons: it is an extreme nonlinearity, and so should be a strong test of our theory, and it is somewhat realistic in that it mimics spike generation. For this model, the responses at that second layer, <italic>y</italic><sub><italic>i</italic></sub>, are given by
<disp-formula id="pcbi.1005497.e016"><alternatives><graphic id="pcbi.1005497.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where Θ is the Heaviside step function (Θ(<italic>x</italic>) = 1 if <italic>x</italic> ≥ 0 and 0 otherwise), and <italic>θ</italic><sub><italic>i</italic></sub> is the spiking threshold of the <italic>i</italic><sup><italic>th</italic></sup> neuron. This is the popular dichotomized Gaussian model [<xref ref-type="bibr" rid="pcbi.1005497.ref052">52</xref>–<xref ref-type="bibr" rid="pcbi.1005497.ref056">56</xref>], which has been shown to provide a good description of population responses in visual cortex, at least in short time windows [<xref ref-type="bibr" rid="pcbi.1005497.ref054">54</xref>], and to provide high-fidelity descriptions of the responses of integrate-and-fire neurons, again in short time windows [<xref ref-type="bibr" rid="pcbi.1005497.ref057">57</xref>].</p>
<p>In our simulations with the step function nonlinearity, as for all of the other cases we considered above, the first layer responses are given by the tuning curve plus noise model (<xref ref-type="disp-formula" rid="pcbi.1005497.e001">Eq (1)</xref>). The tuning curves, <bold>f</bold>(<italic>s</italic>), of the 100-neuron population are again heterogeneous (similar to those in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2A</xref> but with a different random draw from the tuning curve distribution), and the trial-to-trial variability is given by
<disp-formula id="pcbi.1005497.e017"><alternatives><graphic id="pcbi.1005497.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
with <bold>Σ</bold><sub>0</sub> given by <xref ref-type="disp-formula" rid="pcbi.1005497.e010">Eq (5)</xref>. This is the same covariance matrix as in <xref ref-type="disp-formula" rid="pcbi.1005497.e009">Eq (4b)</xref>, except that we have included an overall scale factor, <italic>γ</italic><sub><italic>u</italic></sub>, chosen to ensure that the information in the input layer is independent of both <italic>ϵ</italic><sub><italic>u</italic></sub> and <bold>u</bold>(<italic>s</italic>) (see <xref ref-type="sec" rid="sec009">Methods</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e159">Eq (99)</xref>).</p>
<p>Because these (step function) nonlinearities are infinitely steep, the tuning curves are not sufficiently dense for our mathematical analysis to guarantee that differential correlations are optimal for information propagation. However, we argue in Methods (section titled “Nonlinear gain functions”), that this should be approximately true for large populations. And indeed, that’s what we find with our numerical simulation, as shown in <xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5B</xref>. When <italic>θ</italic><sub><italic>u</italic></sub> = 0 (recall that <italic>θ</italic><sub><italic>u</italic></sub> is the angle between <bold>u</bold>(<italic>s</italic>) and <bold>f</bold>′(<italic>s</italic>)), so that <bold>u</bold>(<italic>s</italic>) = <bold>f</bold>′(<italic>s</italic>), the second term in <xref ref-type="disp-formula" rid="pcbi.1005497.e017">Eq (12)</xref> corresponds to differential correlations; in this case, information increases monotonically with <italic>ϵ</italic><sub><italic>u</italic></sub>. In other words, information propagated through the step function nonlinearity increases as “upstream” correlations become more like pure differential correlations. In contrast, when <italic>θ</italic><sub><italic>u</italic></sub> is nonzero (as in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3A</xref>), information does not propagate well: information decreases as <italic>ϵ</italic><sub><italic>u</italic></sub> increases. This is consistent with our findings for the linear gain function considered in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>. Thus, differential correlations can optimize information transmission even for a nonlinearity as extreme as a step function.</p>
<fig id="pcbi.1005497.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Differential correlations enhance information propagation through “spike-generating” nonlinearities.</title>
<p>Responses in the second layer were generated using the dichotomized Gaussian model of spike generation, in which the input from the first layer was simply binarized via a step function (see <xref ref-type="disp-formula" rid="pcbi.1005497.e016">Eq (11)</xref>). We varied the correlations in these inputs (see <xref ref-type="disp-formula" rid="pcbi.1005497.e017">Eq (12)</xref>) while keeping the input information and input tuning curves fixed. (<bold>A</bold>) Heterogeneous tuning curves in the second layer, evaluated at <italic>ϵ</italic><sub><italic>u</italic></sub> = 0; we show a random subset of 20 cells out of the 100-neuron population studied in panel B. (<bold>B</bold>) Information transmitted by the 100-cell spiking population as a function of <italic>ϵ</italic><sub><italic>u</italic></sub>, which is the strength of the noise in the <bold>u</bold>(<italic>s</italic>) direction, for different angles, <italic>θ</italic><sub><italic>u</italic></sub>, between <bold>u</bold> and <bold>f</bold>′(<italic>s</italic>) (see <xref ref-type="disp-formula" rid="pcbi.1005497.e017">Eq (12)</xref>). The input information was held fixed as <italic>ϵ</italic><sub><italic>u</italic></sub> was varied. The information is averaged over 20 evenly spaced stimuli (see <xref ref-type="sec" rid="sec009">Methods</xref>, section titled “Details for Numerical Examples”).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g005" xlink:type="simple"/>
</fig>
<p>The lack of explicit added noise at the second layer makes this case somewhat unrealistic. In neural circuits, we expect noise to be added at each stage of processing—if nothing else, due to synaptic failures. We thus considered a model in which noise is added before the spike-generation process,
<disp-formula id="pcbi.1005497.e018"><alternatives><graphic id="pcbi.1005497.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ζ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where <italic>ζ</italic><sub><italic>i</italic></sub> is zero-mean noise with covariance matrix <bold>Σ</bold><sub><italic>ζ</italic></sub>.</p>
<p>We computed information for this model using the same input tuning curves, spike thresholds, and covariance matrix, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>, as without the additional noise (i.e., as in <xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5</xref>). To mimic the kind of independent noise expected from synaptic failures, we chose the <italic>ζ</italic><sub><italic>i</italic></sub> to be <italic>iid</italic>, and for simplicity we took them to be Gaussian distributed with variance <inline-formula id="pcbi.1005497.e019"><alternatives><graphic id="pcbi.1005497.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ζ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. We computed the amount of stimulus information, <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), for several different levels of the added input noise <inline-formula id="pcbi.1005497.e020"><alternatives><graphic id="pcbi.1005497.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ζ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. We found that for all levels of noise, differential correlations increase information transmission (<italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>) increases monotonically with <italic>ϵ</italic><sub><italic>u</italic></sub> in <xref ref-type="fig" rid="pcbi.1005497.g006">Fig 6A</xref>, for which <italic>θ</italic><sub><italic>u</italic></sub> = 0). And we again found that when the long axis of the covariance ellipse makes a small angle with the signal direction, information propagates poorly (<xref ref-type="fig" rid="pcbi.1005497.g006">Fig 6B</xref>, for which <italic>θ</italic><sub><italic>u</italic></sub> = 0.1 rad.).</p>
<fig id="pcbi.1005497.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Information propagation through spike-generating nonlinearities with additive input noise.</title>
<p>As with <xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5</xref>, responses in the second layer were generated using the dichotomized model of spike generation, in which the input from the first layer was simply binarized. Here, though, Gaussian noise was added before thresholding; see <xref ref-type="disp-formula" rid="pcbi.1005497.e018">Eq (13)</xref>. We varied the correlations in the input layer (see <xref ref-type="disp-formula" rid="pcbi.1005497.e017">Eq (12)</xref>) while keeping the input information and input tuning curves fixed for the 100-cell population (same tuning curves and covariance matrices as in <xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5</xref>). The additive noise at the second layer (the <italic>ζ</italic><sub><italic>i</italic></sub>) was <italic>iid</italic> Gaussian, with variance <inline-formula id="pcbi.1005497.e021"><alternatives><graphic id="pcbi.1005497.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ζ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>; different colored lines correspond to different values of <inline-formula id="pcbi.1005497.e022"><alternatives><graphic id="pcbi.1005497.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ζ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. (<bold>A</bold>) Output information versus <italic>ϵ</italic><sub><italic>u</italic></sub> for populations with differential correlations (<bold>u</bold> = <bold>f</bold>′(<italic>s</italic>)). (<bold>B</bold>) Same as panel A, but for populations that concentrate noise along an axis, <bold>u</bold>, that makes an angle of 0.1 rad with the <bold>f</bold>′(<italic>s</italic>) direction. For both panels, the input information was held fixed as <italic>ϵ</italic><sub><italic>u</italic></sub> was varied, and the information was averaged over 20 evenly spaced stimuli.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g006" xlink:type="simple"/>
</fig>
<p>These numerical findings for a spike-generating nonlinearity with added noise are similar to the previous cases of a linear transfer function, <italic>g</italic>(⋅), with added input noise (Figs <xref ref-type="fig" rid="pcbi.1005497.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1005497.g003">3</xref>), for which we have analytical results, or a spike generating nonlinearity with no added input noise (<xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5</xref>), for which we do not. We further argue in Methods (section titled “Nonlinear gain functions”), that for nonlinear gain functions differential correlations are likely to be optimal if the tuning curves are optimal (in the case of <xref ref-type="disp-formula" rid="pcbi.1005497.e018">Eq (13)</xref>, if the thresholds <italic>θ</italic><sub><italic>i</italic></sub> are chosen optimally). Taken together, our findings demonstrate that differential correlations in upstream populations generally increase the information that can be propagated downstream through noisy, nonlinear neural circuits.</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>Much work in systems neuroscience has investigated the factors that influence the amount of information about a stimulus that is encoded in neural population activity patterns. Here we addressed a related question that is often overlooked: how do correlations between neurons affect the ability of information to propagate robustly through subsequent stages of neural circuitry? The question of robustness is potentially quite important, as the ability of information to propagate determines how much information from the periphery will reach the deeper neural structures that affect decision making and behavior. To investigate this issue, we considered a model with two cell layers. We varied the covariance matrix of the noise in the first layer (while keeping the tuning curves and information in the first layer fixed), and asked how much information could propagate to the second layer. Our main findings were threefold.</p>
<p>First, population codes with different covariance structures but identical tuning curves and equal amounts of encoded information can differ substantially in their robustness to corruption by additional noise (Figs <xref ref-type="fig" rid="pcbi.1005497.g002">2</xref>, <xref ref-type="fig" rid="pcbi.1005497.g005">5</xref>, <xref ref-type="fig" rid="pcbi.1005497.g006">6</xref> and <xref ref-type="fig" rid="pcbi.1005497.g007">7</xref>). Consequently, measurements of information at the sensory periphery are insufficient to understand the ability of those peripheral structures to propagate information to the brain, as that propagation process inevitably adds noise. For instance, populations of independent neurons can be much worse at transmitting information than can populations displaying correlated variability (<xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5B</xref>). Thus, to understand how the brain efficiently encodes information, we must concern ourselves not just with the amount of information in a population code, but also with the robustness of that encoded information against corruption by noise.</p>
<fig id="pcbi.1005497.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005497.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Not all synergistic population codes are equally robust against corruption by noise.</title>
<p>This figure is similar to <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, but with synergistic instead of redundant population codes. We constructed two model populations—each with the same 100 tuning curves (20 randomly-chosen example tuning curves are shown in panel <bold>A</bold>)—for the first layer of cells. The two populations have different covariance structures <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> for their trial-to-trial variability (see main text, <xref ref-type="disp-formula" rid="pcbi.1005497.e008">Eq (4)</xref>), but convey identical amounts of information, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>), about the stimulus. (<bold>B</bold>) We corrupted the responses of each neural population by Gaussian noise (independently and identically distributed for all cells) of variance <italic>σ</italic><sup>2</sup>, to mimic corruption that might arise as the signals propagate through a multi-layered neural circuit, and computed the output information, <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), that these further-corrupted responses convey about the stimulus (blue and green curves). (<bold>C</bold>) Input information <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) in the two model populations (left; “correlated”) and information that would be conveyed by the model populations if they had their same tuning curves and levels of trial-to-trial variability, but no correlations between cells (right; “trial-shuffled”). For panels B and C, we computed the information for 100 different stimulus values, equally spaced between 0 and 2<italic>π</italic>, and averaged the information over these stimuli.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005497.g007" xlink:type="simple"/>
</fig>
<p>Second, for linear gain functions, or noise-free nonlinear ones with sufficiently dense tuning curves, populations with so-called differential correlations [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>] are maximally robust against noise induced by information propagation. This fact may seem surprising given that differential correlations are the only ones that lead to information saturation in large populations [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>], and the correlations that minimize information in general. However, in hindsight it makes sense: differential correlations correspond to a covariance ellipse aligned with the signal direction (see <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3B</xref>), and added noise simply doesn’t make it much longer. For nonlinear gain functions combined with arbitrary noise, differential correlations are not guaranteed to yield a globally optimal population code for information propagation. However, for the spike-generating nonlinearity we considered here, differential correlations were at least a local optimum (see Figs <xref ref-type="fig" rid="pcbi.1005497.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1005497.g006">6</xref>).</p>
<p>Third, while differential correlations optimize robustness, for linear gain functions that optimum is not unique. Instead, there is a continuous family of covariances that exhibit identical robustness to noise (see <xref ref-type="fig" rid="pcbi.1005497.g004">Fig 4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e012">Eq (7)</xref>). However, within this family, only differential correlations yield population codes that are optimally robust independent of the downstream circuitry. Thus, they are the most flexible of the optima: for all other members of the family, the optimal covariance structure in the first layer depends on the noise in subsequent layers, as well as the weights connecting those layers.</p>
<p>The existence of this family of optimal solutions raises an important point with regards to redundancy and robust population coding. Populations with differential correlations—which are among the optimal solutions in terms of robustness—are highly redundant: a population with differential correlations encodes much less information than would be expected from independent populations with the same tuning curves and levels of trial-to-trial variability (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2C</xref>). It is common knowledge that redundancy can enhance robustness of population codes against noise [<xref ref-type="bibr" rid="pcbi.1005497.ref058">58</xref>], and thus it is worth asking if our robust population coding results are simply an application of this fact. Importantly, the answer is no: as discussed in Methods, section titled “A family of optimal noise structures”, within the family of optimal correlational structures are codes with minimal redundancy. Moreover, as is shown in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2B</xref>, a code can be redundant without being robust to added noise. In other words, redundancy in a population code is neither necessary, nor sufficient, to ensure that the encoded information is robust against added noise. However, there is an important caveat: unless the number of neurons in the second layer is large relative to the number in the first, and/or the added noise in the second layer is small relative to the noise at the first layer, non-redundant codes tend to lose a large amount of information when corrupted by noise. This contrasts sharply with differential correlations, which can tolerate large added noise with very little information loss (see <xref ref-type="sec" rid="sec009">Methods</xref> section titled “Variances of neural responses, and robustness to added noise, for different coding strategies”).</p>
<p>In the case of real neural systems, there will always be a finite amount of information that the population can convey (bounded by the amount of input information that the population receives from upstream sources [<xref ref-type="bibr" rid="pcbi.1005497.ref059">59</xref>]), and so the question of how best to propagate a (fixed) amount of information is of potentially great relevance for neural communication. Our results suggest that the presence of differential correlations serves to allow population-coded information to propagate robustly. Thus, an observation of these correlations in neural recordings might indicate that the population code is optimized for robustness of the encoded information. At the same time, we note that weak differential correlations might be hard to observe experimentally [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>]. Moreover, our calculations indicate that there exists a whole family of possible propagation-enhancing correlation structures, and so differential correlations are not necessary for robust information propagation. This means that observations of either differential correlations, correlation structures matched between subsequent layers of a neural circuit (<xref ref-type="fig" rid="pcbi.1005497.g004">Fig 4</xref>), or a combination of the above would indicate that the system enables robust information propagation.</p>
<p>How might the nervous system shape its responses so as to generate correlations that enhance information propagation? Recent work identified network mechanisms that can lead to differential correlations [<xref ref-type="bibr" rid="pcbi.1005497.ref060">60</xref>]. While it is beyond the scope of this work, it would be interesting to explicitly study the network structures that allow encoded information to propagate most robustly through downstream circuits. Relatedly, [<xref ref-type="bibr" rid="pcbi.1005497.ref038">38</xref>] and [<xref ref-type="bibr" rid="pcbi.1005497.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref030">30</xref>] asked how the connectivity between layers affects the ability of information to propagate. While we identified the optimal patterns of input to the multi-stage circuit, they identified the optimal anatomy of that circuit itself.</p>
<p>Note that we have used linear Fisher information to quantify the population coding efficacy. Other information measures exist, and it is worth commenting on how much our findings generalize to different measures. In the case of jointly Gaussian stimulus and response distributions, correlations that maximize linear Fisher information also maximize Shannon’s mutual information [<xref ref-type="bibr" rid="pcbi.1005497.ref020">20</xref>]. In that regime our findings should generalize well. Moreover, whenever the neural population response distributions belong to the exponential family with linear sufficient statistics, the linear Fisher information is equivalent to the (nonlinear) “full” Fisher information [<xref ref-type="bibr" rid="pcbi.1005497.ref029">29</xref>]. In practice, this is a good approximation to primary visual cortical responses to oriented visual stimuli [<xref ref-type="bibr" rid="pcbi.1005497.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1005497.ref062">62</xref>], and to other stimulus-evoked responses in other brain areas (see [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>] for discussion). Consequently, our use of linear Fisher information in place of other information measures is not a serious limitation.</p>
<p>For encoded sensory information to be useful, it must propagate from the periphery to the deep brain structures that guide behavior. Consequently, information should be encoded in a manner that is robust against corruption that arises during propagation. We showed that the features of population codes that maximize robustness can be substantially different from those that maximize the information content in peripheral layers. Moreover, by elucidating the set of covariances structures that optimize information transmission, we found that redundancy in a population code is neither necessary, nor sufficient, to guarantee robust propagation. In future work, it will be important to determine whether the nervous system uses the class of population codes that maximize information transmission.</p>
<p>Finally, while our main focus was on information propagation, the model we used—linear feedforward weights followed by a nonlinearity—is known to have powerful computational properties [<xref ref-type="bibr" rid="pcbi.1005497.ref046">46</xref>]. It is, in fact, the basic unit in many deep neural networks. Thus, our main conclusion, which is that differential correlations are typically optimal, applies to any computation that can be performed by this architecture.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<p>Here we provide detailed analysis of the relationship between correlations, feedforward weights, and information propagation. Our methods are organized into sections as follows,</p>
<list list-type="bullet">
<list-item>
<p>“Information in the output layer”: we derive an expression for the information in the output layer (<xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>).</p>
</list-item>
<list-item>
<p>“Identifying the family of optimal covariance matrices”: we identify the optimal family of first layer covariance structures when the gain function is linear.</p>
</list-item>
<list-item>
<p>“Nonlinear gain functions”</p>
</list-item>
<list-item>
<p>“Analysis behind the geometry of information loss”</p>
</list-item>
<list-item>
<p>“Minimum information”: we prove that differential correlations minimize information.</p>
</list-item>
<list-item>
<p>“Variances of neural responses, and robustness to added noise, for different coding strategies”</p>
</list-item>
<list-item>
<p>“Information in a population with a rank 1 perturbation to the covariance matrix”: we compute information for a noise structure consisting of an arbitrary covariance matrix plus a rank 1 covariance matrix.</p>
</list-item>
<list-item>
<p>“Details for numerical examples”</p>
</list-item>
</list>
<sec id="sec010">
<title>Information in the output layer</title>
<p>Our analysis focuses on information loss through one layer of circuitry; to compute the loss, we need expressions for the linear Fisher information in the first and second layers. Expressions for those two quantities are given in Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e003">(3a)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e004">(3b)</xref>. The first is standard; here we derive the second.</p>
<p>To make the result as general as possible, we include noise inside the nonlinearity as well as outside it; if nothing else, that’s probably a reasonable model for the spiking nonlinearity given in <xref ref-type="disp-formula" rid="pcbi.1005497.e018">Eq (13)</xref>. We thus generalize slightly <xref ref-type="disp-formula" rid="pcbi.1005497.e002">Eq (2)</xref>, and write
<disp-formula id="pcbi.1005497.e023"><alternatives><graphic id="pcbi.1005497.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">ζ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where <bold><italic>ζ</italic></bold> is zero mean noise with covariance matrix <bold>Σ</bold><sub><italic>ζ</italic></sub>, and here and in what follows we use the convention that <italic>g</italic> is a pointwise nonlinearity, so for any vector <bold>v</bold>, the <italic>i</italic><sup>th</sup> element of <italic>g</italic>(<bold>v</bold>) is <italic>g</italic>(<italic>v</italic><sub><italic>i</italic></sub>). When <bold>Σ</bold><sub><italic>ζ</italic></sub> = 0, we recover exactly the model in <xref ref-type="disp-formula" rid="pcbi.1005497.e002">Eq (2)</xref>.</p>
<p>Using <xref ref-type="disp-formula" rid="pcbi.1005497.e001">Eq (1)</xref> for <bold>x</bold>, <xref ref-type="disp-formula" rid="pcbi.1005497.e023">Eq (14)</xref> becomes
<disp-formula id="pcbi.1005497.e024"><alternatives><graphic id="pcbi.1005497.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">ζ</mml:mi></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where, recall, <bold><italic>ξ</italic></bold> and <bold><italic>η</italic></bold> are zero mean noise with covariance matrices <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> and <bold>Σ</bold><sub><italic>η</italic></sub>, respectively, and <bold>h</bold>(<italic>s</italic>) is the mean drive to neuron <italic>i</italic>,
<disp-formula id="pcbi.1005497.e025"><alternatives><graphic id="pcbi.1005497.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>≡</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">f</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>To compute the linear Fisher information in the second layer, we start with the usual expression,
<disp-formula id="pcbi.1005497.e026"><alternatives><graphic id="pcbi.1005497.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mtext>E</mml:mtext> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mtext>E</mml:mtext> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where E and Cov denote mean and covariance, respectively. The mean value of <bold>y</bold> given <italic>s</italic> is, via <xref ref-type="disp-formula" rid="pcbi.1005497.e024">Eq (15)</xref>),
<disp-formula id="pcbi.1005497.e027"><alternatives><graphic id="pcbi.1005497.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ζ</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ζ</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>≡</mml:mo> <mml:mover><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>Like <italic>g</italic>(⋅), <inline-formula id="pcbi.1005497.e028"><alternatives><graphic id="pcbi.1005497.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is a pointwise nonlinearity. To compute the covariance, we assume, as in the main text, that <bold><italic>ξ</italic></bold> and <bold><italic>η</italic></bold> are independent; in addition, we assume that both are independent of <bold><italic>ζ</italic></bold>. Thus, the covariance of <inline-formula id="pcbi.1005497.e029"><alternatives><graphic id="pcbi.1005497.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi mathvariant="bold">y</mml:mi></mml:math></alternatives></inline-formula> is the sum of the covariances of the first and second terms in <xref ref-type="disp-formula" rid="pcbi.1005497.e024">Eq (15)</xref>. The covariance of the second term is just <bold>Σ</bold><sub><bold><italic>η</italic></bold></sub>. The covariance of the first term is harder. To make progress, we start by implicitly defining the quantity <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub>(<italic>s</italic>) via
<disp-formula id="pcbi.1005497.e030"><alternatives><graphic id="pcbi.1005497.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ζ</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>≡</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ζ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
where <bold>W</bold><sub>eff</sub>(<italic>s</italic>) is the actual feedforward weight multiplied by the average slope of <italic>g</italic>,
<disp-formula id="pcbi.1005497.e031"><alternatives><graphic id="pcbi.1005497.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
and <inline-formula id="pcbi.1005497.e032"><alternatives><graphic id="pcbi.1005497.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the a diagonal matrix with entries corresponding to the average slope of <italic>g</italic>,
<disp-formula id="pcbi.1005497.e033"><alternatives><graphic id="pcbi.1005497.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>g</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>As in the main text, <italic>δ</italic><sub><italic>ij</italic></sub> is the Kronecker delta and a prime denotes a derivative. The above implicit definition of <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> is motivated by the observation that when <italic>g</italic> is linear, <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> vanishes. Below, in Sec., we show that if <bold><italic>ξ</italic></bold> is Gaussian, <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> is positive semi-definite. Here we assume that the noise is sufficiently close to Gaussian that <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> remains positive semi-definite, and thus can be treated as the covariance matrix of an effective noise source. This last assumption is needed below, in the section titled “Nonlinear gain functions”, where we argue that information loss is small when <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> is small (see text following <xref ref-type="disp-formula" rid="pcbi.1005497.e099">Eq (64)</xref>).</p>
<p>Making the additional definition
<disp-formula id="pcbi.1005497.e034"><alternatives><graphic id="pcbi.1005497.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ζ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
and using Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e024">(15)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e030">(19)</xref> and the fact that <bold><italic>η</italic></bold> is independent of both <bold><italic>ξ</italic></bold> and <italic>ζ</italic>, we see that
<disp-formula id="pcbi.1005497.e035"><alternatives><graphic id="pcbi.1005497.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula></p>
<p>Combining this with the expression for the mean value of <bold>y</bold>, <xref ref-type="disp-formula" rid="pcbi.1005497.e027">Eq (18)</xref>, the linear Fisher information, <xref ref-type="disp-formula" rid="pcbi.1005497.e026">Eq (17)</xref> becomes
<disp-formula id="pcbi.1005497.e036"><alternatives><graphic id="pcbi.1005497.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow></mml:msub></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
where we used Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e025">(16)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e031">(20)</xref> to replace <italic>∂<sub>s</sub></italic>E(y|s) with <bold>W</bold><sub>eff</sub> · <bold>f</bold>′ and, to reduce clutter, we have suppresed any dependence on <italic>s</italic>. To pull the effective weights inside the inverse, we use the Woodbury matrix identity to write
<disp-formula id="pcbi.1005497.e037"><alternatives><graphic id="pcbi.1005497.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mtext>        </mml:mtext> <mml:mo>=</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ξ</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula></p>
<p>Then, using the fact that [<bold>A</bold> + <bold>B</bold>]<sup>−1</sup> = <bold>A</bold><sup>−1</sup> · [<bold>A</bold><sup>−1</sup> + <bold>B</bold><sup>−1</sup>]<sup>−1</sup> · <bold>B</bold><sup>−1</sup>, and applying a very small amount of algebra, this becomes
<disp-formula id="pcbi.1005497.e038"><alternatives><graphic id="pcbi.1005497.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mtext>     </mml:mtext> <mml:mo>=</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle> <mml:mo>−</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula>
where <bold>I</bold> is the identity matrix. It is then straightforward to show that
<disp-formula id="pcbi.1005497.e039"><alternatives><graphic id="pcbi.1005497.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow></mml:msub></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula></p>
<p>Inserting this into <xref ref-type="disp-formula" rid="pcbi.1005497.e036">Eq (24)</xref>, we see that the right hand side of that equation is equal to the expression given in <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref> of the main text.</p>
<sec id="sec011">
<title><italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> is positive semi-definite for Gaussian noise</title>
<p>To show that <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> (defined implicitly in <xref ref-type="disp-formula" rid="pcbi.1005497.e030">Eq (19)</xref>), is positive semi-definite for Gaussian noise, we’ll show that it can be written as a covariance. To simplify the analysis, we make the definition
<disp-formula id="pcbi.1005497.e040"><alternatives><graphic id="pcbi.1005497.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>≡</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ζ</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula></p>
<p>With this definition,
<disp-formula id="pcbi.1005497.e041"><alternatives><graphic id="pcbi.1005497.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>δ</mml:mi> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>g</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>-</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula>
where here and in what follows we are suppressing the dependence on <italic>s</italic>, <bold>Σ</bold><sub><italic>χ</italic></sub> is the covariance matrix of <italic>χ</italic>, and <inline-formula id="pcbi.1005497.e042"><alternatives><graphic id="pcbi.1005497.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup></mml:math></alternatives></inline-formula> is defined in <xref ref-type="disp-formula" rid="pcbi.1005497.e033">Eq (21)</xref>. Because we are assuming that both <bold><italic>ξ</italic></bold> and <bold><italic>ζ</italic></bold> are Gaussian, <bold><italic>χ</italic></bold> is also Gaussian.</p>
<p>We’ll show now that <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> is equal to the covariance of the function <inline-formula id="pcbi.1005497.e043"><alternatives><graphic id="pcbi.1005497.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. We start by noting that
<disp-formula id="pcbi.1005497.e044"><alternatives><graphic id="pcbi.1005497.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mfenced></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:mo>[</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mtext>Cov</mml:mtext></mml:mrow> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mfenced> <mml:mo>+</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula></p>
<p>We’ll focus on the second term, which is given explicitly by
<disp-formula id="pcbi.1005497.e045"><alternatives><graphic id="pcbi.1005497.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula></p>
<p>When <italic>P</italic>(<bold><italic>χ</italic></bold>) is Gaussian,
<disp-formula id="pcbi.1005497.e046"><alternatives><graphic id="pcbi.1005497.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e046" xlink:type="simple"/><mml:math display="block" id="M46"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula></p>
<p>Inserting this into <xref ref-type="disp-formula" rid="pcbi.1005497.e045">Eq (31)</xref> and integrating by parts, we arrive at
<disp-formula id="pcbi.1005497.e047"><alternatives><graphic id="pcbi.1005497.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula></p>
<p>Using the fact that <italic>∂</italic><bold><italic><sub>χ</sub></italic></bold> <italic>g</italic>(<bold>h</bold> + <bold><italic>χ</italic></bold>) = <italic>∂</italic> <bold><sub>h</sub></bold> <italic>g</italic>(<bold>h</bold> + <bold><italic>χ</italic></bold>), the above expression becomes
<disp-formula id="pcbi.1005497.e048"><alternatives><graphic id="pcbi.1005497.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mfenced close="]" open="[" separators=""><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">h</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">χ</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula>
where the second equality follows from the definition of <inline-formula id="pcbi.1005497.e049"><alternatives><graphic id="pcbi.1005497.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1005497.e033">Eq (21)</xref>). Inserting this into <xref ref-type="disp-formula" rid="pcbi.1005497.e044">Eq (30)</xref>, we see that the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005497.e044">Eq (30)</xref> is exactly equal to the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005497.e041">Eq (29)</xref>. Thus, <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> can be written as a covariance, and so it must be positive semi-definite.</p>
</sec>
</sec>
<sec id="sec012">
<title>Identifying the family of optimal covariance matrices</title>
<p>Here we address the question: what noise covariance matrix optimizes information transmission? In other words, what covariance matrix <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> maximizes the information given in <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>? That is hard to answer when <italic>g</italic> is nonlinear, because in that case <bold>Σ</bold><sub>eff,<italic>η</italic></sub> depends on <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> via <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> (see Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e030">(19)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e034">(22)</xref>). In this section, then, we consider linear gain functions; in the next we consider nonlinear ones. To make our expressions more readable, we generally suppress the dependence on <italic>s</italic>.</p>
<p>Our goal is to maximize <italic>I</italic><sub><italic>y</italic></sub> with <italic>I</italic><sub><italic>x</italic></sub> fixed. Using the definition of <bold>Σ</bold><sub><italic>y</italic></sub> given in <xref ref-type="disp-formula" rid="pcbi.1005497.e013">Eq (8)</xref>, for linear gain functions the information in the second layer (<xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>) is written
<disp-formula id="pcbi.1005497.e050"><alternatives><graphic id="pcbi.1005497.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula></p>
<p>We use Lagrange multipliers,
<disp-formula id="pcbi.1005497.e051"><alternatives><graphic id="pcbi.1005497.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mfenced close="]" open="[" separators=""><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula>
where <italic>λ</italic> is a Lagrange multiplier that enforces the constraint <inline-formula id="pcbi.1005497.e052"><alternatives><graphic id="pcbi.1005497.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Taking the derivative and setting it to zero yields
<disp-formula id="pcbi.1005497.e053"><alternatives><graphic id="pcbi.1005497.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>λ</mml:mi> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula></p>
<p>In deriving this expression we used the fact that the gain functions are linear, which implies that <bold>Σ</bold><sub><italic>y</italic></sub> does not depend on <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>. Multiplying by <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> + <bold>Σ</bold><sub><italic>y</italic></sub> on both the left and right, we arrive at
<disp-formula id="pcbi.1005497.e054"><alternatives><graphic id="pcbi.1005497.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mi>λ</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula></p>
<p>This is satisfied when
<disp-formula id="pcbi.1005497.e055"><alternatives><graphic id="pcbi.1005497.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∝</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(39)</label></disp-formula></p>
<p>There are two ways this can happen,
<disp-formula id="pcbi.1005497.e056"><alternatives><graphic id="pcbi.1005497.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>∝</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle></mml:math></alternatives> <label>(40a)</label></disp-formula> <disp-formula id="pcbi.1005497.e057"><alternatives><graphic id="pcbi.1005497.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>∝</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi>a</mml:mi></mml:mstyle></mml:math></alternatives> <label>(40b)</label></disp-formula>
where <bold>a</bold> is an arbitrary vector. Combining these linearly, taking into account that <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> is a covariance matrix and thus symmetric, and enforcing equality in <xref ref-type="disp-formula" rid="pcbi.1005497.e054">Eq (38)</xref>, we arrive at
<disp-formula id="pcbi.1005497.e058"><alternatives><graphic id="pcbi.1005497.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>Ω</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(41)</label></disp-formula>
where <italic>I</italic><sub><bold><italic>η</italic></bold></sub> is the information the output layer would have if there was no noise in the input layer,
<disp-formula id="pcbi.1005497.e059"><alternatives><graphic id="pcbi.1005497.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(42)</label></disp-formula>
(this is the same expression as in <xref ref-type="disp-formula" rid="pcbi.1005497.e014">Eq (9)</xref>, it’s repeated here for convenience), Ω is an arbitrary symmetric matrix, <bold>P</bold> is a projection operator, chosen so that <bold>P</bold> · <bold>f</bold>′ = 0,
<disp-formula id="pcbi.1005497.e060"><alternatives><graphic id="pcbi.1005497.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi> <mml:mo>≡</mml:mo> <mml:mi mathvariant="bold">I</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(43)</label></disp-formula>
and <italic>α</italic> is arbitrary (but subject to the constraint that <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> has no negative eigenvalues). Note that <bold>P</bold> is a linear combination of the right hand sides of Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e056">(40a)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e057">(40b)</xref>), with <bold>a</bold> = <bold>f</bold>′ in the latter equation. It is straightforward to verify that when <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> is given by Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e058">(41)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e054">(38)</xref> is satisfied.</p>
<p>To find an explicit expression for <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>, not just its inverse, we apply the Woodbury matrix identity to <xref ref-type="disp-formula" rid="pcbi.1005497.e058">Eq (41)</xref>; that gives us
<disp-formula id="pcbi.1005497.e061"><alternatives><graphic id="pcbi.1005497.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e061" xlink:type="simple"/><mml:math display="block" id="M61"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>α</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>α</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mo mathvariant="bold">Ω</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>α</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(44)</label></disp-formula>
where
<disp-formula id="pcbi.1005497.e062"><alternatives><graphic id="pcbi.1005497.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e062" xlink:type="simple"/><mml:math display="block" id="M62"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>α</mml:mi></mml:msub> <mml:mo>≡</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(45)</label></disp-formula></p>
<p>Inserting this into <xref ref-type="disp-formula" rid="pcbi.1005497.e061">Eq (44)</xref>, we arrive at
<disp-formula id="pcbi.1005497.e063"><alternatives><graphic id="pcbi.1005497.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mo>-</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mo mathvariant="bold">Ω</mml:mo> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">P</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(46)</label></disp-formula></p>
<p>This is the same as <xref ref-type="disp-formula" rid="pcbi.1005497.e012">Eq (7)</xref> in the main text, except in that equation we let <bold>Ω</bold> go to ∞, so we ignore the projection-related term. Ignoring that term is reasonable, as it just puts noise in a direction perpendicular to <bold>f</bold>′, and so has no effect on the information.</p>
<p>By choosing different scalars <italic>α</italic> and matrices <bold>Ω</bold>, a family of optimal <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> is obtained. These all have the same input information, <italic>I</italic><sub><italic>x</italic></sub>, and the same output information, <italic>I</italic><sub><italic>y</italic></sub>, after corruption by noise. An especially interesting covariance matrix is found in the limit <italic>α</italic> = 0, in which case
<disp-formula id="pcbi.1005497.e064"><alternatives><graphic id="pcbi.1005497.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e064" xlink:type="simple"/><mml:math display="block" id="M64"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(47)</label></disp-formula></p>
<p>These are so-called differential correlations [<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>]. Importantly, the choice <italic>α</italic> = 0 is the only one for which the optimal correlational structure is independent of the correlations in the output layer, <bold>Σ</bold><sub><italic>y</italic></sub>. Note that pure differential correlations don’t satisfy <xref ref-type="disp-formula" rid="pcbi.1005497.e055">Eq (39)</xref>. As such, they represent a singular limit, in the sense that <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> in <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref> satisfies <xref ref-type="disp-formula" rid="pcbi.1005497.e055">Eq (39)</xref> with alpha arbitrarily small, but not precisely zero.</p>
<p>The other covariance that we highlight in the text is found for <italic>α</italic> = 1 and <bold>Ω</bold> → ∞, in which case <inline-formula id="pcbi.1005497.e065"><alternatives><graphic id="pcbi.1005497.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. This is the matched covariance case.</p>
</sec>
<sec id="sec013">
<title>Nonlinear gain functions</title>
<p>We now focus on differential correlations, and determine conditions under which they are optimal for information propagation when the gain function, <italic>g</italic>(⋅), is nonlinear. In this regime, the effective noise in the second layer (the second term in brackets in <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>) depends on <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>. This greatly complicates the analysis, and to make headway we need to reformulate our mathematical description of differential correlations. This reformulation is based on the observation that differential correlations correspond to trial-to-trial variability in the value of the stimulus, <italic>s</italic>[<xref ref-type="bibr" rid="pcbi.1005497.ref022">22</xref>]. Consequently, the encoding model in the input layer can be written as a multi-step process,
<disp-formula id="pcbi.1005497.e066"><alternatives><graphic id="pcbi.1005497.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:math></alternatives> <label>(48a)</label></disp-formula> <disp-formula id="pcbi.1005497.e067"><alternatives><graphic id="pcbi.1005497.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e067" xlink:type="simple"/><mml:math display="block" id="M67"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle> <mml:mo>=</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>f</mml:mi></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(48b)</label></disp-formula> <disp-formula id="pcbi.1005497.e068"><alternatives><graphic id="pcbi.1005497.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>y</mml:mi></mml:mstyle> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>W</mml:mi></mml:mstyle> <mml:mo>·</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ζ</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">η</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(48c)</label></disp-formula></p>
<p>Here <italic>s</italic><sub>0</sub> is the value of the stimulus that is actually presented. However, the neurons in the input layer, <bold>x</bold>, encode <italic>s</italic>—a corrupted version of <italic>s</italic><sub>0</sub>. This is indicated by <xref ref-type="disp-formula" rid="pcbi.1005497.e066">Eq (48a)</xref>, which tells us that <italic>s</italic> deviates on a trial-to-trial basis from <italic>s</italic><sub>0</sub>, with deviations that are described by a zero-mean random variable, <italic>δ</italic><sub><italic>s</italic></sub>.</p>
<p>To see that this model does indeed exhibit differential correlations, we Taylor expand <xref ref-type="disp-formula" rid="pcbi.1005497.e067">Eq (48b)</xref> around <italic>s</italic><sub>0</sub>, yielding a model of the form
<disp-formula id="pcbi.1005497.e069"><alternatives><graphic id="pcbi.1005497.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e069" xlink:type="simple"/><mml:math display="block" id="M69"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>≈</mml:mo> <mml:mi mathvariant="bold">f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(49)</label></disp-formula>
for which the covariance matrix is
<disp-formula id="pcbi.1005497.e070"><alternatives><graphic id="pcbi.1005497.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e070" xlink:type="simple"/><mml:math display="block" id="M70"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(50)</label></disp-formula></p>
<p>The first term corresponds to differential correlations.</p>
<p>Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e067">(48b)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e068">(48c)</xref> correspond exactly to our previous model (<xref ref-type="disp-formula" rid="pcbi.1005497.e008">Eq (4a)</xref>). Consequently, the information about <italic>s</italic> in the first and second layers are still given by Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e003">(3a)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e004">(3b)</xref> of the main text. However, we can’t use those equations for the information about <italic>s</italic><sub>0</sub>. For that, we focus on the variance of its optimal estimator given <bold>x</bold>, which we denote <inline-formula id="pcbi.1005497.e071"><alternatives><graphic id="pcbi.1005497.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. Because of the Markov structure of our model (<italic>s</italic><sub>0</sub> ↔ <italic>s</italic> ↔ <italic>x</italic>), we can construct <inline-formula id="pcbi.1005497.e072"><alternatives><graphic id="pcbi.1005497.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula> by first considering the optimal estimator of <italic>s</italic><sub>0</sub> given <italic>s</italic>, and then the optimal estimator of <italic>s</italic> given <bold>x</bold>. The variance of <inline-formula id="pcbi.1005497.e073"><alternatives><graphic id="pcbi.1005497.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula> given <bold>x</bold> is then simply the sum of the variances of these two (independent) noise sources.</p>
<p>The optimal estimator of <italic>s</italic><sub>0</sub> given <italic>s</italic> is simply <italic>s</italic>, with conditional variance <inline-formula id="pcbi.1005497.e074"><alternatives><graphic id="pcbi.1005497.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The optimal estimator of <italic>s</italic> given <bold>x</bold> is <inline-formula id="pcbi.1005497.e075"><alternatives><graphic id="pcbi.1005497.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, with variance <inline-formula id="pcbi.1005497.e076"><alternatives><graphic id="pcbi.1005497.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:mrow><mml:mtext>Var</mml:mtext> <mml:mo>[</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Consequently,
<disp-formula id="pcbi.1005497.e077"><alternatives><graphic id="pcbi.1005497.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>0</mml:mn></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(51)</label></disp-formula></p>
<p>As usual, we approximate the variance of <inline-formula id="pcbi.1005497.e078"><alternatives><graphic id="pcbi.1005497.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> given <italic>s</italic> by the linear Fisher information, yielding an approximation for the total Fisher information about <italic>s</italic><sub>0</sub> given <bold>x</bold>,
<disp-formula id="pcbi.1005497.e079"><alternatives><graphic id="pcbi.1005497.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e079" xlink:type="simple"/><mml:math display="block" id="M79"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(52)</label></disp-formula></p>
<p>Similarly, the Fisher information about <italic>s</italic><sub>0</sub> given <bold>y</bold> is approximated by
<disp-formula id="pcbi.1005497.e080"><alternatives><graphic id="pcbi.1005497.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e080" xlink:type="simple"/><mml:math display="block" id="M80"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(53)</label></disp-formula></p>
<p>Note that we are slightly abusing notation here: above, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) and <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>) referred to the total information about the stimulus; now they refer to the information about the stimulus that is encoded in the first layer, which is different from the actual stimulus, <italic>s</italic><sub>0</sub>. However, it is a convenient abuse, as it allows us to take over our previous results without introducing much new notation.</p>
<p>Our first step is to parametrize the covariance matrix, <bold><italic>ξ</italic></bold>, and Var[<italic>δ</italic><sub><italic>s</italic></sub>], in a way that ensures that the information in the first layer <inline-formula id="pcbi.1005497.e081"><alternatives><graphic id="pcbi.1005497.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> remains fixed while we vary <bold><italic>ξ</italic></bold> and Var[<italic>δ</italic><sub><italic>s</italic></sub>]. A convenient choice is
<disp-formula id="pcbi.1005497.e082"><alternatives><graphic id="pcbi.1005497.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e082" xlink:type="simple"/><mml:math display="block" id="M82"><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mtext> </mml:mtext> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mfrac><mml:mrow><mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mstyle></mml:math></alternatives> <label>(54a)</label></disp-formula> <disp-formula id="pcbi.1005497.e083"><alternatives><graphic id="pcbi.1005497.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e083" xlink:type="simple"/><mml:math display="block" id="M83"><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(54b)</label></disp-formula>
where
<disp-formula id="pcbi.1005497.e084"><alternatives><graphic id="pcbi.1005497.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e084" xlink:type="simple"/><mml:math display="block" id="M84"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(55)</label></disp-formula></p>
<p>Inserting <xref ref-type="disp-formula" rid="pcbi.1005497.e082">Eq (54)</xref> into <xref ref-type="disp-formula" rid="pcbi.1005497.e079">Eq (52)</xref>, we see that <inline-formula id="pcbi.1005497.e085"><alternatives><graphic id="pcbi.1005497.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e085" xlink:type="simple"/><mml:math display="inline" id="M85"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, independent of <bold>Σ</bold><sub>0</sub>(<italic>s</italic>).</p>
<p>The information in the second layer about <italic>s</italic>, <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), is given by <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>, with <bold>Σ</bold><sub>eff,<italic>η</italic></sub> given in <xref ref-type="disp-formula" rid="pcbi.1005497.e034">Eq (22)</xref>. It is convenient to make the definition
<disp-formula id="pcbi.1005497.e086"><alternatives><graphic id="pcbi.1005497.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e086" xlink:type="simple"/><mml:math display="block" id="M86"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mo>≡</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(56)</label></disp-formula></p>
<p>This is the analog of <xref ref-type="disp-formula" rid="pcbi.1005497.e013">Eq (8)</xref>, but for nonlinear gain functions. It is clear from Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e034">(22)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e030">(19)</xref> that <bold>Σ</bold><sub>eff,<italic>η</italic></sub> depends on <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>; consequently, it depends on <italic>ϵ</italic>.</p>
<p>To maximize information with respect to <italic>ϵ</italic>, we take a two step approach. We write
<disp-formula id="pcbi.1005497.e087"><alternatives><graphic id="pcbi.1005497.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e087" xlink:type="simple"/><mml:math display="block" id="M87"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mrow><mml:mo>′</mml:mo> <mml:mi>T</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(57)</label></disp-formula></p>
<p>Here <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic>) and <bold>Σ</bold><sub><italic>y</italic></sub>(<italic>s</italic>,<italic>ϵ</italic><sub>0</sub>) are the same as in Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e083">(54b)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e086">(56)</xref>; we have just made the dependence on <italic>ϵ</italic> explicit. The two steps are to maximize first with respect to <italic>ϵ</italic>, then with respect to <italic>ϵ</italic><sub>0</sub>. If the two maxima occurr in the same place, then we have identified the covariance structure that optimizes information transmission.</p>
<p>In the first step we differentiate <inline-formula id="pcbi.1005497.e088"><alternatives><graphic id="pcbi.1005497.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with respect to <italic>ϵ</italic>. To simplify the expressions, we make the definition
<disp-formula id="pcbi.1005497.e089"><alternatives><graphic id="pcbi.1005497.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e089" xlink:type="simple"/><mml:math display="block" id="M89"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mtext>tot</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>eff</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(58)</label></disp-formula></p>
<p>Combining Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e080">(53)</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e082">(54)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e087">(57)</xref>, we have
<disp-formula id="pcbi.1005497.e090"><alternatives><graphic id="pcbi.1005497.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e090" xlink:type="simple"/><mml:math display="block" id="M90"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>ϵ</mml:mi></mml:mrow></mml:mfrac> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfenced> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mtext>tot</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>ϵ</mml:mi></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mtext>tot</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(59)</label></disp-formula>
where we used the fact that for any square matrix <bold>A</bold>(<italic>x</italic>), (<italic>d/dx</italic>)<bold>A</bold><sup>−1</sup> = −<bold>A</bold><sup>−1</sup> · <italic>d</italic> <bold>A</bold>/<italic>dx</italic> · <bold>A</bold><sup>−1</sup>, and we suppressed much of the <italic>s</italic>, <italic>ϵ</italic> and <italic>ϵ</italic><sub>0</sub> dependence for clarity. Using <xref ref-type="disp-formula" rid="pcbi.1005497.e083">Eq (54b)</xref> for <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic>), the derivative with respect to <italic>ϵ</italic> in the second term is straightforward,
<disp-formula id="pcbi.1005497.e091"><alternatives><graphic id="pcbi.1005497.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e091" xlink:type="simple"/><mml:math display="block" id="M91"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo> <mml:mrow><mml:mo>∂</mml:mo> <mml:mi>ϵ</mml:mi></mml:mrow></mml:mfrac> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mtext> </mml:mtext> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>−</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msubsup> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle></mml:mrow></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mtext> </mml:mtext> <mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mo>−</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mstyle></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(60)</label></disp-formula></p>
<p>Then, applying the definition <inline-formula id="pcbi.1005497.e092"><alternatives><graphic id="pcbi.1005497.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mtext>tot</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (see Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e087">(57)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e089">(58)</xref>), and making the new definition
<disp-formula id="pcbi.1005497.e093"><alternatives><graphic id="pcbi.1005497.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e093" xlink:type="simple"/><mml:math display="block" id="M93"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi> <mml:mo>≡</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mtext>tot</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(61)</label></disp-formula>
we arrive at the expression
<disp-formula id="pcbi.1005497.e094"><alternatives><graphic id="pcbi.1005497.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e094" xlink:type="simple"/><mml:math display="block" id="M94"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>ϵ</mml:mi></mml:mrow></mml:mfrac> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mfrac><mml:msubsup><mml:mi>I</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mrow><mml:mi>y</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mi mathvariant="bold">V</mml:mi> <mml:mo>·</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mfrac><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mrow><mml:mo>′</mml:mo> <mml:mi>T</mml:mi></mml:mrow></mml:msup> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">I</mml:mi></mml:mfenced> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">V</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(62)</label></disp-formula></p>
<p>The right hand side of <xref ref-type="disp-formula" rid="pcbi.1005497.e094">Eq (62)</xref> is negative or zero if the term in brackets is negative semi-definite; that is, if all its eigenvalues are non-positive. Since the term in square brackets is a rank one matrix minus the identity, all but one of its eigenvalues are equal to -1. The remaining eigenvalue is 0, with corresponding eigenvector <inline-formula id="pcbi.1005497.e095"><alternatives><graphic id="pcbi.1005497.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e095" xlink:type="simple"/><mml:math display="inline" id="M95"><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1005497.e084">Eq (55)</xref>). Thus, <inline-formula id="pcbi.1005497.e096"><alternatives><graphic id="pcbi.1005497.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e096" xlink:type="simple"/><mml:math display="inline" id="M96"><mml:mrow><mml:mi>∂</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mi>∂</mml:mi> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>≤</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1005497.e097"><alternatives><graphic id="pcbi.1005497.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> must have a global maximum at <italic>ϵ</italic> = ∞. If <italic>g</italic> is linear, <bold>Σ</bold><sub>eff,<italic>y</italic></sub> doesn’t depend on <italic>ϵ</italic><sub>0</sub>, and <italic>ϵ</italic> = ∞ corresponds to pure differential correlations. We have, therefore, recovered the <italic>α</italic> = 0 limit of <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref>.</p>
<p>When <italic>ϵ</italic> = ∞, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> vanishes, and so the expression for the information in the second layer simplifies considerably. Combining Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e080">(53)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e082">(54a)</xref>, we have, in the <italic>ϵ</italic> → ∞ limit,
<disp-formula id="pcbi.1005497.e098"><alternatives><graphic id="pcbi.1005497.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e098" xlink:type="simple"/><mml:math display="block" id="M98"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mi>y</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>∞</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>x</mml:mi> <mml:mtext>tot</mml:mtext></mml:msubsup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>s</mml:mi> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi>∞</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(63)</label></disp-formula>
where
<disp-formula id="pcbi.1005497.e099"><alternatives><graphic id="pcbi.1005497.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e099" xlink:type="simple"/><mml:math display="block" id="M99"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:mi>∞</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>ζ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mtext>eff</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(64)</label></disp-formula></p>
<p>The latter equation follows by combining the fact that <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>, ∞) = 0 (<xref ref-type="disp-formula" rid="pcbi.1005497.e083">Eq (54b)</xref>) with the definitions of <bold>Σ</bold><sub>eff,<italic>y</italic></sub> and <bold>Σ</bold><sub>eff,<italic>η</italic></sub> (Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e086">(56)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e034">(22)</xref>, respectively).</p>
<p>The total information in the output layer is maximized when <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic><sub>0</sub>; ∞,<italic>ϵ</italic><sub>0</sub>) is maximized. That quantity depends on <italic>ϵ</italic><sub>0</sub> via <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic><sub>0</sub>), the noise covariance in the input layer. As can be seen from <xref ref-type="disp-formula" rid="pcbi.1005497.e083">Eq (54b)</xref>, larger <italic>ϵ</italic><sub>0</sub> implies smaller <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic><sub>0</sub>). That has two effects. First, when <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic><sub>0</sub>) is small enough, the covariance matrix <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> becomes small (see <xref ref-type="disp-formula" rid="pcbi.1005497.e030">Eq (19)</xref>, and note that <italic>δ</italic><bold>Σ</bold><sub><italic>g</italic></sub> is positive definite, as shown in Sec.). This tends to increase <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>). However, the effective tuning curves, <bold>W</bold><sub>eff</sub>(<italic>s</italic>;<italic>ϵ</italic>) · <bold>f</bold>(<italic>s</italic>), also depend on <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic><sub>0</sub>) (see <xref ref-type="disp-formula" rid="pcbi.1005497.e031">Eq (20)</xref>). It is possible that increasing <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>(<italic>s</italic>,<italic>ϵ</italic><sub>0</sub>) modifes the tuning curves such that <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>) increases. Consequently, it is impossible to make completely general statements.</p>
<p>Nevertheless, we can identify two regimes. First, if there is no added noise in the output layer (<bold><italic>η</italic></bold> = <bold><italic>ζ</italic></bold> = 0), then <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>; ∞, <italic>ϵ</italic>) goes to ∞ as <italic>ϵ</italic><sub>0</sub> goes to ∞, thus maximizing the total information. This holds, however, only if the tuning curves are sufficiently dense relative to the steepness of the tuning curves; otherwise, the Fisher information is no longer a good approximation to the true information. For smooth tuning curves this is generally satisfied, but it is not satisfied for the noise-free spike generating mechanism we consider in the main text (<xref ref-type="disp-formula" rid="pcbi.1005497.e016">Eq (11)</xref>), since for that nonlinearity <bold>f</bold>′(<italic>s</italic>) = 0 with probability 1. We expect, though, that in the absence of noise, this particular nonlinearity introduces an error that is <inline-formula id="pcbi.1005497.e100"><alternatives><graphic id="pcbi.1005497.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, implying that <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>; ∞, <italic>ϵ</italic>) ∝ <italic>n</italic><sup>2</sup>. Numerical simulations corroborated this scaling. Thus, for sufficiently large populations, differential correlations are optimal for the noise-free spike-generating nonlinearity. Note, though, that the thresholds must be chosen so that there are always both active and silent neurons; otherwise, in the limit that <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> vanishes, the activity will contain no information at all about the stimulus.</p>
<p>The second regime is one in which the tuning curves have been optimized. In this case, modifying the tuning curves by adding noise decreases information, and again differential correlations optimize information transmission.</p>
<p>To summarize, we have analyzed the scenario considered in the main text (section titled “Nonlinear gain functions”)—namely, the neural activities at the second layer, <bold>y</bold>, are given by a nonlinear function of the neural activities at the first layer, <bold>x</bold>, with noise added both before and after the nonlinearity. In this case, whether or not differential correlations in the first layer optimize information transmission depends on the details. They do if <italic>g</italic> is linear, the tuning curves are optimal, or there is no added noise in the second layer and the tuning curves are sufficiently dense relative to the steepness of the tuning curves. If none of these are satisfied, however, differential correlations may be sub-optimal.</p>
</sec>
<sec id="sec014">
<title>Analysis behind the geometry of information loss</title>
<p>Our goal in this section is to make more rigorous the geometrical arguments in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>. We start with the observation that, for Gaussian distributed neural responses, the 1 standard-deviation probability contours for the responses in the first layer (magenta ellipses in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>) are defined by
<disp-formula id="pcbi.1005497.e101"><alternatives><graphic id="pcbi.1005497.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e101" xlink:type="simple"/><mml:math display="block" id="M101"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(65)</label></disp-formula>
where Δ<bold>r</bold> ≡ <bold>f</bold>(<italic>s</italic>) − <bold>r</bold> represents fluctuations around the mean response to stimulus <italic>s</italic>. In two dimensions, which we’ll focus on here, <xref ref-type="disp-formula" rid="pcbi.1005497.e101">Eq (65)</xref> becomes
<disp-formula id="pcbi.1005497.e102"><alternatives><graphic id="pcbi.1005497.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e102" xlink:type="simple"/><mml:math display="block" id="M102"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>r</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>r</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(66)</label></disp-formula>
where <italic>σ</italic><sub>1</sub> and <italic>σ</italic><sub>2</sub> are the lengths of the principal axes of the covariance ellipse (so <inline-formula id="pcbi.1005497.e103"><alternatives><graphic id="pcbi.1005497.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005497.e104"><alternatives><graphic id="pcbi.1005497.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> are the eigenvalues of <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>) and Δ<italic>r</italic><sub>1</sub> and Δ<italic>r</italic><sub>2</sub> are distances spanned by the magenta ellipses along those axes.</p>
<p>As shown in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>, the intersection between the magenta ellipse (the one defined in <xref ref-type="disp-formula" rid="pcbi.1005497.e102">Eq (66)</xref>) and the signal curve tells us the uncertainty in the value of the stimulus. To quantify this uncertainty, we simply set Δ<bold>r</bold> to <bold>f</bold>′(<italic>s</italic>)Δ<italic>s<sub>x</sub></italic> (the subscript <italic>x</italic> indicates that this is the uncertainty in the input layer), insert that into <xref ref-type="disp-formula" rid="pcbi.1005497.e102">Eq (66)</xref>, and solve for Δ<italic>s</italic><sub><italic>x</italic></sub>. Defining <italic>θ</italic> to be the angle between <bold>f</bold>′(<italic>s</italic>) and the long principal axis (see <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>, and note that <italic>θ</italic> = 0 in panel B), and letting <italic>σ</italic><sub>1</sub> correspond to the length of the ellipse’s major axis (so <italic>σ</italic><sub>1</sub> &gt; <italic>σ</italic><sub>2</sub>), we have
<disp-formula id="pcbi.1005497.e105"><alternatives><graphic id="pcbi.1005497.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e105" xlink:type="simple"/><mml:math display="block" id="M105"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mfenced close="]" open="[" separators=""><mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">cos</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>θ</mml:mi></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">sin</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>θ</mml:mi></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(67)</label></disp-formula></p>
<p>The left hand side is the linear Fisher information in the first layer [<xref ref-type="bibr" rid="pcbi.1005497.ref010">10</xref>], a fact that is useful primarily because it validates our (relatively informal) derivation. More importantly, we can now see how <italic>iid</italic> noise affects information. The addition of <italic>iid</italic> noise simply increases the eigenvalues by <italic>σ</italic><sup>2</sup>, so the ratio of the information in the output layer to that in the input layer is
<disp-formula id="pcbi.1005497.e106"><alternatives><graphic id="pcbi.1005497.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e106" xlink:type="simple"/><mml:math display="block" id="M106"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">cos</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>θ</mml:mi></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">sin</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>θ</mml:mi></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow> <mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">cos</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>θ</mml:mi></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">sin</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mi>θ</mml:mi></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(68)</label></disp-formula></p>
<p>We can identify two limits. First, if <italic>θ</italic> = 0 (as it is in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3B</xref>), this ratio reduces to
<disp-formula id="pcbi.1005497.e107"><alternatives><graphic id="pcbi.1005497.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e107" xlink:type="simple"/><mml:math display="block" id="M107"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mfenced close="|" open="" separators=""><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mfenced> <mml:mrow><mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(69)</label></disp-formula></p>
<p>Second, if tan <italic>θ</italic> ≫ <italic>σ</italic><sub>2</sub>/<italic>σ</italic><sub>1</sub> (which essentially means the green line in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref> intersects the covariance ellipse on the side, as in panel A, rather than somewhere near the end, as in panel B), the ratio of the informations becomes
<disp-formula id="pcbi.1005497.e108"><alternatives><graphic id="pcbi.1005497.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e108" xlink:type="simple"/><mml:math display="block" id="M108"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mfenced close="|" open="" separators=""><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mfenced> <mml:mrow><mml:mo form="prefix">tan</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>≫</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>/</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub> <mml:mo>≈</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(70)</label></disp-formula></p>
<p>Because <italic>σ</italic><sub>1</sub> &gt; <italic>σ</italic><sub>2</sub>, the information loss is larger in the second case than in the first. And the longer and skinnier the covariance ellipse, the larger the difference in information loss. Thus, this analysis quantifies the geometrical picture given in <xref ref-type="fig" rid="pcbi.1005497.g003">Fig 3</xref>, in which there is larger information loss in panel A (where <italic>θ</italic> &gt; 0) than in panel B (where <italic>θ</italic> = 0).</p>
</sec>
<sec id="sec015">
<title>Minimum information</title>
<p>Here we ask: what correlational structure minimizes linear Fisher information? To answer that, we use the multi-dimensional analog of <xref ref-type="disp-formula" rid="pcbi.1005497.e105">Eq (67)</xref>,
<disp-formula id="pcbi.1005497.e109"><alternatives><graphic id="pcbi.1005497.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e109" xlink:type="simple"/><mml:math display="block" id="M109"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mfrac><mml:mrow><mml:msup><mml:mo form="prefix">cos</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(71)</label></disp-formula>
where <inline-formula id="pcbi.1005497.e110"><alternatives><graphic id="pcbi.1005497.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e110" xlink:type="simple"/><mml:math display="inline" id="M110"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> is the <italic>k</italic><sup>th</sup> eigenvalue of the noise covariance matrix and <italic>θ</italic><sub><italic>k</italic></sub> is the angle between <bold>f</bold>′(<italic>s</italic>) and the <italic>k</italic><sup>th</sup> eigenvector [<xref ref-type="bibr" rid="pcbi.1005497.ref010">10</xref>]. We would like to minimize <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) with respect to the angles, <italic>θ</italic><sub><italic>k</italic></sub>, and the eigenvalues, <inline-formula id="pcbi.1005497.e111"><alternatives><graphic id="pcbi.1005497.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e111" xlink:type="simple"/><mml:math display="inline" id="M111"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. Without constraints, this problem is trivial: information is minimized by having infinite variances for the neural activities. To make the problem better-formulated, we add a constraint that prevents the optimization procedure from simply identifying that trivial solution.</p>
<p>We’ll come to the constraint shortly, but first we’ll minimize information with respect to the angles, <italic>θ</italic><sub><italic>k</italic></sub>. That minimum occurs when the eigenvector corresponding to the largest eigenvalue is parallel to <bold>f</bold>′(<italic>s</italic>); ordering the eigenvalues so that <inline-formula id="pcbi.1005497.e112"><alternatives><graphic id="pcbi.1005497.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e112" xlink:type="simple"/><mml:math display="inline" id="M112"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> is the largest eigenvalue, we have cos <italic>θ</italic><sub>0</sub> = 1 and cos <italic>θ</italic><sub><italic>k</italic> &gt; 0</sub> = 0. Consequently, the information at the minimum is
<disp-formula id="pcbi.1005497.e113"><alternatives><graphic id="pcbi.1005497.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e113" xlink:type="simple"/><mml:math display="block" id="M113"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(72)</label></disp-formula></p>
<p>The next step is to minimize <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) with respect to the eigenvalues, subject to a constraint on the covariance matrix. We consider constraints of the form
<disp-formula id="pcbi.1005497.e114"><alternatives><graphic id="pcbi.1005497.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e114" xlink:type="simple"/><mml:math display="block" id="M114"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(73)</label></disp-formula>
where, to avoid the trivial solution (of infinite neural variances), <italic>C</italic> is an increasing function of each of it’s arguments: for all <italic>k</italic>,
<disp-formula id="pcbi.1005497.e115"><alternatives><graphic id="pcbi.1005497.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e115" xlink:type="simple"/><mml:math display="block" id="M115"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>C</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(74)</label></disp-formula></p>
<p>Examples of <inline-formula id="pcbi.1005497.e116"><alternatives><graphic id="pcbi.1005497.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:mrow><mml:mi>C</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are the trace of the covariance matrix (the sum of the eigenvalues) and the Frobenius norm (the square root of the sum of the squares of the eigenvalues).</p>
<p>Because of <xref ref-type="disp-formula" rid="pcbi.1005497.e115">Eq (74)</xref>, the information, <xref ref-type="disp-formula" rid="pcbi.1005497.e113">Eq (72)</xref>, is minimized and the constraint, <xref ref-type="disp-formula" rid="pcbi.1005497.e114">Eq (73)</xref>, is satisfied when all the eigenvalues except <inline-formula id="pcbi.1005497.e117"><alternatives><graphic id="pcbi.1005497.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> are zero. At this global minimum, the covariance matrix, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>, displays purely differential correlations,
<disp-formula id="pcbi.1005497.e118"><alternatives><graphic id="pcbi.1005497.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e118" xlink:type="simple"/><mml:math display="block" id="M118"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>∝</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(75)</label></disp-formula>
where <bold>v</bold><sub>0</sub> is the eigenvector associated with the largest eigenvalue. The last term in this expression follows because the above minimization with respect to the angles forced <bold>v</bold><sub>0</sub> to be parallel to <bold>f</bold>′(<italic>s</italic>). Thus, for a broad, and reasonable, class of constraints on the covariance matrix, differential correlations minimize information.</p>
</sec>
<sec id="sec016">
<title>Variances of neural responses, and robustness to added noise, for different coding strategies</title>
<p>Throughout most of our analysis we focused on optimality of information transmission. However, also important is how much information is transmitted at the optimum. That’s the subject of this section. For simplicity we consider a linear gain function, which we set, without loss of generality, to the identity. That allows us to use the analysis above, in the section titled “Identifying the family of optimal covariance matrices”, and in particular <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref>, which links the noise in the input and output layers.</p>
<p>Our starting point is the derivation of an expression for the ratio of the information in the output layer to that in the input layer. To do that, we dot both sides of <xref ref-type="disp-formula" rid="pcbi.1005497.e053">Eq (37)</xref> by <bold>f</bold>′ on the left and right sides and solve for <italic>λ</italic>; we then do the same, except we dot with <inline-formula id="pcbi.1005497.e119"><alternatives><graphic id="pcbi.1005497.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e119" xlink:type="simple"/><mml:math display="inline" id="M119"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> on the left and its transpose on the right. This yields, after a small amount of algebra,
<disp-formula id="pcbi.1005497.e120"><alternatives><graphic id="pcbi.1005497.e120g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e120" xlink:type="simple"/><mml:math display="block" id="M120"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(76)</label></disp-formula>
where <italic>I</italic><sub><italic>x</italic></sub>, <italic>I</italic><sub><italic>y</italic></sub> and <italic>I</italic><sub><bold><italic>η</italic></bold></sub> are given by Eqs <xref ref-type="disp-formula" rid="pcbi.1005497.e003">(3a)</xref>, <xref ref-type="disp-formula" rid="pcbi.1005497.e004">(3b)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005497.e014">(9)</xref>, respectively. For information to be transmitted efficiently, <italic>I</italic><sub><italic>x</italic></sub>, the information in the input layer, must be small compared to <italic>I</italic><sub><bold><italic>η</italic></bold></sub>, the information associated with the added noise in the output layer. Below, we investigate the conditions under which <italic>I</italic><sub><italic>x</italic></sub> ≪ <italic>I</italic><sub><bold><italic>η</italic></bold></sub>, and thus when information loss is small.</p>
<p>Our strategy is to express <italic>I</italic><sub><italic>x</italic></sub>/<italic>I</italic><sub><bold><italic>η</italic></bold></sub> in terms of the single neuron variability, quantified as the average variance—something that has an easy interpretation. We consider two cases: the weights are set to the identity (<bold>W</bold> = <bold>I</bold>), and the weights are more realistic (each neuron in the input layer connects to a large number of neurons in the output layer). The first case, identity weights, is not very realistic; we include it because it is much simpler than the second.</p>
<p>While the analysis is straightforward, it is somewhat heavy on the algebra, so we summarize the results here. We consider two extremes in the family of optimal covariance structures: the “matched” case (<italic>α</italic> = 1 in <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref>, and, for simplicity, <bold>Ω</bold> = ∞) and differential correlations (<italic>α</italic> = 0). For matched covariances, near complete information transfer (<italic>I</italic><sub><italic>x</italic></sub> ≪ <italic>I</italic><sub><bold><italic>η</italic></bold></sub>) requires the effective variance of the noise in the second layer to be small. For identity feedforward weights, the effective variance in the input and output layers is about the same, so information loss is large. However, identity feedforward weights are never observed in the brain; instead, each neuron in the input layer connects to a large number of neurons in the output layer. Using <italic>N</italic><sub><italic>x</italic></sub> and <italic>N</italic><sub><italic>y</italic></sub> to denote the number of neurons in the input and output layers, respectively, and <italic>K</italic> the average number of connections per neuron, the effective noise is reduced by a factor or <inline-formula id="pcbi.1005497.e121"><alternatives><graphic id="pcbi.1005497.e121g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e121" xlink:type="simple"/><mml:math display="inline" id="M121"><mml:mrow><mml:mi>K</mml:mi> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1005497.e150">Eq 95</xref> below). Thus, if the number of neurons in the output layer is larger than the number in the input layer by a factor much larger than <italic>K</italic><sup>1/2</sup>, near complete information transmission is possible. For pure differential correlations, the story is much simpler: so long as the number of neurons in both layers is large, and the added noise doesn’t have a strong component in the <bold>f</bold>′(<italic>s</italic>) direction, near complete information transmission always occurs.</p>
<sec id="sec017">
<title>Identity feedforward weights</title>
<p>We’ll first consider identity feedforward weight, <bold>W</bold> = <bold>I</bold>. We’ll start with the matched covariance case. Using <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref>, we have
<disp-formula id="pcbi.1005497.e122"><alternatives><graphic id="pcbi.1005497.e122g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e122" xlink:type="simple"/><mml:math display="block" id="M122"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.166667em"/><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(77)</label></disp-formula></p>
<p>Taking the trace of both sides of this expression gives
<disp-formula id="pcbi.1005497.e123"><alternatives><graphic id="pcbi.1005497.e123g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e123" xlink:type="simple"/><mml:math display="block" id="M123"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(78)</label></disp-formula>
where <inline-formula id="pcbi.1005497.e124"><alternatives><graphic id="pcbi.1005497.e124g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e124" xlink:type="simple"/><mml:math display="inline" id="M124"><mml:mrow><mml:mo>〈</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>〉</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the average variance of the input layer noise and <inline-formula id="pcbi.1005497.e125"><alternatives><graphic id="pcbi.1005497.e125g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e125" xlink:type="simple"/><mml:math display="inline" id="M125"><mml:mrow><mml:mo>〈</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>〉</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the average variance of the added noise. If the added noise is on the same order as the noise in the input layer, information loss is high. Because of synaptic failures and chaotic dynamics, we expect the added noise to be substantial, implying that matching covariances is not an especially good strategy for transmitting information, in the case where <bold>W</bold> = <bold>I</bold>.</p>
<p>Next we consider differential correlations (<italic>α</italic> = 0 in <xref ref-type="disp-formula" rid="pcbi.1005497.e063">Eq (46)</xref>),
<disp-formula id="pcbi.1005497.e126"><alternatives><graphic id="pcbi.1005497.e126g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e126" xlink:type="simple"/><mml:math display="block" id="M126"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(79)</label></disp-formula>
where we used <xref ref-type="disp-formula" rid="pcbi.1005497.e014">Eq (9)</xref> for <italic>I</italic><sub><italic>η</italic></sub>, with <bold>Σ</bold><sub><italic>y</italic></sub> replaced by <bold>Σ</bold><sub><italic>η</italic></sub>. Taking the trace of both sides gives us
<disp-formula id="pcbi.1005497.e127"><alternatives><graphic id="pcbi.1005497.e127g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e127" xlink:type="simple"/><mml:math display="block" id="M127"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(80)</label></disp-formula></p>
<p>If the added noise doesn’t have much of a component in the <bold>f</bold>′ direction, then <inline-formula id="pcbi.1005497.e128"><alternatives><graphic id="pcbi.1005497.e128g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e128" xlink:type="simple"/><mml:math display="inline" id="M128"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is <inline-formula id="pcbi.1005497.e129"><alternatives><graphic id="pcbi.1005497.e129g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e129" xlink:type="simple"/><mml:math display="inline" id="M129"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. In this case, in the large <italic>N</italic><sub><italic>x</italic></sub> regime, <italic>I</italic><sub><italic>x</italic></sub> ≪ <italic>I</italic><sub><italic>η</italic></sub>, and (according to <xref ref-type="disp-formula" rid="pcbi.1005497.e120">Eq (76)</xref>) information loss is small. In other words, for large neural populations, differential correlations allow small information loss even when the amount of added noise is large.</p>
<p>An especially instructive case is <italic>iid</italic> noise added at the second layer. Using <inline-formula id="pcbi.1005497.e130"><alternatives><graphic id="pcbi.1005497.e130g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e130" xlink:type="simple"/><mml:math display="inline" id="M130"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> for its variance, <xref ref-type="disp-formula" rid="pcbi.1005497.e127">Eq (80)</xref> simplifies to
<disp-formula id="pcbi.1005497.e131"><alternatives><graphic id="pcbi.1005497.e131g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e131" xlink:type="simple"/><mml:math display="block" id="M131"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(81)</label></disp-formula></p>
<p>Consequently, for differential correlations and reasonably large neural populations, information loss is relatively small unless the variance in the second layer is <italic>much</italic> larger than the average variance in the first layer (by about a factor of <italic>N</italic><sub><italic>x</italic></sub>)—something that is not observed in the brain.</p>
<p>Although pure differential correlations can minimize information loss, they are not biologically realistic, as they do not display Poisson-like variability. That’s because for differential correlations, the variance of neuron <italic>i</italic> scales as <inline-formula id="pcbi.1005497.e132"><alternatives><graphic id="pcbi.1005497.e132g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e132" xlink:type="simple"/><mml:math display="inline" id="M132"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi> <mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> rather than <italic>f</italic><sub><italic>i</italic></sub>(<italic>s</italic>). Fortunately, this can be fixed with very little information loss by adding Poisson-like variability in the input layer. Doing so reduces the information only slightly: for the covariance structure given in <xref ref-type="disp-formula" rid="pcbi.1005497.e008">Eq (4a)</xref>, the information is
<disp-formula id="pcbi.1005497.e133"><alternatives><graphic id="pcbi.1005497.e133g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e133" xlink:type="simple"/><mml:math display="block" id="M133"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(82)</label></disp-formula>
where
<disp-formula id="pcbi.1005497.e134"><alternatives><graphic id="pcbi.1005497.e134g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e134" xlink:type="simple"/><mml:math display="block" id="M134"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(83)</label></disp-formula>
is the information associated with the covariance matrix <bold>Σ</bold><sub>0</sub> (see Sec.). That information is large whenever <bold>Σ</bold><sub>0</sub> doesn’t contain much of a component in the <bold>f</bold>′ direction and <italic>N</italic><sub><italic>x</italic></sub> is large. If these hold, the information in the input layer is approximately equal to 1/<italic>ϵ</italic>—exactly what it is for pure differential correlations. Moreover, so long as <bold>Σ</bold><sub><bold><italic>η</italic></bold></sub> also doesn’t contain much of a component in the <bold>f</bold>′ direction, information in the output layer is also close to 1/<italic>ϵ</italic>, and very little information is lost. Thus, nearly pure differential correlations are biologically realistic and can lead to very small information loss.</p>
</sec>
<sec id="sec018">
<title>Realistic feedforward weights</title>
<p>For realistic feedforward weights, <bold>W</bold>, we need to use <bold>Σ</bold><sub><italic>y</italic></sub> rather than <bold>Σ</bold><sub><italic>η</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1005497.e122">Eq (77)</xref>, with <bold>Σ</bold><sub><italic>y</italic></sub> given by <xref ref-type="disp-formula" rid="pcbi.1005497.e013">Eq (8)</xref>. (Note that because the gain function is the identity, <bold>W</bold><sub>eff</sub> = <bold>W</bold>.) We’ll start, as above, with the matched covariance case. Taking the trace of both sides of <xref ref-type="disp-formula" rid="pcbi.1005497.e122">Eq (77)</xref>, but with <bold>Σ</bold><sub><italic>η</italic></sub> replaced by <bold>Σ</bold><sub><italic>y</italic></sub>, we have
<disp-formula id="pcbi.1005497.e135"><alternatives><graphic id="pcbi.1005497.e135g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e135" xlink:type="simple"/><mml:math display="block" id="M135"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(84)</label></disp-formula>
where tr denotes trace and, as above, <italic>N</italic><sub><italic>x</italic></sub> is the number of neurons in the input layer. Using the fact that for any positive semi-definite square <italic>n</italic> × <italic>n</italic> matrix <bold>A</bold> (i.e., for any covariance matrix <bold>A</bold>),
<disp-formula id="pcbi.1005497.e136"><alternatives><graphic id="pcbi.1005497.e136g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e136" xlink:type="simple"/><mml:math display="block" id="M136"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>tr</mml:mtext> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>n</mml:mi></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mi>n</mml:mi> <mml:mrow><mml:mtext>tr</mml:mtext> <mml:mo>[</mml:mo> <mml:mi mathvariant="bold">A</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(85)</label></disp-formula>
we have
<disp-formula id="pcbi.1005497.e137"><alternatives><graphic id="pcbi.1005497.e137g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e137" xlink:type="simple"/><mml:math display="block" id="M137"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(86)</label></disp-formula>
with the second equality following from <xref ref-type="disp-formula" rid="pcbi.1005497.e013">Eq (8)</xref>.</p>
<p>To get a handle on the size of the trace term in the numerator, we note that it can be written
<disp-formula id="pcbi.1005497.e138"><alternatives><graphic id="pcbi.1005497.e138g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e138" xlink:type="simple"/><mml:math display="block" id="M138"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>W</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(87)</label></disp-formula>
where, defining <bold>v</bold><sub><italic>k</italic></sub> to be the <italic>k</italic><sup>th</sup> eigenvector of <bold>Σ</bold><sub><italic>η</italic></sub>, normalized so that <bold>v</bold><sub><italic>k</italic></sub> · <bold>v</bold><sub><italic>k</italic></sub> = 1, and <inline-formula id="pcbi.1005497.e139"><alternatives><graphic id="pcbi.1005497.e139g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e139" xlink:type="simple"/><mml:math display="inline" id="M139"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> to be its corresponding eigenvalue,
<disp-formula id="pcbi.1005497.e140"><alternatives><graphic id="pcbi.1005497.e140g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e140" xlink:type="simple"/><mml:math display="block" id="M140"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>W</mml:mi></mml:msub> <mml:mo>≡</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mtext>tr</mml:mtext> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(88)</label></disp-formula></p>
<p>To see that this really is a weighted average, note that because the <bold>v</bold><sub><italic>k</italic></sub> form a complete, orthonormal basis,
<disp-formula id="pcbi.1005497.e141"><alternatives><graphic id="pcbi.1005497.e141g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e141" xlink:type="simple"/><mml:math display="block" id="M141"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(89)</label></disp-formula></p>
<p>Inserting <xref ref-type="disp-formula" rid="pcbi.1005497.e138">Eq (87)</xref> into <xref ref-type="disp-formula" rid="pcbi.1005497.e137">Eq (86)</xref> gives us
<disp-formula id="pcbi.1005497.e142"><alternatives><graphic id="pcbi.1005497.e142g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e142" xlink:type="simple"/><mml:math display="block" id="M142"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>≥</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>W</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mtext>tr</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(90)</label></disp-formula></p>
<p>This is similar to <xref ref-type="disp-formula" rid="pcbi.1005497.e123">Eq (78)</xref>, except for two prefactors. The denominator of the first prefactor lies between <inline-formula id="pcbi.1005497.e143"><alternatives><graphic id="pcbi.1005497.e143g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e143" xlink:type="simple"/><mml:math display="inline" id="M143"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>〉</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>η</mml:mi> <mml:mo>,</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005497.e144"><alternatives><graphic id="pcbi.1005497.e144g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e144" xlink:type="simple"/><mml:math display="inline" id="M144"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>〉</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>η</mml:mi> <mml:mo>,</mml:mo> <mml:mo form="prefix" movablelimits="true">min</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. We’ll assume this is <inline-formula id="pcbi.1005497.e145"><alternatives><graphic id="pcbi.1005497.e145g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e145" xlink:type="simple"/><mml:math display="inline" id="M145"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (for <italic>iid</italic> noise it is exactly 1), although we note that it’s possible to make it either relatively large or relatively small. The second prefactor is more interesting, as it is the sum of a large number of terms,
<disp-formula id="pcbi.1005497.e146"><alternatives><graphic id="pcbi.1005497.e146g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e146" xlink:type="simple"/><mml:math display="block" id="M146"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>tr</mml:mtext> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>y</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:munderover> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(91)</label></disp-formula>
where <italic>N</italic><sub><italic>y</italic></sub> is the number of neurons in the output layer. To determine the size of the weights, we use that fact that
<disp-formula id="pcbi.1005497.e147"><alternatives><graphic id="pcbi.1005497.e147g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e147" xlink:type="simple"/><mml:math display="block" id="M147"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(92)</label></disp-formula>
and note that 〈<italic>y</italic><sub><italic>i</italic></sub>〉 and <italic>f</italic><sub><italic>i</italic></sub> should be about the same size, on average. Assuming that each neuron in the input layer connects, on average, to <italic>K</italic> neurons in the output layer, it follows that <italic>W</italic><sub><italic>ij</italic></sub> is nonzero with probability <italic>K</italic>/<italic>N</italic><sub><italic>y</italic></sub>. Consequently,
<disp-formula id="pcbi.1005497.e148"><alternatives><graphic id="pcbi.1005497.e148g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e148" xlink:type="simple"/><mml:math display="block" id="M148"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mi>K</mml:mi></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>y</mml:mi></mml:msub></mml:mfrac> <mml:msub><mml:mi>W</mml:mi> <mml:mtext>typical</mml:mtext></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mtext>typical</mml:mtext></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(93)</label></disp-formula>
where <italic>W</italic><sub>typical</sub> and <italic>f</italic><sub>typical</sub> are the typical sizes of the nonzero weights and the <italic>f</italic><sub><italic>j</italic></sub>, respectively. To ensure that 〈<italic>y</italic><sub><italic>i</italic></sub>〉 and <italic>f</italic><sub><italic>i</italic></sub> are about the same size, we must have
<disp-formula id="pcbi.1005497.e149"><alternatives><graphic id="pcbi.1005497.e149g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e149" xlink:type="simple"/><mml:math display="block" id="M149"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>W</mml:mi> <mml:mtext>typical</mml:mtext></mml:msub> <mml:mo>∼</mml:mo> <mml:mfrac><mml:msub><mml:mi>N</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mi>K</mml:mi></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(94)</label></disp-formula></p>
<p>Inserting this into <xref ref-type="disp-formula" rid="pcbi.1005497.e146">Eq (91)</xref>, and using the fact that <italic>W</italic><sub><italic>ij</italic></sub> is nonzero with probability <italic>K</italic>/<italic>N</italic><sub><italic>y</italic></sub>, we have
<disp-formula id="pcbi.1005497.e150"><alternatives><graphic id="pcbi.1005497.e150g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e150" xlink:type="simple"/><mml:math display="block" id="M150"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>tr</mml:mtext> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mo>∼</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mi>K</mml:mi></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(95)</label></disp-formula></p>
<p>This can be large if <italic>N</italic><sub><italic>y</italic></sub> ≫ <italic>N</italic><sub><italic>x</italic></sub> <italic>K</italic><sup>1/2</sup>. Using this relationship in <xref ref-type="disp-formula" rid="pcbi.1005497.e142">Eq (90)</xref>, we see that information loss can be small in the case of matched covariances, if there is sufficiently large divergence from the input to output layers.</p>
<p>What about differential correlations, <italic>α</italic> = 0? To understand information loss in this case, <bold>Σ</bold><sub><bold><italic>η</italic></bold></sub> is replaced by <bold>Σ</bold><sub><italic>y</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1005497.e127">Eq (80)</xref>, giving us
<disp-formula id="pcbi.1005497.e151"><alternatives><graphic id="pcbi.1005497.e151g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e151" xlink:type="simple"/><mml:math display="block" id="M151"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>η</mml:mi></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(96)</label></disp-formula>
where we used <xref ref-type="disp-formula" rid="pcbi.1005497.e013">Eq (8)</xref> for <bold>Σ</bold><sub><italic>y</italic></sub>. Here the logic is the same as it was in the previous section: so long as <bold>Σ</bold><sub><italic>y</italic></sub> doesn’t have a strong component in the <bold>f</bold>′ direction, <inline-formula id="pcbi.1005497.e152"><alternatives><graphic id="pcbi.1005497.e152g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e152" xlink:type="simple"/><mml:math display="inline" id="M152"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is <inline-formula id="pcbi.1005497.e153"><alternatives><graphic id="pcbi.1005497.e153g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e153" xlink:type="simple"/><mml:math display="inline" id="M153"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and, since <inline-formula id="pcbi.1005497.e154"><alternatives><graphic id="pcbi.1005497.e154g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e154" xlink:type="simple"/><mml:math display="inline" id="M154"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, information loss is <inline-formula id="pcbi.1005497.e155"><alternatives><graphic id="pcbi.1005497.e155g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e155" xlink:type="simple"/><mml:math display="inline" id="M155"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Thus, with realistic feedforward weights, as with the identity case, differential correlations lead to very small information loss in large populations.</p>
</sec>
</sec>
<sec id="sec019">
<title>Information in a population with a rank 1 perturbation to the covariance matrix</title>
<p>In the analysis of nonlinear gain functions in the main text (section titled “Nonlinear gain functions”), it was necessary to construct a covariance matrix such that the information in the first layer was independent of <italic>ϵ</italic><sub><italic>u</italic></sub> and <bold>u</bold>. For that we included a prefactor <italic>γ</italic><sub><italic>u</italic></sub> in the definition of the covariance matrix, <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> (see <xref ref-type="disp-formula" rid="pcbi.1005497.e017">Eq (12)</xref>). Here we determine how <italic>γ</italic><sub><italic>u</italic></sub> should depend on <italic>ϵ</italic><sub><italic>u</italic></sub> and <bold>u</bold>. Our starting point is an expression for the inverse of <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub>. As is straightforward to show, via direct substitution, that’s given by
<disp-formula id="pcbi.1005497.e156"><alternatives><graphic id="pcbi.1005497.e156g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e156" xlink:type="simple"/><mml:math display="block" id="M156"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>γ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mi mathvariant="bold">u</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:mfenced></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>γ</mml:mi> <mml:mi>u</mml:mi></mml:msub></mml:mfrac> <mml:mfenced close="]" open="[" separators=""><mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(97)</label></disp-formula></p>
<p>Thus, the information in the input layer, <inline-formula id="pcbi.1005497.e157"><alternatives><graphic id="pcbi.1005497.e157g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e157" xlink:type="simple"/><mml:math display="inline" id="M157"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, is given by
<disp-formula id="pcbi.1005497.e158"><alternatives><graphic id="pcbi.1005497.e158g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e158" xlink:type="simple"/><mml:math display="block" id="M158"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>γ</mml:mi> <mml:mi>u</mml:mi></mml:msub></mml:mfrac> <mml:mfenced close="]" open="[" separators=""><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(98)</label></disp-formula></p>
<p>To ensure that this information is independent of <italic>γ</italic><sub><italic>u</italic></sub>, we let
<disp-formula id="pcbi.1005497.e159"><alternatives><graphic id="pcbi.1005497.e159g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e159" xlink:type="simple"/><mml:math display="block" id="M159"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>I</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mfrac> <mml:mfenced close="]" open="[" separators=""><mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mo>′</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ϵ</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>·</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mn>0</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(99)</label></disp-formula></p>
<p>Note that <italic>γ</italic><sub><italic>u</italic></sub> depends on <italic>s</italic> as well as <italic>ϵ</italic><sub><italic>u</italic></sub> and <bold>u</bold>.</p>
</sec>
<sec id="sec020">
<title>Details for numerical examples</title>
<p>In this section we provide details for the numerical simulations for each relevant figure.</p>
<sec id="sec021">
<title>
<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref> and its synergistic counterpart, <xref ref-type="fig" rid="pcbi.1005497.g007">Fig 7</xref></title>
<p>For the numerical examples in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, we generated tuning curves for the first layer of cells using Von Mises distributions [<xref ref-type="bibr" rid="pcbi.1005497.ref016">16</xref>],
<disp-formula id="pcbi.1005497.e160"><alternatives><graphic id="pcbi.1005497.e160g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e160" xlink:type="simple"/><mml:math display="block" id="M160"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>ρ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>υ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>β</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mo form="prefix">cos</mml:mo> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>φ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(100)</label></disp-formula></p>
<p>For each cell, the amplitudes, <italic>υ</italic>, widths, <italic>β</italic>, peak locations, <italic>φ</italic>, and baseline offsets, <italic>ρ</italic>, were drawn independently from uniform distributions with the following ranges,</p>
<list list-type="bullet">
<list-item>
<p><italic>υ</italic>: 1–51</p>
</list-item>
<list-item>
<p><italic>β</italic>: 1–6</p>
</list-item>
<list-item>
<p><italic>φ</italic>: 0–2<italic>π</italic></p>
</list-item>
<list-item>
<p><italic>ρ</italic>: 0–1</p>
</list-item>
</list>
<p>The covariance of the noise in the first layer was given by <xref ref-type="disp-formula" rid="pcbi.1005497.e008">Eq (4)</xref>, with the following parameters,</p>
<list list-type="bullet">
<list-item>
<p>blue population: <italic>ϵ</italic> = 10<sup>−3</sup>.</p>
</list-item>
<list-item>
<p>green population: <italic>ϵ</italic><sub><italic>u</italic></sub> varies with stimulus so that, for each stimulus, the blue and green populations have identical information (on average, <italic>ϵ</italic><sub><italic>u</italic></sub> = 8 × 10<sup>−3</sup>); |<bold>u</bold>(<italic>s</italic>)| = |<bold>f</bold>′(<italic>s</italic>)|; angle between <bold>u</bold>(<italic>s</italic>) and <bold>f</bold>′(<italic>s</italic>) = 1/8 of a radian.</p>
</list-item>
</list>
<p>With these parameters, the two populations (blue and green) conveyed the same amount of information about the stimulus.</p>
<p>To rule out the possibility that differences in information robustness were due to differences in average correlations within the populations, we forced the average correlations to be the same for the blue and green populations. To do that, we repeatedly took random draws of the parameters describing the tuning curves (<italic>ρ</italic>, <italic>v</italic>, <italic>β</italic> and <italic>φ</italic>) until the population averaged correlations matched between the two populations. This resulted in average correlations of −7 × 10<sup>−5</sup>, and we used this set of tuning curves for our subsequent information calculations.</p>
<p>We computed the information, <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), in the second-layer responses using <xref ref-type="disp-formula" rid="pcbi.1005497.e004">Eq (3b)</xref>, with <italic>g</italic>(<italic>x</italic>) = <italic>x</italic>, <bold>W</bold> = <bold>I</bold>, and <bold>Σ</bold><sub><bold><italic>η</italic></bold></sub> = <italic>σ</italic><sup>2</sup> <bold>I</bold>. For the trial-shuffled information (<xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2C</xref>), we used <xref ref-type="disp-formula" rid="pcbi.1005497.e003">Eq (3a)</xref>, with all off-diagonal elements of the covariance matrices <bold>Σ</bold><sub><bold><italic>ξ</italic></bold></sub> set to zero. For all of these information calculations, we computed the information, <italic>I</italic><sub><italic>x</italic></sub>(<italic>s</italic>) or <italic>I</italic><sub><italic>y</italic></sub>(<italic>s</italic>), for 100 different stimulus values <italic>s</italic>, uniformly spaced between 0 and 2<italic>π</italic>, and then averaged over these 100 different values.</p>
<p>To assess whether synergistic population codes can similarly vary in their robustness to corruption by noise, we repeated our calculations from <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, but modified the covariance matrices to make the population synergistic (<xref ref-type="fig" rid="pcbi.1005497.g007">Fig 7C</xref>: the correlated responses convey more stimulus information than would independent cells with the same variances). To do that we again used the covariance matrices given in <xref ref-type="disp-formula" rid="pcbi.1005497.e008">Eq (4)</xref>, but we made <italic>ϵ</italic> and <italic>ϵ</italic><sub><italic>u</italic></sub> negative: <italic>ϵ</italic> = −5 × 10<sup>−4</sup> and 〈<italic>ϵ</italic><sub><italic>u</italic></sub>〉 = −3 × 10<sup>−4</sup> (as in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, <italic>ϵ</italic><sub><italic>u</italic></sub> depends on the stimulus, <italic>s</italic>: it was chosen so that for each value of <italic>s</italic> the blue and green populations have identical stimulus information). We chose <bold>u</bold>(<italic>s</italic>) so that it had the same magnitude as <bold>f</bold>′(<italic>s</italic>) and made an angle of 1/4 of a radian with <bold>f</bold>′(<italic>s</italic>). We used the same functions and distributions for the tuning curves as in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, but used a different seed for the random number generator. As in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, the seed was chosen (via multiple draws of the tuning curve parameters) so that the two populations had the same average correlations (in this case 2 × 10<sup>−5</sup>). Also as in <xref ref-type="fig" rid="pcbi.1005497.g002">Fig 2</xref>, the populations were roughly Poisson-like, in the sense that the mean and variance of the activity of each neuron was approximately equal. (Both the “green” and the “blue” populations have average Fano factors—averaged over neurons and stimuli—of 0.99.) We again found that equally-informative population codes could vary significantly in terms of their robustness to noise (<xref ref-type="fig" rid="pcbi.1005497.g007">Fig 7B</xref>).</p>
</sec>
<sec id="sec022">
<title><xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5</xref></title>
<p>To generate <xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5B</xref>, we analytically computed the means of the second layer responses, resulting in the expression
<disp-formula id="pcbi.1005497.e161"><alternatives><graphic id="pcbi.1005497.e161g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e161" xlink:type="simple"/><mml:math display="block" id="M161"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>Φ</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(101)</label></disp-formula>
where <italic>θ</italic><sub><italic>i</italic></sub> is the <italic>i</italic><sup>th</sup> cell’s firing threshold, <italic>σ</italic><sub><italic>i</italic></sub> is the standard deviation of the input noise to the cell, and Φ(⋅) is the Gaussian cumulative distribution function. For each cell, the input function <italic>f</italic><sub><italic>i</italic></sub>(<italic>s</italic>) was given by a Von Mises distribution, <xref ref-type="disp-formula" rid="pcbi.1005497.e160">Eq (100)</xref> (with the same distribution of parameters—<italic>v</italic>, <italic>β</italic>, <italic>φ</italic> and <italic>ρ</italic>—as in the preceding examples), and the spiking threshold, <italic>θ</italic><sub><italic>i</italic></sub>, was set to 3/4 of the peak height of the input tuning curve: <italic>θ</italic><sub><italic>i</italic></sub> = 3(<italic>ρ</italic><sub><italic>i</italic></sub> + <italic>υ</italic><sub><italic>i</italic></sub>)/4.</p>
<p>It is not straightforward to compute the covariance matrix of correlated responses generated by the dichotomized Gaussian model, so we used Monte Carlo methods to estimate the covariance: we took 10<sup>6</sup> draws from the distribution of <bold>x</bold>, and for each draw we computed the corresponding responses, <bold>y</bold>, using the thresholding operation (<xref ref-type="disp-formula" rid="pcbi.1005497.e016">Eq (11)</xref>). We then computed the covariance of these simulated responses, and used them to estimate the linear Fisher information in the second layer activities via the standard expression,
<disp-formula id="pcbi.1005497.e162"><alternatives><graphic id="pcbi.1005497.e162g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e162" xlink:type="simple"/><mml:math display="block" id="M162"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mtext>Cov</mml:mtext> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(102)</label></disp-formula></p>
</sec>
<sec id="sec023">
<title><xref ref-type="fig" rid="pcbi.1005497.g006">Fig 6</xref></title>
<p><xref ref-type="fig" rid="pcbi.1005497.g006">Fig 6</xref> was made in the same fashion as <xref ref-type="fig" rid="pcbi.1005497.g005">Fig 5</xref>, with the exception that noise was added before the spike generation nonlinearity. The noise, <italic>ζ</italic>, was Gaussian and drawn <italic>iid</italic> with variance <inline-formula id="pcbi.1005497.e163"><alternatives><graphic id="pcbi.1005497.e163g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005497.e163" xlink:type="simple"/><mml:math display="inline" id="M163"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ζ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Robert Townley, Kresimir Josic, Fred Rieke, Braden Brinkman, Maxwell Turner, and Alison Weber for helpful comments on the project.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005497.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Britten</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Responses of neurons in macaque MT to stochastic motion signals</article-title>. <source>Visual Neurosci</source>. <year>1993</year>;<volume>10</volume>:<fpage>1157</fpage>–<lpage>1169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0952523800010269" xlink:type="simple">10.1017/S0952523800010269</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Softky</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSP’s</article-title>. <source>J Neurosci</source>. <year>1993</year>;<volume>13</volume>:<fpage>334</fpage>–<lpage>350</lpage>. <object-id pub-id-type="pmid">8423479</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Faisal</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Selen</surname> <given-names>LPJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Noise in the nervous system</article-title>. <source>Nat Rev Neurosci</source>. <year>2008</year>;<volume>9</volume>:<fpage>292</fpage>–<lpage>303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2258" xlink:type="simple">10.1038/nrn2258</ext-link></comment> <object-id pub-id-type="pmid">18319728</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Churchland</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Sugrue</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Corrado</surname> <given-names>GS</given-names></name>, <etal>et al</etal>. <article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title>. <source>Nat Neurosci</source>. <year>2010</year>;<volume>13</volume>:<fpage>369</fpage>–<lpage>378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2501" xlink:type="simple">10.1038/nn.2501</ext-link></comment> <object-id pub-id-type="pmid">20173745</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Franke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Fiscella</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sevelev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Roska</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hierlemann</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>da Silveira</surname> <given-names>R</given-names></name>. <article-title>Structure of neural correlation and how they favor coding</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>89</volume>:<fpage>409</fpage>–<lpage>422</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.12.037" xlink:type="simple">10.1016/j.neuron.2015.12.037</ext-link></comment> <object-id pub-id-type="pmid">26796692</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cafaro</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Turner</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <article-title>Direction-selective circuits shape noise to ensure a precise population code</article-title>. <source>Neuron</source>. <year>2016</year>;(<issue>89</issue>):<fpage>369</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.11.019" xlink:type="simple">10.1016/j.neuron.2015.11.019</ext-link></comment> <object-id pub-id-type="pmid">26796691</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hyde</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Strowbridge</surname> <given-names>BW</given-names></name>. <article-title>Dynamics of robust pattern separability in the hippocampal dentate gyrus</article-title>. <source>Hippocampus</source>. <year>2016</year>;(<issue>29</issue>):<fpage>623</fpage>–<lpage>632</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hipo.22546" xlink:type="simple">10.1002/hipo.22546</ext-link></comment> <object-id pub-id-type="pmid">26482936</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zohary</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title>. <source>Nature</source>. <year>1994</year>;<volume>370</volume>(<issue>6485</issue>):<fpage>140</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/370140a0" xlink:type="simple">10.1038/370140a0</ext-link></comment> <object-id pub-id-type="pmid">8022482</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>The effect of correlated variability on the accuracy of a population code</article-title>. <source>Neural Comput</source>. <year>1999</year>;<volume>11</volume>(<issue>1</issue>):<fpage>91</fpage>–<lpage>101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976699300016827" xlink:type="simple">10.1162/089976699300016827</ext-link></comment> <object-id pub-id-type="pmid">9950724</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yoon</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shamir</surname> <given-names>M</given-names></name>. <article-title>Population coding in neuronal systems with correlated noise</article-title>. <source>Phys Rev E</source>. <year>2001</year>;<volume>64</volume>(<issue>5</issue>):<fpage>051904</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.64.051904" xlink:type="simple">10.1103/PhysRevE.64.051904</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hernandez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zainos</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name>. <article-title>Correlated neuronal discharges that increase coding efficiency during perceptual discrimination</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>38</volume>(<issue>4</issue>):<fpage>649</fpage>–<lpage>657</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(03)00287-3" xlink:type="simple">10.1016/S0896-6273(03)00287-3</ext-link></comment> <object-id pub-id-type="pmid">12765615</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Neural correlations, population coding and computation</article-title>. <source>Nat Rev Neurosci</source>. <year>2006</year>;<volume>7</volume>(<issue>5</issue>):<fpage>358</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1888" xlink:type="simple">10.1038/nrn1888</ext-link></comment> <object-id pub-id-type="pmid">16760916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shamir</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Implications of neuronal diversity on population coding</article-title>. <source>Neural Comput</source>. <year>2006</year>;<volume>18</volume>(<issue>8</issue>):<fpage>1951</fpage>–<lpage>1986</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.8.1951" xlink:type="simple">10.1162/neco.2006.18.8.1951</ext-link></comment> <object-id pub-id-type="pmid">16771659</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>. <article-title>Effects of noise correlations on information encoding and decoding</article-title>. <source>J Neurophys</source>. <year>2006</year>;<volume>95</volume>(<issue>6</issue>):<fpage>3633</fpage>–<lpage>3644</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00919.2005" xlink:type="simple">10.1152/jn.00919.2005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Josić</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>de la Rocha</surname> <given-names>J</given-names></name>. <article-title>Stimulus-dependent correlations and population codes</article-title>. <source>Neural Comput</source>. <year>2009</year>;<volume>21</volume>(<issue>10</issue>):<fpage>2774</fpage>–<lpage>2804</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2009.10-08-879" xlink:type="simple">10.1162/neco.2009.10-08-879</ext-link></comment> <object-id pub-id-type="pmid">19635014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>The Effect of Noise Correlations in Populations of Diversely Tuned Neurons</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>(<issue>40</issue>):<fpage>14272</fpage>–<lpage>14283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2539-11.2011" xlink:type="simple">10.1523/JNEUROSCI.2539-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21976512</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>. <article-title>Measuring and interpreting neuronal correlations</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>7</issue>):<fpage>811</fpage>–<lpage>819</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2842" xlink:type="simple">10.1038/nn.2842</ext-link></comment> <object-id pub-id-type="pmid">21709677</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Latham P, Roudi Y. Role of correlations in population coding. arXiv:11096524 [q-bio/NC]. 2011;.</mixed-citation>
</ref>
<ref id="pcbi.1005497.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>da Silveira</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>. <article-title>High-Fidelity Coding with Correlated Neurons</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>:<fpage>e1003970</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003970" xlink:type="simple">10.1371/journal.pcbi.1003970</ext-link></comment> <object-id pub-id-type="pmid">25412463</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>The Sign Rule and Beyond: Boundary Effects, Flexibility, and Noise Correlations in Neural Population Codes</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>:<fpage>e1003469</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003469" xlink:type="simple">10.1371/journal.pcbi.1003469</ext-link></comment> <object-id pub-id-type="pmid">24586128</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shamir</surname> <given-names>M</given-names></name>. <article-title>Emerging principles of population coding: in search for the neural code</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year>;<volume>25</volume>:<fpage>140</fpage>–<lpage>148</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2014.01.002" xlink:type="simple">10.1016/j.conb.2014.01.002</ext-link></comment> <object-id pub-id-type="pmid">24487341</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kanitscheider</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Information-limiting correlations</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>:<fpage>1410</fpage>–<lpage>1417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3807" xlink:type="simple">10.1038/nn.3807</ext-link></comment> <object-id pub-id-type="pmid">25195105</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>Input nonlinearities can shape beyond-pairwise correlations and improve information transmission by neural populations</article-title>. <source>Phys Rev E</source>. <year>2015</year>;<volume>92</volume>:<fpage>062707</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.92.062707" xlink:type="simple">10.1103/PhysRevE.92.062707</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cayco-Gajic</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>Triplet correlations among similarly-tuned cells impact population coding</article-title>. <source>Front Comput Neurosci</source>. <year>2015</year>;<volume>9</volume>:<fpage>57</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2015.00057" xlink:type="simple">10.3389/fncom.2015.00057</ext-link></comment> <object-id pub-id-type="pmid">26042024</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Deneve</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ducom</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Narrow versus wide tuning curves: what’s best for a population code?</article-title> <source>Neural Comput</source>. <year>1999</year>;<volume>11</volume>:<fpage>85</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976699300016818" xlink:type="simple">10.1162/089976699300016818</ext-link></comment> <object-id pub-id-type="pmid">9950723</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Neuronal tuning: To sharpen or broaden?</article-title> <source>Neural Comput</source>. <year>1999</year>;<volume>11</volume>(<issue>1</issue>):<fpage>75</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976699300016809" xlink:type="simple">10.1162/089976699300016809</ext-link></comment> <object-id pub-id-type="pmid">9950722</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilke</surname> <given-names>SD</given-names></name>, <name name-style="western"><surname>Eurich</surname> <given-names>CW</given-names></name>. <article-title>Representational accuracy of stochastic neural populations</article-title>. <source>Neural Comput</source>. <year>2002</year>;<volume>14</volume>(<issue>1</issue>):<fpage>155</fpage>–<lpage>189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602753284482" xlink:type="simple">10.1162/089976602753284482</ext-link></comment> <object-id pub-id-type="pmid">11747537</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Prentice</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Balasubramanian</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Schneidman</surname> <given-names>E</given-names></name>. <article-title>Optimal population coding by noisy spiking neurons</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2010</year>;<volume>107</volume>(<issue>32</issue>):<fpage>14419</fpage>–<lpage>14424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1004906107" xlink:type="simple">10.1073/pnas.1004906107</ext-link></comment> <object-id pub-id-type="pmid">20660781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bejjanki</surname> <given-names>VR</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Insights from a simple expression for linear fisher information in a recurrently connected population of spiking neurons</article-title>. <source>Neural Comput</source>. <year>2011</year>;<volume>23</volume>(<issue>6</issue>):<fpage>1484</fpage>–<lpage>1502</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00125" xlink:type="simple">10.1162/NECO_a_00125</ext-link></comment> <object-id pub-id-type="pmid">21395435</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Toyoizumi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Aihara</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Amari</surname> <given-names>SI</given-names></name>. <article-title>Fisher information for spike-based population decoding</article-title>. <source>Phys Rev Lett</source>. <year>2006</year>;<volume>97</volume>:<fpage>098102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.97.098102" xlink:type="simple">10.1103/PhysRevLett.97.098102</ext-link></comment> <object-id pub-id-type="pmid">17026405</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bejjanki</surname> <given-names>VR</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>ZL</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Perceptual learning as improved probabilistic inference</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>:<fpage>642</fpage>–<lpage>648</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2796" xlink:type="simple">10.1038/nn.2796</ext-link></comment> <object-id pub-id-type="pmid">21460833</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Impact of correlated synaptic input on output firing rate and variability in simple neuronal models</article-title>. <source>J Neurosci</source>. <year>2000</year>;<volume>20</volume>(<issue>16</issue>):<fpage>6193</fpage>–<lpage>6209</lpage>. <object-id pub-id-type="pmid">10934269</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Correlated neuronal activity and the flow of neural information</article-title>. <source>Nat Rev Neurosci</source>. <year>2001</year>;<volume>2</volume>:<fpage>539</fpage>–<lpage>550</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35086012" xlink:type="simple">10.1038/35086012</ext-link></comment> <object-id pub-id-type="pmid">11483997</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reid</surname> <given-names>RC</given-names></name>. <article-title>Divergence and reconvergence: multielectrode analysis of feedforward connections in the visual system</article-title>. <source>Prog Brain Res</source>. <year>2001</year>;<volume>130</volume>:<fpage>141</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0079-6123(01)30010-9" xlink:type="simple">10.1016/S0079-6123(01)30010-9</ext-link></comment> <object-id pub-id-type="pmid">11480272</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bruno</surname> <given-names>RM</given-names></name>. <article-title>Synchrony in sensation</article-title>. <source>Curr Opin Neurobiol</source>. <year>2011</year>;<volume>21</volume>:<fpage>701</fpage>–<lpage>708</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2011.06.003" xlink:type="simple">10.1016/j.conb.2011.06.003</ext-link></comment> <object-id pub-id-type="pmid">21723114</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abeles</surname> <given-names>M</given-names></name>. <article-title>Role of the cortical neuron: integrator or coincidence detector?</article-title> <source>Isr J Med Sci</source>. <year>1982</year>;<volume>18</volume>:<fpage>83</fpage>–<lpage>92</lpage>. <object-id pub-id-type="pmid">6279540</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Seriès</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations</article-title>. <source>Nat Neurosci</source>. <year>2004</year>;<volume>7</volume>(<issue>10</issue>):<fpage>1129</fpage>–<lpage>1135</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1321" xlink:type="simple">10.1038/nn1321</ext-link></comment> <object-id pub-id-type="pmid">15452579</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Renart</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>van Rossum</surname> <given-names>MCW</given-names></name>. <article-title>Transmission of population-coded information</article-title>. <source>Neural Comput</source>. <year>2011</year>;<volume>24</volume>:<fpage>391</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00227" xlink:type="simple">10.1162/NECO_a_00227</ext-link></comment> <object-id pub-id-type="pmid">22023200</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lampl</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Reichova</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ferster</surname> <given-names>D</given-names></name>. <article-title>Synchronous membrane potential fluctuations in neurons of the cat visual cortex</article-title>. <source>Neuron</source>. <year>1999</year>;<volume>22</volume>:<fpage>361</fpage>–<lpage>374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(00)81096-X" xlink:type="simple">10.1016/S0896-6273(00)81096-X</ext-link></comment> <object-id pub-id-type="pmid">10069341</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alonso</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Usrey</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Reid</surname> <given-names>RC</given-names></name>. <article-title>Precisely correlated firing of cells in the lateral geniculate nucleus</article-title>. <source>Nature</source>. <year>1996</year>;<volume>383</volume>:<fpage>815</fpage>–<lpage>819</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/383815a0" xlink:type="simple">10.1038/383815a0</ext-link></comment> <object-id pub-id-type="pmid">8893005</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goris</surname> <given-names>RLT</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Partitioning neuronal variability</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>:<fpage>858</fpage>–<lpage>865</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3711" xlink:type="simple">10.1038/nn.3711</ext-link></comment> <object-id pub-id-type="pmid">24777419</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>. <article-title>Spatial and temporal scales of neuronal correlation in primary visual cortex</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>:<fpage>12591</fpage>–<lpage>12603</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2929-08.2008" xlink:type="simple">10.1523/JNEUROSCI.2929-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19036953</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>State dependence of noise correlations in macaque primary visual cortex</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>:<fpage>235</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.02.006" xlink:type="simple">10.1016/j.neuron.2014.02.006</ext-link></comment> <object-id pub-id-type="pmid">24698278</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scholvinck</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Saleem</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Benucci</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>. <article-title>Cortical State Determines Global Variability and Correlations in Visual Cortex</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>:<fpage>170</fpage>–<lpage>178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4994-13.2015" xlink:type="simple">10.1523/JNEUROSCI.4994-13.2015</ext-link></comment> <object-id pub-id-type="pmid">25568112</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>IC</given-names></name>, <name name-style="western"><surname>Okun</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>. <article-title>The Nature of Shared Cortical Variability</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>87</volume>:<fpage>644</fpage>–<lpage>656</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.06.035" xlink:type="simple">10.1016/j.neuron.2015.06.035</ext-link></comment> <object-id pub-id-type="pmid">26212710</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hornik</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Stinchcombe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>H</given-names></name>. <article-title>Multilayer feedforward networks are universal function approximators</article-title>. <source>Neural Netw</source>. <year>1989</year>;<volume>2</volume>:<fpage>359</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0893-6080(89)90020-8" xlink:type="simple">10.1016/0893-6080(89)90020-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>, <name name-style="western"><surname>Levick</surname> <given-names>WR</given-names></name>. <article-title>The mechanism of directionally selective units in rabbit’s retina</article-title>. <source>J Physiol</source>. <year>1965</year>;<volume>178</volume>:<fpage>477</fpage>–<lpage>504</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1965.sp007638" xlink:type="simple">10.1113/jphysiol.1965.sp007638</ext-link></comment> <object-id pub-id-type="pmid">5827909</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref048">
<label>48</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cramer</surname> <given-names>H</given-names></name>. <source>Mathematical methods of statistics</source>. <publisher-name>Princeton University Press</publisher-name>; <year>1946</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005497.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rao</surname> <given-names>CR</given-names></name>. <article-title>Information and the accuracy attainable in the estimation of statistical parameters</article-title>. <source>Bull Calcutta Math Soc</source>. <year>1945</year>;<volume>37</volume>:<fpage>81</fpage>–<lpage>89</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005497.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schneidman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Still</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Network Information and Connected Correlations</article-title>. <source>Phys Rev Lett</source>. <year>2003</year>;<volume>91</volume>(<issue>23</issue>):<fpage>238701</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.91.238701" xlink:type="simple">10.1103/PhysRevLett.91.238701</ext-link></comment> <object-id pub-id-type="pmid">14683220</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Adibi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McDonald</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Clifford</surname> <given-names>CWG</given-names></name>, <name name-style="western"><surname>Arabzadeh</surname> <given-names>E</given-names></name>. <article-title>Adaptation improves neural coding efficiency despite increasing correlations in variability</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>:<fpage>2108</fpage>–<lpage>2120</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3449-12.2013" xlink:type="simple">10.1523/JNEUROSCI.3449-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23365247</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Macke</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>Generating spike trains with specified correlation coefficients</article-title>. <source>Neural Comput</source>. <year>2009</year>;<volume>21</volume>(<issue>2</issue>):<fpage>397</fpage>–<lpage>423</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.02-08-713" xlink:type="simple">10.1162/neco.2008.02-08-713</ext-link></comment> <object-id pub-id-type="pmid">19196233</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Macke</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Opper</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>Common Input Explains Higher-Order Correlations and Entropy in a Simple Model of Neural Population Activity</article-title>. <source>Phys Rev Lett</source>. <year>2011</year>;<volume>106</volume>(<issue>20</issue>):<fpage>208102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.106.208102" xlink:type="simple">10.1103/PhysRevLett.106.208102</ext-link></comment> <object-id pub-id-type="pmid">21668265</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Nakahara</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Santos</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Nikolic</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Plenz</surname> <given-names>D</given-names></name>. <article-title>Higher-order correlations characterized in cortical activity</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>:<fpage>17514</fpage>–<lpage>17526</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3127-11.2011" xlink:type="simple">10.1523/JNEUROSCI.3127-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22131413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Amari</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Nakahara</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sakai</surname> <given-names>Y</given-names></name>. <article-title>Synchronous firing and higher-order interactions in neuron pool</article-title>. <source>Neural Comput</source>. <year>2003</year>;<volume>15</volume>(<issue>1</issue>):<fpage>127</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976603321043720" xlink:type="simple">10.1162/089976603321043720</ext-link></comment> <object-id pub-id-type="pmid">12590822</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>. <article-title>Near-maximum entropy models for binary neural representations of natural images</article-title>, <name name-style="western"><surname>Platt</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Koller</surname> <given-names>D.</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>Y.</given-names></name>, <name name-style="western"><surname>Roweis</surname> <given-names>S.</given-names></name>, Editors. <source>Advances in neural information processing systems</source>. <year>2008</year>;<volume>20</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1005497.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Leen</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>E</given-names></name>. <article-title>A simple mechanism for beyond-pairwise correlations in integrate-and-fire neurons</article-title>. <source>J Math Neurosci</source>. <year>2015</year>;<volume>5</volume>:<fpage>17</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/s13408-015-0030-9" xlink:type="simple">10.1186/s13408-015-0030-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barlow</surname> <given-names>H</given-names></name>. <article-title>Redundancy reduction revisited</article-title>. <source>Network-Comput Neural Syst</source>. <year>2001</year>;<volume>12</volume>(<issue>3</issue>):<fpage>241</fpage>–<lpage>253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/net.12.3.241.253" xlink:type="simple">10.1080/net.12.3.241.253</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Not noisy, just wrong: the role of suboptimal inference in behavioral variability</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>(<issue>1</issue>):<fpage>30</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.03.016" xlink:type="simple">10.1016/j.neuron.2012.03.016</ext-link></comment> <object-id pub-id-type="pmid">22500627</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kanitscheider</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Coen-Cagli</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Origin of information-limiting noise correlations</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2015</year>;<volume>112</volume>:<fpage>E6973</fpage>–<lpage>E6982</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1508738112" xlink:type="simple">10.1073/pnas.1508738112</ext-link></comment> <object-id pub-id-type="pmid">26621747</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Graf</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Decoding the activity of neuronal populations in macaque primary visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>:<fpage>239</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2733" xlink:type="simple">10.1038/nn.2733</ext-link></comment> <object-id pub-id-type="pmid">21217762</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005497.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>A Fast and Simple Population Code for Orientation in Primate V1</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>:<fpage>10618</fpage>–<lpage>10626</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1335-12.2012" xlink:type="simple">10.1523/JNEUROSCI.1335-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22855811</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>