<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-01812</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003258</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Predictive Coding of Dynamical Variables in Balanced Spiking Networks</article-title>
<alt-title alt-title-type="running-head">Predictive Coding in Balanced Spiking Networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Boerlin</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Machens</surname><given-names>Christian K.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Denève</surname><given-names>Sophie</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Group for Neural Theory, Département d'Études Cognitives, École Normale Supérieure, Paris, France</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Champalimaud Neuroscience Programme, Champalimaud Centre for the Unknown, Lisbon, Portugal</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">sophie.deneve@ens.fr</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: CKM SD. Performed the experiments: MB SD. Analyzed the data: MB. Wrote the paper: CKM SD.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>11</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>14</day><month>11</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>11</issue>
<elocation-id>e1003258</elocation-id>
<history>
<date date-type="received"><day>14</day><month>11</month><year>2012</year></date>
<date date-type="accepted"><day>21</day><month>8</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Boerlin et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e.g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Two observations about the cortex have puzzled and fascinated neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks representing information reliably and with a small number of spikes. To achieve such efficiency, spikes of individual neurons must communicate prediction errors about a common population-level signal, automatically resulting in balanced excitation and inhibition and highly variable neural responses. We illustrate our approach by focusing on the implementation of linear dynamical systems. Among other things, this allows us to construct a network of spiking neurons that can integrate input signals, yet is robust against many perturbations. Most importantly, our approach shows that neural variability cannot be equated to noise. Despite exhibiting the same single unit properties as other widely used network models, our balanced networks are orders of magnitudes more reliable. Our results suggest that the precision of cortical representations has been strongly underestimated.</p>
</abstract>
<funding-group><funding-statement>This work was supported by a DFG Emmy-Noether grant and an ANR Chaire d'Excellence to CKM, as well as EU grants BACS FP6-IST-027140 and BIND MECT-CT-20095-024831 to SD. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="16"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Neural systems need to integrate, store, and manipulate sensory information before acting upon it. Various neurophysiological and psychophysical experiments have provided examples of how these feats are accomplished in the brain, from the integration of sensory stimuli to decision-making <xref ref-type="bibr" rid="pcbi.1003258-Gold1">[1]</xref>, from the short-term storage of information <xref ref-type="bibr" rid="pcbi.1003258-Major1">[2]</xref> to the generation of movement sequences <xref ref-type="bibr" rid="pcbi.1003258-Wolpert1">[3]</xref>. At the same time, it has been far more difficult to pin down the precise mechanisms underlying these functions.</p>
<p>A lot of research on neural mechanisms has focused on studying neural networks in the framework of attractor dynamics <xref ref-type="bibr" rid="pcbi.1003258-Hertz1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Wang1">[6]</xref>. These models generally assume that the system's state variables are represented by the instantaneous firing rates of neurons. While quite successful in reproducing some features of electrophysiological data, these models have had a hard time reproducing the irregular, Poisson-like statistics of cortical spike trains. A common assumption is that the random nature of spike times is averaged out over larger populations of neurons or longer periods of time <xref ref-type="bibr" rid="pcbi.1003258-Tolhurst1">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Machens1">[10]</xref>. However, the biophysical sources of noise in individual neurons are insufficient to explain such variability <xref ref-type="bibr" rid="pcbi.1003258-Mainen1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Faisal1">[13]</xref>.</p>
<p>Several researchers have therefore suggested that irregular spike timing arises as a consequence of network dynamics <xref ref-type="bibr" rid="pcbi.1003258-Shadlen1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Softky1">[14]</xref>. Indeed, large networks of leaky integrate-and-fire (LIF) neurons with balanced excitation and inhibition can be “chaotic” and generate asynchronous and Poisson-like firing statistics <xref ref-type="bibr" rid="pcbi.1003258-vanVreeswijk1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Tchumatchenko1">[18]</xref>. While these studies explain how relatively deterministic single units can generate similar statistical properties as random spike generators in rate models, they generally do not clarify how particular computations can be carried out, nor do they fundamentally answer why the brain would be operating in such a regime.</p>
<p>Here we show that the properties of balanced networks can be derived from a single efficiency principle, which in turn allows us to design balanced networks that perform a wide variety of computations. We start from the assumption that dynamical variables are encoded such that they can be extracted from output spike trains by simple synaptic integration. We then specify a loss function that measures the system's performance with respect to an idealized dynamical system. We prescribe that neurons should only fire a spike if that decreases the loss function. From these assumptions, we derive a recurrent network of LIF neurons that is able to implement any linear dynamical system. We show that neurons in our network track a prediction error in their membrane potential and only fire a spike if that prediction error exceeds a certain value, a form of predictive coding.</p>
<p>Our work shows how the ideas of predictive coding with spikes, first laid out within a Bayesian framework <xref ref-type="bibr" rid="pcbi.1003258-Deneve1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Boerlin1">20</xref>, can be generalized to design spiking neural networks that implement arbitrary linear dynamical systems. Such multivariate dynamical systems are quite powerful and have remained a mainstay of control-engineering for real-world systems <xref ref-type="bibr" rid="pcbi.1003258-Leigh1">[21]</xref>. Importantly, the networks maintain a tight balance between the excitatory and inhibitory currents received by each unit, as has been reported at several levels of cortical processing <xref ref-type="bibr" rid="pcbi.1003258-Wehr1">[22]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Gentet1">[26]</xref>. The spike trains are asynchronous and irregular. However, this variability is not noise: The neural population essentially acts as a deterministic “super-unit”, tracking the variable with quasi-perfect accuracy while each individual neuron appears to behave stochastically. We illustrate our approach and its usefulness with several biologically relevant examples.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Assumptions</title>
<p>Our basic model strategy is represented in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 A</xref>. Let us consider a linear dynamical system describing the temporal evolution of a vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e001" xlink:type="simple"/></inline-formula> dynamical variables, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e002" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003258.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e003" xlink:type="simple"/><label>(1)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e004" xlink:type="simple"/></inline-formula> is the state transition matrix, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e005" xlink:type="simple"/></inline-formula> are time-varying, external inputs or command variables. We want to build a neural network composed of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e006" xlink:type="simple"/></inline-formula> neurons, taking initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e007" xlink:type="simple"/></inline-formula> and commands <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e008" xlink:type="simple"/></inline-formula> as inputs, and reproducing the temporal trajectory of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e009" xlink:type="simple"/></inline-formula>. Specifically, we want to be able to read an estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e010" xlink:type="simple"/></inline-formula> of the dynamical variable from the network's spike trains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e011" xlink:type="simple"/></inline-formula>. These output spike trains are given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e012" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e013" xlink:type="simple"/></inline-formula> is the time of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e014" xlink:type="simple"/></inline-formula> spike in neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e015" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003258-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g001</object-id><label>Figure 1</label><caption>
<title>Spike-based implementation of linear dynamical systems.</title>
<p>(A) Structure of the network: the neurons receive an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e016" xlink:type="simple"/></inline-formula>, scaled by feedforward weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e017" xlink:type="simple"/></inline-formula>, which is internally processed through fast and slow recurrent connections, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e018" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e019" xlink:type="simple"/></inline-formula>, to yield firing rates that can be read out by a linear decoder with weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e020" xlink:type="simple"/></inline-formula> to yield estimates of the dynamical variables, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e021" xlink:type="simple"/></inline-formula>. Connections: red, excitatory; blue, inhibitory; filled circle endpoints, fast; empty diamond endpoints, slow. (B) Exemplary, effective postsynaptic potentials between neurons from two different networks. (C) Sensory integrator network for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e022" xlink:type="simple"/></inline-formula> (perfect integrator). Top panel: Sensory stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e023" xlink:type="simple"/></inline-formula> (blue line). Before <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e024" xlink:type="simple"/></inline-formula>, the neurons integrate a slightly noisy version of the stimulus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e025" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e026" xlink:type="simple"/></inline-formula> is unit-variance Gaussian noise. At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e027" xlink:type="simple"/></inline-formula>s (downward pointing arrow) all inputs to the network cease (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e028" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e029" xlink:type="simple"/></inline-formula>). Middle panel: Raster plot of 140 model units for a given trial. Top 70 neurons have negative kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e030" xlink:type="simple"/></inline-formula>), and bottom 70 neurons have positive kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e031" xlink:type="simple"/></inline-formula>). Each dot represents a spike. Thin blue line: state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e032" xlink:type="simple"/></inline-formula>. Thick red line: Network estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e033" xlink:type="simple"/></inline-formula>. Bottom panel: Mean firing rate (over 500 presentations of identical stimuli <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e034" xlink:type="simple"/></inline-formula>, but with different instantiations of the sensory noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e035" xlink:type="simple"/></inline-formula>) for the population of neurons with positive kernels (magenta) or negative kernels (green). (D) Same as C but for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e036" xlink:type="simple"/></inline-formula>Hz. Parameters in A–D: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e037" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e038" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e039" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e040" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e041" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e042" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e043" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e044" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e045" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e046" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e047" xlink:type="simple"/></inline-formula> (in C) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e048" xlink:type="simple"/></inline-formula> (in D). Simulation time step (Euler method) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e049" xlink:type="simple"/></inline-formula>msec. The noise parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e050" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e051" xlink:type="simple"/></inline-formula>, represent the standard deviation of the noise injected in each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e052" xlink:type="simple"/></inline-formula>ms time step.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g001" position="float" xlink:type="simple"/></fig>
<p>Our first assumption is that the estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e053" xlink:type="simple"/></inline-formula> is obtained by a weighted, leaky integration of the spike trains,<disp-formula id="pcbi.1003258.e054"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e054" xlink:type="simple"/><label>(2)</label></disp-formula>where the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e055" xlink:type="simple"/></inline-formula> matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e056" xlink:type="simple"/></inline-formula> contains the decoding or output weights of all neurons, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e057" xlink:type="simple"/></inline-formula> is the read-out's decay rate. Whenever neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e058" xlink:type="simple"/></inline-formula> fires, a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e059" xlink:type="simple"/></inline-formula>-function is added to its spike train, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e060" xlink:type="simple"/></inline-formula>. The integration of the respective delta-function contributes a decaying exponential kernel, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e061" xlink:type="simple"/></inline-formula>, weighted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e062" xlink:type="simple"/></inline-formula>, to each dynamical variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e063" xlink:type="simple"/></inline-formula>. This contribution can be interpreted as a simplified postsynaptical potential (PSP). The effect of a neuron's spike can be summarized by its weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e064" xlink:type="simple"/></inline-formula>, which we call the output kernel of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e065" xlink:type="simple"/></inline-formula>. Note that these weights correspond to the columns of the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e066" xlink:type="simple"/></inline-formula>. The estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e067" xlink:type="simple"/></inline-formula> can also be written as a weighted linear summation of the neuron's firing rates, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e068" xlink:type="simple"/></inline-formula>, if we define the time-varying firing rates of the neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e069" xlink:type="simple"/></inline-formula>, as<disp-formula id="pcbi.1003258.e070"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e070" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>Our second assumption is that the network minimizes the distance between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e071" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e072" xlink:type="simple"/></inline-formula> by optimizing over the spike times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e073" xlink:type="simple"/></inline-formula>, and not by changing the <italic>fixed</italic> output weight matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e074" xlink:type="simple"/></inline-formula>. This approach differs from the “liquid computing” approach in which recurrent networks have fixed, random connectivities while the decoding weights are learnt <xref ref-type="bibr" rid="pcbi.1003258-Maass1">[27]</xref>. In our case, the decoding weights are chosen a-priori. In order to track the temporal evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e075" xlink:type="simple"/></inline-formula> as closely as possible, the network minimizes the cumulative mean-squared error between the variable and its estimate, while limiting the cost in spiking. Thus, it minimizes the following cost function,<disp-formula id="pcbi.1003258.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e076" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e077" xlink:type="simple"/></inline-formula> denotes the Eucledian distance (or L2 norm), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e078" xlink:type="simple"/></inline-formula> the Manhattan distance (or L1 norm), which here is simply the sum over all firing rates, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e079" xlink:type="simple"/></inline-formula>. Parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e080" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e081" xlink:type="simple"/></inline-formula> control the cost-accuracy tradeoff. The linear cost term, controlled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e082" xlink:type="simple"/></inline-formula>, forces the network to perform the task with as few spikes as possible, whereas the quadratic cost term, controlled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e083" xlink:type="simple"/></inline-formula>, forces the network to distribute spikes more equally among neurons, as explained in Material and Methods.</p>
</sec><sec id="s2b">
<title>Network dynamics</title>
<p>To derive the network dynamics, we assume that the firing mechanism of the neurons performs a greedy minimization of the cost function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e084" xlink:type="simple"/></inline-formula>. More specifically, neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e085" xlink:type="simple"/></inline-formula> fires a spike whenever this results in a decrease of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e086" xlink:type="simple"/></inline-formula>, i.e., whenever <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e087" xlink:type="simple"/></inline-formula>. As explained in Material and Methods, this prescription gives rise to the firing rule<disp-formula id="pcbi.1003258.e088"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e088" xlink:type="simple"/><label>(5)</label></disp-formula>with<disp-formula id="pcbi.1003258.e089"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e089" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula id="pcbi.1003258.e090"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e090" xlink:type="simple"/><label>(7)</label></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e091" xlink:type="simple"/></inline-formula> is a time-varying variable, whereas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e092" xlink:type="simple"/></inline-formula> is a constant, we identify the former with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e093" xlink:type="simple"/></inline-formula>-th neuron's membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e094" xlink:type="simple"/></inline-formula>, and the latter with its firing threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e095" xlink:type="simple"/></inline-formula>.</p>
<p>In the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e096" xlink:type="simple"/></inline-formula>, the membrane potential of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e097" xlink:type="simple"/></inline-formula>-th neuron can be understood as the projection of the prediction error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e098" xlink:type="simple"/></inline-formula> onto the output kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e099" xlink:type="simple"/></inline-formula>. Whenever this projected prediction error exceeds a threshold, a new spike is fired, ensuring that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e100" xlink:type="simple"/></inline-formula> precisely tracks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e101" xlink:type="simple"/></inline-formula>. For finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e102" xlink:type="simple"/></inline-formula>, the membrane voltage measures a penalized prediction error. If the neuron is already firing at a high rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e103" xlink:type="simple"/></inline-formula>, only a correspondingly larger error will be able to exceed the threshold and lead to a spike.</p>
<p>To connect this firing rule with the desired network dynamics, <xref ref-type="disp-formula" rid="pcbi.1003258.e003">Eqn. (1)</xref>, we take the derivative of each neuron's membrane potential, <xref ref-type="disp-formula" rid="pcbi.1003258.e089">Eqn. (6)</xref>, and consider the limit of large networks (see Material and Methods) to obtain the differential equation<disp-formula id="pcbi.1003258.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e104" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e105" xlink:type="simple"/></inline-formula> is a leak term, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e106" xlink:type="simple"/></inline-formula> is a weight matrix of connectivity filters, explained below, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e107" xlink:type="simple"/></inline-formula> corresponds to a white “background noise” with unit-variance. The leak-term does not strictly follow from the derivation, but has been included for biological realism. A similar rationale holds for the noise term which we add to capture unavoidable sources of stochasticity in biological neurons due to channel noise, background synaptic input, etc. The differential equation then corresponds to a standard LIF neuron with leak term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e108" xlink:type="simple"/></inline-formula>, external, feedforward synaptic inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e109" xlink:type="simple"/></inline-formula>, recurrent synaptic inputs mediated through the weight matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e110" xlink:type="simple"/></inline-formula>, and a firing threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e111" xlink:type="simple"/></inline-formula>, as specified in <xref ref-type="disp-formula" rid="pcbi.1003258.e090">Eqn. (7)</xref>.</p>
<p>The weight matrix of connectivity filters is defined as<disp-formula id="pcbi.1003258.e112"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e112" xlink:type="simple"/><label>(9)</label></disp-formula>and contains both “fast” and “slow” lateral connections, given by the matrices<disp-formula id="pcbi.1003258.e113"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e113" xlink:type="simple"/><label>(10)</label></disp-formula><disp-formula id="pcbi.1003258.e114"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e114" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e115" xlink:type="simple"/></inline-formula> corresponds to the identity matrix. Accordingly, the connectivity of the network is entirely derived from the output weight matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e116" xlink:type="simple"/></inline-formula>, the desired dynamics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e117" xlink:type="simple"/></inline-formula>, and the penalty parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e118" xlink:type="simple"/></inline-formula>. Note that the diagonal elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e119" xlink:type="simple"/></inline-formula> implement a reset in membrane potential after each spike by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e120" xlink:type="simple"/></inline-formula>. With this self-reset, individual neurons become formally equivalent to LIF neurons. Whereas the linear penalty, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e121" xlink:type="simple"/></inline-formula>, influences only the thresholds of the LIF neurons, the quadratic penalty, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e122" xlink:type="simple"/></inline-formula>, influences both the thresholds, resets, and dynamics of the individual neurons, through its impact on the diagonal elements of the connectivity matrix.</p>
<p>Slow and fast lateral connections have typically opposite effects on postsynaptic neurons, and thereby different roles to play. The fast connections, or off-diagonal elements of the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e123" xlink:type="simple"/></inline-formula>, implement a competition among neurons with similar selectivity. If neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e124" xlink:type="simple"/></inline-formula> fires, the corresponding decreases in prediction errors (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e125" xlink:type="simple"/></inline-formula>) are conveyed to all other neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e126" xlink:type="simple"/></inline-formula>. Neurons with similar kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e127" xlink:type="simple"/></inline-formula> inhibit each other, while neurons with opposite kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e128" xlink:type="simple"/></inline-formula>) excite each other. This is schematized by the blue and red connections in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 A</xref>.</p>
<p>In contrast, the slow connections, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e129" xlink:type="simple"/></inline-formula>, implement a cooperation among neurons with similar selectivity. These connections predict the future trajectory of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e130" xlink:type="simple"/></inline-formula> (term “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e131" xlink:type="simple"/></inline-formula>”) but also compensate for the loss of information due to the decoder leak (term “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e132" xlink:type="simple"/></inline-formula>”). For example, when the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e133" xlink:type="simple"/></inline-formula> is static (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e134" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e135" xlink:type="simple"/></inline-formula>), these connections maintain persistent activity in the network, preventing the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e136" xlink:type="simple"/></inline-formula> from decaying back to zero (see below). Note that when the internal dynamics of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e137" xlink:type="simple"/></inline-formula> change on a slower time scale than the decoder (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e138" xlink:type="simple"/></inline-formula>), and if we neglect the cost term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e139" xlink:type="simple"/></inline-formula>, slow and fast connections have the same profile, (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e140" xlink:type="simple"/></inline-formula>), but opposite signs.</p>
<p>The combined effect of fast and slow connections yields the effective PSPs in our network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e141" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e142" xlink:type="simple"/></inline-formula>, which can be obtained by integrating <xref ref-type="disp-formula" rid="pcbi.1003258.e104">Eqn. (8)</xref> for a single spike. Two example PSPs are shown in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 B</xref>. We note that our network model may contain neurons that both inhibit and excite different targets, depending on the kernel sign, a violation of Dale's law. This problem can be addressed by creating separate cost functions for excitatory and inhibitory neurons, as laid out in full detail in <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>. Here, we simply interpret the resulting connectivity as the effective or functional connectivity of a network, akin to the types of connectivities arising in generalized linear models (GLMs) of neural networks <xref ref-type="bibr" rid="pcbi.1003258-Pillow1">[28]</xref>.</p>
</sec><sec id="s2c">
<title>Scaling and physical units</title>
<p>We now briefly consider how the above equations can be mapped onto realistic physical units. This consideration has the additional benefit that it clarifies how the network parameters scale with the number of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e143" xlink:type="simple"/></inline-formula> (see also Material and Methods). In order to express the network dynamics in biophysically relevant units, the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e144" xlink:type="simple"/></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1003258.e089">Eqn. (6)</xref>, and threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e145" xlink:type="simple"/></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1003258.e090">Eqn. (7)</xref>, have to be rescaled accordingly. We can obtain proper membrane potential units in mV if we apply the simple transformations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e146" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e147" xlink:type="simple"/></inline-formula>. In turn, we obtain the modified equations<disp-formula id="pcbi.1003258.e148"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e148" xlink:type="simple"/><label>(12)</label></disp-formula><disp-formula id="pcbi.1003258.e149"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e149" xlink:type="simple"/><label>(13)</label></disp-formula>and the modified dynamics<disp-formula id="pcbi.1003258.e150"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e150" xlink:type="simple"/><label>(14)</label></disp-formula>with a resting potential of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e151" xlink:type="simple"/></inline-formula>. Note that both the feedforward and recurrent connectivities change in this case. Specifically, we obtain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e152" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e153" xlink:type="simple"/></inline-formula>, and a similar expression for the noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e154" xlink:type="simple"/></inline-formula>. In turn, we can freely choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e155" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e156" xlink:type="simple"/></inline-formula> to find realistic units. For instance, we can fix the threshold at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e157" xlink:type="simple"/></inline-formula>, and the reset potential at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e158" xlink:type="simple"/></inline-formula>, which uniquely determines both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e159" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e160" xlink:type="simple"/></inline-formula> for each neuron. In the absence of linear costs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e161" xlink:type="simple"/></inline-formula>), the reset potential becomes simply <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e162" xlink:type="simple"/></inline-formula>.</p>
<p>When we increase the network size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e163" xlink:type="simple"/></inline-formula> while keeping the average firing rates and the read-out constant, we need to change the decoding kernels. Specifically, the decoding kernels need to scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e164" xlink:type="simple"/></inline-formula>. If we assume that the relative importance of the cost terms is held fixed for each neuron, then the original threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e165" xlink:type="simple"/></inline-formula> scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e166" xlink:type="simple"/></inline-formula>, and the original connectivities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e167" xlink:type="simple"/></inline-formula> similarly scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e168" xlink:type="simple"/></inline-formula>, compare <xref ref-type="disp-formula" rid="pcbi.1003258.e112">Eqns. (9</xref>–<xref ref-type="disp-formula" rid="pcbi.1003258.e114">11)</xref>. As a consequence, the rescaled synaptic weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e169" xlink:type="simple"/></inline-formula> do not scale in size when the network becomes larger or smaller. When considering the summation over the different input spike trains, we therefore see that all synaptic inputs into the network scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e170" xlink:type="simple"/></inline-formula>: the feedforward inputs, the slow recurrent input, and the fast recurrent inputs (the latter two are both contained in the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e171" xlink:type="simple"/></inline-formula>). The equal scaling of all inputs maintains the detailed balance of excitation and inhibition in the network.</p>
<p>An instructive case is given if we neglect the cost terms for a moment (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e172" xlink:type="simple"/></inline-formula>) in which case we obtain the following (rescaled) feedforward weights and connectivities:<disp-formula id="pcbi.1003258.e173"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e173" xlink:type="simple"/><label>(15)</label></disp-formula><disp-formula id="pcbi.1003258.e174"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e174" xlink:type="simple"/><label>(16)</label></disp-formula>Accordingly, the strength of the lateral connections is independent of the kernel norm. In contrast, the strength of the feed-forward connections scales with the inverse of the kernel norm. Since smaller kernels provide a more precise representation, the precision of the rescaled network, and its firing rates, are controlled entirely by its input gain.</p>
</sec><sec id="s2d">
<title>Sensory integration and sensory tracking</title>
<p>Once the dynamics and the decoders are chosen, <xref ref-type="disp-formula" rid="pcbi.1003258.e003">Eqn. (1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003258.e054">Eqn. (2)</xref>, the only free parameters of the model are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e175" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e176" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e177" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e178" xlink:type="simple"/></inline-formula>. The model presented previously can in principle implement any linear dynamical system. We will first illustrate the approach with the simplest linear dynamical system possible, a leaky integration of noisy sensory inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e179" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e180" xlink:type="simple"/></inline-formula> can be interpreted as the sensory stimulus while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e181" xlink:type="simple"/></inline-formula> represents shared sensory noise. The corresponding dynamical system, <xref ref-type="disp-formula" rid="pcbi.1003258.e003">Eqn. (1)</xref>, is then given by<disp-formula id="pcbi.1003258.e182"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e182" xlink:type="simple"/><label>(17)</label></disp-formula>The integrated sensory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e183" xlink:type="simple"/></inline-formula> is a scalar (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e184" xlink:type="simple"/></inline-formula>) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e185" xlink:type="simple"/></inline-formula> represents the leak of the sensory integrator.</p>
<p>For a completely homogeneous network, in which the output kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e186" xlink:type="simple"/></inline-formula> of all neurons are the same, we can solve the equations analytically which is shown in <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>. A slightly more interesting case is shown in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C,D</xref>, which illustrate network dynamics for two different choices of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e187" xlink:type="simple"/></inline-formula>. Here we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e188" xlink:type="simple"/></inline-formula> neurons, half of them with positive kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e189" xlink:type="simple"/></inline-formula>), and the other half with negative kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e190" xlink:type="simple"/></inline-formula>). Neurons with positive kernels fire when variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e191" xlink:type="simple"/></inline-formula> is positive or increases, while neurons with negative kernels fire when the variable is negative or decreases. Moreover, we set the cost terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e192" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e193" xlink:type="simple"/></inline-formula> at small values, ensuring that our objective function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e194" xlink:type="simple"/></inline-formula> is dominated by the estimation error, compare <xref ref-type="disp-formula" rid="pcbi.1003258.e076">Eqn. (4)</xref>. As a consequence, the estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e195" xlink:type="simple"/></inline-formula> closely tracks the true variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e196" xlink:type="simple"/></inline-formula>. Albeit small, the cost terms are crucial for generating biologically realistic spike trains. Without them, a single neuron may for example fire at extremely high rates while all others stay completely silent. The regularizing influence of the cost terms is described in more detail in <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>.</p>
<p>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e197" xlink:type="simple"/></inline-formula>, the network is a perfect integrator of a noisy sensory signal. The neural activities resemble the firing rates of LIP neurons that integrate sensory information during a slow motion-discrimination task <xref ref-type="bibr" rid="pcbi.1003258-Gold1">[1]</xref>. In the absence of sensory stimulation, the network sustains a constant firing rate (<xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref> after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e198" xlink:type="simple"/></inline-formula>sec), similar to line attractor networks <xref ref-type="bibr" rid="pcbi.1003258-Seung1">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Machens2">[31]</xref>. In fact, as long as the dynamics of the system are slower than the decoder (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e199" xlink:type="simple"/></inline-formula>), the instantaneous firing rates approximate a (leaky) integration of the sensory signals. On the other hand, if the system varies faster than the decoder (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e200" xlink:type="simple"/></inline-formula>), then neural firing rates track the sensory signal, and model neurons have transient responses at the start or end of sensory stimulation, followed by a decay to a lower sustained rate (<xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 D</xref>). These responses are similar to the adaptive and transient responses observed in most sensory areas.</p>
<p>The overall effect of the lateral connections depends on the relative time scales of the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e201" xlink:type="simple"/></inline-formula> and the decoder <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e202" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 B</xref>). For neurons with similar selectivity (or equal read-out kernels, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e203" xlink:type="simple"/></inline-formula>), the postsynaptic potentials are given by (assuming <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e204" xlink:type="simple"/></inline-formula>),<disp-formula id="pcbi.1003258.e205"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e205" xlink:type="simple"/><label>(18)</label></disp-formula>For neurons with opposite read-out kernels, we obtain just a sign reversal. When (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e206" xlink:type="simple"/></inline-formula>), the interplay of fast inhibition with slower excitation results in a bi-phasic interaction between neurons of similar selectivity (<xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 B</xref>, left). Moreover, the network activity persists after the disappearance of the stimulus. In the extreme case of the perfect integrator (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e207" xlink:type="simple"/></inline-formula>), the temporal integral of this PSP is exactly zero, which explains how the mean network activity can remain perfectly stable (neither increase nor decrease) in the absence of any sensory stimulation. In contrast, lateral interactions are entirely inhibitory when the network tracks the stimulus on a faster time scale than the decoder (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e208" xlink:type="simple"/></inline-formula>, <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 B</xref>, right). The dominance of lateral inhibition explains the transient nature of the network responses and the absence of persistent activity.</p>
<p>Other response properties of the model units are illustrated in <xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2</xref>. We define the tuning curves of the neurons as the mean spike count in response to a 1 s presentation of a constant stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e209" xlink:type="simple"/></inline-formula>. Firing rates monotonically increase (for positive kernels) or decrease (for negative kernels) as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e210" xlink:type="simple"/></inline-formula> and are rectified at zero, resulting in rectified linear tuning curves (<xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 A</xref>). Since all neurons have identical kernels (i.e. all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e211" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e212" xlink:type="simple"/></inline-formula>), neurons with the same kernel signs have identical tuning curves. However, such a homogeneous network is rather implausible since it assumes all-to-all lateral connectivity with identical weights, so that all units in the network receive exactly the same synaptic input and have the same membrane potential.</p>
<fig id="pcbi-1003258-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g002</object-id><label>Figure 2</label><caption>
<title>Response properties of the sensory integrator.</title>
<p>(A) Tuning curves to variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e213" xlink:type="simple"/></inline-formula> for the network with uniform kernels. Plain line: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e214" xlink:type="simple"/></inline-formula>. Dashed line: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e215" xlink:type="simple"/></inline-formula>. Parameters are as in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref>. Tuning curves were obtained by providing a noiseless (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e216" xlink:type="simple"/></inline-formula>) sensory input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e217" xlink:type="simple"/></inline-formula> of various strength during the first 250 ms, then measuring sustained firing in the absence of inputs during the next 1000 ms. The response shown is averaged over 500 trials. (B) Example tuning curves for the inhomogeneous network (Plain lines: all components of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e218" xlink:type="simple"/></inline-formula> positive. Dashed lines: all elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e219" xlink:type="simple"/></inline-formula> negative). Parameters are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e220" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e221" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e222" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e223" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e224" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e225" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e226" xlink:type="simple"/></inline-formula> is a uniform distribution within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e227" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e228" xlink:type="simple"/></inline-formula> is a binomial distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e229" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e230" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e231" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e232" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e233" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e234" xlink:type="simple"/></inline-formula> = 0, based on a simulation with the Euler method and time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e235" xlink:type="simple"/></inline-formula>msec. (C) Inter-spike interval distribution for a typical unit (inhomogeneous network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e236" xlink:type="simple"/></inline-formula>). ISI distribution is measured during “persistent activity” in the absence of sensory stimulation (firing rate is constant at 5 Hz). Red lines show the prediction from a Poisson process with the same rate. (D) Mean cross-correlogram for a pair of units with the same kernel sign (inhomogeneous network). Probability of a spike in unit 1 is plotted at different delays from a spike in unit 2.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g002" position="float" xlink:type="simple"/></fig>
<p>To move to more realistic and heterogeneous networks, we can choose randomized decoding kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e237" xlink:type="simple"/></inline-formula>. Even then, however, the connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e238" xlink:type="simple"/></inline-formula> is strongly constrained. For negligible costs, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e239" xlink:type="simple"/></inline-formula>, the weight matrix has rank one (since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e240" xlink:type="simple"/></inline-formula>). A lot more flexibility can be introduced in the network connections by simultaneously tracking <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e241" xlink:type="simple"/></inline-formula> variables with identical dynamics and identical control <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e242" xlink:type="simple"/></inline-formula>, rather than a single scalar variable. Thus the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e243" xlink:type="simple"/></inline-formula> and the kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e244" xlink:type="simple"/></inline-formula> have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e245" xlink:type="simple"/></inline-formula> dimensions and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e246" xlink:type="simple"/></inline-formula>. We then define the actual network output, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e247" xlink:type="simple"/></inline-formula>, as the mean of those <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e248" xlink:type="simple"/></inline-formula> variables (simply obtained by summing all network outputs). The network estimation error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e249" xlink:type="simple"/></inline-formula>, is an upper bound on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e250" xlink:type="simple"/></inline-formula>, ensuring similar performance as before (see <xref ref-type="fig" rid="pcbi-1003258-g003">Fig. 3</xref>). Importantly, we can choose the output kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e251" xlink:type="simple"/></inline-formula> to fit connectivity constraints imposed by biology. For instance, the output kernels can be made random and sparse (i.e. with many zero elements), resulting in random and sparse (but symmetrical) connection matrices. In such a network, the tuning curves are still rectified-linear, but have different gains for different neurons (<xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 B</xref>).</p>
<fig id="pcbi-1003258-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g003</object-id><label>Figure 3</label><caption>
<title>Response of the inhomogeneous integrator network.</title>
<p>Same format as in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref>. The network is entirely deterministic (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e252" xlink:type="simple"/></inline-formula>). Top panel: sensory input (blue line). Before <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e253" xlink:type="simple"/></inline-formula>, the sensory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e254" xlink:type="simple"/></inline-formula> is corrupted by sensory noise with variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e255" xlink:type="simple"/></inline-formula>. Sensory input and sensory noise stop after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e256" xlink:type="simple"/></inline-formula>s, at which point the network is entirely driven by its own internal and deterministic dynamics. The network is simulated twice using exactly the same initial conditions and input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e257" xlink:type="simple"/></inline-formula>. Up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e258" xlink:type="simple"/></inline-formula>s, the two simulations give exactly the same spike train as represented by the dots (deterministic network with identical inputs). At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e259" xlink:type="simple"/></inline-formula>s, a small perturbation is introduced in the second simulation (a single spike is delayed by 1 ms). The subsequent spike trains are completely re-shuffled by the network dynamics (First simulation: dots. Second simulation: circles). Simulation parameters are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e260" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e261" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e262" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e263" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e264" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e265" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e266" xlink:type="simple"/></inline-formula> is a uniform distribution within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e267" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e268" xlink:type="simple"/></inline-formula> is a binomial distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e269" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e270" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e271" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e272" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e273" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e274" xlink:type="simple"/></inline-formula>, based on a simulation with the Euler method and time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e275" xlink:type="simple"/></inline-formula>msec. Bottom panel shows the mean instantaneous firing rate for the population of neurons with positive kernels (magenta) and negative kernels (green) measured in an exponential time window with width 100 ms.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g003" position="float" xlink:type="simple"/></fig>
<p>Output spike trains of both homogeneous and inhomogeneous networks are asynchronous and highly variable from trial to trial (see raster plots in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C,D</xref> and <xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2</xref>). Fano factors (measured during periods of constant firing rates), CV1, and CV2, were all found to be narrowly distributed around one. The interspike interval (ISI) distribution was close to exponential (<xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 C</xref>). Moreover, noise correlations between neurons are extremely small and do not exceed 0.001 (noise correlations are defined as the cross-correlation coefficient of spike count in a time window of 1 s in response to a constant variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e276" xlink:type="simple"/></inline-formula>). Finally, analysis of auto and cross-correlograms reveals the presence of high-frequency oscillations at the level of the population (<xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 D</xref>). These high frequency oscillations are not visible on <xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 C</xref> since the size of the bin (5 ms) is larger than the period of the oscillations (1 ms). Note that if we add a realistic amount of jitter noise (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e277" xlink:type="simple"/></inline-formula>ms) to spike times, these high frequency oscillations disappear without affecting the response properties of the network.</p>
<p>In contrast to the output spike trains, the membrane potentials of different neurons are highly correlated, since neurons with similar kernels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e278" xlink:type="simple"/></inline-formula>) receive highly correlated feed-forward and lateral inputs (<xref ref-type="fig" rid="pcbi-1003258-g004">Fig. 4 A,B</xref>). In the homogeneous networks without quadratic cost (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e279" xlink:type="simple"/></inline-formula>), these inputs are even identical, resulting in membrane potentials that only differ by the background noise (<xref ref-type="fig" rid="pcbi-1003258-g004">Fig. 4 A</xref>). Despite these strong correlations of the membrane potentials, the neurons fire rarely and asynchronously. <xref ref-type="fig" rid="pcbi-1003258-g004">Fig. 4 C</xref> illustrates why this is the case: let us consider a population of neurons with identical output kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e280" xlink:type="simple"/></inline-formula>, maintaining an estimate of a constant positive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e281" xlink:type="simple"/></inline-formula> (top panel, blue line). Due to the decoder leak <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e282" xlink:type="simple"/></inline-formula>, the network needs to fire periodically in order to maintain its estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e283" xlink:type="simple"/></inline-formula> at the level of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e284" xlink:type="simple"/></inline-formula> (top panel, red line). However, the exact order at which the different neurons fire does not matter, since they all contribute equally. The period between two spikes can be called an “integration cycle”. Within one integration cycle, the prediction errors and thus the membrane potentials, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e285" xlink:type="simple"/></inline-formula>, rise for all neurons (bottom panel, red line). Since all kernels are identical, however, all neurons compute the same prediction error and will reach their firing thresholds at approximately the same time. Only chance (in this case, the background noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e286" xlink:type="simple"/></inline-formula>) will decide which neuron reaches threshold first. This first neuron is the only one firing in this integration cycle (middle panel, colored bars) since it immediately inhibits itself and all other neurons. This starts a new integration cycle. As a result of this mechanism, while the population of neurons fire at regular intervals (hence the high frequency oscillations in <xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 D</xref>) only one neuron fires in each cycle, and its identity is essentially random. The resulting variability has no impact on the network estimate, since all spike orders give the same output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e287" xlink:type="simple"/></inline-formula>. In the presence of a quadratic cost (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e288" xlink:type="simple"/></inline-formula>), neurons that did not fire recently have a higher probability of reaching threshold first (their membrane potential is not penalized by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e289" xlink:type="simple"/></inline-formula>). When the cost term is large compared to the background noise (i.e. when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e290" xlink:type="simple"/></inline-formula>, which is not the case in the example provided here), this tends to regularize the output spike trains and leads to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e291" xlink:type="simple"/></inline-formula>s smaller than 1. However, this regularization is not observed in inhomogeneous networks.</p>
<fig id="pcbi-1003258-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g004</object-id><label>Figure 4</label><caption>
<title>Membrane potential profiles for the integrator networks.</title>
<p>(A) Homogeneous network. Example profiles for two neurons with identical kernels. Vertical line represents a spike in the red unit, plain horizontal line represents the firing threshold, and dashed horizontal line the reset potential. Values are interpreted in mV after rescaling the membrane potential (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e292" xlink:type="simple"/></inline-formula>mV and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e293" xlink:type="simple"/></inline-formula>mV). These profiles are taken from the simulation shown in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref>. (B) Inhomogeneous network. Membrane potential profiles for two neurons with strongly correlated kernels (i.e. large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e294" xlink:type="simple"/></inline-formula>) and no synaptic background noise (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e295" xlink:type="simple"/></inline-formula>). These profiles are taken from the simulation shown in <xref ref-type="fig" rid="pcbi-1003258-g003">Fig. 3</xref>. (C) Schema explaining the distribution of spikes across neurons in a homogeneous network (see text). (D) Same two units as in (B) shown for a longer period of time.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g004" position="float" xlink:type="simple"/></fig>
<p>The inhomogeneous network behaves similarly, except that all neurons do not receive the same inputs and do not reach threshold at the same time (<xref ref-type="fig" rid="pcbi-1003258-g004">Fig. 4 B</xref>). In this case, we can even dispense of the background noise (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e296" xlink:type="simple"/></inline-formula>) since fluctuations due to past network activity will result in a different neuron reaching threshold first in each cycle. The individual ups and downs caused by the synaptic inputs from other neurons will nonetheless appear like random noise when observing a single neuron (<xref ref-type="fig" rid="pcbi-1003258-g004">Fig. 4 B,D</xref>). Furthermore, even in this deterministic regime, the spike trains exhibit Poisson statistics. In fact, changing the timing of a single spike results in a total reordering of later spikes, suggesting that the network is chaotic (as illustrated in <xref ref-type="fig" rid="pcbi-1003258-g003">Fig. 3</xref>).</p>
</sec><sec id="s2e">
<title>2D arm controller</title>
<p>We now apply this approach to the tracking of a 2D point-mass arm based on an efferent motor command. The dynamical variable has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e297" xlink:type="simple"/></inline-formula> dimensions corresponding to the arm positions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e298" xlink:type="simple"/></inline-formula> and the arm velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e299" xlink:type="simple"/></inline-formula>. The arm dynamics are determined by elementary physics, so that<disp-formula id="pcbi.1003258.e300"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e300" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pcbi.1003258.e301"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e301" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e302" xlink:type="simple"/></inline-formula> is a 2D (control) force exerted onto the arm, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e303" xlink:type="simple"/></inline-formula> captures possible friction forces.</p>
<p>To simulate this system, we studied an arm moving from a central position towards different equidistant targets. This reaching out arm movement was obtained by “push-pull” control forces providing strong acceleration at the beginning of the movement, and fast deceleration at the end of the movement (<xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 A</xref>, top panel). As previously, the network predicts the trajectory of the arm perfectly based on the forces exerted on it (<xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 A</xref>, bottom panel; we again use relatively small cost terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e304" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e305" xlink:type="simple"/></inline-formula>). The resulting spike trains are asynchronous, decorrelated, and Poisson-like, with unpredictable spike times (rasters in <xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 A</xref>; Fano factor and CVs close to 1). The membrane potential of neurons with similar kernels are correlated while output spike trains are asynchronous and decorrelated. The effective postsynaptic potentials have biphasic shapes reflecting the integrative nature of the network for small friction forces (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e306" xlink:type="simple"/></inline-formula>).</p>
<fig id="pcbi-1003258-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g005</object-id><label>Figure 5</label><caption>
<title>Spike-based implementation of a 2-D arm forward model.</title>
<p>(A) Network response for a reaching arm movement. Top panel: Control variables (force exerted on the arm in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e307" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e308" xlink:type="simple"/></inline-formula> axis). Bottom panel: raster plot for a sub-population of 140 neurons. Thin lines: Real arm state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e309" xlink:type="simple"/></inline-formula>; Thick lines: network estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e310" xlink:type="simple"/></inline-formula>. Thin and thick lines are perfectly superposed. Blue and green: positions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e311" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e312" xlink:type="simple"/></inline-formula>. Red and cyan: velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e313" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e314" xlink:type="simple"/></inline-formula>. (B) Tuning curve to direction for an example unit. Blue, Red and Magenta correspond respectively to arm speed of 0.2, 0.5, and 1 m/s, as represented by the inlaid schemata. (C) Tuning curves to direction (same neuron as in B) tested at 3 different arm starting position. Blue, Red and magenta correspond to arm position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e315" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e316" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e317" xlink:type="simple"/></inline-formula>. (D) Direction tuning at 3 different arm positions for another example unit (same legend as C). (E) Schema explaining the tuning properties of model units. Dots in panels B and E represents the same arm state. Parameters in A–D: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e318" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e319" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e320" xlink:type="simple"/></inline-formula>, normalization constraint <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e321" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e322" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e323" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e324" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e325" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e326" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e327" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e328" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e329" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g005" position="float" xlink:type="simple"/></fig>
<p>To measure tuning curves in this “center out” reaching task, we varied the speed and direction of the movement, as well as the starting position of the arm. Neural activity was defined as the mean spike count measured during movement. As illustrated in <xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 B,C,D</xref>, instantaneous firing rates are modulated by arm position, velocity and force. We found that tuning curves to arm position are rectified linear, with varying thresholds and slopes (as in <xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 B</xref>). Such linear-rectified gain curves with posture have been reported in premotor and motor cortical areas <xref ref-type="bibr" rid="pcbi.1003258-Batista1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Snyder1">[33]</xref>. In contrast, tuning curves to circular symmetric variables such as movement direction or arm angle are bell-shaped (<xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 B,C,D</xref>). In addition, direction tuning curves are gain modulated by arm speed, such that responses are stronger for larger speed when the arm moves in the preferred direction, and weaker when the arm moves in the anti-preferred direction (<xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 B</xref>). Finally, arm positions have both an additive and a gain modulating effect on the tuning curve, and these modulation can be monotonically increasing (<xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 C</xref>) or decreasing (<xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 D</xref>) with arm position.</p>
<p>These observations have a simple geometric explanation, schematized in <xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 E</xref> for the velocity space, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e330" xlink:type="simple"/></inline-formula>. A neuron is maximally active (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e331" xlink:type="simple"/></inline-formula>; assuming <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e332" xlink:type="simple"/></inline-formula>) when its kernel (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e333" xlink:type="simple"/></inline-formula>, thick vector in <xref ref-type="fig" rid="pcbi-1003258-g005">Fig. 5 E</xref>) points in the direction of the derivative of the prediction error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e334" xlink:type="simple"/></inline-formula>. Since the decoder leak is faster than the arm dynamics, this error mostly points in the direction opposite to the leak, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e335" xlink:type="simple"/></inline-formula> (thin vectors). Within the velocity space, the kernel thus defines the neuron's preferred movement direction (dashed line and filled circles). The neurons is less often recruited when the arm moves away from the kernel's direction (empty circles), resulting in a bell-shaped tuning curve. Finally, since the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e336" xlink:type="simple"/></inline-formula> gets larger at larger speeds, more spikes are required to track the arm state resulting in a linear tuning to movement speed. The same reasoning applies for the position space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e337" xlink:type="simple"/></inline-formula>. These predictions are independent of the choice of kernels and are in direct agreement with experimental data from the pre-motor and motor cortices <xref ref-type="bibr" rid="pcbi.1003258-Batista1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Snyder1">[33]</xref>.</p>
</sec><sec id="s2f">
<title>Differentation and oscillation with heterogeneous networks</title>
<p>We chose to present a sensory integrator and an arm controller for their biological relevance and simplicity. However, any linear dynamical system can be implemented within our framework, and our networks are not limited to performing integration. To illustrate the generality of the approach, we applied the framework to two additional examples. In <xref ref-type="fig" rid="pcbi-1003258-g006">Fig. 6 A</xref>, we simulated a “leaky differentiator” with a transition matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e338" xlink:type="simple"/></inline-formula>. This system of differential equations is designed so that the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e339" xlink:type="simple"/></inline-formula> approximates a temporal derivative of a command signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e340" xlink:type="simple"/></inline-formula>. The command signal, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e341" xlink:type="simple"/></inline-formula>, is shown in the top panel of <xref ref-type="fig" rid="pcbi-1003258-g006">Fig. 6 A</xref>; the input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e342" xlink:type="simple"/></inline-formula> is zero. We used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e343" xlink:type="simple"/></inline-formula> neurons with kernels drawn from a normal distribution, and then normalized to a constant norm of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e344" xlink:type="simple"/></inline-formula>. As in the other examples, the firing statistics are close to Poisson, with a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e345" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003258-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g006</object-id><label>Figure 6</label><caption>
<title>Other example networks, same format as <xref ref-type="fig" rid="pcbi-1003258-g003">Fig. 3</xref>.</title>
<p>(A) Neural implementation of a “leaky differentiator”. The network tracks two dynamical variables with a state transition matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e346" xlink:type="simple"/></inline-formula>. Top panel: command variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e347" xlink:type="simple"/></inline-formula>. (Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e348" xlink:type="simple"/></inline-formula> is zero.) Bottom panel: network response and estimates. Thick red and purple lines: Network estimates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e349" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e350" xlink:type="simple"/></inline-formula>. Thin blue lines: Variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e351" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e352" xlink:type="simple"/></inline-formula>. The variables and network estimates perfectly track each other, making the thin blue lines hard to see. Overlaid dots represent the corresponding output spike trains, with a different color for each neuron. (B) Neural implementation of a damped harmonic oscillator. The network tracks two dynamical variables with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e353" xlink:type="simple"/></inline-formula>. Format as in A. Simulation parameters for A and B: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e354" xlink:type="simple"/></inline-formula> 2-D vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e355" xlink:type="simple"/></inline-formula> were generated by drawing each coordinate from a normal distribution and normalizing the vectors to a constant norm, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e356" xlink:type="simple"/></inline-formula>. Other parameters were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e357" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e358" xlink:type="simple"/></inline-formula>ms, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e359" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e360" xlink:type="simple"/></inline-formula>Hz, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e361" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e362" xlink:type="simple"/></inline-formula>. Dots represent spike trains, one line per neuron, shown in black to improve visibility.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g006" position="float" xlink:type="simple"/></fig>
<p>In <xref ref-type="fig" rid="pcbi-1003258-g006">Fig. 6 B</xref>, we simulated a network that implements a damped harmonic oscillator. Here we chose a transition matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e363" xlink:type="simple"/></inline-formula>. The oscillator is initially kicked out of its resting state through a force given by the command signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e364" xlink:type="simple"/></inline-formula>, as plotted on the top panel. The input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e365" xlink:type="simple"/></inline-formula> is zero. We used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e366" xlink:type="simple"/></inline-formula> neurons with kernels drawn from a normal distribution, and normalized to a constant norm of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e367" xlink:type="simple"/></inline-formula>. The network tracks the position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e368" xlink:type="simple"/></inline-formula> and speed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e369" xlink:type="simple"/></inline-formula> of the damped oscillator until position and speed are too close to zero to allow a reliable approximation. The firing statistics of single units are again Poisson-like, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e370" xlink:type="simple"/></inline-formula>.</p>
<p>Note that in these two examples, the dynamics implemented by the network are faster than the decoder's time scale <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e371" xlink:type="simple"/></inline-formula>. Accordingly, our networks can track changes faster than the time scale of the decoder. This speed-independence relies on a simple scheme: Spikes from neurons with positive kernel weight, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e372" xlink:type="simple"/></inline-formula>, represent instantaneous increases in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e373" xlink:type="simple"/></inline-formula>, whereas spikes from neurons with negative kernel weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e374" xlink:type="simple"/></inline-formula> represent instantaneous decreases in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e375" xlink:type="simple"/></inline-formula>. Even if the inter-spike interval is much shorter that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e376" xlink:type="simple"/></inline-formula>, the decoder can therefore still provide an efficient staircase approximation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e377" xlink:type="simple"/></inline-formula>. In conclusion, the temporal accuracy of these networks is not limited by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e378" xlink:type="simple"/></inline-formula>, but by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e379" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>We have proposed a method for embedding any linear dynamical system in a recurrent network of LIF neurons. The network connectivity and spike generation are entirely derived from a single loss function which seeks to optimally place spikes so that the relevant information can be extracted by postsynaptic integration. Accordingly, the network structure follows exclusively from functional principles, and no extensive parameter searches are required. This approach implies in particular that neurons share information in a smart way, yet fire almost randomly at the level of the single cell.</p>
<p>We also included a cost term in the error function, <xref ref-type="disp-formula" rid="pcbi.1003258.e076">Eqn. (4)</xref>. Due to this cost term, the network finds a solution minimizing the metabolic cost associated with high spike counts. Both linear and quadratic cost terms regularize the firing rate and make the network robust against artefacts such as high firing rates that may be caused by the greedy spiking mechanism (see <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>). Further generalizations or modifications of these predictive coding principles may eventually help to explain other biophysical or electrophysiological phenomena of the brain.</p>
<sec id="s3a">
<title>Relation to other approaches</title>
<p>Our current work both generalizes and modifies our earlier work in which we applied the principle of predictive coding with spikes to a Bayesian inference problem <xref ref-type="bibr" rid="pcbi.1003258-Boerlin1">[20]</xref>. This model tracked a log-probability distribution and implemented a non-linear drift-diffusion model, rather than a generic linear differential equation. In addition, we here introduced cost terms which provided us with greater flexibility in regulating and controlling the dynamics of the spiking networks.</p>
<p>A quite general framework for designing networks of neurons that implement arbitrary dynamical systems has previously been described in the “neuro-engineering” approach <xref ref-type="bibr" rid="pcbi.1003258-Eliasmith1">[30]</xref>. This approach relies on linearly combining the non-linear rate transfer function of LIF neurons. In its essence, the method is therefore based on firing rates, and makes few predictions about the spiking statistics of cortical neurons. A recently developed model, the “ReFiRe network” <xref ref-type="bibr" rid="pcbi.1003258-Druckmann1">[34]</xref> provides a recipe for designing networks maintaining stable memories, and shares some of the features of our networks. Just as the neuro-engineering framework, however, its design is essentially based on firing rates.</p>
<p>Here we have designed a network based on the principle of predictive coding with spikes. Even though indistinguishable from older models on the single cell level, our work is different in several important respects. A first major difference of our approach is that it predicts a detailed balance between excitation and inhibition, rather than imposing it upfront. This balance follows from the attempt of the network to minimize the loss function, <xref ref-type="disp-formula" rid="pcbi.1003258.e076">Eqn. (4)</xref>, which in turn implies that the membrane potential of neurons represents a prediction error and that neurons spike only when this prediction error exceeds a certain value—a form of predictive coding. Any increase in excitation causes an increase in prediction error, immediately compensated by an increase in inhibition to bring down the prediction error (and vice versa). This interplay causes a tight temporal correlation between excitation and inhibition at the time scale of the stimulus but also at a much finer time scale, within a single ISI (<xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 A</xref>). Note that this balance only holds when considering all inputs. In the leaky integrator, for instance, all lateral connections are inhibitory (<xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 B</xref>, right panel). However, the network is still globally balanced when taking into account the contribution from the feedforward connections. Such a tight balance between excitation and inhibition has been observed at several levels of cortical processing <xref ref-type="bibr" rid="pcbi.1003258-Wehr1">[22]</xref>–<xref ref-type="bibr" rid="pcbi.1003258-Gentet1">[26]</xref>.</p>
<fig id="pcbi-1003258-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003258.g007</object-id><label>Figure 7</label><caption>
<title>Distinguishing spiking codes from Poisson rate codes.</title>
<p>(A) Example profile of total excitatory current (red) and inhibitory current (blue) in a single unit on two different time scales (time scale of the stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e380" xlink:type="simple"/></inline-formula> and time scale of an inter-spike interval). Currents were convolved with a 2 ms exponential time window. (B) Response of the homogeneous integrator network (same parameters as in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref>). The input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e381" xlink:type="simple"/></inline-formula> is shown in the top panel. (C) Spike trains (dots), true state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e382" xlink:type="simple"/></inline-formula> (blue), and estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e383" xlink:type="simple"/></inline-formula> (red) for a rate model with the same slow connections and input as in B. Fast connections were removed and the greedy spiking rule was replaced by a random draw from an equivalent instantaneous firing rate. Four different trials are shown (four thick red lines) to illustrate the variability in the rate model's estimate. (D) Spike trains (dots), state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e384" xlink:type="simple"/></inline-formula> (blue) and estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e385" xlink:type="simple"/></inline-formula> (red) when each spike train is recorded from a different trial of the network shown in (B). (E) Estimation error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e386" xlink:type="simple"/></inline-formula>, as a function of the number of recorded neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e387" xlink:type="simple"/></inline-formula>, for a spiking network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e388" xlink:type="simple"/></inline-formula> neurons. For the blue line, all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e389" xlink:type="simple"/></inline-formula> neurons were recorded simultaneously, for the red line, each neuron is recorded in a different trial (red). The red line follows perfectly the prediction for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e390" xlink:type="simple"/></inline-formula> independent Poisson processes. Data are from an homogeneous integrator network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e391" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e392" xlink:type="simple"/></inline-formula>, other parameters as in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref>. (F) Effective connectivity filters of two randomly chosen pairs in the network, as measured through a GLM analysis. (G) Consequence of suddenly inactivating half of the active neurons for the network shown in B. Blue bar: unit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e393" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e394" xlink:type="simple"/></inline-formula> inactivated (membrane potential set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e395" xlink:type="simple"/></inline-formula>). Orange bar: units <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e396" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e397" xlink:type="simple"/></inline-formula> inactivated. Other parameters as in <xref ref-type="fig" rid="pcbi-1003258-g002">Fig. 2 B–D,F</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003258.g007" position="float" xlink:type="simple"/></fig>
<p>Accordingly, spike trains in our network usually resemble independent Poisson processes, with rates tuned to the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e398" xlink:type="simple"/></inline-formula>. We note that spike trains can also be more regular if the networks are smaller and the noise is not too large. A simple example is a network composed of a single neuron (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e399" xlink:type="simple"/></inline-formula>), for which we provide an analytical solution in <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>. Such a LIF neuron responds to a constant positive input with a perfectly regular spike train. In practice, regular firing is observed when only a few neurons are simultaneously co-active (e.g. for networks composed of less than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e400" xlink:type="simple"/></inline-formula> neurons). Firing becomes irregular when many neurons are co-active (e.g. for networks of several hundreds of neurons or more). Increasing synaptic background noise tends to make firing less regular, while increasing the quadratic metabolic costs makes firing more regular. However, for large networks, these effects are small and remain within the range of Fano-factors or CVs observed in cortex. The amount of regularity has no impact on the network performance.</p>
<p>Despite the variability observed in large networks, one cannot replace or approximate one of our spiking networks with an equivalent rate model composed of Poisson spike generators, a second major difference to other network models. This point is illustrated in <xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 B,C</xref> for the homogeneous integrator model, where we removed the fast connections in the network and replaced the integrate-and-fire dynamics by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e401" xlink:type="simple"/></inline-formula> independent Poisson processes (see Material and Methods). The performance of the resulting Poisson generator network is strongly degraded, even though it has the same instantaneous firing rates and slow connections as the LIF network.</p>
<p>We can quantify the benefit of using a deterministic firing rule compared to stochastic rate units by considering how the estimation error scales with the network size. The integrator model tracks the dynamical variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e402" xlink:type="simple"/></inline-formula> with a precision defined by the size of a kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e403" xlink:type="simple"/></inline-formula>. Estimation errors larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e404" xlink:type="simple"/></inline-formula> are immediately corrected by a spike. As the network size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e405" xlink:type="simple"/></inline-formula> increases, maintaining the same firing rates in single units requires that the kernels, and thus, the estimation error, scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e406" xlink:type="simple"/></inline-formula> (see Material and Methods). In contrast, the error made when averaging over a population of independent Poisson neurons diminishes with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e407" xlink:type="simple"/></inline-formula>. Intuitively, the predictive coding network achieves higher reliability because its neurons communicate shared information with each other via the fast synapses, whereas the independent Poisson neurons do not. The communicated information actively anti-correlates all spike trains, which, for networks composed of more than a dozen neurons, will be indistinguishable from the active decorrelation of pairwise spike trains that has recently been observed in vivo <xref ref-type="bibr" rid="pcbi.1003258-Ecker1">[35]</xref>. Therefore, the precision of the neural code cannot be interpolated from single-cell recordings in our network, and combining spike trains recorded in different trials results in a strong degradation of the estimate (<xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 D</xref>).</p>
<p>A third major difference between our network model and those proposed previously concerns the scaling of the network connectivity. Most previous approaches assumed sparse networks and weak connectivity in which the probability of connections (and/or connection strengths) scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e408" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e409" xlink:type="simple"/></inline-formula>. This weak connectivity leads to uncorrelated excitation and inhibition and thus neurons driven by random fluctuations in their input <xref ref-type="bibr" rid="pcbi.1003258-vanVreeswijk1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Roudi1">[36]</xref>. For comparison, the connectivity in our network is finite (once the membrane have been rescaled by the kernel norm to occupy a fixed range of voltage). Our approach is therefore reminiscent of a recent model with finite connection probability <xref ref-type="bibr" rid="pcbi.1003258-Renart1">[17]</xref>. As in our model, stronger connectivity leads to correlation between excitation and inhibition but uncorrelated spike trains. The strong network connectivity in turn swamps the membrane potential of each neuron with currents. The excitatory and inhibitory currents driving the neural response grow linearly with the number of neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e410" xlink:type="simple"/></inline-formula>, and are thus much larger than the membrane potential (prediction error) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e411" xlink:type="simple"/></inline-formula>, which is bounded by the (fixed) threshold. In turn, the leak currents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e412" xlink:type="simple"/></inline-formula> become negligible in large networks. For example, the integrator network in <xref ref-type="fig" rid="pcbi-1003258-g001">Fig. 1 C</xref> has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e413" xlink:type="simple"/></inline-formula> neurons and can maintain information for 100 s (it takes 100 seconds for the network activity to decay by half) despite the fact that the membrane time constant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e414" xlink:type="simple"/></inline-formula>) is only 0.1 s. Thus, according to our model, spiking neurons can fire persistently and thereby maintain information because their leaks are dwarfed by the currents they receive from recurrent connections.</p>
</sec><sec id="s3b">
<title>Network limitations</title>
<p>There are several non-trivial circumstances under which our network models may fail. First, we notice that the spiking rule that we derive amounts to a greedy optimization of the loss function. Future costs are not taken into account. This may cause problems in real neurons which can only communicate with time delays, but it may also cause problems when neurons have opposing kernels. For instance, two neurons with opposing kernels may become engaged in rapidly firing volleys of spikes, each trying in fast succession to decrease the error introduced by the previous spike from the other neuron (see <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>), a problem that we call the “ping-pong” effect. This effect can become a serious problem if the network dynamics is corrupted by strongly correlated noise, which may occur in the presence of synaptic failures. However, it is usually possible to diminish or eliminate this effect by increasing the spike count cost (see <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>).</p>
<p>Second, the leak term we introduced in the voltage equation provides only an approximation to the actual voltage equation (see Material and Methods). Specifically, the term we approximate is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e415" xlink:type="simple"/></inline-formula> times smaller than the other terms in the membrane potential dynamics. In practice, we can therefore always increase the network size to reach an acceptable level of performance. For a given network size, however, the approximation may break down when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e416" xlink:type="simple"/></inline-formula> becomes too large or when both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e417" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e418" xlink:type="simple"/></inline-formula> are too small (of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e419" xlink:type="simple"/></inline-formula>).</p>
<p>Third, the speed at which the linear dynamical system can evolve will be limited from a practical point of view, even in the limit of large networks. While the time scale of the decoder, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e420" xlink:type="simple"/></inline-formula> does not put any strict limitations on the speed (since spikes corresponding to positive and negative kernels can always provide an efficient stair-case approximation to any time-varying function), faster dynamics can only be obtained if the linear dynamical system compensates for the decoder filtering. This compensation or inversion process is a case of deconvolution, and bound to be severely limited in practice due to the noise inherent in all physical systems.</p>
<p>Finally, the network requires finely tuned lateral connections in order to balance excitation and inhibition (from feed-forward and lateral connections). In particular, the strength of the fast connections between two neurons corresponds to minus the correlation coefficient of their feed-forward connections (and thus, to their level of shared inputs). Whether such finely tuned motifs exist in biological networks is still an open question. We showed recently that fast lateral connections can be learnt using unsupervised Hebbian learning <xref ref-type="bibr" rid="pcbi.1003258-Bourdoukan1">[37]</xref>, suggesting that networks with the appropriate form of plasticity would be able to develop and maintain this tight balance. We note that the performance of the networks is quite sensitive to global perturbations of the balance between excitation and inhibition, an issue that we discuss in more detail in <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>.</p>
</sec><sec id="s3c">
<title>Experimental predictions</title>
<p>The most crucial work left to the future will be to test the predictions derived from this theory, three of which are described here. First, one could test how the decoding error scales with the numbers of simultaneously recorded neurons. A single unit in the model network (considered in isolation) is in fact exactly as reliable as a Poisson spike generator with the same rate. As the number of simultaneously recorded neurons increases, the decoding error initially decreases as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e421" xlink:type="simple"/></inline-formula>, similar to a Poisson rate model. However, as the number of neurons reaches a certain threshold (10% for the network models simulated here), the error from the spiking network decreases faster than predicted for a Poisson rate model (<xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 E</xref>). So far, single-unit recordings or multi-electrode recordings have only sampled from a very small subpart of the population, making it impossible to see this difference (and in turn, potentially leading to an under-estimation of the precision of the neural code). However, with newer techniques, such as dense multi-electrode arrays or optical imaging, as well as with focusing on smaller networks (such as the oculomotor integrator or insect systems), these model predictions are nowadays within experimental reach. We note that one has to carefully account for the effect of shared sensory noise (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e422" xlink:type="simple"/></inline-formula>) to see the predicted scaling effect. Shared noise (absent in <xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 E</xref>) introduces correlations between neurons and results in a saturation of the error with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e423" xlink:type="simple"/></inline-formula>. In our network, such a saturation would only be seen if there were limits to the sensory information available in the first place; saturation would not be seen as a consequence of neural noise or correlations (as proposed for example in <xref ref-type="bibr" rid="pcbi.1003258-Abbott1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Zohary1">[39]</xref>).</p>
<p>Second, one could look at the global interaction between neurons of similar selectivity, for example by applying a GLM model to the data <xref ref-type="bibr" rid="pcbi.1003258-Pillow1">[28]</xref>. The model predicts that neurons involved in slow integration tasks or working memory tasks should inhibit and excite each other at different delays. In particular, neurons with similar selectivities should be (paradoxically) negatively correlated at short delays. Thus, applying GLM analysis even on a small sub-population can uncover the effective PSPs caused by the lateral connections and, indirectly, the dynamical equation implemented by the network. <xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 F</xref> shows the GLM filters learnt from the inhomogeneous integrator network during working memory (i.e. sustained activity in the absence of sensory input). The analysis recovered the shape of the filters between neurons of similar kernels and opposite kernels, despite the fact that only 10 simultaneously recorded neurons (2.5% of the population) were used in this analysis.</p>
<p>Third, the spiking network is by essence self-correcting and will thus be resilient to lesions or many sudden perturbations (an exception being perturbations of the global balance of excitation and inhibition, see above). Equipping neural networks with such resilience or robustness has been a well-studied theoretical problem. For the specific example of the neural integrator, solutions range from constructing discrete attractor states <xref ref-type="bibr" rid="pcbi.1003258-Koulakov1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Goldman1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Cain1">[42]</xref>, adding adaptation or learning mechanisms to a network <xref ref-type="bibr" rid="pcbi.1003258-Moreau1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Renart2">[44]</xref>, or changing the nature of network feedback <xref ref-type="bibr" rid="pcbi.1003258-Goldman2">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003258-Li1">[46]</xref>. In the case of the neural integrator, the robustness of our network could likely be interpreted as a case of derivative feedback <xref ref-type="bibr" rid="pcbi.1003258-Li1">[46]</xref>.</p>
<p>While we know that biological neural networks are quite robust against partial lesions, their response to sudden, yet partial perturbations is less well known. For example, suddenly inactivating half of the active neurons in our sensory integrator increases the firing rates of the remaining neurons but has essentially no effect on the network performance (<xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 G</xref>). This instantaneous increase in firing rates without performance loss generates a strong prediction for our network model, a prediction that distinguishes our network from previously proposed solutions to the robustness problem. Indeed, as long as the pool of available kernels remains sufficient to track <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e424" xlink:type="simple"/></inline-formula>, and as long as increased firing rates are not affected by saturation, inactivation will not affect the network's computation. This prediction could be tested using for example optogenetic methods.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Derivation of the spiking rule, <xref ref-type="disp-formula" rid="pcbi.1003258.e088">Eqns. (5</xref>–<xref ref-type="disp-formula" rid="pcbi.1003258.e090">7)</xref></title>
<p>We here derive the network equations using compact matrix-vector notation. In <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>, we also consider the special case of a homogeneous network and a single neuron, for which the derivations are simpler. We consider the error function, <xref ref-type="disp-formula" rid="pcbi.1003258.e076">Eqn. (4)</xref>, which is given by<disp-formula id="pcbi.1003258.e425"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e425" xlink:type="simple"/><label>(21)</label></disp-formula>The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e426" xlink:type="simple"/></inline-formula>-th neuron should spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e427" xlink:type="simple"/></inline-formula> if<disp-formula id="pcbi.1003258.e428"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e428" xlink:type="simple"/><label>(22)</label></disp-formula>A spike by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e429" xlink:type="simple"/></inline-formula>-th neuron adds a single delta-function to its spike train. This additional spike enters the right-hand-side of the read-out equation, <xref ref-type="disp-formula" rid="pcbi.1003258.e054">Eqn. (2)</xref>. Integration of this extra delta-function amounts to adding a decaying exponential kernel, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e430" xlink:type="simple"/></inline-formula> to the read-out. Hence, if neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e431" xlink:type="simple"/></inline-formula> spikes at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e432" xlink:type="simple"/></inline-formula>, we have<disp-formula id="pcbi.1003258.e433"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e433" xlink:type="simple"/><label>(23)</label></disp-formula><disp-formula id="pcbi.1003258.e434"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e434" xlink:type="simple"/><label>(24)</label></disp-formula>where the latter equation describes the instantaneous change in firing rate due to the additional spike. Note that the standard Eucledian basis vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e435" xlink:type="simple"/></inline-formula> is a vector in which the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e436" xlink:type="simple"/></inline-formula>-th element is one, and all others are zero.</p>
<p>Each spike influences the read-out several time intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e437" xlink:type="simple"/></inline-formula> into the future. To see whether a spike leads to a decrease of the error function, we therefore need to look into the future (from time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e438" xlink:type="simple"/></inline-formula> onwards). For a future time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e439" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e440" xlink:type="simple"/></inline-formula>, the spiking rule in <xref ref-type="disp-formula" rid="pcbi.1003258.e428">Eqn. (22)</xref> translates into<disp-formula id="pcbi.1003258.e441"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e441" xlink:type="simple"/><label>(25)</label></disp-formula>We can expand the terms on the left-hand-side, and then eliminate identical terms on both sides. For that, we remind the reader that the relation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e442" xlink:type="simple"/></inline-formula> holds for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e443" xlink:type="simple"/></inline-formula> norm, whereas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e444" xlink:type="simple"/></inline-formula> holds for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e445" xlink:type="simple"/></inline-formula> norm in our case, since all elements (firing rates) are positive by definition. Hence we obtain<disp-formula id="pcbi.1003258.e446"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e446" xlink:type="simple"/><label>(26)</label></disp-formula>We rearrange the inequality by moving all terms that depend on the dynamical variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e447" xlink:type="simple"/></inline-formula>, the estimates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e448" xlink:type="simple"/></inline-formula>, or the firing rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e449" xlink:type="simple"/></inline-formula> to the left, and all other terms to the right, and we then multiply both sides by minus one, to obtain<disp-formula id="pcbi.1003258.e450"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e450" xlink:type="simple"/><label>(27)</label></disp-formula>Moving the kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e451" xlink:type="simple"/></inline-formula> to the front of the integrals and noticing that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e452" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e453" xlink:type="simple"/></inline-formula>, we obtain<disp-formula id="pcbi.1003258.e454"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e454" xlink:type="simple"/><label>(28)</label></disp-formula>The integral on the left-hand-side weights the influence of the spike, as given by the decaying exponential kernel, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e455" xlink:type="simple"/></inline-formula>, against the future development of the error signal, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e456" xlink:type="simple"/></inline-formula>, and firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e457" xlink:type="simple"/></inline-formula>. These future signals are unknown: while we may be able to extrapolate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e458" xlink:type="simple"/></inline-formula>, given its dynamical equation, we cannot safely extrapolate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e459" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e460" xlink:type="simple"/></inline-formula>, since this would require knowledge of all future spikes. We therefore choose a “greedy” approximation in which we only look a time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e461" xlink:type="simple"/></inline-formula> into the future. For the relevant times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e462" xlink:type="simple"/></inline-formula>, we can then approximate the integrands as constants so that (using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e463" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e464" xlink:type="simple"/></inline-formula>)<disp-formula id="pcbi.1003258.e465"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e465" xlink:type="simple"/><label>(29)</label></disp-formula>which is our decision to spike, and corresponds exactly to <xref ref-type="disp-formula" rid="pcbi.1003258.e088">Eqns. (5</xref>–<xref ref-type="disp-formula" rid="pcbi.1003258.e090">7)</xref>. We notice that the right-hand-side is a constant whereas the left-hand-side is a dynamical quantity which corresponds to the projection of the prediction error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e466" xlink:type="simple"/></inline-formula>, onto the output kernel of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e467" xlink:type="simple"/></inline-formula>-th neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e468" xlink:type="simple"/></inline-formula>, subtracted by a term depending on the firing rate of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e469" xlink:type="simple"/></inline-formula>-th neuron. Given this threshold rule, it seems only natural to identify the left-hand-side with the membrane voltage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e470" xlink:type="simple"/></inline-formula> of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e471" xlink:type="simple"/></inline-formula>-th neuron and the right-hand-side with its spiking threshold, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e472" xlink:type="simple"/></inline-formula>, which is what we did in the main text.</p>
</sec><sec id="s4b">
<title>Derivation of the voltage equation, <xref ref-type="disp-formula" rid="pcbi.1003258.e104">Eqn. (8)</xref></title>
<p>If we write the voltage of all neurons as one long vector, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e473" xlink:type="simple"/></inline-formula>, then we can write<disp-formula id="pcbi.1003258.e474"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e474" xlink:type="simple"/><label>(30)</label></disp-formula>We generally assume that there are more neurons than variables to represent so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e475" xlink:type="simple"/></inline-formula>. We also assume that the output kernel matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e476" xlink:type="simple"/></inline-formula>, has rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e477" xlink:type="simple"/></inline-formula>, and that the dynamical variables are not degenerate or linearly dependent on each other. In this case, the left pseudo-inverse of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e478" xlink:type="simple"/></inline-formula> exists and is given by<disp-formula id="pcbi.1003258.e479"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e479" xlink:type="simple"/><label>(31)</label></disp-formula>so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e480" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e481" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e482" xlink:type="simple"/></inline-formula>-matrix, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e483" xlink:type="simple"/></inline-formula> has size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e484" xlink:type="simple"/></inline-formula>. In turn, we can solve the voltage equation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e485" xlink:type="simple"/></inline-formula> by multiplying with the pseudo-inverse from the left so that<disp-formula id="pcbi.1003258.e486"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e486" xlink:type="simple"/><label>(32)</label></disp-formula>Taking the derivative of the voltages, we obtain<disp-formula id="pcbi.1003258.e487"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e487" xlink:type="simple"/><label>(33)</label></disp-formula>Replacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e488" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e489" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e490" xlink:type="simple"/></inline-formula> with their respective equations, <xref ref-type="disp-formula" rid="pcbi.1003258.e003">Eqns. (1</xref>–<xref ref-type="disp-formula" rid="pcbi.1003258.e070">3)</xref>, we obtain<disp-formula id="pcbi.1003258.e491"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e491" xlink:type="simple"/><label>(34)</label></disp-formula>In turn, we can replace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e492" xlink:type="simple"/></inline-formula> with <xref ref-type="disp-formula" rid="pcbi.1003258.e486">Eqn. (32)</xref> to obtain<disp-formula id="pcbi.1003258.e493"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e493" xlink:type="simple"/><label>(35)</label></disp-formula>Sorting some of the terms, and remembering that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e494" xlink:type="simple"/></inline-formula>, we obtain<disp-formula id="pcbi.1003258.e495"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e495" xlink:type="simple"/><label>(36)</label></disp-formula></p>
<p>To evaluate the relative importance of the different terms, we consider the limit of large networks, i.e., the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e496" xlink:type="simple"/></inline-formula>. First, we impose that the average firing rates of individual neurons should remain constant in this limit. Second, we require that the read-out does not change. Given the scaling of the firing rates, and since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e497" xlink:type="simple"/></inline-formula>, the output kernels must scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e498" xlink:type="simple"/></inline-formula>. Accordingly, the pseudo-inverse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e499" xlink:type="simple"/></inline-formula> scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e500" xlink:type="simple"/></inline-formula>. Finally, we need to choose how the cost terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e501" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e502" xlink:type="simple"/></inline-formula>, scale with respect to the read-out error. The linear and quadratic error terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e503" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e504" xlink:type="simple"/></inline-formula> scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e505" xlink:type="simple"/></inline-formula>. To avoid a contribution of the cost term increasing with network size, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e506" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e507" xlink:type="simple"/></inline-formula> should scale (at the least) with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e508" xlink:type="simple"/></inline-formula>. However, even if the cost terms scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e509" xlink:type="simple"/></inline-formula>, they will still dominate the network dynamics. For instance, the threshold, <xref ref-type="disp-formula" rid="pcbi.1003258.e090">Eqn. (7)</xref> becomes independent of the output kernel, while the contribution of fast lateral connections becomes negligible. In practice, this causes the performance to degrade quickly with increasing network size. A better choice is to require <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e510" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e511" xlink:type="simple"/></inline-formula> to scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e512" xlink:type="simple"/></inline-formula>, keeping the relative contribution of the kernel and cost to each neuron's dynamics fixed. With such scaling, large networks can still track the variable while the performance increase with network size.</p>
<p>Given the scaling of the output kernels and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e513" xlink:type="simple"/></inline-formula>, the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e514" xlink:type="simple"/></inline-formula> scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e515" xlink:type="simple"/></inline-formula>, compare <xref ref-type="disp-formula" rid="pcbi.1003258.e090">Eqn. (7)</xref>. In turn, since the voltage is bounded by the threshold from above (and bounded from below due to the existence of neurons with opposing kernels; see also below), the voltage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e516" xlink:type="simple"/></inline-formula> also scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e517" xlink:type="simple"/></inline-formula>. Accordingly, in a large network, the first, voltage-dependent term in <xref ref-type="disp-formula" rid="pcbi.1003258.e495">Eqn. (36)</xref> scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e518" xlink:type="simple"/></inline-formula>, as do the terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e519" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e520" xlink:type="simple"/></inline-formula>. In contrast, the terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e521" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e522" xlink:type="simple"/></inline-formula> represent a sum over all neurons in the population, and thus scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e523" xlink:type="simple"/></inline-formula>, similar to the inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e524" xlink:type="simple"/></inline-formula>. For large networks, we can therefore neglect the terms that scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e525" xlink:type="simple"/></inline-formula>. We note that none of the terms involving delta functions (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e526" xlink:type="simple"/></inline-formula>) can be neglected. We keep a generic leak term, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e527" xlink:type="simple"/></inline-formula>, although the term is essentially irrelevant in large networks, and may be detrimental in very small ones (e.g., less than 10 neurons). Hence, we approximate <xref ref-type="disp-formula" rid="pcbi.1003258.e495">Eqn. (36)</xref> by<disp-formula id="pcbi.1003258.e528"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e528" xlink:type="simple"/><label>(37)</label></disp-formula>with<disp-formula id="pcbi.1003258.e529"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e529" xlink:type="simple"/><label>(38)</label></disp-formula><disp-formula id="pcbi.1003258.e530"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e530" xlink:type="simple"/><label>(39)</label></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e531" xlink:type="simple"/></inline-formula> and since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e532" xlink:type="simple"/></inline-formula>, we can define the effective connectivities<disp-formula id="pcbi.1003258.e533"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e533" xlink:type="simple"/><label>(40)</label></disp-formula>to obtain the voltage equation<disp-formula id="pcbi.1003258.e534"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e534" xlink:type="simple"/><label>(41)</label></disp-formula>which is the vectorized version of <xref ref-type="disp-formula" rid="pcbi.1003258.e104">Eqn. (8)</xref> without the noise term.</p>
</sec><sec id="s4c">
<title>Precision of the network estimate and comparison with stochastic neural models</title>
<sec id="s4c1">
<title>The predictive coding network</title>
<p>For simplicity, we will first discuss the homogeneous network, with one population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e535" xlink:type="simple"/></inline-formula> identical neurons, and no cost terms. A complete analytical solution of the homogeneous network is provided in <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>. If the network is sufficiently large, we can replace the spike trains of the individual neurons by their population firing rate as shown in the <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>. Here, we will furthermore assume that the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e536" xlink:type="simple"/></inline-formula> being tracked by the neurons is a constant. In this case, the network will produce a readout <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e537" xlink:type="simple"/></inline-formula> that is approximately constant as well, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e538" xlink:type="simple"/></inline-formula>. Solving the read-out equation for the rate, we therefore obtain<disp-formula id="pcbi.1003258.e539"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e539" xlink:type="simple"/><label>(42)</label></disp-formula>where we neglected time since all variables are constants. Note use of the label “pop” which should remind us that this is the <italic>population</italic> firing rate, i.e., the sum of the firing rates of the individual units. These latter firing rates are simply given by dividing the population firing rate through the number of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e540" xlink:type="simple"/></inline-formula> (since all units are equal) so that<disp-formula id="pcbi.1003258.e541"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e541" xlink:type="simple"/><label>(43)</label></disp-formula>To keep these firing rates constant as the size of the network increases, the kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e542" xlink:type="simple"/></inline-formula> should therefore scale as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e543" xlink:type="simple"/></inline-formula>.</p>
<p>To estimate the precision of the network readout, we note that the predictive coding scheme prescribed through the loss function implies that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e544" xlink:type="simple"/></inline-formula>, if the costs are negligible. Furthermore, since the neurons are firing at a constant rate, the voltage is bounded from below by the reset potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e545" xlink:type="simple"/></inline-formula>. This bound holds in the limit of (infinitely) small noise. In turn, the readout is also bounded so that<disp-formula id="pcbi.1003258.e546"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e546" xlink:type="simple"/><label>(44)</label></disp-formula>The standard deviation of the network estimate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e547" xlink:type="simple"/></inline-formula>, in the time window of the decoder <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e548" xlink:type="simple"/></inline-formula> is thus strictly bounded by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e549" xlink:type="simple"/></inline-formula>, which, as stated previously, should scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e550" xlink:type="simple"/></inline-formula> to maintain the same firing rate in the neurons. Accordingly, the standard deviation of the network estimate scales with<disp-formula id="pcbi.1003258.e551"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e551" xlink:type="simple"/><label>(45)</label></disp-formula>as the network size increases.</p>
</sec><sec id="s4c2">
<title>The equivalent Poisson network</title>
<p>Let us consider now a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e552" xlink:type="simple"/></inline-formula> neurons firing spikes at the same rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e553" xlink:type="simple"/></inline-formula> according to a homogeneous Poisson process. The minimal variance of an estimator based on measuring these neuron's responses in a time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e554" xlink:type="simple"/></inline-formula> is given by the Cramer-Rao bound <xref ref-type="bibr" rid="pcbi.1003258-Brunel2">[47]</xref><disp-formula id="pcbi.1003258.e555"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e555" xlink:type="simple"/><label>(46)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e556" xlink:type="simple"/></inline-formula> are the tuning curves of the neurons, i.e., their firing rate as a function of the encoded variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e557" xlink:type="simple"/></inline-formula>. If we assume the best-case scenario, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e558" xlink:type="simple"/></inline-formula>, then the tuning curves are simply given by <xref ref-type="disp-formula" rid="pcbi.1003258.e541">Eqn. (43)</xref>, so that<disp-formula id="pcbi.1003258.e559"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e559" xlink:type="simple"/><label>(47)</label></disp-formula>Hence, the Cramer-Rao bound becomes<disp-formula id="pcbi.1003258.e560"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e560" xlink:type="simple"/><label>(48)</label></disp-formula>As before, the kernels need to scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e561" xlink:type="simple"/></inline-formula>, so that in this case the standard deviation scales with<disp-formula id="pcbi.1003258.e562"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e562" xlink:type="simple"/><label>(49)</label></disp-formula></p>
</sec><sec id="s4c3">
<title>The general case</title>
<p>The exact analytical results cannot be readily extended to the representation of multi-dimensional variables, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e563" xlink:type="simple"/></inline-formula>, since the tuning curves of the individual neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e564" xlink:type="simple"/></inline-formula>, depend on the exact spatial configuration of the kernels. Here, we use instead arguments on how the kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e565" xlink:type="simple"/></inline-formula> and the errors scale with the size of the network (assuming fixed firing rates and decoder leak). For simplicity of argumentation, we will assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e566" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e567" xlink:type="simple"/></inline-formula>, so that all output kernels have the same norm, and we can again neglect the cost term.</p>
<p>In the case of a multi-dimensional variable, the firing rule ensures that the projection of the prediction error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e568" xlink:type="simple"/></inline-formula>, on any kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e569" xlink:type="simple"/></inline-formula> in the population does not exceed half of the kernel norm. Thus, if we place ourselves along the direction of any kernel, the projection of the prediction error is bounded by<disp-formula id="pcbi.1003258.e570"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e570" xlink:type="simple"/><label>(50)</label></disp-formula>As argued above, to keep the firing rates of the individual neurons constant, the kernels need to scale with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e571" xlink:type="simple"/></inline-formula> as the size of the network increases. Hence,<disp-formula id="pcbi.1003258.e572"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e572" xlink:type="simple"/><label>(51)</label></disp-formula>and the projection of the error on each kernel direction thus scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e573" xlink:type="simple"/></inline-formula>. Let us assume that the kernels are sufficiently dense to cover all quadrant of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e574" xlink:type="simple"/></inline-formula>-dimensional variable space. In this case, there is at least one kernel in each of the subspaces formed by all possible combinations of signs for the components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e575" xlink:type="simple"/></inline-formula>. The minimum number of neurons is therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e576" xlink:type="simple"/></inline-formula>. In each quadrant, there is then a kernel that will cause its associated neuron to fire as soon as the dot-product of this kernel with the error exceeds half the kernel norm. Consequently, the error is strictly bounded in a hypersphere centered on the origin whose radius is bounded by the constant kernel norm, <xref ref-type="disp-formula" rid="pcbi.1003258.e570">Eqn. (50)</xref>. Thus, as before, the standard deviation of the prediction error scales with the kernels and thus with<disp-formula id="pcbi.1003258.e577"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e577" xlink:type="simple"/><label>(52)</label></disp-formula></p>
<p>Let us now consider a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e578" xlink:type="simple"/></inline-formula> independent Poisson neurons representing the multi-dimensional variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e579" xlink:type="simple"/></inline-formula>. The minimal variance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e580" xlink:type="simple"/></inline-formula>, of the estimation errors are given by the inverse of the Fisher information matrix. The precise formula does not matter, though we note that this variance scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e581" xlink:type="simple"/></inline-formula>. Thus, as before, we obtain<disp-formula id="pcbi.1003258.e582"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e582" xlink:type="simple"/><label>(53)</label></disp-formula>and we can conclude that the ratio of the network estimation error to the estimation error incurred by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e583" xlink:type="simple"/></inline-formula> independent Poisson neurons with similar rates rates will always scale as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e584" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s4d">
<title>Rate model matching the firing rate of the homogeneous integrator network</title>
<p>In the homogeneous integrator network (with low noise and small costs), the membrane potentials of neurons with identical kernels are approximately equal, which allows us to write down an analytical solution (see <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>). Briefly, the population inter-spike interval, i.e. the interval between two successive spikes from any neuron, corresponds to the time it takes for this “common” membrane potential to rise from the reset potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e585" xlink:type="simple"/></inline-formula> to the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e586" xlink:type="simple"/></inline-formula>. We call this time period an “integration cycle”. Note that this interval is typically much shorter than the ISI of an individual neuron or the time constant of the decoder. During this short time interval, the leak term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e587" xlink:type="simple"/></inline-formula> can be neglected, and the derivative of the membrane potential, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e588" xlink:type="simple"/></inline-formula>, is approximately constant. The population ISI is thus simply given by the time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e589" xlink:type="simple"/></inline-formula> it takes to integrate from the reset, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e590" xlink:type="simple"/></inline-formula>, to the threshold, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e591" xlink:type="simple"/></inline-formula>, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e592" xlink:type="simple"/></inline-formula>. All neurons with the same kernel have identical firing rates, and, since only half of the population is spiking at any value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e593" xlink:type="simple"/></inline-formula> (in the limit of small noise), the firing rates of individual neurons are equal to the population firing divided by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e594" xlink:type="simple"/></inline-formula>. Thus, the firing rate of each neuron can be approximated as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e595" xlink:type="simple"/></inline-formula>.</p>
<p>To construct the Poisson generator network, we removed the fast connections (but not the slow connections) and replaced the LIF neurons by Poisson spike generators with the same instantaneous firing rate, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e596" xlink:type="simple"/></inline-formula>. The resulting recurrent network roughly matches the instantaneous firing rates (but not the performance) in the LIF network. The match could be enhanced, for example by adding a small baseline firing rate or a refractory period; However, these changes can only decrease the performance of the Poisson rate model.</p>
</sec><sec id="s4e">
<title>Measuring GLM filters</title>
<p>To obtain the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e597" xlink:type="simple"/></inline-formula> filters in the integrator network (<xref ref-type="fig" rid="pcbi-1003258-g007">Fig. 7 F</xref>), we performed the following procedure: The inhomogeneous integrator was driven by an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e598" xlink:type="simple"/></inline-formula> sampled from Gaussian white noise (with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e599" xlink:type="simple"/></inline-formula>, standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e600" xlink:type="simple"/></inline-formula>) and convolved by an exponential filter of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e601" xlink:type="simple"/></inline-formula>ms. The spike trains of the ten “recorded” neurons were modeled as independent Poisson processes with instantaneous firing rates<disp-formula id="pcbi.1003258.e602"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e602" xlink:type="simple"/><label>(54)</label></disp-formula>The feed-forward weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e603" xlink:type="simple"/></inline-formula> and lateral filters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e604" xlink:type="simple"/></inline-formula> were estimated by maximizing the log-likelihood of the spike trains, following the method of <xref ref-type="bibr" rid="pcbi.1003258-Pillow1">[28]</xref>. Briefly, the filters were discretized in 500 time bins of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e605" xlink:type="simple"/></inline-formula>, and conjugate gradient ascent of the log likelihood was performed on the value of the filters in each time bin for the equivalent of 5 hours of recording.</p>
</sec><sec id="s4f">
<title>Measuring the coefficient of variation</title>
<p>The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e606" xlink:type="simple"/></inline-formula> of a spike train is defined as<disp-formula id="pcbi.1003258.e607"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003258.e607" xlink:type="simple"/><label>(55)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e608" xlink:type="simple"/></inline-formula> is the total number of spike in the spike train, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e609" xlink:type="simple"/></inline-formula> is the duration of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e610" xlink:type="simple"/></inline-formula> inter-spike interval. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e611" xlink:type="simple"/></inline-formula> reported in the paper are the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e612" xlink:type="simple"/></inline-formula> measured in each neuron and averaged over the population.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003258.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003258.s001" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>In the supporting <xref ref-type="supplementary-material" rid="pcbi.1003258.s001">Text S1</xref>, we address several issues regarding the biological plausibility of our approach and the generality of our results. In particular, we demonstrate that the networks can be separated into purely excitatory and inhibitory neurons. We discuss the problem of perturbing synaptic weights and show that the networks are robust to synaptic failures and to noise in the lateral connections. We furthermore derive analytical results for the dynamics of a network with identical kernels tracking a scalar dynamical variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003258.e613" xlink:type="simple"/></inline-formula>, in the limit of high firing rates.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Boris Gutkin, Laureline Logiaco, David Barrett, and Pedro Gonçalves for great suggestions and constructive comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003258-Gold1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name> (<year>2007</year>) <article-title>The neural basis of decision making</article-title>. <source>Annu Rev Neurosci</source> <volume>30</volume>: <fpage>535</fpage>–<lpage>574</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Major1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Major</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>Persistent neural activity: prevalence and mechanisms</article-title>. <source>Curr Opin Neurobiol</source> <volume>14</volume>: <fpage>675</fpage>–<lpage>684</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Wolpert1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name> (<year>2000</year>) <article-title>Computational principles of movement neuroscience</article-title>. <source>Nat Neurosci</source> <volume>3</volume>: <fpage>1212</fpage>–<lpage>1217</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Hertz1"><label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">Hertz J, Palmer R, Krogh A (1991) Introduction to the theory of neural computation Santa Fe Institute: Westview Press.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Vogels1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogels</surname><given-names>TP</given-names></name>, <name name-style="western"><surname>Rajan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2005</year>) <article-title>Neural network dynamics</article-title>. <source>Annu Rev Neurosci</source> <volume>28</volume>: <fpage>357</fpage>–<lpage>376</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Wang1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>X-J</given-names></name> (<year>2008</year>) <article-title>Decision making in recurrent neuronal circuits</article-title>. <source>Neuron</source> <volume>60</volume>: <fpage>215</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Tolhurst1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tolhurst</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Movshon</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dean</surname><given-names>A</given-names></name> (<year>1982</year>) <article-title>The statistical reliability of signals in single neurons in cat and monkey visual cortex</article-title>. <source>Vision Res</source> <volume>23</volume>: <fpage>775</fpage>–<lpage>785</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Shadlen1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name> (<year>1998</year>) <article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title>. <source>J Neurosci</source> <volume>18</volume> (<issue>10</issue>)  <fpage>3870</fpage>–<lpage>3896</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Compte1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Compte</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname><given-names>PS</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2000</year>) <article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title>. <source>Cereb Cortex</source> <volume>10</volume> (<issue>9</issue>)  <fpage>910</fpage>–<lpage>923</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Machens1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name>, <name name-style="western"><surname>Romo</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name> (<year>2005</year>) <article-title>Flexible control of mutual inhibition: a neural model of two-interval discrimination</article-title>. <source>Science</source> <volume>307</volume>: <fpage>1121</fpage>–<lpage>1124</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Mainen1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mainen</surname><given-names>ZF</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>1995</year>) <article-title>Reliability of spike timing in neocortical neurons</article-title>. <source>Science</source> <volume>268</volume> (<issue>5216</issue>)  <fpage>1503</fpage>–<lpage>1506</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Schneidman1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Freedman</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>I</given-names></name> (<year>1998</year>) <article-title>Ion channel stochasticity may be critical in determining the reliability and precision of spike timing</article-title>. <source>Neural Comput</source> <volume>10</volume> (<issue>7</issue>)  <fpage>1679</fpage>–<lpage>1703</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Faisal1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faisal</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Selen</surname><given-names>LP</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2008</year>) <article-title>Noise in the nervous system</article-title>. <source>Nat Rev Neurosci</source> <volume>9</volume>: <fpage>292</fpage>–<lpage>303</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Softky1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Softky</surname><given-names>WR</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>1993</year>) <article-title>Noise in the nervous system</article-title>. <source>J Neurosci</source> <volume>13</volume> (<issue>1</issue>)  <fpage>334</fpage>–<lpage>350</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-vanVreeswijk1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Vreeswijk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1998</year>) <article-title>Chaotic balanced state in a model of cortical circuits</article-title>. <source>Neural Comput</source> <volume>10</volume> (<issue>6</issue>)  <fpage>1321</fpage>–<lpage>1371</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Brunel1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2000</year>) <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory neurons</article-title>. <source>J Comput Neurosci</source> <volume>8</volume>: <fpage>183</fpage>–<lpage>208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Renart1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Renart</surname><given-names>A</given-names></name>, <name name-style="western"><surname>de la Rocha</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bartho</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Hollender</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Parga</surname><given-names>N</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>The asynchronous state in cortical circuits</article-title>. <source>Science</source> <volume>327</volume>: <fpage>587</fpage>–<lpage>590</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Tchumatchenko1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tchumatchenko</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Malyshev</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Geisel</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Volgushev</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wolf</surname><given-names>F</given-names></name> (<year>2010</year>) <article-title>Correlations and synchrony in threshold neuron models</article-title>. <source>Phys Rev Lett</source> <volume>104</volume>: <fpage>058102</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Deneve1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Bayesian spiking neurons i: inference</article-title>. <source>Neural Comput</source> <volume>20</volume>: <fpage>91</fpage>–<lpage>117</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Boerlin1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boerlin</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Spike-based population coding and working memory</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1001080</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Leigh1"><label>21</label>
<mixed-citation publication-type="book" xlink:type="simple">Leigh JR (2004) Control Theory: A Guided Tour. London, UK: Institution of Electrical Engineers.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Wehr1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wehr</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name> (<year>2003</year>) <article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title>. <source>Nature</source> <volume>426</volume>: <fpage>442</fpage>–<lpage>6</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Okun1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lampl</surname><given-names>I</given-names></name> (<year>2008</year>) <article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>535</fpage>–<lpage>537</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Haider1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haider</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Duque</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hasenstaub</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>McCormick</surname><given-names>DA</given-names></name> (<year>2006</year>) <article-title>Neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>4535</fpage>–<lpage>4545</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Shu1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hasenstaub</surname><given-names>A</given-names></name>, <name name-style="western"><surname>McCormick</surname><given-names>DA</given-names></name> (<year>2003</year>) <article-title>Turning on and off recurrent balanced cortical activity</article-title>. <source>Nature</source> <volume>423</volume>: <fpage>288</fpage>–<lpage>293</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Gentet1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gentet</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>Avermann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Matyas</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Staiger</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Petersen</surname><given-names>CCH</given-names></name> (<year>2010</year>) <article-title>Membrane potential dynamics of GABAergic neurons in the barrel cortex of behaving mice</article-title>. <source>Neuron</source> <volume>65</volume>: <fpage>422</fpage>–<lpage>435</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Maass1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschlager</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume> (<issue>11</issue>)  <fpage>2531</fpage>–<lpage>60</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Pillow1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Spatiotemporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source> <volume>454</volume> (<issue>7207</issue>)  <fpage>995</fpage>–<lpage>999</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Seung1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>How the brain keeps the eyes still</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>93</volume> (<issue>23</issue>)  <fpage>13339</fpage>–<lpage>13344</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Eliasmith1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eliasmith</surname><given-names>C</given-names></name> (<year>2005</year>) <article-title>A unified approach to building and controlling spiking attractor networks</article-title>. <source>Neural Comput</source> <volume>17</volume> (<issue>6</issue>)  <fpage>1276</fpage>–<lpage>1314</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Machens2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name>, <name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name> (<year>2008</year>) <article-title>Design of continuous attractor networks with monotonic tuning using a symmetry principle</article-title>. <source>Neural Comput</source> <volume>20</volume>: <fpage>452</fpage>–<lpage>485</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Batista1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Batista</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Buneo</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Snyder</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Andersen</surname><given-names>R</given-names></name> (<year>1999</year>) <article-title>Reach plans in eye-centered coordinates</article-title>. <source>Science</source> <volume>285</volume>: <fpage>257</fpage>–<lpage>60</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Snyder1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Snyder</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Grieve</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Brotchie</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Andersen</surname><given-names>R</given-names></name> (<year>1998</year>) <article-title>Separate body- and world-referenced representations of visual space in parietal cortex</article-title>. <source>Nature</source> <volume>394</volume>: <fpage>887</fpage>–<lpage>91</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Druckmann1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Druckmann</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>Over-complete representations on recurrent neural networks can support persistent percepts</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>23</volume>: <fpage>541</fpage>–<lpage>549</lpage> (Lafferty J, Williams CKI, Shawe-Taylor J, Zemel R, Culotta A, editors).</mixed-citation>
</ref>
<ref id="pcbi.1003258-Ecker1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ecker</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Keliris</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Tolias</surname><given-names>AS</given-names></name> (<year>2007</year>) <article-title>Decorrelated neuronal firing in cortical microcircuits</article-title>. <source>Science</source> <volume>327</volume>: <fpage>584</fpage>–<lpage>587</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Roudi1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name> (<year>2007</year>) <article-title>A balanced memory network</article-title>. <source>PLoS Comput Biol</source> <volume>3</volume> (<issue>9</issue>)  <fpage>1679</fpage>–<lpage>1700</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Bourdoukan1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bourdoukan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Barrett</surname><given-names>DGT</given-names></name>, <name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name>, <name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name> (<year>2012</year>) <article-title>Learning Optimal Spike-based Representations</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>25</volume>: <fpage>2294</fpage>–<lpage>2302</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Abbott1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>1999</year>) <article-title>The effect of correlated variability on the accuracy of a population code</article-title>. <source>Neural Computation</source> <volume>11</volume>: <fpage>91</fpage>–<lpage>101</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Zohary1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zohary</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name> (<year>1994</year>) <article-title>Correlated neuronal discharge rate and its implication for psychophysical performance</article-title>. <source>Nature</source> <volume>370</volume>: <fpage>140</fpage>–<lpage>143</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Koulakov1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koulakov</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lisman</surname><given-names>JE</given-names></name> (<year>2002</year>) <article-title>Model for a robust neural integrator</article-title>. <source>Nature Neurosci</source> <volume>5</volume> (<issue>8</issue>)  <fpage>775</fpage>–<lpage>782</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Goldman1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldman</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Levine</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Major</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2003</year>) <article-title>Robust persistent neural activity in a model integrator with multiple hysteretic dendrites per neuron</article-title>. <source>Cerebral cortex</source> <volume>13</volume> (<issue>11</issue>)  <fpage>1185</fpage>–<lpage>1195</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Cain1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cain</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Barreiro</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>Neural integrators for decision making: a favorable tradeo_ between robustness and sensitivity</article-title>. <source>J Neurophysiol</source> <volume>109</volume> (<issue>10</issue>)  <fpage>2542</fpage>–<lpage>59</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Moreau1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moreau</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sontag</surname><given-names>E</given-names></name> (<year>2003</year>) <article-title>Balancing at the border of instability</article-title>. <source>Physical Rev E</source> <volume>68</volume> (<issue>2</issue>)  <fpage>020901</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Renart2"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Renart</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Song</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X-J</given-names></name> (<year>2003</year>) <article-title>Robust spatial working memory through homeostatic synaptic scaling in heterogeneous cortical networks</article-title>. <source>Neuron</source> <volume>38</volume> (<issue>3</issue>)  <fpage>473</fpage>–<lpage>485</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Goldman2"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldman</surname><given-names>MS</given-names></name> (<year>2009</year>) <article-title>Memory without feedback in a neural network</article-title>. <source>Neuron</source> <volume>61</volume> (<issue>4</issue>)  <fpage>621</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Li1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Goldman</surname><given-names>MS</given-names></name> (<year>2013</year>) <article-title>Balanced cortical microcircuitry for maintaining information in working memory</article-title>. <source>Nat Neurosci</source> <volume>16</volume>: <fpage>1306</fpage>–<lpage>1314</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003258-Brunel2"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>J</given-names></name> (<year>1998</year>) <article-title>Mutual information, Fisher information, and population coding</article-title>. <source>Neural Comput</source> <volume>10</volume> (<issue>7</issue>)  <fpage>1731</fpage>–<lpage>57</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>