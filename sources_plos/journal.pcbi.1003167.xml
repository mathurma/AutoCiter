<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00131</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003167</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject><subject>Sensory systems</subject></subj-group></subj-group><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Shape Similarity, Better than Semantic Membership, Accounts for the Structure of Visual Object Representations in a Population of Monkey Inferotemporal Neurons</article-title>
<alt-title alt-title-type="running-head">Structure of Visual Object Representations in IT</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Baldassi</surname><given-names>Carlo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Alemi-Neissi</surname><given-names>Alireza</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Pagan</surname><given-names>Marino</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>James J.</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Zecchina</surname><given-names>Riccardo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Zoccolan</surname><given-names>Davide</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Applied Science and Technology &amp; Center for Computational Sciences, Politecnico di Torino, Torino, Italy</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Human Genetics Foundation (HuGeF), Torino, Torino, Italy</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>International School for Advanced Studies (SISSA), Trieste, Italy</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, Massachusetts Institute of Technology (MIT), Cambridge, Massachusetts, United States of America</addr-line></aff>
<aff id="aff5"><label>5</label><addr-line>Department of Psychology, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Einhäuser</surname><given-names>Wolfgang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Philipps-University Marburg, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">zoccolan@sissa.it</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JJD DZ. Performed the experiments: DZ. Analyzed the data: CB AAN MP RZ. Contributed reagents/materials/analysis tools: JJD CB RZ. Wrote the paper: CB AAN MP DZ.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>8</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>8</day><month>8</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>8</issue>
<elocation-id>e1003167</elocation-id>
<history>
<date date-type="received"><day>24</day><month>1</month><year>2013</year></date>
<date date-type="accepted"><day>19</day><month>6</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Baldassi et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>The anterior inferotemporal cortex (IT) is the highest stage along the hierarchy of visual areas that, in primates, processes visual objects. Although several lines of evidence suggest that IT primarily represents visual shape information, some recent studies have argued that neuronal ensembles in IT code the semantic membership of visual objects (i.e., represent conceptual classes such as animate and inanimate objects). In this study, we investigated to what extent semantic, rather than purely visual information, is represented in IT by performing a multivariate analysis of IT responses to a set of visual objects. By relying on a variety of machine-learning approaches (including a cutting-edge clustering algorithm that has been recently developed in the domain of statistical physics), we found that, in most instances, IT representation of visual objects is accounted for by their similarity at the level of shape or, more surprisingly, low-level visual properties. Only in a few cases we observed IT representations of semantic classes that were not explainable by the visual similarity of their members. Overall, these findings reassert the primary function of IT as a conveyor of explicit visual shape information, and reveal that low-level visual properties are represented in IT to a greater extent than previously appreciated. In addition, our work demonstrates how combining a variety of state-of-the-art multivariate approaches, and carefully estimating the contribution of shape similarity to the representation of object categories, can substantially advance our understanding of neuronal coding of visual objects in cortex.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>To build meaningful representations of the external word, the stream of sensory information that reaches our senses is continuously processed and interpreted by the brain. Ultimately, such a processing allows the brain to arrange sensory (e.g., visual) inputs into a hierarchy of categories (such as animate and inanimate objects) and sub-categories (such as faces, animals, buildings, tools, etc). Crucially, while many objects can be assigned to the same category based on their visual similarity (e.g., oranges and apples), formation of most categories also requires arbitrarily associating objects sharing similar functions/meaning, but not similar shape (e.g., bananas and apples). A long-standing debate exists about whether the representation of visual objects in the higher visual centers of the brain (such as the inferotemporal cortex; IT) purely reflects shape similarity or also (and, perhaps, mainly) shape-unrelated categorical knowledge. In this study, we have addressed this issue by applying a variety of computational approaches. Our results show that the response patterns of a population of inferotemporal neurons are better accounted for by shape similarity than categorical membership. This reasserts the primary function of IT as a visual area and demonstrates how state-of-the-art computational approaches can advance our understanding of neuronal coding in the brain.</p>
</abstract>
<funding-group><funding-statement>This work was supported by The National Institute of Mental Health Conte, NIH-P20-MH66239 and NIH-R01-EY014970 (JJD), an HFSP Long Term Postdoctoral Fellowship (DZ), an Accademia Nazionale dei Lincei - Compagnia di San Paolo Grant (DZ), a Programma Neuroscienze Grant of the Compagnia di San Paolo (DZ and RZ), and ERC Grant 267915 (CB and RZ). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="20"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>In primates, visual object information is processed through a hierarchy of cortico-cortical stages (the <italic>ventral visual pathway</italic>) that culminates with the inferotemporal cortex (IT) <xref ref-type="bibr" rid="pcbi.1003167-Logothetis1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-DiCarlo1">[6]</xref>. Uncovering the nature of visual object representations in IT is central to our understanding of how visually presented objects are perceived, identified and categorized, yet it is extremely challenging. In fact, because of the non-linear mapping between the visual input space and IT neuronal responses, it is virtually impossible to precisely estimate the tuning of individual IT neurons over the image space (but see <xref ref-type="bibr" rid="pcbi.1003167-Brincat1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Yamane1">[8]</xref>). As a result, it is somewhat arbitrary to assign IT the proper rank along the continuum that goes from extraction of simple visual features to formation of conceptual, semantic categories – are IT neurons closer to the local edge detectors found in primary visual areas or to the concept cells recently found in human middle temporal lobe <xref ref-type="bibr" rid="pcbi.1003167-Quiroga1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Quiroga3">[11]</xref>?</p>
<p>While most literature supports the notion that IT neurons code moderately to highly complex configurations of visual features <xref ref-type="bibr" rid="pcbi.1003167-Tanaka1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-DiCarlo1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Brincat2">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kourtzi1">[13]</xref>, recent work has argued that IT neuronal ensembles code the semantic membership of visual objects (i.e., represent behaviorally salient conceptual categories, such as animate and non-animate objects, animals, body parts, etc) rather than their visual properties <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>. A related study also compared how a set of visual objects was represented in monkey IT and its human homologous, finding that many semantic categories were represented equally well in both species (with a primary, sharp distinction between animate and inanimate objects) and reporting the inadequacy of various image-based similarity metrics to account for the observed patterns of neuronal responses <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>. Finally, a recent fMRI study concluded that object representations in monkey IT are spatially segregated according to semantic relationship <xref ref-type="bibr" rid="pcbi.1003167-Bell1">[16]</xref>, a finding that matches the segregation by function/meaning (rather than by shape) found in the topography of human high-level representations of visual objects <xref ref-type="bibr" rid="pcbi.1003167-Kanwisher1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Connolly1">[24]</xref>.</p>
<p>Finding that abstract category information is represented in IT is not surprising per se, since several studies have shown how IT neurons can represent the association of arbitrary image pairs, either through explicit <xref ref-type="bibr" rid="pcbi.1003167-Sakai1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Higuchi1">[26]</xref> or implicit <xref ref-type="bibr" rid="pcbi.1003167-Miyashita1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Li2">[29]</xref> associative learning. However, while these mechanisms can explain why extensively trained categories <xref ref-type="bibr" rid="pcbi.1003167-Meyers1">[30]</xref> or behaviorally salient categories (such as faces and body parts <xref ref-type="bibr" rid="pcbi.1003167-Tsao1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Popivanov1">[34]</xref>) are represented in IT, they can hardly explain why category information was found to be represented in IT more systematically and robustly than visual shape information <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>. In fact, several studies have shown that IT neurons are robustly tuned for object-defining visual features <xref ref-type="bibr" rid="pcbi.1003167-Brincat1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Yamane1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-OpdeBeeck1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Zoccolan2">[39]</xref> and one study has shown that in IT, differently from prefrontal cortex, semantic category information is not greater than what expected based on the visual similarity of category members <xref ref-type="bibr" rid="pcbi.1003167-Freedman1">[40]</xref>. Finally, a very recent monkey fMRI study has found no sharp segregation between the representations of animate and inanimate objects in IT <xref ref-type="bibr" rid="pcbi.1003167-Popivanov1">[34]</xref>.</p>
<p>In this study, we have applied an array of multivariate approaches (some of which were recently developed in the domain of statistical mechanics) to investigate how an IT neuronal population represents pictures of natural objects. Our analysis shows that neuronal representations in IT largely depend on objects' similarity at the level of shape or, more surprisingly, low-level visual properties, with semantic membership only accounting for the representation of a few, behaviorally salient categories of animate objects (such as four-limbed animals and birds). Overall, these findings show that monkey IT is primarily a conveyor of explicit visual shape information, in which a surprisingly broad spectrum of visual feature complexity is represented.</p>
</sec><sec id="s2">
<title>Results</title>
<p>In this study, we recorded 94 well-isolated single units from the anterior inferotemporal cortex (IT) of two monkeys. Neurons were sampled across a ∼5×4 mm area of the ventral superior temporal sulcus (STS) and ventral surface lateral to the anterior middle temporal sulcus (AMTS), as shown in <xref ref-type="fig" rid="pcbi-1003167-g001">Figure 1</xref> (see blue dots and red-shaded areas). No attempt was done to target specific IT patches containing cells with similar preference for faces, such as the AF (anterior fundus), AL (anterior lateral) and AM (anterior medial) face patches <xref ref-type="bibr" rid="pcbi.1003167-Tsao1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref> (the range of possible locations of these patches is also shown in <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>, based on <xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref>), or other IT regions that are rich of face selective neurons (summarized in <xref ref-type="bibr" rid="pcbi.1003167-Baylis1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Rolls2">[42]</xref>). Finally, for a better comparison with previous findings, it is important to notice that we recorded from a region with a smaller anteroposterior (AP) and mediolateral (ML) extent than the region sampled by <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, although both regions were roughly centered at the same AP position in anterior IT (compare <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref> with <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref> in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>).</p>
<fig id="pcbi-1003167-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.g001</object-id><label>Figure 1</label><caption>
<title>Recording locations.</title>
<p>The blue dots show the projections of the recording chamber grid-point locations from the top of the skull to the ventral bank of the superior temporal sulcus (STS) and the ventral surface lateral to the anterior middle temporal sulcus (AMTS). The projections are shown over a sequence of MRI images (spanning a 13–17 anteroposterior range; Horsley-Clarke coordinates) that were collected, for one of the monkeys, before the chamber implant surgery. Only the grid locations in which the electrode was inserted at least once are shown. The red-shaded areas highlight the estimated cortical span that was likely sampled during recording, given that: 1) each electrode penetration usually spanned the whole depth of the targeted cortical bank (either STS or AMTS); and 2) the upper bound of the variability of each recording location along the mediolateral axis (due to bending of the electrode during insertion) can be estimated as ±2 mm <xref ref-type="bibr" rid="pcbi.1003167-Cox1">[80]</xref>. The figure also shows the range of possible locations of the three anterior face patches (AL, AF and AM) according to <xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref>, so as to highlight their potential overlap with the recording locations.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.g001" position="float" xlink:type="simple"/></fig>
<p>All neurons were probed with a set of 213 grayscale pictures of natural objects (see <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref>) presented at a rate of 5 images/s, while the animals were engaged in a simple object detection task. To understand how these objects were mapped into the IT neuronal space, we used linear classifiers and a variety of clustering algorithms, and we measured to what extent object clusters in the IT neuronal representation could be accounted by three different object attributes: 1) shared semantic membership; 2) shared shape features (i.e., shape similarity); and 3) shared low-level visual properties.</p>
<fig id="pcbi-1003167-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.g002</object-id><label>Figure 2</label><caption>
<title>The stimulus set.</title>
<p>The full set of 213 objects used in our study. The set consists of: i) 188 images of real-world objects belonging to 94 different categories (e.g., two hats, two accordions, two monkey faces, etc.); ii) 5 cars, 5 human faces, and 5 abstract silhouettes; iii) 5 patches of texture (e.g., random dots and oriented bars); iv) a blank frame; v) 4 low contrast (10%, 3%, 2% and 1.5%) images of one of the objects (a camera).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.g002" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Gradient in object area explains object clustering at the most “superordinate” level</title>
<p>The nature of visual object representations in IT can be studied by examining what features are shared by objects that produce similar population responses in the IT neuronal representation space. The similarity between the neuronal representations of a pair of visual objects (<italic>neuronal-level similarity</italic> in the following) was computed as the Pearson correlation coefficient of the normalized population response vectors produced by the two objects (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). To gain some intuition into possible trends in the representation of our object set, the neuronal-level similarity between each object pair was color-coded in the matrix shown in <xref ref-type="fig" rid="pcbi-1003167-g003">Figure 3A</xref>. The order of the objects along the axes of the similarity matrix was determined by the dendrogram shown at the top, which was obtained by applying an agglomerative hierarchical clustering algorithm to the neural population vectors. This allowed objects evoking similar population responses to lie nearby in the matrix, so that clusters of objects that were similar in the neural representation space appeared as compact dark squares along the diagonal of the matrix.</p>
<fig id="pcbi-1003167-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.g003</object-id><label>Figure 3</label><caption>
<title>Similarity matrix, hierarchical clustering and PCA of IT population responses to visual objects.</title>
<p>(A) Each pixel in the matrix color-codes the correlation (i.e., similarity) between the neuronal population vectors representing a pair of visual objects. The order of the objects along the axes is defined by the dendrogram produced by hierarchical clustering of the population vectors (to avoid crowding, one every three objects is shown; the complete object set is shown in <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref>). The first two branches of the dendrogram (shown at the top) are colored in cyan and magenta. (B) The fraction of animate and inanimate objects is not significantly different in the first two branches of the dendrogram (NS, <italic>p</italic>&gt;0.1, <italic>χ</italic><sup>2</sup> test). (C) The proportion of large and small objects is significantly different in the first two branches of the dendrogram (**, <italic>p</italic>&lt;0.001, <italic>χ</italic><sup>2</sup> test), (D) Layout of visual objects in the two-dimensional space defined by the first two principal components of the IT population responses (to avoid crowding, only some of the objects are shown). (E) Object area and object ranking along the first principal component are linearly related (<italic>r</italic> = −0.69, <italic>p</italic>&lt;0.001, <italic>t</italic>-test).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.g003" position="float" xlink:type="simple"/></fig>
<p>Contrary to what was recently reported by <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>, but in agreement with <xref ref-type="bibr" rid="pcbi.1003167-Popivanov1">[34]</xref>, visual inspection of the similarity matrix revealed that objects did not show a tendency to cluster into two main, compact clusters, corresponding to the semantic categories of animate and inanimate entities. For instance, the set of faces (top of the ordinate matrix axis) was separated from the set of birds and four-limbed animals (approximately in the middle of the axis) by many inanimate objects. Other animate objects (such as insects, crustaceans, fishes, and some four-limbed animals) were scattered all over the matrix axis and intermixed with inanimate objects (such as man-made tools, trees, flowers, etc). Finally, although some animate objects, such as faces, appeared to cluster according to the subordinate semantic category they belonged to (i.e., the face category), such clusters were generally embedded within larger clusters of animate and inanimate objects with similar shape (i.e., the face cluster lay within a larger group of similarly round shapes – a cup, a ball, a brain, an urchin, etc.). To better quantify whether a segregation between animate and inanimate objects could be observed at the top level of the dendrogram obtained by hierarchical clustering, we measured the fraction of animate and inanimate objects in the first two branches of the dendrogram (i.e., the cyan vs. the magenta branch in the dendrogram shown in <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3A</xref>). Animate objects amounted to ∼40% of the total in both branches and their fraction did not significantly differ in the two branches (<italic>p</italic>&gt;0.1, <italic>χ</italic><sup>2</sup> test; see <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3B</xref>).</p>
<p>While animate and inanimate objects were not sharply segregated in the neuronal representation space, a different property appeared to determine object clustering in the two top-level branches of the dendrogram – a gradient in object area could be observed along the matrix axes, with bulkier objects (e.g., faces and other round shapes) at one end of the axes and thinner objects (e.g., an ant, a dolphin, a guitar, etc) at the other end. To quantify this trend, objects were divided in two equally sized subsets of “large” and “small” objects, depending on whether their area was above or below the median of the full object set (object area is defined in <xref ref-type="sec" rid="s4">Materials and Methods</xref>). The proportion of large and small objects was significantly different in the first two branches of the dendrogram (<italic>p</italic>&lt;0.001, <italic>χ</italic><sup>2</sup> test), with large objects representing more than 60% of the total in one branch and only about 20% in the other (<xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3C</xref>).</p>
<p>To further investigate what properties shaped the representation of the objects in the IT neuronal space, we performed a Principal Component Analysis (PCA) of the recorded neuronal population vectors. The total variance explained by the first two principal components was fairly low (∼15%). This is not surprising, since our object set was highly varied in terms of visual properties and shape features and it is unlikely that high-level visual neurons, such as those sampled in our IT population, would represent/code only a few of such visual properties. Therefore, the goal of this analysis was not to find a few stimulus dimensions that could account for most of the variability in the representation of the visual objects. Rather, our goal was to check whether any principal component existed that could be associated to the variation of some global visual property across the object set. Interestingly, plotting the objects in the 2-dimensional space defined by the first two principal components revealed a trend that was consistent with the dendrogram obtained by hierarchical clustering. Namely, objects were distributed along the first principal component axis according to a gradient in object area, with large objects at one end of the axis and thin objects at the other end (see <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3D</xref>). This trend was confirmed by showing that object area and object raking along the first principal component axis were highly and significantly anticorrelated (<italic>r</italic> = −0.69, <italic>p</italic>&lt;0.001, <italic>t</italic>-test; see <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3E</xref>). Similarly, object luminance (defined in the <xref ref-type="sec" rid="s4">Materials and Methods</xref>) was significantly anticorrelated with the third principal component (<italic>r</italic> = −0.46, <italic>p</italic>&lt;0.001, t-test). No significant correlation was found between the second principal component and any other low-level visual property considered in this study (i.e., contrast and aspect ratio, as defined in the <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
<p>Overall, the analyses shown in <xref ref-type="fig" rid="pcbi-1003167-g003">Figure 3</xref> indicate that visual objects, in the recorded IT neuronal representation space, were loosely segregated at the coarser (i.e., more “superordinate”) level according to a low-level visual property – object area (not to be confused with object size, which, in this study, was kept constant to 2° of visual angle for every object, and which is defined as the diameter of the larger circle fully enclosing the object).</p>
</sec><sec id="s2b">
<title>Definition of three alternative clustering hypotheses</title>
<p>To gain further insight into the principles underlying the grouping of visual objects in the recorded neuronal representation, we divided the object set in categories, according to three different clustering hypotheses: 1) shared semantic membership; 2) shared shape features (i.e., shape similarity); and 3) shared low-level visual properties.</p>
<p>Eleven semantic categories were built – four-limbed animals, birds, faces, fishes, insects, sea invertebrates, trees, vehicles, tools, music instruments and buildings (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1A</xref>). The two superordinate semantic categories of animate and inanimate objects (which included, respectively, the first 6 and last 5 subordinate categories listed above) were also considered. All the semantic categories were built according to criteria established in previous studies <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> (e.g., the trees were included in the inanimate category).</p>
<p>Fifteen categories of objects sharing visual shape features (named <italic>shape-based categories</italic> in the following) were defined as the 15 clusters obtained by running a <italic>k</italic>-means clustering algorithm over the objects' representation provided by the output layer of a brain-inspired object recognition model <xref ref-type="bibr" rid="pcbi.1003167-Serre1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Mutch1">[44]</xref> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). Each of these categories/clusters contained objects that occupied nearby positions (and were, therefore, similar) in the representational space of the object recognition model. Being such a similarity measured in a high-dimensional multivariate representation, it is impossible to precisely know what shared features brought two objects to cluster in the same category. Therefore, the shape-based categories were simply labeled by sequential numbers (from 1 to 15; see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1B</xref>). However, when the shape features underlying formation of a given category could be guessed by visual inspection, we assigned to such a category a descriptive name (e.g., the <italic>round</italic> objects' category or the <italic>horizontal thin</italic> objects' category). It should be kept in mind that these names are only used for the sake of readability, but they cannot possibly capture the true combinations of shape features underlying object clustering in the model representational space.</p>
<p>Eight Categories of objects sharing low-level visual properties (named <italic>low-level categories</italic> in the following) were defined on the base of four global properties of the images of the objects – luminance, contrast, area and aspect ratio (defined in the <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Each category contained 15 images having either the highest or the lowest values of one of such properties (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1C</xref>).</p>
<p>It should be emphasized that no rigorous (or agreed-upon) definition exists of what should be considered low-level and high-level in terms of visual feature complexity. For this reason, our definitions of shape-based and low-level categories are essentially operational. That is, they refer to the complexity of the image processing that was performed to obtain them. In the case of the shape-based categories, the images of the objects were processed by banks of nonlinear filters in a multi-layered, feed-forward neural network (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Since these filters, collectively, extract visual features across a wide spectrum of complexity, the resulting shape-based categories included not only sets of moderately-to-highly complex visual patterns (such as round, oriented or star-like shapes), but also object sets that appeared to be defined mainly (but not exclusively) by lower-level image properties (such as contrast, luminance or texture). In the case of the low-level categories, the defining features were global image properties that could simply be extracted by segmenting the foreground image from the uniform-gray background. However, some of these properties, such as aspect ratio, can arguably be considered as moderately complex shape features. As a result, a few of the shaped-based categories substantially overlapped with the low-level categories and were assigned similar names (e.g., the <italic>bright</italic> and the <italic>dim</italic> shape-based categories partially overlapped, respectively, with the <italic>high-luminance</italic> and the <italic>low-contrast</italic> low-level categories; compare <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Figs. S1B and C</xref>). Such an overlap should not sound surprising, since the terms <italic>shape-based</italic> and <italic>low-level</italic> refer to the complexity of the operations underlying the definition of the categories, rather than to the content of the resulting categories. More in general, it should be stressed that the assessment of shape coding carried out in this study did not aim at precisely identifying what visual features were critical to elicit a response in specific neurons (or neuronal subpopulations). While methods to extract critical visual features exist (e.g., reverse correlation, image classification, or other fitting procedures of neuronal/behavioral responses to image properties <xref ref-type="bibr" rid="pcbi.1003167-Brincat1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Yamane1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Murray1">[45]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Gosselin1">[51]</xref>), the goal of our analysis was to assess how well various sets of visually similar objects clustered in the neuronal representation space, no matter whether visual similarity could be precisely defined in terms of specific visual properties (as in the case of the low-level categories) or not (as in the case of the shape-based categories).</p>
</sec><sec id="s2c">
<title>Overlap between clustering hypotheses and <italic>k</italic>-means clusters in the IT neuronal space</title>
<p>Having defined object categories based on three different hypotheses, we assessed to what extent the members of each category occupied nearby positions in the neuronal representation space. This was achieved by applying a <italic>k</italic>-means clustering algorithm to the neuronal population vectors, with the number of clusters <italic>k</italic> set to 15 according to both a Bayes and an Akaike Information Criterion <xref ref-type="bibr" rid="pcbi.1003167-Claeskens1">[52]</xref> (hence, the choice of using such a number also in the <italic>k</italic>-means procedure that lead to the definition of the shape-based categories shown in <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1B</xref>; see previous section). The resulting clusters were then compared to the semantic and visual similarity-based categories defined in the previous section, to check for any possible substantial overlap.</p>
<p><xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4A</xref> shows 15 object clusters that were obtained by a typical run of the <italic>k</italic>-means algorithm over the neuronal representation space (the <italic>k</italic>-means is not deterministic, therefore each run produces slightly different partitions of the data set; see below for further discussion). The order of the clusters in the figure was determined by applying an agglomerative hierarchical clustering algorithm to their centroids. This produced the dendrogram shown at the top of the figure, which allows appreciating the relationship among the <italic>k</italic>-means clusters (i.e., neighboring clusters in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4A</xref> lie nearby in the neural representation space). These clusters (named <italic>neuronal-based clusters</italic> in the following) were compared to the object categories of the three clustering hypotheses defined previously, some of which are shown in <xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4B–D</xref> (all the categories are shown in <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1</xref>). This was achieved by defining an overlap score that measured the fraction of objects in common between any given neuronal-based cluster and any given category in the three hypotheses. For easier comparison with Kiani et al., 2007, the same score defined in that study was used (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>; statistical significance of the overlap was computed through a permutation test, with Bonferroni corrected significance level <italic>p</italic>&lt;0.05).</p>
<fig id="pcbi-1003167-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.g004</object-id><label>Figure 4</label><caption>
<title>Overlap between <italic>k</italic>-means clusters in the IT neuronal space and object categories of the clustering hypotheses.</title>
<p>(A) Fifteen object clusters obtained by a typical run of the <italic>k</italic>-means algorithm over the IT neuronal representation space. The clusters' arrangement was determined by applying a hierarchical clustering algorithm to their centroids (see the dendrogram on the top; the same approach was used to arrange the shape-based categories shown in C, which resulted from the <italic>k</italic>-means object clustering in the output layer of an object recognition model <xref ref-type="bibr" rid="pcbi.1003167-Mutch1">[44]</xref>). (B–D) The semantic (B), shape-based (C) and low-level (D) categories that significantly overlapped with some of the neuronal-based clusters shown in A. Overlapping neuronal-based clusters and categories are indicated by matching names (e.g., <italic>faces</italic>) in A and B–D, with the objects in common between a cluster and a category enclosed by either a yellow (semantic), a red (shape-based) or a cyan (low-level) frame. (E) Average number of significant overlaps between neuronal-based clusters and semantic (first bar), shape-based (second bar) and low-level (third bar) categories across 1,000 runs of the <italic>k</italic>-means algorithm over both the neuronal representation space and the model representation space. The yellow, red and cyan striped portion of the first bar indicates the number of neuronal-based clusters that significantly overlapped with both a semantic category and either a shape-based or a low-level category.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.g004" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4</xref> shows what neuronal-based clusters (A), on the one hand, and what semantic (B; yellow frames), shape-based (C; red frames) and low-level (D; cyan frames) categories, on the other hand, significantly overlapped (objects belonging to both a neuronal-based cluster and its matching category are shown within the corresponding frames; see the descriptive names on top of each cluster/category in A-D to navigate the figure and find matches between neuronal-based clusters and categories). Out of the fifteen neuronal-based clusters, five significantly overlapped with a semantic category, seven with a shape-based category, and three with a low-level category. Interestingly, some clusters significantly overlapped with multiple categories, each belonging to a different clustering hypothesis. For instance, the first cluster shown in <xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4A</xref> overlapped both with the semantic category of <italic>fishes</italic> (forth category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4B</xref>) and with the shape-based category #6 (that we named <italic>horizontal thin</italic>; see the second category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4C</xref>). Similarly, the twelfth cluster in <xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4A</xref> overlapped with both the semantic category of <italic>faces</italic> (first category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4B</xref>), the shape-based category #2 (that we named <italic>round</italic>; see the fifth category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4C</xref>), and the low-level category of <italic>high area</italic> objects (first category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4D</xref>). Noticeably, in all these cases, the overlap was larger with the shape-based (or low-level) category than with the semantic category. Moreover, the objects overlapping with the semantic category were typically a subset of the objects overlapping with the shape-based category (see how the yellow frames are included within the red/cyan frames in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4A</xref>). That is, the objects belonging to a given semantic category were typically embedded within a larger group of objects with similar shape but different semantic membership (e.g., the fishes were embedded within a set of similarly horizontally elongated shapes, while the faces were embedded within a set of similarly round shapes). This implies that shape similarity (e.g., roundness) and not semantic membership (e.g., being a face) was at the root of these clusters within the neuronal representation space. On the other hand, a few neuronal-based clusters were found that significantly overlapped only with a semantic category. This is the case of the third and forth clusters in <xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4A</xref>, which overlapped, respectively, with the <italic>birds</italic> (third category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4B</xref>) and the <italic>four-limbed animals</italic> (second category in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4B</xref>).</p>
<p>To obtain a more robust assessment of what fraction of neuronal-based clusters significantly overlapped with categories of the three hypotheses and, in particular, how often semantic membership could be taken as the only explanation of the observed clusters, 1,000 runs of the <italic>k</italic>-means algorithm were performed (this produced 1,000 slightly different neuronal-based clusters and shape-based categories; the semantic and low-level categories were unchanged, since they were not obtained by a <italic>k</italic>-means procedure). <xref ref-type="fig" rid="pcbi-1003167-g004">Figure 4E</xref> shows the average number of neuronal-based clusters that, across these 1,000 <italic>k</italic>-means runs, significantly overlapped with categories of the three hypotheses. On average, about four, five and three clusters were found that significantly overlapped, respectively, with semantic, shape-based and low-level categories. Noticeably, more than half of the clusters that significantly overlapped with a semantic category, did so also with one of the categories defined by visual object similarity (see the yellow, red and cyan striped portion of the first bar in <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4E</xref>). In all such cases, since the overlap was larger with the similarity-based category than with the semantic category, semantic membership cannot be taken as the factor at the root of object clustering in the neuronal representation. Rather, it is visual similarity among the members of those semantic categories that is driving object clustering.</p>
<p>Finally, to further test whether animate and inanimate objects were significantly segregated in the IT representation, 100 <italic>k</italic>-means runs were performed with <italic>k</italic> = 2, and the average absolute difference between the fraction of animate objects in the two clusters produced by each <italic>k</italic>-means run was computed. Such a difference amounted to ∼7% and was not significantly larger than expected by chance (i.e., by randomly shuffling the animate and inanimate objects among the clusters produced by each <italic>k</italic>-means run; <italic>p</italic> = 0.39), thus confirming the result of the analysis based on hierarchical clustering (see <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3B</xref>).</p>
<p>Overall, the <italic>k</italic>-means analysis strongly suggests that most object clusters in the recorded IT neuronal representation are explainable by the visual similarity of their members at the level of both shape and, more surprisingly (being IT the highest purely visual brain area), low-level visual properties. Nevertheless, at least a couple of semantic categories exist (i.e., the <italic>four-limbed animals</italic> and the <italic>birds</italic>), whose significant representation in the recorded neuronal population is not accounted by either the shape-based or the low-level visual similarity metrics we used.</p>
</sec><sec id="s2d">
<title>Overlap between clustering hypotheses and D-MST clusters in the IT neuronal space</title>
<p>As a more refined way to infer the structure of visual object representation in IT, we sought an unsupervised approach that would embody the advantages of <italic>k</italic>-means-like partition algorithms (which allow measuring the fraction of overlapping objects between neuronal-based clusters and arbitrary object categories; see <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4</xref>) and hierarchical approaches (which allow assessing the fine-grain relationship between objects within the representation space; see <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3A</xref>). This was achieved by applying a method that has been recently developed in the domain of statistical physics – the D-MST clustering algorithm <xref ref-type="bibr" rid="pcbi.1003167-Bayati1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-BaillyBechet1">[54]</xref>. This method interpolates between Affinity Propagation (a recent, state-of-the art partition algorithm that has been successfully applied in a variety of contexts <xref ref-type="bibr" rid="pcbi.1003167-Frey1">[55]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Leone1">[59]</xref>) and hierarchical Single Linkage clustering <xref ref-type="bibr" rid="pcbi.1003167-Duda1">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Jain1">[61]</xref>. The main advantage of the D-MST method over the <italic>k</italic>-means (and similar partition methods, such as Affinity Propagation) is to allow non-spherical clusters, i.e., to allow loosening the implicit assumption that all the elements of a cluster lie within some distance to some point (i.e., the centre of the cluster). In fact, the output of this method is not simply a partition of the elements into clusters, but, rather, it is a forest, i.e., a partition of the elements into trees (see <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>). As a result, the outcome of the D-MST algorithm contains richer information about the topology/structure of the data, as compared to the output of the <italic>k</italic>-means (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
<fig id="pcbi-1003167-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.g005</object-id><label>Figure 5</label><caption>
<title>Overlap between D-MST clusters in the IT neuronal space and object categories of the clustering hypotheses.</title>
<p>The five most stable clusters resulting from applying the D-MST clustering algorithm to the IT object representation (see also <xref ref-type="supplementary-material" rid="pcbi.1003167.s002">Fig. S2</xref>). The colored frames indicate the subsets of objects that, within each cluster, significantly overlapped with a semantic, a shape-based or a low-level category. The name of the overlapping category is reported near to each frame, together with the overlap's significance level (same overlap score and significance level symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>). The width and shade of the links connecting the images reflect the robustness of the links across different runs of the D-MST algorithm: thinner/lighter links appeared less frequently in the D-MST outcome with respect to thicker/darker links.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.g005" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003167-g005">Figure 5</xref> shows the five most stable clusters (see also <xref ref-type="supplementary-material" rid="pcbi.1003167.s002">Fig. S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003167.s004">Text S1</xref>) extracted by the D-MST algorithm from the recorded IT object representation (named <italic>neuronal-based clusters</italic> in the following). The fact that the number of D-MST clusters was much lower than the optimal number of <italic>k</italic>-means clusters (see previous section and <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4A</xref>) is not surprising – the nature of these two clustering methods is very different, and the number of clusters they yield cannot be directly compared. In fact, the D-MST clusters have an inner hierarchical structure that incorporates as sub-trees what partition methods (such as the <italic>k</italic>-means) would segregate into separate clusters. The advantage of the D-MST approach is to make explicit the relationship among such sub-trees, thus providing additional topological information that, with other methods, would be lost. This can be appreciated by inspecting, for example, cluster #3, which is mostly made of objects with low area, but with different sub-trees containing objects with different features (e.g., vertically oriented edges, horizontally oriented edges, curved boundaries, etc.); or cluster #4, which is made of two distinct sub-trees, one containing round objects and another containing horizontally elongated objects; or cluster #5, in which there is a transition from star-shaped objects (on the left sub-trees) to objects containing sharp edges (on the right sub-trees), passing through a central region of spiky objects.</p>
<p><xref ref-type="fig" rid="pcbi-1003167-g005">Figure 5</xref> also shows what subsets of objects, within each cluster, significantly overlapped with one of the object categories of the clustering hypotheses. Critically, the significance of the overlap was computed through a permutation test that took into account the unrooted-tree internal structure of the D-MST clusters and the existence of <italic>twin</italic> objects (i.e., the fact that, as in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>, our object set contained at least two exemplars/twins of any given object – two horses, two hats, two monkey faces, five human faces, etc; see <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref>). This was achieved by measuring the fraction of overlapping objects between a given category and all possible sub-trees of contiguous objects within a cluster, with the significance of the overlap assessed by randomly permuting sets of <italic>twin</italic> objects across the categories of a given clustering hypotheses (1,000,000 permutations were run; see <xref ref-type="supplementary-material" rid="pcbi.1003167.s004">Text S1</xref> for details). Twins' sets, rather than individual objects, were permuted, because visual inspection of <xref ref-type="fig" rid="pcbi-1003167-g005">Figure 5</xref> revealed that twins had a strong tendency to lie nearby in the IT representation space (i.e., a strong tendency to be directly connected in the D-MST clusters). This is not surprising, since twins are, in general, very similar at the pixel level, and, as a result, they typically belong to the same shape-based and low-level category, beside belonging, by definition, to the same semantic category (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1</xref>). Therefore, the presence of twins tends to inflate the overlap between sub-trees within the D-MST clusters and object categories. Permuting twins' sets, rather than individual objects, allows taking into account this bias in the construction of the null distributions of overlap scores, against which the measured overlaps are compared to establish their significance. This yields a very conservative test, in which each set of twins counts as a single object, thus removing, de facto, any contribution of pixel-level similarity among twins to the computation of chance overlap scores.</p>
<p>As expected, this approach provided a very conservative outcome: only a few categories of the clustering hypotheses were found that significantly overlapped with sub-trees within the D-MST clusters (Holm-Bonferroni corrected ** <italic>p</italic>&lt;0.01 and * <italic>p</italic>&lt;0.05; see third-to-last column in <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref>). This number increased if the Holm-Bonferroni correction was released, yielding two, four and four significant overlaps with categories, respectively, of the semantic, the shape-based and the low-level hypotheses (see third-to-last column in <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref> and corresponding yellowish, reddish and bluish frames in <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>).</p>
<table-wrap id="pcbi-1003167-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.t001</object-id><label>Table 1</label><caption>
<title>Overlapping between semantic categories and D-MST neuronal-based clusters.</title>
</caption><alternatives><graphic id="pcbi-1003167-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Category</td>
<td align="left" rowspan="1" colspan="1">D-MST Cluster</td>
<td align="left" rowspan="1" colspan="1">Ratio 1</td>
<td align="left" rowspan="1" colspan="1">Ratio 2</td>
<td align="left" rowspan="1" colspan="1">Overlap</td>
<td align="left" rowspan="1" colspan="1"><italic>p</italic> (twins)</td>
<td align="left" rowspan="1" colspan="1">Signif.</td>
<td align="left" rowspan="1" colspan="1">p (obj.)</td>
<td align="left" rowspan="1" colspan="1">Signif.</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Four-limb. anim.</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.73</td>
<td align="left" rowspan="1" colspan="1">0.96</td>
<td align="left" rowspan="1" colspan="1">0.71</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Faces</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">0.78</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.78</td>
<td align="left" rowspan="1" colspan="1">0.0023</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Fishes</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.75</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.75</td>
<td align="left" rowspan="1" colspan="1">0.0742</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0007</td>
<td align="left" rowspan="1" colspan="1">*+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Sea invertebr.</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.86</td>
<td align="left" rowspan="1" colspan="1">0.46</td>
<td align="left" rowspan="1" colspan="1">0.0840</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0004</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Birds</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.48</td>
<td align="left" rowspan="1" colspan="1">0.48</td>
<td align="left" rowspan="1" colspan="1">0.1048</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0003</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Music instr.</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.75</td>
<td align="left" rowspan="1" colspan="1">0.43</td>
<td align="left" rowspan="1" colspan="1">0.1140</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0012</td>
<td align="left" rowspan="1" colspan="1">*+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Vehicles</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.46</td>
<td align="left" rowspan="1" colspan="1">0.67</td>
<td align="left" rowspan="1" colspan="1">0.37</td>
<td align="left" rowspan="1" colspan="1">0.2617</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0065</td>
<td align="left" rowspan="1" colspan="1">++</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Insects</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.58</td>
<td align="left" rowspan="1" colspan="1">0.47</td>
<td align="left" rowspan="1" colspan="1">0.35</td>
<td align="left" rowspan="1" colspan="1">0.3635</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0192</td>
<td align="left" rowspan="1" colspan="1">+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Tools</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.58</td>
<td align="left" rowspan="1" colspan="1">0.44</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.4587</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0365</td>
<td align="left" rowspan="1" colspan="1">+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Trees</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">0.6240</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0979</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Buildings</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.8883</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.1471</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>The table reports the overlap (fifth column) between each semantic category (first column) and the D-MST neuronal-based cluster (second column) containing the best matching sub-tree of contiguous objects, according to a score defined as the ratio between the intersection of the sub-tree with the category and their union (fifth column). Significance of the overlap was computed by permuting (1,000,000 times) either sets of twin objects (forth- and third-to-last columns) or individual objects (second-to-last and last columns) across the categories of a given clustering hypotheses: Holm-Bonferroni corrected <italic>p</italic>&lt;0.01 (**) and <italic>p</italic>&lt;0.05 (* and *+); and uncorrected <italic>p</italic>&lt;0.01 (++ and *+) and <italic>p</italic>&lt;0.05 (+). For comparison with <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, two other overlap metrics (Ratio 1 = the fraction of objects in the category overlapping with the cluster; and Ratio 2 = the fraction of objects in the cluster overlapping with the category) are also reported.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1003167-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.t002</object-id><label>Table 2</label><caption>
<title>Overlapping between shape-based categories and D-MST neuronal-based clusters.</title>
</caption><alternatives><graphic id="pcbi-1003167-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Category</td>
<td align="left" rowspan="1" colspan="1">D-MST Cluster</td>
<td align="left" rowspan="1" colspan="1">Ratio 1</td>
<td align="left" rowspan="1" colspan="1">Ratio 2</td>
<td align="left" rowspan="1" colspan="1">Overlap</td>
<td align="left" rowspan="1" colspan="1"><italic>p</italic> (twins)</td>
<td align="left" rowspan="1" colspan="1">Signif.</td>
<td align="left" rowspan="1" colspan="1"><italic>p</italic> (obj.)</td>
<td align="left" rowspan="1" colspan="1">Signif.</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">#2 (round)</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#14 (star-like)</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">0.71</td>
<td align="left" rowspan="1" colspan="1">0.91</td>
<td align="left" rowspan="1" colspan="1">0.67</td>
<td align="left" rowspan="1" colspan="1">0.0007</td>
<td align="left" rowspan="1" colspan="1">*+</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#8 (dim)</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.78</td>
<td align="left" rowspan="1" colspan="1">0.78</td>
<td align="left" rowspan="1" colspan="1">0.64</td>
<td align="left" rowspan="1" colspan="1">0.0097</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#13 (vertical thin)</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.52</td>
<td align="left" rowspan="1" colspan="1">0.68</td>
<td align="left" rowspan="1" colspan="1">0.42</td>
<td align="left" rowspan="1" colspan="1">0.0347</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">0.0002</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#6 (horiz. thin)</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.41</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.41</td>
<td align="left" rowspan="1" colspan="1">0.0520</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0003</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#1 (bright)</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.57</td>
<td align="left" rowspan="1" colspan="1">0.66</td>
<td align="left" rowspan="1" colspan="1">0.44</td>
<td align="left" rowspan="1" colspan="1">0.0748</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0004</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#5 (horiz. thick)</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.44</td>
<td align="left" rowspan="1" colspan="1">0.87</td>
<td align="left" rowspan="1" colspan="1">0.41</td>
<td align="left" rowspan="1" colspan="1">0.0927</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0008</td>
<td align="left" rowspan="1" colspan="1">*+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#12 (diagonal)</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.47</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.32</td>
<td align="left" rowspan="1" colspan="1">0.4299</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0392</td>
<td align="left" rowspan="1" colspan="1">+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#15</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.4878</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0368</td>
<td align="left" rowspan="1" colspan="1">+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#10</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.45</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.31</td>
<td align="left" rowspan="1" colspan="1">0.5313</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0667</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#11</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.31</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.30</td>
<td align="left" rowspan="1" colspan="1">0.5347</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0582</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#4</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.45</td>
<td align="left" rowspan="1" colspan="1">0.41</td>
<td align="left" rowspan="1" colspan="1">0.28</td>
<td align="left" rowspan="1" colspan="1">0.7109</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.1694</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#7 (pointy)</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">0.60</td>
<td align="left" rowspan="1" colspan="1">0.23</td>
<td align="left" rowspan="1" colspan="1">0.9279</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.4949</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#9</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.29</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.22</td>
<td align="left" rowspan="1" colspan="1">0.9451</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.5630</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#3</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.40</td>
<td align="left" rowspan="1" colspan="1">0.22</td>
<td align="left" rowspan="1" colspan="1">0.9530</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.5768</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>The table reports the overlap (fifth column) between each shape-based category (first column) and the D-MST neuronal-based cluster (second column) containing the best matching sub-tree of contiguous objects. Same table structure and symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1003167-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.t003</object-id><label>Table 3</label><caption>
<title>Overlapping between low-level categories and D-MST neuronal-based clusters.</title>
</caption><alternatives><graphic id="pcbi-1003167-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Category</td>
<td align="left" rowspan="1" colspan="1">D-MST Cluster</td>
<td align="left" rowspan="1" colspan="1">Ratio 1</td>
<td align="left" rowspan="1" colspan="1">Ratio 2</td>
<td align="left" rowspan="1" colspan="1">Overlap</td>
<td align="left" rowspan="1" colspan="1"><italic>p</italic> (twins)</td>
<td align="left" rowspan="1" colspan="1">Signif.</td>
<td align="left" rowspan="1" colspan="1"><italic>p</italic> (obj.)</td>
<td align="left" rowspan="1" colspan="1">Signif.</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">High area</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">0.93</td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1">0.93</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low contrast</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.60</td>
<td align="left" rowspan="1" colspan="1">0.82</td>
<td align="left" rowspan="1" colspan="1">0.53</td>
<td align="left" rowspan="1" colspan="1">0.0103</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">0.0000</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low area</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.60</td>
<td align="left" rowspan="1" colspan="1">0.69</td>
<td align="left" rowspan="1" colspan="1">0.47</td>
<td align="left" rowspan="1" colspan="1">0.0333</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">0.0001</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">High luminance</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.53</td>
<td align="left" rowspan="1" colspan="1">0.80</td>
<td align="left" rowspan="1" colspan="1">0.47</td>
<td align="left" rowspan="1" colspan="1">0.0352</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">0.0001</td>
<td align="left" rowspan="1" colspan="1">**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low aspect ratio</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.40</td>
<td align="left" rowspan="1" colspan="1">0.86</td>
<td align="left" rowspan="1" colspan="1">0.37</td>
<td align="left" rowspan="1" colspan="1">0.1910</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0049</td>
<td align="left" rowspan="1" colspan="1">++</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">High aspect ratio</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.83</td>
<td align="left" rowspan="1" colspan="1">0.31</td>
<td align="left" rowspan="1" colspan="1">0.4760</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.0454</td>
<td align="left" rowspan="1" colspan="1">+</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low luminance</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.42</td>
<td align="left" rowspan="1" colspan="1">0.28</td>
<td align="left" rowspan="1" colspan="1">0.9240</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.5116</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">High contrast</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.36</td>
<td align="left" rowspan="1" colspan="1">0.21</td>
<td align="left" rowspan="1" colspan="1">0.9761</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.7167</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label/><p>The table reports the overlap (fifth column) between each low-level category (first column) and the D-MST neuronal-based cluster (second column) containing the best matching sub-tree of contiguous objects. Same table structure and symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>.</p></fn></table-wrap-foot></table-wrap>
<p>Noticeably, out of the two semantic categories that significantly overlapped with a sub-tree within a D-MST cluster, only for the <italic>four-limbed animals</italic> (in cluster #1) such an overlap was not accountable by the similarity of their members, since the <italic>faces</italic> (in cluster #4) were part of a larger sub-tree of <italic>round</italic> objects with <italic>high area</italic>. Moreover, although cluster #1 contained both a large subset of <italic>four-limbed animals</italic> and a large subset of <italic>birds</italic>, only the former was compactly represented, while the latter was very scattered, thus suggesting that the proximity of the birds was mostly mediated by other objects in the cluster. Since our overlap measure took into account the compactness of a given object category within a tree (see above), no significant overlap between the <italic>birds</italic> category and any sub-tree within cluster #1 was found.</p>
<p>The results shown in <xref ref-type="fig" rid="pcbi-1003167-g005">Figure 5</xref> provide a very robust and conservative assessment of what semantic and visual similarity-based categories were represented in our recorded IT population. However, in previous studies <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, the significance of the overlap between neuronal-based clusters and object categories was computed without compensating for the existence of multiple (very similar) exemplars of the same objects (i.e., twins). For easier comparison with such studies, we also computed the significance of the overlap scores reported in <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref> by randomly shuffling individual objects, rather than twins' sets. This yielded an additional set of semantic categories that significantly overlapped with sub-trees within the D-MST clusters – <italic>birds</italic>, <italic>sea invertebrates</italic>, <italic>fishes</italic> and <italic>music instruments</italic> (Holm-Bonferroni corrected ** <italic>p</italic>&lt;0.01 and * <italic>p</italic>&lt;0.05; see last column in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>). However, an even larger increase of overlaps was found between sub-trees within the D-MST clusters and shape-based and low-level object categories (see last column in <xref ref-type="table" rid="pcbi-1003167-t002">Tables 2</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref>). Critically, in most cases, these overlaps with the visual similarity-based categories accounted for the overlaps with the semantic categories (the same way roundness accounted for the clustering of faces in cluster #4 of <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>). In fact, the <italic>sea invertebrates</italic> were part of the larger cluster of <italic>star-like</italic> shapes in cluster #5; the <italic>fishes</italic> were part of the larger cluster of <italic>horizontal thick</italic> objects in cluster #1; and the <italic>music instruments</italic> were part of the larger cluster of <italic>horizontal thin</italic> objects in cluster #3 (cross-compare <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref> and the third-to-last and last columns of <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t002">2</xref>). Therefore, regardless of the level of conservativeness of the permutation test, the D-MST clustering analysis strongly suggests that visual similarity, rather than semantic membership, was at the root of the structure of visual object representations in the recorded IT population (with the noticeable exception to the <italic>four-limbed animals</italic> and, to a lesser extent, the <italic>birds</italic> semantic categories).</p>
<p>This conclusion was strengthened by the qualitative observation of the D-MST clusters, whose internal structure provided a richness of information that was not always captured by our overlap and similarity metrics. For instance, four-legged grand-pianos and four-wheeled cars (among other inanimate objects) belonged to the same cluster of the four-limbed animals, thus suggesting that some shared, hard-to-quantify visual property, rather than semantic membership, may have underlain the grouping of objects in cluster #1. Similarly, shared visual features likely played a relevant role in determining the clustering of other groups of objects (see, for instance, the objects with high spatial frequency texture/patterns in tree #2, or the objects with curved or round elements in tree #3).</p>
<p>Overall, the object clustering produced by the D-MST algorithm suggests the existence of a rich multi-level object representation in IT, which is largely driven by the similarity of visual objects across a spectrum of visual properties, ranging from low-level image attributes to complex combinations of shape features that are often hard to model and quantify.</p>
</sec><sec id="s2e">
<title>Read-out of object category membership from the IT population activity</title>
<p>Unsupervised approaches, such as the clustering methods described in the previous sections, have the main advantage of discovering the “natural” internal structure of neuronal object representations, but do not provide a direct assessment of how much information a neuronal population conveys about a given object set (e.g., a semantic or a visual similarity-based category). In addition, since they are based on average firing rates computed in a time epoch following stimulus presentation, they do not take into account the trial-by-trial variability of neuronal responses <xref ref-type="bibr" rid="pcbi.1003167-Quiroga4">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Rolls3">[63]</xref>. As an alternative, a useful tool to directly estimate the representational power of a neuronal population (and take into account trial-by-trial response variability) is provided by supervised decoding approaches, such as discriminant-based linear classifiers <xref ref-type="bibr" rid="pcbi.1003167-Quiroga4">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Hung1">[64]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Agam1">[67]</xref>. These approaches are particularly appealing when dealing with neuronal representations, since they are based on linear read-out schemes that are plausibly implementable by the neuronal machinery.</p>
<p>We estimated the power of the recorded IT population to support classification of the objects belonging to the categories of the clustering hypotheses, by building binary Fisher Linear Discriminants (FLDs) <xref ref-type="bibr" rid="pcbi.1003167-Duda1">[60]</xref>. The FLDs were trained to learn the mapping between the neuronal population response vectors and the labels that were assigned to each object according to a given binary classification task (e.g., faces vs. all other objects in the set). We then measured the performance of the classifiers at generalizing to novel population responses (i.e., at correctly labeling left-out population vectors that were not used during training), using standard cross-validation procedures to establish the variability and significance of the classification performance (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Specifically, we tested the capability of the FLDs to correctly classify visual objects that were not used to build (i.e., train) the classifiers. That is, all the population vectors obtained across different presentations of a given object in a given category (e.g., a given face in the <italic>faces</italic> category) were excluded from the training set, and one of such left-out population vectors was used to test the classifier performance in the cross-validation procedure.</p>
<p>The average classification performance of the FLDs was significantly higher than what expected by randomly permuting the object labels (<italic>p</italic>&lt;0.05; see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details) for all the semantic categories, most of the shape-based categories (13 out of 15), and all the low-level categories (see <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6A</xref>). At first, this result may seem surprising (and at odd with our previous analyses; see <xref ref-type="fig" rid="pcbi-1003167-g003">Figs. 3</xref>–<xref ref-type="fig" rid="pcbi-1003167-g005">5</xref>), but it can be easily understood, by considering the existence of multiple (very similar) exemplars of the same objects (i.e., the twins) in our stimulus set (see <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref>). Indeed, the large (and significant) classification performance obtained for virtually all the FLDs in <xref ref-type="fig" rid="pcbi-1003167-g006">Figure 6A</xref> is fully consistent with the large number of significant overlaps between D-MST clusters and object categories reported in the last column of <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref> (i.e., when the significance of the overlap was computed without compensating for the existence of twins).</p>
<fig id="pcbi-1003167-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.g006</object-id><label>Figure 6</label><caption>
<title>Fisher Linear Discriminant (FLD) analysis of IT population activity.</title>
<p>(A) Each gray bar reports the average performance of a binary FLD at correctly classifying members of a given object category (e.g., faces) from all other objects in the set. For each binary classification task, the standard deviation of the performance (error bars), and the mean and standard deviation of the null distribution (gray circles and their error bars), against which significant deviation of performance from chance was assessed (same significance level symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>), are also reported (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for a description of the cross-validation and permutation procedures yielding these summary statistics). (B) Examples of “pruned” semantic, shape-based and low-level categories that were obtained by subsampling the original object categories (shown in <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1</xref>), so as to minimize the overlap between semantic and visual information (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). (C) Performance of the FLDs at correctly classifying members of the pruned categories (same symbols as in A).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.g006" position="float" xlink:type="simple"/></fig>
<p>To understand how twins can explain the high performances of the FLDs, it should be recalled that, as shown by the D-MST clusters (see <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>), twins typically lay nearby in the neuronal representation space. Therefore, it is not surprising that an FLD, trained to classify a given member in pair of twins, successfully classifies the other member of the pair (when this member is used as the left-out test object). The problem is that, for most twins, it is impossible to know whether it is their shared semantic membership or their visual similarity that drives their clustering in the neuronal space (and, therefore, the high performance of the FLDs). In fact, twins belong, by construction, to the same semantic category (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1A</xref>), but, in most cases, they also belong to the same shape-based or low-level category (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Figs. S1B</xref>–C), being twins, in general, very similar, in terms of shape, orientation, pose, contrast, luminance, etc (compare adjacent objects in <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref>).</p>
<p>The issue with twins brings up the more general issue of how to fully disentangle the contributions of semantic membership and shape similarity to the establishment of cortical visual object representations, when sets of natural objects (containing many similar members of the same semantic categories) are used to probe such representations. To tackle this issue, and better dissociate semantic information from visual information, we subsampled/pruned the object categories, so as to obtain semantic categories made only of dissimilar objects, and shape-based/low-level categories made only of objects with different semantic membership. This was achieved by imposing the constraints that: 1) no pair of objects in any given semantic category belonged to the same shape-based or low-level category; 2) no pair of objects in any given shape-based or low-level category belonged to the same semantic category; and 3) only a single exemplar of any set of twins (e.g., a single human face or a single hat) belonged to any given category. Since many different “pruned” categories could be obtained from any of the original object categories, the subsampling procedure was repeated many times (once for each cross-validation run; see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details; examples of pruned categories are shown <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6B</xref>). We then measured the performance of the FLDs at correctly classifying left-out objects from such pruned categories (see <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>).</p>
<p>As expected, the classification performance of the FLDs was much reduced, as compared to what obtained with the original categories (compare <xref ref-type="fig" rid="pcbi-1003167-g006">Figs. 6A and C</xref>). Only three of the subordinate semantic categories (<italic>birds</italic>, <italic>four-limbed animals</italic>, and <italic>insects</italic>; see <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>, first panel) were classified with a performance that was higher than what expected by chance (<italic>p</italic>&lt;0.05, permutation test; see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). In addition, the animate category (as a whole) was discriminated with higher than chance performance from the inanimate category. Among the categories defined by visual similarity, five shape-based categories (<italic>round</italic>, <italic>star-like</italic>, <italic>horizontal thin, pointy</italic> and <italic>vertical thin</italic> objects), as well as six low-level categories (<italic>high</italic> and <italic>low area</italic>, <italic>high</italic> and <italic>low luminance</italic> and <italic>high</italic> and <italic>low aspect ratio</italic> objects), were all classified with higher than chance performance by the FLDs (see second and third panels in <xref ref-type="fig" rid="pcbi-1003167-g006">Figs. 6C</xref>). Among all tested categories, the highest classification performance (&gt;75% correct) was obtained for the shape-based category of <italic>round</italic> objects (this was the only performance to remain significantly higher than chance, after that a Bonferroni correction for multiple comparisons was applied).</p>
<p>Overall, the result of the FLD analysis, applied to the pruned categories, was in good agreement with the result of the D-MST clustering, when significance was computed by permuting twins' sets (see <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref> and <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref>, third-to-last column). Comparing the outcome of the two analyses (see <xref ref-type="table" rid="pcbi-1003167-t004">Tables 4</xref>–<xref ref-type="table" rid="pcbi-1003167-t006">6</xref>), only a few differences emerged. For instance, the <italic>insects</italic> (among the semantic categories) and the <italic>pointy objects</italic> (among the shape-based categories) were significantly represented in the neuronal space according to the FLD analysis, but not according to the D-MST. Similarly, the animate and inanimate categories were linearly separable according to the FLD analysis, although animate and inanimate objects were not sharply segregated in different D-MST clusters (as also shown by the hierarchical clustering and <italic>k</italic>-means analysis; see <xref ref-type="fig" rid="pcbi-1003167-g003">Figs. 3A-B</xref> and <xref ref-type="fig" rid="pcbi-1003167-g004">4</xref>). Such discrepancies are not surprising, since, in general, supervised and unsupervised multivariate approaches provide complementary information about data representations – for instance, linear separability (as measured by FLDs' classification performance) is not bound to perfectly match the clustering of data in a representational space (see further comments in the <xref ref-type="sec" rid="s3">Discussion</xref>). Hence, the importance of combing both kinds of approaches when exploring a multivariate data set. When this was done, and the outcomes of the D-MST and FLD analyses were taken together, a very conservative assessment of what object categories were represented by the recorded IT population was achieved (see last column in <xref ref-type="table" rid="pcbi-1003167-t004">Tables 4</xref>–<xref ref-type="table" rid="pcbi-1003167-t006">6</xref>) – one semantic category (the <italic>four-limbed animals</italic>), three shape-based categories (<italic>round</italic>, <italic>star-like</italic> and <italic>vertical thin</italic> objects), and three low-level categories (<italic>high area</italic>, <italic>low area</italic> and <italic>high luminance</italic>) turned out to be significantly represented according to both approaches. Overall, this confirmed that visual similarity (at the level of both shape and lower-order properties) accounted for the neuronal representation of visual objects better than semantic membership did.</p>
<table-wrap id="pcbi-1003167-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.t004</object-id><label>Table 4</label><caption>
<title>Semantic categories significantly represented in IT according to the D-MST and the FLD analyses.</title>
</caption><alternatives><graphic id="pcbi-1003167-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Category</td>
<td align="left" rowspan="1" colspan="1">Signif. D-MST (twins' sets perm.)</td>
<td align="left" rowspan="1" colspan="1">Signif. FLD (pruned cat.)</td>
<td align="left" rowspan="1" colspan="1">Signif. D-MST &amp; FLD</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Four-limb. anim.</td>
<td align="left" rowspan="1" colspan="1">**</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Faces</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Birds</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Insects</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt104"><label/><p>The second and third columns report what semantic categories were found to be significantly represented in IT according, respectively, to the D-MST analysis (when significance was computed by permuting twins' sets; i.e., same data as in <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref> and in the third-to-last column of <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>) and to the FLD analysis (when classifiers were applied to the pruned object categories; i.e., same data as in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>). Same significance level symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>. The last column shows what semantic categories were found to be significantly represented in IT according to both the D-MST and the FLD analyses.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1003167-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.t005</object-id><label>Table 5</label><caption>
<title>Shape-based categories significantly represented in IT according to the D-MST and the FLD analyses.</title>
</caption><alternatives><graphic id="pcbi-1003167-t005-5" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.t005" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Category</td>
<td align="left" rowspan="1" colspan="1">Signif. D-MST (twins' sets perm.)</td>
<td align="left" rowspan="1" colspan="1">Signif. FLD (pruned cat.)</td>
<td align="left" rowspan="1" colspan="1">Signif. D-MST &amp; FLD</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">#2 (round)</td>
<td align="left" rowspan="1" colspan="1">**</td>
<td align="left" rowspan="1" colspan="1">*+</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#14 (star-like)</td>
<td align="left" rowspan="1" colspan="1">*+</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#8 (dim)</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#13 (vertical thin)</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#6 (horiz. thin)</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">#7 (pointy)</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt105"><label/><p>The second and third columns report what shape-based categories were found to be significantly represented in IT according, respectively, to the D-MST analysis (when significance was computed by permuting twins' sets; i.e., same data as in <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref> and in the third-to-last column of <xref ref-type="table" rid="pcbi-1003167-t002">Table 2</xref>) and to the FLD analysis (when classifiers were applied to the pruned object categories; i.e., same data as in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>). Same significance level symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>. The last column shows what shape-based categories were found to be significantly represented in IT according to both the D-MST and the FLD analyses.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1003167-t006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003167.t006</object-id><label>Table 6</label><caption>
<title>Low-level categories significantly represented in IT according to the D-MST and the FLD analyses.</title>
</caption><alternatives><graphic id="pcbi-1003167-t006-6" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003167.t006" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Category</td>
<td align="left" rowspan="1" colspan="1">Signif. D-MST (twins' sets perm.)</td>
<td align="left" rowspan="1" colspan="1">Signif. FLD (pruned cat.)</td>
<td align="left" rowspan="1" colspan="1">Signif. D-MST &amp; FLD</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">High area</td>
<td align="left" rowspan="1" colspan="1">**</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low contrast</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low area</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">High luminance</td>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1">++</td>
<td align="left" rowspan="1" colspan="1">✓</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low aspect ratio</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">High aspect ratio</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Low luminance</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">+</td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt106"><label/><p>The second and third columns report what low-level categories were found to be significantly represented in IT according, respectively, to the D-MST analysis (when significance was computed by permuting twins' sets; i.e., same data as in <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref> and in the third-to-last column of <xref ref-type="table" rid="pcbi-1003167-t003">Table 3</xref>) and to the FLD analysis (when classifiers were applied to the pruned object categories; i.e., same data as in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>). Same significance level symbols as in <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>. The last column shows what low-level categories were found to be significantly represented in IT according to both the D-MST and the FLD analyses.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s3">
<title>Discussion</title>
<p>This study investigated what visual object properties were represented in a neuronal population that was recorded from monkey inferotemporal cortex. To this aim, we defined three alternative hypotheses that could underlie the clustering of a battery of visual objects within the IT neuronal representation space: 1) shared semantic membership; 2) shared visual shape features (i.e., shape similarity); and 3) shared low-level visual properties. We then applied an array of unsupervised and supervised machine learning approaches to understand whether the object categories defined by these hypotheses were robustly represented in the recorded IT neuronal population. Based on these approaches, we concluded that the coarse clustering of visual objects in the neuronal representation space was mainly driven by low-level visual properties, while its finer-grain structure depended on higher-level shape features, with little role played by semantic membership (although our analyses cannot exclude that at least one semantic category – the <italic>four-limbed animals</italic> – was also robustly represented in the recorded IT population).</p>
<p>These conclusions are mostly in disagreement with those of two recent studies <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> that also investigated the nature of object representations in monkey IT (and its human homologous). In these studies, the authors found a sharp segregation between animate and inanimate objects, and a finer-grain clustering within the animate category that matched closely several subordinates semantic categories (named “intuitive” or “human-conventional” categories by the authors), such as faces, body parts, four-limbed animals, fishes, reptiles, butterflies, etc. Most remarkably, these studies were unable to find any visual-similarity metric that could produce object clusters matching those found in the neuronal representation.</p>
<p>The conclusions reached by our study are consistent with <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, only as far as the representation of a few animate categories is concerned: the <italic>four-limbed animals</italic> (see <xref ref-type="fig" rid="pcbi-1003167-g004">Figs. 4</xref>, <xref ref-type="fig" rid="pcbi-1003167-g005">5</xref> and <xref ref-type="fig" rid="pcbi-1003167-g006">6C</xref>, and <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref> and <xref ref-type="table" rid="pcbi-1003167-t004">4</xref>) and, to a lesser extent, the <italic>birds</italic> (see <xref ref-type="fig" rid="pcbi-1003167-g004">Figs. 4</xref> and <xref ref-type="fig" rid="pcbi-1003167-g006">6C</xref>, and <xref ref-type="table" rid="pcbi-1003167-t004">Table 4</xref>). However, we did not find any other semantic category that was significantly represented in the recorded IT population according to all (as in the case of <italic>four-limbed animals</italic>) or most (as in the case of <italic>birds</italic>) the multivariate approaches we applied. For instance, the <italic>insects</italic> were found to be linearly discriminable by the FLDs (see <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>), but no compact clusters of insects were found by the <italic>k</italic>-means and the D-MST clustering algorithms. In the case of <italic>faces</italic>, their clustering in the neuronal representation space was accounted for by their visual similarity, rather than their shared semantic membership (as shown by the fact that faces were part of a larger cluster of objects with round shape and large area; see <xref ref-type="fig" rid="pcbi-1003167-g004">Figs. 4</xref> and <xref ref-type="fig" rid="pcbi-1003167-g005">5</xref>) – when pruned face categories made only of dissimilar faces were built, the FLDs were no longer able to correctly classify them (compare <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6A and C</xref>). Finally, no sharp segregation between animate and inanimate objects was observed (but see further discussion below). On the other hand, we found several shape features and lower-level visual properties that successfully accounted for the clustering of some visual objects in the IT neuronal representation. Among others, the more prominent are: 1) object area, which determined the gross topology of object clustering in the IT representation (see <xref ref-type="fig" rid="pcbi-1003167-g003">Figs. 3C–E</xref>); 2) other low-level image properties, such as object luminance and aspect ratio (see <xref ref-type="fig" rid="pcbi-1003167-g004">Figs. 4</xref>–<xref ref-type="fig" rid="pcbi-1003167-g006">6</xref> and <xref ref-type="table" rid="pcbi-1003167-t003">Tables 3</xref> and <xref ref-type="table" rid="pcbi-1003167-t006">6</xref>); 3) shape features, such as specific arrangements of edges and boundaries that defined round, horizontally elongated, vertically elongated and star-like objects (see <xref ref-type="fig" rid="pcbi-1003167-g004">Figs. 4</xref>–<xref ref-type="fig" rid="pcbi-1003167-g006">6</xref> and <xref ref-type="table" rid="pcbi-1003167-t002">Tables 2</xref> and <xref ref-type="table" rid="pcbi-1003167-t005">5</xref>).</p>
<sec id="s3a">
<title>Animate and inanimate objects are not sharply segregated in the IT representation</title>
<p>In our study, animate and inanimate objects were found to be equally distributed among the first two nodes of the dendrogram produced by hierarchical clustering (see <xref ref-type="fig" rid="pcbi-1003167-g003">Figs. 3A, B</xref>) and in the two clusters obtained by running the <italic>k</italic>-means algorithm with <italic>k</italic> = 2. In addition, most of the clusters produced by the <italic>k</italic>-means (<xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4</xref>) and D-MST (<xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>) algorithms contained a mixture of animate and inanimate objects. However, the FLDs were able to distinguish animate from inanimate objects with higher than chance performance, even after that visual similarity among members within each category was minimized (see <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>). The latter finding is not contradictory with the results of the cluster analyses, since it is indicative of the compactness of some subordinate semantic categories (such as the <italic>four-limbed animals</italic> and the <italic>faces</italic>; see <xref ref-type="fig" rid="pcbi-1003167-g004">Figs. 4</xref> and <xref ref-type="fig" rid="pcbi-1003167-g005">5</xref>), rather than of the superordinate animate category as a whole. In particular, FLDs, being supervised approaches, do not need to follow the “natural” object segregation in the IT representation (as revealed by the unsupervised clustering methods). Rather, given the high dimensionality of the representation space, FLDs could find a hyperplane segregating the two main animate groups (i.e., <italic>four-limbed animals</italic> and the <italic>faces</italic>) from the inanimate objects, even if those groups belong to different “natural” clusters.</p>
<p>In conclusion, our analysis strongly suggests that animate and inanimate objects are not sharply segregated within the IT representation, at least as we have sampled it here. At the same time, however, they are not randomly scattered across the IT neuronal space. Instead, some subordinate animate categories form compact clusters in the IT representation (although, in some cases, simply because of the visual similarity of their members). This conclusion, while being at odd with <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>, is in agreement with a recent fMRI study showing that, in the body-selective regions of monkey inferotemporal cortex, objects do not primarily segregate according to whether they belong to the animate or the inanimate categories <xref ref-type="bibr" rid="pcbi.1003167-Popivanov1">[34]</xref>.</p>
</sec><sec id="s3b">
<title>Comparison with other studies</title>
<p>The discrepancy between our and previous results <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> is not easily explained. The stimulus presentation protocols (monkeys viewing images presented in rapid sequence) and the region from which the neuronal responses were recorded (anterior IT) are comparable (although not fully overlapping; see further discussion below). The analytical approaches are at least partially overlapping, although in our study more advanced tools derived from statistical mechanics were used.</p>
<p>One potentially important difference is the way in which statistical significance of the overlap between the object categories and the neuronal-based clusters was evaluated. We took into account the effect of having sets of very similar exemplars of the same objects (i.e., twin objects) on the outcome of the statistical tests (see <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref> and the third-to-last row in <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref>). We also tried to fully dissociate the representation of visual similarity and semantic membership by building semantic categories that contained only very dissimilar objects, and shape-based categories that contained only objects with different semantic membership (see <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6B–C</xref>). As far as we understand, the effect of twins on the overlap score was not taken into account by Kiani and colleagues <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, in spite of the many different exemplars of the same objects contained in their object set. As shown by our results, shuffling objects rather than twins' sets in the statistical analysis, dramatically increased the number of significant overlaps between D-MST neuronal-based clusters and object categories (compare the third-to-last and last columns in <xref ref-type="table" rid="pcbi-1003167-t001">Tables 1</xref>–<xref ref-type="table" rid="pcbi-1003167-t003">3</xref>). The impact of shape similarity on the representation of semantic categories was shown to be even more dramatic in the case of the FLD analysis – minimizing shape similarity within semantic categories dramatically reduced the number of categories, whose elements were classified with higher than chance performance by FLDs (compare the first panels in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6A</xref> and <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6C</xref>).</p>
<p>The failure of the visual similarity metrics used by <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> to account for object clustering in IT could be explained by the different metrics used in their studies and ours. In particular, although we used the same object recognition model <xref ref-type="bibr" rid="pcbi.1003167-Serre1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Riesenhuber1">[68]</xref> to quantify shape similarity, our implementation of the model included a much larger number of output units (24,451) as compared to <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref> (674). In fact, we did not try to match the number of model output units to the number of recorded neurons (as done by <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>), since our goal was not to model IT, but, rather, to find a metric that was as powerful as possible in capturing the visual shape similarity among the objects in our set.</p>
<p>Another substantial difference is represented by the stimulus set. The objects used in our experiments were grayscale pictures of natural objects, while, in the studies of <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>, color pictures were used. Color is obviously a very salient object feature that could have strongly influenced the object clustering reported in those studies. For instance, human faces, hands, body parts and, to a lesser extent, monkey faces, as well as the fur of many animals, all have a pink/brownish hue that could have driven their clustering in the superordinate category of animate objects. Noticeably, in the above-mentioned fMRI study that found no segregation between animate and inanimate objects, grayscale pictures were used <xref ref-type="bibr" rid="pcbi.1003167-Popivanov1">[34]</xref>. In conclusion, the use of colorful images in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> represents a major confounding factor, since IT color tuning may interact with IT shape tuning in ways that are hard to quantify/model.</p>
<p>Yet another difference is the lower number of visual objects we tested (213), the smaller population of IT neurons we recorded (94), and the smaller extent of IT cortex we sampled, as compared to Kiani and colleagues (who tested 1,084 objects and recorded the responses of 674 IT neurons). These are three separate, but related, issues, each deserving a specific discussion.</p>
<p>While, in general, recording from a wider IT neuronal sample would lead to a more refined assessment of IT neuronal population coding, it is unclear whether major qualitative differences in the structure of visual object representations would emerge as a function of the size of the recorded neuronal pool. Previous investigations of population coding in IT have shown a gradual increase of the amount of information conveyed by a pool of IT neurons about object identity or category as a function of the pool size, but they have not reported any dramatic qualitative shift in what the neuronal pool would code depending on its size <xref ref-type="bibr" rid="pcbi.1003167-Meyers1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Hung1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Rust1">[66]</xref>. In addition, these studies have revealed that small pools of IT neurons can be as effective (or more effective) than much larger populations, as long as their selectivity for object identity or category is very strong. In this regard, it should be noted that Kiani and colleagues recorded every neuron they could isolate regardless of its stimulus responsiveness or selectivity, which could potentially have resulted in a neuronal pool with many unresponsive or non-selective cells (they report that 38% of their neurons were category selective). By contrast, we recorded only cells with a statistically reliable response to at least one of the objects in our stimulus set, thus obtaining a population of neurons with robust tuning across the tested objects (see <xref ref-type="bibr" rid="pcbi.1003167-Zoccolan1">[38]</xref>). Based on the above-mentioned population coding studies, this suggests that Kiani and colleagues' larger IT sample could only be marginally better than our smaller (but more selective) neuronal pool at estimating IT neuronal representations of visual objects (the large performances achieved by the FLDs in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6</xref> confirm the effectiveness of the sampled IT population at conveying information about features/properties of our object set).</p>
<p>As far as the size of the stimulus set is concerned, it should be noted that a larger stimulus set does not necessarily mean a better stimulus set, when it comes to disentangling alternative clustering hypotheses. First, as pointed out above, a large number of very similar exemplars per category could lead to an overestimation of the significance of the overlap between neuronal-based clusters and, for instance, semantic categories, if not properly taken into account in the statistical analysis. Second, although our semantic categories typically contained less exemplars than those used by Kiani and colleagues, the superordinate categories of animate and animate objects used in our study contained a large number of exemplars. Nevertheless, as pointed out above, we did not found any sharp segregation of these two categories in the IT representation.</p>
<p>Finally, one factor that could explain some of the discrepancies between our conclusions and those of Kiani and colleagues is the different extent of IT cortex that was sampled in the two studies. Our recordings targeted the most medial part of the ventral bank of STS and of the ventral surface lateral to AMTS (see blue dots and red-shaded areas in <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>) and spanned a 13–17 mm anteroposterior range, while Kiani and colleagues sampled a larger portion of IT, both mediolaterally (i.e., including the gyrus between STS and AMTS), and anteroposteriorly (i.e., a 13/15–20 mm span; see <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref> in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>). This suggests that recordings in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref> may have sampled sub-regions in IT that are known to contain enriched populations of face-selective cells (i.e., the anterior face patches AL and AM <xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref>; see <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>), while, in our study, only a minimal overlap between recording sites and face patch AM could, in principle, be expected (in practice, our IT sample did not contain any cell that was sharply tuned for faces; see <xref ref-type="supplementary-material" rid="pcbi.1003167.s003">Fig. S3</xref> and further discussion in the next Section). This could explain why in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, differently from our study, a sharp clustering of human, monkey, and animal faces was found in the IT representation.</p>
<p>To conclude, it is hard to infer what methodological differences may be at the root of the discrepancies between our study and <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>. Above, we have listed some of the differences that could be crucial. Ultimately, however, only a re-analysis of Kiani and colleagues' data with our analytical/statistical approaches, or, better, a full new set of recordings (e.g., with grayscale versions of the images used by Kiani and colleagues) could shed more light on the causes of these discrepancies. Both approaches are clearly beyond the scope of this study, but could be an interesting target of future investigations by ours or other groups.</p>
</sec><sec id="s3c">
<title>Validity and implications of our findings</title>
<p>As pointed out in a recent review <xref ref-type="bibr" rid="pcbi.1003167-Kourtzi1">[13]</xref>, two main competing ideas exist about what kind of object information is coded by the ventral stream, and, in particular, by its highest stage – anterior IT. On the one hand, many single-unit studies in monkeys support the notion of structural (or shape-based) representations along the ventral stream – i.e., combinations of object-defining visual features of increasing complexity are coded along the ventral steam, with the highest complexity of configural coding reached in anterior IT (see <xref ref-type="bibr" rid="pcbi.1003167-Tanaka1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Connor1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-DiCarlo1">[6]</xref> for a review). On the other hand, another line of evidence (mainly coming from human lesion and fMRI studies) supports the existence of semantic categorical representations along the ventral stream – i.e., human high-level representations of visual objects segregate according to object function/meaning rather than shape <xref ref-type="bibr" rid="pcbi.1003167-Kanwisher1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Connolly1">[24]</xref>. The findings reported by <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> have added evidence based on monkey single-unit recordings to support the latter notion.</p>
<p>Our study, on the contrary, strongly supports the notion that inferotemporal neuronal ensembles, in the monkey brain, mainly represent visual, rather than semantic, information. In particular, our analyses show that IT response patterns code not only structural/configural shape information of various complexity, but also a whole array of low-level image properties (such a overall luminance, area, aspect ratio, etc.). On the one hand, this is surprising, since this kind of low-level information contained in the visual input is typically though to be extracted by lower-level visual areas and not to be preserved and coded in IT. On the other hand, previous computational and empirical studies have shown that object identity is represented along with other low-level properties in IT, namely position and size. In particular, it has been shown that IT neuronal ensembles can code not only object identity regardless of position/size <xref ref-type="bibr" rid="pcbi.1003167-Hung1">64</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Rust1">66</xref> (thus conveying a position/size invariant object representation), but they can also code object position/size regardless of object identity <xref ref-type="bibr" rid="pcbi.1003167-Hung1">[64]</xref>, and can jointly code object position and identity <xref ref-type="bibr" rid="pcbi.1003167-Li3">[65]</xref> (i.e., report the identity of a specific object at a specific visual field location). Our findings not only confirm these previous conclusions (extending them to a larger set of low-level properties), but also show how, topologically, some low-level properties (e.g., area of the visual field subtended by each visual object) and higher-order shape features are co-represented in IT (with the former determining the gross topology of object representations and the latter determining their finer-grain structure).</p>
<p>As far as semantic categories are concerned, only for the four-limbed animals we observed a significant and robust representation of semantic membership that could not be accounted by their visual similarity (see <xref ref-type="table" rid="pcbi-1003167-t004">Table 4</xref>). This could either be the result of an extremely transformation-invariant population code of animal-like objects (and, therefore, still shape-based, although not captured by our visual similarity metrics), or could reflect learned associations between objects with dissimilar shape but similar meaning/function. The latter hypothesis would be consistent with the finding that neurons in higher-order areas of both the ventral and the dorsal streams can learn to encode general categorical associations between arbitrary visual patterns <xref ref-type="bibr" rid="pcbi.1003167-Sakai1">[25]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Miyashita1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Miyashita2">[69]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Fitzgerald1">[71]</xref>, and would support the notion that semantic (or categorical) representations do exist in monkey IT <xref ref-type="bibr" rid="pcbi.1003167-Meyers1">[30]</xref>, at least for a few selected, behaviorally relevant categories.</p>
<p>This raises the issue of what object categories, in our stimulus set, can be considered as behaviorally relevant (or meaningful) for the monkeys. This is obviously an important issue, when considering the generality of our conclusions, since the failure to observe a significant representation for most semantic categories could be due to their lack of “meaning” for the monkeys. While some animate categories (such as the four-limbed animals and the faces) are likely meaningful for the monkeys (either because innately such, or because meaning may have been acquired through repeated exposure to members of these categories, e.g., other monkeys and humans), other categories (especially among the inanimate set) are likely arbitrary collections of objects for the monkeys. Regardless of their likely meaningfulness for the monkeys, there are three reasons why it was important for us to ask how well all these categories were represented in IT. First, our semantic categories were defined so as to match as close as possible those defined by Kiani and colleagues <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, who found a significant representation in IT not only for four-limbed animals and faces, but also for most other animate categories (i.e., birds, reptiles, butterflies, fishes, etc.). Moreover, although in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref> only one of the inanimate subordinate categories (cars) was found to be significantly represented in IT, the inanimate category, as a whole, was sharply segregated from the animate category. Since one of the goals of our study was to provide a comparison with the findings of Kiani and colleagues, it was essential to test how well the animate and inanimate categories, as well as all their possible subordinate categories were represented in IT. Second, our monkeys had a daily, prolonged exposure not only to other monkeys and humans, but also to a variety of inanimate objects, such as toys, fruits, vegetables, furniture, tools and equipment used in the animal facility and in the lab (some of which are similar to the inanimate objects contained of our stimulus set). Therefore, if the representation of visual objects in monkey IT is organized, at its most superordinate level, according to an animate/inanimate distinction (as concluded in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref>), there is no reason to believe that the development of such an animate/inanimate segregation was precluded to our monkeys. Hence, the relevance of testing the existence of such a segregation and provide a comparison with <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>. Third, testing the representation of inanimate (but also animate) categories without any obvious meaning for the monkeys (e.g., music instruments or sea invertebrates; see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1</xref>) served as a demonstration that shape similarity among members of the same semantic category, if not properly taken into account in the statistical analysis, can easily lead to an overestimation of how well semantic membership is represented in visual cortex. This is shown by the many semantic categories that were found to have a significant representation in IT according to the D-MST and FLD analyses, unless shape similarity (e.g., the presence of twins) was properly accounted for (compare the significance levels in the third-to-last and last columns of <xref ref-type="table" rid="pcbi-1003167-t001">Table 1</xref>, and compare <xref ref-type="fig" rid="pcbi-1003167-g006">Figs. 6A and C</xref>). In summary, testing the many animate and inanimate categories used in our study provides a valuable comparison with previous reports <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Kriegeskorte1">[15]</xref> (e.g., about the animate/inanimate segregation), and cautions against giving semantic interpretations of cortical activity patterns that may actually reflect visual shape similarity. Finally, it is worth pointing out that, as a way to better understand to what extent behaviorally relevant categories are represented in monkey IT, future studies should first try to establish what objects are naturally perceived/judged by monkeys as belonging to the same categories (e.g., by relying on priming or adaptation aftereffect paradigms that allow measuring what objects are spontaneously judged as similar by a subject <xref ref-type="bibr" rid="pcbi.1003167-Leopold1">[72]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Suzuki1">[78]</xref>).</p>
<p>To conclude, it should be stressed that the validity and generality of our conclusions are intrinsically limited by the limited extent of cortex that was explored through single-unit recordings here, as compared to the large cortical areas that are imaged in fMRI studies. In particular, single-unit recordings, unless paired with fMRI, cannot precisely target cortical regions that are known to represent specific object categories in monkey IT. For instance, our recordings did not specifically target any of the so-called monkey face patches <xref ref-type="bibr" rid="pcbi.1003167-Bell1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Tsao1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref> or other IT regions that are rich of face selective neurons (summarized in <xref ref-type="bibr" rid="pcbi.1003167-Baylis1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Rolls2">[42]</xref>) and, therefore, it is not surprising that no clusters entirely made of faces were found in our study (instead, the face cluster was part of larger clusters of objects with round shape and large area; e.g., see cluster # 4 in <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>). In particular, given the across-monkey variability in the precise locations of face patches (<xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref> shows the range of possible locations for the three anterior face patches, based on <xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref>), the fact that we did not record from the dorsal bank of STS (thus excluding an overlap between our recording sites and face patch AF; see <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>), and the fact that our recordings targeted the most medial part of IT (thus excluding any overlap between our recording sites and face patch AL; see <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>), it is very unlikely that our IT sample contained a large fraction of face cells. Although an overlap between our recording sites and face patch AM is, in principle, possible (see <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>), we verified that our sampled IT population did not contain any cell that was sharply tuned for faces, by computing, for each neuron, the Face Selectivity Index (FSI) proposed by <xref ref-type="bibr" rid="pcbi.1003167-Tsao2">[32]</xref>. Differently from what reported for face cells (e.g., see <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref> in <xref ref-type="bibr" rid="pcbi.1003167-Tsao2">[32]</xref>), none of the neurons recorded in our study had a FSI exceeding 0.5 (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s003">Fig. S3A</xref>). Moreover, those few cells with FSI∼0.5 typically did not show a sharp segregation between responses to faces and non-faces, and often had, as preferred stimuli, non-face objects (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s003">Fig. S3B</xref>). Because of such a lack of sharp tuning for faces at the single cell level, it is not surprising that neither the <italic>k</italic>-means (see <xref ref-type="fig" rid="pcbi-1003167-g004">Fig. 4</xref>) nor the D-MST clustering algorithms (see <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>) returned any pure cluster of faces. Very likely, if our recordings had targeted a wider extent of IT cortex (as in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>) or had focused on sub-regions, within IT, that are rich of face-selective neurons <xref ref-type="bibr" rid="pcbi.1003167-Tsao1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003167-Tsao3">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Baylis1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Rolls2">[42]</xref>, pure face clusters would have been observed. On the other hand, having found compact clusters of four-limbed animals and (to a lesser extent) birds suggests that at least a fraction of the neurons sampled in our study may have belonged to body selective IT regions (whose existence is also well-established in monkey IT <xref ref-type="bibr" rid="pcbi.1003167-Bell1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Tsao1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Popivanov1">[34]</xref>). In other words, our data, while showing that visual shape similarity is the main factor determining IT object representations, do not contradict the findings of earlier fMRI studies about the existence of face and body patches in IT.</p>
<p>In summary, the quantitative characterization of the IT response patterns performed in this study, while leaving open the possibility that a few, behaviorally salient semantic categories may be represented in monkey inferotemporal cortex, strongly reasserts the primary function of IT as a visual area, in which, in addition to moderately to highly complex shape information, a surprisingly large number of low-level visual properties is also represented.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>The data analyzed in this study were obtained from the same experiments described in <xref ref-type="bibr" rid="pcbi.1003167-Zoccolan1">[38]</xref>. We point the reader to this former study for a full description of surgical, behavioral, and recording procedures. Here we only provide those details that are essential to the understanding of the present study. All animal procedures were performed in accord with National Institute of Health guidelines and the Massachusetts Institute of Technology Committee on Animal Care.</p>
<sec id="s4a">
<title>Visual stimuli and behavioral task</title>
<p>All recorded neurons were probed with a fixed set of 213 grayscale pictures of isolated objects that included: 1) 188 images of real-world objects belonging to 94 different categories (e.g., two hats, two accordions, two monkey faces, etc.) of the Caltech 101 database <xref ref-type="bibr" rid="pcbi.1003167-FeiFei1">[79]</xref>; 2) five cars, five human faces, and five abstract silhouettes; 3) five patches of texture; 4) four low-contrast images of one of the objects; and 5) a blank frame. The full set is shown in <xref ref-type="fig" rid="pcbi-1003167-g002">Figure 2</xref>.</p>
<p>All objects subtended 2° of visual angle. During recordings, both monkeys were engaged in a simple recognition task that required the detection of a fixed target shape (a red triangle) that was presented at the end of a temporal sequence of object conditions drawn from our stimulus set (see <xref ref-type="bibr" rid="pcbi.1003167-Zoccolan1">[38]</xref>). The total number of stimulus conditions presented on each behavioral trial ranged from 3 to 20. The target was always the last in the sequence, and each monkey was rewarded for maintaining fixation (1.5° fixation window) until the appearance of the target and then making a saccade to a fixed visual field location (7° eccentricity) within 800 ms after the appearance of the target. Visual stimuli were presented at a rate of 5 per second; i.e., each stimulus condition was shown for 100 ms, followed by 100 ms of a gray screen (no stimulus), followed by another stimulus condition for 100 ms, etc. This task was meant to obtain a large amount of data, while still engaging the animal in a recognition task</p>
</sec><sec id="s4b">
<title>Neuronal recordings</title>
<p>During each recording session, a single extracellular metal electrode was advanced into IT through a stainless still guide tube that was inserted into a plastic cylindrical recording chamber (Crist Instruments). The chamber was placed over a craniotomy targeting the temporal lobe in the left hemisphere from the top of the skull. Over ∼6 months of daily recording sessions in the two monkeys, we sampled neurons over an ∼5×4 mm area of the ventral superior temporal sulcus and ventral surface lateral to the anterior middle temporal sulcus (Horsley-Clarke anteroposterior coordinates: 13–17 mm), corresponding to several 1 mm-spaced grid locations of the recording chamber (see <xref ref-type="fig" rid="pcbi-1003167-g001">Fig. 1</xref>). We recorded a total of 94 well-isolated single units. Each isolated neuron was initially tested for responsiveness across the set of 213 objects, presented at the center of gaze, using the following criterion: a neuron was considered responsive if its mean firing rate was significantly higher than background rate for at least one of these objects (<italic>t</italic> test, <italic>p</italic>&lt;0.005). Responsive neurons were further screened to identify their preferred receptive field location (RF center) within a 2° span around the center of gaze (see <xref ref-type="bibr" rid="pcbi.1003167-Zoccolan1">[38]</xref> for details). Following these screening procedures, complete recordings from each neuron were obtained by presenting the full set of 213 objects at the neuron's RF center. Five to thirty presentation repetitions were collected for each object condition.</p>
</sec><sec id="s4c">
<title>Similarity metric for population responses</title>
<p>Neuronal responses were quantified by computing the average number of spikes per second fired by a neuron (i.e., average firing rate) across all repetitions of a given object, over a time window starting 100 ms and ending 200 ms after stimulus presentation. Similarly to what done in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, the responses of a neuron across the object set were normalized by first subtracting their mean value (across the set) and then dividing by their standard deviation. This normalization compensated for differences in baseline activity and firing rate range across the recorded neuronal population, and allowed weighting equally all the neurons contributing to the population representation of a given object. Each visual object was thus represented by a neuronal population vector having as components the normalized responses of all the recorded neurons to that object. As in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, the similarity between the population vectors representing two visual objects <italic>i</italic> and <italic>j</italic> was measured by computing their Pearson correlation coefficient (<italic>r<sub>ij</sub></italic>). This metric was chosen because it is sensitive to the profile of activation of the neurons produced by a given object, rather than to the absolute magnitude of the activation. The distance (or dissimilarity) between the population vectors <italic>i</italic> and <italic>j</italic> was then defined as <italic>d<sub>ij</sub></italic> = 1−<italic>r<sub>ij</sub></italic> (the resulting dissimilarity matrix <italic>D</italic> is depicted in <xref ref-type="fig" rid="pcbi-1003167-g003">Fig. 3A</xref>).</p>
</sec><sec id="s4d">
<title>Unsupervised multivariate approaches</title>
<p>Three standard unsupervised approaches were used to understand the structure of visual object representations in IT: 1) average linkage hierarchical agglomerative clustering; 2) <italic>k</italic>-means clustering; and 3) Principal Components Analysis (PCA). The optimal number of <italic>k</italic>-means clusters was determined by the Bayes Information Criterion (BIC) and the Akaike Information Criterion (AIC) <xref ref-type="bibr" rid="pcbi.1003167-Claeskens1">[52]</xref>. In addition to these standard approaches (whose description can be found in various textbooks and reviews; see <xref ref-type="bibr" rid="pcbi.1003167-Duda1">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Jain1">[61]</xref>), a more advanced method, developed within the domain of Statistical Physics, was also applied to strengthen our multivariate analysis: the D-MST clustering algorithm. This is a recently proposed method <xref ref-type="bibr" rid="pcbi.1003167-Bayati1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-BaillyBechet1">[54]</xref>, which allows interpolating between <italic>partitional</italic> clustering methods, such as <italic>k</italic>-means <xref ref-type="bibr" rid="pcbi.1003167-Duda1">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Jain1">[61]</xref> and Affinity Propagation <xref ref-type="bibr" rid="pcbi.1003167-Frey1">[55]</xref>, and hierarchical clustering methods <xref ref-type="bibr" rid="pcbi.1003167-Duda1">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Jain1">[61]</xref>. Its output is a so-called forest, i.e., a set of clusters, each of which is a tree (see <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>). As the <italic>k</italic>-means, the D-MST clustering algorithm is non-deterministic, and takes two parameters as input: 1) the maximum depth of the trees <italic>d</italic><sub>max</sub> (i.e., the maximum number of links between any image and the image at the center of a tree); and 2) <italic>λ</italic>, which determines the number of resulting clusters (a bigger <italic>λ</italic> results in less clusters). As a way to determine the set of parameters that gave the most robust assessment of object clustering in IT, we imposed that both the number of clusters and their internal structure (i.e., the overlap between the clusters/trees resulting from repeated executions of the algorithm) be stable over a large range of parameters (50 executions of the algorithm were run for each assignment of the parameters). This yielded a single region of the parameter space fulfilling our stability criteria (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s002">Fig. S2</xref>), corresponding to a partition of the object set into five trees with depth <italic>d</italic><sub>max</sub> = 6. From this region, five trees/clusters were extracted by keeping the most stable links across multiple runs of the D-MST (shown in <xref ref-type="fig" rid="pcbi-1003167-g005">Fig. 5</xref>). A more detailed description of the method is provided in <xref ref-type="supplementary-material" rid="pcbi.1003167.s004">Text S1</xref>.</p>
</sec><sec id="s4e">
<title>Clustering hypotheses</title>
<p>The neuronal-based object clusters produced by the algorithms described above were compared to object categories obtained according to three different clustering hypotheses: 1) shared semantic membership; 2) shared shape features; and 3) shared low-level visual properties.</p>
<p>Eleven semantic categories (shown in <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1A</xref>) were built according to the criteria established in <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>. These categories were further grouped into the two superordinate categories of animate and inanimate objects.</p>
<p>Fifteen categories of objects sharing shape features (shown in <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1B</xref>) were obtained as the result of object clustering in the output layer of a well-known hierarchical model of object recognition <xref ref-type="bibr" rid="pcbi.1003167-Serre1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Mutch1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003167-Riesenhuber1">[68]</xref>. For our application, we have chosen the version of the model described in <xref ref-type="bibr" rid="pcbi.1003167-Mutch1">[44]</xref> (and downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.mit.edu/~jmutch/fhlib/" xlink:type="simple">http://www.mit.edu/~jmutch/fhlib/</ext-link> – version 8), which consists of four layers of artificial neural units named S1, C1, S2, and C2. Units S1 are a bank of Gabor filters with various orientations, spatial frequencies, positions and scales. Units C1 implement an OR-like operation on subsets of S1 afferent units, having the same orientation tuning but in different positions/scales. Units S2 perform a template matching (AND-like) operation on subsets of C1 afferent units to gain tuning for a particular combination of visual features. In this version of the model, the templates to which these units are tuned are random patches of images taken from the Caltech 101 database (different S2 units are built having as a template the same image patch, but at different positions and scales). In the output layer of the model, C2 units perform again an OR-like operation on subsets of S2 afferent units tuned for the same image patch, but at different positions and scales. In our instantiation of the models, 24,451 C2 output units were built. These units convey the more explicit (i.e., more shape selective and position/scale tolerant) representation of visual objects provided by the model. They could therefore be used to assess the similarity of our visual objects at the level of shared middle- to high-level shape features. This was achieved by running a <italic>k</italic>-means clustering algorithm over the representation of our object set provided by the model's output units, so as to obtain 15 groups of objects with similar features. The number of groups was set to 15 to match the optimal number of <italic>k</italic>-means clusters found in the IT neuronal representation using the BIC and AIC criteria (see previous section).</p>
<p>Eight categories of objects sharing low-level visual properties (shown in <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1C</xref>) were defined on the base of four global properties of the images of the objects – luminance, contrast, area and aspect ratio. Each category contained 15 images having either the highest or the lowest values of one of such properties, which were defined as following. Luminance was defined as the average pixel intensity of the object image, divided by the maximum of the grayscale range (i.e., 255). Area was defined as the fraction of pixel, in the image frame, that was occupied by the image of the object. Note that object area, as defined here, is different from object size, which was fixed to ∼2° of visual angle for all the objects. Contrast was defined as: (median(pixels&gt;128)−median(pixels&lt;128))/(median(pixels&gt;128)+median(pixels&lt;128)). Aspect-ratio was defined as the maximum, across all the possible rotations, of the height of an object image divided by its width.</p>
</sec><sec id="s4f">
<title>Overlap score</title>
<p>For easier comparison with <xref ref-type="bibr" rid="pcbi.1003167-Kiani1">[14]</xref>, the overlap between a <italic>k</italic>-means neuronal-based cluster and an object category was assessed with the same score used in that study, i.e., as the average of Ratio 1 and Ratio 2 (where Ratio 1 is the fraction of objects in the category overlapping with the cluster and Ratio 2 is the fraction of objects in the cluster overlapping with the category). Significance of the overlap score was assessed by a permutation test, in which, after reshuffling the objects among the clusters, the overlap scores were recomputed to obtain a null distribution. In the case of the clusters produced by the D-MST algorithm, the overlap score was defined as the intersection between a given cluster and a given category, divided by their union. Significance was assessed by a permutation test, which, as explained in the <xref ref-type="sec" rid="s2">Results</xref>, took into account the presence of twin exemplars in the object set (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s004">Text S1</xref> for a full description).</p>
</sec><sec id="s4g">
<title>Fisher Linear Discriminant (FLD) analysis</title>
<p>The ability of the recorded IT population to code the category membership of visual objects was estimated by building binary Fisher Linear Discriminants/classifiers (FLDs). Each classifier was trained to find the best hyperplane separating, in the neuronal representation space, the objects belonging to a given category from all other objects (FLDs achieve this by maximizing the ratio of the between-category variance to the within-category variance <xref ref-type="bibr" rid="pcbi.1003167-Duda1">[60]</xref>). Since the neurons were not recorded simultaneously, pseudo-population response vectors were built by assigning to each component of any given vector the number of spikes that each neuron fired in a randomly sampled (with replacement) presentation of a given object. Seven of such pseudo-population vectors were built for each object (being seven the median number of repetitions per object and neuron obtained during recordings). The entire set of pseudo-population vectors were built anew for each cross-validation run of the classifier (see below).</p>
<p>Classifier performance was measured in cross-validation loops. In each loop, the classifier was trained using all the available population vectors, with the exception of all the vectors corresponding to two left-out objects, one from the category that the classifier was being trained to discriminate, and one from the complementary set. For any given classification task, performance at correctly classifying left-out vectors was measured over a set of 30 cross-validation loops, with a different pair of left-out objects randomly chosen in each loop. Each set of cross-validation loops constituted a cross-validation run and 3,500 such runs were executed for each binary classification task, so as to obtain average performances and their standard errors (see histogram bars and their error bars in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6</xref>). Following <xref ref-type="bibr" rid="pcbi.1003167-Agam1">[67]</xref>, significance of the classification performance was assessed with a permutation test, in which object labels were shuffled before executing each cross-validation loop, so as to obtain null distributions of the performance (see gray circles and their error bars in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6</xref>).</p>
<p>The same cross-validation scheme was used to test the significance of the performance at classifying “pruned” categories, i.e., object sets obtained by sub-sampling the original categories and their complementary (negative) sets, so as to disentangle as much as possible semantic from visual information. Pruned categories were built by solving two constraint optimization problems. The goal was to build the largest possible set of objects belonging to a given category, such that visual features would not interfere with semantic features and vice versa. Therefore, when testing for discrimination of a semantic category, we imposed that no pair of objects would belong to the same shape-based or low-level category; when testing for discrimination of a shape-based or low-level category, we imposed that no pair of objects would belong to the same semantic category. For example, when testing for discrimination of the <italic>round</italic> category, which includes, among other objects, also many faces (see <xref ref-type="supplementary-material" rid="pcbi.1003167.s001">Fig. S1B</xref>), only one of the faces was allowed to be included in the pruned category (see second panel in <xref ref-type="fig" rid="pcbi-1003167-g006">Fig. 6B</xref>). We also imposed that no twins appeared together in any pruned category. These constraints applied both to the positive and to the negative (i.e., complementary) classes, and, therefore, the problem had to be solved twice every time. This problem can be easily framed as an integer linear programming problem, and solved using standard kits (<ext-link ext-link-type="uri" xlink:href="http://www.gnu.org/software/glpk/" xlink:type="simple">http://www.gnu.org/software/glpk/</ext-link>). Since several solutions are possible, we introduced a small random noise and solved the problem repeatedly in order to sample from the set of all solutions – one pruned version of the category to be discriminated (and the complementary object set) was built for each of the 3,500 cross-validation runs (see above). In a few instances (e.g., the <italic>fishes</italic>), the resulting “pruned” category had too few objects for the linear classifier analysis to be performed. To assess the significance of the classification performances we built null categories, by first shuffling the twin indices over the whole stimulus set, and then sampling the null positive and negative categories with the same constraints as above. That is, we required the null “semantic” categories to be made of visually dissimilar objects, and the null “visual-based/low-level” categories to be made of objects with different semantic membership. In addition, we forced the null categories to have the same size as the corresponding pruned categories.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003167.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003167.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Object categories of the three clustering hypotheses.</bold> The 11 semantic categories (A), the 15 shape-based categories (B) and the 8 low-level object categories (C). See main text (<xref ref-type="sec" rid="s4">Materials and Methods</xref>) for a definition of the categories.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003167.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003167.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Computation of the stability region in the parameter space of the D-MST clustering algorithm.</bold> Average number of clusters and average overlap (inset) in repeated D-MST clustering outcomes, showing the only stable region of the parameters (found at <italic>d</italic><sub>max</sub> = 6, λ ∈ [0.74,0.88]). The main panel shows the average number of clusters at <italic>d</italic><sub>max</sub> = 6 as a function of the parameter λ (error bars = standard deviations across 50 repeated outcomes of D-MST clustering). The stable region is highlighted in light red. The yellow line represents the linear fit for that region, corresponding to a number of clusters = 4.55±0.03. The inset shows the average overlap between repeated outcomes of the clustering at <italic>d</italic><sub>max</sub> = 6 as a function of λ. For each point, the average overlap is computed over all D-MST outcomes in a sliding window of width 0.15 centered at that point. The blue dot represents the value corresponding to the stable region (overlap = 0.94±0.04). The span of that region is highlighted in light green.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003167.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003167.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Face selectivity of the recorded inferotemporal neurons.</bold> (A) Histogram showing the distribution of the Face Selectivity Index (FSI) across the recorded population of IT neurons. The index was defined, according to Tsao et al (<italic>Science</italic>, 2006), as: FSI = (mean response<sub>faces</sub>−mean response<sub>non-face objects</sub>)/(mean response<sub>faces</sub>+mean response<sub>non-face objects</sub>). Differently from Tsao et al, no neurons were found with a sharp tuning for faces (i.e., with FSI larger than 0.5). (B) Rank-order tuning curves for the four neurons with the largest FSI. Each plot shows the response (i.e., average firing rate) of a neuron across the set of 213 objects used in our study (shown in <xref ref-type="fig" rid="pcbi-1003167-g002">Fig. 2</xref>). For each neuron, objects along the abscissa are ranked based on the response they evoked. The responses evoked by faces (either human, monkey, or dog faces) are marked by specific symbols (see legend in the figure). These tuning curves show how, even for our most face selective cells, non-face objects were often the cells' preferred stimuli, and no sharp segregation between responses to faces and non-face objects was found.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003167.s004" mimetype="application/msword" xlink:href="info:doi/10.1371/journal.pcbi.1003167.s004" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p><bold>Supporting </bold><xref ref-type="sec" rid="s4"><bold>Materials and Methods</bold></xref><bold>.</bold> Description of how the D-MST clustering algorithm was applied in the context of this study.</p>
<p>(DOC)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank G. Kreiman, H. Op de Beeck and T. Shallice for helpful comments on our manuscript. We thank W. Freiwald for his help in establishing what fraction of the inferotemporal neurons recorded in this study may have been sampled from the so-called monkey face patches.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003167-Logothetis1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Sheinberg</surname><given-names>DL</given-names></name> (<year>1996</year>) <article-title>Visual object recognition</article-title>. <source>Ann Rev Neurosci</source> <volume>19</volume>: <fpage>577</fpage>–<lpage>621</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Tanaka1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name> (<year>1996</year>) <article-title>Inferotemporal cortex and object vision</article-title>. <source>Annual Review of Neuroscience</source> <volume>19</volume>: <fpage>109</fpage>–<lpage>139</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Rolls1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>2000</year>) <article-title>Functions of the primate temporal lobe cortical visual areas in invariant visual object and face recognition</article-title>. <source>Neuron</source> <volume>27</volume>: <fpage>205</fpage>–<lpage>218</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Connor1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Brincat</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Pasupathy</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Transformation of shape information in the ventral pathway</article-title>. <source>Current Opinion in Neurobiology</source> <volume>17</volume>: <fpage>140</fpage>–<lpage>147</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Orban1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname><given-names>GA</given-names></name> (<year>2008</year>) <article-title>Higher Order Visual Processing in Macaque Extrastriate Cortex</article-title>. <source>Physiological Reviews</source> <volume>88</volume>: <fpage>59</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-DiCarlo1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name> (<year>2012</year>) <article-title>How Does the Brain Solve Visual Object Recognition?</article-title> <source>Neuron</source> <volume>73</volume>: <fpage>415</fpage>–<lpage>434</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Brincat1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brincat</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name> (<year>2004</year>) <article-title>Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>880</fpage>–<lpage>886</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Yamane1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamane</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Carlson</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Bowman</surname><given-names>KC</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name> (<year>2008</year>) <article-title>A neural code for three-dimensional object shape in macaque inferotemporal cortex</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>1352</fpage>–<lpage>1360</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&amp;cmd=Retrieve&amp;dopt=AbstractPlus&amp;list_uids=18836443" xlink:type="simple">http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&amp;cmd=Retrieve&amp;dopt=AbstractPlus&amp;list_uids=18836443</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Quiroga1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname><given-names>RQ</given-names></name>, <name name-style="western"><surname>Reddy</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Fried</surname><given-names>I</given-names></name> (<year>2005</year>) <article-title>Invariant visual representation by single neurons in the human brain</article-title>. <source>Nature</source> <volume>435</volume>: <fpage>1102</fpage>–<lpage>1107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Quiroga2"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname><given-names>RQ</given-names></name> (<year>2012</year>) <article-title>Concept cells: the building blocks of declarative memory functions</article-title>. <source>Nat Rev Neurosci</source> <volume>13</volume>: <fpage>587</fpage>–<lpage>597</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3251" xlink:type="simple">10.1038/nrn3251</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Quiroga3"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname><given-names>RQ</given-names></name>, <name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Fried</surname><given-names>I</given-names></name> (<year>2008</year>) <article-title>Sparse but not “Grandmother-cell” coding in the medial temporal lobe</article-title>. <source>Trends in Cognitive Sciences</source> <volume>12</volume>: <fpage>87</fpage>–<lpage>91</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2007.12.003" xlink:type="simple">10.1016/j.tics.2007.12.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Brincat2"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brincat</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name> (<year>2006</year>) <article-title>Dynamic shape synthesis in posterior inferotemporal cortex</article-title>. <source>Neuron</source> <volume>49</volume>: <fpage>17</fpage>–<lpage>24</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Kourtzi1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kourtzi</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name> (<year>2011</year>) <article-title>Neural Representations for Object Perception: Structure, Category, and Adaptive Coding</article-title>. <source>Annual Review of Neuroscience</source> <volume>34</volume>: <fpage>45</fpage>–<lpage>67</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-060909-153218" xlink:type="simple">10.1146/annurev-neuro-060909-153218</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Kiani1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Esteky</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Mirpour</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>97</volume>: <fpage>4296</fpage>–<lpage>4309</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Kriegeskorte1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source> <volume>60</volume>: <fpage>1126</fpage>–<lpage>1141</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.10.043" xlink:type="simple">10.1016/j.neuron.2008.10.043</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Bell1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname><given-names>AH</given-names></name>, <name name-style="western"><surname>Hadj-Bouziane</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Frihauf</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Tootell</surname><given-names>RBH</given-names></name>, <name name-style="western"><surname>Ungerleider</surname><given-names>LG</given-names></name> (<year>2009</year>) <article-title>Object Representations in the Temporal Cortex of Monkeys and Humans as Revealed by Functional Magnetic Resonance Imaging</article-title>. <source>J Neurophysiol</source> <volume>101</volume>: <fpage>688</fpage>–<lpage>700</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90657.2008" xlink:type="simple">10.1152/jn.90657.2008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Kanwisher1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>, <name name-style="western"><surname>McDermott</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Chun</surname><given-names>MM</given-names></name> (<year>1997</year>) <article-title>The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception</article-title>. <source>J Neurosci</source> <volume>17</volume>: <fpage>4302</fpage>–<lpage>4311</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Downing1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Downing</surname><given-names>PE</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Shuman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name> (<year>2001</year>) <article-title>A Cortical Area Selective for Visual Processing of the Human Body</article-title>. <source>Science</source> <volume>293</volume>: <fpage>2470</fpage>–<lpage>2473</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1063414" xlink:type="simple">10.1126/science.1063414</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Mahon1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mahon</surname><given-names>BZ</given-names></name>, <name name-style="western"><surname>Milleville</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Negri</surname><given-names>GAL</given-names></name>, <name name-style="western"><surname>Rumiati</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Caramazza</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Action-Related Properties Shape Object Representations in the Ventral Stream</article-title>. <source>Neuron</source> <volume>55</volume>: <fpage>507</fpage>–<lpage>520</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.07.011" xlink:type="simple">10.1016/j.neuron.2007.07.011</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Mahon2"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mahon</surname><given-names>BZ</given-names></name>, <name name-style="western"><surname>Caramazza</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>Concepts and Categories: A Cognitive Neuropsychological Perspective</article-title>. <source>Annual Review of Psychology</source> <volume>60</volume>: <fpage>27</fpage>–<lpage>51</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.psych.60.110707.163532" xlink:type="simple">10.1146/annurev.psych.60.110707.163532</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Naselaris1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Kay</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>Oliver</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2009</year>) <article-title>Bayesian Reconstruction of Natural Images from Human Brain Activity</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>902</fpage>–<lpage>915</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.09.006" xlink:type="simple">10.1016/j.neuron.2009.09.006</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Kanwisher2"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Functional specificity in the human brain: A window into the functional architecture of the mind</article-title>. <source>PNAS</source> <volume>107</volume>: <fpage>11163</fpage>–<lpage>11170</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1005062107" xlink:type="simple">10.1073/pnas.1005062107</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Clarke1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Taylor</surname><given-names>KI</given-names></name>, <name name-style="western"><surname>Devereux</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Randall</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Tyler</surname><given-names>LK</given-names></name> (<year>2012</year>) <article-title>From Perception to Conception: How Meaningful Objects Are Processed over Time</article-title>. <source>Cereb Cortex</source> <volume>23</volume>: <fpage>187</fpage>–<lpage>197</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://cercor.oxfordjournals.org/content/early/2012/01/23/cercor.bhs002" xlink:type="simple">http://cercor.oxfordjournals.org/content/early/2012/01/23/cercor.bhs002</ext-link>. Accessed 18 December 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Connolly1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Connolly</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Guntupalli</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Gors</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hanke</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Halchenko</surname><given-names>YO</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>The Representation of Biological Classes in the Human Brain</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>2608</fpage>–<lpage>2618</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5547-11.2012" xlink:type="simple">10.1523/JNEUROSCI.5547-11.2012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Sakai1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sakai</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1991</year>) <article-title>Neural organization for the long-term memory of paired associates</article-title>. <source>Nature</source> <volume>354</volume>: <fpage>152</fpage>–<lpage>155</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/101038/354152a0" xlink:type="simple">101038/354152a0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Higuchi1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Higuchi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1996</year>) <article-title>Formation of mnemonic neuronal responses to visual paired associates in inferotemporal cortex is impaired by perirhinal and entorhinal lesions</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>93</volume>: <fpage>739</fpage>–<lpage>743</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Miyashita1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1988</year>) <article-title>Neuronal correlate of visual associative long-term memory in the primate temporal cortex</article-title>. <source>Nature</source> <volume>335</volume>: <fpage>817</fpage>–<lpage>820</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/335817a0" xlink:type="simple">10.1038/335817a0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Li1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2008</year>) <article-title>Unsupervised Natural Experience Rapidly Alters Invariant Object Representation in Visual Cortex</article-title>. <source>Science</source> <volume>321</volume>: <fpage>1502</fpage>–<lpage>1507</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1160028" xlink:type="simple">10.1126/science.1160028</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Li2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Representation in Inferior Temporal Cortex</article-title>. <source>Neuron</source> <volume>67</volume>: <fpage>1062</fpage>–<lpage>1075</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.08.029" xlink:type="simple">10.1016/j.neuron.2010.08.029</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Meyers1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyers</surname><given-names>EM</given-names></name>, <name name-style="western"><surname>Freedman</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2008</year>) <article-title>Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex</article-title>. <source>J Neurophysiol</source> <volume>100</volume>: <fpage>1407</fpage>–<lpage>1419</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90248.2008" xlink:type="simple">10.1152/jn.90248.2008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Tsao1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsao</surname><given-names>DY</given-names></name>, <name name-style="western"><surname>Freiwald</surname><given-names>WA</given-names></name>, <name name-style="western"><surname>Knutsen</surname><given-names>TA</given-names></name>, <name name-style="western"><surname>Mandeville</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Tootell</surname><given-names>RBH</given-names></name> (<year>2003</year>) <article-title>Faces and objects in macaque cerebral cortex</article-title>. <source>Nature Neuroscience</source> <volume>6</volume>: <fpage>989</fpage>–<lpage>995</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1111" xlink:type="simple">10.1038/nn1111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Tsao2"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsao</surname><given-names>DY</given-names></name>, <name name-style="western"><surname>Freiwald</surname><given-names>WA</given-names></name>, <name name-style="western"><surname>Tootell</surname><given-names>RBH</given-names></name>, <name name-style="western"><surname>Livingstone</surname><given-names>MS</given-names></name> (<year>2006</year>) <article-title>A Cortical Region Consisting Entirely of Face-Selective Cells</article-title>. <source>Science</source> <volume>311</volume>: <fpage>670</fpage>–<lpage>674</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1119983" xlink:type="simple">10.1126/science.1119983</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Tsao3"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsao</surname><given-names>DY</given-names></name>, <name name-style="western"><surname>Moeller</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Freiwald</surname><given-names>WA</given-names></name> (<year>2008</year>) <article-title>Comparing face patch systems in macaques and humans</article-title>. <source>PNAS</source> <volume>105</volume>: <fpage>19514</fpage>–<lpage>19519</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0809662105" xlink:type="simple">10.1073/pnas.0809662105</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Popivanov1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Popivanov</surname><given-names>ID</given-names></name>, <name name-style="western"><surname>Jastorff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Vanduffel</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Vogels</surname><given-names>R</given-names></name> (<year>2012</year>) <article-title>Stimulus representations in body-selective regions of the macaque cortex assessed with event-related fMRI</article-title>. <source>NeuroImage</source> <volume>63</volume>: <fpage>723</fpage>–<lpage>741</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2012.07.013" xlink:type="simple">10.1016/j.neuroimage.2012.07.013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-OpdeBeeck1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Op de Beeck</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Wagemans</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Vogels</surname><given-names>R</given-names></name> (<year>2001</year>) <article-title>Inferotemporal neurons represent low-dimensional configurations of parameterized shapes</article-title>. <source>Nat Neurosci</source> <volume>4</volume>: <fpage>1244</fpage>–<lpage>1252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Kayaert1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayaert</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Vogels</surname><given-names>R</given-names></name> (<year>2003</year>) <article-title>Shape Tuning in Macaque Inferior Temporal Cortex</article-title>. <source>J Neurosci</source> <volume>23</volume>: <fpage>3016</fpage>–<lpage>3027</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Kayaert2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayaert</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Op de Beeck</surname><given-names>HP</given-names></name>, <name name-style="western"><surname>Vogels</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>Tuning for shape dimensions in macaque inferior temporal cortex</article-title>. <source>European Journal of Neuroscience</source> <volume>22</volume>: <fpage>212</fpage>–<lpage>224</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.2005.04202.x" xlink:type="simple">10.1111/j.1460-9568.2005.04202.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Zoccolan1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kouh</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Dicarlo</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Trade-off between object selectivity and tolerance in monkey inferotemporal cortex</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>12292</fpage>–<lpage>12307</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Zoccolan2"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2005</year>) <article-title>Multiple object response normalization in monkey inferotemporal cortex</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>8150</fpage>–<lpage>8164</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Freedman1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freedman</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name> (<year>2003</year>) <article-title>A comparison of primate prefrontal and inferior temporal cortices during visual categorization</article-title>. <source>J Neurosci</source> <volume>23</volume>: <fpage>5235</fpage>–<lpage>5246</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Baylis1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baylis</surname><given-names>GC</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Leonard</surname><given-names>CM</given-names></name> (<year>1987</year>) <article-title>Functional subdivisions of the temporal lobe neocortex</article-title>. <source>J Neurosci</source> <volume>7</volume>: <fpage>330</fpage>–<lpage>342</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Rolls2"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>2012</year>) <article-title>Invariant Visual Object and Face Recognition: Neural and Computational Bases, and a Model, VisNet</article-title>. <source>Front Comput Neurosci</source> <volume>6</volume>: <fpage>35</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3378046/" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3378046/</ext-link>. Accessed 22 May 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Serre1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2007</year>) <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>104</volume>: <fpage>6424</fpage>–<lpage>6429</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=17404214" xlink:type="simple">http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Citation&amp;list_uids=17404214</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Mutch1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mutch</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lowe</surname><given-names>DG</given-names></name> (<year>2008</year>) <article-title>Object Class Recognition and Localization Using Sparse Features with Limited Receptive Fields</article-title>. <source>International Journal of Computer Vision</source> <volume>80</volume>: <fpage>45</fpage>–<lpage>57</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11263-007-0118-0" xlink:type="simple">10.1007/s11263-007-0118-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Murray1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murray</surname><given-names>RF</given-names></name> (<year>2011</year>) <article-title>Classification images: A review</article-title>. <source>J Vis</source> <volume>11</volume> Available: <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/11/5/2" xlink:type="simple">http://www.journalofvision.org/content/11/5/2</ext-link>. Accessed 29 October 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Nielsen1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nielsen</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Rainer</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>Object features used by humans and monkeys to identify rotated shapes</article-title>. <source>J Vis</source> <volume>8</volume>: <fpage>9 1</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Willmore1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willmore</surname><given-names>BDB</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2010</year>) <article-title>Neural representation of natural images in visual area V2</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>2102</fpage>–<lpage>2114</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4099-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4099-09.2010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-AlemiNeissi1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alemi-Neissi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rosselli</surname><given-names>FB</given-names></name>, <name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name> (<year>2013</year>) <article-title>Multifeatural Shape Processing in Rats Engaged in Invariant Visual Object Recognition</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>5939</fpage>–<lpage>5956</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3629-12.2013" xlink:type="simple">10.1523/JNEUROSCI.3629-12.2013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Schwartz1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2006</year>) <article-title>Spike-triggered neural characterization</article-title>. <source>J Vis</source> <volume>6</volume>: <fpage>484</fpage>–<lpage>507</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Ringach1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ringach</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name> (<year>2004</year>) <article-title>Reverse correlation in neurophysiology</article-title>. <source>Cognitive Science</source> <volume>28</volume>: <fpage>147</fpage>–<lpage>166</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogsci.2003.11.003" xlink:type="simple">10.1016/j.cogsci.2003.11.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Gosselin1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name> (<year>2001</year>) <article-title>Bubbles: a technique to reveal the use of information in recognition tasks</article-title>. <source>Vision Res</source> <volume>41</volume>: <fpage>2261</fpage>–<lpage>2271</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Claeskens1"><label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Claeskens,Gerda, Hjort NL (2008) Model Selection and Model Averaging. Cambridge: Cambridge University Press. Available: <ext-link ext-link-type="uri" xlink:href="http://ideas.repec.org/b/cup/cbooks/9780521852258.html#download" xlink:type="simple">http://ideas.repec.org/b/cup/cbooks/9780521852258.html#download</ext-link>. Accessed 22 February 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Bayati1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bayati</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Borgs</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Braunstein</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chayes</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ramezanpour</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Statistical Mechanics of Steiner Trees</article-title>. <source>Phys Rev Lett</source> <volume>101</volume>: <fpage>037208</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.101.037208" xlink:type="simple">10.1103/PhysRevLett.101.037208</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-BaillyBechet1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bailly-Bechet</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bradde</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Braunstein</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Flaxman</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Foini</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Clustering with shallow trees</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source> <volume>2009</volume>: <fpage>P12010</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1742-5468/2009/12/P12010" xlink:type="simple">10.1088/1742-5468/2009/12/P12010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Frey1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frey</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Dueck</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>Clustering by Passing Messages Between Data Points</article-title>. <source>Science</source> <volume>315</volume>: <fpage>972</fpage>–<lpage>976</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1136800" xlink:type="simple">10.1126/science.1136800</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Garnett1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garnett</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Edelman</surname><given-names>EJ</given-names></name>, <name name-style="western"><surname>Heidorn</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Greenman</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Dastur</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Systematic identification of genomic markers of drug sensitivity in cancer cells</article-title>. <source>Nature</source> <volume>483</volume>: <fpage>570</fpage>–<lpage>575</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11005" xlink:type="simple">10.1038/nature11005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Iorio1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iorio</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Bosotti</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Scacheri</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Belcastro</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Mithbaokar</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Discovery of drug mode of action and drug repositioning from transcriptional responses</article-title>. <source>PNAS</source> <volume>107</volume>: <fpage>14621</fpage>–<lpage>14626</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1000138107" xlink:type="simple">10.1073/pnas.1000138107</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-LindorffLarsen1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Piana</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Dror</surname><given-names>RO</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>DE</given-names></name> (<year>2011</year>) <article-title>How Fast-Folding Proteins Fold</article-title>. <source>Science</source> <volume>334</volume>: <fpage>517</fpage>–<lpage>520</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1208351" xlink:type="simple">10.1126/science.1208351</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Leone1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leone</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sumedha</surname></name>, <name name-style="western"><surname>Weigt</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Clustering by soft-constraint affinity propagation: applications to gene-expression data</article-title>. <source>Bioinformatics</source> <volume>23</volume>: <fpage>2708</fpage>–<lpage>2715</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btm414" xlink:type="simple">10.1093/bioinformatics/btm414</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Duda1"><label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Duda RO, Hart PE, Stork DG (2001) Pattern classification. Wiley. 688 p.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Jain1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jain</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Murty</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Flynn</surname><given-names>PJ</given-names></name> (<year>1999</year>) <article-title>Data clustering: a review</article-title>. <source>ACM Comput Surv</source> <volume>31</volume>: <fpage>264</fpage>–<lpage>323</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/331499.331504" xlink:type="simple">10.1145/331499.331504</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Quiroga4"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname><given-names>RQ</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title>. <source>Nature Reviews Neuroscience</source> <volume>10</volume>: <fpage>173</fpage>–<lpage>185</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2578" xlink:type="simple">10.1038/nrn2578</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Rolls3"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>The neuronal encoding of information in the brain</article-title>. <source>Progress in Neurobiology</source> <volume>95</volume>: <fpage>448</fpage>–<lpage>490</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.pneurobio.2011.08.002" xlink:type="simple">10.1016/j.pneurobio.2011.08.002</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Hung1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hung</surname><given-names>CP</given-names></name>, <name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2005</year>) <article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title>. <source>Science</source> <volume>310</volume>: <fpage>863</fpage>–<lpage>866</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Li3"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2009</year>) <article-title>What Response Properties Do Individual Neurons Need to Underlie Position and Clutter “Invariant” Object Recognition?</article-title> <source>J Neurophysiol</source> <volume>102</volume>: <fpage>360</fpage>–<lpage>376</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90745.2008" xlink:type="simple">10.1152/jn.90745.2008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Rust1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Selectivity and Tolerance (“Invariance”) Both Increase as Visual Information Propagates from Cortical Area V4 to IT</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>12978</fpage>–<lpage>12995</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0179-10.2010" xlink:type="simple">10.1523/JNEUROSCI.0179-10.2010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Agam1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Agam</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Papanastassiou</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Buia</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Golby</surname><given-names>AJ</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Robust Selectivity to Two-Object Images in Human Visual Cortex</article-title>. <source>Current Biology</source> <volume>20</volume>: <fpage>872</fpage>–<lpage>879</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2010.03.050" xlink:type="simple">10.1016/j.cub.2010.03.050</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Riesenhuber1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source> <volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Miyashita2"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1993</year>) <article-title>Inferior Temporal Cortex: Where Visual Perception Meets Memory</article-title>. <source>Annu Rev Neurosci</source> <volume>16</volume>: <fpage>245</fpage>–<lpage>263</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.ne.16.030193.001333" xlink:type="simple">10.1146/annurev.ne.16.030193.001333</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Freedman2"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freedman</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Assad</surname><given-names>JA</given-names></name> (<year>2006</year>) <article-title>Experience-dependent representation of visual categories in parietal cortex</article-title>. <source>Nature</source> <volume>443</volume>: <fpage>85</fpage>–<lpage>88</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature05078" xlink:type="simple">10.1038/nature05078</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Fitzgerald1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitzgerald</surname><given-names>JK</given-names></name>, <name name-style="western"><surname>Freedman</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Assad</surname><given-names>JA</given-names></name> (<year>2011</year>) <article-title>Generalized associative representations in parietal cortex</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>1075</fpage>–<lpage>1079</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2878" xlink:type="simple">10.1038/nn.2878</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Leopold1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leopold</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>O'Toole</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Vetter</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Blanz</surname><given-names>V</given-names></name> (<year>2001</year>) <article-title>Prototype-referenced shape encoding revealed by high-level aftereffects</article-title>. <source>Nat Neurosci</source> <volume>4</volume>: <fpage>89</fpage>–<lpage>94</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Tafazoli1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tafazoli</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Di Filippo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Transformation-Tolerant Object Recognition in Rats Revealed by Visual Priming</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>21</fpage>–<lpage>34</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3932-11.2012" xlink:type="simple">10.1523/JNEUROSCI.3932-11.2012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Wiggs1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiggs</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>A</given-names></name> (<year>1998</year>) <article-title>Properties and mechanisms of perceptual priming</article-title>. <source>Current Opinion in Neurobiology</source> <volume>8</volume>: <fpage>227</fpage>–<lpage>233</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(98)80144-X" xlink:type="simple">10.1016/S0959-4388(98)80144-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Biederman1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>EE</given-names></name> (<year>1991</year>) <article-title>Evidence for complete translational and reflectional invariance in visual object priming</article-title>. <source>Perception</source> <volume>20</volume>: <fpage>585</fpage>–<lpage>593</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Afraz1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Afraz</surname><given-names>S-R</given-names></name>, <name name-style="western"><surname>Cavanagh</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Retinotopy of the face aftereffect</article-title>. <source>Vision Res</source> <volume>48</volume>: <fpage>42</fpage>–<lpage>54</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Bar1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bar</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name> (<year>1998</year>) <article-title>Subliminal Visual Priming</article-title>. <source>Psychological Science</source> <volume>9</volume>: <fpage>464</fpage>–<lpage>468</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1467-9280.00086" xlink:type="simple">10.1111/1467-9280.00086</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-Suzuki1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Suzuki</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Cavanagh</surname><given-names>P</given-names></name> (<year>1998</year>) <article-title>A shape-contrast effect for briefly presented stimuli</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>24</volume>: <fpage>1315</fpage>–<lpage>1341</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-1523.24.5.1315" xlink:type="simple">10.1037/0096-1523.24.5.1315</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003167-FeiFei1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Fergus</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Perona</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories</article-title>. <source>IEEE</source> CVPR 2004, Workshop on Generative-Model Based Vision.</mixed-citation>
</ref>
<ref id="pcbi.1003167-Cox1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Papanastassiou</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Oreper</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Andken</surname><given-names>BB</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2008</year>) <article-title>High-Resolution Three-Dimensional Microelectrode Brain Mapping Using Stereo Microfocal X-ray Imaging</article-title>. <source>J Neurophysiol</source> <volume>100</volume>: <fpage>2966</fpage>–<lpage>2976</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90672.2008" xlink:type="simple">10.1152/jn.90672.2008</ext-link></comment></mixed-citation>
</ref>
</ref-list></back>
</article>