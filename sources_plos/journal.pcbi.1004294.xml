<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01468</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004294</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Opponent Channel Population Code of Sound Location Is an Efficient Representation of Natural Binaural Sounds</article-title>
<alt-title alt-title-type="running-head">Learning Auditory Spatial Receptive Fields from Natural Stereo Sounds</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>M≈Çynarski</surname> <given-names>Wiktor</given-names></name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="fn" rid="currentaff001"><sup>¬§</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Max-Planck Institute for Mathematics in the Sciences, Leipzig, Germany</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of T√ºbingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The author has declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: WM. Performed the experiments: WM. Analyzed the data: WM. Contributed reagents/materials/analysis tools: WM. Wrote the paper: WM.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¬§</label>
<p>Current address: Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">wiktor.m≈Çynarski@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>5</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>21</day>
<month>5</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>5</issue>
<elocation-id>e1004294</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>9</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>4</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Wiktor M≈Çynarski</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004294" xlink:type="simple"/>
<abstract>
<p>In mammalian auditory cortex, sound source position is represented by a population of broadly tuned neurons whose firing is modulated by sounds located at all positions surrounding the animal. Peaks of their tuning curves are concentrated at lateral position, while their slopes are steepest at the interaural midline, allowing for the maximum localization accuracy in that area. These experimental observations contradict initial assumptions that the auditory space is represented as a topographic cortical map. It has been suggested that a ‚Äúpanoramic‚Äù code has evolved to match specific demands of the sound localization task. This work provides evidence suggesting that properties of spatial auditory neurons identified experimentally follow from a general design principle- learning a sparse, efficient representation of natural stimuli. Natural binaural sounds were recorded and served as input to a hierarchical sparse-coding model. In the first layer, left and right ear sounds were separately encoded by a population of complex-valued basis functions which separated phase and amplitude. Both parameters are known to carry information relevant for spatial hearing. Monaural input converged in the second layer, which learned a joint representation of amplitude and interaural phase difference. Spatial selectivity of each second-layer unit was measured by exposing the model to natural sound sources recorded at different positions. Obtained tuning curves match well tuning characteristics of neurons in the mammalian auditory cortex. This study connects neuronal coding of the auditory space with natural stimulus statistics and generates new experimental predictions. Moreover, results presented here suggest that cortical regions with seemingly different functions may implement the same computational strategy-efficient coding.</p></abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Ability to localize the position of a sound source is vital to many organisms, since audition provides information about areas which are not accessible visually. While its importance is undisputed, its neuronal mechanisms are not well understood. It has been observed in experimental studies that despite the crucial role of sound localization, single neurons in the auditory cortex of mammals carry very little information about the sound position. The joint activity of multiple neurons is required to accurately localize sound, and it is an open question how this computation is performed by auditory cortical circuits. In this work I propose a statistical model of natural stereo sounds. The model is based on the theoretical concept of sparse, efficient coding which has provided candidate explanations of how different sensory systems may work. When adapted to binaural sounds recorded in a natural environment, the model reveals properties highly similar to those of neurons in the mammalian auditory cortex, suggesting that mechanisms of neuronal auditory coding can be understood in terms of general, theoretical principles.</p>
</abstract>
<funding-group>
<funding-statement>This work was funded by the German Science Foundation graduate college "InterNeuro". The funder had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="31"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The precise role played by the auditory cortex in hearing remains unclear. Before reaching cortical areas, raw sounds undergo numerous transformations in the brainstem and the thalamus. This subcortical processing seems more substantial than in other senses and is a specific property of the auditory system. What computations are performed by the cortex on the output generated by lower auditory regions is a question far from being answered.</p>
<p>One of the issues in functional characterization of the auditory cortex is an apparent lack of specificity. Spiking activity of cortical auditory neurons is modulated by sound features such as pitch, timbre and spatial location [<xref ref-type="bibr" rid="pcbi.1004294.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref002">2</xref>]. Responses invariant to any of those features seem rare. This interdependence is especially puzzling in the context of extracting spatial information. A number of studies attempted to identify ‚Äúwhat‚Äù and ‚Äúwhere‚Äù streams in the auditory system (e.g. [<xref ref-type="bibr" rid="pcbi.1004294.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref004">4</xref>]). Despite those efforts the existence of a sharp separation of spatial and identity information in the auditory cortex is still not definitely confirmed [<xref ref-type="bibr" rid="pcbi.1004294.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref006">6</xref>].</p>
<p>Neurons reveal sensitivity to sound position in most parts of the mammalian auditory cortex [<xref ref-type="bibr" rid="pcbi.1004294.ref007">7</xref>]. Their spatial tuning is quite broad ‚Äî neural firing can be modulated by sounds located anywhere on the azimuthal plane. While activity of single units does not carry information sufficient to accurately localize sounds, larger numbers of neurons seem to form a population code for sound location [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1004294.ref011">11</xref>]. These observations strongly differ from assumptions made early in the field that the auditory space is represented by a topographic cortical map, where neighboring units would encode the presence of a sound source at proximal positions [<xref ref-type="bibr" rid="pcbi.1004294.ref012">12</xref>].</p>
<p>Results described above constitute a conceptual challenge for theoretical models of the auditory cortex and understanding its role in spatial hearing in particular. Nevertheless, a number of candidate roles for this region have been proposed. It has been suggested, for instance, that the main function of the primary auditory cortex (A1) is to process sound features extracted by subcortical structures [<xref ref-type="bibr" rid="pcbi.1004294.ref013">13</xref>] on multiple time scales. Another theory proposes that the auditory cortex in the cat represents abstract entities (such as a bird song or wind) rather than low-level spectrotemporal features of the stimulus [<xref ref-type="bibr" rid="pcbi.1004294.ref014">14</xref>]. It is also debated whether auditory cortical areas bear similarities to visual areas, and if yes, what sort of understanding can be gained by combining knowledge about those brain regions [<xref ref-type="bibr" rid="pcbi.1004294.ref015">15</xref>]. From a theoretical perspective one question seems to be particularly important ‚Äî is there any general principle behind the functioning of auditory cortex, or does it carry out computations which are task- or modality-specific and therefore not performed in other parts of the brain?</p>
<p>A particularly succesful theoretical framework attempting to explain general mechanisms behind the functioning of the nervous system is provided by the Efficient Coding Hypothesis [<xref ref-type="bibr" rid="pcbi.1004294.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref017">17</xref>]. It proposes that sensory systems maximize the amount of transmitted stimulus information. Under the additional assumption that a typical stimulus activates only a small fraction of a neuronal population, the hypothesis is known as <italic>sparse coding</italic>[<xref ref-type="bibr" rid="pcbi.1004294.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref019">19</xref>]. Perhaps the strongest prediction of the efficient coding hypothesis is that the neuronal activity at consecutive stages of sensory processing should become less and less redundant, hence more independent. This prediction has been experimentally tested in the auditory system of the cat. Chechik and colleagues [<xref ref-type="bibr" rid="pcbi.1004294.ref020">20</xref>] recorded neuronal responses to natural sounds at three levels of the auditory hierarchy ‚Äî Inferior Colliculus (IC), Medial Genniculate Body (MGB) and A1. They observed that spiking activity was progressively less redundant between single neurons, as quantified using information theoretic measures. This result suggests that audition can be understood using concepts provided by the efficient coding hypothesis.</p>
<p>In order to form an efficient stimulus representation, neuronal codes should reflect regularities present in the sensory environment [<xref ref-type="bibr" rid="pcbi.1004294.ref021">21</xref>]. This implies that by studying statistics of natural input, one should be able to predict neuronal tuning properties. In audition, this idea has been followed by a number of researchers. Starting at the lowest level of the auditory system, Lewicki and Smith [<xref ref-type="bibr" rid="pcbi.1004294.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref023">23</xref>] demonstrated that learning a sparse representation of natural sound chunks reproduces shapes of cochlear filters of the cat. A recent extension of this work has suggested that while the auditory nerve may be optimally encoding all sounds it encounters, neurons in the cochlear nucleus may be tuned to efficiently represent particular sound classes [<xref ref-type="bibr" rid="pcbi.1004294.ref024">24</xref>]. Climbing higher in the auditory hierarchy ‚Äî Carlson et al [<xref ref-type="bibr" rid="pcbi.1004294.ref025">25</xref>] have reproduced shapes of spectrotemporal receptive fields (STRFs) in the inferior colliculus by learning sparse codes of speech sounds. The relationship between spectrotemporal tuning of cortical neurons and sparse representation of speech spectrograms were explored by Klein, Koerding and Koenig [<xref ref-type="bibr" rid="pcbi.1004294.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref027">27</xref>]. More recently, some aspects of the topographic structure of the auditory cortex were shown to emerge from statistics of speech sounds by Terashima and Okada [<xref ref-type="bibr" rid="pcbi.1004294.ref028">28</xref>]. Terashima and colleagues have also studied the connection between sparse codes of natural sound spectra and harmonic relationships revealed by receptive fields of macaque A1 neurons [<xref ref-type="bibr" rid="pcbi.1004294.ref029">29</xref>]. Maximizing coding efficiency by learning sparse codes has also lead to emergence of signal representations useful in spatial hearing tasks. Asari et al [<xref ref-type="bibr" rid="pcbi.1004294.ref030">30</xref>] learned overcomplete dictionaries of monaural spectrograms and demonstrated that this representation allows for the segregation of acoustically overlapping and yet spatially separate sound sources (the ‚Äúcocktail party problem‚Äù). A recent study found that sparse coding of a spectrotemporal representation of binaural sound extracts spatial features invariant to sound identity [<xref ref-type="bibr" rid="pcbi.1004294.ref031">31</xref>].</p>
<p>As discussed above, a growing body of evidence seems to point to efficient coding as a computational process implemented by the auditory system. To date however, the connection between natural stimulus statistics and auditory spatial receptive fields remains unexplained. It is therefore unclear if spatial computations performed by the auditory cortex are unique to this brain area or whether they can be also predicted in a principled way from a broader theoretical perspective.</p>
<p>The present work attempts to connect spatial computations carried out by the auditory cortex with statistics of the natural stimulus. Here, a hierarchical statistical model of stereo sounds recorded in a real auditory environment is proposed. Based on principles of sparse coding the model learns the spectrotemporal and interaural structure of the stimulus. In the next step, it is demonstrated that when probed with spatially localized sounds, higher level units reveal spatial tuning which strongly resembles spatial tuning of neurons in the mammalian auditory cortex. Additionally, the learned code forms an interdependent representation of spatial information and spectrotemporal quality of a sound. Activity of higher units is therefore modulated by sound‚Äôs position and identity, as observed in the auditory system.</p>
<p>This study provides computational evidence that spatial tuning of auditory cortical neurons may be a manifestation of an underlying general design principle ‚Äî efficient coding. Present results suggest that the role of the auditory cortex is to reduce redundancy of the stimulus representation preprocessed by the brainstem. Representation obtained in this way may facilitate tasks performed by higher brain areas, such as sound localization.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Recorded sounds</title>
<p>Binaural sound used to train the model was recorded by a human subject walking freely in a wooded area, in the presence of another speaker. The obtained recording included ambient (wind, flowing stream) and transient environmental sounds (wood cracking, steps) as well as human speech. The auditory scene changed over time due to the motion of the the recorder, the speaker, and changes in the environment. It consisted of multiple sound sources emanating from a diverse set of locations, creating together a complex, natural auditory environment. Please refer to the Methods section for details of the recording.</p>
</sec>
<sec id="sec004">
<title>Overview of the hierarchical model</title>
<p>The present study proposes a hierarchical statistical model of binaural sounds, which captures binaural and spectrotemporal structure present in natural stimuli. The architecture of the model is shown in <xref ref-type="fig" rid="pcbi.1004294.g001">Fig 1</xref>. It consists of the input layer and two hidden layers. The input to the model was <italic>N</italic> sample-long epochs of binaural sound: from the left ear‚Äî<italic>x</italic><sub><italic>L</italic></sub> and from the right ear‚Äî<italic>x</italic><sub><italic>R</italic></sub>. The role of the first layer was to extract and separate phase and amplitude information from each ear by encoding them in an efficient manner. Monaural sounds were transformed into phase (<italic>œï</italic><sub><italic>L</italic></sub>, <italic>œï</italic><sub><italic>R</italic></sub>) and amplitude (<italic>a</italic><sub><italic>L</italic></sub>, <italic>a</italic><sub><italic>R</italic></sub>) vectors. This layer can be thought of as a statistical analogy to cochlear filtering. Phase vectors were further modified by computing interaural phase differences (IPDs) ‚Äî a major sound localization cue [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. This tranformation may be considered an attempt to mimic functioning of the medial superior olive (MSO) ‚Äî the brainstem nucleus where phase differences are extracted [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>].</p>
<fig id="pcbi.1004294.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The graphical model representing variable dependencies.</title>
<p>The lowest layer represents sound epochs perceived by the left and the right ear <italic>x</italic><sub><italic>L</italic></sub> and <italic>x</italic><sub><italic>R</italic></sub>. They are decomposed by a sparse coding algorithm into phase and amplitude vectors <italic>œï</italic><sub><italic>L</italic></sub>, <italic>œï</italic><sub><italic>R</italic></sub> and <italic>a</italic><sub><italic>L</italic></sub>, <italic>a</italic><sub><italic>R</italic></sub>. Phases are further substracted from each other in order to obtain an IPD vector Œî<italic>œï</italic>. The second layer encodes jointly monaural amplitudes and IPDs. Auxiliary variables (phase offset and the scaling factor <italic>w</italic>) are depicted in gray.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g001"/>
</fig>
<p>The second layer of the model learned a joint sparse representation of monaural amplitudes (<italic>a</italic><sub><italic>L</italic></sub>, <italic>a</italic><sub><italic>R</italic></sub>) and phase differences (Œî<italic>œï</italic>). Level (amplitude) and temporal (phase) information from each ear was jointly encoded by a population of <italic>M</italic> units. Each of the units was therefore capturing higher-order spectrotemporal patterns of sound in each ear. Additionally, by combining monaural information into single units higher level representation achieved spatial tuning not present in the first layer. The second hidden layer can be interpreted as a model of auditory neurons which receive converging monaural input and jointly operate on phase and amplitude ‚Äî two kinds of information known to be important for spatial hearing.</p>
</sec>
<sec id="sec005">
<title>First layer: sparse, complex-valued sound representation</title>
<p>As demonstrated in previous work, filtering properties of the auditory nerve can be explained by sparse coding models of natural sounds [<xref ref-type="bibr" rid="pcbi.1004294.ref022">22</xref>]. There, short epochs of natural sounds are modelled as a linear combination of real-valued basis functions multiplied by sparse, independent coefficients (i.e. having highly curtotic marginal distributions). Adapted to sets of natural sound chunks, basis functions become localized in time and/or frequency matching properties of cochlear filters.</p>
<p>While being capable of capturing interesting properties of the data, real valued representations are not well suited for modeling binaural sounds. This is because binaural hearing mechanisms utilize interaural level and time differences (ILDs and ITDs respectively). In narrowband channels, differences in time correspond to phase displacements known as interaural phase differences (IPDs). Therefore a desired representation should both be adapted to the data (i.e. non-redundant) and separate amplitude from phase (where phase is understood as a temporal shift smaller than the oscillatory cycle of a particular frequency).</p>
<p>The present work addresses this twofold constraint with complex-valued sparse coding. Each data vector <italic>x</italic> ‚àà ‚Ñù<sup><italic>N</italic></sup> is represented in the following way:
<disp-formula id="pcbi.1004294.e001"><alternatives><graphic id="pcbi.1004294.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mi mathvariant="fraktur">R</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>Œ∑</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>z</italic><sub><italic>i</italic></sub> ‚àà ‚ÑÇ are complex coefficients, * denotes a complex conjugation, <italic>A</italic><sub><italic>i</italic></sub> ‚àà ‚ÑÇ<sup><italic>T</italic></sup> are complex basis functions and <italic>Œ∑</italic> ‚àº ùìù(0, <italic>œÉ</italic>) is additive gaussian noise. Complex coefficients in Euler‚Äôs form become <inline-formula id="pcbi.1004294.e002"><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (where <inline-formula id="pcbi.1004294.e003"><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mo>‚àí</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>) therefore <xref ref-type="disp-formula" rid="pcbi.1004294.e001">Eq (1)</xref> can be rewritten to explicitely represent phase <italic>œï</italic> and amplitude <italic>a</italic> as separate variables:
<disp-formula id="pcbi.1004294.e004"><alternatives><graphic id="pcbi.1004294.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo form="prefix">cos</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi mathvariant="fraktur">R</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mo form="prefix">sin</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi mathvariant="fraktur">I</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>Œ∑</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>Real and imaginary parts <inline-formula id="pcbi.1004294.e005"><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mtext mathvariant="fraktur">R</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="pcbi.1004294.e006"><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mtext mathvariant="fraktur">I</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> of basis functions <inline-formula id="pcbi.1004294.e007"><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> span a subspace within which the position of a data sample is determined by amplitude <italic>a</italic><sub><italic>i</italic></sub> and phase <italic>œï</italic><sub><italic>i</italic></sub>. Depending on number of basis functions <italic>N</italic> (each of them is formed by a pair of vectors), the representation can be complete (<italic>N</italic>/2 = <italic>T</italic>) or overcomplete (<italic>N</italic>/2 &gt; <italic>T</italic>).</p>
<p>In a probabilistic formulation, Eqs (<xref ref-type="disp-formula" rid="pcbi.1004294.e001">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004294.e004">2</xref>) can be understood as a likelihood model of the data, given coefficients <italic>z</italic> and basis functions <italic>A</italic>:
<disp-formula id="pcbi.1004294.e008"><alternatives><graphic id="pcbi.1004294.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>œÉ</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>œÄ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:munderover><mml:mo>‚àè</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <inline-formula id="pcbi.1004294.e009"><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mtext mathvariant="fraktur">R</mml:mtext> <mml:mo stretchy="false">{</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> is the reconstruction of the <italic>t</italic>‚àí<italic>th</italic> dimension of the data vector <italic>x</italic>. A prior over complex coefficients applied here assumes independence between subspaces and promotes sparse solutions i.e. solutions with most amplitudes close to 0:
<disp-formula id="pcbi.1004294.e010"><alternatives><graphic id="pcbi.1004294.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e010"/><mml:math id="M10" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>Z</mml:mi></mml:mfrac> <mml:munderover><mml:mo>‚àè</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>Œª</mml:mi> <mml:mi>S</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>Z</italic> is a normalizing constant. Function <italic>S</italic>(<italic>a</italic><sub><italic>i</italic></sub>) promotes sparsity by penalizing large amplitude values. Here, a Cauchy prior on amplitudes is assumed i.e. <inline-formula id="pcbi.1004294.e011"><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>g</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. One should note however that amplitudes are always non-negative and that in general the Cauchy distribution is defined over the entire real domain. The model attempts to form a data representation keeping complex amplitudes maximally independent across subspaces, while still allowing dependence between coordinates <italic>z</italic><sup>ùïΩ</sup>, <italic>z</italic><sup>ùï¥</sup> which determine position within each subspace. Inference of coefficients <italic>z</italic> which represent data vector <italic>x</italic> in the basis <italic>A</italic> is performed by minimizing the following energy function
<disp-formula id="pcbi.1004294.e012"><alternatives><graphic id="pcbi.1004294.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>‚àù</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>Œª</mml:mi> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
which corresponds to the negative log-posterior <italic>p</italic>(<italic>z</italic>‚à£<italic>x</italic>, <italic>A</italic>). This model was introduced in [<xref ref-type="bibr" rid="pcbi.1004294.ref033">33</xref>] and used to learn motion and form invariances from short chunks of natural movies. Assuming <italic>N</italic> = <italic>T</italic>/2 and <italic>œÉ</italic> = 0, it is equivalent to 2-dimensional Independent Subspace Analysis(ISA) [<xref ref-type="bibr" rid="pcbi.1004294.ref034">34</xref>].</p>
<p>When trained on natural image patches, real and imaginary parts of basis functions <italic>A</italic> form pairs of Gabor-like filters, which have the same frequency, position, scale and orientation. The only differing factor is phase‚Äîreal and imaginary vectors are typically in a quadrature-phase relationship (shifted by <inline-formula id="pcbi.1004294.e013"><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>œÄ</mml:mi> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>). By extension, one might expect that the same model trained on natural sounds should form a set of frequency localized phase-invariant subspaces, where imaginary vector is equal to the real one shifted a quarter of a cycle in time. Somewhat surprisingly, such representation does not emerge, and learned subspaces capture different aspects of the data ‚Äî bandwidth, frequency or time invariance [<xref ref-type="bibr" rid="pcbi.1004294.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref036">36</xref>].</p>
<p>In order to learn a representation from the statistics of the data that preserves a desired property such as phase invariance, one could select a parametric form of basis functions and adapt the parameter set [<xref ref-type="bibr" rid="pcbi.1004294.ref037">37</xref>]. Such a parametric approach has the disadvantage that the assumed family of solutions might not be flexible enough to efficiently span the data space. Another, more flexible alternative to learn a structured representation is to regularize basis functions by imposing temporal-coherence promoting priors [<xref ref-type="bibr" rid="pcbi.1004294.ref036">36</xref>]. This, however, requires determining the strength of regularizing priors.</p>
<p>To overcome these problems, a different approach was taken here. The first-layer representation was created in two steps. Firstly a real-valued sparse code was trained (see <xref ref-type="sec" rid="sec014">Methods</xref>). Learned basis functions were well localized in time or frequency and tiled the time-frequency plane in a uniform and non-overlapping manner (<xref ref-type="fig" rid="pcbi.1004294.g002">Fig 2B</xref>). They were taken as real vectors <italic>A</italic><sup>‚Ñú</sup> of complex basis functions <italic>A</italic>. In the second step, imaginary parts were created by performing the Hilbert transform of real vectors. The Hilbert transform of a time varying signal <italic>y</italic>(<italic>t</italic>) is defined as follows:
<disp-formula id="pcbi.1004294.e014"><alternatives><graphic id="pcbi.1004294.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e014"/><mml:math id="M14" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>œÄ</mml:mi></mml:mfrac> <mml:mi>p</mml:mi> <mml:mo>.</mml:mo> <mml:mi>v</mml:mi> <mml:mo>.</mml:mo> <mml:msubsup><mml:mo>‚à´</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>‚àû</mml:mi></mml:mrow> <mml:mi>‚àû</mml:mi></mml:msubsup> <mml:mfrac><mml:mrow><mml:mi>y</mml:mi> <mml:mo>(</mml:mo> <mml:mi>œÑ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>œÑ</mml:mi></mml:mrow></mml:mfrac> <mml:mi>d</mml:mi> <mml:mi>œÑ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
Where <italic>p</italic>.<italic>v</italic>. stands for Cauchy principal value. In such a way every real vector <inline-formula id="pcbi.1004294.e015"><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>‚Ñú</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> was paired with its Hilbert transform <inline-formula id="pcbi.1004294.e016"><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>‚Ñë</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>H</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>‚Ñú</mml:mo></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> i.e. a vector which complex Fourier‚Äôs coefficients are all shifted by <inline-formula id="pcbi.1004294.e017"><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>œÄ</mml:mi> <mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> in phase. The obtained dictionary is adapted to the stimulus ensemble, hence providing a non-redundant data representation, yet makes phase clearly interpretable as a temporal displacement.</p>
<fig id="pcbi.1004294.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g002</object-id>
<label>Fig 2</label>
<caption>
<title>First layer basis.</title>
<p>A) Representative real (black) and imaginary (gray) vectors. B) Isoprobability contours of Wigner-Ville distributions associated with each real vector. Time‚Äîfrequency plane is tiled uniformly with a weak overlap. Gray-filled oval corresponds to the framed basis function on panel A.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g002"/>
</fig>
<p>The model was trained using <italic>T</italic> = 128 sample-long chunks of sound sampled at 8 kHz, which corresponds to 16 ms duration. The complete representation of 128 real basis functions was trained, and each of them was paired with its Hilbert transform, resulting in the total number of 256 basis vectors. Selected basis functions are displayed in <xref ref-type="fig" rid="pcbi.1004294.g002">Fig 2A</xref>. Real vectors are plotted in black together with associated imaginary ones plotted in gray. Panel B of the same figure displays isoprobability contours of Wigner-Ville distributions associated with the 256 basis functions. This form of representation localizes each temporal feature on a time-frequency plane [<xref ref-type="bibr" rid="pcbi.1004294.ref038">38</xref>] (one should note that real and imaginary vectors within each pair are represented by the same contour on that plot). A clear separation into two classes is visible. Low frequency basis functions (below 1 kHz) are non-localized in time (spanning the entire 16 ms interval), while in higher frequency regions their temporal precision increases. An interesting bandwidth reversal is visible around 3 kHz, where temporal accuracy is traded for frequency precision. Interestingly, the sharp separation into frequency and time localized basis functions, which emerged in this study was not clearly visible in other studies which performed sparse coding of sound [<xref ref-type="bibr" rid="pcbi.1004294.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref038">38</xref>]. Time-frequency properties observed here reflect the statistical structure of the recorded auditory scene, which mostly consisted of non-harmonic environmental sounds sparsely interspersed with human speech.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004294.g003">Fig 3</xref> depicts a typical distribution of binaural phase. Phases of the same basis function in each ear reveal dependence in their difference. This means that joint probability of monaural phases depends solely on the IPD:
<disp-formula id="pcbi.1004294.e018"><alternatives><graphic id="pcbi.1004294.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e018"/><mml:math id="M18" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>L</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>‚àù</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where Œî<italic>œï</italic><sub><italic>i</italic></sub> = <italic>œï</italic><sub><italic>i</italic>, <italic>L</italic></sub>‚àí<italic>œï</italic><sub><italic>i</italic>, <italic>R</italic></sub> is the IPD. This property is a straightforward consequence of physics of sound ‚Äî sounds arrive to each ear with a varying delay giving rise to positive and negative phase shifts. From a statistical point of view this means that monaural phases become conditionally independent given their difference and a phase offset <italic>œï</italic><sub><italic>i</italic>, <italic>O</italic></sub>:
<disp-formula id="pcbi.1004294.e019"><alternatives><graphic id="pcbi.1004294.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e019"/><mml:math id="M19" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>L</mml:mi></mml:mrow></mml:msub> <mml:mo>‚ä•</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>Œî</mml:mo></mml:mrow> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<fig id="pcbi.1004294.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Joint distribution of monaural phases.</title>
<p>The distribution was estimated by independently encoding left and right ear sounds from an ensemble of binaural sound epochs and creating a histogram of phase values associated with the basis function depicted at the bottom of the figure. Visible ridge-like pattern implies that monaural phases reveal a dependence in their difference.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g003"/>
</fig>
<p>The phase offset <italic>œï</italic><sub><italic>i</italic>, <italic>O</italic></sub> is the absolute phase value ‚Äî indicating the time from the beginning of the oscillatory cycle. It can be therefore said that:
<disp-formula id="pcbi.1004294.e020"><alternatives><graphic id="pcbi.1004294.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>L</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>O</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pcbi.1004294.e021"><alternatives><graphic id="pcbi.1004294.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>O</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
This particular statistical property allows us to understand IPDs not as an ad-hoc computed feature but as an inherent property of the probability distribution underlying the data. It is reflected in the structure of the graphical model (see <xref ref-type="fig" rid="pcbi.1004294.g001">Fig 1</xref>). Since the phase offset <italic>œï</italic><sub><italic>i</italic>, <italic>O</italic></sub> does not carry spatial information for the purposes of current study it is treated as an auxiliary variable and therefore marked in gray.</p>
</sec>
<sec id="sec006">
<title>Second layer: joint representation of monaural amplitudes and interaural phase differences</title>
<p>In an approach to model the cochlear coding of sound, monaural sound epochs <italic>x</italic><sub><italic>L</italic></sub> and <italic>x</italic><sub><italic>R</italic></sub> were encoded independently using the same dictionary of complex basis functions <italic>A</italic> described in the previous section. Signal from both ears converged in the second hidden layer, which role was to form a joint, higher-order representation of the entire stimulus processed by the auditory system.</p>
<p>The celebrated Duplex Theory of spatial hearing specifies two kinds of cues used to solve the sound-localization task: interaural level and time (or phase) differences [<xref ref-type="bibr" rid="pcbi.1004294.ref039">39</xref>]. While IPDs are supposed to be mostly used in localizing low-frequency sounds, ILDs are a cue, which (at least in the laboratory conditions) can be used to identify the position of high frequency sources. Phase and level cues are known to be computed in lateral and medial superior olive (LSO and MSO respectively) ‚Äî separated anatomical regions in the brainstem [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. However, an assumption made here was that neurons in the auditory cortex receive converging input from subcortical structures. This would enable them to form their spatial sensitivity using both fine structure phase and amplitude information. One can take also the inverse perspective: a single object (a ‚Äúcause‚Äù) in the environment generates level and phase cues at the same time. Its identification therefore has to rely on observing dependencies between those features of the stimulus.</p>
<p>The second layer formed a joint representation of monaural amplitudes and interaural phase differences. However, not all IPDs were modelled in that stage. Humans stop utilizing fine structure IPDs in higher frequency regimes (roughly above 1.3 kHz), since this cue becomes ambiguous [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. Aditionally, cues above around 700 Hz become ambiguous (a single cue value does not correspond to a unique source position). For those reasons, and in order to reduce the number of data dimensions, 20 out of 128 IPD values were selected. The selection criteria were the following: (i) an associated basis function should have the peak of the Fourier spectrum below 0.75 kHz (which provided the upper frequency bound), and (ii) it should have at least one full cycle (which provided the lower bound). All basis functions fulfilling these criteria were non-localized in time (they spanned entire 16 ms interval). As a result, the second layer of the model was jointly encoding <italic>T</italic> = 128 log-amplitude values from each ear and <italic>P</italic> = 20 phase differences.</p>
<p>Monaural log-amplitude vectors <italic>a</italic><sub><italic>L</italic></sub>, <italic>a</italic><sub><italic>R</italic></sub> ‚àà ‚Ñù<sup><italic>T</italic></sup> were concatenated into a single vector <italic>a</italic> ‚àà ‚Ñù<sup>2√ó<italic>T</italic></sup>, and encoded using a dictionary of amplitude basis functions <italic>B</italic>. Representation of IPDs (Œî<italic>œï</italic>) was formed using a separate feature dictionary <italic>Œæ</italic>. Both ‚Äî phase and amplitude basis functions (<italic>B</italic> and <italic>Œæ</italic>), were coupled by associated sparse coefficients <italic>s</italic>. The overall generative model of phases and amplitudes was defined in the following way:
<disp-formula id="pcbi.1004294.e022"><alternatives><graphic id="pcbi.1004294.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e022"/><mml:math id="M22" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>a</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>Œ∑</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula> <disp-formula id="pcbi.1004294.e023"><alternatives><graphic id="pcbi.1004294.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e023"/><mml:math id="M23" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>Œæ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>œµ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
The amplitude noise was assumed to be gaussian (<italic>Œ∑</italic> ‚àº ùìù(0, <italic>œÉ</italic><sub>2</sub>)) with <italic>œÉ</italic><sub>2</sub> variance. Since phase is a circular variable its noise <italic>Œµ</italic> was modelled by the von Mises distribution with concentration parameter <italic>Œ∫</italic>.</p>
<p>The second layer was encoding two different physical quantities ‚Äî phases, which are circular values, and log-amplitudes, which are real numbers. The goal was to form a joint representation of both parameters and learn their dependencies from the data. A simple, linear sparse coding model could be in principle used to achieve this task. However, if a single set of sparse coefficients <italic>s</italic><sub><italic>i</italic></sub> was used to model both quantities, scaling problems could arise, namely a coefficient value which explains well the amplitude vector may be too large or too small to explain the concomittant IPD vector. For this reason an additional phase multiplier <italic>w</italic> was introduced. It enters <xref ref-type="disp-formula" rid="pcbi.1004294.e022">Eq 11</xref> as a scaling factor, which gives the model additional flexibility required to learn joint probability distribution of amplitudes and IPDs. <xref ref-type="fig" rid="pcbi.1004294.g001">Fig 1</xref> depicts it in gray as an auxiliary variable. In this way, amplitude values and phase differences were modelled by variables sharing a common, sparse support (coefficients <italic>s</italic>), with a sufficient flexibility.</p>
<p>Pairs of basis functions <italic>B</italic><sub><italic>i</italic></sub>, <italic>Œæ</italic><sub><italic>i</italic></sub> represent binaural spectrotemporal stimulus and IPD patterns respectively, while sparse coefficients <italic>s</italic> signal their joint presence in the encoded sound epoch. An <italic>i</italic>‚àí<italic>th</italic> second-layer unit was activated (<italic>s</italic><sub><italic>i</italic></sub> ‚â† 0) whenever a pattern of IPDs represented by the basis function <italic>Œæ</italic><sub><italic>i</italic></sub> or a pattern of amplitudes represented by <italic>B</italic><sub><italic>i</italic></sub> was present in its receptive field. The activity was maximized, when both features were present at the same time. For this reason, when seeking analogies between the higher-level representation and auditory neurons, coefficients <italic>s</italic> can be interpreted as neuronal activity (e.g. firing rate) and basis function pairs <italic>B</italic><sub><italic>i</italic></sub>, <italic>Œæ</italic><sub><italic>i</italic></sub> as receptive fields (i.e. stimulus preferred by a neuron).</p>
<p>The likelihood of amplitudes and phase differences defined by the second layer was given by:
<disp-formula id="pcbi.1004294.e024"><alternatives><graphic id="pcbi.1004294.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>w</mml:mi> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Œæ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msub><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>œÄ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi></mml:mrow></mml:msup> <mml:munderover><mml:mo>‚àè</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi></mml:mrow></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mspace width="1pt"/><mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>œÄ</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Œ∫</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>P</mml:mi></mml:msup> <mml:munderover><mml:mo>‚àè</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>P</mml:mi></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>Œ∫</mml:mi> <mml:mo form="prefix">cos</mml:mo> <mml:mo>(</mml:mo> <mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>^</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where <inline-formula id="pcbi.1004294.e025"><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="pcbi.1004294.e026"><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>ÃÇ</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo stretchy="false">‚à£</mml:mo> <mml:mi>w</mml:mi> <mml:mo stretchy="false">‚à£</mml:mo> <mml:msubsup><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>Œæ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are amplitude and phase reconstructions repsectively and <italic>I</italic><sub>0</sub> is the modified Bessel function of order 0. The joint distribution of coefficients <italic>s</italic> was assumed to be equal to the product of marginals:
<disp-formula id="pcbi.1004294.e027"><alternatives><graphic id="pcbi.1004294.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>Z</mml:mi></mml:mfrac> <mml:munderover><mml:mo>‚àè</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>Œª</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where <italic>Œª</italic><sub>2</sub> is a sparsity controlling parameter. A Cauchy distribution was assumed as a prior over marginal coefficients (i.e. <inline-formula id="pcbi.1004294.e028"><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>log</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>). To prevent degenerate solutions, where sparse coefficients <italic>s</italic> are very small and the scaling coefficient <italic>w</italic> grows undbounded, a prior <italic>p</italic>(<italic>w</italic>) constraining it from above and from below was placed. A generalized Gaussian distribution of the following form was used:
<disp-formula id="pcbi.1004294.e029"><alternatives><graphic id="pcbi.1004294.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>Œ≤</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>Œ±</mml:mi> <mml:mi>Œì</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>Œ≤</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>-</mml:mo> <mml:mi>Œº</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mi>Œ±</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>Œ≤</mml:mi></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>Œì denotes tha gamma function, <italic>Œ±</italic>, <italic>Œ≤</italic> and <italic>Œº</italic> denote the scale, shape and location parameters respectively. When the shape parameter <italic>Œ≤</italic> is set to a large value (here <italic>Œ≤</italic> = 8), the distribution approximates a uniform distribution. Varying the scale parameter <italic>Œ±</italic> changes the upper and the lower limit of the interval.</p>
<p>Taken together the negative log-posterior over the second layer coefficients was defined by the energy function:
<disp-formula id="pcbi.1004294.e030"><alternatives><graphic id="pcbi.1004294.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e030"/><mml:math id="M30" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>w</mml:mi> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Œæ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>‚àù</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msubsup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>√ó</mml:mo> <mml:mi>T</mml:mi></mml:mrow></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>Œ∫</mml:mi> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>P</mml:mi></mml:munderover> <mml:mo form="prefix">cos</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>^</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>Œª</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>Œª</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>-</mml:mo> <mml:mi>Œº</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mi>Œ±</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>Œ≤</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
the <italic>Œª</italic><sub><italic>w</italic></sub> coefficient was introduced to control the strength of the prior on the scaling coefficient w. Similarly as in the first model layer, learning of basis functions and inference of coefficients was performed using gradient descent (see <xref ref-type="sec" rid="sec014">Methods</xref>). The total number <italic>M</italic> of basis function pairs was set to 256.</p>
</sec>
</sec>
<sec id="sec007">
<title>Properties of the second layer representation</title>
<p>The second layer learned cooccuring phase and amplitude patterns forming a sparse, combinatorial code of the first layer output. <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4</xref> displays 10 representative examples of basis function pairs <italic>Œæ</italic><sub><italic>i</italic></sub> and <italic>B</italic><sub><italic>i</italic></sub>, which encoded amplitudes and IPDs respectively.</p>
<fig id="pcbi.1004294.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Higher-order basis functions.</title>
<p>A) Explanation of the visualization of second layer basis functions. Top two panels depict the binaural amplitude basis function <italic>B</italic><sub><italic>i</italic></sub>. Spectrotemporal information in each ear is represented using isoprobability contours of Wigner-Ville distributions of first-layer basis functions (see <xref ref-type="fig" rid="pcbi.1004294.g002">Fig 2</xref>). Colors correspond to the log-amplitude weight. The bottom panel represents the IPD basis function <italic>Œæ</italic><sub><italic>i</italic></sub>. Each gray bar represents one of 20 selected low-layer basis functions. Here almost all values are positive (the bars point upwards), which corresponds to the right-ear precedence. B)-J) Basis functions ordered vertically by spectral modulation and horizontally by the dominating side.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g004"/>
</fig>
<p>Each amplitude basis function consisted of two monaural parts corresponding to the left and right ear. First-layer, temporal features were visualized using contours of Wigner-Ville distribution and colored according to the relative weight. Entries of IPD basis functions were values (marked by gray bars) modelling interaural phase disparities in each of selected 20 frequency channels.</p>
<p>The subset of 9 basis functions depicted in subpanels B-J of <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4</xref> constitutes a good representation of the entire dictionary. Their vertical ordering corresponds to spectrotemporal properties of <italic>B</italic><sub><italic>i</italic></sub> basis functions. Amplitude features displayed in the first row (B, E, H) reveal pronounced spectral modulation, while the last row (D, G, J) are features which are strongly temporaly modulated. Columns are ordered according to the ear each basis function pair prefered. Left column (B, C, D) are left-sided basis functions. Higher amplitude values are visible in the left ear parts (although differences are rather subtle), while associated IPD features are all negative. IPDs smaller than 0 imply that the encoded waveform was delayed in the right ear, hence the sound source was closer to the left ear. The last column (H, I, J) depicts more right-sided basis functions. Features displayed in the middle column (E, F, G) weight binaural amplitudes equally, however entries of associated phase vectors are either mostly negative or mostly positive.</p>
<p>As <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4</xref> shows, higher level representation learned spectrotemporal properties of the auditory scene, reflected in shapes of amplitude basis functions <italic>B</italic><sub><italic>i</italic></sub>. Binaural relations were captured by relative weighting of amplitudes in both ears and the shape of the IPD basis function.</p>
<p>To get a more detailed understanding of the spectrotemporal features captured by the representation, analysis of modulation spectra was performed. A modulation spectrum is a 2D Fourier transform of the spectrotemporal representation of a signal. It is known that modulation spectra of natural sounds posess specific structure [<xref ref-type="bibr" rid="pcbi.1004294.ref040">40</xref>]. Here, modulation spectrum was computed separately for monaural parts of amplitude basis functions <italic>B</italic><sub><italic>i</italic></sub> (see <xref ref-type="sec" rid="sec014">Methods</xref>). In the next step a center of mass of each of the modulation spectra was computed. Centers of mass are represented by single points in <xref ref-type="fig" rid="pcbi.1004294.g005">Fig 5A</xref>.</p>
<fig id="pcbi.1004294.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Spectrotemporal properties of the representation.</title>
<p>A) Centers of mass of monaural modulation spectra. B) Centers of mass of temporal modulation in monaural parts of <italic>B</italic><sub><italic>i</italic></sub> basis functions plotted C) Centers of mass of spectral modulation in monaural parts of <italic>B</italic><sub><italic>i</italic></sub> basis functions plotted. Letters correspond to panels in <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4</xref>. Black dashed lines depict linear regression fits. Parameters of each fit are written in figure insets.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g005"/>
</fig>
<p>A clear tradeoff between spectral and temporal modulation was visible. Basis functions which were strongly temporally modulated revealed simultaneously weak spectral modulation (and vice versa). It is evident as a ‚Äútriangular‚Äù shape of the point distribution in <xref ref-type="fig" rid="pcbi.1004294.g005">Fig 5A</xref>. This seems to be a robust property of natural sounds [<xref ref-type="bibr" rid="pcbi.1004294.ref040">40</xref>] and was shown to be captured by sparse coding models [<xref ref-type="bibr" rid="pcbi.1004294.ref025">25</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1004294.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref042">42</xref>]. Interestingly, spectro-temporal receptive fields of auditory neurons share this property [<xref ref-type="bibr" rid="pcbi.1004294.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref044">44</xref>]. Auditory neurons which reveal sensitivity to spectrotemporal sound patterns seem to prefer sounds which are either modulated in time or in frequency, but not both. When compared with modulation spectra of natural sound this form of tuning may be understood as an adaptation to the environmental stimulus statistics.</p>
<p>Average temporal modulation in the left ear is plotted against the right ear in panel B. As visible ‚Äî the amplitude modulation of basis functions B varied between 0 and 40 Hz, and a general linear trend was present. A linear regression model was fitted to these data (the fit is depicted in <xref ref-type="fig" rid="pcbi.1004294.g005">Fig 5</xref> as a black dashed line). The fit has revealed a relatively strong linear relationship between the temporal variation of monaural parts (with Pearson‚Äôs correlation <italic>œÅ</italic> = 0.70).</p>
<p>Spectral amplitude modulation revealed a weaker interaural correlation (<italic>œÅ</italic> = 0.36). This is visible in <xref ref-type="fig" rid="pcbi.1004294.g005">Fig 5C</xref>‚Äîpoints representing amplitude basis functions are scattered stronger than in panel B of the same figure. This property can be explained by head filtering characteristics. Acting as a low-pass filter, the head attenuates higher frequencies. For this reason, fine spectral information above 1.5 kHz was typically more pronounced in a single ear, affecting the strength of spectral modulation. This may be considered as an example of how stimulus statistics are determined not only by the environmental properties, but also by the anatomy of an organism. The majority of basis functions revealed spectral modulation smaller than 0.4 cycle per octave, and only a single one exceeded this value.</p>
<p>In the following analysis step, the goal was to analyze similarity in the monaural spectrotemporal patterns encoded by each second-layer unit. To this end binaural similarity index (BSI) of each amplitude basis function [<xref ref-type="bibr" rid="pcbi.1004294.ref043">43</xref>] was computed. The BSI is a correlation coefficient between the left and the right parts of a binaural, spectrotemporal feature. If the BSI was close to 0, the corresponding unit was representing different spectrotemporal patterns in each ear, while values close to 1 implied high similarity. BSIs are plotted in <xref ref-type="fig" rid="pcbi.1004294.g006">Fig 6A</xref>.</p>
<fig id="pcbi.1004294.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Binaural properties of the representation.</title>
<p>A) Binaural Similarity Index of amplitude basis functions <italic>B</italic><sub><italic>i</italic></sub>. The BSI is a correlation coefficient between left and right ear subvectors. The inset depicte the BSI histogram. B) Distribution of binaural amplitude dominance. Values above 0 imply domination of the left, and below 0 of the right ear. Histograms of BAD values of amplitude basis functions associated with negative IPD basis functions are colored gray and those associated with positive <italic>Œæ</italic><sub><italic>i</italic></sub> values are colored black. C) Distribution of averages of normalized <italic>Œæ</italic><sub><italic>i</italic></sub> basis functions.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g006"/>
</fig>
<p>Clearly, an overwhelming majority of basis functions revealed high interaural similarity (<italic>BSI</italic> &gt; 0.8, see the histogram at the inset). BSI of only one basis function was slightly below 0. If information encoded by amplitude basis functions in each ear was independent, the BSI distribution should peak at 0. This observation suggests that most of the second-layer units captured the same ‚Äúcause‚Äù underlying the stimulus i.e. a binaurally redundant spectrotemporal pattern. While the BSI index measures similarity of encoded monaural sound features, it is not informative about the side-preference of each unit. To asess whether amplitude basis functions were biased more towards the left or towards the right ear, another statistic ‚Äî a binaural amplitude dominance (BAD) was computed. The amplitude dominance was defined in the following way:
<disp-formula id="pcbi.1004294.e031"><alternatives><graphic id="pcbi.1004294.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e031"/><mml:math id="M31" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>B</mml:mi> <mml:mi>A</mml:mi> <mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mo>‚à•</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>L</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>‚à•</mml:mo></mml:mrow> <mml:mrow><mml:mo>‚à•</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>R</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>‚à•</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <italic>B</italic><sub><italic>i</italic>, <italic>L</italic></sub> = <italic>B</italic><sub><italic>i</italic>, (1, ‚Ä¶, <italic>T</italic>)</sub>, <italic>B</italic><sub><italic>i</italic>, <italic>R</italic></sub> = <italic>B</italic><sub><italic>i</italic>, (<italic>T</italic>+1, ‚Ä¶,2√ó<italic>T</italic>)</sub> are left and right ear parts of an amplitude basis function <italic>B</italic><sub><italic>i</italic></sub>. Each of them was pointwise exponentiated to map the entries from real log-amplitude values to the positive amplitude domain. The BAD index value larger than 0 means that the left-ear amplitude vector had a larger norm (i.e., it dominated the input to the particular unit). Balanced units had a BAD value close to 0 while right-ear dominance was indicated by negative values. Two histograms of dominance scores are displayed in panel B of <xref ref-type="fig" rid="pcbi.1004294.g006">Fig 6</xref>. The black one is an empirical distribution of BAD values of amplitude basis functions associated with IPD features of a negative average value (left-side preferring). The gray one corresponds to amplitude features matched with right-side biased phase basis functions. Both distributions are roughly symmetric with their modes located quite close to 0. Such bimodal distribution of the amplitude dominance score implies that amplitude basis functions could be divided into two opposite populations ‚Äî each preferring input from a different ear. Moreover, amplitude and phase information modelled by basis functions <italic>B</italic><sub><italic>i</italic></sub> and <italic>Œæ</italic><sub><italic>i</italic></sub> was dependent ‚Äî amplitude features dominated by information from one ear were associated with IPD features biased towards the same ear.</p>
<p>While amplitude representation encoded the quality of the sound together with binaural differences, the IPD dictionary was representing solely spatial aspects of the stimulus i.e. the temporal difference between the ears. In almost entire feature population, single entries of each of the phase difference basis functions <italic>Œæ</italic><sub><italic>i</italic></sub> all had the same sign. Negative phase differences corresponded to the left-side bias (it meant that the soundwave arrived first to the left-ear generating a smaller phase value) and positive to the right-side one. These two properties allowed us to asess the spatial preference of IPD basis functions simply by computing the average of their entries. The histogram of averages of vectors <italic>Œæ</italic><sub><italic>i</italic></sub> (normalized to have the maximal absolute value of 1) is depicted in <xref ref-type="fig" rid="pcbi.1004294.g006">Fig 6C</xref>. A clear bimodality is visible in the distribution. The positive peak corresponds to right-sided basis functions and the negative one to the left-sided subpopulation. Almost no balanced features (close to 0) were present in the dictionary. This dichotomy is visible also in <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4</xref>‚Äîbinaurally balanced amplitude basis functions (middle column) were associated with phase vectors biased towards either side. This result may be related to a previous study, which showed that a representation of natural IPD distribution designed to maximize stimulus discriminability (Fisher information) also has a form of two distinct channels [<xref ref-type="bibr" rid="pcbi.1004294.ref045">45</xref>], where each of the channels preferred IPDs of an opposite sign.</p>
<sec id="sec008">
<title>Spatial tuning of second layer units</title>
<p>The second layer of the model learned a distributed representation of sound features accesible to neurons in the auditory cortex. Assuming that the cortical auditory code indeed develops driven by principles of efficiency and sparsity, one can interpret second layer basis functions as neuronal receptive fields and sparse coefficients <italic>s</italic> as a measure of neuronal activity (e.g. firing rates). The model can be then probed using spatial auditory stimuli. If it indeed provides an approximation to real neuronal computations, its responses should be comparable with spatial tuning properties of the auditory cortex.</p>
<p>In order to verify whether this was true, a test recording was performed. As a test sound the hiss of two pieces of paper rubbed against each other was used. It was a broadband signal, reminiscent of white noise used in physiological experiments, yet posessing natural structure. Recording was performed in an anechoic chamber, where a person walked around the recording subject while rubbing two pieces of paper (see <xref ref-type="sec" rid="sec014">Methods</xref> for a detailed description). The recording was divided into 18 windows, each corresponding to a 20 degree part of a full circle. The number of windows was selected to match experimental parameters in [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref010">10</xref>]. From each window 3000 epochs were drawn and each of them was encoded using the model. Computing histograms of coefficients <italic>s</italic> at each angular position <italic>Œ∏</italic>, provided an estimate of conditional distributions <italic>p</italic>(<italic>s</italic><sub><italic>i</italic></sub>‚à£<italic>Œ∏</italic>). Panel A in <xref ref-type="fig" rid="pcbi.1004294.g007">Fig 7</xref> displays a conditional histogram of coefficient <italic>s</italic> corresponding to the basis function pair depicted in <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4A</xref>.</p>
<fig id="pcbi.1004294.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Spatial tuning curves of second-layer units.</title>
<p>A) Conditional distribution of the coefficient <italic>s</italic><sub><italic>i</italic></sub> corresponding to basis functions <italic>B</italic><sub><italic>i</italic></sub>, <italic>Œæ</italic><sub><italic>i</italic></sub> depicted in <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4A</xref>. The red line depicts the average value conditioned in sound position. B) Experimentally measured spatial tuning curves measured in the A1 area of the cat. The left panel depicts contra- and the right panel ipsi- laterally tuned units. Figure modified from [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>] C) All position-modulated tuning curves belonging to each of the two clusters. Thin gray lines are single tuning curves, while thick black lines depict cluster averages.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g007"/>
</fig>
<p>Distributions of sparse coefficients revealed a strong dependence in the position of the sound source. As visible in the figure, the conditional mean of the distribution <italic>p</italic>(<italic>s</italic><sub><italic>i</italic></sub>‚à£<italic>Œ∏</italic>) traced by the red line varied in a pronounced way across all positions. By analogy to averaged firing rates of neurons, average unit responses at each position were further studied to understand the spatial sensitivity of basis functions. Mean vectors <italic>Œº</italic><sub><italic>i</italic>, <italic>Œ∏</italic></sub> were constructed for each second-layer unit by taking its average response at the sound source position <italic>Œ∏</italic>. Each mean vector was shifted and scaled such that its minimum value was equal to 0 and the maximum to 1. Such transformation was analogical to physiological studies [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>] and allowed for comparison with experimetally measured spatial tuning curves of auditory neurons, and for this reason scaled vectors <italic>Œº</italic><sub><italic>i</italic></sub> will be referred to as model tuning curves in the remainder of the paper. In order to identify spatial tuning preferences, the population of model tuning curves was grouped into two clusters using the k-means algorithm. Obtained clusters consisted of 118 and 138 similar vectors. Tuning curves belonging to both clusters and revealing a strong correlation (‚à£<italic>œÅ</italic>‚à£ &gt; 0.75) with sound position are plotted in <xref ref-type="fig" rid="pcbi.1004294.g007">Fig 7C</xref> as gray lines. Cluster centroids (averages of all tuning curves belonging to a cluster) are plotted in black. Second layer units were tuned broadly‚Äîmost of them were modulated by sound located at all positions surrounding the subject‚Äôs head. A clear spatial preference is visible‚Äîmembers of cluster 1 were most highly activated (on average) by sounds localized close to the left ear (<italic>Œ∏</italic> ‚âà ‚àí90¬∞), while cluster 2 consisted of units tuned to the right ear (<italic>Œ∏</italic> ‚âà 90¬∞). Very similar tuning properties of auditory neurons were identified in the cat‚Äôs auditory cortex [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>]. Data from this study is plotted for comparison in the subfigure B of <xref ref-type="fig" rid="pcbi.1004294.g007">Fig 7</xref>. Neuronal recordings were performed in the right hemisphere and two panels depict two subpopulations of neurons. The larger contra- and the smaller ipsi-lateral one. It is important to note, that the notion of ipsi, and contra laterality is not meaningful in the proposed model, therefore one should compare shapes of model and experimental tuning curves, not the numerosity of units in each population or cluster.</p>
<p>Two major features of cortical auditory neurons responsive to sound position were observed experimentally: (i) tuning curve peaks were localized mostly at extremely lateral positions (opposite to each ear) and (ii) slopes of tuning curves were steepest close to the auditory midline. Both properties are visible in model tuning curves in <xref ref-type="fig" rid="pcbi.1004294.g007">Fig 7</xref>. However, in order to perform a more direct comparison between the model and experimental data, analysis analogous to the one described in [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>] was performed. First, tuning curve centroids were computed. A centroid was defined as an average position, where the unit activation was equal to 0.75 or larger (see <xref ref-type="sec" rid="sec014">Methods</xref>). In the following step, the position of maximal slope towards midline was identified for each unit. This meant that for units tuned to the left hemifield (cluster 1) the position of the minimal slope value was taken, while the position of the maximal one was taken for units tuned to the right hemifield (cluster 2). In this way, the position of maximal sensitivity to changes in sound location was identified. Distributions of model centroids and maximal slope positions are depicted in <xref ref-type="fig" rid="pcbi.1004294.g008">Fig 8B</xref>. Centroids were distributed close to lateral positions, opposite in each cluster (‚àí90¬∞ cluster 1, +90¬∞ cluster 2). Distribution peaks were located at positions close to each ear. No uniform tiling of the space by centroid values was present. At the same time, maximal slope values were tightly packed around the midline‚Äîpeaks of their distributions were located precisely at, or very close to 0 degrees. This means that while the maximal response was on average triggered by lateral stimuli, the largest changes were triggered by sounds located close to the midline. Both properties were in good agreement with the experimental data reported in [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>]. <xref ref-type="fig" rid="pcbi.1004294.g008">Fig 8A</xref> depicts in three panels centroid and slopes distributions measured in three different regions of cat‚Äôs auditory cortex‚ÄîPrimary Auditory Field (A1), Posterior Auditory Field (PAF) and Dorsal Zone (DZ). A close resemblance between the model and physiological data was visible.</p>
<fig id="pcbi.1004294.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Distribution of tuning curve centroids and maximal slope positions in the model and experimental data.</title>
<p>A) Histograms of positions of tuning curve centroids (gray) and maximal slopes towards the midline (black) measured experimentally in the auditory cortical areas from the cat. Figure modified from [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>]. B) Distribution of the same features computed for model tuning curves belonging to each cluster.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g008"/>
</fig>
<p>It has been argued that while single neurons in the auditory cortex provide coarse spatial information, their populations form a distributed code for sound localization [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref010">10</xref>]. Here, a decoding analysis was performed to verify whether similar statement can be made about the proposed model.</p>
<p>A gaussian mixture model (GMM) was utilized as a decoder. The GMM modelled the marginal distribution of sparse coefficients as a linear combination of 18 gaussian components, each corresponding to a particular position of a sound source (i.e. the <italic>Œ∏</italic> value). In the first part of the decoding analysis, single coefficients were used to identify the sound position. The GMM was fitted using the training dataset consisting of coefficient values <italic>s</italic><sub><italic>i</italic></sub> and associated position labels <italic>Œ∏</italic>. In the testing stage, position estimates <inline-formula id="pcbi.1004294.e032"><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> were estimated (decoded) using unlabeled coefficients from the test dataset (see <xref ref-type="sec" rid="sec014">Methods</xref> section for a detailed description of the decoding procedure). For each of the coefficients, a confusion matrix was computed. A confusion matrix is a two-dimensional histogram of <italic>Œ∏</italic> and <inline-formula id="pcbi.1004294.e033"><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and can be understood as an estimate of the joint probability distribution of these two variables. Using a confusion matrix, an estimate of mutual information (i.e., the number of bits shared between the position estimate <inline-formula id="pcbi.1004294.e034"><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and its actual value <italic>Œ∏</italic>) was obtained. <xref ref-type="fig" rid="pcbi.1004294.g009">Fig 9A</xref> depicts histograms of information carried by each coefficient <italic>s</italic><sub><italic>i</italic></sub> about the sound source position, estimated as described above. A general observation is that single coefficients carried very little information about the sound location. The histogram peaks at a value close to 0.1 bits. Only few units coded approximately 1 bit of positional information. Even 1 bit, however, suffices merely to identify a hemifield, not to mention the precise sound position. As can be predicted from the broad shapes of the tuning curves, single second-layer units carried little spatial information. A similar result was obtained for neurons in different areas of the cats auditory cortex [<xref ref-type="bibr" rid="pcbi.1004294.ref012">12</xref>]. The amount of information about the sound position encoded by spike count of neurons in A1 and PAF regions has a distribution closely similar to that of model units (compare with the left panel of figure 11 in [<xref ref-type="bibr" rid="pcbi.1004294.ref046">46</xref>]). Spike count (which essentially corresponds to a firing rate) is a feature of a neuronal response most directly corresponding to coefficients <italic>s</italic> in the model described here. The median of mutual information estimated from model coefficients (marked by a diamond symbol in panel A) aligns well with the same quantity estimated from neuronal data, and is close to 0.2 bits [<xref ref-type="bibr" rid="pcbi.1004294.ref046">46</xref>]. Overall, physiological measurements and the behavior of the model were highly similar.</p>
<fig id="pcbi.1004294.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Population decoding analysis.</title>
<p>A) Histogram of position-specific information carried by second layer sparse coefficients <italic>s</italic>. The diamond symbol marks the distribution median. B) Mutual information plotted as a function of the number of units used to decode the position. Colors of lines correspond to data averaged over different number of samples. The scale ends at 4.17 bits, which is the amount of information required to perform errorless decoding (log<sub>2</sub>(18) = 4.17) C) Confusion matrix for decoding of population responses from a single sample, and D) averaged across 16 samples.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g009"/>
</fig>
<p>While single neurons did not carry much spatial information, the joint population activity was sufficient to decode the sound position [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1004294.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref046">46</xref>]. Therefore in the second step of the decoding analysis, multiple coefficients <italic>s</italic> were used to train and test the GMM decoder. Results of the population decoding are plotted in <xref ref-type="fig" rid="pcbi.1004294.g009">Fig 9B</xref>. The decoder was trained with a progressively larger number of second-layer units (from 1 to 256) and the mutual information was estimated from obtained confusion matrices. Each line in the plot depicts the number of bits as a function of the number of units used to perform decoding. Line colors correspond to the number of samples over which the average activity was computed. Broadly speaking, larger populations of second-layer units allowed for a more precise position decoding. As in the case of single units, averages over larger amounts of samples were also more informative‚Äîpopulation activity averaged over 32 samples saturated amount of bits required to perform errorless decoding (4.17). Two confusion matrices obtained from raw population activity and an average over 16 samples are displayed in subfigures Fig <xref ref-type="fig" rid="pcbi.1004294.g009">9C</xref> and <xref ref-type="fig" rid="pcbi.1004294.g009">9D</xref>. In the former case, the decoder was mostly misclassifying sound positions within each hemifield. Averaging over 16 sound samples yielded an almost diagonal (errorless) confusion matrix. The decoding analysis allowed us to draw the conclusion that while single units carried very little spatial information, their population encoded source location accurately, consistent with experimental data.</p>
<p>Second layer units achieved spatial tuning by assigning different weights to amplitudes in each ear, and to IPD values in different frequency channels. At the same time they encoded spectrotemporal features of sound, as depicted in <xref ref-type="fig" rid="pcbi.1004294.g004">Fig 4</xref>. Their activity should therefore be modulated by both sound position as well as its quality. Such comodulation is a prominent feature of the majority of cortical auditory neurons [<xref ref-type="bibr" rid="pcbi.1004294.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref007">7</xref>]. In order to verify this, model spatial tuning curves were estimated with a second sound source, very different from a hiss created by rubbing paper‚Äîhuman speech (see <xref ref-type="sec" rid="sec014">Materials and Methods</xref> for details). Frequency spectra of both test stimuli are depicted in <xref ref-type="fig" rid="pcbi.1004294.g010">Fig 10D</xref>. Test sounds distributed their energy over non-overlapping parts of the frequency spectrum. While speech consisted mostly of harmonic peaks below 1.5 kHz, the paper sound was much more broadband and its energy was uniformely distributed between 1.5 and 4 kHz.</p>
<fig id="pcbi.1004294.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004294.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Comodulation of unit responses by sound position and identity.</title>
<p>A)-C) Three representative second layer basis functions plotted with spatial tuning curves obtained using two different sounds‚Äîfemale speech (gray) and paper noise (black). D) Frequency spectra of both test sounds.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004294.g010"/>
</fig>
<p>Panels A-C of <xref ref-type="fig" rid="pcbi.1004294.g010">Fig 10</xref> depict three amplitude/IPD basis function pairs together with their spatial tuning curves estimated using different sounds. The spatial preference of depicted units (left or right hemifield) was predictable from their binaural composition. Each of them, however, was activated stronger by a stimulus, which spectrum matched better amplitude basis functions. Basis functions visible in panels A and C had a lot of energy accumulated in higher frequencies, therefore the paper sound activated them stronger (on average). Basis function B) was spectrally better corresponding to speech sounds, therefore speech was a preferred class of stimuli. This observation suggests that tuning curves i.e. position-conditional means <italic>Œº</italic><sub><italic>i</italic>, <italic>Œ∏</italic></sub> should be understood not as averages of coefficient ensembles conditioned only on the sound position <italic>Œ∏</italic> but also on spectral properties of sound. When interpreting coefficients <italic>s</italic> as neuronal activity this means that spatial tuning curves would alter their shapes when the neuron is tested with two different sound sources. Taken together, one can state that the second-layer representation encoded position and identity of the stimulus in an interdependent fashion.</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>Previously proposed statistical models of natural acoustic stimuli focused predominantly on monaural sounds [<xref ref-type="bibr" rid="pcbi.1004294.ref022">22</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1004294.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref038">38</xref>]. Studies modelling binaural stimuli were constrained to a limited representation‚Äîeither IPDs [<xref ref-type="bibr" rid="pcbi.1004294.ref045">45</xref>] or spectrograms [<xref ref-type="bibr" rid="pcbi.1004294.ref031">31</xref>]. In contrast, the assumption behind the present work was that spatial sensitivity of cortical neurons is formed by fusing different cues. Therefore, in order to understand the role played by the auditory cortex in spatial hearing, the entire natural input processed by the auditory system was analyzed.</p>
<p>To this end, a novel probabilistic model of natural stereo sounds has been proposed. The model is based on principles of sparse, efficient coding‚Äîits task was to learn progressively less redundant representations of natural signal. It consisted of two hidden layers, each of them could be interpreted as an analogy to different stages of sound processing in the nervous system. The purpose of the first layer was to form a sparse, non-redundant representation of natural sound in each ear. By analogy to the cochlea, the encoding was supposed to extract and separate temporal information (i.e. phase) from the amplitude of the signal. In order to do so, a dictionary of complex-valued basis functions was adapted to short sound epochs. On top of the first model layer, which encoded sound in each ear independently, the second layer was trained. Its goal was to encode jointly amplitude and phase‚Äîtwo kinds of information crucial for sound localization, which may be fused together in higher stages of the auditory system. The higher-order representation captured spectrotemporal composition of the signal, by learning amplitude patterns of the first layer output as well as interaural disparities present in form of interaural phase and amplitude differences. It is important to stress that the model was fully unsupervised‚Äîat no point information about positions of sounds sources or the spatial configuration of the environment was accessible. Yet, when tested with a set of spatial sounds, activity of second layer units revealed strong dependence on sound position. Tuning curves describing relation between the sound position and model activity were in good correspondence with experimentally measured spatial tuning properties of cortical auditory neurons.</p>
<sec id="sec010">
<title>A sparse representation of natural binaural sounds forms a panoramic population code for sound location</title>
<p>In mammals, the location of a sound is encoded by two populations of broadly tuned, spatially non-specific units [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. This finding challenges initial expectations of finding a ‚Äúlabelled-line code‚Äù (i.e. a topographic map of neurons narrowly tuned to small areas of space). The ‚Äúspatiotopic map‚Äù was expected by analogy to the tonotopic structure of the cortex, as well as the high localisation accuracy of humans and animals. Instead, it has been found that auditory cortical neurons within each hemisphere are predominantly tuned to far, contralateral positions. Peaks of observed tuning curves did not tile the auditory space uniformly, rather they were clustered around the two lateral positions. A prominent observed feature of cortical representation of sound location were slopes of the tuning curves. Regardless of the position of the tuning curve peak, slopes were steepest close to the interaural midline‚Äîthe area where behavioral localisation acuity is highest [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. From described observations, two prominent conclusions were drawn. Firstly, that the slope of tuning curves, not the distribution of their peaks determines spatial acuity [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref048">48</xref>]. Secondly that sound position is encoded by distributed patterns of population activity, not single neurons [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1004294.ref010">10</xref>]. It has been argued that these properties are a manifestation of a coding mechanism which evolved to specifically meet the demand of binaural hearing tasks [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. Here it is shown that crucial properties of cortical spatial tuning emerge in an unsupervised learning model, which learns a sparse representation of natural binaural sounds. The objective of the model was to code the stimulus efficiently (i.e. with a minimal redundancy within limits of applied transformations), while minimizing unit activity. Properties of the learned representation are therefore a reflection of stimulus statistics, not of any task-specific coding strategy (required for instance to localize sounds with the highest accuracy at the midline).</p>
<p>The position of the sound-generating object is a latent variable for the auditory system. It means that its value is not explicitly present in the raw stimulus‚Äîit has to be estimated. This estimation, (or inference) is a non-trivial task in the real acoustic environment, where sounds reaching ear membranes are a reflection of intricate auditory scenes. Sensory neurons perform transformations of those sound waveforms to reconstruct the spatial configuration of the scene. Therefore, in an attempt to understand cortical representation of space, it may be helpful to think what is the statistical structure of the naturally encountered binaural stimulus that the auditory system operates on. Sounds reaching the ear contain information about their generating sources, the spatial configuration of the scene, position and motion of the organism and the geometry of its head and outer ears.</p>
<p>Results obtained here suggest that the shapes of the model spatial tuning curves reflect regularities imposed on the sensory data by the filtering properties of the head. At lateral positions (directly next to the left or the right ear) there is no acoustic attenuation by the skull, hence sounds are loudest and least delayed. This in turn, elicits the strongest response in units preferring that side. When the sound is at a contralateral position, response is much weaker, due to the maximal head attenuation and largest delay. The curve connecting those two extrema is steepest in the transition area‚Äîat the midline. Since the auditory environment was uniformly sampled at both sides of the head, model units were clustered into two roughly equal subpopulations, basing on the shapes of their tuning curves. Clusters were symmetric with respect to each other‚Äîone tuned to to the left and the other to the right hemifield. This groupping is reminiscent of the ‚Äúopponent-channel‚Äù representation of the auditory space, which has been postulated before [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>]. Present results provide a theoretical interpretation of this tuning pattern. They suggest that neuronal population which forms a sparse, efficient representation of natural stimuli would reveal two broadly tunned channels, when probed with sounds located at different positions.</p>
<p>It has been shown previously that IPD coding strategies in different species can be predicted from statistics of binaural sound [<xref ref-type="bibr" rid="pcbi.1004294.ref045">45</xref>]. Harper and McAlpine demonstrated that if the goal of the nervous system is to represent IPD values with the maximal possible accuracy (quantified by Fisher information) two populations of neurons tuned to opposite locations constitute an optimal representation of low-frequency IPDs. Their approach differs significantly from the one presented here. On the most abstract level, the authors of [<xref ref-type="bibr" rid="pcbi.1004294.ref045">45</xref>] assume that the purpose of IPD sensitive neurons is to maximize Fisher information, while here mutual information is the quantity implicitly maximized by the representation (although interesting relationships exist between those two measures [<xref ref-type="bibr" rid="pcbi.1004294.ref049">49</xref>]). Secondly, Harper and McAlpine limit their analysis to IPD statistics only‚Äîhere entire binaural waveforms are modelled. Finally the current study does not assume any parameteric shape of tuning curves, nor make any other assumptions about physiology as is the case in [<xref ref-type="bibr" rid="pcbi.1004294.ref045">45</xref>]. The similarity of model responses and neuronal activity emerges from data statistics.</p>
</sec>
<sec id="sec011">
<title>Interdependent coding of spatial information and other features of the sound</title>
<p>There is an ongoing debate about the presence (or lack of thereof) of two-separate ‚Äúwhat‚Äù and ‚Äúwhere‚Äù streams in the auditory cortex [<xref ref-type="bibr" rid="pcbi.1004294.ref005">5</xref>]. The streams would separate spatial information from other sound features which determine its identity. An important prediction formed by this dual-stream hypothesis is that there should exist neurons selective to sound position and invariant to other aspects in the auditory cortex. While some evidence has been found supporting this notion [<xref ref-type="bibr" rid="pcbi.1004294.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref004">4</xref>] it seems that at least in vast parts of the auditory cortex neural activity can be modulated by multiple features of sound such as pitch, timbre and location [<xref ref-type="bibr" rid="pcbi.1004294.ref001">1</xref>]. Neurons are sensitive to sound position (i.e. changing position affects their firing patterns), but not selective nor invariant to it. The majority of studies analyzing spatial sensitivity in the auditory cortex use a single class of sound and the source position is the only varying parameter. Therefore, despite initial efforts, the influence jointly exerted by sound quality and position on neuronal activity is not yet well understood.</p>
<p>The statistical model proposed here suggests that no dissociation of spatial and non-spatial information is necessary to either reconstruct the sound source or identify its position. The learned second-layer representation carries both kinds of information‚Äîabout the sound quality (contained in the spectrotemporal structure of basis functions) and about spatial aspects (contained in the binaural amplitude weighting and IPD vectors). The learned code forms a ‚Äúwhat is where‚Äù representation of the stimulus (i.e., those two aspects are represented interdependently). A manifestation of this fact is visible in different scaling of spatial tuning curves, when probed with two different sound sources. Such comodulation of neuronal activity by sound position and quality has been observed experimentally [<xref ref-type="bibr" rid="pcbi.1004294.ref001">1</xref>], which may suggest that recorded neurons form a sparse, efficient representation of binaural sound. An advantage of an interdependent ‚Äúwhat is where‚Äù representation is the absence of the ‚Äúfeature binding problem‚Äù, which has to be solved if spatial information is processed independently. After separating the location of a source from its identity they would have to be fused at processing stages beyond the auditory cortex. A code similar to the one described here does not create such a problem. This idea goes in hand with results of a recent perceptual study [<xref ref-type="bibr" rid="pcbi.1004294.ref050">50</xref>]. Parise et al. demonstrated that the perception of sound source elevation is strongly influenced by its frequency. Furthermore they show that this relationship can be explained by adaptation to the joint distribution of natural sounds‚Äô positions and spectra. This implies that the quality of the sound source as well as its spatial position are mutually dependent, and as such should be represented jointly, if the goal of the nervous system is to increase coding efficiency.</p>
</sec>
<sec id="sec012">
<title>Limitations and possible extensions</title>
<p>The model proposed in this work is a statistical one‚Äîit constitutes an attempt to describe functional, not anatomical modules of the auditory system. Rather than explicitly modelling stages of the auditory pathway, its goal is to approximate the distribution of natural binaural sounds. The behaviour of units in the highest layer reveals a strong resemblance to cortical auditory neurons in an abstract, information processing domain. In the mammalian auditory system the sound is processed in at least five anatomical structures before it reaches the cortex [<xref ref-type="bibr" rid="pcbi.1004294.ref051">51</xref>]. It is therefore almost certain that the stimulus is subjected to many more complex transformations than the ones proposed here. On the other hand, the fact that similiarities between cortical and model responses emerge despite this lack of detail, imply that the model may be capturing some aspects of information processing, as it happens in the real auditory system.</p>
<p>The relationship between abstract computational principles such as sparse coding and neurophysiology is an area of ongoing research [<xref ref-type="bibr" rid="pcbi.1004294.ref052">52</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1004294.ref054">54</xref>]. An interesting extension of the present work would attempt to increase the level of biological detail, and see whether this allows formation of more refined experimental predictions. This could be done by implementing sparse coding computations using spiking neuron models, as it has been done in studies of the visual system (e.g. [<xref ref-type="bibr" rid="pcbi.1004294.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref054">54</xref>]). The match between the model and biology could be also improved by including phenomena specific to the auditory system, such as the phase locking in the auditory nerve.</p>
<p>This study focuses predominantly on explaining the broad spatial tuning of cortical auditory neurons estimated by the analysis of firing rates. With progressively larger amounts of biological detail added to the model, one could attempt to explain other aspects of spatial information encoding. For instance, the notion of spike timing does not exist in the approach proposed here, while temporal spike patterns of cortical neurons seem to carry relevant spatial information [<xref ref-type="bibr" rid="pcbi.1004294.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref046">46</xref>]. Moreover, as mentioned in the results section, the concept of contra- and ipsilaterality is spurious for high-layer model units since they are not associated with any anatomical locus (left or right hemisphere). Overrepresentation of the contralateral ear is an interesting feature of panoramic population codes [<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>], which is also not addressed by the present work. Further exploration of the relationship between specific biological observations and spatial information processing constitutes a possible goal for future research.</p>
<p>It is highly likely that the main result of this study (i.e., spatial tuning properties of the binaural sound representation) could be reproduced by replacing the first layer with a different sort of spectrotemporal signal representation. It would not necessarily have to be the sparse, efficient encoding of sound epochs. A spectrogram could be a candidate signal, although it has been demonstrated that a sparse code of relatively long binaural spectrogram chunks generates features of very different spatial tuning [<xref ref-type="bibr" rid="pcbi.1004294.ref031">31</xref>]. In this work, for the sake of theoretical consistency, both layers were learned using the same principles and statistical assumptions‚Äîsparse factorial coding.</p>
<p>The data used for comparisons originated from studies of cat auditory cortex ([<xref ref-type="bibr" rid="pcbi.1004294.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref046">46</xref>]). Since statistics of the binaural signal are affected by the geometry of ears and the head of the organisms, one could argue that model trained on binaural recordings performed by a human should not be compared with cat physiology. As long as detailed features of neuronal tuning to a sound position may vary across those species, tuning patterns highly similar to those of the cat have been observed in the auditory cortex of primates [<xref ref-type="bibr" rid="pcbi.1004294.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref056">56</xref>]. Overall, the cortical representation of sound position seems to be highly similar across mammals [<xref ref-type="bibr" rid="pcbi.1004294.ref032">32</xref>].</p>
<p>Finally, in the current study a binaural recording of only a single auditory scene was used to train the model. Even though the recording included many types of sound‚Äîambient environmental noises, transient cracks and clicks and harmonic structures such as the human speech, it did not include many other possible sources (for instance animal vocalizations). The recording included also only a narrow range of other parameters which characterize natural auditory scenes, such as reverberation. Analysis of longer recordings performed in different environmental settings may generate more diverse results and additional insights. One should note however, that certain properties of the learned representation (such as the tradeoff in the spectrotemporal modulation) seem to be a general proprerty of natural sounds as such and remain invariant to a specific dataset [<xref ref-type="bibr" rid="pcbi.1004294.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref040">40</xref>]. Basing on this observation one may expect that units revealing similar spatial tuning can be learned from recordings of numerous, diverse sets of natural sounds.</p>
</sec>
<sec id="sec013">
<title>Conclusion</title>
<p>Taken together, this paper proposes a candidate theoretical mechanism explaining how neurons in the auditory cortex represent spatial information. This model allows us to speculate they do not have to implement any task-dependent strategy. Instead, their behavior can be explained by sparse coding‚Äîa statistical model which has succesfully predicted properties of multiple other sensory systems [<xref ref-type="bibr" rid="pcbi.1004294.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref021">21</xref>]. Taking a broad perspective, (as suggested by Barlow in his later work [<xref ref-type="bibr" rid="pcbi.1004294.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref058">58</xref>]) this means that redundancy reduction by sparse coding can be used by the brain to identify sensory data patterns allowing sucesful interaction with the environment.</p>
</sec>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Ethics statement</title>
<p>Sound recordings received approval of the Ethics Council of the Max-Planck Society. Human participants provided a written consent to participate in recordings.</p>
</sec>
<sec id="sec016">
<title>Binaural recordings</title>
<p>Sounds used to train and test the model were recorded using Soundman OKM-II binaural microphones placed in the ear channels of a human subject, whose head circumference was 60 cm. While recording training sounds, the subject walked freely in a wooded area accompanied by another person who spoke rarely. In this way, collected data included transient and ambient environmental sounds as well as harmonic speech. The binaural composition of sound was affected by spatial configuration of the environment and motion patterns of the recording subject. The recording used to train the model was 60 seconds long in total. Binaural recordings are availible in the supplementary material of [<xref ref-type="bibr" rid="pcbi.1004294.ref059">59</xref>].</p>
<p>Test recordings used to map the spatial tuning of second-layer units was performed in an anechoic chamber at the Department of Biology, University of Leipzig. The same recording subject was seated in the middle of the chamber. A female speaker walked at a constant pace following a circular path surrounding the recording subject. While walking she counted out loud. This was repeated four times. The second test recording was performed in a similar fashion, however instead of speaking the walking person rubbed two pieces of cardboard against each other, generating a broadband sound. To estimate the conditional distribution of sparse coefficients given the position and identity of the sound, test recordings were divided into 18 intervals, each corresponding to the same position on a circle.</p>
<p>All recordings were registered in an uncompressed wave format at 44100 Hz sampling rate. Prior to training the model, sounds were downsampled to 8000 Hz. Test recordings are availible in the supplementary material (<xref ref-type="supplementary-material" rid="pcbi.1004294.s001">S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s002">S2</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s003">S3</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s004">S4</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s005">S5</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s006">S6</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s007">S7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004294.s008">S8</xref> Files).</p>
</sec>
<sec id="sec017">
<title>Learning and inference</title>
<p>The goal of the learning procedure was to estimate first- (<italic>A</italic>), and second- layer basis functions (<italic>B</italic>, <italic>Œæ</italic>). This was done using a two-step approach. Firstly maximum a posteriori (MAP) estimates of model coefficients (<italic>z</italic> in the first layer, <italic>s</italic> and <italic>w</italic> in the second) were inferred via gradient descent [<xref ref-type="bibr" rid="pcbi.1004294.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref033">33</xref>]. Secondly, a gradient update on basis functions was perormed using current coefficient estimates. Those two steps were consecutively iterated until the model converged.</p>
<p>A dictionary of complex-basis functions in the first layer was created by first, training a standard sparse code of sound epochs <italic>x</italic> ‚àà ‚Ñù<sup><italic>T</italic></sup>:
<disp-formula id="pcbi.1004294.e035"><alternatives><graphic id="pcbi.1004294.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e035"/><mml:math id="M35" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mo>Œò</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>Œ∑</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>The negative log-posterior of this model was:
<disp-formula id="pcbi.1004294.e036"><alternatives><graphic id="pcbi.1004294.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e036"/><mml:math id="M36" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Œò</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>‚àù</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>Œª</mml:mi> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
where <inline-formula id="pcbi.1004294.e037"><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mo>Œò</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the reconstruction of the data vector. Corresponding gradients over linear coefficients <italic>c</italic> and basis functions Œò were given by:
<disp-formula id="pcbi.1004294.e038"><alternatives><graphic id="pcbi.1004294.e038g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e038"/><mml:math id="M38" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msub><mml:mo>Œò</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>Œª</mml:mi> <mml:mfrac><mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula> <disp-formula id="pcbi.1004294.e039"><alternatives><graphic id="pcbi.1004294.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e039"/><mml:math id="M39" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mo>Œò</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>Learned basis functions Œò<sub><italic>i</italic></sub> were used as real vectors <inline-formula id="pcbi.1004294.e040"><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>i</mml:mi> <mml:mo>‚Ñú</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and extended with their Hilbert transforms. Such complex basis function dictionary was used to encode monaural sound epochs. Gradients of <xref ref-type="disp-formula" rid="pcbi.1004294.e012">Eq 5</xref> over phase <italic>œï</italic><sub><italic>i</italic></sub> and amplitudes <italic>a</italic><sub><italic>i</italic></sub> of complex coefficients <italic>z</italic><sub><italic>i</italic></sub> were equal to:
<disp-formula id="pcbi.1004294.e041"><alternatives><graphic id="pcbi.1004294.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e041"/><mml:math id="M41" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mo>(</mml:mo> <mml:mo form="prefix">cos</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>‚Ñú</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mo form="prefix">sin</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>‚Ñë</mml:mi></mml:msubsup> <mml:mo>)</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>Œª</mml:mi> <mml:mfrac><mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula> <disp-formula id="pcbi.1004294.e042"><alternatives><graphic id="pcbi.1004294.e042g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e042"/><mml:math id="M42" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>‚Ñë</mml:mi></mml:msubsup> <mml:mo form="prefix">cos</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>‚Ñë</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>‚Ñú</mml:mi></mml:msubsup> <mml:mo form="prefix">sin</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mi>‚Ñú</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula></p>
<p>The second layer of the model was trained after the first layer converged, and cofficient values <italic>z</italic> were inferred for all training data samples. The higher order encoding formed by coefficients <italic>s</italic> as well as the scaling factor <italic>w</italic> was inferred via gradient descent on function <italic>E</italic><sub>2</sub> (<xref ref-type="disp-formula" rid="pcbi.1004294.e024">Eq 13</xref>):
<disp-formula id="pcbi.1004294.e043"><alternatives><graphic id="pcbi.1004294.e043g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e043"/><mml:math id="M43" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>√ó</mml:mo> <mml:mi>T</mml:mi></mml:mrow></mml:munderover> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>Œ∫</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>P</mml:mi></mml:munderover> <mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>^</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>Œæ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:msub><mml:mi>Œª</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mfrac><mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula> <disp-formula id="pcbi.1004294.e044"><alternatives><graphic id="pcbi.1004294.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e044"/><mml:math id="M44" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mi>Œ∫</mml:mi> <mml:mfrac><mml:mi>w</mml:mi> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>P</mml:mi></mml:munderover> <mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>^</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>^</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>Œª</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>Œ±</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>Œ≤</mml:mi></mml:msup> <mml:mi>Œ≤</mml:mi> <mml:mi>w</mml:mi> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mi>Œ≤</mml:mi> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula></p>
<p>The gradients steered sparse coefficients <italic>s</italic> to explain amplitude and phase vectors <italic>a</italic> and Œî<italic>œï</italic> while preserving maximal sparsity. Simultaneously the multiplicative factor <italic>w</italic> was adjusted to appropriately scale the estimated vector <inline-formula id="pcbi.1004294.e045"><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>ÃÇ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p>
<p>Finally, learning rules for second-layer dictionaries were given by:
<disp-formula id="pcbi.1004294.e046"><alternatives><graphic id="pcbi.1004294.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e046"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>‚àù</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfrac> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula> <disp-formula id="pcbi.1004294.e047"><alternatives><graphic id="pcbi.1004294.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e047"/><mml:math id="M47" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mi>‚àÇ</mml:mi> <mml:mrow><mml:mi>‚àÇ</mml:mi> <mml:msub><mml:mi>Œæ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msub><mml:mi>E</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>‚àù</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>Œ∫</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Œî</mml:mo> <mml:msub><mml:mi>œï</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>Œî</mml:mo> <mml:mi>œï</mml:mi></mml:mrow> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula></p>
<p>Altogether 75000 epochs of binaural sound were used to train the model. Each of them was <italic>T</italic> = 128 samples long, which corresponded to 16 ms. Both layers were trained separately. Before training the first layer, Principal Component Analyis was perfomed and 18 out of 128 principal components were rejected, which corresponded to low pass filtering the data. Left and right ear sound epochs were shuffled together to create a 150000 sample training dataset for the first layer. The first layer sparsity coefficient <italic>Œª</italic> was set to 0.2. Noise variance <italic>œÉ</italic><sup>2</sup> was equal to 2. The sparse coding algorithm converged after 200000 iterations.</p>
<p>A complex-valued dictionary was created by extending the real valued one with Hilbert-transformed basis functions. Amplitude and phase vectors <italic>a</italic> and <italic>œï</italic> were inferred for each sample using 20 gradient steps. Amplitude vectors were concatenated and transformed with a logarithmic function, and IPD vectors Œî<italic>œï</italic> were computed by substracting left ear phase vectors <italic>œï</italic><sub><italic>L</italic></sub> from right ear ones <italic>œï</italic><sub><italic>R</italic></sub>. The second layer was trained by performing 250000 gradient updates on basis functions <italic>B</italic> and <italic>Œæ</italic>. The amplitude sparsity coefficient <italic>Œª</italic><sub>2</sub> was set to 1. The <italic>Œª</italic><sub><italic>w</italic></sub> parameter was set to 0.01 and the noise variance <inline-formula id="pcbi.1004294.e048"><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>œÉ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> as well as the von Mises concentration parameter <italic>Œ∫</italic> were set to 2.</p>
<p>Numerical values of the prior-controlling parameters <italic>Œª</italic>, <italic>Œª</italic><sub>2</sub>, <italic>Œª</italic><sub><italic>w</italic></sub> as well as noise parameters <italic>œÉ</italic>, <italic>œÉ</italic><sub>2</sub>, <italic>Œ∫</italic> were set empirically in this study. By running simulations with multiple parameter settings it has been found that due to the presence of a strong environmental noise in the training recording, noise variances <italic>œÉ</italic>, <italic>œÉ</italic><sub>2</sub> and the von Mises concentration parameter <italic>Œ∫</italic> should be relatively large in order to achieve convergence. Sparsity of the high layer representation was set to be larger than that of the first layer in order to mimic the biological intuition that neural responses in the ascending auditory pathway become progressively less redundant and sparser [<xref ref-type="bibr" rid="pcbi.1004294.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004294.ref060">60</xref>]. It has been found however, that the exact value of sparsity paramaters did not affect the spectrotemporal properties, nor the spatial tuning of the second layer units strongly. The <italic>Œª</italic><sub><italic>w</italic></sub> parameter which controls the strength of the prior over the multiplicative factor <italic>w</italic> was set to be relatively small. Otherwise the <italic>w</italic> prior term in the <xref ref-type="disp-formula" rid="pcbi.1004294.e030">Eq 16</xref> became too strong and dominated learning, preventing the convergence. More principled and theoretically sound ways of parameter selection are possible. One could ask what are the natural noise levels and sparsity values of the training data by specifying them as hyperparameters of the model and learning the appropriate values. Also the number of basis functions at each level could be treated as a parameter and estimated from the data, not chosen ad-hoc. After extending the model in this way, the choice of the correct parameter setting could be performed by cross-validation or Bayesian model selection (as in [<xref ref-type="bibr" rid="pcbi.1004294.ref061">61</xref>]).</p>
</sec>
<sec id="sec018">
<title>Computation of modulation spectra of second-layer basis functions</title>
<p>Spectrograms of amplitude basis functions <italic>B</italic><sub><italic>i</italic></sub> were computed by combining spectrograms of real, first layer basis functions <inline-formula id="pcbi.1004294.e049"><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>n</mml:mi> <mml:mo>‚Ñú</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, linearly weighted by a corresponding weight exp(<italic>B</italic><sub><italic>i</italic>, <italic>n</italic></sub>). First layer spectrograms were computed using <italic>T</italic> = 29 windows, each 16 samples (0.002 second) long, with a 12 sample overlap. Altogether, <italic>F</italic> = 128 logarithmically-spaced frequencies were sampled. A two-dimensional fourier transform of each spectrogram was computed using the matlab built-in function fft2. The amplitude spectrum of obtained transform is called the Modulation Transfer Function (MTF) of each second layer feature [<xref ref-type="bibr" rid="pcbi.1004294.ref040">40</xref>]. The center of mass i.e. the point <inline-formula id="pcbi.1004294.e050"><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>S</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>S</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of each monaural part (<italic>S</italic> ‚àà {<italic>L</italic>, <italic>R</italic>}) of basis functions <italic>B</italic><sub><italic>i</italic></sub> was computed in the following way:
<disp-formula id="pcbi.1004294.e051"><alternatives><graphic id="pcbi.1004294.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e051"/><mml:math id="M51" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi> <mml:msub><mml:mi>S</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mi>t</mml:mi></mml:munder> <mml:mi>t</mml:mi> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mi>f</mml:mi></mml:munder> <mml:mi>M</mml:mi> <mml:mi>T</mml:mi> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>S</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula> <disp-formula id="pcbi.1004294.e052"><alternatives><graphic id="pcbi.1004294.e052g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e052"/><mml:math id="M52" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi> <mml:msub><mml:mi>S</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>f</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mi>f</mml:mi></mml:munder> <mml:mi>f</mml:mi> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mi>t</mml:mi></mml:munder> <mml:mi>M</mml:mi> <mml:mi>T</mml:mi> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mi>S</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula>
where <italic>t</italic> and <italic>f</italic> are time and frequency respectively.</p>
</sec>
<sec id="sec019">
<title>Estimation of spatial tuning curves</title>
<p>To estimate conditional distribution of sparse coefficients given the position and identity of the sound, test recordings of a sound source (either speech, or rubbed paper) moving around the recording subject were used. Each source circled the recording person 4 times resulting in 4 recordings. Each of them was divided into 18 intervals. Intervals corresponding to the same area on the circle were joined together across all recordings. For each out of 18 sound positions 3000 random sound chunks were drawn and encoded by the model. Position-conditional ensembles were then used to compute conditional histograms. Conditional mean vectors <italic>Œº</italic><sub><italic>i</italic>, <italic>Œ∏</italic></sub> were computed by averaging all values of coefficient <italic>s</italic><sub><italic>i</italic></sub> at position <italic>Œ∏</italic>. Mean vectors were mapped to a [0, 1] interval by adding the absolute value of a minimal entry and dividing it by the value of the maximum. For plotting purposes in <xref ref-type="fig" rid="pcbi.1004294.g010">Fig 10</xref>, endings of tuning curves were connected if values at ‚àí180¬∞ and 180¬∞ were not exactly equal.</p>
</sec>
<sec id="sec020">
<title>Decoding of stimulus position</title>
<p>The decoding analysis was performed using <italic>K</italic> second-layer sparse coefficients <italic>s</italic> averaged over <italic>D</italic> of samples. The response vectors <italic>d</italic> ‚àà ‚Ñù<sup><italic>K</italic></sup> were therefore formed as:
<disp-formula id="pcbi.1004294.e053"><alternatives><graphic id="pcbi.1004294.e053g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e053"/><mml:math id="M53" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>D</mml:mi></mml:mfrac> <mml:munderover><mml:mo>‚àë</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>D</mml:mi></mml:munderover> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>‚Ä¶</mml:mo> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula>
Such averaging procedure can be interpreted as an analogy to computation of firing rates in real neurons.</p>
<p>The marginal distribution of response coefficients <italic>d</italic> over all 18 sound positions <italic>Œ∏</italic> ‚àà {‚àí180¬∞, ‚àí160¬∞, ‚Ä¶,160¬∞,180¬∞} was equal to:
<disp-formula id="pcbi.1004294.e054"><alternatives><graphic id="pcbi.1004294.e054g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e054"/><mml:math id="M54" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mi>Œ∏</mml:mi></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>d</mml:mi> <mml:mo>|</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula>
where each conditional <italic>p</italic>(<italic>d</italic>‚à£<italic>Œ∏</italic>) was a <italic>K</italic>-dimensional Gaussian distribution with class specific mean vector <italic>Œº</italic><sub><italic>Œ∏</italic></sub> and covariance matrix <italic>C</italic><sub><italic>Œ∏</italic></sub>:
<disp-formula id="pcbi.1004294.e055"><alternatives><graphic id="pcbi.1004294.e055g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e055"/><mml:math id="M55" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>d</mml:mi> <mml:mo>|</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>ùìù</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Œº</mml:mi> <mml:mi>Œ∏</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mi>Œ∏</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula></p>
<p>The prior over class labels <italic>p</italic>(<italic>Œ∏</italic>) was uniformly distributed i.e. <inline-formula id="pcbi.1004294.e056"><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>Œ∏</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>18</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> for each <italic>i</italic>.</p>
<p>The decoding procedure iterated over all class labels and returned the one, which maximized the likelihood of the observed data vector. Out of the entire dataset, 80% was used to train the model and remaining 20% to test and estimate the confusion matrix.</p>
<p>Confusion matrix <italic>M</italic> was a joint histogram of a decoded and true sound position <inline-formula id="pcbi.1004294.e057"><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <italic>Œ∏</italic>. After normalization, it was an estimate of a joint probability mass function <inline-formula id="pcbi.1004294.e058"><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Mutual information was estimated from each confusion matrix as:
<disp-formula id="pcbi.1004294.e059"><alternatives><graphic id="pcbi.1004294.e059g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004294.e059"/><mml:math id="M59" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mspace width="0.277778em"/><mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:munder> <mml:munder><mml:mo>‚àë</mml:mo> <mml:mi>Œ∏</mml:mi></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo form="prefix">log</mml:mo> <mml:mn>2</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Œ∏</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Œ∏</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula></p>
</sec>
</sec>
<sec id="sec021">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004294.s001" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Recording of a test sound source (paper hiss) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s002" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s002" xlink:type="simple">
<label>S2 File</label>
<caption>
<title>Recording of a test sound source (paper hiss) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s003" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s003" xlink:type="simple">
<label>S3 File</label>
<caption>
<title>Recording of a test sound source (paper hiss) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s004" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s004" xlink:type="simple">
<label>S4 File</label>
<caption>
<title>Recording of a test sound source (paper hiss) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s005" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s005" xlink:type="simple">
<label>S5 File</label>
<caption>
<title>Recording of a test sound source (female voice) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s006" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s006" xlink:type="simple">
<label>S6 File</label>
<caption>
<title>Recording of a test sound source (female voice) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s007" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s007" xlink:type="simple">
<label>S7 File</label>
<caption>
<title>Recording of a test sound source (female voice) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004294.s008" position="float" mimetype="audio/x-wav" xlink:href="info:doi/10.1371/journal.pcbi.1004294.s008" xlink:type="simple">
<label>S8 File</label>
<caption>
<title>Recording of a test sound source (female voice) moving around the recording subject.</title>
<p>The recording was used to estimate spatial tuning curves of model units. The sound source circles the recording subject in a clockwise direction.</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>I would like to thank Timm Lochmann for many interesting remarks and proofreading of the manuscript, Philipp Benner and Charles Cadieu for helpful discussions, Mikaella Sarrou and Maria M≈Çynarska for help with binaural recordings and Kevin Woods for detailed corrections of the text.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004294.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Silverman</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>. <article-title>Interdependent encoding of pitch, timbre, and spatial location in auditory cortex</article-title>. <source>The Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>7</issue>):<fpage>2064</fpage>‚Äì<lpage>2075</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4755-08.2009" xlink:type="simple">10.1523/JNEUROSCI.4755-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19228960</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Harrington</surname> <given-names>IA</given-names></name>, <name name-style="western"><surname>Stecker</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Macpherson</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Spatial sensitivity of neurons in the anterior, posterior, and primary fields of cat auditory cortex</article-title>. <source>Hearing research</source>. <year>2008</year>;<volume>240</volume>(<issue>1</issue>):<fpage>22</fpage>‚Äì<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.heares.2008.02.004" xlink:type="simple">10.1016/j.heares.2008.02.004</ext-link></comment> <object-id pub-id-type="pmid">18359176</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Romanski</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fritz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mishkin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Rauschecker</surname> <given-names>JP</given-names></name>. <article-title>Dual streams of auditory afferents target multiple domains in the primate prefrontal cortex</article-title>. <source>Nature neuroscience</source>. <year>1999</year>;<volume>2</volume>(<issue>12</issue>):<fpage>1131</fpage>‚Äì<lpage>1136</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/16056" xlink:type="simple">10.1038/16056</ext-link></comment> <object-id pub-id-type="pmid">10570492</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lomber</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Malhotra</surname> <given-names>S</given-names></name>. <article-title>Double dissociation of‚Äôwhat‚Äôand‚Äôwhere‚Äôprocessing in auditory cortex</article-title>. <source>Nature neuroscience</source>. <year>2008</year>;<volume>11</volume>(<issue>5</issue>):<fpage>609</fpage>‚Äì<lpage>616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2108" xlink:type="simple">10.1038/nn.2108</ext-link></comment> <object-id pub-id-type="pmid">18408717</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Processing of complex sounds in the auditory system</article-title>. <source>Current opinion in neurobiology</source>. <year>2008</year>;<volume>18</volume>(<issue>4</issue>):<fpage>413</fpage>‚Äì<lpage>417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2008.08.014" xlink:type="simple">10.1016/j.conb.2008.08.014</ext-link></comment> <object-id pub-id-type="pmid">18805485</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cohen</surname> <given-names>YE</given-names></name>, <name name-style="western"><surname>Russ</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Gifford</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Kiringoda</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>MacLean</surname> <given-names>KA</given-names></name>. <article-title>Selectivity for the spatial and nonspatial attributes of auditory stimuli in the ventrolateral prefrontal cortex</article-title>. <source>The journal of neuroscience</source>. <year>2004</year>;<volume>24</volume>(<issue>50</issue>):<fpage>11307</fpage>‚Äì<lpage>11316</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3935-04.2004" xlink:type="simple">10.1523/JNEUROSCI.3935-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15601937</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>KM</given-names></name>. <article-title>Sensitivity and selectivity of neurons in auditory cortex to the pitch, timbre, and location of sounds</article-title>. <source>The Neuroscientist</source>. <year>2010</year>;<volume>16</volume>(<issue>4</issue>):<fpage>453</fpage>‚Äì<lpage>469</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1073858410371009" xlink:type="simple">10.1177/1073858410371009</ext-link></comment> <object-id pub-id-type="pmid">20530254</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stecker</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Harrington</surname> <given-names>IA</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Location coding by opponent neural populations in the auditory cortex</article-title>. <source>PLoS biology</source>. <year>2005</year>;<volume>3</volume>(<issue>3</issue>):<fpage>e78</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030078" xlink:type="simple">10.1371/journal.pbio.0030078</ext-link></comment> <object-id pub-id-type="pmid">15736980</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Clock</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>. <article-title>A panoramic code for sound location by cortical neurons</article-title>. <source>Science</source>. <year>1994</year>;<volume>264</volume>(<issue>5160</issue>):<fpage>842</fpage>‚Äì<lpage>844</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.8171339" xlink:type="simple">10.1126/science.8171339</ext-link></comment> <object-id pub-id-type="pmid">8171339</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stecker</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Distributed coding of sound locations in the auditory cortex</article-title>. <source>Biological cybernetics</source>. <year>2003</year>;<volume>89</volume>(<issue>5</issue>):<fpage>341</fpage>‚Äì<lpage>349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-003-0439-1" xlink:type="simple">10.1007/s00422-003-0439-1</ext-link></comment> <object-id pub-id-type="pmid">14669014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Woods</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Lopez</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Rahman</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Recanzone</surname> <given-names>GH</given-names></name>. <article-title>Effects of stimulus azimuth and intensity on the single-neuron activity in the auditory cortex of the alert macaque monkey</article-title>. <source>Journal of neurophysiology</source>. <year>2006</year>;<volume>96</volume>(<issue>6</issue>):<fpage>3323</fpage>‚Äì<lpage>3337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00392.2006" xlink:type="simple">10.1152/jn.00392.2006</ext-link></comment> <object-id pub-id-type="pmid">16943318</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <chapter-title>Distributed Cortical Representation of Sound Locations</chapter-title>. In: <source>Perspectives on Auditory Research</source>. <publisher-name>Springer</publisher-name>; <year>2014</year>. p. <fpage>361</fpage>‚Äì<lpage>378</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Fishbach</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Farkas</surname> <given-names>D</given-names></name>. <source>Primary auditory cortex of cats: feature detection or something else? Biological cybernetics</source>. <year>2003</year>;<volume>89</volume>(<issue>5</issue>):<fpage>397</fpage>‚Äì<lpage>406</lpage>. <object-id pub-id-type="pmid">14669020</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chechik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Auditory abstraction from spectro-temporal features to coding auditory entities</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;<volume>109</volume>(<issue>46</issue>):<fpage>18968</fpage>‚Äì<lpage>18973</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1111242109" xlink:type="simple">10.1073/pnas.1111242109</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Unraveling the principles of auditory cortical processing: can we learn from the visual system?</article-title> <source>Nature neuroscience</source>. <year>2009</year>;<volume>12</volume>(<issue>6</issue>):<fpage>698</fpage>‚Äì<lpage>701</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2308" xlink:type="simple">10.1038/nn.2308</ext-link></comment> <object-id pub-id-type="pmid">19471268</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Attneave</surname> <given-names>F</given-names></name>. <article-title>Some informational aspects of visual perception</article-title>. <source>Psychological review</source>. <year>1954</year>;<volume>61</volume>(<issue>3</issue>):<fpage>183</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0054663" xlink:type="simple">10.1037/h0054663</ext-link></comment> <object-id pub-id-type="pmid">13167245</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="other">Barlow HB. Possible principles underlying the transformation of sensory messages. Sensory communication. 1961;p. 217‚Äì234.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>‚Äì<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding of sensory inputs</article-title>. <source>Current opinion in neurobiology</source>. <year>2004</year>;<volume>14</volume>(<issue>4</issue>):<fpage>481</fpage>‚Äì<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2004.07.007" xlink:type="simple">10.1016/j.conb.2004.07.007</ext-link></comment> <object-id pub-id-type="pmid">15321069</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chechik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bar-Yosef</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Tishby</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Reduction of information redundancy in the ascending auditory pathway</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>51</volume>(<issue>3</issue>):<fpage>359</fpage>‚Äì<lpage>368</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.06.030" xlink:type="simple">10.1016/j.neuron.2006.06.030</ext-link></comment> <object-id pub-id-type="pmid">16880130</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Natural image statistics and neural representation</article-title>. <source>Annual review of neuroscience</source>. <year>2001</year>;<volume>24</volume>(<issue>1</issue>):<fpage>1193</fpage>‚Äì<lpage>1216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.24.1.1193" xlink:type="simple">10.1146/annurev.neuro.24.1.1193</ext-link></comment> <object-id pub-id-type="pmid">11520932</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Efficient coding of natural sounds</article-title>. <source>Nature neuroscience</source>. <year>2002</year>;<volume>5</volume>(<issue>4</issue>):<fpage>356</fpage>‚Äì<lpage>363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn831" xlink:type="simple">10.1038/nn831</ext-link></comment> <object-id pub-id-type="pmid">11896400</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Smith</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Efficient auditory coding</article-title>. <source>Nature</source>. <year>2006</year>;<volume>439</volume>(<issue>7079</issue>):<fpage>978</fpage>‚Äì<lpage>982</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04485" xlink:type="simple">10.1038/nature04485</ext-link></comment> <object-id pub-id-type="pmid">16495999</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stilp</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>M</given-names></name>. <article-title>Statistical structure of speech sound classes is congruent with cochlear nucleus response properties</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2013</year>;<volume>134</volume>(<issue>5</issue>):<fpage>4229</fpage>‚Äì<lpage>4229</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.4831534" xlink:type="simple">10.1121/1.4831534</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Carlson</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Ming</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>Sparse codes for speech predict spectrotemporal receptive fields in the inferior colliculus</article-title>. <source>PLoS computational biology</source>. <year>2012</year>;<volume>8</volume>(<issue>7</issue>):<fpage>e1002594</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002594" xlink:type="simple">10.1371/journal.pcbi.1002594</ext-link></comment> <object-id pub-id-type="pmid">22807665</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Klein</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>K√∂nig</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>K√∂rding</surname> <given-names>KP</given-names></name>. <article-title>Sparse spectrotemporal coding of sounds</article-title>. <source>EURASIP Journal on Applied Signal Processing</source>. <year>2003</year>;<volume>2003</volume>:<fpage>659</fpage>‚Äì<lpage>667</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1155/S1110865703303051" xlink:type="simple">10.1155/S1110865703303051</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="other">K√∂rding KP, K√∂nig P, Klein DJ. Learning of sparse auditory receptive fields. In: Proceedings of the International Joint Conference on Neural Networks (IJCNN). vol. 2; 2002. p. 1103‚Äì1108.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="other">Terashima H, Okada M. The topographic unsupervised learning of natural sounds in the auditory cortex. In: NIPS; 2012. p. 2321‚Äì2329.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Terashima</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hosoya</surname> <given-names>H</given-names></name>. <article-title>Sparse codes of harmonic natural sounds and their modulatory interactions</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2009</year>;<volume>20</volume>(<issue>4</issue>):<fpage>253</fpage>‚Äì<lpage>267</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/09548980903447751" xlink:type="simple">10.3109/09548980903447751</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Asari</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pearlmutter</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Zador</surname> <given-names>AM</given-names></name>. <article-title>Sparse representations for the cocktail party problem</article-title>. <source>The Journal of neuroscience</source>. <year>2006</year>;<volume>26</volume>(<issue>28</issue>):<fpage>7477</fpage>‚Äì<lpage>7490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1563-06.2006" xlink:type="simple">10.1523/JNEUROSCI.1563-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16837596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>M≈Çynarski</surname> <given-names>W</given-names></name>. <article-title>Efficient coding of spectrotemporal binaural sounds leads to emergence of the auditory space representation</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2014</year>;<volume>8</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grothe</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pecka</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McAlpine</surname> <given-names>D</given-names></name>. <article-title>Mechanisms of sound localization in mammals</article-title>. <source>Physiological Reviews</source>. <year>2010</year>;<volume>90</volume>(<issue>3</issue>):<fpage>983</fpage>‚Äì<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/physrev.00026.2009" xlink:type="simple">10.1152/physrev.00026.2009</ext-link></comment> <object-id pub-id-type="pmid">20664077</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Learning intermediate-level representations of form and motion from natural movies</article-title>. <source>Neural computation</source>. <year>2012</year>;<volume>24</volume>(<issue>4</issue>):<fpage>827</fpage>‚Äì<lpage>866</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00247" xlink:type="simple">10.1162/NECO_a_00247</ext-link></comment> <object-id pub-id-type="pmid">22168556</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hyv√§arinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hoyer</surname> <given-names>P</given-names></name>. <article-title>Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces</article-title>. <source>Neural Computation</source>. <year>2000</year>;<volume>12</volume>(<issue>7</issue>):<fpage>1705</fpage>‚Äì<lpage>1720</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976600300015312" xlink:type="simple">10.1162/089976600300015312</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="other">Wang J, Olshausen B, Ming V. A sparse subspace model of higher-level sound structure. COSYNE Proceedings. 2008;.</mixed-citation></ref>
<ref id="pcbi.1004294.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="other">M≈Çynarski W. Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity priors. arXiv preprint arXiv:13124695. 2013;.</mixed-citation></ref>
<ref id="pcbi.1004294.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tosic</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Frossard</surname> <given-names>P</given-names></name>. <article-title>Dictionary learning</article-title>. <source>Signal Processing Magazine, IEEE</source>. <year>2011</year>;<volume>28</volume>(<issue>2</issue>):<fpage>27</fpage>‚Äì<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/MSP.2010.939537" xlink:type="simple">10.1109/MSP.2010.939537</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="other">Abdallah SA, Plumbley MD. If the independent components of natural images are edges, what are the independent components of natural sounds. In: Proceedings of International Conference on Independent Component Analysis and Signal Separation (ICA2001); 2001. p. 534‚Äì539.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Strutt</surname> <given-names>JW</given-names></name>. <article-title>On our perception of sound direction</article-title>. <source>Philosophical Magazine</source>. <year>1907</year>;<volume>13</volume>:<fpage>214</fpage>‚Äì<lpage>232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/14786440709463595" xlink:type="simple">10.1080/14786440709463595</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Singh</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2003</year>;<volume>114</volume>:<fpage>3394</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.1624067" xlink:type="simple">10.1121/1.1624067</ext-link></comment> <object-id pub-id-type="pmid">14714819</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="other">E Bumbacher VM. Pitch-sensitive Components Emerge from Hierarchical Sparse Coding of Natural Sounds. ICPRAM. 2012;.</mixed-citation></ref>
<ref id="pcbi.1004294.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="other">Bhand M, Mudur R, Suresh B, Saxe A, Ng AY. Unsupervised learning models of primary cortical receptive fields and receptive field plasticity. In: Advances in neural information processing systems; 2011. p. 1971‚Äì1979.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Escab√≠</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>2002</year>;<volume>87</volume>(<issue>1</issue>):<fpage>516</fpage>‚Äì<lpage>527</lpage>. <object-id pub-id-type="pmid">11784767</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hsu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Woolley</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Fremouw</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>Modulation power and phase spectrum of natural sounds enhance neural encoding performed by single auditory neurons</article-title>. <source>The Journal of neuroscience</source>. <year>2004</year>;<volume>24</volume>(<issue>41</issue>):<fpage>9201</fpage>‚Äì<lpage>9211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2449-04.2004" xlink:type="simple">10.1523/JNEUROSCI.2449-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15483139</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Harper</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>McAlpine</surname> <given-names>D</given-names></name>. <article-title>Optimal neural population coding of an auditory spatial cue</article-title>. <source>Nature</source>. <year>2004</year>;<volume>430</volume>(<issue>7000</issue>):<fpage>682</fpage>‚Äì<lpage>686</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature02768" xlink:type="simple">10.1038/nature02768</ext-link></comment> <object-id pub-id-type="pmid">15295602</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stecker</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Mickey</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Macpherson</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Spatial sensitivity in field PAF of cat auditory cortex</article-title>. <source>Journal of neurophysiology</source>. <year>2003</year>;<volume>89</volume>(<issue>6</issue>):<fpage>2889</fpage>‚Äì<lpage>2903</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00980.2002" xlink:type="simple">10.1152/jn.00980.2002</ext-link></comment> <object-id pub-id-type="pmid">12611946</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Butts</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Goldman</surname> <given-names>MS</given-names></name>. <article-title>Tuning curves, neuronal variability, and sensory coding</article-title>. <source>PLoS biology</source>. <year>2006</year>;<volume>4</volume>(<issue>4</issue>):<fpage>e92</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0040092" xlink:type="simple">10.1371/journal.pbio.0040092</ext-link></comment> <object-id pub-id-type="pmid">16529529</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gordon</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Shackleton</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Responses of neurons in the inferior colliculus to binaural disparities: insights from the use of Fisher information and mutual information</article-title>. <source>Journal of neuroscience methods</source>. <year>2008</year>;<volume>169</volume>(<issue>2</issue>):<fpage>391</fpage>‚Äì<lpage>404</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jneumeth.2007.11.005" xlink:type="simple">10.1016/j.jneumeth.2007.11.005</ext-link></comment> <object-id pub-id-type="pmid">18093660</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>. <article-title>Mutual information, Fisher information, and population coding</article-title>. <source>Neural Computation</source>. <year>1998</year>;<volume>10</volume>(<issue>7</issue>):<fpage>1731</fpage>‚Äì<lpage>1757</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976698300017115" xlink:type="simple">10.1162/089976698300017115</ext-link></comment> <object-id pub-id-type="pmid">9744895</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Parise</surname> <given-names>CV</given-names></name>, <name name-style="western"><surname>Knorre</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>. <article-title>Natural auditory scene statistics shapes human spatial hearing</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>16</issue>):<fpage>6104</fpage>‚Äì<lpage>6108</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1322705111" xlink:type="simple">10.1073/pnas.1322705111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Schnupp</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>A</given-names></name>. <source>Auditory neuroscience: Making sense of sound</source>. <publisher-name>MIT Press</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004294.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell receptive fields</article-title>. <source>PLoS computational biology</source>. <year>2011</year>;<volume>7</volume>(<issue>10</issue>):<fpage>e1002250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002250" xlink:type="simple">10.1371/journal.pcbi.1002250</ext-link></comment> <object-id pub-id-type="pmid">22046123</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>King</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Zylberberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>. <article-title>Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of V1</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>13</issue>):<fpage>5475</fpage>‚Äì<lpage>5485</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4188-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4188-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23536063</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Savin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Joshi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Triesch</surname> <given-names>J</given-names></name>. <article-title>Independent component analysis in spiking neurons</article-title>. <source>PLoS computational biology</source>. <year>2010</year>;<volume>6</volume>(<issue>4</issue>):<fpage>e1000757</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000757" xlink:type="simple">10.1371/journal.pcbi.1000757</ext-link></comment> <object-id pub-id-type="pmid">20421937</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Woods</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Lopez</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Rahman</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Recanzone</surname> <given-names>GH</given-names></name>. <article-title>Effects of stimulus azimuth and intensity on the single-neuron activity in the auditory cortex of the alert macaque monkey</article-title>. <source>Journal of neurophysiology</source>. <year>2006</year>;<volume>96</volume>(<issue>6</issue>):<fpage>3323</fpage>‚Äì<lpage>3337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00392.2006" xlink:type="simple">10.1152/jn.00392.2006</ext-link></comment> <object-id pub-id-type="pmid">16943318</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Recanzone</surname> <given-names>GH</given-names></name>. <article-title>Populations of auditory cortical neurons can accurately encode acoustic space across stimulus intensity</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year>;<volume>106</volume>(<issue>14</issue>):<fpage>5931</fpage>‚Äì<lpage>5935</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0901023106" xlink:type="simple">10.1073/pnas.0901023106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barlow</surname> <given-names>H</given-names></name>. <article-title>Redundancy reduction revisited</article-title>. <source>Network: computation in neural systems</source>. <year>2001</year>;<volume>12</volume>(<issue>3</issue>):<fpage>241</fpage>‚Äì<lpage>253</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/net.12.3.241.253" xlink:type="simple">10.1080/net.12.3.241.253</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>. <article-title>Unsupervised learning</article-title>. <source>Neural computation</source>. <year>1989</year>;<volume>1</volume>(<issue>3</issue>):<fpage>295</fpage>‚Äì<lpage>311</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1989.1.3.295" xlink:type="simple">10.1162/neco.1989.1.3.295</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>M≈Çynarski</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Jost</surname> <given-names>J</given-names></name>. <article-title>Statistics of Natural Binaural Sounds</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>10</issue>):<fpage>e108968</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0108968" xlink:type="simple">10.1371/journal.pone.0108968</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>DeWeese</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wehr</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zador</surname> <given-names>AM</given-names></name>. <article-title>Binary spiking in auditory cortex</article-title>. <source>The Journal of neuroscience</source>. <year>2003</year>;<volume>23</volume>(<issue>21</issue>):<fpage>7940</fpage>‚Äì<lpage>7949</lpage>. <object-id pub-id-type="pmid">12944525</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004294.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="other">Berkes P, Turner R, Sahani M. On sparsity and overcompleteness in image models. In: Advances in neural information processing systems; 2008. p. 89‚Äì96.</mixed-citation>
</ref>
</ref-list>
</back>
</article>