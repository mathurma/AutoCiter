<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">08-PLCB-RA-0287R5</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000336</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Natural Image Coding in V1: How Much Use Is Orientation Selectivity?</article-title><alt-title alt-title-type="running-head">Orientation Selective Coding of Natural Images</alt-title></title-group><contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Eichhorn</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Sinz</surname><given-names>Fabian</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Bethge</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1">          <addr-line>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Zhaoping</surname><given-names>Li</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">mbethge@tuebingen.mpg.de</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: MB. Performed the experiments: JE FS. Analyzed the data: JE FS. Contributed reagents/materials/analysis tools: JE FS MB. Wrote the paper: JE FS MB.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>4</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>3</day><month>4</month><year>2009</year></pub-date><volume>5</volume><issue>4</issue><elocation-id>e1000336</elocation-id><history>
<date date-type="received"><day>21</day><month>4</month><year>2008</year></date>
<date date-type="accepted"><day>18</day><month>2</month><year>2009</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2009</copyright-year><copyright-holder>Eichhorn et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Orientation selectivity is the most striking feature of simple cell coding in V1 that has been shown to emerge from the reduction of higher-order correlations in natural images in a large variety of statistical image models. The most parsimonious one among these models is linear Independent Component Analysis (ICA), whereas second-order decorrelation transformations such as Principal Component Analysis (PCA) do not yield oriented filters. Because of this finding, it has been suggested that the emergence of orientation selectivity may be explained by higher-order redundancy reduction. To assess the tenability of this hypothesis, it is an important empirical question how much more redundancy can be removed with ICA in comparison to PCA or other second-order decorrelation methods. Although some previous studies have concluded that the amount of higher-order correlation in natural images is generally insignificant, other studies reported an extra gain for ICA of more than 100%. A consistent conclusion about the role of higher-order correlations in natural images can be reached only by the development of reliable quantitative evaluation methods. Here, we present a very careful and comprehensive analysis using three evaluation criteria related to redundancy reduction: In addition to the multi-information and the average log-loss, we compute complete rate–distortion curves for ICA in comparison with PCA. Without exception, we find that the advantage of the ICA filters is small. At the same time, we show that a simple spherically symmetric distribution with only two parameters can fit the data significantly better than the probabilistic model underlying ICA. This finding suggests that, although the amount of higher-order correlation in natural images can in fact be significant, the feature of orientation selectivity does not yield a large contribution to redundancy reduction within the linear filter bank models of V1 simple cells.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Since the Nobel Prize winning work of Hubel and Wiesel it has been known that orientation selectivity is an important feature of simple cells in the primary visual cortex. The standard description of this stage of visual processing is that of a linear filter bank where each neuron responds to an oriented edge at a certain location within the visual field. From a vision scientist's point of view, we would like to understand why an orientation selective filter bank provides a <italic>useful</italic> image representation. Several previous studies have shown that orientation selectivity arises when the individual filter shapes are optimized according to the statistics of natural images. Here, we investigate quantitatively how critical the feature of orientation selectivity is for this optimization. We find that there is a large range of non-oriented filter shapes that perform nearly as well as the optimal orientation selective filters. We conclude that the standard filter bank model is not suitable to reveal a strong link between orientation selectivity and the statistics of natural images. Thus, to understand the role of orientation selectivity in the primary visual cortex, we will have to develop more sophisticated, nonlinear models of natural images.</p>
</abstract><funding-group><funding-statement>This study was financially supported by the German Ministry of Education, Science, Research and Technology through the Bernstein award (BMBF; FKZ: 01GQ0601) and a scholarship by the German National Academic Foundation. The design and conduct of the study was independent of the ministry.</funding-statement></funding-group><counts><page-count count="16"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>It is a long standing hypothesis that neural representations in sensory systems are adapted to the statistical regularities of the environment <xref ref-type="bibr" rid="pcbi.1000336-Attneave1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Barlow1">[2]</xref>. Despite widespread agreement that neural processing in the early visual system must be influenced by the statistics of natural images, there are many different viewpoints on how to precisely formulate the computational goal the system is trying to achieve. At the same time, different goals might be achieved by the same optimization criterion or learning principle. Redundancy reduction <xref ref-type="bibr" rid="pcbi.1000336-Barlow1">[2]</xref>, the most prominent example of such a principle, can be beneficial in various ways: it can help to maximize the information to be sent through a channel of limited capacity <xref ref-type="bibr" rid="pcbi.1000336-Linsker1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Atick1">[4]</xref>, it can be used to learn the statistics of the input <xref ref-type="bibr" rid="pcbi.1000336-Barlow2">[5]</xref> or to facilitate pattern recognition <xref ref-type="bibr" rid="pcbi.1000336-Watanabe1">[6]</xref>.</p>
<p>Besides redundancy reduction, a variety of other interesting criteria such as <italic>sparseness</italic> <xref ref-type="bibr" rid="pcbi.1000336-Fldik1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Olshausen1">[8]</xref>, <italic>temporal coherence</italic> <xref ref-type="bibr" rid="pcbi.1000336-Fldiak1">[9]</xref>, <italic>predictive information</italic> <xref ref-type="bibr" rid="pcbi.1000336-Bialek1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Becker1">[11]</xref> , or <italic>bottom-up saliency</italic> <xref ref-type="bibr" rid="pcbi.1000336-Zhaoping1">[12]</xref> have been formulated. An important commonality among all these ideas is the tight link to density estimation of the input signal.</p>
<p>At the level of primary visual cortex there is a large increase in the number of neurons. Hence, at this stage the idea of redundancy reduction cannot be motivated by a need for compression. However, the redundancy reduction principle is not limited to be useful for compression only. More generally, it can be interpreted as a special form of density estimation where the goal is to model the statistics of the input by finding a mapping which transforms the data into a representation with statistically independent coefficients <xref ref-type="bibr" rid="pcbi.1000336-Barlow2">[5]</xref>. In statistics, this idea is known as projection pursuit density estimation <xref ref-type="bibr" rid="pcbi.1000336-Friedman1">[13]</xref> where density estimation is carried out by optimizing over a set of possible transformations in order to match the statistics of the transformed signal as good as possible to a pre-specified target distribution. Once the distribution has been matched, applying the inverse transformation effectively yields a density model for the original data. From a neurobiological point of view, we may think of the neural response properties as an implementation of such transformations. Accordingly, we here think of redundancy reduction mainly in terms of projection pursuit density estimation.</p>
<p>A crucial aspect of this kind of approach is the class of transformations over which to optimize. From a statistician's point of view it is important to choose a regularized function space in order to avoid overfitting. On the other hand, if the class of possible transformations is too restricted, it may be impossible to find a good match to the target distribution. From a visual neuroscientist's point of view, the choice of transformations should be related to the class of possible computations in the early visual system. Here we assume the simplest case of linear transformations, optionally followed by a pointwise nonlinearity.</p>
<p>Intriguingly, a number of response properties of visual neurons have been reproduced by optimizing over the class of linear transformations on natural images for redundancy reduction (for a review see <xref ref-type="bibr" rid="pcbi.1000336-Zhaoping1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Simoncelli1">[14]</xref>). For instance, Buchsbaum and Gottschalk as well as Ruderman et al. revealed a link between the second-order statistics of color images and opponent color coding of retinal ganglion cells by demonstrating that decorrelating natural images in the trichromatic color space with Principal Component Analysis (PCA) yields the luminance, the red-green, and the blue-yellow channel <xref ref-type="bibr" rid="pcbi.1000336-Buchsbaum1">[15]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Ruderman1">[16]</xref>. Atick and Redlich derived the center-surround receptive fields by optimizing a symmetric decorrelation transformation <xref ref-type="bibr" rid="pcbi.1000336-Atick2">[17]</xref>. Later, also spatio-temporal correlations in natural images or sequences of natural images were linked to the receptive field properties in the retina and the lateral geniculate nucleus (LGN) <xref ref-type="bibr" rid="pcbi.1000336-vanHateren1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Dan1">[20]</xref>.</p>
<p>On the way from LGN to primary visual cortex, orientation selectivity emerges as a striking new receptive field property. A number of researchers (e.g., <xref ref-type="bibr" rid="pcbi.1000336-Hancock1">[21]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Li1">[22]</xref>) have used the covariance properties of natural images to derive linear basis functions that exhibit similar properties. Decorrelation alone, however, was not sufficient to achieve this goal. Rather, additional constraints were necessary, such as spatial locality or symmetry.</p>
<p>It was not until the reduction of higher-order correlations were taken into account that the derivation of localized and oriented band-pass filters—resembling orientation selective receptive fields in V1— was achieved without the necessity to assume any further constraints. Those filters were derived with Independent Component Analysis (ICA), a generalization of Principal Component Analysis (PCA), which aims at reducing higher-order correlations as well <xref ref-type="bibr" rid="pcbi.1000336-Olshausen1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Bell1">[23]</xref>.</p>
<p>This finding suggests that within the linear filter model, orientation selectivity can serve as a further mechanism for redundancy reduction. The tenability of this hypothesis can be tested by measuring how large the advantage of orientation selective filters is over non-oriented filter shapes. The importance of such a <italic>quantitative</italic> assessment has first been pointed out by Li and Atick <xref ref-type="bibr" rid="pcbi.1000336-Li1">[22]</xref> and are the main focus of several publications <xref ref-type="bibr" rid="pcbi.1000336-Zhaoping1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1000336-Li1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Chandler1">[29]</xref>. Generally speaking, two different approaches have been taken in the past: In the first approach, nonparametric methods such as histograms or nearest neighbor statistics have been used with the goal to estimate the total redundancy of natural images <xref ref-type="bibr" rid="pcbi.1000336-Li1">[22]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Petrov1">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Chandler1">[29]</xref>. While this approach seeks to answer the more difficult question how large the total redundancy of natural images is, the second approach compares the importance of orientation selectivity for redundancy reduction only within the class of models that are commonly used to describe V1 simple cell responses <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref>.</p>
<p>Using histogram estimators, Zhaoping and coworkers <xref ref-type="bibr" rid="pcbi.1000336-Li1">[22]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Petrov1">[27]</xref> argued that the contribution of higher-order correlations to the redundancy of natural images is five times smaller than the amount of second-order correlations. They concluded that this amount is so small that higher-order redundancy minimization is unlikely to be the main principle in shaping the cortical receptive fields.</p>
<p>Two objections may be raised against this conclusion: First, it is not clear how generally valid the result of <xref ref-type="bibr" rid="pcbi.1000336-Petrov1">[27]</xref> is. The study relies on the assumption that higher-order dependencies at distances beyond three pixels are negligible. More recent work based on nearest neighbor methods <xref ref-type="bibr" rid="pcbi.1000336-Chandler1">[29]</xref>, however, finds a substantially larger amount of higher-order correlations when taking dependencies over longer distances into account. Secondly, even if the contribution of higher-order correlation was only 20% of the amount of second-order correlations, this contribution is not necessarily negligible. Several previous studies report that the redundancy reduction achieved with ICA for gray level images is at the same level at about 20% <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>. Taken together these two findings suggest that orientation selective ICA filters can account for virtually all higher-order correlations in natural images. If this was true, it would strongly support the idea that redundancy reduction could be the main principle in shaping the cortical receptive fields.</p>
<p>In general, however, density estimation in high dimensions is a hard problem and the results reported in the literature do not fit into a consistent view. Therefore, the crucial challenge is to control for all technical issues in order to allow for safe conclusions about the effect of orientation selectivity on redundancy reduction. Here, we address many such issues that have not been addressed before. In our study, we take the second approach and focus on “linear redundancy reduction”—the removal of statistical dependencies that can be achieved by linear filtering. While most studies have been carried out for gray level images the two studies on color images find the advantage of ICA over PCA to be many times larger for color images than for gray level images with an improvement of more than 100% <xref ref-type="bibr" rid="pcbi.1000336-Wachtler1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>. Since it is not clear how to explain the large difference between color and gray value images, we reinvestigate the comparison between the orientation selective ICA filters and the PCA filters for color images using the same data set as in <xref ref-type="bibr" rid="pcbi.1000336-Wachtler1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>.</p>
<p>Our goal is to establish a reliable reference against which more sophisticated image models can be compared to in the future. We elaborate on our own previous work <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> by optimizing the ICA algorithm for the multi-information estimators used in the comparison. Additionally, we now test the advantage of the resulting orientation selective ICA filters comprehensively with three different types of analyses that are related to the notion of redundancy reduction, density estimation, and coding efficiency: (A) multi-information reduction, (B) average log-likelihood, and (C) rate-distortion curves.</p>
<p>Our results show that orientation selective ICA filters do not excel in any of these measures: We find that the gain of ICA in redundancy reduction over a random decorrelation method is only about 3% for color and gray-value images. In terms of rate-distortion curves, ICA performs even worse than PCA. Furthermore, we demonstrate that a simple spherically symmetric model with only two parameters fits the filter responses significantly better than a model that assumes marginal independence . Since in this model the specific shape of the filters is ignored, we conclude that it is unlikely that orientation selectivity plays a critical role for redundancy reduction even if the class of transformations is extended to include contrast gain control mechanisms <xref ref-type="bibr" rid="pcbi.1000336-Sinz1">[30]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lyu1">[31]</xref>. While many of the previous studies do not provide enough detail in order to explain their different outcomes, we provide our code and the dataset online (<ext-link ext-link-type="uri" xlink:href="http://www.kyb.tuebingen.mpg.de/bethge/code/QICA/" xlink:type="simple">http://www.kyb.tuebingen.mpg.de/bethge/code/QICA/</ext-link>) in order to ensure the reproducibility and verifiability of our results.</p>
</sec><sec id="s2">
<title>Materials and Methods</title>
<p>An important difficulty in setting up a quantitative comparison originates from the fact that it bears several issues that may be critical for the results. In particular, choices have to be made regarding the <italic>evaluation criteria</italic>, the <italic>image data</italic>, the <italic>estimation methods</italic>, which <italic>linear transformations</italic> to include in the comparison, and which <italic>particular implementation of ICA</italic> to use. The significance of the outcome of the comparison will depend on how careful these choices have been made. The most relevant issues will be addressed in the following.</p>
<sec id="s2a">
<title>Notation and Nomenclature</title>
<p>For both, color and gray-value data, we write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e001" xlink:type="simple"/></inline-formula> to refer to single vectors which contain the raw pixel intensities. Vectors are indicated by bold font while the same letter in normal font with a subindex denotes one of its components. Vectors without subindices usually denote random variables, while subindices indicate specific examples. In some cases it is convenient to define the corresponding data matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e002" xlink:type="simple"/></inline-formula> which holds single images patches in its columns. The letter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e003" xlink:type="simple"/></inline-formula> denotes the number of examples in the dataset, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e004" xlink:type="simple"/></inline-formula> is used for the dimension of a single data point.</p>
<p>Transformations are denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e005" xlink:type="simple"/></inline-formula>, oftentimes with a subindex to distinguish different types. The result of a transformation to either a vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e006" xlink:type="simple"/></inline-formula> or a data matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e007" xlink:type="simple"/></inline-formula> will be written as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e008" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e009" xlink:type="simple"/></inline-formula>, respectively.</p>
<p>Probability densities are denoted with the letters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e010" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e011" xlink:type="simple"/></inline-formula>, sometimes with a subindex to indicate differences between distributions whenever it seems necessary for clarity. In general, we use the hat symbol to distinguish between true entities and their empirical estimates. For instance, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e012" xlink:type="simple"/></inline-formula> is the true probability density of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e013" xlink:type="simple"/></inline-formula> after applying a fixed transformation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e014" xlink:type="simple"/></inline-formula>, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e015" xlink:type="simple"/></inline-formula> refers to the corresponding empirical estimate.</p>
<p>A distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e016" xlink:type="simple"/></inline-formula> is called <italic>factorial</italic>, or <italic>marginally independent</italic>, if it can be written as a product of its marginals, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e017" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e018" xlink:type="simple"/></inline-formula> is obtained by integrating <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e019" xlink:type="simple"/></inline-formula> over all components but <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e020" xlink:type="simple"/></inline-formula>.</p>
<p>Finally, the expectation over some entity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e021" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e022" xlink:type="simple"/></inline-formula> is written as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e023" xlink:type="simple"/></inline-formula>. Sometimes, we use the density instead of the random variable in the subindex to indicate the distribution, over which the expectation is taken. If there is no risk for confusion we drop the subindex. Just as above, the empirical expectation is marked with a hat symbol, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e024" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2b">
<title>How to Compare Early Vision Models?</title>
<p>A principal complicacy in low-level vision is the lack of a clearly defined task. Therefore, it is difficult to compare different image representations as it is not obvious <italic>a priori</italic> what measure should be used.</p>
<sec id="s2b1">
<title>Multi-information</title>
<p>The first measure we consider is the <italic>multi-information</italic> <xref ref-type="bibr" rid="pcbi.1000336-Perez1">[32]</xref>, which is the original objective function that is minimized by ICA over the choice of filters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e025" xlink:type="simple"/></inline-formula>. The multi-information assesses the total amount of statistical dependencies between the components <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e026" xlink:type="simple"/></inline-formula> of a filtered patch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e027" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e028" xlink:type="simple"/><label>(1)</label></disp-formula>The terms <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e029" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e030" xlink:type="simple"/></inline-formula> denote the marginal and the joint entropies of the true distribution, respectively. The <italic>Kullback-Leibler-Divergence</italic> or <italic>Relative Entropy</italic><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e031" xlink:type="simple"/></disp-formula>is an information theoretic dissimilarity measure between two distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e033" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000336-Cover1">[33]</xref>. It is always non-negative and zero if and only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e034" xlink:type="simple"/></inline-formula> equals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e035" xlink:type="simple"/></inline-formula>. If the redundancy reduction hypothesis is taken literally, the multi-information is the right measure to minimize, since it measures how close to factorial the true distribution of the image patches in the representation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e036" xlink:type="simple"/></inline-formula> really is.</p>
<p>The application of linear ICA algorithms to ensembles of natural images reliably yields transformations consisting of localized and oriented bandpass filters similar to the receptive fields of neurons in V1. It is less clear, however, whether these filter properties also critical to the minimization of the multi-information? In order to assess the tenability of the idea that a V1 simple cell is adjusted to the purpose of redundancy reduction, it is important to know whether such a tuning can—<italic>in principle</italic>—result in a large reduction of the multi-information. One way to address this question is to measure <italic>how much</italic> more the multi-information is actually reduced by the ICA filters in comparison to others such as PCA filters. This approach has been taken in <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref>.</p>
<p>One problem with estimating multi-information is that it involves the joint entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e037" xlink:type="simple"/></inline-formula> of the true distribution which is generally hard to estimate. In certain cases, however, the problem can be bypassed by evaluating the difference in the multi-information between two representations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e039" xlink:type="simple"/></inline-formula>. In particular, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e040" xlink:type="simple"/></inline-formula> is related to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e041" xlink:type="simple"/></inline-formula> by the linear transformation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e042" xlink:type="simple"/></inline-formula> it follows from definition (1) and the transformation theorem for probability densities<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e043" xlink:type="simple"/></disp-formula>that difference in multi-information can be expressed as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e044" xlink:type="simple"/></disp-formula>For convenience, we chose a volume-conserving gauge <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> where all linear decorrelation transforms are of determinant one, and hence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e045" xlink:type="simple"/></inline-formula>. This means that differences in multi-information are equal to differences of marginal entropies which can be estimated robustly. Thus, our empirical estimates of the multi-information differences are given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e046" xlink:type="simple"/><label>(2)</label></disp-formula>For estimating the entropy of the univariate marginal distributions, we employ the OPT estimator introduced in <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> which uses the exponential power family to fit the marginal distributions by OPTimizing over the shape parameter. This estimator has been shown to give highly reliable results for natural images. In particular, it is much more robust than entropy estimators based on the sample kurtosis which easily overestimate the multi-information.</p>
</sec><sec id="s2b2">
<title>Average log loss (ALL)</title>
<p>As mentioned earlier, redundancy reduction can be interpreted as a special form of density estimation where the goal is to find a mapping which transforms the data into a representation with statistically independent coefficients. This means that any given transformation specifies a density model over the data. Our second measure, the average log-loss (ALL), evaluates the agreement of this density model with the actual distribution of the data:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e047" xlink:type="simple"/><label>(3)</label></disp-formula>The average log-loss is a principled measure quantifying how different the model density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e048" xlink:type="simple"/></inline-formula> is from the true density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e049" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000336-Bernardo1">[34]</xref>. Since the KL-divergence is positive and zero if and only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e050" xlink:type="simple"/></inline-formula> the ALL is minimal only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e051" xlink:type="simple"/></inline-formula> matches the true density. Furthermore, differences in the average log-loss correspond to differences in the coding cost (i.e., information rate) in the case of sufficiently fine quantization. For natural images, different image representations have been compared with respect to this measure in <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>.</p>
<p>For the estimation of the average log-loss, we compute the empirical average<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e052" xlink:type="simple"/><label>(4)</label></disp-formula>This estimator is equivalent to the first method in Lewicki et al. <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lewicki2">[35]</xref> apart from an extra term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e053" xlink:type="simple"/></inline-formula> in their defining equation. This extra term is only necessary if one aims at relating the result to a discrete entropy obtained for a particular bin width <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e054" xlink:type="simple"/></inline-formula>.</p>
<p>While the empirical average in Eq. 4 in principle can be prone to overfitting, we control for this risk by evaluating all estimates on an independent test set, whose data has not been used during the parameter fit. Furthermore, we compare the average log-loss to the parametric entropy estimates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e055" xlink:type="simple"/></inline-formula> that we use in (A) for estimating the multi-information changes (see Eq. 2). The difference between both quantities has been named <italic>differential log-likelihood</italic> <xref ref-type="bibr" rid="pcbi.1000336-Hulle1">[36]</xref> and can be used to assess the goodness of fit of a model distribution:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e056" xlink:type="simple"/></disp-formula></p>
<p>The shape of the parametric model is well matched to the actual distribution if the differential log-likelihood converges to zero with increasing number of data points.</p>
</sec><sec id="s2b3">
<title>Rate-distortion curves</title>
<p>Finally, we consider <italic>efficient coding</italic> or <italic>minimum mean square error reconstruction</italic> as a third objective. In contrast to the previous objectives, it is now assumed that there is some limitation of the amount of information that can be transmitted, and the goal is to maximize the amount of <italic>relevant</italic> information transmitted about the image. In the context of neural coding, the redundancy reduction hypothesis has oftentimes been motivated in terms of coding efficiency. In fact, instead of minimizing the multi-information one can equivalently ask for the linear transformation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e057" xlink:type="simple"/></inline-formula> which maximizes the mutual information between its input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e058" xlink:type="simple"/></inline-formula> and its output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e059" xlink:type="simple"/></inline-formula> when additive noise <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e060" xlink:type="simple"/></inline-formula> is added to the output <xref ref-type="bibr" rid="pcbi.1000336-Linsker1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Nadal1">[37]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Bell2">[38]</xref>. It is important to note, however, that this minimalist approach of “information maximization” is ignorant with respect to how useful or <italic>relevant</italic> the information is that has been transmitted <xref ref-type="bibr" rid="pcbi.1000336-Simoncelli1">[14]</xref>.</p>
<p>For natural images, the source signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e061" xlink:type="simple"/></inline-formula> is a continuous random variable which requires infinitely many bits to be specified with unlimited precision. In reality, however, the precision is always limited so that only a finite amount of bits can be represented. Both, the multi-information and the average log-loss do not take into account the problem what information should be encoded and what information can be discarded. Therefore, it is interesting to compare the redundancy reduction of the linear transforms with respect to the relevant image information (while the irrelevant information can be discarded anyway). To this end, we here resort to the framework of linear transform coding as it has been developed in the field of image compression <xref ref-type="bibr" rid="pcbi.1000336-Goyal1">[39]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Gray1">[40]</xref>, and which constitutes the theoretical foundation of the JPEG standard.</p>
<p>It is clear that at the level of V1 the number of neurons, encoding the retinal image, is substantially larger than the number of fibers in the optic nerve. Therefore, it is not the need for compression that makes rate distortion theory interesting at this stage. However, Barlow's redundancy reduction hypothesis must not be equated with compression. In more recent work, Barlow introduced the term ‘redundancy exploitation’ instead of ‘redundancy reduction’ in order to avoid this misunderstanding <xref ref-type="bibr" rid="pcbi.1000336-Barlow3">[41]</xref>. But also if we think in terms of density estimation rather than compression, it is still important to take into account that not all possible changes in the image pixels may be of equal importance for inferring the content of an image. Therefore, we here want to combine the notion of redundancy reduction with a measure for the quality with which the image can be reconstructed from the information that is preserved by the representation. Following Lewicki and coworkers (method 2 in <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lewicki2">[35]</xref>) we will consider the mean squared error reconstruction that can be achieved at a certain quantization level of the transformed representation. This objective is in fact very much related to the task of image compression.</p>
<p>Clearly, we expect that the criteria for judging image compression algorithms may not provide a good proxy to an accurate judgement of what information is considered relevant in a biological vision system. In particular, the existence of selective attention suggests that different aspects of image information are transmitted at different times depending on the behavioral goals and circumstances <xref ref-type="bibr" rid="pcbi.1000336-Zhaoping1">[12]</xref>. That is, a biological organism can change the relevance criteria dynamically on demand while for still image compression algorithms it is rather necessary that this assessment is made once and forever in a fixed and static fashion.</p>
<p>These issues are outside the scope of this paper. Instead we follow the common path in the past to use the mean squared reconstruction error for the pixel intensities. This is the measure of choice for high-rate still image compression <xref ref-type="bibr" rid="pcbi.1000336-Wang1">[42]</xref>. In particular, it is common to report on the performance of a code by determining its rate–distortion curve which specifies the required information rate for a given reconstruction error (and vice versa) <xref ref-type="bibr" rid="pcbi.1000336-Gray1">[40]</xref>. Consequently, we will ask for a given information rate, how do the image representations compare with respect to the reconstruction error. As result, we will obtain a so-called rate–distortion curve which displays the average reconstruction error as a function of the information rate or vice versa. The second method used in <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lewicki2">[35]</xref> is an estimate of a single point on this curve for a particular fixed value of the reconstruction error.</p>
<p>The estimation of the rate–distortion curve is clearly the most difficult task among the three criteria. The framework of transform coding <xref ref-type="bibr" rid="pcbi.1000336-Goyal1">[39]</xref>, which is extensively used in still image compression, makes several simplifying assumptions that allow one to obtain a clear picture. The encoding task is divided into two steps: First, the image patches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e062" xlink:type="simple"/></inline-formula> are linearly transformed into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e063" xlink:type="simple"/></inline-formula>. Then the coefficients <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e064" xlink:type="simple"/></inline-formula> are quantized independently of each other. Using this framework, we can ask whether the use of an ICA image transformation leads to a smaller reconstruction error after coefficient quantization than PCA or any other transform.</p>
<p>As for quantizing the coefficients, we resort to the framework of variable rate entropy coding <xref ref-type="bibr" rid="pcbi.1000336-Gray2">[43]</xref>. In particular, we apply uniform quantization, which is close to optimal for high-rate compression <xref ref-type="bibr" rid="pcbi.1000336-Goyal1">[39]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Gish1">[44]</xref>. For uniform quantization, it is only required to specify the bin width of the coefficients. There is also the possibility to use a different number of quantization levels for the different coefficients. The question of how to set these numbers is known as the ‘bit allocation problem’ because the amount of bits needed to encode one coefficient will depend monotonically on the number of quantization levels. The number of quantization levels can be adjusted in two different but equivalent ways: One possibility is to use a different bin width for each individual coefficient. Alternatively, it is also possible to use the same bin width for all coefficients and multiply all coefficients with an appropriate scale factor before quantization. The larger the variance of an individual coefficient, the more bits will be allocated to represent it.</p>
<p>Here, we will employ the latter approach, for which the bit allocation problem becomes an inherent part of the transformation: Any bit allocation scheme can be obtained via post-multiplication with a diagonal matrix. Thus, in contrast to the objective function of ICA, the rate–distortion criterion is not invariant against post-multiplication with a diagonal matrix. For ICA and PCA, we will determine the rate–distortion curve for both, normalized output variances (“white ICA” and “white PCA”) and normalized basis functions (“normalized ICA” and “orthonormal PCA”), respectively.</p>
</sec></sec><sec id="s2c">
<title>Decorrelation Transforms</title>
<p>The particular shape of the ICA basis functions is obtained by minimization of the multi-information over all invertible linear transforms <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e065" xlink:type="simple"/></inline-formula>. In contrast, the removal of second-order correlations alone generally does not yield localized, oriented, and bandpass image basis functions. ICA additionally removes higher-order correlations which are generated by linear mixing. In order to assess the importance of this type of higher-order correlations for redundancy reduction and coding efficiency we will compare ICA to other decorrelating image bases.</p>
<p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e066" xlink:type="simple"/></inline-formula> be the covariance matrix of the data and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e067" xlink:type="simple"/></inline-formula> its eigen-decomposition. Then, any linear second-order decorrelation transform can be written as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e068" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e069" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e070" xlink:type="simple"/></inline-formula> are defined as above, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e071" xlink:type="simple"/></inline-formula> is an arbitrary orthogonal matrix and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e072" xlink:type="simple"/></inline-formula> is an arbitrary diagonal matrix. It is easily verified that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e073" xlink:type="simple"/></inline-formula> has diagonal covariance for all choices of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e074" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e075" xlink:type="simple"/></inline-formula>, i.e., all second-order correlations vanish. This means that any particular choice of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e076" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e077" xlink:type="simple"/></inline-formula> determines a specific decorrelation transform. Based on this observation we introduce a number of linear transformations for later reference. All matrices are square and are chosen to be of determinant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e078" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e079" xlink:type="simple"/></inline-formula> is the number of columns (or rows) of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e080" xlink:type="simple"/></inline-formula> (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e081" xlink:type="simple"/></inline-formula> is the geometrical mean of the eigenvalues <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e082" xlink:type="simple"/></inline-formula>).</p>
<sec id="s2c1">
<title>Orthogonal principal component analysis (oPCA)</title>
<p>If the variances of the principle components (i.e., the diagonal elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e083" xlink:type="simple"/></inline-formula>) are all different, PCA is the only metric-preserving decorrelation transform and is heavily used in digital image coding. It corresponds to choosing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e084" xlink:type="simple"/></inline-formula> as the identity matrix and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e085" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e086" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c2">
<title>White principal component analysis (wPCA)</title>
<p>Equalizing the output variances in the PCA representation sets the stage for the derivation of further decorrelation transforms different from PCA. In order to assess the effect of variance equalization for coding efficiency, we also include this “white PCA” representation into our analysis: Choose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e087" xlink:type="simple"/></inline-formula> as for orthonormal PCA and then set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e088" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e089" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e090" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c3">
<title>Symmetric whitening (SYM)</title>
<p>Among the non-orthogonal decorrelation transforms, symmetric whitening stays as close to the input representation as possible (in Frobenius norm) <xref ref-type="bibr" rid="pcbi.1000336-Fan1">[45]</xref>. In terms of early vision this may be seen as an implementation of a wiring length minimization principle. Remarkably, the basis functions of symmetric whitening resemble the center-surround shape of retinal ganglion cell receptive fields when applied to the pixel representation of natural images <xref ref-type="bibr" rid="pcbi.1000336-Atick2">[17]</xref>. The symmetric whitening transform is obtained by setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e091" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e092" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e093" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c4">
<title>Random whitening (RND)</title>
<p>As a baseline which neither exploits a special structure with respect to the input representation nor makes use of higher-order correlations we also consider a completely random transformation. To obtain a random orthogonal matrix we first draw a random matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e094" xlink:type="simple"/></inline-formula> from a Gaussian matrix-variate distribution and then we set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e095" xlink:type="simple"/></inline-formula>. With <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e096" xlink:type="simple"/></inline-formula> we obtain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e097" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c5">
<title>White independent component analysis (wICA)</title>
<p>Finally, ICA is the transformation which has been suggested to explain the orientation selectivity of V1 simple cells <xref ref-type="bibr" rid="pcbi.1000336-Olshausen1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Bell1">[23]</xref>. Set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e098" xlink:type="simple"/></inline-formula> for which the multi-information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e099" xlink:type="simple"/></inline-formula> takes a minimum. With <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e100" xlink:type="simple"/></inline-formula> we obtain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e101" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c6">
<title>Normalized independent component analysis (nICA)</title>
<p>Normalized independent component analysis (nICA) differs from white ICA (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e102" xlink:type="simple"/></inline-formula>) only by a different choice of the second diagonal matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e103" xlink:type="simple"/></inline-formula>. Instead of having equal variance in each coefficient, we now choose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e104" xlink:type="simple"/></inline-formula> such that the corresponding basis vector of each coefficient has the same length in pixel space. It is easy to see that our first two criteria, the multi-information and the negative log-likelihood, are invariant under changes in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e105" xlink:type="simple"/></inline-formula>. It makes a difference for the rate–distortion curves as in our setup the variance (or, more precisely, the standard deviation) determines the bit allocation. Practically, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e106" xlink:type="simple"/></inline-formula> can be determined by using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e107" xlink:type="simple"/></inline-formula> as follows: First, we compute the matrix inverse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e108" xlink:type="simple"/></inline-formula> and determine the Euclidean norm <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e109" xlink:type="simple"/></inline-formula> of the column vectors of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e110" xlink:type="simple"/></inline-formula>. With <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e111" xlink:type="simple"/></inline-formula>, we then obtain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e112" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s2d">
<title>ICA Algorithm</title>
<p>If the true joint probability distribution is known, the minimization of the multi-information over all linear transformations can be formulated without any assumptions about the shape of the distribution. In practice, the multi-information has to be estimated from a finite amount of data which requires to make assumptions about the underlying density.</p>
<p>There are many different ICA algorithms which differ in the assumptions made and also in the optimization technique employed. The choice of the particular ICA algorithm used here was guided by a set of requirements that arise from the specific problem setting. Although a wide variety of ICA algorithms has been published, none of them fits exactly all of our requirements.</p>
<p>We would like to use an ICA algorithm, which gives the ICA image basis the best chance for the comparison with other image representations. For the comparison of the multi-information reduction, we are using the OPT estimator introduced in <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> which has been found to give the most reliable results. This estimator employs a parametric estimate of the coefficient distributions based on the exponential power family which is known to provide an excellent fit to the coefficient distributions of natural images <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Srivastava1">[46]</xref>. Our ICA algorithm should make the same assumptions about the data as we make for the final comparison of the multi-information reduction. Therefore, we are also using the exponential power family model for the marginal densities during the minimization of the multi-information. In addition, we want to have an ICA basis which is indistinguishable from the other image representations with respect to the second-order statistics. Therefore, we are using a pre-whitened ICA algorithm, whose search space is restricted to the subgroup of orthogonal matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e113" xlink:type="simple"/></inline-formula>. One of the most efficient ICA methods in the public domain specialized to pre-whitened ICA is FastICA <xref ref-type="bibr" rid="pcbi.1000336-Hyvrinen1">[47]</xref>. We use this fixed-point algorithm as an initialization. Subsequently, the solution is further refined by performing a gradient ascent over the manifold of orthogonal matrices on the likelihood of the data, when each marginal is modelled by a the exponential power distribution as in the case of the OPT estimator.</p>
<p>In order to optimize the objective function over the subspace of orthogonal matrices, we adapted the algorithms for Stiefel manifolds proposed by Edelman et al. <xref ref-type="bibr" rid="pcbi.1000336-Edelman1">[48]</xref> to the simpler case of orthogonal groups and combined it with the line-search routine dbrent from <xref ref-type="bibr" rid="pcbi.1000336-Press1">[49]</xref> to achieve a rather straightforward gradient descent algorithm. For the initialization with FastICA, we use the Gaussian non-linearity, the symmetric approach and a tolerance level of 10<sup>−5</sup>.</p>
</sec><sec id="s2e">
<title>Spherically Symmetric Model</title>
<p>A well known result by Maxwell <xref ref-type="bibr" rid="pcbi.1000336-Maxwell1">[50]</xref> states that the only factorial distribution invariant against arbitrary orthogonal transformations is the isotropic Gaussian distribution. Natural images exhibit marginals which are significantly more peaked than Gaussian. Nevertheless, their distribution does share the spherical symmetry with the Gaussian as already found by <xref ref-type="bibr" rid="pcbi.1000336-Zetzsche1">[51]</xref> for gabor filter pairs and lately exploited by <xref ref-type="bibr" rid="pcbi.1000336-Lyu1">[31]</xref> for nonlinear image representations. Therefore, it makes sense to compare the performance of the ICA model with a spherically symmetric model of the whitened data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e114" xlink:type="simple"/></inline-formula>. Note that any spherically symmetric model is still invariant under orthogonal transformations while only the Gaussian additionally exhibits marginal independence.</p>
<p>While the radial distribution of a Gaussian (i.e., the distribution over the lengths of the random vectors) is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e115" xlink:type="simple"/></inline-formula>, whose shape and scale parameter is determined by the number of dimensions and the variance, respectively, the spherical symmetric model may be seen as a generalization of the Gaussian, for which the radial distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e116" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e117" xlink:type="simple"/></inline-formula> can be of arbitrary shape. The density of the spherically symmetric distribution (SSD) is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e118" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e119" xlink:type="simple"/></inline-formula> is the surface area of a sphere in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e120" xlink:type="simple"/></inline-formula> with radius <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e121" xlink:type="simple"/></inline-formula>. For simplicity we will model the radial distribution with a member of the Gamma family<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e122" xlink:type="simple"/><label>(6)</label></disp-formula>with shape parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e123" xlink:type="simple"/></inline-formula> and scale parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e124" xlink:type="simple"/></inline-formula>, which can be easily matched to the mean and variance of the empirical distribution via <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e125" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e126" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2f">
<title>Dataset</title>
<p>The difference in the performance between ICA and other linear transformations clearly depends on the data. For <italic>gray-scale</italic> images we observed in our previous study <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> that the difference in the multi-information between ICA and any other decorrelation transform is consistently smaller than 5%. In particular, we controlled for the use of different pictures and for the effect of different pre-processing steps.</p>
<p>Here, we resort to the dataset used in a previous study <xref ref-type="bibr" rid="pcbi.1000336-Wachtler1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>, which among all previous studies reported the largest advantage of ICA compared to PCA. This <italic>color</italic> image dataset is based on the Bristol Hyperspectral Images Database <xref ref-type="bibr" rid="pcbi.1000336-Brelstaff1">[52]</xref> that contains multi-spectral recordings of natural scenes taken in the surroundings of Bristol, UK and in the greenhouses of Bristol Botanical Gardens. The authors of <xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref> kindly provided to us a pre-processed version of the image data where spectral radiance vectors were already converted into LMS values. During subsequent processing the reflectance standard was cut out and images were converted to log intensities <xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>.</p>
<p>All images come at a resolution of 256×256 pixels. From each image circa 5000 patches of size 7×7 pixels were drawn at random locations (circa 40000 patches in total). For chromatic images with three color channels (LMS) each patch is reshaped as a 7×7×3 = 147-dimensional vector. To estimate the contribution of color information, a comparison with monochromatic images was performed where gray-value intensities were computed as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e127" xlink:type="simple"/></inline-formula> and exactly the same patches were used for analysis. In the latter case, the dimensionality of a data sample is thus reduced to 49 dimensions. All experiments are carried out over ten different training and test sets sampled independently from the original images.</p>
<p>Our motivation to chose 7×7 patches is to keep the same setting as in <xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref> for the sake of comparability. As this patch size is rather small, we performed the same analysis for patch sizes of 15×15 as well. All results in the paper refer to the case of 7×7 image patches. The results for 15×15 can be found in the supplementary material (<xref ref-type="supplementary-material" rid="pcbi.1000336.s001">Text S1</xref>).</p>
<p>The statistics of the average illumation in the image patches, the DC component, differs significantly from image to image. Therefore, we first separated the DC component from the patches before further transforming them. In order to leave the entropy of the data unaffected, we used an orthogonal transformation. The projector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e128" xlink:type="simple"/></inline-formula> is computed such that the first (for each color channel) component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e129" xlink:type="simple"/></inline-formula> corresponds to the DC component(s) of that patch. One such a possible choice is the matrix<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e130" xlink:type="simple"/></disp-formula>However, this is not an orthogonal transformation. Therefore, we decompose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e131" xlink:type="simple"/></inline-formula> into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e132" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e133" xlink:type="simple"/></inline-formula> is upper triangular and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e134" xlink:type="simple"/></inline-formula> is an orthogonal transform. Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e135" xlink:type="simple"/></inline-formula>, the first column of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e136" xlink:type="simple"/></inline-formula> must be a multiple of the vector with all coefficients equal to one (due to the upper triangluarity of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e137" xlink:type="simple"/></inline-formula>). Therefore, the first component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e138" xlink:type="simple"/></inline-formula> is a multiple of the DC component. Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e139" xlink:type="simple"/></inline-formula> is an orthonomal transform, using all but the first row of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e140" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e141" xlink:type="simple"/></inline-formula> projects out the DC component. In the case of color images <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e142" xlink:type="simple"/></inline-formula> becomes a block-diagonal matrix with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e143" xlink:type="simple"/></inline-formula> as diagonal elements for each channel.</p>
<p>By removing the DC component in that manner, all linear transformations are applied in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e144" xlink:type="simple"/></inline-formula> dimensions, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e145" xlink:type="simple"/></inline-formula> denotes the number of pixels in the original image patch. In this case the marginal entropy of the DC-components has to be included in the computation of the multi-information in order to ensure a valid comparison with the original pixel basis. We use the same estimators as in <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> to estimate the marginal entropy of DC-component.</p>
</sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Filter Shapes</title>
<p>As in previous studies <xref ref-type="bibr" rid="pcbi.1000336-Olshausen1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Bell1">[23]</xref> the filters derived with ICA exhibited orientation selective tuning properties similar to those observed for V1 simple cells (see <xref ref-type="fig" rid="pcbi-1000336-g001">Figure 1</xref>). For illustration, we also show the basis functions learned with PCA and RND in <xref ref-type="fig" rid="pcbi-1000336-g001">Figure 1</xref>. The basis functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e146" xlink:type="simple"/></inline-formula> are obtained by inverting the filter matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e147" xlink:type="simple"/></inline-formula> (including the DC component). The result is displayed in the upper panel (<xref ref-type="fig" rid="pcbi-1000336-g001">Figure 1A–C</xref>). Following common practice, we also visualize the basis functions after symmetric whitening (<xref ref-type="fig" rid="pcbi-1000336-g001">Figure 1D–F</xref>).</p>
<fig id="pcbi-1000336-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g001</object-id><label>Figure 1</label><caption>
<title>Examples for Receptive Fields of Various Image Transforms.</title>
<p>Basis functions of a random decorrelation transform (RND), principal component analysis (PCA) and independent component analysis (ICA) in pixel space (A–C) and whitened space (E–F). The image representation in whitened space is obtained by left multiplication with the matrix square root of the inverse covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e148" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g001" xlink:type="simple"/></fig>
<p>The basis functions of both PCA and ICA exhibit color opponent coding but the basis functions of ICA are additionally localized and orientation selective. The basis functions of the random decorrelation transform does not exhibit any regular structure besides the fact that they are bandpass. The following quantitative comparisons will show, however, that the distinct shape of the ICA basis functions does not yield a clear advantage for redundancy reduction and coding efficiency.</p>
</sec><sec id="s3b">
<title>Multi-Information</title>
<p>The multi-information is the original objective function that is minimized by ICA over all possible linear decorrelation transforms. <xref ref-type="fig" rid="pcbi-1000336-g002">Figure 2</xref> shows the reduction in multi-information achieved with different decorrelation transforms including ICA for chromatic and gray value images, respectively. For each representation, the results are reported in bits per component, i.e., as marginal entropies averaged over all dimensions:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e149" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<fig id="pcbi-1000336-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g002</object-id><label>Figure 2</label><caption>
<title>Multi-Information Reduction per Dimension.</title>
<p>Average differential entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e150" xlink:type="simple"/></inline-formula> for the pixel basis (PIX), after separation of the DC component (DCS), and after application of the different decorrelation transforms. The difference between PIX and RND corresponds to the redundancy reduction that is achieved with a random second-order decorrelation transform. The small difference between RND and ICA is the maximal amount of higher-order redundancy reduction that can be achieved by ICA. Diagram (A) shows the results for chromatic images and diagram (B) for gray value images. For both types of images, only a marginal amount can be accounted to the reduction of higher order dependencies.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g002" xlink:type="simple"/></fig>
<p><xref ref-type="table" rid="pcbi-1000336-t001">Table 1</xref> shows the corresponding values for the transformations RND, SYM, PCA and ICA. For both chromatic images and gray-value intensities, the lowest and highest reduction is achieved with RND or ICA, respectively. However, the additional gain in the multi-information reduction achieved with ICA on top of RND constitutes only 3.20% for chromatic images and 2.39% for achromatic in comparison with the total reduction relative to the pixel basis (PIX). This means that only a small fraction of redundancy reduction can actually be accounted to the removal of higher-order redundancies with ICA.</p>
<table-wrap id="pcbi-1000336-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.t001</object-id><label>Table 1</label><caption>
<title>Comparision of the Multi-Information Reduction for Chromatic and Achromatic Images.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000336-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="3" rowspan="1">Absolute Difference</td>
<td align="left" colspan="3" rowspan="1">Relative Difference</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Color</td>
<td align="left" colspan="1" rowspan="1">Gray</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Color</td>
<td align="left" colspan="1" rowspan="1">Gray</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">RND-PIX</td>
<td align="left" colspan="1" rowspan="1">−4.0694±0.0043</td>
<td align="left" colspan="1" rowspan="1">−3.1252±0.0043</td>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1"/>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SYM-RND</td>
<td align="left" colspan="1" rowspan="1">−0.0593±0.0004</td>
<td align="left" colspan="1" rowspan="1">−0.0259±0.0006</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e151" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">1.44±0.01</td>
<td align="left" colspan="1" rowspan="1">0.82±0.02</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PCA-RND</td>
<td align="left" colspan="1" rowspan="1">−0.0627±0.0008</td>
<td align="left" colspan="1" rowspan="1">−0.0353±0.0011</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e152" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">1.52±0.02</td>
<td align="left" colspan="1" rowspan="1">1.12±0.03</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ICA-RND</td>
<td align="left" colspan="1" rowspan="1">−0.1345±0.0008</td>
<td align="left" colspan="1" rowspan="1">−0.0767±0.0008</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e153" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">3.20±0.02</td>
<td align="left" colspan="1" rowspan="1">2.39±0.02</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><p>Differences in the multi-information reduction between various decorrelation transforms (SYM, PCA, ICA) relative to a random decorrelation transform (RND) compared to the multi-information reduction achieved with the random decorrelation transform relative to the original pixel basis (RND-PIX). The <italic>absolute</italic> multi-information reduction is given in bits/component on the left hand side. The right hand side shows how much more the special decorrelation transforms SYM, PCA and ICA can reduce the multi-information <italic>relative</italic> to the random (RND) one.</p></fn></table-wrap-foot></table-wrap>
<p>One may argue that the relatively small patch size of 7×7 pixel may be responsible for the small advantage of ICA as all decorrelation functions already getting the benefit of localization. In order to address the question how the patch size affects the linear redundancy reduction, we repeated the same analysis on a whole range of different patch sizes. <xref ref-type="fig" rid="pcbi-1000336-g003">Figure 3</xref> shows the multi-information reduction with respect to the pixel representation (PIX) achieved by the transformations RND and ICA. The achievable reduction quickly saturates with increasing patch size such that its value for 7×7 image patches is already at about 90% of its asymptote. In particular, one can see that the relative advantage of ICA over other transformations is still small (∼3%) also for large patch sizes. All Tables and Figures for patch size 15×15 can be found in the additional material (<xref ref-type="supplementary-material" rid="pcbi.1000336.s001">Text S1</xref>).</p>
<fig id="pcbi-1000336-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g003</object-id><label>Figure 3</label><caption>
<title>Redundancy Reduction as a Function of Patch Size.</title>
<p>The graph shows the multi-information reduction achieved by the transformations RND and ICA for chromatic (A) and achromatic images (B). The gain quickly saturates with increasing patch size such that its value for 7×7 image patches is already at about 90% of its asymptote. This demonstrates that the advantage of ICA over other transformations does not increase with increasing patch size.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g003" xlink:type="simple"/></fig></sec><sec id="s3c">
<title>Average Log-Loss</title>
<p>Since redundancy reduction can also be interpreted as a special form of density estimation we also look at the average log-loss which quantifies how well the underlying density model of the different transformations is matched to the statistics of the data. <xref ref-type="table" rid="pcbi-1000336-t002">Table 2</xref> shows the average log-loss (ALL) and <xref ref-type="table" rid="pcbi-1000336-t003">Table 3</xref> the differential log-likelihood (DLL) in bits per component. For the average log-loss, ICA achieved an ALL of 1.78 bits per component for chromatic images and 1.85 bits per component for achromatic images. Compared to the ALL in the RND representation of 1.9 bits and 1.94 bits, respectively, the gain achieved by ICA is again small. Additionally, the ALL values were very close to the differential entropies, resulting in small DLL values. This confirms that the exponential power distribution fits the shape of the individual marginal coefficient distributions well. Therefore, we can safely conclude that the advantage of ICA is small not only in terms of redundancy reduction as measured by the multi-information, but also in the sense of density estimation.</p>
<table-wrap id="pcbi-1000336-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.t002</object-id><label>Table 2</label><caption>
<title>Average Log-Loss (ALL) for Chromatic and Achromatic Images.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000336-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Color</td>
<td align="left" colspan="1" rowspan="1">Gray</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">ALL</td>
<td align="left" colspan="1" rowspan="1">ALL</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">RND</td>
<td align="left" colspan="1" rowspan="1">1.9486±0.0035</td>
<td align="left" colspan="1" rowspan="1">1.9414±0.0044</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SYM-RND</td>
<td align="left" colspan="1" rowspan="1">−0.0881±0.0004</td>
<td align="left" colspan="1" rowspan="1">−0.0402±0.0005</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PCA-RND</td>
<td align="left" colspan="1" rowspan="1">−0.0751±0.0009</td>
<td align="left" colspan="1" rowspan="1">−0.0391±0.0011</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ICA-RND</td>
<td align="left" colspan="1" rowspan="1">−0.1637±0.0007</td>
<td align="left" colspan="1" rowspan="1">−0.0880±0.0007</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SSD-RND</td>
<td align="left" colspan="1" rowspan="1">−0.2761±0.0025</td>
<td align="left" colspan="1" rowspan="1">−0.2868±0.0032</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt102"><p>The first row shows the average log-loss (ALL, in bits/component) of the density model determined by the linear transformation RND. The value was obtained by averaging over 10 separately sampled training and test sets of size 40.000 and 50.000, respectively. The following rows show the difference of the ALL of the models SYM, PCA, ICA and of the spherically symmetric density (SSD) to the ALL of the RND model. The smaller average log-loss of the SSD model compared to the ICA model fundamentally contradicts the assumptions underlying the ICA model.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1000336-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.t003</object-id><label>Table 3</label><caption>
<title>Differential Log-Likelihood (DLL) for Chromatic and Achromatic Images.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000336-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.t003" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="2" rowspan="1">Color</td>
<td align="left" colspan="2" rowspan="1">Gray</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">DLL</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e154" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">DLL</td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e155" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">RND</td>
<td align="left" colspan="1" rowspan="1">−0.0113±0.0007</td>
<td align="left" colspan="1" rowspan="1">1.0413±0.0026</td>
<td align="left" colspan="1" rowspan="1">−0.0057±0.0006</td>
<td align="left" colspan="1" rowspan="1">1.0132±0.0046</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SYM</td>
<td align="left" colspan="1" rowspan="1">−0.0388±0.0009</td>
<td align="left" colspan="1" rowspan="1">0.8961±0.0021</td>
<td align="left" colspan="1" rowspan="1">−0.0195±0.0009</td>
<td align="left" colspan="1" rowspan="1">0.9486±0.0040</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PCA</td>
<td align="left" colspan="1" rowspan="1">−0.0224±0.0007</td>
<td align="left" colspan="1" rowspan="1">0.9145±0.0024</td>
<td align="left" colspan="1" rowspan="1">−0.0087±0.0007</td>
<td align="left" colspan="1" rowspan="1">0.9425±0.0025</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ICA</td>
<td align="left" colspan="1" rowspan="1">−0.0378±0.0009</td>
<td align="left" colspan="1" rowspan="1">0.7687±0.0017</td>
<td align="left" colspan="1" rowspan="1">−0.0154±0.0011</td>
<td align="left" colspan="1" rowspan="1">0.8434±0.0025</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt103"><p>The small DLL values suggest, that the exponential power distribution fits the shape of the individual coefficient distributions well. In addition, we also report the average exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e156" xlink:type="simple"/></inline-formula> of the exponential power family fit to the individual coefficient distributions (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e157" xlink:type="simple"/></inline-formula> corresponds to a Laplacian shape).</p></fn></table-wrap-foot></table-wrap><sec id="s3c1">
<title>Comparison to a Spherical Symmetric Model</title>
<p>The fact that ICA fits the distribution of natural images only marginally better than a random decorrelation transform implies that the generative model underlying ICA does not apply to natural images. In order to assess the importance of the actual filter shape, we fitted a spherically symmetric model to the filter responses. The likelihood of such a model is invariant under post-multiplication of an orthogonal matrix, i.e., the actual shape of the filter. Therefore, a good fit of such a model provides strong evidence against a critical role of certain filter shapes.</p>
<p>As shown in <xref ref-type="table" rid="pcbi-1000336-t002">Table 2</xref>, the ALL of the SSD model is 1.67 bits per component for chromatic images and 1.65 bits per component for achromatic images. This is significantly smaller than the ALL of ICA indicating that it fits the distribution of natural images much better than ICA does. This result is particularly striking if one compares the number of parameters fitted in the ICA model compared to the SSD case: After whitening, the optimization in ICA is done over the manifold of orthogonal matrices which has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e158" xlink:type="simple"/></inline-formula> free parameters (where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e159" xlink:type="simple"/></inline-formula> denotes the number of dimensions without the DC components). The additional optimization of the shape parameters for the exponential power family fitted to each individual component adds another <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e160" xlink:type="simple"/></inline-formula> parameters. For the case of 7×7 color image patches we thus have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e161" xlink:type="simple"/></inline-formula> parameters. In stark contrast, there are only two free parameters in the SSD model with a radial Gamma distribution, the shape parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e162" xlink:type="simple"/></inline-formula> and the scale parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e163" xlink:type="simple"/></inline-formula>. Nevertheless, for chromatic images the gain of the SSD model relative to random whitening is almost twice as large as that of ICA and even three and a half times as large for achromatic images.</p>
<p>Since the SSD model is completely independent of the choice of the orthogonal transformation after whitening, its superior performance compared with ICA provides a very strong argument against the hypothesis that orientation selectivity plays a critical role for redundancy reduction. In addition, it is also corroborates earlier arguments that has been given to show that the statistics of natural images does not conform to the generative model underlying ICA <xref ref-type="bibr" rid="pcbi.1000336-Zetzsche1">[51]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Baddeley1">[53]</xref>.</p>
<p>Besides the better fit of the data by the SSD model, there is also a more direct way of demonstrating the dependencies of the ICA coefficients: If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e164" xlink:type="simple"/></inline-formula> is data in the wICA representation, then the independence assumption of ICA can be simulated by applying independent random permutations to the rows of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e165" xlink:type="simple"/></inline-formula>. Certainly, such a shuffling procedure does not alter the histograms of the individual coefficients but it is suited to destroy potential statistical dependencies among the coefficients. Subsequently, we can transform the shuffled data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e166" xlink:type="simple"/></inline-formula> back to the RND basis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e167" xlink:type="simple"/></inline-formula>. If the ICA coefficients were independent, the shuffling procedure would not alter the joint statistics, and hence, one should find no difference in the multi-information between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e168" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e169" xlink:type="simple"/></inline-formula>. But infact, we observe a large discrepancy between the two (<xref ref-type="fig" rid="pcbi-1000336-g004">Figure 4</xref>). The distributions of the sRND coefficients were very close to Gaussians and the average marginal entropy of sRND yielded <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e170" xlink:type="simple"/></inline-formula> bits in contrast to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e171" xlink:type="simple"/></inline-formula> bits. In other words, the finding that for natural images the marginals of a random decorrelation transform have Laplacian shape (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e172" xlink:type="simple"/></inline-formula>) stands in clear contradiction to the generative model underlying ICA. If the ICA model was valid, one would expect that the sum over the ICA coefficients would yield Gaussian marginals due to the central limit theorem. In conclusion, we have very strong evidence that the ICA coefficients are not independent in case of natural images.</p>
<fig id="pcbi-1000336-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g004</object-id><label>Figure 4</label><caption>
<title>The Distribution of Natural Images does not Conform with the Generative Model of ICA.</title>
<p>In order to test for statistical dependencies among the coefficients <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e173" xlink:type="simple"/></inline-formula> of whithened ICA for single data samples, the coefficients were shuffled among the data points along each dimension. Subsequently, we transform the resulting data matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e174" xlink:type="simple"/></inline-formula> into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e175" xlink:type="simple"/></inline-formula>. This corresponds to a change of basis from the ICA to the random decorrelation basis (RND). The plot shows the log-histogram over the coefficients over all dimensions. If the assumptions underlying ICA were correct, there would be no difference between the histogram of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e176" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e177" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g004" xlink:type="simple"/></fig></sec></sec><sec id="s3d">
<title>Rate-Distortion Curves</title>
<p>There are different ways to account for the limited precision that is imposed by neural noise and firing rate limitations. As mentioned above the advantage with respect to a plain information maximization criterion can equivalently be measured by the multi-information criterion considered above <xref ref-type="bibr" rid="pcbi.1000336-Nadal1">[37]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Nadal2">[54]</xref>. In order to additionally account for the question which representation optimally encodes the <italic>relevant</italic> image information, we also present rate distortion curves which show the minimal reconstruction error as a function of the information rate.</p>
<p>We compare the rate–distortion curves of wICA, nICA, wPCA and oPCA (see <xref ref-type="fig" rid="pcbi-1000336-g005">Figure 5</xref>). Despite the fact that ICA is optimal in terms of redundancy reduction (see <xref ref-type="table" rid="pcbi-1000336-t002">Table 2</xref>), oPCA performs optimal with respect to the rate-distortion trade-off. wPCA in turn performes worst and remarkably similar to wICA. Since wPCA and wICA differ only by an orthogonal transformation, both representations are bound to the same metric. oPCA is the only transformation which has the same metric as the pixel representation according to which the reconstruction error is determined. By normalizing the length of the ICA basis vectors in the pixel space, the metric of nICA becomes more similar to the pixel basis and the performance with respect to the rate-distortion trade-off improved considerably. Nevertheless, for a fixed reconstrucion error the discrete entropy after quantization in the oPCA basis is up to 1 bit/component <italic>smaller</italic> than for the corresponding nICA-basis.</p>
<fig id="pcbi-1000336-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g005</object-id><label>Figure 5</label><caption>
<title>Rate-distortion Curves.</title>
<p>Rate-distortion curve for PCA and ICA when equalizing the output variances (wPCA and wICA) and when equalizing the norm of the corresponding image bases in pixel space (oPCA and nICA). The plot shows the discrete entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e178" xlink:type="simple"/></inline-formula> in bits (averaged over all dimensions) against the log of the squared reconstruction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e179" xlink:type="simple"/></inline-formula>. oPCA outperforms all other transforms in terms of the rate-distortion trade-off. wPCA in turn performes worst and remarkably similar to wICA. Since wPCA and wICA differ only by an orthogonal transformation, both representations are bound to the same metric. oPCA is the only transformation which has the same metric as the pixel representation according to which the reconstruction error is determined. By normalizing the length of the ICA basis vectors in the pixel space, the metric of nICA becomes more similar to the pixel basis and the performance with respect to the rate-distortion trade-off can be seen to improve considerably.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g005" xlink:type="simple"/></fig>
<p>In order to understand this result more precisely, we analyzed how the quantization of the coefficients affects the two variables of the rate–distortion function, <italic>discrete entropy</italic> and <italic>reconstruction error</italic>.</p>
<p><xref ref-type="fig" rid="pcbi-1000336-g006">Figure 6</xref> shows an illustrative example in order to make the following analysis more intuitive. The example demonstrates that the quality of a transform code not only depends on the redundancy of the coefficients but also on the shape of the partition cells induced by the quantization. In particular, when the cells are small (i.e., the entropy rate is high), then the reconstruction error mainly depends on having cell shapes that minimize the average distance to the center of the cell. Linear transform codes can only produce partitions into parallelepipeds (<xref ref-type="fig" rid="pcbi-1000336-g006">Figure 6B</xref>). The best parallelepipeds are cubes (<xref ref-type="fig" rid="pcbi-1000336-g006">Figure 6A</xref>). This is why PCA yields the (close to) optimal trade-off between minimizing the redundancy <italic>and</italic> the distortion, as it is the only orthogonal transform that yields uncorrelated coefficients. For a more comprehensive introduction to transform coding we refer the reader to the excellent review by Goyal <xref ref-type="bibr" rid="pcbi.1000336-Goyal1">[39]</xref>.</p>
<fig id="pcbi-1000336-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g006</object-id><label>Figure 6</label><caption>
<title>The Partition Cell Shape is Crucial for the Quantization Error.</title>
<p>The quality of a source code depends on both the shapes of the partition cells and on how the sizes of the cells vary with respect to the source density. When the cells are small (i.e., the entropy rate is high), then, the quality mainly depends on having cell shapes that minimize the average distance to the center of the cell. For a given volume, a body in Euclidean space that minimizes the average distance to the center is a sphere. The best packings (including the hexagonal case) cannot be achieved with linear transform codes. Transform codes can only produce partitions into parallelepipeds, as shown here for two dimensions. The best parallelepipeds are cubes which are only obtained in the case of orthogonal transformations. Therefore PCA yields the (close to) optimal trade-off between minimizing the redundancy <italic>and</italic> the distortion as it is the only <italic>orthogonal</italic> decorrelation transform (see <xref ref-type="bibr" rid="pcbi.1000336-Goyal1">[39]</xref> for more details). The figure shows 50.000 samples from a bivariate Gaussian random variable. Plot (A) depicts a uniform binning (bin width <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e180" xlink:type="simple"/></inline-formula>, only some bin borders are shown) induced by the only orthogonal basis for which the coefficients <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e181" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e182" xlink:type="simple"/></inline-formula> are decorrelated. Plot (B) shows uniform binning in a decorrelated, but not orthogonal basis (indicated by the blue lines). Both cases have been chosen such that the multi-information between the coefficients is identical and the same entropy rate was used to encode the signal. However, due to the shape of the bins in plot (B) the total quadratic error increases from 0.4169 to 0.9866. The code for this example can be also downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.kyb.tuebingen.mpg.de/bethge/code/QICA/" xlink:type="simple">http://www.kyb.tuebingen.mpg.de/bethge/code/QICA/</ext-link>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g006" xlink:type="simple"/></fig><sec id="s3d1">
<title>Discrete entropy</title>
<p>Given a uniform binning of width <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e183" xlink:type="simple"/></inline-formula> the discrete entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e184" xlink:type="simple"/></inline-formula> of a probability density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e185" xlink:type="simple"/></inline-formula> is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e186" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e187" xlink:type="simple"/></inline-formula> denotes the interval defined by the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e188" xlink:type="simple"/></inline-formula> bin. For small bin-sizes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e189" xlink:type="simple"/></inline-formula>, there is a close relationship between <italic>discrete</italic> and <italic>differential</italic> entropy: Because of the mean value theorem we can approximate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e190" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e191" xlink:type="simple"/></inline-formula>, and hence<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e192" xlink:type="simple"/></disp-formula>Thus, we have the relationship <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e193" xlink:type="simple"/></inline-formula> for sufficiently small <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e194" xlink:type="simple"/></inline-formula> (i.e., high-rate quantization). In other words, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e195" xlink:type="simple"/></inline-formula> asymptotically grows linearly with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e196" xlink:type="simple"/></inline-formula>. Therefore, we can fit a linear function to the asymptotic branch of the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e197" xlink:type="simple"/></inline-formula> which is plotted in <xref ref-type="fig" rid="pcbi-1000336-g007">Figure 7A</xref> (more precisely we are plotting the average over all dimensions). If we take the ordinate intercept of the linear approximation, we obtain a nonparametric estimate of the differential entropy which can be compared to the entropy estimates reported above (Those estimates were determined with the OPT estimator). Equivalently, one can consider the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e198" xlink:type="simple"/></inline-formula> which gives a better visualization of the error of the linear approximation (<xref ref-type="fig" rid="pcbi-1000336-g007">Figure 7</xref>, left, dashed line). For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e199" xlink:type="simple"/></inline-formula> the differential entropy is obtained in the limit <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e200" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1000336-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g007</object-id><label>Figure 7</label><caption>
<title>Discrete vs. Differential Entropy.</title>
<p>(A) Relationship between discrete and differential entropy. Discrete entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e201" xlink:type="simple"/></inline-formula> averaged over all channels as a function of the negative log bin width. The straight lines constitute the linear approximation to the asymptotic branch of the function. Their interception with the y-axis are visualized by the gray shaded, horizontal lines. The dashed lines represent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e202" xlink:type="simple"/></inline-formula> which converge to the gray shaded lines for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e203" xlink:type="simple"/></inline-formula>. (B) There are only small differences in the average discrete entropy for oPCA, wPCA, wICA, nICA as a function of the negative log bin width. Since the discrete entropy of the DC component is the same for all transforms, it is not included in that average but plotted separately instead.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g007" xlink:type="simple"/></fig>
<p>This analysis shows that differences in differential entropy in fact translate into differences in discrete entropy after uniform quantization with sufficiently small bins. Accordingly, the minimization of the multi-information as proposed by the redundancy reduction hypothesis does in fact also minimize the discrete entropy of a uniformly quantized code. In particular, if we look at the discrete entropy of the four different transforms, oPCA, wPCA, wICA, nICA (<xref ref-type="fig" rid="pcbi-1000336-g007">Figure 7B</xref>), we find that asymptotically the two PCA transforms require slightly more entropy than the two ICA transforms, and there is no difference anymore between oPCA and wPCA or wICA and nICA. This close relationship between discrete and differential entropy for high-rate quantization, however, is not sufficient to determine the coding performance evaluated by the rate–distortion curve. The latter requires to compare also the reconstruction error for the given quantization.</p>
</sec><sec id="s3d2">
<title>Reconstruction error</title>
<p>The reconstruction error is defined as the mean squared distance in the pixel basis between the original image and the image obtained by reconstruction from the quantized coefficients of the considered transformation. For the reconstruction, we simply use the inverse of the considered transformation, which is optimal in the limit of high-rate quantization.</p>
<p>When looking at the reconstruction error as a function of the bin width (<xref ref-type="fig" rid="pcbi-1000336-g008">Figure 8</xref>) we can observe much more pronounced differences between the different transformations than it was the case for the entropy. As a consequence, the differences in the reconstruction error turn out to be much more important for the rate-distortion trade-off than the differences in the entropy. Only the two transformations with exactly the same metric, wPCA and wICA, exhibit no difference in the reconstruction error. This suggests that minimization of the multi-information is strictly related to efficient coding if and only if the transformation with respect to the pixel basis is orthogonal. As we have seen that the potential effect of higher-order redundancy reduction is rather small, we expect that the PCA transform constitutes a close approximation to the minimizer of the multi-information among all orthogonal transforms because PCA is the only orthogonal transform which removes all second-order correlations.</p>
<fig id="pcbi-1000336-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g008</object-id><label>Figure 8</label><caption>
<title>Reconstruction Error vs. Bin Width of Discrete Entropy.</title>
<p>Reconstruction error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e204" xlink:type="simple"/></inline-formula> as a function of the bin width <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e205" xlink:type="simple"/></inline-formula>, shown on a logarithmic scale. The differences between the different transforms are relatively large. Only the two transformations with exactly the same metric, wPCA and wICA, exhibit no difference in the reconstruction error.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g008" xlink:type="simple"/></fig></sec></sec></sec><sec id="s4">
<title>Discussion</title>
<p>The structural organization of orientation selectivity in the primary visual cortex has been associated with self-organization since the early seventies <xref ref-type="bibr" rid="pcbi.1000336-Malsburg1">[55]</xref>, and much progress has been made to narrow down the range of possible models compatible with the empirical findings (e.g., <xref ref-type="bibr" rid="pcbi.1000336-Kaschube1">[56]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Wimbauer1">[58]</xref>). The link to visual information processing, however, still remains elusive <xref ref-type="bibr" rid="pcbi.1000336-Horton1">[59]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Masland1">[61]</xref>.</p>
<p>More abstract unsupervised learning models which obtain orientation selective filters using sparse coding <xref ref-type="bibr" rid="pcbi.1000336-Olshausen1">[8]</xref> or ICA <xref ref-type="bibr" rid="pcbi.1000336-Bell1">[23]</xref> try to address this link between image processing and the self-organization of neural structure. In particular, these models not only seek to reproduce the orientation tuning properties of V1 simple cells but they additionally address the question of how the simple cell responses collectively can instantiate a representation for arbitrary images. Furthermore, these image representations are learned from an information theoretic principle assuming that the learned filters exhibit advantageous coding properties.</p>
<p>The goal of this study is to quantitatively test this assumption in the simple linear transform coding framework. To this end, we investigated three criteria, the multi-information—i.e., the objective function of ICA—the average log-loss, and rate-distortion curves. There are a number of previous studies which also aimed at quantifying how large the advantage of the orientation selective ICA filters is relative to second-order decorrelation transformations. In particular, four papers <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref>, are most closely related to this study as all of them compare the average log-loss of different transformations. However, they did not provide a coherent answer to the question how large the advantage of ICA is compared to other decorrelation transforms.</p>
<p>Lewicki and Olshausen <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref> found that their learned bases show a 15–20% improvement over traditional bases. However, their result cannot be used to compare second-order and higher-order redundancy reduction because the entire analysis is based on a dataset in which all images have been preprocessed with a bandpass filter as in olshausen:1996. Since bandpass filtering already removes a substantial fraction of second-order correlations in natural images, their study is likely to systematically underestimate the total amount of second-order correlations in natural images.</p>
<p>Lee et al. <xref ref-type="bibr" rid="pcbi.1000336-Wachtler1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref> reported an advantage of over 100% percent for ICA in the case of color images and a more moderate but substantial gain of about 20% for gray-value images. In order to avoid possible differences due to the choice of data set we here used exactly the same data as in <xref ref-type="bibr" rid="pcbi.1000336-Wachtler1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>. Very consistently, we find only a small advantage for ICA of less than five percent for both multi-information and the average log-loss. In particular, we are not able to reproduce the very large difference between color and gray-value images that they reported. Unfortunately, we cannot pinpoint where the differences in the numbers ultimately come from because it is not clear which estimation procedure was used in <xref ref-type="bibr" rid="pcbi.1000336-Wachtler1">[25]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Lee1">[26]</xref>.</p>
<p>The estimators used for the measurements in the present study have been shown previously to give correct results on artificial data <xref ref-type="bibr" rid="pcbi.1000336-Bethge1">[28]</xref> and we provide our code online for verification. Furthermore, Weiss and Freeman showed for an undirected probabilistic image model that whitening already yields 98% of the total performance <xref ref-type="bibr" rid="pcbi.1000336-Weiss1">[62]</xref>. Finally, the superior performance of the simple SSD model with only two free parameters provides a very strong explanation for why the gain achieved with ICA is so small relative to a random decorrelation transform: Since a spherically symmetric model is invariant under orthogonal transformations and provides a better fit to the data, the actual shape of the filter does not seem to be critical. It also shows that the fundamental assumption underlying ICA—the data are well described by a linear generative model with independent sources—is not justified in the case of natural images.</p>
<p>From all these results, we can safely conclude that the actual gain of ICA compared to PCA is smaller than 5% for both gray level images and color images.</p>
<sec id="s4a">
<title>Is Smaller Than 5% Really Small?</title>
<p>A valid question to ask is whether comparing the amount of higher-order correlations to the amount of second-order correlations is the right thing to do. Even if the amount of higher-order correlations may be small in comparison to the amount of second-order correlations, we still know that higher-order correlations can be a critical signature of the content of an image. For example, textures are very useful to demonstrate how changes in higher-order correlations can change the perceptual meaning of an image.</p>
<p>Our results on the rate-distortion trade-off can be taken as an indication that the fraction of higher-order correlations captured by ICA is perceptually less relevant. This interpretation is further corroborated by a psychophysical comparison of the <italic>perceptual</italic> redundancy of the ICA and the PCA basis <xref ref-type="bibr" rid="pcbi.1000336-Bethge2">[63]</xref>. Another confirmation of this interpretation can be obtained if we use the learned image representations as generative models. Perceptually image patches sampled from the ICA model do not look more similar to natural image patches than those sampled from the random decorrelation basis (<xref ref-type="fig" rid="pcbi-1000336-g009">Figure 9</xref>). Currently, we are running psychophysical experiments which also show quantitatively that there is no significant difference between the ICA model and the PCA model if the subjects have to discriminate between textures that are generated by these models.</p>
<fig id="pcbi-1000336-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000336.g009</object-id><label>Figure 9</label><caption>
<title>Comparison of Patches Sampled From Different Image Models.</title>
<p>The figure demonstrates that the perceptual similarity between samples from the ICA image model (C) and samples from natural images (B) is not significantly increased relative to the perceptual similarity between samples from the RND image model (A) and (B).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.g009" xlink:type="simple"/></fig>
<p>In summary, we were not able thus far to come up with a meaningful interpretation for which the improvement of ICA would be recognized as being large. On the basis of the present study it seems rather unlikely that such a measure can be found for linear ICA. Instead, we believe that more sophisticated, nonlinear image models are necessary to demonstrate a clear advantage of orientation selectivity.</p>
</sec><sec id="s4b">
<title>What about Nonparametric Approaches?</title>
<p>The focus on linear redundancy reduction models in this study is motivated by the goal to first establish a solid and reproducible result for the simplest possible case before moving on to more involved nonlinear transformations. Nevertheless, it is important to discuss what we can expect if the restriction to linear transformations is dropped. From a nonparametric analysis <xref ref-type="bibr" rid="pcbi.1000336-Petrov1">[27]</xref>, Petrov and Zhaoping concluded that higher-order correlations in general contribute only very little to the redundancy in natural images and, hence, are probably not the main cause for the receptive field properties in V1. The empirical support for this claim, however, is limited by the fact that their comparison is based on mutual information estimates within a very small neighborhood of five pixels only. This is problematic as it is known that many kinds of higher-order correlations in natural images become apparent only in much higher-dimensional statistics <xref ref-type="bibr" rid="pcbi.1000336-Bethge3">[64]</xref>. Furthermore, their estimate of the amount of second-order correlations is not invariant against pointwise nonlinear transformations of the pixel intensities.</p>
<p>In a more recent non-parametric study, Chandler and Field arrived at a very different result regarding the relative contribution of second-order and higher-order dependencies <xref ref-type="bibr" rid="pcbi.1000336-Chandler1">[29]</xref>. They use nearest-neighbor based methods to estimate the joint entropy of natural images in comparison to “spectrum-equalized” noise and white noise, where “spectrum-equalized” noise denotes Gaussian noise with exactly the same spectrum as that of natural images. As shown in Figure 18 of <xref ref-type="bibr" rid="pcbi.1000336-Chandler1">[29]</xref> they find a smaller difference between spectrum-equalized noise and white noise than between natural images and spectrum-equalized noise. Hence, from their finding, it seems that the amount of higher-order correlations in natural images is even larger than the amount of second-order correlations. Also this result has to be taken with care: Reliable non-parametric estimates in high-dimensions are difficult to obtain even if one resorts to nearest-neighbor based methods, and the estimate of the amount of second-order correlations in <xref ref-type="bibr" rid="pcbi.1000336-Chandler1">[29]</xref> is not invariant against pointwise nonlinear transformations of the pixel intensities, too.</p>
<p>In summary, the present nonparametric studies do not give a unique answer regarding the total amount of higher-order correlations in natural images. Since estimating the absolute amount of multi-information is an extremely difficult task in high dimensions, the differences in the results can easily originate from the different assumptions and approximations made in these studies. Consequently, it remains an open question how large the true total redundancy of natural images is. In any case, it is clear that there are many higher-order redundancies in natural images that play a crucial role for visual perception. No matter how large these redundancies are in comparison to the second-order correlations, we need to develop better image models that have the right structure to capture these regularities.</p>
</sec><sec id="s4c">
<title>What about Nonlinear Image Models?</title>
<p>Apart from the non-parametric approaches, a large number of nonlinear image models has been proposed over the years which are capable to capture significantly more statistical regularities of natural images than linear ICA can do (e.g., <xref ref-type="bibr" rid="pcbi.1000336-Weiss1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1000336-Wainwright1">[65]</xref>–<xref ref-type="bibr" rid="pcbi.1000336-Hammond1">[72]</xref>). In fact, Olshausen and Field <xref ref-type="bibr" rid="pcbi.1000336-Olshausen1">[8]</xref> already used a more general model than linear ICA when they originally derived the orientation selective filters from higher-order redundancy reduction. In contrast to plain ICA, they used an <italic>overcomplete</italic> generative model which assumes more source signals than pixel dimensions. In addition, the sources are modeled as latent variables like in a factor analysis model. That is the data is assumed to be generated according to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e206" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e207" xlink:type="simple"/></inline-formula> denotes the overcomplete dictionary, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e208" xlink:type="simple"/></inline-formula> is distributed according to a sparse factorial distribution, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e209" xlink:type="simple"/></inline-formula> is a Gaussian random variable. The early quantitative study by Lewicki and Olshausen <xref ref-type="bibr" rid="pcbi.1000336-Lewicki1">[24]</xref> could not demonstrate an advantage of overcomplete coding in terms of the rate-distortion trade-off and also the more recent work by Seeger <xref ref-type="bibr" rid="pcbi.1000336-Seeger1">[70]</xref> seems to confirm this conclusion. The addition of a Gaussian random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e210" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e211" xlink:type="simple"/></inline-formula>, however is likely to be advantageous as it may help to interpolate betweem the plain ICA model on the one hand and the spherically symmetric model on the other hand. A comparison of the average log-loss between this model and plain ICA has not been done yet but we can expect that this model can achieve a similar or even better match to the natural image statistics as the spherically symmetric model.</p>
<p>The spherical symmetric model can also be modeled by a redundancy reduction transformation which changes the radial component such that the output distribution is sought to match a Gaussian distribution <xref ref-type="bibr" rid="pcbi.1000336-Lyu1">[31]</xref>. Hence, the redundancy reduction of this model is very similar to the average log-loss of the spherically symmetric distribution. From a biological vision point of view, this type of model is particularly interesting as it allows one to draw a link to divisive normalization, a prominent contrast gain control mechanism observed for virtually all neurons in the early visual system. Our own ongoing work <xref ref-type="bibr" rid="pcbi.1000336-Sinz1">[30]</xref> shows that this idea can be generalized to a larger class of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000336.e212" xlink:type="simple"/></inline-formula> symmetric distributions <xref ref-type="bibr" rid="pcbi.1000336-Hyvrinen2">[67]</xref>. In this way, it is possible to find an optimal interpolation between ICA and the spherically symmetric case <xref ref-type="bibr" rid="pcbi.1000336-Sinz2">[73]</xref>. That is, one can combine orientation selectivity with divisive normalization in a joint model. Our preliminary results suggests that optimal divisive normalization together with orientation selectivity allows for about 10% improvement while divisive normalization alone (i.e., the spherical symmetric model) is only 2% worse <xref ref-type="bibr" rid="pcbi.1000336-Sinz1">[30]</xref>.</p>
</sec><sec id="s4d">
<title>Concluding Remarks</title>
<p>Taken together, the effect of orientation selectivity on redundancy reduction is very limited within the common linear filter bank model of V1 simple cells. In contrast to Zhaoping and coworkers, we do not claim that higher-order redundancy minimization is unlikely to be the main constraint in shaping the cortical receptive fields <xref ref-type="bibr" rid="pcbi.1000336-Li1">[22]</xref>,<xref ref-type="bibr" rid="pcbi.1000336-Petrov1">[27]</xref>. Our conclusion is that although there are significant higher-order correlations in natural images, orientation selective filtering turns out to be not very effective for capturing these. Nevertheless, we do expect that visual representations in the brain aim to model those higher-order correlations, because they are perceptually relevant. Therefore, we think it is important to further explore which type of nonlinear transformations would be suitable to capture more pronounced higher-order correlations. The objective functions studied in this paper are related to factorial coding, density estimation and minimization of the pixel mean square reconstruction error. Of course, there are also other alternatives that are interesting, too. For example, Zhaoping proposed that one possible goal of V1 is to explicitly represent bottom-up saliency in its neural responses for visual attentional selection <xref ref-type="bibr" rid="pcbi.1000336-Zhaoping1">[12]</xref>. As a further alternative, we are currently trying to extend the efficient coding framework to deal with other loss functions. Obviously, the goal of the visual system is not to preserve the pixel representation of the visual input. Instead, seeing serves the purpose to make successful predictions about behaviorally relevant aspects of the environment <xref ref-type="bibr" rid="pcbi.1000336-Helmholtz1">[74]</xref>. Since 3D shape inference is necessary to almost any naturally relevant task, it seems particularly interesting to explore the role of orientation selectivity in the context of 3D shape inference <xref ref-type="bibr" rid="pcbi.1000336-Fleming1">[75]</xref>. For a quantitative account of this problem one can seek to minimize the reconstruction error for the 3D shape rather than for its 2D image. Certainly, this task is much more involved than image reconstruction. Nevertheless, we need to think more about how to tackle the problem of visual inference within the framework of unsupervised learning in order to unravel the principles of neural processing in the brain that are ultimately responsible for our ability to see.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000336.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000336.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>In the article we chose a patch size of 7×7 in order to enhance the comparability to previous work. The supplementary material contains all results (figures and tables) for patch size 15×15.</p>
<p>(2.82 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Philipp Berens, Roland Fleming, Jakob Macke and Bruno Olshausen for fruitful discussions and helpful comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000336-Attneave1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Attneave</surname><given-names>F</given-names></name>
</person-group>             <year>1954</year>             <article-title>Informational aspects of visual perception.</article-title>             <source>Psychol Rev</source>             <volume>61</volume>             <fpage>183</fpage>             <lpage>193</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Barlow1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name>
</person-group>             <year>1959</year>             <article-title>Sensory mechanisms, the reduction of redundancy, and intelligence.</article-title>             <source>The Mechanisation of Thought Processes</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Her Majesty's Stationery Office</publisher-name>             <fpage>535</fpage>             <lpage>539</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Linsker1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Linsker</surname><given-names>R</given-names></name>
</person-group>             <year>1988</year>             <article-title>Self-organization in a perceptual network.</article-title>             <source>Computer</source>             <volume>21</volume>             <fpage>105</fpage>             <lpage>117</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Atick1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Atick</surname><given-names>J</given-names></name>
</person-group>             <year>1992</year>             <article-title>Could information theory provide an ecological theory of sensory processing?</article-title>             <source>Network</source>             <volume>3</volume>             <fpage>213</fpage>             <lpage>251</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Barlow2"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name>
</person-group>             <year>1989</year>             <article-title>Unsupervised learning.</article-title>             <source>Neural Comput</source>             <volume>1</volume>             <fpage>295</fpage>             <lpage>311</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Watanabe1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Watanabe</surname><given-names>S</given-names></name>
</person-group>             <year>1981</year>             <article-title>Pattern recognition as a quest for minimum entropy.</article-title>             <source>Pattern Recognit</source>             <volume>13</volume>             <fpage>381</fpage>             <lpage>387</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Fldik1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Földiák</surname></name>
</person-group>             <year>1990</year>             <article-title>Forming sparse representations by local anti-hebbian learning.</article-title>             <source>Biol Cybern</source>             <volume>64</volume>             <fpage>165</fpage>             <lpage>170</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Olshausen1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>D</given-names></name>
</person-group>             <year>1996</year>             <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>560</fpage>             <lpage>561</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Fldiak1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Földiak</surname><given-names>P</given-names></name>
</person-group>             <year>1991</year>             <article-title>Learning invariance from transformation sequences.</article-title>             <source>Neural Comput</source>             <volume>3</volume>             <fpage>194</fpage>             <lpage>200</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Bialek1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>
</person-group>             <year>2001</year>             <article-title>Predictability, complexity, and learning.</article-title>             <source>Neural Comput</source>             <volume>13</volume>             <fpage>2409</fpage>             <lpage>2463</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Becker1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Becker</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>
</person-group>             <year>1992</year>             <article-title>Self-organizing neural network that discovers surfaces in random-dot stereograms.</article-title>             <source>Nature</source>             <volume>355</volume>             <fpage>161</fpage>             <lpage>163</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Zhaoping1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name>
</person-group>             <year>2006</year>             <article-title>Theoretical understanding of the early visual processes by data compression and data selection.</article-title>             <source>Network</source>             <volume>17</volume>             <fpage>301</fpage>             <lpage>334</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Friedman1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friedman</surname><given-names>JH</given-names></name>
<name name-style="western"><surname>Stuetzle</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>A</given-names></name>
</person-group>             <year>1984</year>             <article-title>Projection pursuit density estimation.</article-title>             <source>J Am Stat Assoc</source>             <volume>19</volume>             <fpage>599</fpage>             <lpage>608</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Simoncelli1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
</person-group>             <year>2001</year>             <article-title>Natural image statistics and neural representation.</article-title>             <source>Annu Rev Neurosci</source>             <volume>24</volume>             <fpage>1193</fpage>             <lpage>1216</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Buchsbaum1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buchsbaum</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Gottschalk</surname><given-names>A</given-names></name>
</person-group>             <year>1983</year>             <article-title>Trichromacy, opponent colours coding and optimum colour information transmission in the retina.</article-title>             <source>Proc R Soc Lond B Biol Sci</source>             <volume>220</volume>             <fpage>89</fpage>             <lpage>113</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Ruderman1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ruderman</surname><given-names>DL</given-names></name>
<name name-style="western"><surname>Cronin</surname><given-names>TW</given-names></name>
<name name-style="western"><surname>Chiao</surname><given-names>C</given-names></name>
</person-group>             <year>1998</year>             <article-title>Statistics of cone responses to natural images: implications for visual coding.</article-title>             <source>J Opt Soc Am A</source>             <volume>15</volume>             <fpage>2036</fpage>             <lpage>2045</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Atick2"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Atick</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Redlich</surname><given-names>A</given-names></name>
</person-group>             <year>1992</year>             <article-title>What does the retina know about natural scenes.</article-title>             <source>Neural Comput</source>             <volume>4</volume>             <fpage>196</fpage>             <lpage>210</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-vanHateren1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>van Hateren</surname><given-names>J</given-names></name>
</person-group>             <year>1993</year>             <article-title>Spatiotemporal contrast sensitivity of early vision.</article-title>             <source>Vision Res</source>             <volume>33</volume>             <fpage>257</fpage>             <lpage>267</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Dong1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dong</surname><given-names>DW</given-names></name>
<name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name>
</person-group>             <year>1995</year>             <article-title>Statistics of natural time-varying images.</article-title>             <source>Network</source>             <volume>6</volume>             <fpage>345</fpage>             <lpage>358</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Dan1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name>
<name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name>
</person-group>             <year>1996</year>             <article-title>Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory.</article-title>             <source>J Neurosci</source>             <volume>16</volume>             <fpage>3351</fpage>             <lpage>3362</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Hancock1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hancock</surname><given-names>PJB</given-names></name>
<name name-style="western"><surname>Baddeley</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Smith</surname><given-names>LS</given-names></name>
</person-group>             <year>1992</year>             <article-title>The principal components of natural images.</article-title>             <source>Network</source>             <volume>3</volume>             <fpage>61</fpage>             <lpage>70</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Li1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Li</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name>
</person-group>             <year>1994</year>             <article-title>Toward a theory of the striate cortex.</article-title>             <source>Neural Comput</source>             <volume>6</volume>             <fpage>127</fpage>             <lpage>146</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Bell1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bell</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>1997</year>             <article-title>The “independent components” of natural scenes are edge filters.</article-title>             <source>Vision Res</source>             <volume>37</volume>             <fpage>3327</fpage>             <lpage>3338</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Lewicki1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lewicki</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
</person-group>             <year>1999</year>             <article-title>Probabilistic framework for the adaptation and comparison of image codes.</article-title>             <source>J Opt Soc Am A</source>             <volume>16</volume>             <fpage>1587</fpage>             <lpage>1601</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Wachtler1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wachtler</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>TW</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>Chromatic structure of natural scenes.</article-title>             <source>J Opt Soc Am A</source>             <volume>18</volume>             <fpage>65</fpage>             <lpage>77</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Lee1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lee</surname><given-names>TW</given-names></name>
<name name-style="western"><surname>Wachtler</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>2002</year>             <article-title>Color opponency is an efficient representation of spectral properties in natural scenes.</article-title>             <source>Vision Res</source>             <volume>42</volume>             <fpage>2095</fpage>             <lpage>2103</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Petrov1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Petrov</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name>
</person-group>             <year>2003</year>             <article-title>Local correlations, information redundancy, and the sufficient pixel depth in natural images.</article-title>             <source>J Opt Soc Am A</source>             <volume>20</volume>             <fpage>56</fpage>             <lpage>66</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Bethge1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
</person-group>             <year>2006</year>             <article-title>Factorial coding of natural images: How effective are linear model in removing higher-order dependencies?</article-title>             <source>J Opt Soc Am A</source>             <volume>23</volume>             <fpage>1253</fpage>             <lpage>1268</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Chandler1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chandler</surname><given-names>DM</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name>
</person-group>             <year>2007</year>             <article-title>Estimates of the information content and dimensionality of natural scenes from proximity distributions.</article-title>             <source>J Opt Soc Am A</source>             <volume>24</volume>             <fpage>922</fpage>             <lpage>941</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Sinz1"><label>30</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sinz</surname><given-names>FH</given-names></name>
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>How much can orientation selectivity and contrast gain control reduce the redundancies in natural images.</article-title>             <comment>Technical Report 169. Tübingen, Germany: Max Planck Institute for Biological Cybernetics</comment>          </element-citation></ref>
<ref id="pcbi.1000336-Lyu1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lyu</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
</person-group>             <year>2008</year>             <article-title>Nonlinear image representation using divisive normalization.</article-title>             <source>IEEE Conf Comput Vis Pattern Recognit</source>             <volume>2008</volume>             <fpage>1</fpage>             <lpage>8</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Perez1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Perez</surname><given-names>A</given-names></name>
</person-group>             <year>1977</year>             <article-title>ε-admissible simplification of the dependence structure of a set of random variables.</article-title>             <source>Kybernetika</source>             <volume>13</volume>             <fpage>439</fpage>             <lpage>444</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Cover1"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cover</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Thomas</surname><given-names>J</given-names></name>
</person-group>             <year>1991</year>             <source>Elements of Information Theory</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>J. Wiley &amp; Sons</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000336-Bernardo1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bernardo</surname><given-names>JM</given-names></name>
</person-group>             <year>1979</year>             <article-title>Expected information as expected utility.</article-title>             <source>Ann Stat</source>             <volume>7</volume>             <fpage>686</fpage>             <lpage>690</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Lewicki2"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lewicki</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>2000</year>             <article-title>Learning overcomplete representations.</article-title>             <source>Neural Comput</source>             <volume>12</volume>             <fpage>337</fpage>             <lpage>365</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Hulle1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hulle</surname><given-names>MMV</given-names></name>
</person-group>             <year>2005</year>             <article-title>Mixture density modeling, kullback-leibler divergence, and differential log-likelihood.</article-title>             <source>Signal Processing</source>             <volume>85</volume>             <fpage>951</fpage>             <lpage>963</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Nadal1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Parga</surname><given-names>N</given-names></name>
</person-group>             <year>1994</year>             <article-title>Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer.</article-title>             <source>Network</source>             <volume>5</volume>             <fpage>565</fpage>             <lpage>581</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Bell2"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bell</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>1995</year>             <article-title>An information maximisation approach to blind separation and blind deconvolution.</article-title>             <source>Neural Comput</source>             <volume>7</volume>             <fpage>1129</fpage>             <lpage>1159</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Goyal1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Goyal</surname><given-names>V</given-names></name>
</person-group>             <year>2001</year>             <article-title>Theoretical foundations of transform coding.</article-title>             <source>IEEE Signal Process Mag</source>             <volume>18</volume>             <fpage>9</fpage>             <lpage>21</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Gray1"><label>40</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gray</surname><given-names>R</given-names></name>
</person-group>             <year>1990</year>             <source>Entropy and Information Theory</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000336-Barlow3"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name>
</person-group>             <year>2001</year>             <article-title>The exploitation of regularities in the environment by the brain.</article-title>             <source>Behav Brain Sci</source>             <volume>24</volume>             <fpage>602</fpage>             <lpage>607</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Wang1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name>
<name name-style="western"><surname>Bovic</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Lu</surname><given-names>L</given-names></name>
</person-group>             <year>2002</year>             <article-title>Why is image quality assessment so difficult?</article-title>             <source>Proc IEEE Int Conf Acoust Speech Signal Process ICASSP</source>             <volume>4</volume>             <fpage>3313</fpage>             <lpage>3316</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Gray2"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gray</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Neuhoff</surname><given-names>D</given-names></name>
</person-group>             <year>1998</year>             <article-title>Quantization.</article-title>             <source>IEEE Trans Inf Theory</source>             <volume>44</volume>             <fpage>2325</fpage>             <lpage>2383</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Gish1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gish</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Pierce</surname><given-names>JN</given-names></name>
</person-group>             <year>1968</year>             <article-title>Asymptotically efficient quantizing.</article-title>             <source>IEEE Trans Inf Theory</source>             <volume>14</volume>             <fpage>676</fpage>             <lpage>683</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Fan1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fan</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Hoffman</surname><given-names>AJ</given-names></name>
</person-group>             <year>1955</year>             <article-title>Some metric inequalities in the space of matrices.</article-title>             <source>Proc Am Math Soc</source>             <volume>6</volume>             <fpage>111</fpage>             <lpage>116</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Srivastava1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Srivastava</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Zhu</surname><given-names>S</given-names></name>
</person-group>             <year>2003</year>             <article-title>On advances in statistical modeling of natural images.</article-title>             <source>J Math Imaging Vis</source>             <volume>18</volume>             <fpage>17</fpage>             <lpage>33</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Hyvrinen1"><label>47</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Karhunen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Oja</surname><given-names>E</given-names></name>
</person-group>             <year>2001</year>             <source>Independent Component Analysis</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>John Wiley &amp; Sons</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000336-Edelman1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Edelman</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Arias</surname><given-names>TA</given-names></name>
<name name-style="western"><surname>Smith</surname><given-names>ST</given-names></name>
</person-group>             <year>1999</year>             <article-title>The geometry of algorithms with orthogonality constraints.</article-title>             <source>SIAM J Matrix Anal Appl</source>             <volume>20</volume>             <fpage>303</fpage>             <lpage>353</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Press1"><label>49</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Press</surname><given-names>WH</given-names></name>
<name name-style="western"><surname>Teukolsky</surname><given-names>SA</given-names></name>
<name name-style="western"><surname>Vetterling</surname><given-names>WT</given-names></name>
<name name-style="western"><surname>Flannery</surname><given-names>BP</given-names></name>
</person-group>             <year>1992</year>             <source>Numerical Recipes in C: The Art of Scientific Computing</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000336-Maxwell1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Maxwell</surname><given-names>JC</given-names></name>
</person-group>             <year>1855</year>             <article-title>Experiments on colour as perceived by the eye, with remarks on colour-blindness.</article-title>             <source>Transactions of the Royal Society of Edinburgh XXI</source>             <volume>2</volume>             <fpage>275</fpage>             <lpage>298</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Zetzsche1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zetzsche</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Krieger</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Wegmann</surname><given-names>B</given-names></name>
</person-group>             <year>1999</year>             <article-title>The atoms of vision: Cartesian or polar?</article-title>             <source>J Opt Soc Am A</source>             <volume>16</volume>             <fpage>1554</fpage>             <lpage>1565</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Brelstaff1"><label>52</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brelstaff</surname><given-names>GJ</given-names></name>
<name name-style="western"><surname>Parraga</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Troscianko</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Carr</surname><given-names>D</given-names></name>
</person-group>             <year>1995</year>             <article-title>Hyperspectral camera system: acquisition and analysis.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Lurie</surname><given-names>BJ</given-names></name>
<name name-style="western"><surname>Pearson</surname><given-names>JJ</given-names></name>
<name name-style="western"><surname>Zilioli</surname><given-names>E</given-names></name>
</person-group>             <source>Proceedings of SPIE</source>             <fpage>150</fpage>             <lpage>159</lpage>             <comment>Volume 2587</comment>             <comment>The database can be downloaded from: <ext-link ext-link-type="uri" xlink:href="http://psy223.psy.bris.ac.uk/hyper/" xlink:type="simple">http://psy223.psy.bris.ac.uk/hyper/</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1000336-Baddeley1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baddeley</surname><given-names>R</given-names></name>
</person-group>             <year>1996</year>             <article-title>An efficient code in v1.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>560</fpage>             <lpage>561</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Nadal2"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Parga</surname><given-names>N</given-names></name>
</person-group>             <year>1998</year>             <article-title>Nonlinear feedforward networks with stochastic outputs: infomax implies redundancy reduction.</article-title>             <source>Network</source>             <volume>9</volume>             <fpage>207</fpage>             <lpage>217</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Malsburg1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Malsburg</surname></name>
</person-group>             <year>1973</year>             <article-title>Self-organization of orientation sensitive cells in the striate cortex.</article-title>             <source>Biol Cybern</source>             <volume>14</volume>             <fpage>85</fpage>             <lpage>100</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Kaschube1"><label>56</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kaschube</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Schnabel</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Loewel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Coppola</surname><given-names>D</given-names></name>
<name name-style="western"><surname>White</surname><given-names>LE</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>Universal pinwheel statistics in the visual cortex.</article-title>             <source>Neuroscience Meeting Planner, 545.9/T11</source>             <publisher-loc>Atlanta, Georgia</publisher-loc>             <publisher-name>Society for Neuroscience</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000336-Wolf1"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wolf</surname><given-names>F</given-names></name>
</person-group>             <year>2005</year>             <article-title>Symmetry, multistability, and long-range interactions in brain development.</article-title>             <source>Phys Rev Lett</source>             <volume>95</volume>             <fpage>208701</fpage>          </element-citation></ref>
<ref id="pcbi.1000336-Wimbauer1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wimbauer</surname></name>
<name name-style="western"><surname>Wenisch</surname></name>
<name name-style="western"><surname>Miller</surname></name>
<name name-style="western"><surname>van Hemmen</surname></name>
</person-group>             <year>1997</year>             <article-title>Development of spatiotemporal receptive fields of simple cells: I. model formulation.</article-title>             <source>Biol Cybern</source>             <volume>77</volume>             <fpage>453</fpage>             <lpage>461</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Horton1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Adams</surname><given-names>DL</given-names></name>
</person-group>             <year>2005</year>             <article-title>The cortical column: a structure without a function.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>360</volume>             <fpage>837</fpage>             <lpage>862</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Olshausen2"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name>
</person-group>             <year>2005</year>             <article-title>How close are we to understanding v1?</article-title>             <source>Neural Comput</source>             <volume>17</volume>             <fpage>1665</fpage>             <lpage>1699</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Masland1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Masland</surname><given-names>RH</given-names></name>
<name name-style="western"><surname>Martin</surname><given-names>PR</given-names></name>
</person-group>             <year>2007</year>             <article-title>The unsolved mystery of vision.</article-title>             <source>Curr Biol</source>             <volume>17</volume>             <fpage>R577</fpage>             <lpage>R582</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Weiss1"><label>62</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Weiss</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Freeman</surname><given-names>W</given-names></name>
</person-group>             <year>2007</year>             <article-title>What makes a good model of natural images?</article-title>             <source>IEEE Conf Comput Vis Pattern Recognit</source>             <volume>2007</volume>             <fpage>1</fpage>             <lpage>8</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Bethge2"><label>63</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Wiecki</surname><given-names>TV</given-names></name>
<name name-style="western"><surname>Wichmann</surname><given-names>FA</given-names></name>
</person-group>             <year>2007</year>             <article-title>The independent components of natural images are perceptually dependent.</article-title>             <source>Proceedings of SPIE. Volume 6492</source>             <fpage>A1</fpage>             <lpage>A12</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Bethge3"><label>64</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Berens</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>Near-maximum entropy models for binary neural representations of natural images.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Platt</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Koller</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Singer</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Roweis</surname><given-names>S</given-names></name>
</person-group>             <source>21th Neural Information Processing Systems Conference</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>97</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Wainwright1"><label>65</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wainwright</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name>
</person-group>             <year>2000</year>             <article-title>Scale mixtures of Gaussians and the statistics of natural images.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Solla</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Leen</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Müller</surname><given-names>KR</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems (NIPS*99) 12</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>855</fpage>             <lpage>861</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Karklin1"><label>66</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Karklin</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name>
</person-group>             <year>2005</year>             <article-title>A hierarchical Bayesian model for learning nonlinear statistical regularities in nonstationary natural signals.</article-title>             <source>Neural Comput</source>             <volume>17</volume>             <fpage>397</fpage>             <lpage>423</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Hyvrinen2"><label>67</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Köster</surname><given-names>U</given-names></name>
</person-group>             <year>2007</year>             <article-title>Complex cell pooling and the statistics of natural images.</article-title>             <source>Network</source>             <volume>18</volume>             <fpage>81</fpage>             <lpage>100</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Osindero1"><label>68</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Osindero</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name>
</person-group>             <year>2008</year>             <article-title>Modeling image patches with a directed hierarchy of Markov random fields.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Platt</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Koller</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Singer</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Roweis</surname><given-names>S</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems 20</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>1121</fpage>             <lpage>1128</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Garrigues1"><label>69</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Garrigues</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
</person-group>             <year>2008</year>             <article-title>Learning horizontal connections in a sparse coding model of natural images.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Platt</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Koller</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Singer</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Roweis</surname><given-names>S</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems 20</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>505</fpage>             <lpage>512</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Seeger1"><label>70</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seeger</surname><given-names>MW</given-names></name>
</person-group>             <year>2008</year>             <article-title>Bayesian inference and optimal design for the sparse linear model.</article-title>             <source>J Mach Learn Res</source>             <volume>9</volume>             <fpage>759</fpage>             <lpage>813</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-GuerreroColn1"><label>71</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Guerrero-Colón</surname><given-names>JA</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
<name name-style="western"><surname>Portilla</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>Image denoising using mixtures of Gaussian scale mixtures.</article-title>             <source>Proc Int Conf Image Proc</source>             <volume>15</volume>             <fpage>565</fpage>             <lpage>568</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Hammond1"><label>72</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hammond</surname><given-names>DK</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
</person-group>             <year>2008</year>             <article-title>Image modeling and denoising with orientation-adapted Gaussian scale mixtures.</article-title>             <source>IEEE Trans Image Process</source>             <volume>17</volume>             <fpage>2089</fpage>             <lpage>2101</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Sinz2"><label>73</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sinz</surname><given-names>FH</given-names></name>
<name name-style="western"><surname>Gerwinn</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>Characterization of the p-generalized normal distribution.</article-title>             <source>J Multivar Anal</source>             <volume>100</volume>             <fpage>817</fpage>             <lpage>820</lpage>          </element-citation></ref>
<ref id="pcbi.1000336-Helmholtz1"><label>74</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Helmholtz</surname><given-names>H</given-names></name>
</person-group>             <year>1878</year>             <article-title>The facts of perception.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Kahl</surname><given-names>R</given-names></name>
</person-group>             <source>Selected Writings of Hermann Helmholtz</source>             <publisher-loc>Middletown, Connecticut</publisher-loc>             <publisher-name>Wesleyan University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000336-Fleming1"><label>75</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fleming</surname><given-names>RW</given-names></name>
<name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Adelson</surname><given-names>EH</given-names></name>
</person-group>             <year>2004</year>             <article-title>Specular reflections and the perception of shape.</article-title>             <source>J Vis</source>             <volume>4</volume>             <fpage>798</fpage>             <lpage>820</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>