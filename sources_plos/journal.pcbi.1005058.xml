<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005058</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-02072</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Molecular biology</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Sequencing techniques</subject><subj-group><subject>Sequence analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Sequencing techniques</subject><subj-group><subject>Sequence analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The Representation of Prediction Error in Auditory Cortex</article-title>
<alt-title alt-title-type="running-head">The Representation of Prediction Error in Auditory Cortex</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rubin</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ulanovsky</surname>
<given-names>Nachum</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6645-107X</contrib-id>
<name name-style="western">
<surname>Nelken</surname>
<given-names>Israel</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tishby</surname>
<given-names>Naftali</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Edmond and Lily Safra Center for Brain Sciences, Hebrew University, Jerusalem, Israel</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Neurobiology, Weizmann Institute of Science, Rehovot, Israel</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Neurobiology, Institute of Life Sciences, Hebrew University, Jerusalem, Israel</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>The Benin School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Theunissen</surname>
<given-names>Frédéric E.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of California at Berkeley, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: NU IN NT. Performed the experiments: NU IN. Analyzed the data: JR. Wrote the paper: JR IN NT NU.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">israel@cc.huji.ac.il</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>4</day>
<month>8</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>8</issue>
<elocation-id>e1005058</elocation-id>
<history>
<date date-type="received">
<day>10</day>
<month>12</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>7</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Rubin et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005058"/>
<abstract>
<p>To survive, organisms must extract information from the past that is relevant for their future. How this process is expressed at the neural level remains unclear. We address this problem by developing a novel approach from first principles. We show here how to generate low-complexity representations of the past that produce optimal predictions of future events. We then illustrate this framework by studying the coding of ‘oddball’ sequences in auditory cortex. We find that for many neurons in primary auditory cortex, trial-by-trial fluctuations of neuronal responses correlate with the theoretical prediction error calculated from the short-term past of the stimulation sequence, under constraints on the complexity of the representation of this past sequence. In some neurons, the effect of prediction error accounted for more than 50% of response variability. Reliable predictions often depended on a representation of the sequence of the last ten or more stimuli, although the representation kept only few details of that sequence.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>A crucial aspect of all life is the ability to use past events in order to guide future behavior. To do that, creatures need the ability to predict future events. Indeed, predictability has been shown to affect neuronal responses in many animals and under many conditions. Clearly, the quality of predictions should depend on the amount and detail of the past information used to generate them. Here, by using a basic principle from information theory, we show how to derive explicitly the tradeoff between quality of prediction and complexity of the representation of past information. We then apply these ideas to a concrete case–neuronal responses recorded in auditory cortex during the presentation of oddball sequences, consisting of two tones with varying probabilities. We show that the neuronal responses fit quantitatively the prediction errors of optimal predictors derived from our theory, and use that result in order to deduce the properties of the representations of the past in the auditory system. We conclude that these memory representations have surprisingly long duration (10 stimuli back or more), but keep relatively little detail about this past. Our theory can be applied widely to other sensory systems.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by grants from the Israel Science Foundation (ISF), the US-Israel Binational Science Foundation (BSF), and the German-Israeli Foundation (GIF) to IN; by a F.I.R.S.T. grant and by the DARPA MSEE project support to NT; and by the Gatsby Charitable Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files are available from the dryad database (accession number doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.3m5v5" xlink:type="simple">10.5061/dryad.3m5v5</ext-link>)</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Organisms often operate in unknown and uncertain environments. Therefore, extracting aspects of past observations, which are maximally predictive of the relevant future, is essential for survival. It has been suggested that the sensory cortex evolved to extract the statistical regularities of the world [<xref ref-type="bibr" rid="pcbi.1005058.ref001">1</xref>]. Adaptation of the nervous system to the statistical structure of the input is reflected in studies of neuronal responses to natural stimuli. For example, in the auditory system, auditory nerve fibers–part of the auditory periphery–have been shown to achieve high coding efficiency by implementing a “tuned” nonlinear filter that selectively amplifies the anticipated signal [<xref ref-type="bibr" rid="pcbi.1005058.ref002">2</xref>]. Similarly, in the visual system, Laughlin [<xref ref-type="bibr" rid="pcbi.1005058.ref003">3</xref>] showed that the contrast-response function of interneurons in the fly's compound eye approximates the cumulative probability distribution of contrast levels in natural scenes.</p>
<p>The central auditory system shows sensitivity to stimulus statistics as well. Event-related potentials recorded in humans show sensitivity to deviant stimuli. This sensitivity may occur rather early, at the mid-latency potentials range [<xref ref-type="bibr" rid="pcbi.1005058.ref004">4</xref>], and has been intensively studied in the context of the mismatch negativity (MMN), peaking about 150 ms after the point of deviance [<xref ref-type="bibr" rid="pcbi.1005058.ref005">5</xref>]. Similar sensitivity occurs also in the responses of single neurons in auditory cortex: using oddball sequences composed of two frequencies with different probabilities, Ulanovsky et al. [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>] found that neurons in cat auditory cortex responded more strongly to a given tone frequency when it was rare than when it was common. This sensitivity, named stimulus-specific adaptation (SSA) by Ulanovsky et al. [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>], has been by now shown in multiple mammalian species and even in birds (See [<xref ref-type="bibr" rid="pcbi.1005058.ref007">7</xref>] for review). SSA may be linked to deviance detection in the time frame of the mid-latency potentials rather than to MMN (Grimm, Escera and Nelken 2015). Thus, we hypothesize that neurons in the auditory system encode some notion of a prediction error.</p>
<p>The coding of prediction error is considered central to learning [<xref ref-type="bibr" rid="pcbi.1005058.ref008">8</xref>], memory formation [<xref ref-type="bibr" rid="pcbi.1005058.ref009">9</xref>], and decision-making [<xref ref-type="bibr" rid="pcbi.1005058.ref010">10</xref>]. Prediction errors are also known to be related to efficient coding where only the unexpected at one stage of processing should be transmitted to the next stage [<xref ref-type="bibr" rid="pcbi.1005058.ref011">11</xref>].</p>
<p>This paper has two goals. The first is to present a theory of prediction error from first principles. For an organism that operates in a statistically stationary world (as is generally the case in laboratory experiments), prediction quality is limited by the statistical structure of the data that the organism collects from the past. The theory depends only on this statistical structure, and not on any assumptions about specific brain mechanisms. We assume that the brain forms a <italic>reduced representation</italic> of the past, which serves to generate predictions of future events. We use information theory to quantify both the <italic>complexity</italic> of these reduced representations and the <italic>predictive information</italic> they carry with respect to future events [<xref ref-type="bibr" rid="pcbi.1005058.ref012">12</xref>]. The term complexity refers here to the rate of information (specifically, the mutual information in bits/s or, equivalently but more conveniently here, in bits/stimulus) that the representation carries about the past. Predictive information refers to the rate of information (again, mutual information in bits/s or bits/stimulus) that the reduced representation carries about future events. Both terms are defined precisely below. Extraction of the predictive aspects of the past stimuli can be formalized as an optimization problem: minimize the complexity of the reduced representation of the past while preserving a predefined level of predictive information. A reduced representation provides a predictive probability for every event–this is the probability assigned to the event just before it actually occurred, given the reduced representation of the past. Our theory uses these predictive probabilities to calculate prediction errors–events with low predictive probability generate large prediction error, while events with a high predictive probability generate small prediction errors.</p>
<p>The constrained optimization problem we use to calculate the reduced representations is a special instance of the <italic>Information Bottleneck</italic> (IB) principle [<xref ref-type="bibr" rid="pcbi.1005058.ref013">13</xref>]. The IB principle applies to any two random variables X and Y with a known joint distribution. Like the special case described here in details, the IB principle provides a way of finding reduced representations (as defined later in the paper) of X that are maximally informative about Y. Here we apply it to the past of the stimulation sequence (X) and to its future (Y). Much of the detailed discussion below can be considered as a primer for the use of the IB principle.</p>
<p>The second goal of the paper is to demonstrate the usefulness of the theory by applying it to the neuronal responses evoked by random tone sequences consisting of two frequencies with varying probabilities (‘oddball’ sequences). We show that prediction errors calculated by the theory correlate well with neuronal responses. Most importantly, we use the theory to extract parameters of the reduced representations that underlie these responses: we show that these reduced representations have a long duration (typically N ≥ 10 preceding stimuli) but keep only coarse details about the sequence that was presented (i.e., the reduced representations have low complexity). These results show how to establish properties of the neural code rigorously from first principles.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We treat the process of perception and prediction within an information theoretical framework, where information flows between the environment and the organism (<bold><xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref></bold>). In the following, we describe the general framework and at the same time apply it to the special case of random two-tone sequences that will be used later to show that the theory can be applied exactly to (admittedly simple) realistic scenarios.</p>
<fig id="pcbi.1005058.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Information flow between the organism and the environment.</title>
<p>The environment consists of a stationary random process. Here the environment produces a Bernoulli sequence of two stimuli. The organism perceives the sequence, summarizing the recent past by a state of a reduced representation (<italic>m</italic>), then uses <italic>m</italic> to produce a prediction for the next stimulus in the sequence. Perception is characterized by <italic>P(m|past)</italic>, mapping a sequence of <italic>past</italic> observations (<italic>N</italic> = 4 in this illustration) into states <italic>m</italic> of the reduced representation. In this example, the reduced representation consists of the number of red stimuli among the last <italic>N</italic> stimuli. Prediction is characterized by <italic>P(future|m)</italic>, which assigns to each state <italic>m</italic> a set of subjective expectations for the next (<italic>future</italic>) stimulus. As the number of red stimuli in the past increases, the probability assigned to a red stimulus increases and that assigned to a blue stimulus decreases. The numbers shown here correspond to the posterior probabilities for the corresponding stimulus given a uniform prior on the probability of a red stimulus.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g001" xlink:type="simple"/>
</fig>
<p>The starting point of the theory is a stationary ‘world’–in practice a sequence of stimuli, with a known probability distribution. To simplify the theoretical treatment, we assume the stimuli occur at a fixed rate and have a fixed duration. For the special case of two-tone sequences, we assume that the stimuli are single tones of one of two possible frequencies. For each stimulus presentation, one or the other of the two frequencies are selected with probabilities <italic>p</italic> and 1 − <italic>p</italic> which are fixed along the sequence. We also assume that choices of the frequency in subsequent trials are independent. To avoid confusion later in the paper, the terms ‘frequency’ and ‘probability’ will always be used in the same way–frequency refers to tone frequency, probability to its probability of occurrence. Such sequences are commonly used to study MMN in humans as well as SSA in animal models [<xref ref-type="bibr" rid="pcbi.1005058.ref014">14</xref>]. Typically, in these sequences the tone duration is relatively short–tens or a couple of hundreds of ms–whereas the inter-stimulus interval is much longer, e.g. 300 ms or 730 ms or 1200 ms. In the data analyzed later in the paper, tone duration was 230 ms and the inter-stimulus interval was 730 ms.</p>
<p>We conceptualize the organism as a prediction machine (<bold><xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref></bold>)–it has to predict the next stimulus as accurately as possible. In particular, we assume that the organism forms a reduced representation of the past stimuli (in some predefined time window, to be specified later), which it uses to generate predictions of future events. The reduced representation is a (potentially probabilistic) function of the recent history of the stimulation sequence.</p>
<p>We treat the observed sequence of stimuli, its reduced representation, and the next stimulus, as three random variables denoted respectively by <italic>past</italic>, <italic>m</italic>, and <italic>future</italic> (<bold><xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref></bold>). The reduced representation of the past is specified by a conditional probability distribution <italic>P(m|past)</italic>, mapping a sequence of <italic>N</italic> past stimuli into states of <italic>m</italic> (<italic>N</italic> will be discussed further below). This is the sense in which the reduced representation is a function of the past. Predictions are carried by another conditional probability distribution <italic>P(future|m)</italic> (the predictive probability distribution) that assigns a set of expectations for future events to each state of <italic>m</italic>, and is therefore a probabilistic function of the reduced representation. Importantly, all uses of the term ‘representation’ in this paper refer to the notion as defined here–a random variable <italic>m</italic>, which is a function, potentially probabilistic, of the past, with its associated predictive probability distribution. A reduced representation is therefore fully characterized by the two associated conditional probability distributions, <italic>P</italic>(<italic>m</italic>|<italic>past</italic>) and <italic>P</italic>(<italic>future</italic>|<italic>m</italic>). As usual in probability theory, by abuse of notation we will use <italic>m</italic> to denote both the random variable (specified by the conditional probability <italic>P</italic>(<italic>m</italic>|<italic>past</italic>)) and the specific values it can take (the ‘states’ of the reduced representation). The meaning should be clear from the context. We will derive the reduced representations (and corresponding predictive probability distributions) from the IB principle as described later.</p>
<p>As a concrete application of the theory to oddball sequences, assume that the temporal window extending to the past is of length <italic>N = 4</italic> stimuli (<bold><xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref></bold>) and that we want to predict the frequency of the next tone. The oddball sequences are assumed to be generated in the following way: before the beginning of each sequence, the experimenter selects a probability <italic>p</italic>. Then, for each tone presentation, the experimenter selects to use tone ‘A’ (blue in <xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref>) with probability <italic>1-p</italic> and tone ‘B’ (red in <xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref>) with probability <italic>p</italic>. Importantly, while the probability of the two tones is selected <italic>a-priori</italic> by the experimenter, it is unknown to the subject of the experiment, and therefore the past and future of the stimulation sequence are not independent of each other. Instead, the relative abundance of 'A' and 'B' tones in the near past carries information about the probability of the next tone frequency. In this example, the past random variable is simply the list of the last four tone frequencies (‘A’ or ‘B’) in the order in which they occurred, and the future variable is the next tone in the sequence. We next need to specify the reduced representation, <italic>m</italic>. One obvious choice consists of setting <italic>m</italic> to be the full past, that is, the list of the last four tone frequencies in the order in which they occurred. In that case, the reduced representation is not really reduced, <italic>P(m|past) = 1</italic> for the state of the representation that corresponds to the past that actually occurred and <italic>P(m|past) = 0</italic> otherwise (<bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref></bold> case (i)).</p>
<fig id="pcbi.1005058.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Reduced representations through the Information Bottleneck method.</title>
<p>(<bold>a</bold>) Illustration of different reduced representations of past sequences in the oddball paradigm (for <italic>N</italic> = 4 stimuli). Reduced representations are depicted by the conditional probability distribution <italic>P(m|past)</italic> that maps sequences of observations into states <italic>m</italic>. (<italic>i</italic>) The topmost representation maps each possible sequence of <italic>N</italic> = 4 stimuli to unique state <italic>m</italic>, resulting in 16 states. (<italic>ii</italic>) The middle representation is a reduced version, in which sequences with the same number of occurrences of each tone are grouped together resulting in 5 different states. This is the minimal sufficient statistic, and is also the representation illustrated in <xref ref-type="fig" rid="pcbi.1005058.g001">Fig 1</xref>. (<italic>iii</italic>) The bottom representation with only 3 states results from further constraining the complexity (mutual information between the reduced representation and the past) to 1 bit, and is the optimal representation (with highest predictive power) with that complexity. For this representation, the mapping for past to the state of the representation is probabilistic. (<bold>b</bold>) The tradeoff between predictive power and complexity of reduced representations in the oddball paradigm for two durations of the past (<italic>N</italic> = 4 stimuli in gray; <italic>N</italic> = 10 stimuli in black). Each point along the solid curves shows the complexity (abscissa) and predictive power (ordinate) of one unique solution of the tradeoff, with maximal predictive power for the given complexity and, equivalently, minimal complexity for the given predictive power. The rightmost points correspond to the complexity and predictive power of the full representations that assign a unique state <italic>m</italic> to each and every possible sequence of <italic>N</italic> stimuli. Dashed lines connect these values with the points corresponding to the representations based on the minimal sufficient statistic (number of red stimuli). Using the minimal sufficient statistic produces representations that are less complex but provide the same predictive power as the full representations. Further constraints on the complexity result in representations that have lower predictive power. The complexity and predictive power of representation (iii) are shown explicitly on the N = 4 curve: the complexity of this representation is lower than that of the sufficient statistic, and in consequence its predictive power is lower as well. (<bold>c</bold>) Prediction errors along an oddball sequence calculated using the transformation illustrated in panel <bold>a</bold> ((i) and (ii), which produce the same prediction errors). In this example, for each stimulus, the preceding four stimuli determine a unique state <italic>m</italic> of the reduced representation; prediction error is calculated from the predictive distribution using that state <italic>m</italic>. Bar heights represent the prediction error associated with each stimulus.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g002" xlink:type="simple"/>
</fig>
<p>However, it is clear that this representation is too detailed. In order to predict the next tone as accurately as possible from an oddball sequence, where the stimuli are independent and identically distributed (i.i.d), it is enough to know the number of B tones that occurred among the last 4 tones. This number is a <italic>minimal sufficient statistic</italic> in the case of the oddball sequences: given this number, no additional information about the past (e.g. the order in which these four tones were presented) can improve the prediction of future tone frequencies. If we choose <italic>m</italic> to be the number of B tones among the last four tones, we have a reduced representation that can be in 5 different states, corresponding to 0 B tones, 1 B tone and so on up to 4 B tones. Some of the conditional probabilities that define <italic>m</italic> are for example <italic>P(m = 0|AAAA) = 1</italic>, <italic>P(m = 3|ABAA) = 0</italic>, <italic>P(m = 2|ABAB) = 1</italic>. The matrix that defines this reduced representation is shown in Figs <xref ref-type="fig" rid="pcbi.1005058.g001">1</xref> and <xref ref-type="fig" rid="pcbi.1005058.g002">2A</xref> case (ii).</p>
<p>A third possible reduced representation is shown in <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref> (iii) ('case (iii)' below). While cases (i) and (ii) are deterministic, case (iii) is a probabilistic (‘soft’) assignment of the past into 3 classes, which can be roughly characterized as ‘a lot of A tones’, ‘about the same number of A and B tones’, and ‘a lot of B tones’. The assignment is soft in the sense that it allows each past to be assigned with some non-zero probability to each of the three possible states of the reduced representation. Thus, the past ABAA is assigned with high probability to the class ‘a lot of A tones’, but it has some probability to be assigned to the class ‘about the same number of A and B tones’. Similarly, the past AABB is assigned with high probability to the class ‘about the same number of A and B tones’, but has some probability to be assigned to either of the other two classes.</p>
<p>While such probabilistic reduced representations may seem somewhat contrived at this point, we will see later that they actually occur naturally as part of the theory, as optimal solutions to the problem of simplifying (in a sense to be made precise below) the minimal sufficient statistic. Such simplifications may be required because of imperfect or constrained sensory capacity–for example, at the time that the presentation of the last tone ends and the state <italic>m</italic> is established, the identity of the tones has been already forgotten to some degree. As a result, there is some uncertainty about the past sequence, reflected in the soft character of such reduced representation.</p>
<p>To complete the specification of the reduced representations for cases (i)-(iii), we need, in addition to the mapping <italic>P</italic>(<italic>m</italic>|<italic>past</italic>), to specify the predictive probability distributions <italic>P</italic>(<italic>future</italic>|<italic>m</italic>) for each case. In all three cases, <italic>P</italic>(<italic>future</italic>|<italic>m</italic>) is a specification of the probabilities for the A and B tones to occur, and these sum to 1. It can therefore be specified by a single number, the probability of the B tone given the state of the reduced representation. In case (i), a naïve choice for this probability would be <italic>P(B|m) = #(B tones)/4</italic>. This choice turns out not to be optimal (the optimal choice depends on the <italic>a-priori</italic> distribution of <italic>p</italic>). For case (ii), the similarly naïve choice would be <italic>P(B|m) = m/4</italic>. For case (iii), we have to specify three probabilities, one for each class of the reduced representation. Clearly, for the class corresponding to ‘about the same number of A and B tones’, the predictive probability for a B tone should be 0.5, whereas it is smaller for ‘a lot of A tones’ and larger for ‘a lot of B tones’. The exact values will be derived later.</p>
<p>We go back to the general theory. Given that there are many possible reduced representations (for example, we described at least three different ones in <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref>), we have to specify selection rules–which reduced representations are good and which are not. This is really the heart of the theory. The IB principle postulates that the set of good reduced representations is formed of the solutions to a tradeoff between the quality of the predictions that the reduced representation provides about the future, and how complex the reduced representation is. We now define precisely what we mean by these terms.</p>
<p>To quantify the quality of the predictions derived from a given reduced representation we first compute prediction errors. <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2C</xref> illustrates the derivation of the prediction errors. To calculate the prediction error associated with the occurrence of a given stimulus, we consider the N stimuli that just preceded it as the past, and the given stimulus as the upcoming, ‘future’, stimulus. In <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2C</xref>, for example, when considering a past whose duration is N = 4 stimuli, the first B tone (red, 5<sup>th</sup> tone in the sequence) has a past that is composed four successive A stimuli (in blue). Once these four stimuli occurred, we can use the reduced representation in order to predict what would be the upcoming (future) stimulus–the red tone in this case. To do that, we use the probability distribution <italic>P</italic>(<italic>m</italic>|<italic>AAAA</italic>) (part of the specification of the reduced representation) to select a specific state for <italic>m</italic>. For the example of <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2C</xref>, we use case (ii), so that the state <italic>m</italic> is determined uniquely as <italic>m</italic> = 1 (see <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref>). We now determine the probability of the stimulus that actually occurred, the B tone, which is <italic>P</italic>(<italic>B</italic>|<italic>m</italic> = 1) (again, part of the specification of the reduced representation). As discussed above, this probability should be small (close to 0) for any reasonable reduced representation.</p>
<p>We can repeat the same process for all locations along the sequence, assigning a predictive probability for each stimulus–the probability with which that specific stimulus is expected, given the state <italic>m</italic> of the reduced representation, determined from the immediate past of the stimulation sequence. For example, in <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2C</xref>, the next 3 stimuli are A tones (blue), their immediate past is AAAB, AABA and AAAB respectively, all three of which are mapped into state m = 2 (see <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref>, case(ii)). The probability for observing A when m = 2 should be relatively large (although smaller than the probability for observing A when m = 1).</p>
<p>We define the prediction error by −log<sub>2</sub>(p), where <italic>p</italic> is the probability with which the future event is expected given the state <italic>m</italic> of the reduced representation that corresponds to the immediate past, as described in the preceding paragraphs (<bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2C</xref></bold>; see <xref ref-type="sec" rid="sec011">Methods</xref> for details). In the case of the oddball sequences, <italic>p</italic> will be simply <italic>p(B|m)</italic> if tone B occurred, and <italic>1-p(B|m)</italic> if tone A occurred. This choice conforms with the intuition that a stimulus that had small predictive probability is more surprising than a stimulus with large predictive probability. Indeed, for probabilities close to 0, -log(p) is large, while for probabilities close to 1, -log(p) is close to 0 (in both cases, it is positive, since p&lt;1). In <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2C</xref>, for example, the prediction error at the 5<sup>th</sup> position is large, since the past had 4 A tones but the future was a B tone, while the next 3 prediction errors are smaller, since at those positions that past had 3 A’s and 1 B tones, and the future was an A tone. As importantly, this is essentially the only consistent choice for prediction error if we require it to have a small number of natural properties ([<xref ref-type="bibr" rid="pcbi.1005058.ref015">15</xref>], an excellent intuitive discussion can be found in [<xref ref-type="bibr" rid="pcbi.1005058.ref016">16</xref>]).</p>
<p>The prediction error fluctuates on a trial-by-trial basis, so it cannot be used ‘as is’ to quantify the quality of the predictions based on the reduced representation. Furthermore, even when the sequence of stimuli is given, the state <italic>m</italic> can still be a random variable; therefore, the prediction error is itself a random variable. Thus, the relevant quantifier is the expected value of the prediction error, which is well-defined for a stationary process. Even the expected prediction error itself is technically somewhat inconvenient to use. Instead, we use a simple transformation of the expected prediction error. In the Methods section, we demonstrate that the prediction error is equal to the difference <italic>H</italic>(<italic>future</italic>) − <italic>I</italic>(<italic>m</italic>; <italic>future</italic>), where <italic>H</italic>(<italic>future</italic>) is the entropy of the upcoming stimulus (which doesn’t depend on the reduced representation) and <italic>I</italic>(<italic>m</italic>; <italic>future</italic>) is the mutual information between the reduced representation and the upcoming stimulus. In consequence, the expected prediction error and the mutual information <italic>I</italic>(<italic>m</italic>; <italic>future</italic>) are negatively related to each other. While the expected prediction error is the quantity of interest, the mutual information is easier to work with. Therefore, we define the <italic>predictive power</italic> of a reduced representation <italic>m</italic> as the mutual information between the reduced representation and the upcoming stimulus, <italic>I(m;future)</italic>. The larger it is, the better are the predictions derived from the reduced representation.</p>
<p>We turn now to the definition of the <italic>complexity</italic> of the reduced representation. The past may include a substantial amount of details that are irrelevant for predicting the future. These details can be ignored in the reduced representation without affecting prediction quality. Our definition of complexity relates to this relationship between the reduced representation and the past, and it encodes the amount of details that the reduced representation extracts from the past sequence in order to make predictions about the future. This notion is quantified by the mutual information <italic>I(past;m)</italic>. A reduced representation of lower complexity keeps less detail about the past than a reduced representation of higher complexity.</p>
<p>The use of mutual information to measure predictive power and complexity provides the theory with absolute scales. The complexity cannot be larger than the entropy of the past, so that a reduced representation whose complexity is equal to that of the past is equivalent to storing the full details of the past stimulation sequence. Similarly, the predictive power cannot exceed the entropy of the future, but in general, it has a tighter bound: even the best representation of the past (equivalent to storing the full past) cannot in general achieve full predictability of the future. In fact, the predictive power is bounded by <italic>I(past;future)</italic>, which is generally much smaller than the entropy of the future.</p>
<p>These definitions link the theory in a natural way with the notions of sufficient statistics and minimal sufficient statistics [<xref ref-type="bibr" rid="pcbi.1005058.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref017">17</xref>]. A reduced representation that achieves the upper bound on predictive power (the mutual information between past and future) is a ‘sufficient statistic’ in the sense of classical statistical theory and a sufficient statistic of minimal complexity (by our definitions, and the Data Processing Inequality) is a ‘minimal sufficient statistic’ in the same sense ([<xref ref-type="bibr" rid="pcbi.1005058.ref018">18</xref>] pp. 115–119). We are however interested also in reduced representations that are simpler than the minimal sufficient statistic. Such reduced representations cannot achieve the full predictive power, but as we will show below, substantial reductions in complexity can result in rather minimal losses of predictive power.</p>
<p>In the case of the oddball sequences, we can explicitly calculate the predictive power and complexity of the reduced representations. For <bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref></bold> case (i), complexity is 3.64 bits and predictive power is 0.173 bits. For case (ii), complexity is 2.32 bits and predictive power is 0.173 bits. Note that case (i) has the same predictive power as case (ii), but a higher complexity. Thus, case (i) is in a sense suboptimal. In fact, case (ii) is the simplest possible among all reduced representations that achieve the maximal predictive power, a reflection of the fact that it is the minimal sufficient statistic for estimating the probability of tone B ([<xref ref-type="bibr" rid="pcbi.1005058.ref018">18</xref>] pp. 92–95). For case (iii) (with optimally specified probabilities, as described later) the complexity is 1 bit and predictive power is 0.134 bits. While case (iii) has lower predictive power than case (ii), the decrease in predictive power (by a factor of 0.77) is much smaller than the decrease in complexity (by a factor of 0.43). Case (iii) is in fact the reduced representation with minimal complexity among all those whose predictive power is not smaller than 0.134 bits.</p>
<p>Going back to the general theory, the IB principle posits that in general, among all reduced representations that achieve a certain level of predictive power (such as e.g. cases (i) and (ii) above), the one that has the lowest complexity is the best. We can always achieve maximal predictive power by setting <italic>m</italic> to be the full past (as in case (i) above), and try to simplify it as we did when going from case (i) to case (ii). However, simple sufficient statistics (simpler than storing the entire past) are available only for a specific class of distributions (exponential families; [<xref ref-type="bibr" rid="pcbi.1005058.ref018">18</xref>] pp. 102–110), and even then may be too complex (e.g. requiring a sensory resolution that is too high). Thus, we want to consider reduced representations that are less complex than the sufficient statistics, paying the price of potentially reducing the predictive power. In fact, we consider the reduced representation to be a ‘bottleneck’ between perception (coding of the past) and prediction. Case (iii) suggests that by accepting minor reductions in the predictive power, we can produce in many cases reduced representations of the oddball sequences that are considerably simpler than even the minimal sufficient statistic.</p>
<p>We want therefore simple reduced representations that are have high predictive power, or alternatively reduced representations with high predictive power that are simple. The two requirements are in opposition to each other–the simpler the reduced representation is, the lower the predictive power we can expect it to have. This is the essential tradeoff posited by the IB principle. A good reduced representation is one that has maximal predictive power given its complexity, or alternatively that is the simplest possible given its predictive power.</p>
<p>Technically, tradeoffs of this kind are solved by constrained maximization. We impose a constraint on the predictive power, requiring it to have a value <italic>I</italic><sub><italic>pred</italic></sub> within its allowed range, and then search for the reduced representation that has that predictive power but minimal complexity. The search is performed over all reduced representations–that is, conditional probability functions <italic>P</italic>(<italic>m</italic>|<italic>past</italic>) and <italic>P</italic>(<italic>future</italic>|<italic>m</italic>)–which have the desired level of predictive power. However, given <italic>P</italic>(<italic>past</italic>|<italic>future</italic>) (which is assumed to be known <italic>a-priori</italic>), <italic>P</italic>(<italic>future</italic>|<italic>m</italic>) can be computed from <italic>P</italic>(<italic>m</italic>|<italic>past</italic>). Thus, it is enough to optimize over all <italic>P</italic>(<italic>m</italic>|<italic>past</italic>) that fulfill the constraint.</p>
<p>Formally, given a statistical characterization of the stationary stimulation process <italic>P(past</italic>,<italic>future)</italic>, we look for a reduced representation with minimal complexity that has the imposed level of predictive power (s.t. stands for ‘such that’):
<disp-formula id="pcbi.1005058.e001">
<alternatives>
<graphic id="pcbi.1005058.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="italic">past</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mrow><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Alternatively, we can restate the optimization principle by considering its dual problem: find a reduced representation of the past with some desired level of complexity that maximizes its predictive power (and thus minimizes the expected prediction error).</p>
<disp-formula id="pcbi.1005058.e002">
<alternatives>
<graphic id="pcbi.1005058.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="italic">past</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mrow><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub>
</mml:math>
</alternatives>
</disp-formula>
<p>These two dual problems are the mathematical expression of the IB principle [<xref ref-type="bibr" rid="pcbi.1005058.ref013">13</xref>]. A solution to these dual problems consists of the two conditional distributions: <italic>P(m|past)</italic> and <italic>P(future|m)</italic> that characterize how the state of the reduced representation is determined from the past and how predictions are carried out given the state of the reduced representation. There is a family of such solutions, determined by the values of the constraints <italic>I</italic><sub><italic>pred</italic></sub> or <italic>I</italic><sub><italic>complex</italic></sub>. The solutions can be found by applying the Information Bottleneck algorithm [<xref ref-type="bibr" rid="pcbi.1005058.ref013">13</xref>], an iterative algorithm that solves the constrained optimization problem for all allowed values of the constraints, given the joint probability distribution of past and future. For details, see <xref ref-type="sec" rid="sec011">Methods</xref>. A given solution to one problem will be also a solution to the other one: it will have the maximal predictive power among all models that have the same complexity, and will have the minimal complexity among all models that have the same predictive power.</p>
<sec id="sec003">
<title>Information Bottleneck Analysis of the Oddball Paradigm</title>
<p>In order to demonstrate the relevance of our approach to neuronal processing, we studied responses of single neurons in primary auditory cortex (A1) to oddball sequences. Oddball sequences are generated by selecting two stimuli (two pure tones in our case) and then forming a sequence composed of these two stimuli in which one of the two is common and the other is rare (<bold><xref ref-type="fig" rid="pcbi.1005058.g003">Fig 3A</xref></bold>). Usually, the overall number of times each of the stimuli occur in the sequence is fixed. We will however test here a slightly different statistical model, in which the probability of each of the tones is fixed. We will furthermore assume that successive tones are selected independently of each other with a given probability (‘Bernoulli sequences’). This assumption, which is a very good approximation to the experimental setting when the number of tones in the sequence is large, makes the application of the theory particularly transparent.</p>
<fig id="pcbi.1005058.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Neural responses in the oddball paradigm.</title>
<p>(<bold>a</bold>) Two examples of oddball stimuli blocks, one with <italic>p</italic> = 10% and the other with <italic>p</italic> = 90%. Different colors (blue and red) represent the two stimuli (‘low’ and ‘high’ tones of the oddball sequences). (<bold>b</bold>) Single-neuron responses in cat primary auditory cortex (A1) to auditory oddball stimuli. Each point shows the average spike counts of one neuron to the same stimulus (either low-tone or high-tone, represented by different colors) when presented in the <italic>p</italic> = 10% block versus the same physical stimulus in the <italic>p</italic> = 90% block. Note that most dots fall above the diagonal, indicating stronger response to rare stimuli.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g003" xlink:type="simple"/>
</fig>
<p>Neurons in auditory cortex are sensitive to stimulus probability in such sequences [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1005058.ref023">23</xref>]. The majority of neurons in cat auditory cortex produced a larger average response to the same tone frequency when it was rare than when it was common (<bold><xref ref-type="fig" rid="pcbi.1005058.g003">Fig 3B</xref></bold>). These responses often had substantial sustained components that could outlast stimulus duration (e.g. [<xref ref-type="bibr" rid="pcbi.1005058.ref024">24</xref>]), resulting in large spike counts within the counting window we used (0–330 ms after stimulus onset, for stimuli whose total duration was 230 ms, same as in [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref020">20</xref>]). We interpret these responses as encoding a prediction error: in the oddball sequences, the probability of the next stimulus to be either of the two frequencies is roughly given by the probability with which it occurred in the past. The prediction error, −log<sub>2</sub>(<italic>p</italic>), for the tone when rare is therefore larger than for the same tone when common. Thus, responses of neurons in auditory cortex seem to fit qualitatively the notion of prediction error as defined here.</p>
<p>Using the theory presented above, we aimed to quantify this intuition rigorously. We start by describing in substantial detail the family of optimal reduced representations in the case of the oddball sequences, and then we show how we used these representations to extract information about the memory duration and complexity of the reduced representation underlying the dependence of neuronal responses in auditory cortex on tone probability.</p>
<p>In the experiments, sequences with a number of different probability values have been used: each stimulus was used both as common (with probabilities of 90% and 70%) and as rare (with probabilities of 10% and 30%), as well as in equiprobable sequences. Formally, the stimulus sequence can be modeled as a Bernoulli process, where one stimulus is drawn with probability <italic>p</italic> (0&lt;<italic>p</italic>&lt;1) and the other with probability 1-<italic>p</italic>. We assume that the parameter <italic>p</italic> is drawn from a uniform distribution prior to generating each stimulus block. This assumption is not crucial–it changes the exact numerical values, but not the trends that will be discussed below, as long as all the tone probability conditions that occurred in the experiment are allowed under the prior. These assumptions determine the joint probability <italic>P</italic><sub><italic>N</italic></sub><italic>(past</italic>,<italic>future)</italic> characterizing the statistical structure of the stimulus sequence, where <italic>past</italic> denotes the sequence of the last <italic>N</italic> stimuli and <italic>future</italic> denotes the next stimulus in the sequence (see <xref ref-type="sec" rid="sec011">Methods</xref>). The memory duration, <italic>N</italic>, is a parameter of the model and will be discussed in more details below.</p>
<p>As we discussed above, the most detailed (and complex) representation in this case would consist of a perfect encoding of the past sequence by identifying each of the 2<sup><italic>N</italic></sup> possible configurations of the stimulation sequence with a unique state <italic>m</italic> (<bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref></bold>; case (i) is this representation for N = 4). This is, however, an unnecessarily detailed representation since the number of occurrences of the B tone among the last <italic>N</italic> is a sufficient statistic. The IB method identifies the relevant information for predicting the next stimulus: in our case, it assigns a single and unique state to all sequences that share the same number of occurrences of A tones and B tones <bold>(<xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A</xref>;</bold> case (ii)<bold>).</bold> This reduced representation filters out non-relevant details of the past (i.e. the exact order of the tones): it maintains the maximal predictive information possible (for a given <italic>N</italic>), but at a lower complexity (2.3 bits instead of 3.6 bits for <italic>N</italic> = 4). <bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2B</xref></bold> (gray line) displays the maximal predictive power at each complexity of the reduced representation for <italic>N</italic> = 4, and cases (i) and (ii) can be directly compared to each other.</p>
<p>Using the IB method enabled us to reduce the complexity of the representation even further. While representations that are simpler than the minimal sufficient statistic (case ii) do reduce the quality of the prediction of the next stimulus, the loss in predictive power may be relatively small. For example, for a memory of <italic>N</italic> = 4 tones, constraining the reduced representation complexity to 1 bit results in a ‘noisy’ representation with 3 unique states (<bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2A and 2B</xref></bold>, case (iii)). In this case, given the reduced representation alone, it is impossible to recover the exact number of B tones that occurred in the past (their exact order was already lost in the reduction to the minimal sufficient statistic). Nevertheless, this solution is optimal in the sense that its predictive power is maximal among all possible reduced representations with a complexity of 1 bit (for <italic>N</italic> = 4). For longer past durations and large complexities, the low sensitivity of predictive power to complexity is even more prominent. For a memory of <italic>N</italic> = 10 tones, the complexity reduction when using the sufficient statistic is larger than for <italic>N</italic> = 4, and further reducing the complexity of the representation by another 57% (from 3.46 to 1.49 bits) results in a loss of merely 12% in its predictive power (from 0.223 to 0.196 bits; <bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2B</xref></bold>, black).</p>
<p>We examined the tradeoff between the complexity and the predictive power of the reduced representation for memory durations from <italic>N</italic> = 1 to <italic>N</italic> = 50 (<bold><xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref></bold>) by applying the IB method to the probability distributions of the oddball paradigm <italic>P</italic><sub><italic>N</italic></sub><italic>(past</italic>,<italic>future)</italic>. Note that these probability distributions depend on N, the duration of the past. We used the IB algorithm to find, for each N, 200 reduced representations whose complexity spanned the full allowed range (see below), and whose predictive power was maximal for each value of the complexity. Each curve in <xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref> is produced by linearly interpolating between these 200 points. The 200 points, however, sample the relevant ranges so densely that the linear interpolation is imperceptible. Thus, each point along the curves shown in <xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref> corresponds to a unique optimal reduced representation: as described above, such reduced representation is given by the two conditional distributions <italic>P(m|past)</italic> and <italic>P(future|m)</italic>. The curves in <xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref> plot the predictive power as a function of the complexity for these 200 reduced representations (the N = 4 and N = 10 cases are the same lines plotted in <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2B</xref>). These convex curves describe quantitatively the tradeoff between complexity and predictive power. Their convexity is a general property of the tradeoff postulated by the IB principle (see [<xref ref-type="bibr" rid="pcbi.1005058.ref013">13</xref>]).</p>
<fig id="pcbi.1005058.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Tradeoff between complexity and predictive power in the oddball paradigm.</title>
<p>(<bold>a</bold>) Tradeoff curves calculated for different durations of the past sequences, from <italic>N</italic> = 1 (blue) to <italic>N</italic> = 50 (red). The curves corresponding to <italic>N =</italic> 4 and <italic>N =</italic> 10 are the same as those shown in <xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2</xref>. Each curve spans complexity values from 0 to that of the minimal sufficient statistic. While more complex representations exist, they cannot have higher predictive power. Each point along the curves represents one optimal solution (achieving maximal predictive power for its complexity constraint for the corresponding duration of the past). These curves separate achievable versus non-achievable combinations of complexity and predictive power (below versus above each curve, respectively). (<bold>b</bold>) The maximal predictive power that can be achieved as a function of the past duration <italic>N</italic>, for different constraints imposed on the complexity: 0.5 bit (light gray), 1 bits (dark gray), 2 bits (black). The dashed line corresponds to the predictive power of the minimal sufficient statistic, and is therefore the maximal predictive power at each past duration. Note the diminishing returns for increases in memory duration <italic>N</italic> (i.e. predictive power does not increase much beyond <italic>N</italic> = 10), as well as for increases in complexity (i.e. predictive power does not increase much beyond a complexity of 2 bits).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g004" xlink:type="simple"/>
</fig>
<p>For each <italic>N</italic>, complexity (abscissa of <xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref>) spans the range between 0 and the complexity of the sufficient statistic. The predictive power (ordinate in <xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref>) spans the range between 0 and the mutual information between past and future, which is equal to the mutual information between the sufficient statistic and the future. Thus, the rightmost point of each line shows the complexity of the sufficient statistic (abscissa) and the predictive power of the sufficient statistic (ordinate). More complex representations do exist (e.g. the full past), but do not result in an increased predictive power. As complexity is lowered below that of the sufficient statistic (moving leftward along a curve), predictive power is lost, but at least initially the loss of predictive power is rather minor.</p>
<p>The curves separate achievable and non-achievable combinations of complexity and predictive power. Any point on or below the curve is achievable, in the sense that there is (at least one) pair of conditional distributions <italic>P(m|past)</italic> and <italic>P(future|m)</italic> with the corresponding complexity and predictive power. Any point above the curve is non-achievable in this sense–there is no way to process the information from the past, keeping that level of complexity, and still get better predictions than those specified by the curve.</p>
<p>The dependence of optimal predictive power on <italic>N</italic>, the duration of the past, is illustrated in <bold><xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4B</xref></bold> for different constraints imposed on the complexity (0.5, 1, 2 bits and with no constraints). Two effects are readily apparent. First, for each level of complexity, increasing past duration much above <italic>N</italic> = 10 does not result in a major increase in predictive power. Second, as hinted above, although the complexity of the sufficient statistic for long memory duration may be as high as 4–5 bits (<bold><xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4A</xref></bold>), there is a strong effect of ‘diminishing returns’–complexity above 2 bits does not add substantial amount of predictive power to the reduced representation, even for a past duration of <italic>N</italic> = 50 stimuli (<bold><xref ref-type="fig" rid="pcbi.1005058.g004">Fig 4B</xref>)</bold>. One way of understanding these effects is by noting that the predictive power reflects the precision of the probability estimates of the B tone. While this precision increases with both <italic>N</italic> and complexity, beyond a certain point increasing the precision at which the probability of the B tone is known does not improve much the predictions anymore.</p>
</sec>
<sec id="sec004">
<title>Neuronal Representation of Prediction Error</title>
<p>To test the hypothesis that neurons in the auditory cortex represent the prediction error derived from a reduced representation of the recent past, we correlated the prediction errors, derived from the reduced representations we computed, with the neuronal responses to oddball sequences (<bold><xref ref-type="fig" rid="pcbi.1005058.g005">Fig 5</xref></bold>) [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref020">20</xref>]. We calculated separately the prediction errors for each one of the reduced representations we computed above (defined by memory duration N = 1 to N = 50 and 200 complexity values ranging from 0 to the maximum possible for each N, for a total of 200*50 = 10,000 reduced representations).</p>
<fig id="pcbi.1005058.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Neuronal representation of prediction error in the oddball paradigm.</title>
<p>(<bold>a</bold>) Spike counts of single trial responses of one neuron to one of the two frequencies with which it was tested, plotted as a function of the expected prediction error for the same trial. The prediction errors were computed using optimal reduced representations at three past durations (<italic>N</italic> = 5, <italic>N</italic> = 10 and <italic>N</italic> = 15) with a complexity of 1 bit. A small amount of jitter was added to the <italic>x</italic> coordinate for visualization purposes only. Colors correspond to the different experimental blocks (<italic>p</italic> = 10% in red; p = 50% in black; p = 90% in blue). Error bars indicate the mean and the 25<sup>th</sup> and 75<sup>th</sup> percentiles for each value of the prediction error. (<bold>b</bold>) Responses of three different neurons (plotted separately for the two tones, top and bottom panels) as a function of the expected prediction error for a past duration of <italic>N</italic> = 10 stimuli at a complexity of 2 bits. Responses of the top leftmost panel belong to the same neuron shown in panel <bold>a</bold>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g005" xlink:type="simple"/>
</fig>
<p>For each reduced representation, for each neuron in the dataset, and for each of the two frequencies with which the neurons were tested, we used the actual sequence of tone presentations used in the experiment to calculate a corresponding sequence of prediction errors. We followed the prescription described above. For each tone presentation along the sequence, we used the previous <italic>N</italic> tone presentations as the past. Each past leads to a state <italic>m</italic> of the reduced representation, with an associated estimate <italic>P(future|m)</italic> for the probability of the upcoming stimulus. The prediction error associated with that stimulus was −log<sub>2</sub> <italic>P</italic>(<italic>future</italic>|<italic>m</italic>). As explained above, the prediction error itself is a random variable. The transformation from past to reduced representation, <italic>P(m|past)</italic>, provided a set of probabilities to be in each of the states of the reduced representation, and through them to each of the possible prediction errors (see <xref ref-type="sec" rid="sec011">Methods</xref>). Therefore, instead of a single prediction error for each stimulus, we calculated a set of prediction errors, one for each possible state of the reduced representation <italic>m</italic>, together with their probabilities. To compare the prediction errors with the neuronal responses, we used linear regression of the neuronal responses against the prediction errors, weighted by the corresponding probabilities (see <xref ref-type="sec" rid="sec011">Methods</xref> for details). For illustration purposes, in <bold><xref ref-type="fig" rid="pcbi.1005058.g005">Fig 5</xref></bold> we plot neuronal responses (spike counts evoked by individual tone presentations, ordinate) against the average prediction error (−log<sub>2</sub> <italic>P</italic>(<italic>future</italic>|<italic>m</italic>) averaged over all possible values for the state <italic>m</italic> of the reduced presentation, each with its probability <italic>P</italic>(<italic>m</italic>|<italic>past</italic>)) for that same stimulus presentation.</p>
<p>In the experiments, neurons were tested using blocks composed of two tones whose frequency separation was about half octave (f<sub>high</sub>/f<sub>low</sub> = 1.44; this is the Δf = 0.37 case of [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref020">20</xref>]). The probabilities of the two tones in each block were fixed, but their order was random. For the main analysis (Figs <xref ref-type="fig" rid="pcbi.1005058.g005">5</xref> and <xref ref-type="fig" rid="pcbi.1005058.g006">6</xref>), we used a conservative counting window (0–330 ms after stimulus onset; tone duration was 230 ms and onset-to-onset interval was 730 ms), and we used the responses of all 68 neurons tested with 3 blocks in which the tone probabilities were 90%/10%, 50%/50%, and 10%/90% (for the low frequency and high frequency tones respectively). The use of responses from these 3 blocks ensured sampling of the full range of relevant values of prediction errors. The analysis was performed separately for each frequency, resulting in 136 combinations of neuron and frequency, with 117/136 combinations showing responses to the corresponding tone that were significantly larger than the spontaneous rate.</p>
<fig id="pcbi.1005058.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Population analysis.</title>
<p>(<bold>a</bold>) Histogram of the best goodness-of-fit scores <inline-formula id="pcbi.1005058.e003"><alternatives><graphic id="pcbi.1005058.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> over the entire population (<italic>n</italic> = 117 combinations of 68 neurons × 2 test frequencies that evoked significant responses). Significant scores (permutation test, <italic>p</italic>&lt;0.05; see <xref ref-type="sec" rid="sec011">Methods</xref>) are indicated in red (<italic>n =</italic> 78/117, 67%). (<bold>b</bold>) Scatter plot of the largest fractions of explained variance achieved for the two frequencies tested with each neuron in the main analysis (<italic>n</italic> = 68). Neurons with significant <inline-formula id="pcbi.1005058.e004"><alternatives><graphic id="pcbi.1005058.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> in both tested frequencies are indicated in red while neurons with significant <inline-formula id="pcbi.1005058.e005"><alternatives><graphic id="pcbi.1005058.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> in only one frequency are indicated in gray. (<bold>c</bold>) Two-dimensional, color-coded population analysis histograms of the complexity and predictive power underlying the ‘good representations’ (i.e., reduced representations that achieved at least 90% of <inline-formula id="pcbi.1005058.e006"><alternatives><graphic id="pcbi.1005058.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>). Analysis was performed over combinations of neuron and frequency with <inline-formula id="pcbi.1005058.e007"><alternatives><graphic id="pcbi.1005058.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:mn>0.1</mml:mn></mml:math></alternatives></inline-formula> (<italic>n</italic> = 34/117). Color scale (from blue to red) represents the fraction of neurons for which that combination was ‘good’. An abundance of 100% (red color) means that this combination of parameters was in the ‘good’ set of parameters for each and every neuron in the analyzed population. (<bold>d</bold>) Same analysis as in panel <bold>c</bold>, results are shown as a function of past duration and complexity. The 50% contour in the plot is marked by the white line. <bold>(e)</bold> Same analysis as in panel <bold>c</bold>, results are shown as a function of past duration and predictive power. Panels <bold>d</bold> and <bold>e</bold> use the same color-code as panel <bold>c</bold>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g006" xlink:type="simple"/>
</fig>
<p>Prediction error was significantly correlated with neuronal responses in many cases. <bold><xref ref-type="fig" rid="pcbi.1005058.g005">Fig 5A</xref></bold> compares responses of one neuron (spike counts evoked by single presentations of one of the two frequencies with which it was tested) with the expected prediction errors calculated for each tone presentation using three different optimal reduced representations. These representations had a complexity of 1 bit and past durations of <italic>N</italic> = 5, <italic>N</italic> = 10 and <italic>N</italic> = 15 stimuli. Each block had 400 stimuli, of which 90% (360), 50% (200) or 10% (40) consisted of the relevant frequency. To avoid using stimuli whose past sequence was not well defined, the first 50 stimuli of each block were removed from the analysis. Due to the randomized nature of the sequences, for each combination of neuron and test frequency we ended with 311–322 stimuli in the p = 90% condition (blue points), 168–182 stimuli in the p = 50% condition (black points), and 28–39 stimuli in the p = 10% condition. As the tones presented in the 10% condition were, by definition, rare, they were associated with large prediction errors, and therefore the red points are mostly concentrated at the right of the scatter plots. Similarly, the tones presented in the 90% condition were, by definition, common, and were associated with small prediction errors. Therefore the blue points are mostly concentrated at the left of the scatter plots. On the other hand, for the 50% condition, prediction errors varied quite substantially, leading to a larger dispersion along the abscissa between (and partially overlapping) the two extremes.</p>
<p>While the correlation with the prediction errors corresponding to <italic>N</italic> = 10 was slightly larger than for shorter or longer memory durations, this example mainly illustrates the finding that the correlation between the prediction error and neuronal responses was sometimes only weakly dependent on past duration, a finding we will return to later. To illustrate the range of goodness of fit that could be achieved, <xref ref-type="fig" rid="pcbi.1005058.g005">Fig 5B</xref> compares single-trial responses of three neurons (columns; the rows show the two frequencies used for testing each neuron) versus their expected prediction errors, calculated using reduced representations with complexity of 2 bits and past duration of <italic>N</italic> = 10 stimuli. Neurons 1 and 2 had strong dependence of their responses on prediction error, while Neuron 3 was typical for the data.</p>
<p>To quantify these observations for the entire data set, we performed weighted linear regression analysis, separately for each combination of a neuron and frequency that evoked significant responses (<italic>n</italic> = 117 combinations), using the entire set of pre-calculated optimal solutions. The distribution of the resulting best goodness-of-fit scores (<inline-formula id="pcbi.1005058.e008"><alternatives><graphic id="pcbi.1005058.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) is displayed in <bold><xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6A</xref></bold>.</p>
<p>We tested the significance of the fit separately for each combination of neuron and frequency. The goodness of fit scores were calculated as the maximum <italic>r</italic><sup>2</sup> over all possible reduced representations, so that standard significance tests could not be applied due to the inherent multiple comparisons involved. We therefore tested the significance of a fit using a permutation test. We repeated the same analysis for the measured spike counts but replacing the stimuli by 20 randomly permuted sequences with the same tone probabilities, thus breaking the short-term relationships between responses and associated prediction errors. The effect of prediction error on the neuronal responses was considered to be significant (<italic>p</italic>&lt;0.05) if <inline-formula id="pcbi.1005058.e009"><alternatives><graphic id="pcbi.1005058.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for the actual stimulation sequence was larger than <inline-formula id="pcbi.1005058.e010"><alternatives><graphic id="pcbi.1005058.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> calculated using 20 different random permutations. The prediction error significantly influenced the neuronal responses in about 2/3 of cases (78/117 combinations, 67%; see <bold><xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6A</xref></bold>, red bars), supporting our hypothesis that auditory cortical neurons represent this formal notion of prediction error in their responses.</p>
<p>Most neurons contributed to <bold><xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6A</xref></bold> twice, so the figure could overstate the degree to which prediction error and neuronal responses are related to each other. <bold><xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6B</xref></bold> is a scatter plot of the goodness of fit values for the two frequencies, with the corresponding histograms for the two frequencies separately. There was a mild correlation between the goodness of fit values for the two frequencies (r = 0.34, n = 68, <italic>p</italic>&lt;0.05, using the normal approximation to the Fisher z-transformed correlation), suggesting that the representation of prediction error in the neuronal responses was at least to some degree a property of the neuron itself. Considering each frequency by itself, 41/62 neurons that responded significantly to the low frequency and 37/55 of those that responded significantly to the high frequency showed a significant dependence of neuronal responses on prediction error. More conservatively, 52 neurons had significant responses to both frequencies, and 27 of these neurons showed significant dependence of the responses to both frequencies on prediction error. Thus, depending on the criterion, between 1/2 and 2/3 of the neurons showed significant dependence (<italic>p</italic>&lt;0.05) of the neuronal responses on prediction error.</p>
<p>In order to characterize the past duration and the complexity of the representations that best fitted the neuronal responses, we analyzed the parameters of these representations that best explained the data. For this analysis we used only combinations of neuron and test frequency with explanatory power of <inline-formula id="pcbi.1005058.e011"><alternatives><graphic id="pcbi.1005058.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:mn>0.1</mml:mn></mml:math></alternatives></inline-formula> (adjusted correlation coefficient &gt; 0.33, <italic>n</italic> = 34/117, see <xref ref-type="sec" rid="sec011">Methods</xref>). We found that the dependence of the goodness of fit scores on the parameters (memory duration and complexity) could be rather weak once either past duration or complexity were large enough, so that many different models achieved approximately the same goodness of fit. Therefore, we defined, for each combination of neuron and test frequency, a set of <italic>good representations</italic> consisting of those that achieved goodness-of-fit scores higher than an (admittedly somewhat arbitrary) threshold: <inline-formula id="pcbi.1005058.e012"><alternatives><graphic id="pcbi.1005058.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>≥</mml:mo><mml:mn>0.9</mml:mn><mml:mo>∙</mml:mo><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>. The parameters of these ‘good representations’ (past duration, complexity and predictive power) are summarized in two-dimensional, color-coded histograms (<bold>Figs <xref ref-type="fig" rid="pcbi.1005058.g006">6C–6E</xref></bold>). For each reduced representation, the histograms show the fraction of cases for which that specific representation belonged to the set of good representations. The reduced representations are index in three different ways (by complexity and predictive power, <xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6C</xref>; complexity and duration, <xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6D</xref>; predictive power and duration, <xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6E</xref>). Thus, <xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6D</xref> shows that reduced representations with duration <italic>N</italic> &lt; 10 stimuli were outside the set of good representations for most cases. Thus, for a reduced representation to be good for a large fraction of cases, it had to have a relatively long past duration, <italic>N</italic> ≥ 10 stimuli (corresponding to 7.3 seconds or longer). Similarly, <xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6D</xref> shows that for a reduced representation to be good for a large fraction of cases, it could have a complexity as low as about 2–3 bits (white line in <xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6D</xref>)–reduced representation with lower complexities tended to be outside the set of good representations for most cases. Moreover, although the resulting reduced representations were relatively coarse (low complexity) their predictive power almost matched the predictive power attainable with models that have maximal complexity (<xref ref-type="fig" rid="pcbi.1005058.g006"><bold>Fig 6C</bold> and <bold>6E</bold></xref>; compare with <xref ref-type="fig" rid="pcbi.1005058.g004"><bold>Fig 4A</bold> and <bold>4B</bold></xref>). Thus, the good reduced representation with shortest memory tended to have a long memory (N&gt;10), coarse (complexity of 2–3 bits relative to maximal complexity of ~5 bits), but keep a high degree of predictive power.</p>
</sec>
<sec id="sec005">
<title>Controls and Extensions</title>
<sec id="sec006">
<title>Prediction errors and single-block trial-by-trial variability</title>
<p>The previous analysis demonstrated that prediction error had a substantial effect on the responses of many neurons. However, the responses were collected from three different experimental blocks that had different overall probability for the tone, and therefore different levels of typical prediction error. Since we have already shown that the average responses depended on overall probability (<bold><xref ref-type="fig" rid="pcbi.1005058.g002">Fig 2</xref></bold>), the correlation between responses and prediction errors could be solely due to the difference between the average responses to the same tone in the three blocks in which it was tested, and not to trial-by-trial fluctuations of the prediction error within one block.</p>
<p>To verify that trial-by-trial response variability reflected variation in the prediction error, we repeated the above analysis confined to individual blocks. Neurons were tested in five probability conditions: 10%, 30%, 50%, 70% and 90%. All neurons in the dataset (n = 99) were tested in the 10% and in the 90% conditions. A subset (n = 68) was also tested in the 50%/50% condition (these are the neurons used in the main analysis, <bold>Figs <xref ref-type="fig" rid="pcbi.1005058.g005">5</xref></bold> and <bold><xref ref-type="fig" rid="pcbi.1005058.g006">6</xref></bold>), and a subset of those (n = 29) was tested in two additional probability conditions, 70%/30% and 30%/70%.</p>
<p>For interpreting the results of these analyses, it should be kept in mind that the range of prediction errors considered in the main analysis was larger than the range of prediction errors within each block. For example, in the 90%/10% block, the probable tone was not very surprising–it occurred on average 90% of the time in each segment of <italic>N</italic> stimuli. Because of the random nature of the sequence, there were nevertheless small fluctuations in the overall number of the two tones within each block of <italic>N</italic> stimuli, leading to small fluctuations in the prediction error. On the other hand, the variability of the responses, conditioned on the prediction error computed from the reduced representations (essentially the width of the scatter around the regression lines in the examples of <xref ref-type="fig" rid="pcbi.1005058.g005">Fig 5</xref>), remained roughly the same within block as it was across blocks. Thus, the fraction of explained variance of the responses by the prediction error (and therefore the significance of the dependence of the responses on prediction error) was expected to be smaller in the within-block analysis than in the main analysis. Formally, assume <italic>Y</italic> = <italic>aX</italic> + <italic>b</italic> + <italic>n</italic>, where <italic>Y</italic> are the responses, <italic>X</italic> the prediction errors, <italic>a</italic> and <italic>b</italic> are the regression coefficients (assumed perfectly known here for simplicity), and <italic>n</italic> the noise around the regression line, usually assumed to be independent of <italic>X</italic>. The explained variance is
<disp-formula id="pcbi.1005058.e013">
<alternatives>
<graphic id="pcbi.1005058.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
so that the explained variance is a monotonic function of var(<italic>X</italic>). In the within-block analysis, var(<italic>X</italic>) is smaller than in the main analysis, and therefore the explained variance is expected to decrease.</p>
<p>In the equiprobable block the prediction error spanned the largest range of values (var(<italic>X</italic>) was largest in the notation above). The <inline-formula id="pcbi.1005058.e014"><alternatives><graphic id="pcbi.1005058.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> scores of each combination of neuron and frequency in the single-block analysis are plotted in <bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7A</xref></bold> against the corresponding value for the main analysis (that included in addition the 10%/90% and 90%/10% blocks). Only combinations of neuron and frequency with significant neuronal responses within the equiprobable block are shown in this analysis (n = 110/136 combinations). Significant correlations (<italic>p</italic>&lt;0.05, see <xref ref-type="sec" rid="sec011">Methods</xref>) of prediction errors and neuronal responses were found in 34/110 combinations of neurons and frequencies with significant responses (31%; see <bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7A</xref></bold>, red dots). As expected, the fraction of explained variance was generally smaller for the responses restricted to the equiprobable block than for the corresponding data in the main analysis.</p>
<fig id="pcbi.1005058.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Controls and extensions of the main analysis.</title>
<p>(a) Comparison of the best goodness-of-fit scores achieved for each combination of neuron and test frequency that had significant response in the ‘equiprobable’ block (p = 50%) and the goodness-of-fit score of the same neuron and frequency in the main analysis (using all responses with p = 10%, 50% and 90%). Significant scores in the equiprobable block are indicated in red (permutation test, <italic>p</italic>&lt;0.05). (b) Fraction of cases with significant effects of prediction error on the neuronal responses for all the single-block analyses. (c) Fraction of cases with significant effects of prediction error on the neuronal responses in different time windows (marked below the histogram) and for different selection of blocks. (d) Comparison of the fraction of explained variance (<inline-formula id="pcbi.1005058.e015"><alternatives><graphic id="pcbi.1005058.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) and the fraction of explainable variance. The red points corresponds to correction by a noise estimates using unbiased variances, while the green points correspond to the conservative correction by the (smaller) noise estimates using the biased variances. (e) Comparison of SSA indices (SI) and explained variance (<inline-formula id="pcbi.1005058.e016"><alternatives><graphic id="pcbi.1005058.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>). Cases with significant correlation (permutation test, <italic>p</italic>&lt;0.05) are indicated by blue (low tones) or red (high tones).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.g007" xlink:type="simple"/>
</fig>
<p>We performed the single-block analysis for all probability conditions. <bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7B</xref></bold> shows the fraction of combinations of neuron and frequency that showed a significant effect of prediction error on the neuronal responses (out of the combinations with significant responses in each condition). The substantial number of cases with significant dependence of neuronal responses on prediction error confirms that prediction error could account for trial-by-trial fluctuations within blocks as well as across blocks. The larger number of significant cases found in the mid-range probability conditions may be due to the larger range of values spanned by the prediction error in these blocks, increasing the power of the statistical test.</p>
</sec>
<sec id="sec007">
<title>Prediction error and response time course</title>
<p>Ulanovsky et al. [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref020">20</xref>] showed that the dependence of the responses on context developed over time–it was maximal about 100 ms after stimulus onset, and somewhat weaker around stimulus onset and offset. The analysis window used here (0–330 ms after stimulus onset) contained all response components, including the onset, sustained and offset components. We therefore repeated the same analyses using different time windows for counting spikes. In all cases, combinations of neurons and tones that had significant responses in that time window were selected for statistical evaluation. <bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7C</xref></bold> displays the fraction of combinations of neuron and frequency that showed significant effects of prediction error in the different time windows, for various combinations of probability conditions. Gray bars correspond to the use of the 90%/10%, 50%/50% and 10%/90% blocks as in the main analysis. The fraction of cases that showed significant effects of the prediction error on neuronal responses when using these blocks was mostly between 0.6 and 0.7, except for the offset time window (260–330 ms after stimulus onset, stimulus duration was 230 ms). Still, even for this late time window, over half the cases (71/117) had significant responses, and in about 40% of these cases, there were significant effects of prediction error on the neuronal responses. The average fraction of explained variance in the late time window was, however, significantly smaller than while the stimulus was on, even after selecting only cases with significant effects of prediction error (linear mixed effects model, time window with random unit effect; effect of time window: F(3,215) = 5.66, p = 0.00095; post hoc comparisons with <italic>p</italic>&lt;0.05). We repeated this analysis using the 10% and 90% conditions only (<bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7C</xref></bold>, blue bars) and also using the all probability conditions– 10%, 30%, 50%, 70% and 90% (<bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7C</xref></bold>, red bars), with a very similar pattern of results.</p>
</sec>
<sec id="sec008">
<title>Effects of noise variance</title>
<p>We address here the observation that even when prediction error significantly modulated the responses, the fraction of explained variance accounted for by the prediction errors was often rather small. We return here to the conditions of the main analysis: using only three probability conditions (10%, 50% and 90%) with the full counting window (0–330 ms after stimulus onset).</p>
<p>The fraction of explained variance is the ratio between the predictor variance (that part of the overall variability of the spike counts that is accounted for by the linear fit to the prediction errors) and the overall variance of the spike counts. We used an approach introduced by Sahani and Linden [<xref ref-type="bibr" rid="pcbi.1005058.ref025">25</xref>] and recently used by others [<xref ref-type="bibr" rid="pcbi.1005058.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref027">27</xref>]. In this approach, the overall variance is partitioned into two parts. One part is the noise variance–the variability of the spike counts that cannot be accounted in any way by the experimental manipulation. The other part is the explainable variance (‘signal power’ in [<xref ref-type="bibr" rid="pcbi.1005058.ref025">25</xref>]), which consists of those aspects of the spike counts that are reproducible given the stimulation sequence. The fraction of explained variance may be small because the noise variance is large rather than because the model is inadequate. Thus, for assessing the quality of a model, it makes sense to compare the explained variance with the explainable variance only, ignoring the noise, which is experimentally uncontrolled. Such calculation produces a ‘fraction of explainable variance’, in contrast with the fraction of explained variance, <inline-formula id="pcbi.1005058.e017"><alternatives><graphic id="pcbi.1005058.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, used throughout the rest of the paper.</p>
<p>Since we study the effect of stimulus context on neuronal responses, the noise variance is the variance of the evoked spike counts conditioned on the exact sequence of previous stimuli. Such noise variance is often computed from the responses evoked by the same long stimulus sequence presented many times (e.g. Sahani and Linden 2003). Unfortunately, such data were unavailable here. We therefore used an approximation.</p>
<p>We really have to estimate the variance of the spike counts conditioned on the previous long stimulus sequence (e.g. 50 stimuli back, as long as the longest memory duration we used for calculating the prediction errors). There are however 2<sup>50</sup> ≈ 10<sup>15</sup> such sequences, and each neuron was tested with only about 10<sup>3</sup> individual tone presentations (distributed over separate experimental blocks), each of which sampled one of the possible sequences of preceding 50 stimuli. This sparse sampling made it impossible to directly estimate noise variance.</p>
<p>Instead of conditioning the variance on the 50 preceding stimuli, we conditioned the variance on the preceding 7 stimuli, resulting in at most 128 different past sequences, but in addition we conditioned on the block in which the sequence occurred. By conditioning on the block, we took at least partially into account the differences in the more remote past between the same short (7 stimulus long) contextual sequences as they occurred in different probability conditions. We ignored the initial 50 stimuli from each block, since they were omitted from the main analysis. Our estimate of the noise was a weighted average of the variances of each such set of responses (the ‘conditional variance’, with identical preceding sequence of 7 stimuli, computed for each probability condition separately). The weights were the number of responses used to calculate each conditional variance, so that conditional variances that were estimated better (from more responses) had proportionately greater representation in the estimate of the noise. Finally, we calculated the fraction of explainable variance by dividing the predictor variance by the explainable variance, calculated by subtracting the estimate of the noise variance from the overall variance of the spike counts.</p>
<p>We used two estimates of the conditional variance. The first used the unbiased variance estimated for each set of responses (sum of squared deviations from the mean of the set, divided by n-1). To be conservative, we also used the biased estimate of the variance (dividing the sum of squared deviations from the mean by n rather than by n-1). This reduced the estimates of the conditional variances, and ensured that the estimated noise variance was always smaller than the overall variance (which is not always the case when using the unbiased estimate of the variance).</p>
<p><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7D</xref> shows a scatter plot of the fraction of explained variance (<inline-formula id="pcbi.1005058.e018"><alternatives><graphic id="pcbi.1005058.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> as used previously, abscissa) and the fraction of explainable variance (ordinate). The red points corresponds to the use of the unbiased conditional variances, while the green points correspond to the use of the (smaller) noise estimates using the biased conditional variances. Some of the corrected values using the unbiased variance estimates were negative (markers at the bottom of the plot) or greater than 1 (markers at the top of the plots). Negative estimates correspond to cases in which the unbiased estimates of the noise were larger than the actual variance of the counts. Estimates of fraction of explainable variance that were larger than 1 correspond to cases in which the noise estimate was smaller than the overall noise variance (as it should be) but the remainder was smaller than the predictor variance, suggesting that the noise estimates in these cases also were too large. When using unbiased noise estimates, such cases of over-correction are expected to occur. On the other hand, the conservative estimates of fraction of explainable variance were highly correlated with the uncorrected estimates and all of them were positive and smaller than 1, suggesting that the conservative noise estimates consistently underestimated the true noise variance.</p>
<p>Even when under-corrected (green points), the fraction of explainable variance was consistently larger than the uncorrected fraction of explained variance, showing that noise variance had a substantial effect, weakening the strength of the linear relationships between prediction error and neuronal responses. <xref ref-type="table" rid="pcbi.1005058.t001">Table 1</xref> summarizes the results.</p>
<table-wrap id="pcbi.1005058.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005058.t001</object-id>
<label>Table 1</label> <caption><title>Comparing fractions of explained and explainable variance.</title></caption>
<alternatives>
<graphic id="pcbi.1005058.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005058.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Fraction of:</th>
<th align="justify">Explained variance</th>
<th align="justify">Explainable variance, conservative</th>
<th align="justify">Explainable variance, unbiased</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify">&gt;0.1, significant</td>
<td align="justify">34/78, 44%</td>
<td align="justify">73/78, 94%</td>
<td align="justify">74/78, 95%</td>
</tr>
<tr>
<td align="justify">&gt;0.1, non-significant</td>
<td align="justify">0/39, 0%</td>
<td align="justify">3/39, 7.7%</td>
<td align="justify">17/39, 44%</td>
</tr>
<tr>
<td align="justify">&gt;0.64, significant</td>
<td align="justify">1/78, 1.3%</td>
<td align="justify">16/78, 21%</td>
<td align="justify">54/78, 69%</td>
</tr>
<tr>
<td align="justify">&gt;0.64, non-significant</td>
<td align="justify">0/39, 0%</td>
<td align="justify">0/39, 0%</td>
<td align="justify">4/39, 10%</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Significant and non-significant refer to the effect of prediction error; there were 78/117 cases with significant effects, 39/117 with non-significant effects.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>For both correction schemes, almost all cases with significant effects of prediction error achieved a fraction of explainable variance greater than 0.1 (correlation coefficient of 0.33). The unbiased correction had also a large increase in the number of cases with non-significant effects of prediction error that achieved a fraction of explainable variance greater than 0.1; we don’t believe that these cases are necessarily significant–rather, this increase probably reflects the weaker statistical stability of this scheme.</p>
<p>More importantly, both schemes increased substantially the number of cases with large fraction explainable variance. We use explainable variance &gt; 0.64 (correlation coefficient of 0.8) as an arbitrary cutoff, but the pattern of results does not depend on this specific cutoff. Only one case achieved larger explained variance (with no correction for noise variance). However, 16 and 54 cases (out of 78) achieved a fraction of explainable variance larger than 0.64 using the two correction schemes. Only 0 and 4 (out of 39) non-significant cases achieved such high fraction of explainable variance. These calculations suggest that in more than half the cases, the fraction of explainable variance by a linear fit to the prediction error was substantial (at least greater than 0.5).</p>
<p>Nevertheless, even in the best cases, the fraction of explainable variance didn’t seem to fully saturate by the linear fits to the prediction errors. The failure to saturate the explainable variance could reflect the existence of non-linear relationships between prediction errors and neuronal response. For example, a sigmoidal or a threshold-linear relationship could fit better the data. Because of the additional complexity of these models, we did not explore them further here.</p>
</sec>
<sec id="sec009">
<title>Prediction errors and SSA indices</title>
<p>In the large majority of studies of stimulus-specific adaptation in the auditory system, the strength of the adaptation is quantified using the contrast between average standard and deviant responses (e.g. Ulanovsky et al. 2003, Reches and Gutfreund 2008, Malmierca et al. 2009), called the SSA index or SI. The goodness of fit used here, measured by <inline-formula id="pcbi.1005058.e019"><alternatives><graphic id="pcbi.1005058.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, is a different measure of the difference between the responses to standards (with small prediction errors) and deviants (with large prediction errors). However, these two measures take into account somewhat different aspects of the responses–while the SI depends only on the average responses to standards and deviants, <inline-formula id="pcbi.1005058.e020"><alternatives><graphic id="pcbi.1005058.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is also sensitive to the variability around these means. In consequence, a large SI may come with a small <inline-formula id="pcbi.1005058.e021"><alternatives><graphic id="pcbi.1005058.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> because response variability is large, and a small SI may come with a large <inline-formula id="pcbi.1005058.e022"><alternatives><graphic id="pcbi.1005058.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> in case the variability in the responses is much smaller than the difference between the standard and deviant responses.</p>
<p><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7E</xref> is a scatter plot of the SI (abscissa) and <inline-formula id="pcbi.1005058.e023"><alternatives><graphic id="pcbi.1005058.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> (ordinate), illustrating these general comments. Thus, for example, cases with SI&gt;0.6 had <inline-formula id="pcbi.1005058.e024"><alternatives><graphic id="pcbi.1005058.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> spanning the range from very small (below 0.1) to the largest achieved in this sample (close to 0.7). Similarly, cases with reasonable large <inline-formula id="pcbi.1005058.e025"><alternatives><graphic id="pcbi.1005058.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, for example larger than 0.4, had SIs spanning the range from about 0.3 to 1. In conclusion, neither the SI nor the <inline-formula id="pcbi.1005058.e026"><alternatives><graphic id="pcbi.1005058.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> fully characterize the context sensitivity of a neuron–both supply useful, to some extent independent, information about it.</p>
</sec>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>We present here an information theoretic formulation for the problem of sensory perception and prediction in the brain. We suggest that the representation of past stimuli reflects a tradeoff between the complexity of the reduced representation and its predictive power. The prediction errors, calculated from the statistics of the stimulation sequence using first principles, were significantly associated with the measured neuronal responses.</p>
<p>Importantly, these results suggest that the relevant time scale for the calculation of the prediction error is very long relative to the time scale usually considered in sensory coding–on the order of several seconds or longer. Our results therefore suggest that neurons in auditory cortex rely on surprisingly long time scales to calculate prediction errors, although the representations of the past, which underlie the computation of the prediction errors over these time scales, were found to be rather coarse.</p>
<p>Attneave [<xref ref-type="bibr" rid="pcbi.1005058.ref028">28</xref>] and Barlow [<xref ref-type="bibr" rid="pcbi.1005058.ref029">29</xref>] already suggested that neural information processing might follow principles of information theory. Information theory provides universal bounds on the minimal expected prediction error that can be achieved, independent of other assumptions on the implementation of a predictive process by the brain. In this sense the theory is normative: it specifies absolute bounds that cannot be improved in any way, and that are achieved by specific reduced representations of the past sequence. These bounds are governed by only two parameters, the complexity of the reduced representation (or, alternatively, its predictive power) and the duration of the past memory used for perception.</p>
<p>The notion of predictive coding is not new. The importance of the interplay between the representation complexity and the accuracy of predictions has been noticed before (e.g. [<xref ref-type="bibr" rid="pcbi.1005058.ref012">12</xref>]). In recent years, the tradeoff has been used extensively by Friston [<xref ref-type="bibr" rid="pcbi.1005058.ref030">30</xref>]. Friston used an approximation to the exact Bayesian inference problem, which is very difficult. Our approach circumvent this difficulty: crucially, it differs from previous attempts to formally use the interplay between complexity and prediction quality in that the prediction error in our formulation is derived from a reduced representation rather than directly from the explicit past stimulus sequence. Most importantly, one unique aspect of our approach is that we use a model-independent (information theoretic) bound.</p>
<p>In spite of the generality of this approach, we could apply it to real experimental situations: we show how to use it for studying rigorously the neuronal code in one simple case. It turns out that the neural responses reflect the prediction error derived from efficient reduced representations, allowing us to extract bounds on the duration of the memory that underlie the observed neuronal responses as well as on its complexity: memory duration is remarkably long (longer than 10 stimuli back) but rather coarse (with a complexity less than 2 bits).</p>
<p>The theory presented here can be considered as part of a more general theory of neural function. Since the reduced representation is assumed to be internal to the organism, its complexity may be related to the metabolic cost required to maintain and update it. As we illustrated in the case of the oddball sequences, less complex representations in our sense are also simpler to implement biologically–they require a smaller number of states, and incorporate noisy assignments of past sequences to states of the representation. On the other hand, constraints on expected prediction error may stand for general constraints on future value. Thus, our theory can be seen as an application of the general principle that organisms attempt to minimize metabolic or other costs subject to future value constraints. This principle unifies most known learning and control theoretic models, potentially linking information theoretic measures with general biological constraints directly [<xref ref-type="bibr" rid="pcbi.1005058.ref031">31</xref>].</p>
<p>Our approach provides a principled way to study long-term dependencies in neuronal responses, which are inaccessible to many models of auditory responses [<xref ref-type="bibr" rid="pcbi.1005058.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1005058.ref034">34</xref>]. These models relate the neuronal responses with the preceding content of the stimulus within a short time window (typically 50–100 ms). Attempts to include context in such models end up with analyzing relatively short-term contextual effects, from a few ms [<xref ref-type="bibr" rid="pcbi.1005058.ref016">16</xref>] to a few tens of ms [<xref ref-type="bibr" rid="pcbi.1005058.ref035">35</xref>] and up to a few hundreds of ms [<xref ref-type="bibr" rid="pcbi.1005058.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref037">37</xref>]. Only a few attempts to quantify longer-range dependencies have been published. For example, Ulanovsky et al. [<xref ref-type="bibr" rid="pcbi.1005058.ref020">20</xref>] demonstrated (in a previous analysis of the data presented here) that the larger responses to rare tones could not be accounted for by taking into account only a recent past (<italic>N</italic> ≤ 4 preceding stimuli, 3 seconds). They concluded that the larger responses to rare stimuli depended on longer segments of the stimulation sequence, although the effective memory duration and its content were left unspecified in these studies. Similarly, studies that fitted more mechanistic models of synaptic depression to such data [<xref ref-type="bibr" rid="pcbi.1005058.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref023">23</xref>] concluded that such models cannot fully account for the data, but such conclusions were limited by the restricted range of models that have been considered.</p>
<p>Our main experimental observation in this paper is that the dependence of neuronal responses on prediction error calculated over long time scales can be made precise. This observation supports the notion that auditory cortex neurons carry a prediction error signal. Furthermore, we specify for the first time potential properties of the reduced representation underlying auditory cortical responses to oddball sequences at such long time constants. In particular, we demonstrate that the neuronal responses are compatible with reduced representations that have long memory duration but low complexity.</p>
<p>Importantly, the descriptive power of our theory is not restricted to statistically simple sequences such as the oddball paradigm presented here. The full power of the theory resides in more complex stimulation sequences that may lack sufficient statistics (e.g. [<xref ref-type="bibr" rid="pcbi.1005058.ref021">21</xref>]). Of particular interest are the statistical regularities related to the syntax and semantics of language and music, which span multiple temporal scales. The tools developed here allow for a formal examination of the sensitivity of neurons to the complex statistical regularities of real-world soundscapes and therefore present a broad framework for characterizing sensory perception both qualitatively and quantitatively.</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec012">
<title>Electrophysiology and Stimulus Presentation</title>
<p>The neuronal responses are those described in [<xref ref-type="bibr" rid="pcbi.1005058.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005058.ref020">20</xref>], which also contain the detailed experimental methods. In brief, extracellular recordings were made in primary auditory cortex (A1) of halothane-anesthetized cats. Anesthesia was induced by Ketamine and Xylazine and maintained with halothane using standard protocols authorized by the committee for animal care and ethics of the Hebrew University—Hadassah Medical School. Single units were spike sorted on-line using template-based sorting, and in most cases they were also sorted off-line, to improve unit isolation. Stimuli were presented to the animal through sealed, calibrated earphones. For the oddball paradigm, two frequencies were selected close to the best frequency of the neuron, with a frequency ratio f<sub>high</sub>/f<sub>low</sub> = 1.44. Ulanovsky et al. (2003; 2004) defined the frequency difference slightly differently, and this is their Δf = 0.37 condition. The two frequencies were presented in a number of blocks. Each block contained 400 pure tone stimuli of identical duration (230 ms), inter-stimulus interval (736 ms onset to onset) and tone level (approximately 40 dB above the neuron’s minimal threshold). The blocks differed by the number of times each frequency was presented. For example, in the 90%/10% block, 360 of the stimuli had frequency f<sub>low</sub> and 40 had frequency f<sub>high</sub>, presented using a random permutation. We also used blocks with probabilities 70%/30%, 50%/50%, 30%/70%, and 10%/90%. The dataset was composed of 99 neurons tested in the 90%/10% and the 10%/90% cases. Of these, 68 were also tested in the 50%/50% (these are the neurons used in the main analysis); and of those 68 neurons, 29 neurons were additionally tested in the 70%/30% and 30%/70% conditions.</p>
</sec>
<sec id="sec013">
<title>The Information Bottleneck (IB) Method</title>
<p>For a detailed presentation of the IB principle &amp; algorithm see Tishby et al. [<xref ref-type="bibr" rid="pcbi.1005058.ref013">13</xref>]. In short, given a joint distribution <italic>P(x</italic>,<italic>y)</italic>, the IB finds a <italic>compressed</italic> representation of <italic>x</italic> denoted by <italic>m</italic> that is most <italic>informative</italic> on the target variable <italic>y</italic>. The compression of the representation is quantified by the mutual information between <italic>x</italic> and <italic>m</italic>, given by <italic>I(x;m)</italic>, and the information that <italic>m</italic> carries on the target variable <italic>y</italic> is quantified by <italic>I(m;y)</italic>. Using the IB algorithm we effectively pass the information that <italic>x</italic> provides about <italic>y</italic> through a ‘bottleneck’ formed by the reduced representation <italic>m</italic>, defined by <italic>P(m|x)</italic>. In practice, the reduced representation is determined by minimization of the Lagrangian, <inline-formula id="pcbi.1005058.e027"><alternatives><graphic id="pcbi.1005058.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mi mathvariant="script">L</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> with respect to <italic>P(m|x)</italic>. The positive Lagrange multiplier <italic>β</italic>, associated with the constraint on <italic>I(m;y)</italic>, controls smoothly the tradeoff between preserving relevant information and the compactness of the representation. The optimal assignment that minimizes the IB Lagrangian satisfies the following equations:
<disp-formula id="pcbi.1005058.e028">
<alternatives>
<graphic id="pcbi.1005058.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e028" xlink:type="simple"/>
<mml:math display="block" id="M28">
<mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1005058.e029">
<alternatives>
<graphic id="pcbi.1005058.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e029" xlink:type="simple"/>
<mml:math display="block" id="M29">
<mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1005058.e030">
<alternatives>
<graphic id="pcbi.1005058.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>Z(x</italic>,<italic>β)</italic> is a normalization (partition) function. We used here an iterative algorithm (over the set of self consistent equations) to find the optimal solution <italic>P(m|x)</italic> for a given <italic>P(x</italic>,<italic>y)</italic> and <italic>β</italic>. In our case, <italic>x</italic> and <italic>y</italic> are the past and future of the stimulus, respectively, and <italic>m</italic> is the reduced representation of the past.</p>
</sec>
<sec id="sec014">
<title>Reduced Representations in the Oddball Paradigm</title>
<p>We computed reduced representations for the oddball paradigm, for a given past duration of <italic>N</italic> stimuli. First, we calculated the joint distribution <italic>P</italic><sub><italic>N</italic></sub><italic>(past</italic>,<italic>future)</italic> corresponding to the oddball sequences. For any given value of the parameter <italic>p</italic> (the probability of the ‘high tone’ in the block) the events are independent by construction. Thus, for a uniform prior over <italic>p</italic>, the posterior probability can be calculated explicitly,
<disp-formula id="pcbi.1005058.e031">
<alternatives>
<graphic id="pcbi.1005058.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac linethickness="0pt"><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
where <italic>k</italic> = 0,1,..,<italic>N</italic> indicates the number ‘high tones’ in <italic>past</italic>. Then we applied the IB method over <italic>P</italic><sub><italic>N</italic></sub><italic>(past</italic>,<italic>future)</italic> to find reduced representations of the past that are predictive with respect to the future (i.e., the next stimulus). We repeated this procedure with 200 different values of <italic>β</italic> to span the entire range of complexity and predictive power levels, and with 50 different values of <italic>N</italic> spanning past durations from <italic>N</italic> = 1 to <italic>N</italic> = 50. We ended up with a set of 200 × 50 reduced representations, where each one was defined through the conditional probability distributions: <italic>P(m|past)</italic> and <italic>P(future|m)</italic>.</p>
</sec>
<sec id="sec015">
<title>Prediction Errors</title>
<p>We used the conditional probability distributions <italic>P(m|past)</italic> and <italic>P(future|m)</italic> to calculate prediction errors along the stimulation sequence, as follows. For each stimulus (<italic>future</italic>) we considered its previous <italic>N</italic> stimuli (<italic>past</italic>) to determine the probability of entering each state <italic>m</italic> of the reduced representation using <italic>P(m|past)</italic>. The probability with which the future stimulus is then expected <italic>P(future|m)</italic> was used to calculate the prediction error −log<sub>2</sub> <italic>P</italic>(<italic>future</italic>|<italic>m</italic>) associated with that stimulus. Since this quantity depends on the unknown state <italic>m</italic>, we used the <italic>expected</italic> value of the prediction error (with respect to <italic>m</italic>) to generate <xref ref-type="fig" rid="pcbi.1005058.g005">Fig 5</xref>:
<disp-formula id="pcbi.1005058.e032">
<alternatives>
<graphic id="pcbi.1005058.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e032" xlink:type="simple"/>
<mml:math display="block" id="M32">
<mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="italic">past</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Note that the expected prediction error with respect to the stimulus distribution <italic>P(past</italic>,<italic>future)</italic> is negatively related to the predictive power, <italic>I(m;future)</italic>:
<disp-formula id="pcbi.1005058.e033">
<alternatives>
<graphic id="pcbi.1005058.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e033" xlink:type="simple"/>
<mml:math display="block" id="M33">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>;</mml:mo><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
</disp-formula>
where <italic>H(Future)</italic> is the entropy of the future stimulus and does not depend on the reduced representation. It follows that maximizing the predictive power is equivalent to minimizing the expected prediction error.</p>
</sec>
<sec id="sec016">
<title>Fitting Neural Data</title>
<p>For the main analysis, spike counts were measured in a window of 330 ms, starting at stimulus onset and ending 100 ms after stimulus offset. For the analysis displayed in <bold><xref ref-type="fig" rid="pcbi.1005058.g007">Fig 7</xref></bold>, shorter windows were used as well. We collected the neuronal responses (represented as spike counts) for each neuron and for each test frequency (‘low’ or ‘high’) across the oddball stimuli blocks.</p>
<p>For each combination of neuron, test frequency, and reduced representation (200 × 50 pre-calculated reduced representations) we calculated the prediction error at each stimulus along the actual stimulation sequence used in the experiments. Since the prediction error is calculated with respect to a past window of up to <italic>N</italic> = 50 stimuli, the first 50 stimuli in each block were omitted from the analysis, resulting in (400 − 50) × 3 = 1050 stimuli. Dividing the stimuli further into ‘low’ and ‘high’ tone-frequencies, resulted in about 525 stimuli for the analysis of each combination of neuron and test frequency. The reduced representation determines the prediction error associated with each stimulus for each state −log<sub>2</sub> <italic>P</italic>(<italic>future</italic>|<italic>m</italic>). For each stimulus we considered the previous <italic>N</italic> stimuli, using them to estimate the probability of entering each state <italic>m</italic> by <italic>P(m|past)</italic>. These probabilities served as weights in calculating the regression between spike counts and prediction error values. Using the method of weighted linear regression allowed us to take the uncertainty in the unknown state <italic>m</italic> into account. Finally, the fraction of explained variance based on this weighted linear regression (weighted <italic>r</italic><sup>2</sup>) was used to measure the goodness-of-fit associated with each one of the reduced representations. For each combination of neuron and test frequency we denoted the highest fraction of explained variance over the entire set of reduced representations by <inline-formula id="pcbi.1005058.e034"><alternatives><graphic id="pcbi.1005058.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
<p>We considered reduced representations that achieved a score of at least 90% of <inline-formula id="pcbi.1005058.e035"><alternatives><graphic id="pcbi.1005058.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> as ‘good representations’. For each combination of neuron and test frequency used in the main analysis, we constructed two-dimensional binary maps indicating the set of ‘good representations’ over the parameter space (complexity, predictive power and past duration: see <bold><xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6C–6E</xref></bold>). For <bold><xref ref-type="fig" rid="pcbi.1005058.g006">Fig 6</xref></bold>, We calculated the averages of these binary maps over a subset of the population, which had high explanatory power (<inline-formula id="pcbi.1005058.e036"><alternatives><graphic id="pcbi.1005058.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005058.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:mn>10</mml:mn><mml:mi>%</mml:mi></mml:math></alternatives></inline-formula>; <italic>n</italic> = 34/117).</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005058.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Lewen</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <article-title>de Ruyter Van Steveninck RR. Efficiency and ambiguity in an adaptive neural code</article-title>. <source>Nature</source>. <year>2001</year>;<volume>412</volume>(<issue>6849</issue>):<fpage>787</fpage>–<lpage>92</lpage>. <object-id pub-id-type="pmid">11518957</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bodnar</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents</article-title>. <source>Proceedings Biological sciences / The Royal Society</source>. <year>1995</year>;<volume>262</volume>(<issue>1365</issue>):<fpage>259</fpage>–<lpage>65</lpage>. Epub 1995/12/22. <object-id pub-id-type="pmid">8587884</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname> <given-names>S</given-names></name>. <article-title>A simple coding procedure enhances a neuron's information capacity</article-title>. <source>Zeitschrift fur Naturforschung Section C: Biosciences</source>. <year>1981</year>;<volume>36</volume>(<issue>9–10</issue>):<fpage>910</fpage>–<lpage>2</lpage>. Epub 1981/09/01.</mixed-citation></ref>
<ref id="pcbi.1005058.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grimm</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Escera</surname> <given-names>C</given-names></name>. <article-title>Auditory deviance detection revisited: evidence for a hierarchical novelty system</article-title>. <source>Int J Psychophysiol</source>. <year>2012</year>;<volume>85</volume>(<issue>1</issue>):<fpage>88</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ijpsycho.2011.05.012" xlink:type="simple">10.1016/j.ijpsycho.2011.05.012</ext-link></comment> <object-id pub-id-type="pmid">21669238</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naatanen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tervaniemi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sussman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Paavilainen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Winkler</surname> <given-names>I</given-names></name><article-title>. "Primitive intelligence" in the auditory cortex</article-title>. <source>Trends in neurosciences</source>. <year>2001</year>;<volume>24</volume>(<issue>5</issue>):<fpage>283</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">11311381</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Processing of low-probability sounds by cortical neurons</article-title>. <source>Nat Neurosci</source>. <year>2003</year>;<volume>6</volume>(<issue>4</issue>):<fpage>391</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">12652303</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Stimulus-specific adaptation and deviance detection in the auditory system: experiments and models</article-title>. <source>Biological cybernetics</source>. <year>2014</year>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>. <article-title>Neuronal coding of prediction errors</article-title>. <source>Annual review of neuroscience</source>. <year>2000</year>;<volume>23</volume>:<fpage>473</fpage>–<lpage>500</lpage>. Epub 2000/06/09. <object-id pub-id-type="pmid">10845072</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ranganath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rainer</surname> <given-names>G</given-names></name>. <article-title>Neural mechanisms for detecting and remembering novel events</article-title>. <source>Nature reviews Neuroscience</source>. <year>2003</year>;<volume>4</volume>(<issue>3</issue>):<fpage>193</fpage>–<lpage>202</lpage>. Epub 2003/03/04. <object-id pub-id-type="pmid">12612632</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref010"><label>10</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <chapter-title>Bayesian brain: probabilistic approaches to neural coding</chapter-title>. <publisher-loc>Cambridge, Mass.</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2007</year>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>87</lpage>. <object-id pub-id-type="pmid">10195184</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Nemenman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Tishby</surname> <given-names>N</given-names></name>. <article-title>Predictability, complexity, and learning</article-title>. <source>Neural Comput</source>. <year>2001</year>;<volume>13</volume>(<issue>11</issue>):<fpage>2409</fpage>–<lpage>63</lpage>. <object-id pub-id-type="pmid">11674845</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Tishby N, Pereira F, Bialek W. The information bottleneck method. Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing. 1999:368–77.</mixed-citation></ref>
<ref id="pcbi.1005058.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>. <article-title>Change detection, mismatch negativity and stimulus-specific adaptation in animal models</article-title>. <source>J Psychophysiol</source>. <year>2007</year>;<volume>21</volume>:<fpage>214</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cover</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>JA</given-names></name>. <source>Elements of Information Theory</source>. <edition>99th ed</edition>: <publisher-name>Wiley-Interscience</publisher-name>; <year>1991</year> <month>August</month>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gill</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Woolley</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Fremouw</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>What's that sound? Auditory area CLM encodes stimulus surprise, not intensity or intensity changes</article-title>. <source>Journal of neurophysiology</source>. <year>2008</year>;<volume>99</volume>(<issue>6</issue>):<fpage>2809</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.01270.2007" xlink:type="simple">10.1152/jn.01270.2007</ext-link></comment> <object-id pub-id-type="pmid">18287545</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamir</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sabato</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tishby</surname> <given-names>N</given-names></name>. <article-title>Learning and generalization with the information bottleneck</article-title>. <source>Theoretical Computer Science</source>. <year>2010</year>;<volume>411</volume>(<issue>29–30</issue>):<fpage>2696</fpage>–<lpage>711</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Schervish</surname> <given-names>MJ</given-names></name>. <source>Theory of Statistics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>1995</year>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taaseh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Yaron</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Stimulus-specific adaptation and deviance detection in the rat auditory cortex</article-title>. <source>PLoS ONE</source>. <year>2011</year>;<volume>6</volume>(<issue>8</issue>):<fpage>e23369</fpage>. Epub 2011/08/20. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0023369" xlink:type="simple">10.1371/journal.pone.0023369</ext-link></comment> <object-id pub-id-type="pmid">21853120</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Farkas</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Multiple time scales of adaptation in auditory cortex neurons</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>(<issue>46</issue>):<fpage>10440</fpage>–<lpage>53</lpage>. <object-id pub-id-type="pmid">15548659</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yaron</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hershenhoren</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Sensitivity to Complex Statistical Regularities in Rat Auditory Cortex</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>(<issue>3</issue>):<fpage>603</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.08.025" xlink:type="simple">10.1016/j.neuron.2012.08.025</ext-link></comment> <object-id pub-id-type="pmid">23141071</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Stimulus-specific adaptation and deviance detection in the auditory system: experiments and models</article-title>. <source>Biol Cybern</source>. <year>2014</year>;<volume>108</volume>(<issue>5</issue>):<fpage>655</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-014-0585-7" xlink:type="simple">10.1007/s00422-014-0585-7</ext-link></comment> <object-id pub-id-type="pmid">24477619</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hershenhoren</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Taaseh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Antunes</surname> <given-names>FM</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Intracellular correlates of stimulus-specific adaptation</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>9</issue>):<fpage>3303</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2166-13.2014" xlink:type="simple">10.1523/JNEUROSCI.2166-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24573289</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moshitch</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bar-Yosef</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Responses of neurons in primary auditory cortex (A1) to pure tones in the halothane-anesthetized cat</article-title>. <source>J Neurophysiol</source>. <year>2006</year>;<volume>95</volume>(<issue>6</issue>):<fpage>3756</fpage>–<lpage>69</lpage>. <object-id pub-id-type="pmid">16554513</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref025"><label>25</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Linden</surname> <given-names>JF</given-names></name>. <chapter-title>How Linear are Auditory Cortical responses?</chapter-title> In: <name name-style="western"><surname>Becker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Thrun</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Obermayer</surname> <given-names>K</given-names></name>, editors. <source>Advnaces in Neural Information Processing System</source> <volume>15</volume>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2003</year>. p. <fpage>109</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Englitz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Ahrens</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tolnai</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rubsamen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jost</surname> <given-names>J</given-names></name>. <article-title>Multilinear models of single cell responses in the medial nucleus of the trapezoid body</article-title>. <source>Network</source>. <year>2010</year>;<volume>21</volume>(<issue>1–2</issue>):<fpage>91</fpage>–<lpage>124</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/09548981003801996" xlink:type="simple">10.3109/09548981003801996</ext-link></comment> <object-id pub-id-type="pmid">20735339</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schoppe</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Harper</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>Willmore</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>. <article-title>Measuring the Performance of Neural Models</article-title>. <source>Front Comput Neurosci</source>. <year>2016</year>;<volume>10</volume>:<fpage>10</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2016.00010" xlink:type="simple">10.3389/fncom.2016.00010</ext-link></comment> <object-id pub-id-type="pmid">26903851</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attneave</surname> <given-names>F</given-names></name>. <article-title>Some informational aspects of visual perception</article-title>. <source>Psychol Rev</source>. <year>1954</year>;<volume>61</volume>(<issue>3</issue>):<fpage>183</fpage>–<lpage>93</lpage>. <object-id pub-id-type="pmid">13167245</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname> <given-names>H</given-names></name>. <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory Communication</source>. <year>1961</year>:<fpage>217</fpage>–<lpage>34</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005058.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nature reviews Neuroscience</source>. <year>2010</year>;<volume>11</volume>(<issue>2</issue>):<fpage>127</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2787" xlink:type="simple">10.1038/nrn2787</ext-link></comment> <object-id pub-id-type="pmid">20068583</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>JC</given-names></name>. <article-title>The metabolic cost of neural information</article-title>. <source>Nat Neurosci</source>. <year>1998</year>;<volume>1</volume>(<issue>1</issue>):<fpage>36</fpage>–<lpage>41</lpage>. Epub 1999/04/09. <object-id pub-id-type="pmid">10195106</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vinje</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name><article-title>. Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</article-title>. <source>Network</source>. <year>2001</year>;<volume>12</volume>(<issue>3</issue>):<fpage>289</fpage>–<lpage>316</lpage>. <object-id pub-id-type="pmid">11563531</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <etal>et al</etal>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>. <article-title>Computational identification of receptive fields</article-title>. <source>Annu Rev Neurosci</source>. <year>2013</year>;<volume>36</volume>:<fpage>103</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-062012-170253" xlink:type="simple">10.1146/annurev-neuro-062012-170253</ext-link></comment> <object-id pub-id-type="pmid">23841838</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahrens</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Linden</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>. <article-title>Nonlinearities and contextual influences in auditory cortical responses modeled with multilinear spectrotemporal methods</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>(<issue>8</issue>):<fpage>1929</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3377-07.2008" xlink:type="simple">10.1523/JNEUROSCI.3377-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18287509</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinowitz</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Willmore</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Spectrotemporal contrast kernels for neurons in primary auditory cortex</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>(<issue>33</issue>):<fpage>11271</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1715-12.2012" xlink:type="simple">10.1523/JNEUROSCI.1715-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22895711</object-id></mixed-citation></ref>
<ref id="pcbi.1005058.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Integration over multiple timescales in primary auditory cortex</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>49</issue>):<fpage>19154</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2270-13.2013" xlink:type="simple">10.1523/JNEUROSCI.2270-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24305812</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>