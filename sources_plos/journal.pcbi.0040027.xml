<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="allenpress-id">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0040027</article-id><article-id pub-id-type="publisher-id">07-PLCB-RA-0583R2</article-id><article-id pub-id-type="sici">plcb-04-01-15</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology</subject><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="System Taxonomy"><subject>Homo (human)</subject></subj-group></article-categories><title-group><article-title>Why is Real-World Visual Object Recognition Hard?</article-title><alt-title alt-title-type="running-head">Real-World Visual Object Recognition</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Pinto</surname><given-names>Nicolas</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>
            <sup>1</sup>
          </sup>
          </xref><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref></contrib><contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Cox</surname><given-names>David D</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>
            <sup>1</sup>
          </sup>
          </xref><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref><xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>James J</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>
            <sup>1</sup>
          </sup>
          </xref><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref><xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref></contrib></contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>
				 McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
			</addr-line>
      </aff><aff id="aff2">
        <label>2</label>
        <addr-line>
				 Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
			</addr-line>
      </aff><aff id="aff3">
        <label>3</label>
        <addr-line>
				 The Rowland Institute at Harvard, Cambridge, Massachusetts, United States of America
			</addr-line>
      </aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes><corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">dicarlo@mit.edu</email></corresp><fn fn-type="con" id="ack1"><p> NP, DDC, and JJD conceived and designed the experiments and wrote the paper. NP performed the experiments, and analyzed the data. NP and DDC contributed analysis tools.</p></fn><fn fn-type="conflict" id="ack3"><p> The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="ppub"><month>1</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>25</day><month>1</month><year>2008</year></pub-date><volume>4</volume><issue>1</issue><elocation-id>e27</elocation-id><history><date date-type="received"><day>26</day><month>9</month><year>2007</year></date><date date-type="accepted"><day>10</day><month>12</month><year>2007</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder> Pinto et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation.</p></abstract><abstract abstract-type="synopsis"><title>Author Summary</title><p>The ease with which we recognize visual objects belies the computational difficulty of this feat. At the core of this challenge is image variation—any given object can cast an infinite number of different images onto the retina, depending on the object's position, size, orientation, pose, lighting, etc. Recent computational models have sought to match humans' remarkable visual abilities, and, using large databases of “natural” images, have shown apparently impressive progress. Here we show that caution is warranted. In particular, we found that a very simple neuroscience “toy” model, capable only of extracting trivial regularities from a set of images, is able to outperform most state-of-the-art object recognition systems on a standard “natural” test of object recognition. At the same time, we found that this same toy model is easily defeated by a simple recognition test that we generated to better span the range of image variation observed in the real world. Together these results suggest that current “natural” tests are inadequate for judging success or driving forward progress. In addition to tempering claims of success in the machine vision literature, these results point the way forward and call for renewed focus on image variation as a central challenge in object recognition.</p></abstract><funding-group><funding-statement> This work was supported by The National Eye Institute (NIH-R01-EY014970), The Pew Charitable Trusts (PEW UCSF 2893sc), and The McKnight Foundation.</funding-statement></funding-group><counts><page-count count="6"/></counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group><custom-meta><meta-name>citation</meta-name><meta-value>Pinto N, Cox DD, DiCarlo JJ (2008) Why is real-world visual object recognition hard? PLoS Comput Biol 4(1): e27. doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0040027" xlink:type="simple">10.1371/journal.pcbi.0040027</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Visual object recognition is an extremely difficult computational problem. The core problem is that each object in the world can cast an infinite number of different 2-D images onto the retina as the object's position, pose, lighting, and background vary relative to the viewer (e.g., [<xref ref-type="bibr" rid="pcbi-0040027-b001">1</xref>]). Yet the brain solves this problem effortlessly. Progress in understanding the brain's solution to object recognition requires the construction of artificial recognition systems that ultimately aim to emulate our own visual abilities, often with biological inspiration (e.g., [<xref ref-type="bibr" rid="pcbi-0040027-b002">2</xref>–<xref ref-type="bibr" rid="pcbi-0040027-b006">6</xref>]). Such computational approaches are critically important because they can provide experimentally testable hypotheses, and because instantiation of a working recognition system represents a particularly effective measure of success in understanding object recognition. However, a major challenge is assessing the recognition performance of such models. Ideally, artificial systems should be able to do what our own visual systems can, but it is unclear how to evaluate progress toward this goal. In practice, this amounts to choosing an image set against which to test performance.</p><p>Although controversial ([<xref ref-type="bibr" rid="pcbi-0040027-b007">7</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b008">8</xref>]), a popular recent approach in the study of vision is the use of “natural” images [<xref ref-type="bibr" rid="pcbi-0040027-b007">7</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b009">9</xref>–<xref ref-type="bibr" rid="pcbi-0040027-b012">12</xref>], in part because they ostensibly capture the essence of problems encountered in the real world. For example, in computational vision, the Caltech101 image set has emerged as a gold standard for testing “natural” object recognition performance [<xref ref-type="bibr" rid="pcbi-0040027-b013">13</xref>]. The set consists of a large number of images divided into 101 object categories (e.g., images containing planes, cars, faces, flamingos, etc.; see <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>A) plus an additional “background” category (for 102 categories total). While a number of specific concerns have been raised with this set (see [<xref ref-type="bibr" rid="pcbi-0040027-b014">14</xref>] for more details), its images are still currently widely used by neuroscientists, both in theoretical (e.g., [<xref ref-type="bibr" rid="pcbi-0040027-b002">2</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b015">15</xref>]) and experimental (e.g., [<xref ref-type="bibr" rid="pcbi-0040027-b016">16</xref>]) contexts. The logic of Caltech101 (and sets like it; e.g., Caltech256 [<xref ref-type="bibr" rid="pcbi-0040027-b017">17</xref>]) is that the sheer number of categories and the diversity of those images place a high bar for object recognition systems and require them to solve the computational crux of object recognition. Because there are 102 object categories, chance performance is less than 1% correct. In recent years, several object recognition models (including biologically inspired approaches) have shown what appears to be impressively high performance on this test—better than 60% correct [<xref ref-type="bibr" rid="pcbi-0040027-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b018">18</xref>–<xref ref-type="bibr" rid="pcbi-0040027-b021">21</xref>], suggesting that these approaches, while still well below human performance, are at least heading in the right direction.</p><fig id="pcbi-0040027-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0040027.g001</object-id><label>Figure 1</label><caption><title>Performance of a Simple V1-Like Model Relative to Current Performance of State-of-the-Art Artificial Object Recognition Systems (Some Biologically Inspired) on an Ostensibly “Natural” Standard Image Database (Caltech101)</title><p>(A) Example images from the database and their category labels.</p><p>(B) Two example images from the “car” category.</p><p>(C) Reported performance of five state-of-the-art computational object recognition systems on this “natural” database are shown in gray (1 = Wang et al. 2006; 2 = Grauman and Darrell 2006; 3 = Mutch and Lowe 2006; 4 = Lazebnik et al. 2006; 5 = Zhang et al. 2006). In this panel, 15 training examples were used to train each system. Since chance performance on this 102-category task is less than 1%, performance values greater than ∼40% have been taken as substantial progress. The performance of the simple V1-like model is shown in black (+ is with “ad hoc” features; see <xref ref-type="sec" rid="s4">Methods</xref>). Although the V1-like model is extremely simple and lacks any explicit invariance-building mechanisms, it performs as well as, or better than, state-of-the-art object recognition systems on the “natural” databases (but see Varma and Ray 2007 for a recent hybrid approach, that pools the above methods to achieve higher performance).</p><p>(D) Same as (C) except that 30 training examples were used. The dashed lines indicates performance achieved using an untransformed grayscale pixel space representation and a linear SVM classifier (15 training examples: 16.1%, SD 0.4; 30 training examples: 17.3%, SD 0.8). Error bars (barely visible) represent the standard deviation of the mean performance of the V1-like model over ten random training and testing splits of the images. The authors of the state-of-the-art approaches do not consistently report this variation, but when they do they are in the same range (less than 1%). The V1-model also performed favorably with fewer training examples (see <xref ref-type="supplementary-material" rid="pcbi-0040027-sg004">Figure S4</xref>).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0040027.g001" xlink:type="simple"/></fig><p>However, we argue here for caution, as it is not clear to what extent such “natural” image tests actually engage the core problem of object recognition. Specifically, while the Caltech101 set certainly contains a large number of images (9,144 images), variations in object view, position, size, etc., between and within object category are poorly defined and are not varied systematically. Furthermore, image backgrounds strongly covary with object category (see <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>B). The majority of images are also “composed” photographs, in that a human decided how the shot should be framed, and thus the placement of objects within the image is not random and the set may not properly reflect the variation found in the real world. Furthermore, if the Caltech101 object recognition task is hard, it is not easy to know what makes it hard—different kinds of variation (view, lighting, exemplar, etc.) are all inextricably mixed together. Such problems are not unique to the Caltech101 set, but also apply to other uncontrolled “natural” image sets (e.g., Pascal VOC [<xref ref-type="bibr" rid="pcbi-0040027-b022">22</xref>]).</p></sec><sec id="s2"><title>Results</title><p>To explore this issue, we used the simplest, most obvious starting point for a biologically inspired object recognition system—a “V1-like” model based roughly on the known properties of simple cells of the first primate cortical visual processing stage (area V1). In particular, the model was a population of locally normalized, thresholded Gabor functions spanning a range of orientations and spatial frequencies (see <xref ref-type="sec" rid="s4">Methods</xref> for details). This is a neuroscience “null” model because it is only a first-order description of the early visual system, and one would not expect it to be good for real-world object recognition tasks. Specifically, it contains no explicit mechanisms to enable recognition to tolerate variation in object position, size, or pose, nor does it contain a particularly sophisticated representation of shape. Nevertheless, null models are useful for establishing baselines, and we proceeded to test this null model on a gold-standard “natural” object recognition task (i.e., Caltech101 [<xref ref-type="bibr" rid="pcbi-0040027-b013">13</xref>]), using standard, published procedures [<xref ref-type="bibr" rid="pcbi-0040027-b021">21</xref>].</p><p>We found that this simple V1-like model performed remarkably well on the Caltech101 object recognition task—indeed, it outperformed reported state-of-the-art computational efforts (biologically inspired or not; see <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>). <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref> shows the cross-validated performance of two versions of this simple model: one where only the model's outputs are fed into a standard linear classifier, and one where some additional ad-hoc features are also used (e.g., local feature intensity histograms; see <xref ref-type="sec" rid="s4">Methods</xref> for details). In both cases, performance is surprisingly good (61% and 67% correct with 15 and 30 training examples), and comparable to, or better than, the current reported performance in the literature ([<xref ref-type="bibr" rid="pcbi-0040027-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b018">18</xref>–<xref ref-type="bibr" rid="pcbi-0040027-b021">21</xref>]; but see [<xref ref-type="bibr" rid="pcbi-0040027-b023">23</xref>]).</p><p>Given the V1-like model's surprisingly good performance on this “natural” image set (<xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>), there are two possibilities. Either this model is a previously overlooked good model of visual object recognition, or current “natural” tests do not appropriately engage the object recognition problem. Given that our V1-like model contains no special machinery for tolerating image variation (and it would generally be considered a “straw man” model by neuroscientists), we were suspicious that this result had more to do with the test set, than the model itself. Nevertheless, to distinguish between these two possibilities, we designed a second more carefully controlled object recognition test that directly engages the core problem of object recognition.</p><p>Specifically, we constructed a series of two-category image sets, consisting of rendered images of plane and car objects. By the logic of the Caltech101 “natural” image test, this task should be substantially easier—there are only two object categories (rather than 102), and only a handful of specific objects per category (<xref ref-type="fig" rid="pcbi-0040027-g002">Figure 2</xref>A). In these sets, however, we explicitly and parametrically introduced real-world variation in the image that each object produced (see <xref ref-type="sec" rid="s4">Methods</xref>). In spite of the much smaller number of categories that the system was required to identify, the problem proved substantially harder for the V1-like model, exactly as one would expect for an incomplete model of object recognition. <xref ref-type="fig" rid="pcbi-0040027-g002">Figure 2</xref> shows how performance rapidly degrades toward chance-level as even modest amounts of real-world object image variation are systematically introduced in this simple two-category problem (see <xref ref-type="supplementary-material" rid="pcbi-0040027-sg002">Figure S2</xref> for a comparable demonstration with more than two object categories). Given this result, we conclude that the “V1-like” model performed well on the “natural” object recognition test (<xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>), not because it is a good model of object recognition, but because the “natural” image test is inadequate.</p><fig id="pcbi-0040027-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0040027.g002</object-id><label>Figure 2</label><caption><title>The Same Simple V1-Like Model That Performed Well in <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref> Is Not a Good Model of Object Recognition—It Fails Badly on a “Simple” Problem That Explicitly Requires Tolerance to Image Variation</title><p>(A) We used 3-D models of cars and planes to generate image sets for performing a cars-versus-planes two-category test. By using 3-D models, we were able to parametrically control the amount of identity-preserving variation that the system was required to tolerate to perform the task (i.e., variation in each object's position, scale, pose). The 3-D models were rendered using ray-tracing software (see <xref ref-type="sec" rid="s4">Methods</xref>), and were placed on either a white noise background (shown here), a scene background, or a phase scrambled background (these backgrounds are themselves another form of variation that a recognition system must tolerate; see <xref ref-type="supplementary-material" rid="pcbi-0040027-sg001">Figure S1</xref>).</p><p>(B) As the amount of variation was increased (<italic>x</italic>-axis), performance drops off, eventually reaching chance level (50%). Here, we used 100 training and 30 testing images for each object category. However, using substantially more exemplar images (1,530 training, 1,530 testing) yielded only mild performance gains (e.g., 2.7% for the fourth variation level using white noise background), indicating that the failure of this model is not due to under-training. Error bars represent the standard error of the mean computed over ten random splits of training and testing images (see <xref ref-type="sec" rid="s4">Methods</xref>). This result highlights a fundamental problem in the current use of “natural” images to test object recognition models. By the logic of the “natural” Caltech101 test set, this task should be easy, because it has just two object categories, while the Caltech101 test should be hard (102 object categories). However, this V1-like model fails badly with this “easy” set, in spite of high performance on the Caltech101 test set (<xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0040027.g002" xlink:type="simple"/></fig><p>These results (re-)emphasize that object recognition is hard, not because images are natural or “complex,” but because each object can produce a very wide range of retinal images. Although the Caltech101 and other such “natural” sets were useful in that they encouraged the use of common performance tests on which all recognition models should compete, the results presented here show that a different direction is needed to create the content of those tests. This question is not simply an academic concern—great effort is now being expended to test object recognition models against a new, larger image set: the “Caltech256.” However, as with its predecessor, it fails to reflect real-world variation, and our “null” V1 model also performs well above chance (24% accuracy with 15 training examples to discriminate 257 categories), and competitively with early published performance estimates on this new set (see <xref ref-type="supplementary-material" rid="pcbi-0040027-sg003">Figure S3</xref>).</p></sec><sec id="s3"><title>Discussion</title><p>How should we gauge progress in solving object recognition? First, the results presented here underscore that simple chance performance level is far from a good baseline and that our intuitions about “hard” and “easy” recognition problems are often far from correct. Indeed, it is disconcerting how little variation we needed to introduce to break a model that performs quite well according to current “natural' object recognition tests. Thus, simple “null” models (that are able to exploit regularities in the image database) are needed to objectively judge the difficulty of recognition tasks and to establish a baseline for each such task. The V1-like model presented here provides one possible “null” model, and portable code for building and evaluating it is freely available upon request.</p><p>Second, the development of appropriate recognition tests is critical to guiding the development of object recognition models and testing performance of neuronal populations that might support recognition [<xref ref-type="bibr" rid="pcbi-0040027-b024">24</xref>]. The construction of such tests is not trivial because the issues cut deeper than simple performance evaluation—this is a question of how we think about the problem of object recognition and why it is hard [<xref ref-type="bibr" rid="pcbi-0040027-b001">1</xref>]. Because the number of images in any practical recognition database will be small relative to the dimensionality of the problem domain, test images must be chosen in a manner that properly samples this domain so as to capture the essence of the recognition problem and thus avoid “solutions” that rely on trivial regularities or heuristics.</p><p>One approach would be to generate a very large database of “natural” images, like the Caltech sets, but captured in an unbiased way (i.e., with great care taken to avoid the implicit biases that occur in framing a snapshot). Done correctly, this approach has the advantage of directly sampling the true problem domain. However, annotating such an image set is extremely labor-intensive (but see the LabelMe project [<xref ref-type="bibr" rid="pcbi-0040027-b025">25</xref>], Peekaboom [<xref ref-type="bibr" rid="pcbi-0040027-b026">26</xref>], and the StreetScenes dataset [<xref ref-type="bibr" rid="pcbi-0040027-b002">2</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b027">27</xref>]). More importantly, a set that truly reflects all real-world variation may be too stringent of an assay to guide improvement in recognition models. That is, if the problem is too hard, it is not easy to construct a reduced version that still engages the core problem of object recognition.</p><p>Another approach, an extension of the one taken here, would be to use synthetic images, where ground truth is known by design. Paradoxically, such synthetic image sets may in many ways be more natural than an arbitrary collection of ostensibly “natural” photographs, because, for a fixed number of images, they better span the range of possible image transformations observed in the real world (see also the NORB dataset [<xref ref-type="bibr" rid="pcbi-0040027-b028">28</xref>]). The synthetic image approach obviates labor-intensive and error-prone labeling procedures, and can be easily used to isolate performance on different components of the task. Such an approach also has the advantage that it can be parametrically made more difficult as needed (e.g., when a given model has achieved the ability to tolerate a certain amount of variation, a new instantiation of the test set with greater variation can be generated). Given the difficulty of real-world object recognition, this ability to gradually “ratchet” task difficulty, while still engaging the core computational problem, may provide invaluable guidance of computational efforts.</p><p>While standardized benchmarks are important for assessing progress, designing benchmarks that properly define what constitutes “progress” is extremely difficult. On one hand, a benchmark that captures too little of the complexity of the real world (no matter how complex it may seem at first glance) invites over-optimization to trivial regularities in the test set (e.g., Caltech101). On the other hand, a benchmark that embraces too much of the “real” problem can be too difficult for any model to gain traction (e.g., the detection challenge in Pascal VOC [<xref ref-type="bibr" rid="pcbi-0040027-b022">22</xref>]), giving little insight on which approaches are most promising. This problem is compounded by the fact that there are many more <italic>kinds</italic> of image variation in the real world beyond those used in our simple synthetic test set (e.g., lighting, occlusion, deformation, etc.). At the center of this challenge is the need to clearly define what the problem is, why it is difficult, and what results would constitute success. The path forward will not be easy, but it is time for the field to give this problem much more central attention.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4a"><title>A V1-like recognition system.</title><p>Area V1 is the first stage of cortical processing of visual information in the primate and is the gateway of subsequent processing stages. We built a very basic representation inspired by known properties of V1 “simple” cells (a subpopulation of V1 cells). The responses of these cells to visual stimuli are well-described by a spatial linear filter, resembling a Gabor wavelet [<xref ref-type="bibr" rid="pcbi-0040027-b029">29</xref>–<xref ref-type="bibr" rid="pcbi-0040027-b031">31</xref>], with a nonlinear output function (threshold and saturation) and some local normalization (roughly analogous to “contrast gain control”). Operationally, our V1-like model consisted of the following processing steps.</p><p><italic>Image preparation.</italic> First we converted the input image to grayscale and resized by bicubic interpolation the largest edge to a fixed size (150 pixels for Caltech datasets) while preserving its aspect ratio. The mean was subtracted from the resulting two-dimensional image and we divided it by its standard deviation. The resulting image had zero mean, unit variance, and a size of H × W. Because images have different aspect ratios, H and W vary from image to image.</p><p><italic>Local input divisive normalization</italic>. For each pixel in the input image, we subtracted the mean of the pixel values in a fixed window (3 × 3 pixels, centered on the pixel), and we divided this value by the euclidean norm of the resulting 9-dimensional vector (3 × 3 window) if the norm was greater than 1 (i.e., roughly speaking, the normalization was constrained such that it could reduce responses, but not enhance them).</p><p><italic>Linear filtering with a set of Gabor filters.</italic> We convolved the normalized images with a set of two-dimensional Gabor filters of fixed size (43 × 43 pixels), spanning 16 orientations (equally spaced around the clock) and six spatial frequencies (1/2, 1/3, 1/4, 1/6, 1/11, 1/18 cycles/pixel) with a fixed Gaussian envelope (standard deviation of 9 cycles/pixel in both directions) and fixed phase (0) for a total of N = 96 filters. Each filter had zero-mean and euclidean norm of one. This dimensionality expansion approximates the roughly 100-fold increase in the number of primate V1 neurons relative to the number of retinal ganglion cell axons. To speed this step, the Gabor filters were decomposed via singular value decomposition into a form suitable for use in a separable convolution (this is possible because the Gabor filters are of low rank), and the decomposed filters retained at least 90% of their original variation.</p><p><italic>Thresholding and saturation</italic>. The output of each Gabor filter was passed through a standard output non-linearity—a threshold and response saturation. Specifically, all negative output values were set to 0 and all values greater than 1 were set to 1.</p><p><italic>Local output divisive normalization.</italic> The result of the Gabor filtering was a three-dimensional matrix of size H × W × N where each two-dimensional slice (H × W) is the output of each Gabor filter type. For each filter output, we subtracted the mean of filter outputs in a fixed spatial window (3 × 3 pixels, centered) across all orientations and spatial scales (total of 864 elements). We then divided by the euclidean norm of the values in this window (864 elements), except when the norm was less than 1.</p></sec><sec id="s4b"><title>Comparison to other biologically inspired recognition models.</title><p>Some of the other models whose performance is shown in <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref> were biologically inspired, and thus also have V1-like stages contained within them, as well as additional machinery intended to allow invariant object recognition (e.g., [<xref ref-type="bibr" rid="pcbi-0040027-b002">2</xref>,<xref ref-type="bibr" rid="pcbi-0040027-b019">19</xref>]). Thus, it might be surprising that the simple V1-like model presented here outperforms those models. Although detailed comparisons are beyond the scope of this study and tangential to our main point, we note that the V1-like model presented here contains a number of differences from the V1-like portions of these other models (higher dimensionality, larger receptive fields, inclusion of threshold nonlinearities, local normalization, etc.) that probably produce better performance than these models.</p></sec><sec id="s4c"><title>Classification.</title><p>To test the utility of our V1-like representation for performing object recognition tasks, we performed a standard cross-validated classification procedure on the high-dimensional output of the model.</p><p><italic>Dimensionality reduction.</italic> To speed computation and improve classification performance, we reduced the dimensionality of the model output prior to classification. The output of V1-like model (above) was a stack of 96 output images, one per Gabor filter type. Because the dimensionality of this stack can be very high (up to 2,160,000 output values per input image depending on its size), standard dimensionality reduction techniques were used to prepare the data for classification. Specifically, each of the 96 output images was low-pass filtered (17 × 17 boxcar) and down-sampled to a smaller size (30 × 30). Thus, regardless of the original input image size, the total dimensionality for classification was always 86,400 (30 × 30 × 96). The data were then sphered (i.e., each filter output was standardized by subtraction of its mean and division by its standard deviation across the training image set; see below), and the dimensionality of the representation was further reduced by principal components analysis (PCA), keeping as many dimensions as there were data points in the training set. For the Caltech101 experiments (e.g., <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>), the dimensionality of the final feature vector was 1530 or 3060 (depending on the number of training examples: 15 or 30, respectively).</p><p><italic>Additional “ad hoc” features.</italic> To further explore the utility of this V1-like model, we generated some additional easy-to-obtain features and concatenated these to the final feature vector, prior to PCA dimensionality reduction. These features included: raw grayscale input images (downsampled to 100 × 100 by bicubic interpolation; 10,000 features), and model output histograms for some intermediate stages of the model: pre-normalization (one local histogram per quadrant of the image), post-normalization (full image), and post downsampling (full image)—roughly 30,000 features total. No color information was used in these additional features. Throughout the text, results from the system containing these extra “ad hoc” features are reported separately from those obtained with the system that did not have these extra features. These extra features were added to demonstrate what was possible using additional obvious, “cheap” (but still fair) tricks that improve performance without incurring additional conceptual complexity.</p><p><italic>Training.</italic> Training and test images were carefully separated to ensure proper cross-validation. 15 training example images, and 30 testing example images were drawn from the full image set. Sphering parameters and PCA eigenvectors were computed from the training images (see Dimensionality Reduction, above), and the dimensionality-reduced training data were used to train a linear support vector machine (SVM) using libsvm-2.82 [<xref ref-type="bibr" rid="pcbi-0040027-b032">32</xref>]. A standard one-versus-all approach was used to generate the multi-class SVM classifier from the training images.</p><p><italic>Testing protocol.</italic> Following training, absolutely no changes to the representation or classifier were made. Each test image was sphered using parameters determined from the training images, projected through the V1-like model onto the eigenvectors computed from the training images, and the trained SVM was used to report the predicted category of the test image</p><p><italic>Number of training and testing images.</italic> Classifiers were trained using a fixed number of examples (15 and 30 example images; see <xref ref-type="fig" rid="pcbi-0040027-g001">Figure 1</xref>C and <xref ref-type="fig" rid="pcbi-0040027-g001">1</xref>D). The performance scores reported here are the average of performances obtained from ten random splits of training and testing sets. For testing, 30 images were classified per category, except in categories where there were not enough images available, in which case the maximum number of available images was used (e.g., “inline_skate”, the smallest category, has only 31 examples; when 30 examples were used for training, then only one example was available for testing). Since the Caltech101 sets contains a different number of images for each category, care must be taken to ensure that per-category performance is normalized by the number of test examples considered in each category—otherwise, average performance can be biased toward the performance obtained from categories with larger numbers of images available. This is a particular problem for the Caltech101 set, because some of the largest categories are also empirically the easiest (e.g., cars, airplanes, faces, motorbikes). For the performance values reported in this paper, average performance was computed per category, and then these performances were averaged together to obtain an overall performance value (reported in the text and figures).</p><p><italic>Further controls.</italic> To ensure the validity of our results, we undertook a number of checks to verify that the classification procedure used here was correct. Two different SVM front-ends were used (PyML and libsvm command line tools) and produced identical results. To confirm proper cross-validation, we manually inspected training and test set splits to certify that there were no images in common between the two sets (this control was partially motivated by the fact that an earlier version of the Caltech101 dataset contained duplicates). The classification procedure was also repeated with noise images, and for image sets with category labels scrambled; both tests yielded chance performance, as expected.</p></sec><sec id="s4d"><title>Synthetic dataset generation.</title><p>Synthetic images of cars and planes were generated using POV-Ray, a free, high-quality ray tracing software package (<ext-link ext-link-type="uri" xlink:href="http://www.povray.org" xlink:type="simple">http://www.povray.org</ext-link>). 3-D models of cars and planes (purchased from Dosch Design and TurboSquid) were converted to the POV-Ray format. This general approach could be used to generate image sets with arbitrary numbers of different objects, undergoing controlled ranges of variation. For example, in <xref ref-type="fig" rid="pcbi-0040027-g002">Figure 2</xref> each “pooled variation” level on the <italic>x</italic>-axis shows the maximum deviation of each of five object viewing parameters (zero variation is shown in <xref ref-type="fig" rid="pcbi-0040027-g002">Figure 2</xref>A assuming centering in the image). Given a “pooled variation” level, a set of images was generated by randomly sampling each viewing parameter uniformly within its specified maximum deviation (e.g., +/−30° in plane rotation). Each image in the set was the result of using one such parameter sample to render the view of the object on a given background (see <xref ref-type="supplementary-material" rid="pcbi-0040027-sg001">Figure S1</xref>). 100% position variation is a full non-overlapping shift of the object's bounding box; 100% scale variation is one octave of change.</p><p>While this image set is useful for demonstrating the inadequacy of our V1-like model (in spite of its apparent success at the Caltech101 test), we do not believe it represents any sort of new “standard” against which models of object recognition should be tested. Instead, we believe that the <italic>approach</italic> is more important—identifying the problem, generating sets that span limited regions of the problem space, building models, and then “ratcheting” the problem to a higher difficulty level once the limited version of the problem has been solved.</p></sec></sec><sec id="s5"><title>Supporting Information</title><supplementary-material id="pcbi-0040027-sg001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0040027.sg001" xlink:type="simple"><label>Figure S1</label><caption><title>Backgrounds Used</title><p>Model performance for our “simple” two class image set was assessed with the 3-D models rendered onto three types of backgrounds—white noise, phase-scrambled scene images (∼1/f noise), and intact scene images. Performance with each of these types of background is shown in <xref ref-type="fig" rid="pcbi-0040027-g002">Figure 2</xref>.</p><p>(3.8 MB TIF)</p></caption></supplementary-material><supplementary-material id="pcbi-0040027-sg002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0040027.sg002" xlink:type="simple"><label>Figure S2</label><caption><title>Performance Fall-Off for Increasing Numbers of Object Categories</title><p><xref ref-type="fig" rid="pcbi-0040027-g002">Figure 2</xref> shows that relatively modest amounts of image transformation push the performance of our simple V1-like model down to chance. Here we show that this fall-off becomes slightly steeper as more categories-to-be-discriminated are added.</p><p>(A) Four categories of objects (cars, planes, boats, and animals) were used to measure performance when 2, 3, or 4 categories are considered.</p><p>(B) Average identification performance (“is object category X present or not”) is plotted as a function of view variation and number of object categories to be discriminated. Chance performance is 50% for all three lines, because average one-versus-all performance is shown here, not n-way recognition performance (i.e., “which object is present”).</p><p>(1.1 MB TIF)</p></caption></supplementary-material><supplementary-material id="pcbi-0040027-sg003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0040027.sg003" xlink:type="simple"><label>Figure S3</label><caption><title>Performance on the Caltech256</title><p>1 = Griffin et al. (2007).</p><p>(366 KB TIF)</p></caption></supplementary-material><supplementary-material id="pcbi-0040027-sg004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0040027.sg004" xlink:type="simple"><label>Figure S4</label><caption><title>Performance on the Caltech101 as a Function of the Number of Training Examples, Including Small Numbers of Training Examples</title><p>Points marked with asterisks are not exact, but were estimated from published plots.</p><p>(967 KB TIF)</p></caption></supplementary-material></sec></body><back><ack><p>We would like to thank J. Maunsell, J. Mutch, T. Poggio, E. Simoncelli, and A. Torralba for helpful comments and discussion.</p></ack><ref-list><title>References</title><ref id="pcbi-0040027-b001"><label>1</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name></person-group>
					<year>2007</year>
					<article-title>Untangling invariant object recognition.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>11</volume>
					<fpage>333</fpage>
					<lpage>341</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b002"><label>2</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name><name name-style="western"><surname>Wolf</surname><given-names>L</given-names></name><name name-style="western"><surname>Bileschi</surname><given-names>S</given-names></name><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>
					<year>2007</year>
					<article-title>Object recognition with cortex-like mechanisms.</article-title>
					<source>IEEE PAMI</source>
					<volume>9</volume>
					<fpage>411</fpage>
					<lpage>426</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b003"><label>3</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lowe</surname><given-names>DG</given-names></name></person-group>
					<year>2004</year>
					<article-title>Distinctive image features from scale-invariant keypoints.</article-title>
					<source>IEEE IJCV</source>
					<volume>60</volume>
					<fpage>91</fpage>
					<lpage>110</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b004"><label>4</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>H</given-names></name><name name-style="western"><surname>Berg</surname><given-names>AC</given-names></name><name name-style="western"><surname>Maire</surname><given-names>M</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J</given-names></name></person-group>
					<year>2006</year>
					<article-title>SVM-KNN: Discriminative nearest neighbor classification for visual category recognition.</article-title>
					<source>IEEE CVPR 2006</source>
					<fpage>2126</fpage>
					<lpage>2136</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b005"><label>5</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Weber</surname><given-names>M</given-names></name><name name-style="western"><surname>Welling</surname><given-names>M</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P</given-names></name></person-group>
					<year>2000</year>
					<article-title>Unsupervised learning of models for recognition.</article-title>
					<source>IEEE ECCV 2000</source>
					<fpage>18</fpage>
					<lpage>32</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b006"><label>6</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Arathorn</surname><given-names>D</given-names></name></person-group>
					<year>2002</year>
					<source>Map-seeking circuits in visual cognition: A computational mechanism for biological and machine vision</source>
					<publisher-loc>Stanford (California)</publisher-loc>
					<publisher-name>Stanford University Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">240</size>
				</element-citation></ref><ref id="pcbi-0040027-b007"><label>7</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Felsen</surname><given-names>G</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>
					<year>2005</year>
					<article-title>A natural approach to studying vision.</article-title>
					<source>Nat Neurosci</source>
					<volume>8</volume>
					<fpage>1643</fpage>
					<lpage>1646</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b008"><label>8</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name></person-group>
					<year>2005</year>
					<article-title>In praise of artifice.</article-title>
					<source>Nat Neurosci</source>
					<volume>8</volume>
					<fpage>1647</fpage>
					<lpage>1649</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b009"><label>9</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name><name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>
					<year>1998</year>
					<article-title>Neural activity in areas V1, V2, and V4 during free viewing of natural scenes compared to control images.</article-title>
					<source>Neuroreport</source>
					<volume>9</volume>
					<fpage>85</fpage>
					<lpage>89</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b010"><label>10</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name></person-group>
					<year>2001</year>
					<article-title>How do visual neurons respond in the real world?</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>11</volume>
					<fpage>437</fpage>
					<lpage>442</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b011"><label>11</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>
					<year>1997</year>
					<article-title>The “independent components” of natural scenes are edge filters.</article-title>
					<source>Vision Res</source>
					<volume>37</volume>
					<fpage>3327</fpage>
					<lpage>3338</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b012"><label>12</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name></person-group>
					<year>2001</year>
					<article-title>Natural image statistics and neural representation.</article-title>
					<source>Annu Rev Neurosci</source>
					<volume>24</volume>
					<fpage>1193</fpage>
					<lpage>1216</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b013"><label>13</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name><name name-style="western"><surname>Fergus</surname><given-names>R</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P</given-names></name></person-group>
					<year>2004</year>
					<article-title>Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories.</article-title>
					<source>IEEE CVPR</source>
					<volume>2004</volume>
					<fpage>178</fpage>
				</element-citation></ref><ref id="pcbi-0040027-b014"><label>14</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ponce</surname><given-names>J</given-names></name><name name-style="western"><surname>Berg</surname><given-names>TL</given-names></name><name name-style="western"><surname>Everingham</surname><given-names>M</given-names></name><name name-style="western"><surname>Forsyth</surname><given-names>DA</given-names></name><name name-style="western"><surname>Hebert</surname><given-names>M</given-names></name><etal/></person-group>
					<year>2006</year>
					<article-title>Dataset issues in object recognition.</article-title>
					<source>Toward category-level object recognition</source>
					<publisher-loc>Berlin: Springer-Verlag</publisher-loc>
					<publisher-name>Lect Notes Comput Sci</publisher-name>
					<fpage>29</fpage>
					<lpage>48</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b015"><label>15</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Masquelier</surname><given-names>T</given-names></name><name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group>
					<year>2007</year>
					<article-title>Unsupervised learning of visual features through Spike Timing Dependent Plasticity.</article-title>
					<source>PLoS Comp Bio</source>
					<volume>3</volume>
					<elocation-id>e31.</elocation-id>
					<comment>doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0030031." xlink:type="simple">10.1371/journal.pcbi.0030031</ext-link>.</comment>
				</element-citation></ref><ref id="pcbi-0040027-b016"><label>16</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Einhäuser</surname><given-names>W</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Makeig</surname><given-names>S</given-names></name></person-group>
					<year>2007</year>
					<article-title>The duration of the attentional blink in natural scenes depends on stimulus category.</article-title>
					<source>Vision Res</source>
					<volume>47</volume>
					<fpage>597</fpage>
					<lpage>607</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b017"><label>17</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Griffin</surname><given-names>G</given-names></name><name name-style="western"><surname>Holub</surname><given-names>A</given-names></name><name name-style="western"><surname>Perona</surname><given-names>P</given-names></name></person-group>
					<year>2007</year>
					<source>Caltech-256 Object Category Dataset</source>
					<publisher-loc>Pasadena (California)</publisher-loc>
					<publisher-name>Caltech Technical Report</publisher-name>
				</element-citation></ref><ref id="pcbi-0040027-b018"><label>18</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>G</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group>
					<year>2006</year>
					<article-title>Using dependent regions for object categorization in a generative framework.</article-title>
					<source>IEEE CVPR</source>
					<volume>2006</volume>
					<fpage>1597</fpage>
					<lpage>1604</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b019"><label>19</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Mutch</surname><given-names>J</given-names></name><name name-style="western"><surname>Lowe</surname><given-names>DG</given-names></name></person-group>
					<year>2006</year>
					<article-title>Multiclass object recognition with sparse, localized features.</article-title>
					<source>IEEE CVPR</source>
					<volume>2006</volume>
					<fpage>11</fpage>
					<lpage>18</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b020"><label>20</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lazebnik</surname><given-names>S</given-names></name><name name-style="western"><surname>Schmid</surname><given-names>C</given-names></name><name name-style="western"><surname>Ponce</surname><given-names>J</given-names></name></person-group>
					<year>2006</year>
					<article-title>Beyond bags of features: spatial pyramid matching for recognizing natural scene categories.</article-title>
					<source>IEEE CVPR</source>
					<volume>2006</volume>
					<fpage>2169</fpage>
					<lpage>2178</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b021"><label>21</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Grauman</surname><given-names>K</given-names></name><name name-style="western"><surname>Darrell</surname><given-names>T</given-names></name></person-group>
					<year>2006</year>
					<source>Pyramid match kernels: Discriminative classification with sets of image features (version 2)</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT</publisher-name>
					<comment>Technical Report CSAIL-TR-2006–020.</comment>
				</element-citation></ref><ref id="pcbi-0040027-b022"><label>22</label><element-citation publication-type="other" xlink:type="simple">
					<source>PASCAL Object Recognition Database Collection, Visual Object Classes Challenge</source>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.pascal-network.org/challenges/VOC" xlink:type="simple">http://www.pascal-network.org/challenges/VOC</ext-link>. Accessed 26 December 2007.</comment>
				</element-citation></ref><ref id="pcbi-0040027-b023"><label>23</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Varma</surname><given-names>M</given-names></name><name name-style="western"><surname>Ray</surname><given-names>D</given-names></name></person-group>
					<year>2007</year>
					<article-title>Learning the discriminative power-invariance trade-off.</article-title>
					<comment>In:</comment>
					<source>Proceedings of the Eleventh IEEE International Conference on Computer Vision</source>
					<conf-date>14–20 October 2007;</conf-date>
					<publisher-loc>Rio de Janeiro, Brazil</publisher-loc>
					<publisher-name>IEEE ICCV</publisher-name>
				</element-citation></ref><ref id="pcbi-0040027-b024"><label>24</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hung</surname><given-names>CP</given-names></name><name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group>
					<year>2005</year>
					<article-title>Fast readout of object identity from macaque inferior temporal cortex.</article-title>
					<source>Science</source>
					<volume>310</volume>
					<fpage>863</fpage>
					<lpage>866</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b025"><label>25</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Russell</surname><given-names>B</given-names></name><name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name><name name-style="western"><surname>Murphy</surname><given-names>K</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>WT</given-names></name></person-group>
					<year>2005</year>
					<source>LabelMe: a database and web-based tool for image annotation</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Artificial Intelligence Lab Memo AIM-2005–025</publisher-name>
				</element-citation></ref><ref id="pcbi-0040027-b026"><label>26</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Von Ahn</surname><given-names>L</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R</given-names></name><name name-style="western"><surname>Blum</surname><given-names>M</given-names></name></person-group>
					<year>2006</year>
					<article-title>Peekaboom: a game for locating objects in images.</article-title>
					<source>ACM SIGCHI</source>
					<volume>2006</volume>
					<fpage>55</fpage>
					<lpage>64</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b027"><label>27</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bileschi</surname><given-names>S</given-names></name></person-group>
					<year>2006</year>
					<source>StreetScenes: Towards scene understanding in still images</source>
					<comment>[Ph.D. Thesis].</comment>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT EECS</publisher-name>
				</element-citation></ref><ref id="pcbi-0040027-b028"><label>28</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name><name name-style="western"><surname>Huang</surname><given-names>FJ</given-names></name><name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name></person-group>
					<year>2004</year>
					<article-title>Learning methods for generic object recognition with invariance to pose and lighting.</article-title>
					<source>IEEE CVPR</source>
					<volume>2004</volume>
					<fpage>97</fpage>
					<lpage>104</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b029"><label>29</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name></person-group>
					<year>1962</year>
					<article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.</article-title>
					<source>J Physiol</source>
					<volume>160</volume>
					<fpage>106</fpage>
					<lpage>154</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b030"><label>30</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name></person-group>
					<year>1968</year>
					<article-title>Receptive fields and functional architecture of monkey striate cortex.</article-title>
					<source>J Physiol</source>
					<volume>195</volume>
					<fpage>215</fpage>
					<lpage>243</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b031"><label>31</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>JP</given-names></name><name name-style="western"><surname>Palmer</surname><given-names>LA</given-names></name></person-group>
					<year>1987</year>
					<article-title>An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex.</article-title>
					<source>J Neurophysiol</source>
					<volume>58</volume>
					<fpage>1233</fpage>
					<lpage>1258</lpage>
				</element-citation></ref><ref id="pcbi-0040027-b032"><label>32</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>CC</given-names></name><name name-style="western"><surname>Lin</surname><given-names>CJ</given-names></name></person-group>
					<year>2001</year>
					<source>LIBSVM: a Library for Support Vector Machines</source>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" xlink:type="simple">http://www.csie.ntu.edu.tw/∼cjlin/libsvm</ext-link>. Accessed 26 December 2007.</comment>
				</element-citation></ref></ref-list></back></article>