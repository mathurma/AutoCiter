<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
   <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
         <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
         <publisher-name>Public Library of Science</publisher-name>
         <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
   </journal-meta>
   <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00983</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1003024</article-id>
      <article-categories>
         <subj-group subj-group-type="heading">
            <subject>Research Article</subject>
         </subj-group>
<subj-group subj-group-type="Discipline-v2"><subject>Biology</subject>
<subj-group>
<subject>Neuroscience</subject>
<subj-group>
<subject>Computational neuroscience</subject>
<subject>Learning and memory</subject>
</subj-group>
</subj-group>
</subj-group>
      </article-categories>
      <title-group>
         <article-title>Reinforcement Learning Using a Continuous Time Actor-Critic Framework with Spiking Neurons</article-title>
         <alt-title alt-title-type="running-head">Actor-Critic Learning with Spiking Neurons</alt-title>
      </title-group>
      <contrib-group>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Frémaux</surname>
<given-names>Nicolas</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Sprekeler</surname>
<given-names>Henning</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Gerstner</surname><given-names>Wulfram</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref>
         </contrib>
      </contrib-group>
      <aff id="aff1"><label>1</label><addr-line>School of Computer and Communication Sciences and School of Life Sciences, Brain Mind Institute, École Polytechnique Fédérale de Lausanne, 1015 Lausanne EPFL, Switzerland</addr-line></aff>
      <aff id="aff2"><label>2</label><addr-line>Theoretical Neuroscience Lab, Institute for Theoretical Biology, Humboldt-Universität zu Berlin, Berlin, Germany</addr-line></aff>
      <contrib-group>
         <contrib contrib-type="editor" xlink:type="simple">
            <name name-style="western"><surname>Graham</surname>
<given-names>Lyle J.</given-names>
            </name><role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
         </contrib>
      </contrib-group>
      <aff id="edit1"><addr-line>Université Paris Descartes, Centre National de la Recherche Scientifique, France</addr-line></aff>
      <author-notes>
         <corresp id="cor1">* E-mail: <email xlink:type="simple">wulfram.gerstner@epfl.ch</email></corresp>
         <fn fn-type="conflict">
            <p>The authors have declared that no competing interests exist.</p>
         </fn>
         <fn fn-type="con">
            <p>Conceived and designed the experiments: NF HS. Performed the experiments: NF. Analyzed the data: NF. Wrote the paper: NF WG. Contributed to the analytical derivations: NF HS WG.</p>
         </fn>
      </author-notes>
      <pub-date pub-type="collection">
         <month>4</month>
         <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
         <day>11</day>
         <month>4</month>
         <year>2013</year>
      </pub-date>
      <volume>9</volume>
      <issue>4</issue>
      <elocation-id>e1003024</elocation-id>
      <history>
         <date date-type="received">
            <day>15</day>
            <month>6</month>
            <year>2012</year>
         </date>
         <date date-type="accepted">
            <day>22</day>
            <month>2</month>
            <year>2013</year>
         </date>
      </history>
      <permissions>
         <copyright-year>2013</copyright-year>
         <copyright-holder>Frémaux et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
      <abstract>
         <p>Animals repeat rewarded behaviors, but the physiological basis of reward-based learning has only been partially elucidated. On one hand, experimental evidence shows that the neuromodulator dopamine carries information about rewards and affects synaptic plasticity. On the other hand, the theory of reinforcement learning provides a framework for reward-based learning. Recent models of reward-modulated spike-timing-dependent plasticity have made first steps towards bridging the gap between the two approaches, but faced two problems. First, reinforcement learning is typically formulated in a discrete framework, ill-adapted to the description of natural situations. Second, biologically plausible models of reward-modulated spike-timing-dependent plasticity require precise calculation of the reward prediction error, yet it remains to be shown how this can be computed by neurons. Here we propose a solution to these problems by extending the continuous temporal difference (TD) learning of Doya (2000) to the case of spiking neurons in an actor-critic network operating in continuous time, and with continuous state and action representations. In our model, the critic learns to predict expected future rewards in real time. Its activity, together with actual rewards, conditions the delivery of a neuromodulatory TD signal to itself and to the actor, which is responsible for action choice. In simulations, we show that such an architecture can solve a Morris water-maze-like navigation task, in a number of trials consistent with reported animal performance. We also use our model to solve the acrobot and the cartpole problems, two complex motor control tasks. Our model provides a plausible way of computing reward prediction error in the brain. Moreover, the analytically derived learning rule is consistent with experimental evidence for dopamine-modulated spike-timing-dependent plasticity.</p>
      </abstract>
      <abstract abstract-type="summary">
         <title>Author Summary</title>
         <p>As every dog owner knows, animals repeat behaviors that earn them rewards. But what is the brain machinery that underlies this reward-based learning? Experimental research points to plasticity of the synaptic connections between neurons, with an important role played by the neuromodulator dopamine, but the exact way synaptic activity and neuromodulation interact during learning is not precisely understood. Here we propose a model explaining how reward signals might interplay with synaptic plasticity, and use the model to solve a simulated maze navigation task. Our model extends an idea from the theory of reinforcement learning: one group of neurons form an “actor,” responsible for choosing the direction of motion of the animal. Another group of neurons, the “critic,” whose role is to predict the rewards the actor will gain, uses the mismatch between actual and expected reward to teach the synapses feeding both groups. Our learning agent learns to reliably navigate its maze to find the reward. Remarkably, the synaptic learning rule that we derive from theoretical considerations is similar to previous rules based on experimental evidence.</p>
      </abstract>
      <funding-group>
         <funding-statement>This work was supported by Project FP7-243914(Brain-I-Nets) of the European Community's Seventh Framework Program (<ext-link ext-link-type="uri" xlink:href="http://cordis.europa.eu/fp7/home_en.html" xlink:type="simple">http://cordis.europa.eu/fp7/home_en.html</ext-link>) and a Sinergia grant of the Swiss National Science Foundation (SNF, <ext-link ext-link-type="uri" xlink:href="http://www.snf.ch" xlink:type="simple">http://www.snf.ch</ext-link>, grant no. CRSIK2\_122697, State representations in reward-based learning). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
<counts>
<page-count count="21"/>
</counts>
</article-meta>
</front>
<body>
   <sec id="s1">
      <title>Introduction</title>
      <p>Many instances of animal behavior learning such as path finding in foraging, or – a more artificial example – navigating the Morris water-maze, can be interpreted as exploration and trial-and-error learning. In both examples, the behavior eventually learned by the animal is the one that led to high reward. These can be appetite rewards (i.e., food) or more indirect rewards, such as the relief of finding the platform in the water-maze.</p>
      <p>Important progress has been made in understanding how learning of such behaviors takes place in the mammalian brain. On one hand, the framework of reinforcement learning <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref> provides a theory and algorithms for learning with sparse rewarding events. A particularly attractive formulation of reinforcement learning is temporal difference (TD) learning <xref ref-type="bibr" rid="pcbi.1003024-Sutton2">[2]</xref>. In the standard setting, this theory assumes that an agent moves between states in its environment by choosing appropriate actions in discrete time steps. Rewards are given in certain conjunctions of states and actions, and the agent's aim is to choose its actions so as to maximize the amount of reward it receives. Several algorithms have been developed to solve this standard formulation of the problem, and some of these have been used with spiking neural systems. These include REINFORCE <xref ref-type="bibr" rid="pcbi.1003024-Williams1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Xie1">[4]</xref> and partially observable Markov decision processes <xref ref-type="bibr" rid="pcbi.1003024-Baxter1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, in case the agent has incomplete knowledge of its state.</p>
      <p>On the other hand, experiments show that dopamine, a neurotransmitter associated with pleasure, is released in the brain when reward, or a reward-predicting event, occurs <xref ref-type="bibr" rid="pcbi.1003024-Schultz1">[7]</xref>. Dopamine has been shown to modulate the induction of plasticity in timing non-specific protocols <xref ref-type="bibr" rid="pcbi.1003024-Wickens1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Reynolds3">[11]</xref>. Dopamine has also recently been shown to modulate spike-timing-dependent plasticity (STDP), although the exact spike-timing and dopamine requirements for induction of long-term potentiation (LTP) and long-term depression (LTD) are still unclear <xref ref-type="bibr" rid="pcbi.1003024-Pawlak1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Pawlak2">[14]</xref>.</p>
      <p>A crucial problem in linking biological neural networks and reinforcement learning is that typical formulations of reinforcement learning rely on discrete descriptions of states, actions and time, while spiking neurons evolve naturally in continuous time and biologically plausible “time-steps” are difficult to envision. Earlier studies suggested that an external reset <xref ref-type="bibr" rid="pcbi.1003024-Potjans1">[15]</xref> or theta oscillations <xref ref-type="bibr" rid="pcbi.1003024-Vasilaki1">[16]</xref> might be involved, but no evidence exists to support this and it is not clear why evolution would favor slower decision steps over a continuous decision mechanism. Indeed biological decision making is often modeled by an integrative process in continuous time <xref ref-type="bibr" rid="pcbi.1003024-Gold1">[17]</xref>, where the actual decision is triggered when the integrated value reaches a threshold.</p>
      <p>In this study, we propose a way to narrow the conceptual gap between reinforcement learning models and the family of spike-timing-dependent synaptic learning rules by using continuous representations of state, actions and time, and by deriving biologically plausible synaptic learning rules. More precisely, we use a variation of the Actor-Critic architecture <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Barto1">[18]</xref> for TD learning. Starting from the continuous TD formulation by Doya <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, we derive reward-modulated STDP learning rules which enable a network of model spiking neurons to efficiently solve navigation and motor control tasks, with continuous state, action and time representations. This can be seen as an extension of earlier works <xref ref-type="bibr" rid="pcbi.1003024-Arleo1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Foster1">[21]</xref> to continuous actions, continuous time and spiking neurons. We show that such a system has a performance on par with that of real animals and that it offers new insight into synaptic plasticity under the influence of neuromodulators such as dopamine.</p>
   </sec>
   <sec id="s2">
      <title>Results</title>
      <p>How do animals learn to find their way through a maze? What kind of neural circuits underlie such learning and computation and what synaptic plasticity rules do they rely on? We address these questions by studying how a simulated animal (or <italic>agent</italic>) could solve a navigation task, akin to the Morris water-maze. Our agent has to navigate through a maze, looking for a (hidden) platform that triggers reward delivery and the end of the trial. We assume that our agent can rely on place cells <xref ref-type="bibr" rid="pcbi.1003024-OKeefe1">[22]</xref> for a representation of its current position in the maze (<xref ref-type="fig" rid="pcbi-1003024-g001">Figure 1</xref>).</p>
      <fig id="pcbi-1003024-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g001</object-id><label>Figure 1</label>
         <caption>
            <title>Navigation task and actor-critic network.</title>
            <p>From bottom to top: the simulated agent evolves in a maze environment, until it finds the reward area (green disk), avoiding obstacles (red). Place cells maintain a representation of the position of the agent through their tuning curves. Blue shadow: example tuning curve of one place cell (black); blue dots: tuning curves centers of other place cells. Right: a pool of critic neurons encode the expected future reward (value map, top right) at the agent's current position. The change in the predicted value is compared to the actual reward, leading to the temporal difference (TD) error. The TD error signal is broadcast to the synapses as part of the learning rule. Left: a ring of actor neurons with global inhibition and local excitation code for the direction taken by the agent. Their choices depending on the agent's position embody a policy map (top left).</p>
         </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g001" position="float" xlink:type="simple"/></fig>
      <p>Temporal difference learning methods provide a theory explaining how an agent should interact with its environment to maximize the rewards it receives. TD learning is built on the formalism of Markov decision processes. In what follows, we reformulate the framework of Markov decision process in continuous time, state and action, before we turn to the actor-critic neural network and the learning rule we used to solve the maze task.</p>
      <p>Let us consider a learning agent navigating through the maze. We can describe its position at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e001" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e002" xlink:type="simple"/></inline-formula>, corresponding to a continuous version of the state in the standard reinforcement learning framework. The temporal evolution of the state is governed by the agent's action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e003" xlink:type="simple"/></inline-formula>, according to<disp-formula id="pcbi.1003024.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e004" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e005" xlink:type="simple"/></inline-formula> describes the dynamics of the environment. Throughout this paper we use the dot notation to designate the derivative of a term with respect to time.</p>
      <p>We model place cells as simple spiking processes (inhomogeneous Poisson, see <xref ref-type="sec" rid="s4">Models</xref>) that fire only when the agent approaches their respective center. The centers are arranged on a grid, uniformly covering the surface of the maze.</p>
      <p>Reward is dispensed to the agent in the form of a reward rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e006" xlink:type="simple"/></inline-formula>. A localized reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e007" xlink:type="simple"/></inline-formula> at a single position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e008" xlink:type="simple"/></inline-formula> would correspond to the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e009" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e010" xlink:type="simple"/></inline-formula> denotes the Dirac <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e011" xlink:type="simple"/></inline-formula>-function. However, since any realistic reward (e.g., a piece of chocolate or the hidden platform in the water-maze) has a finite extent, we prefer to work with a temporally extended reward. In our model, rewards are attributed based on spatially precise events, but their delivery is temporally extended (see <xref ref-type="sec" rid="s4">Models</xref>). The agent is rewarded for reaching the goal platform and punished (negative reward) for running into walls.</p>
      <p>The agent follows a policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e012" xlink:type="simple"/></inline-formula> which determines the probability that an action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e013" xlink:type="simple"/></inline-formula> is taken in the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e014" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003024.e015"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e015" xlink:type="simple"/><label>(2)</label></disp-formula>The general aim of the agent is to find the policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e016" xlink:type="simple"/></inline-formula> that ensures the highest reward return in the long run.</p>
      <p>Several algorithms have been proposed to solve the discrete version of the reinforcement problem problem described above, such as Q-Learning <xref ref-type="bibr" rid="pcbi.1003024-Watkins1">[23]</xref> or Sarsa <xref ref-type="bibr" rid="pcbi.1003024-Sutton3">[24]</xref>. Both of these use a representation of the future rewards in form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e017" xlink:type="simple"/></inline-formula>-values for each state-action pair. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e018" xlink:type="simple"/></inline-formula>-values are then used both to evaluate the current policy (<italic>evaluation</italic> problem) and to choose the next action (<italic>control</italic> problem). As we show in <xref ref-type="sec" rid="s4">Models</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e019" xlink:type="simple"/></inline-formula>-values lead to difficulties when one wishes to move to a continuous representation while preserving biological plausibility. Instead, here we use an approach dubbed “Actor-Critic” <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Wickens1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Foster1">[21]</xref>, where the agent is separated in two parts: the control problem is solved by an <italic>actor</italic> and the evaluation problem is solved by a <italic>critic</italic> (<xref ref-type="fig" rid="pcbi-1003024-g001">Figure 1</xref>).</p>
      <p>The rest of the Results section is structured as follows. First we have a look at the TD formalism in continuous time. Next, we show how spiking neurons can implement a critic, to represent and learn the expected future rewards. Third, we discuss a spiking neuron actor, and how it can represent and learn a policy. Finally, simulation results show that our network successfully learns the simulated task.</p>
      <sec id="s2a">
         <title>Continuous TD</title>
         <p>The goal of a reinforcement learning agent is to maximize its future rewards. Following Doya <xref ref-type="bibr" rid="pcbi.1003024-Reynolds2">[10]</xref>, we define the continuous-time value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e020" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003024.e021"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e021" xlink:type="simple"/><label>(3)</label></disp-formula>where the brackets represent the expectation over all future trajectories <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e022" xlink:type="simple"/></inline-formula> and future action choices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e023" xlink:type="simple"/></inline-formula>, dependent on the policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e024" xlink:type="simple"/></inline-formula>. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e025" xlink:type="simple"/></inline-formula> represents the reward discount time constant, analogous to the discount factor of discrete reinforcement learning. Its effect is to make rewards in the near future more attractive than distant ones. Typical values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e026" xlink:type="simple"/></inline-formula> for a task such as the water-maze task would be on the order of a few seconds. <xref ref-type="disp-formula" rid="pcbi.1003024.e021">Eq. 3</xref> represents the total quantity of discounted reward that an agent in position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e027" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e028" xlink:type="simple"/></inline-formula> and following policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e029" xlink:type="simple"/></inline-formula> can expect. The policy should be chosen such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e030" xlink:type="simple"/></inline-formula> is maximized for all locations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e031" xlink:type="simple"/></inline-formula>. Taking the derivative of <xref ref-type="disp-formula" rid="pcbi.1003024.e021">Eq. 3</xref> with respect to time yields the self-consistency equation <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref><disp-formula id="pcbi.1003024.e032"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e032" xlink:type="simple"/><label>(4)</label></disp-formula></p>
         <p>Calculating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e033" xlink:type="simple"/></inline-formula> requires knowledge of the reward function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e034" xlink:type="simple"/></inline-formula> and of the environment dynamics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e035" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003024.e004">Eq 1</xref>). These are, however, unknown to the agent. Typically, the best an agent can do is to maintain a parametric estimator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e036" xlink:type="simple"/></inline-formula> of the “true” value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e037" xlink:type="simple"/></inline-formula>. This estimator being imperfect, it is not guaranteed to satisfy <xref ref-type="disp-formula" rid="pcbi.1003024.e032">Eq. 4</xref>. Instead, the temporal difference error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e038" xlink:type="simple"/></inline-formula> is defined as the mismatch in the self-consistency,<disp-formula id="pcbi.1003024.e039"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e039" xlink:type="simple"/><label>(5)</label></disp-formula>This is analog to the discrete TD error <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref><disp-formula id="pcbi.1003024.e040"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e040" xlink:type="simple"/><label>(6)</label></disp-formula>where the reward discount factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e041" xlink:type="simple"/></inline-formula> plays a role similar to the reward discount time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e042" xlink:type="simple"/></inline-formula>. More precisely, for short steps <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e043" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e044" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>.</p>
         <p>An estimator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e045" xlink:type="simple"/></inline-formula> can be said to be a good approximation to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e046" xlink:type="simple"/></inline-formula> if the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e047" xlink:type="simple"/></inline-formula> is close to zero for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e048" xlink:type="simple"/></inline-formula>. This suggests a simple way to learn a value function estimator: by a gradient descent on the squared TD error in the following way<disp-formula id="pcbi.1003024.e049"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e049" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e050" xlink:type="simple"/></inline-formula> is a learning rate parameter and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e051" xlink:type="simple"/></inline-formula> is the set of parameters (synaptic weights) that control the estimator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e052" xlink:type="simple"/></inline-formula> of the value function. This approach, dubbed residual gradient <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Baird1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Harmon1">[26]</xref>, yields a learning rule that is formally correct, but in our case suffers from a noise bias, as shown in <xref ref-type="sec" rid="s4">Models</xref>.</p>
         <p>Instead, we use a different learning rule, suggested for the discrete case by Sutton and Barto <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>. Translated in a continuous framework, the aim of their optimization approach is that the value function approximation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e053" xlink:type="simple"/></inline-formula> should match the true value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e054" xlink:type="simple"/></inline-formula>. This is equivalent to minimizing an objective function<disp-formula id="pcbi.1003024.e055"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e055" xlink:type="simple"/><label>(8)</label></disp-formula>A gradient descent learning rule on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e056" xlink:type="simple"/></inline-formula> yields<disp-formula id="pcbi.1003024.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e057" xlink:type="simple"/><label>(9)</label></disp-formula>Of course, because <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e058" xlink:type="simple"/></inline-formula> is unknown, this is not a particularly useful learning rule. On the other hand, using <xref ref-type="disp-formula" rid="pcbi.1003024.e032">Eq. 4</xref>, this becomes<disp-formula id="pcbi.1003024.e059"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e059" xlink:type="simple"/><label>(10)</label></disp-formula>where we merged <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e060" xlink:type="simple"/></inline-formula> into the learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e061" xlink:type="simple"/></inline-formula> without loss of generality. In the last step, we replaced the real value function derivative with its estimate, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e062" xlink:type="simple"/></inline-formula>, and then used the definition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e063" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003024.e039">Eq. 5</xref>.</p>
         <p>The substitution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e064" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e065" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref> is an approximation, and there is in general no guarantee that the two values are similar. However the form of the resulting learning rule suggests it goes in the direction of reducing the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e066" xlink:type="simple"/></inline-formula>. For example, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e067" xlink:type="simple"/></inline-formula> is positive at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e068" xlink:type="simple"/></inline-formula>, updating the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e069" xlink:type="simple"/></inline-formula> in the direction suggested by <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref>, will increase the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e070" xlink:type="simple"/></inline-formula>, and thus decrease <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e071" xlink:type="simple"/></inline-formula>.</p>
         <p>In <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, a heuristic shortcut was used to go directly from the residual gradient (<xref ref-type="disp-formula" rid="pcbi.1003024.e049">Eq. 7</xref>) to <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref>. As noted by Doya <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, the form of the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref> is a continuous version of the discrete <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e072" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Sutton4">[27]</xref> with function approximation (here with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e073" xlink:type="simple"/></inline-formula>). This has been shown to converge with probability 1 <xref ref-type="bibr" rid="pcbi.1003024-Dayan1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Tsitsiklis1">[29]</xref>, even in the case of infinite (but countable) state space. This must be the case also for arbitrarily small time steps (such as the finite steps usually used in computer simulations of a continuous system <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>), and thus it seems reasonable to expect that the continuous version also converges under reasonable assumptions, even though to date no proof exists.</p>
         <p>An important problem in reinforcement learning is the concept of temporal credit assignment, i.e., how to propagate information about rewards back in time. In the framework of TD learning, this means propagating the TD error at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e074" xlink:type="simple"/></inline-formula> so that the value function at earlier times is updated in consequence. The learning rule <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref> does not by itself offer a solution to this problem, because the expression of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e075" xlink:type="simple"/></inline-formula> explicitly refers only to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e076" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e077" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e078" xlink:type="simple"/></inline-formula>. Therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e079" xlink:type="simple"/></inline-formula> does not convey information about other times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e080" xlink:type="simple"/></inline-formula> and minimizing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e081" xlink:type="simple"/></inline-formula> does not <italic>a priori</italic> affect values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e083" xlink:type="simple"/></inline-formula>. This is in contrast to the discrete version of the TD error (<xref ref-type="disp-formula" rid="pcbi.1003024.e040">Eq. 6</xref>), where the expression of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e084" xlink:type="simple"/></inline-formula> explicitly links to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e085" xlink:type="simple"/></inline-formula> and thus the TD error is back-propagated during subsequent learning trials.</p>
         <p>If, however, one assumes that the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e086" xlink:type="simple"/></inline-formula> is continuous and continuously differentiable, changing the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e087" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e088" xlink:type="simple"/></inline-formula> implies changing the values of these functions in a finite vicinity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e089" xlink:type="simple"/></inline-formula>. This is in particular the case if one uses a parametric form for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e090" xlink:type="simple"/></inline-formula>, in the form of a weighted mixture of smooth kernels (as we do here, see next section). Therefore, the conjunction of a function approximation of the value function in the form of a linear combination of smooth kernels ensures that the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e091" xlink:type="simple"/></inline-formula> is propagated in time in the continuous case, allowing the temporal credit assignment problem to be solved.</p>
      </sec>
      <sec id="s2b">
         <title>Spiking Neuron Critic</title>
         <p>We now take the above derivation a step further by assuming that the value function estimation is performed by a spiking neuron with firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e092" xlink:type="simple"/></inline-formula>. A natural way of doing this is<disp-formula id="pcbi.1003024.e093"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e093" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e094" xlink:type="simple"/></inline-formula> is the value corresponding to no spiking activity and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e095" xlink:type="simple"/></inline-formula> is a scaling factor with units of [reward units]×s. A choice of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e096" xlink:type="simple"/></inline-formula> enables negative values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e097" xlink:type="simple"/></inline-formula>, despite the fact that the rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e098" xlink:type="simple"/></inline-formula> is always positive. We call this neuron a <italic>critic neuron</italic>, because its role is to maintain an estimate of the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e099" xlink:type="simple"/></inline-formula>.</p>
         <p>Several aspects should be discussed at this point. Firstly, since the value function in <xref ref-type="disp-formula" rid="pcbi.1003024.e093">Eq. 11</xref> must depend on the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e100" xlink:type="simple"/></inline-formula> of the agent, we must assume that the neuron receives some meaningful synaptic input about the state of the agent. In the following we make the assumption that this input is feed-forward from the place cells to the (spiking) critic neuron.</p>
         <p>Secondly, while the value function is in theory a function only of the state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e101" xlink:type="simple"/></inline-formula>, a spiking neuron implementation (such as the simplified model we use here, see <xref ref-type="sec" rid="s4">Models</xref>) will reflect the recent past, in a manner determined by the shape of the excitatory postsynaptic potentials (EPSP) it receives. This is a limitation shared by all neural circuits processing sensory input with finite synaptic delays. In the rest of this study, we assume that the evolution of the state of the agent is slow compared to the width of an EPSP. In that limit, the firing rate of a critic neuron at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e102" xlink:type="simple"/></inline-formula> actually reflects the position of the agent at that time.</p>
         <p>Thirdly, the firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e103" xlink:type="simple"/></inline-formula> of a single spike-firing neuron is itself a vague concept and multiple definitions are possible. Let's start from its spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e104" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e105" xlink:type="simple"/></inline-formula> is the set of the neuron's spike times and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e106" xlink:type="simple"/></inline-formula> is the Dirac delta, not to be confused with the TD signal). The expectation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e107" xlink:type="simple"/></inline-formula> is a statistical average of the neuron's firing over many repetitions. It is the theoretically favored definition of the firing rate, but in practice it is not available in single trials in a biologically plausible setting. Instead, a common workaround is to use a temporal average, for example by filtering the spike train with a kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e108" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003024.e109"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e109" xlink:type="simple"/><label>(12)</label></disp-formula>Essentially, this amounts to a trade-off between temporal accuracy and smoothness of the rate function, of which extreme cases are respectively the spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e110" xlink:type="simple"/></inline-formula> (extreme temporal accuracy) and a simple spike count over a long time window with smooth borders (no temporal information, extreme smoothness). In choosing a kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e111" xlink:type="simple"/></inline-formula>, it should hold that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e112" xlink:type="simple"/></inline-formula>, so that each spike is counted once, and one often wishes the kernel to be causal (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e113" xlink:type="simple"/></inline-formula>), so that the current firing rate is fully determined by <italic>past</italic> spike times and independent of future spikes.</p>
         <p>Another common approximation for the firing rate of a neuron consists in replacing the statistical average by a population average, over many neurons encoding the same value. Provided they are statistically independent of each other (for example if the neurons are not directly connected), averaging their responses over a single trial is equivalent to averaging the responses of a single neuron over the same number of trials.</p>
         <p>Here we combine temporal and population averaging, redefining the value function as an average firing rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e114" xlink:type="simple"/></inline-formula> neurons<disp-formula id="pcbi.1003024.e115"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e115" xlink:type="simple"/><label>(13)</label></disp-formula>where the instantaneous firing rate of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e116" xlink:type="simple"/></inline-formula> is defined by <xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eq. 12</xref>, using its spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e117" xlink:type="simple"/></inline-formula> and a kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e118" xlink:type="simple"/></inline-formula> defined by<disp-formula id="pcbi.1003024.e119"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e119" xlink:type="simple"/><label>(14)</label></disp-formula>This kernel rises with a time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e120" xlink:type="simple"/></inline-formula> and decays to 0 with time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e121" xlink:type="simple"/></inline-formula>. One advantage of the definition of <xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eq. 12</xref> is that the derivative of the firing rate of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e122" xlink:type="simple"/></inline-formula> with respect to time is simply<disp-formula id="pcbi.1003024.e123"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e123" xlink:type="simple"/><label>(15)</label></disp-formula>so that computing the derivative of the firing rate is simply a matter of filtering the spike train with the derivative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e124" xlink:type="simple"/></inline-formula> of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e125" xlink:type="simple"/></inline-formula> kernel. This way, the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e126" xlink:type="simple"/></inline-formula> of <xref ref-type="disp-formula" rid="pcbi.1003024.e039">Eq. 5</xref> can be expressed as<disp-formula id="pcbi.1003024.e127"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e127" xlink:type="simple"/><label>(16)</label></disp-formula>where, again, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e128" xlink:type="simple"/></inline-formula> denotes the spike train of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e129" xlink:type="simple"/></inline-formula> in the pool of critic neurons.</p>
         <p>Suppose that feed-forward weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e130" xlink:type="simple"/></inline-formula> lead from a state-representation neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e131" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e132" xlink:type="simple"/></inline-formula> in the population of critic neurons. Can the critic neurons learn to approximate the value function by changing the synaptic weights? An answer to this question is obtained by combining <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref> with <xref ref-type="disp-formula" rid="pcbi.1003024.e115">Eqs 13</xref> and <xref ref-type="disp-formula" rid="pcbi.1003024.e127">16</xref>, leading to a weights update<disp-formula id="pcbi.1003024.e133"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e133" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e134" xlink:type="simple"/></inline-formula> is the time course of an EPSP and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e135" xlink:type="simple"/></inline-formula> is the spike train of the presynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e136" xlink:type="simple"/></inline-formula>, restricted to the spikes posterior to the last spike time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e137" xlink:type="simple"/></inline-formula> of postsynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e138" xlink:type="simple"/></inline-formula>. For simplicity, we merged all constants into a new learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e139" xlink:type="simple"/></inline-formula>. A more formal derivation can be found in <xref ref-type="sec" rid="s4">Models</xref>.</p>
         <p>Let us now have a closer look at the shape of the learning rule suggested by <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>. The effective learning rate is given by a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e140" xlink:type="simple"/></inline-formula>. The rest of the learning rule consists of a product of two terms. The first one is the TD error term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e141" xlink:type="simple"/></inline-formula>, which is the same for all synapses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e142" xlink:type="simple"/></inline-formula>, and can thus be considered as a global factor, possibly transmitted by one or more neuromodulators (<xref ref-type="fig" rid="pcbi-1003024-g001">Figure 1</xref>). This neuromodulator broadcasts information about inconsistency between the reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e143" xlink:type="simple"/></inline-formula> and the value function encoded by the population of critic neurons to all neurons in the network. The second term is synapse-specific and reflects the coincidence of EPSPs caused by presynaptic spikes of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e144" xlink:type="simple"/></inline-formula> with the postsynaptic spikes of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e145" xlink:type="simple"/></inline-formula>. The postsynaptic term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e146" xlink:type="simple"/></inline-formula> is a consequence of the exponential non-linearity used in the neuron model (see <xref ref-type="sec" rid="s4">Models</xref>). This coincidence, “Hebbian” term is in turn filtered through the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e147" xlink:type="simple"/></inline-formula> kernel which corresponds to the effect of a postsynaptic spike on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e148" xlink:type="simple"/></inline-formula>. It reflects the responsibility of the synapse in the recent value function. Together these two terms form a three-factor rule, where the pre- and postsynaptic activities combine with the global signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e149" xlink:type="simple"/></inline-formula> to modify synaptic strengths (<xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2A</xref>, top). Because it has, roughly, the form of “TD error signal<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e150" xlink:type="simple"/></inline-formula>Hebbian LTP”, we call this learning rule TD-LTP.</p>
         <fig id="pcbi-1003024-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g002</object-id><label>Figure 2</label>
            <caption>
               <title>Critic learning in a linear track task.</title>
               <p>A: Learning rule with three factors. Top: TD-LTP is the learning rule given in <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>. It works by passing the presynaptic spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e151" xlink:type="simple"/></inline-formula> (factor 1) and the postsynaptic spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e152" xlink:type="simple"/></inline-formula> (factor 2) through a coincidence window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e153" xlink:type="simple"/></inline-formula>. Spikes are counted as coincident if the postsynaptic spike occurs within after a few ms of a presynaptic spike. The result of the pre-post coincidence measure is filtered through a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e154" xlink:type="simple"/></inline-formula> kernel, and then multiplied by the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e155" xlink:type="simple"/></inline-formula> (factor 3) to yield the learning rule which controls the change <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e156" xlink:type="simple"/></inline-formula> of the synaptic weight. Bottom: TD-STDP is a TD-modulated variant of R-STDP. The main difference with TD-LTP is the presence of a post-before-pre component in the coincidence window. B: Linear track task. The linear track experiment is a simplified version of the standard maze task. The actor's choice is forced to the correct direction with constant velocity (left), while the critic learns to represent value (right). C: Value function learning by the critic. Each colored trace shows the value function represented by the critic neurons activity against time in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e157" xlink:type="simple"/></inline-formula> first simulation trials (from dark blue in trial 1 to dark red in trial 20), with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e158" xlink:type="simple"/></inline-formula> corresponding to the time of the reward delivery. The black line shows an average over trials 30 to 50, after learning converged. The gray dashed line shows the theoretical value function. D: TD signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e159" xlink:type="simple"/></inline-formula> corresponding to the simulation in C. The gray dashed line shows the reward time course <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e160" xlink:type="simple"/></inline-formula>.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g002" position="float" xlink:type="simple"/></fig>
         <p>We would like to point out the similarity of the TD-LTP learning rule to a reward-modulated spike-timing-dependent plasticity rule we call R-STDP <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Vasilaki1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Izhikevich1">[30]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref>. In R-STDP, the effects of classic STDP <xref ref-type="bibr" rid="pcbi.1003024-Gerstner1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Song1">[36]</xref> are stored into an exponentially decaying, medium term (time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e161" xlink:type="simple"/></inline-formula>), synapse-specific memory, called an <italic>eligibility trace</italic>. This trace is only imprinted into the actual synaptic weights when a global, neuromodulatory success signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e162" xlink:type="simple"/></inline-formula> is sent to the synapses. In R-STDP, the neuromodulatory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e163" xlink:type="simple"/></inline-formula> is the reward minus a baseline, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e164" xlink:type="simple"/></inline-formula>. It was shown <xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref> that for R-STDP to maximize reward, the baseline must precisely match the mean (or expected) reward. In this sense, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e165" xlink:type="simple"/></inline-formula> is a reward prediction error signal; a system to compute this signal is needed. Since the TD error is also a reward prediction error signal, it seems natural to use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e166" xlink:type="simple"/></inline-formula> instead of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e167" xlink:type="simple"/></inline-formula>. This turns the reward-modulated learning rule R-STDP into a TD error-modulated TD-STDP rule (<xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2A</xref>, bottom). In this form, TD-STDP is very similar to TD-LTP. The major difference between the two is the influence of post-before-pre spike pairings on the learning rule: while these are ignored in TD-LTP, they cause a negative contribution to the coincidence detection in TD-STDP.</p>
         <p>The filtering kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e168" xlink:type="simple"/></inline-formula>, which was introduced to filter the spike trains into differentiable firing rates serves a role similar to the eligibility trace in R-STDP, and also in the discrete TD(<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e169" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>. As noted in the previous section, this is the consequence of the combination of a smooth parametric function approximation of the value function (each critic spike contributes a shape <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e170" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e171" xlink:type="simple"/></inline-formula>) and the form of the learning rule from <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref>. The filtering kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e172" xlink:type="simple"/></inline-formula> is crucial to back-propagation of the TD error, and thus to the solving of the temporal credit assignment problem.</p>
      </sec>
      <sec id="s2c">
         <title>Linear Track Simulation</title>
         <p>Having shown how spiking neurons can represent and learn the value function, we next test these results through simulations. However, in the actor-critic framework, the actor and the critic learn in collaboration, making it hard to disentangle the effects of learning in either of the two. To isolate learning by the critic and disregard potential problems of the actor, we temporarily sidestep this difficulty by using a forced action setup. We transform the water-maze into a linear track, and “clamp” the action choice to a value which leads the agent straight to the reward. In other words, the actor neurons are not simulated, see <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2B</xref>, and the agent simply “runs” to the goal. Upon reaching it at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e173" xlink:type="simple"/></inline-formula>, a reward is delivered and the trial ends.</p>
         <p><xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref> shows the value function over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e174" xlink:type="simple"/></inline-formula> color-coded trials (from blue to red) as learned by a critic using the learning rule we described above. On the first run (dark blue trace), the critic neurons are naive about the reward and therefore represent a (noisy version of a) zero value function. Upon reaching the goal, the TD error (<xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2D</xref>) matches the reward time course, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e175" xlink:type="simple"/></inline-formula>. According to the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>, this causes strengthening of those synapses that underwent pre-post activity recently before the reward (with “recent” defined by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e176" xlink:type="simple"/></inline-formula> kernel). This is visible already at the second trial, when the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e177" xlink:type="simple"/></inline-formula> just before reward becomes positive.</p>
         <p>In the next trials, this effect repeats, until the TD error vanishes. Suppose that, in a specific trials, reward starts at the time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e178" xlink:type="simple"/></inline-formula> when the agent has reached the goal. According to the definition of the TD error, for all times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e179" xlink:type="simple"/></inline-formula> the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e180" xlink:type="simple"/></inline-formula>-value is self consistent only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e181" xlink:type="simple"/></inline-formula> — or equivalently <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e182" xlink:type="simple"/></inline-formula>. The gray dashed line in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref> shows the time course of the theoretical value function; over many repetitions the colored traces, representing the value function in the different trials, move closer and closer to the theoretical value. The black line in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref> represents the average value function over 20 late trials, after learning has converged: it nicely matches the theoretical value.</p>
         <p>An interesting point that appears in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref> is the clearly visible back-propagation of information about the reward expressed in the shape of the value function. In the first trials, the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e183" xlink:type="simple"/></inline-formula> rises only for a short time just prior to the reward time. This causes, in the following trial, a TD error at earlier times. As trials proceed, synaptic weights corresponding to even earlier times increase. After <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e184" xlink:type="simple"/></inline-formula> trials in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref>, the value function roughly matches the theoretical value just prior to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e185" xlink:type="simple"/></inline-formula>, but not earlier. In subsequent trials, the point of mismatch is pushed back in time.</p>
         <p>This back-propagation phenomenon is a signature of TD learning algorithms. Two things should be noted here. Firstly, the speed with which the back-propagation occurs is governed by the shape of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e186" xlink:type="simple"/></inline-formula> kernel in the Hebbian part of the learning rule. It plays a role equivalent to the eligibility trace in reinforcement learning: it “flags” a synapse after it underwent pre-before-post activity with a decaying trace, a trace that is only consolidated into a weight change when a global confirmation signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e187" xlink:type="simple"/></inline-formula> arrives. This “eligibility trace” role of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e188" xlink:type="simple"/></inline-formula> is distinct from its original role in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e189" xlink:type="simple"/></inline-formula> term, where it is used to smooth the spiking activity of the critic neurons (<xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eq. 12</xref>). As such, one might be tempted to change the decay time constant of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e190" xlink:type="simple"/></inline-formula> term in the learning rule so as to control back-propagation speed, while keeping the “other” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e191" xlink:type="simple"/></inline-formula> of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e192" xlink:type="simple"/></inline-formula> signal fixed. In separate simulations (not shown), we found that such an ad-hoc approach did not lead to a gain in learning performance.</p>
         <p>Secondly, we know by construction that this back-propagation of the reward information is driven by the TD error signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e193" xlink:type="simple"/></inline-formula>. However, visual inspection of <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2D</xref>, which shows the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e194" xlink:type="simple"/></inline-formula> traces corresponding to the experiment in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref>, does not reveal any clear back-propagation of the TD error. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e195" xlink:type="simple"/></inline-formula>, a large peak mirroring the reward signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e196" xlink:type="simple"/></inline-formula> (gray dashed line) is visible in the early traces (blue lines) and recedes quickly as the value function correctly learns to expect the reward. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e197" xlink:type="simple"/></inline-formula>, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e198" xlink:type="simple"/></inline-formula> is dominated by fast noise, masking any back-propagation of the error signal, even though the fact that the value function is learned properly shows it is indeed present and effective. One might speculate that if a biological system was using such a TD error learning system with spiking neuron, and if an experimenter was to record a handful of critic neurons he would be at great pain to measure any significant TD error back-propagation. This is a possible explanation for the fact that no back-propagation signal has been observed in experiments.</p>
         <p>We have already discussed the structural similarity of a TD-modulated version of the R-STDP rule <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Izhikevich1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Legenstein1">[31]</xref> with TD-LTP. Simulations of the linear track experiment with the TD-STDP rule show that it behaves similarly to our learning rule (data not shown), i.e., the difference between the two rules (the post-before-pre part of the coincidence detection window, see <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2A</xref>) does not appear to play a crucial role in this case.</p>
      </sec>
      <sec id="s2d">
         <title>Spiking Neuron Actor</title>
         <p>We have seen above that spiking neurons in the “critic” population can learn to represent the expected rewards. We next ask how a spiking neuron agent chooses its actions so as to maximize the reward.</p>
         <p>In the classical description of reinforcement learning, actions, like states and time, are discrete. While discrete actions can occur, for example when a laboratory animal has to choose which lever to press, most motor actions, such as hand reaching or locomotion in space, are more naturally described by continuous variables. Even though an animal only has a finite number of neurons, neural coding schemes such as <italic>population vector coding</italic> <xref ref-type="bibr" rid="pcbi.1003024-Georgopoulos1">[37]</xref> allow a discrete number of neurons to code for a continuum of actions.</p>
         <p>We follow the population coding approach and define the actor as a group of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e199" xlink:type="simple"/></inline-formula> spiking neurons (<xref ref-type="fig" rid="pcbi-1003024-g003">Figure 3A</xref>), each coding for a different direction of motion. Like the critic neurons, these actor neurons receive connections from place cells, representing the current position of the agent. The spike trains generated by these neurons are filtered to produce a smooth firing rate, which is then multiplied by each neuron's preferred direction (see <xref ref-type="sec" rid="s4">Models</xref> for all calculation details). We finally sum these vectors to obtain the actual agent action at that particular time. To ensure a clear choice of actions, we use a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e200" xlink:type="simple"/></inline-formula>-winner-take-all lateral connectivity scheme: each neuron excites the neurons with similar tuning and inhibits all other neurons (<xref ref-type="fig" rid="pcbi-1003024-g003">Figure 3B</xref>). We manually adjusted the connection strength so that there was always a single “bump” of neurons active. An example of the activity in the pool of actor neurons and the corresponding action readout over a (successful) trial is given in <xref ref-type="fig" rid="pcbi-1003024-g003">Figure 3C</xref>. The corresponding maze trajectory is shown in <xref ref-type="fig" rid="pcbi-1003024-g003">Figure 3D</xref>.</p>
         <fig id="pcbi-1003024-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g003</object-id><label>Figure 3</label>
            <caption>
               <title>Actor neurons.</title>
               <p>A: A ring of actor neurons with lateral connectivity (bottom, green: excitatory, red: inhibitory) embodies the agent's policy (top). B: Lateral connectivity. Each neuron codes for a distinct motion direction. Neurons form excitatory synapses to similarly tuned neurons and inhibitory synapses to other neurons. C: Activity of actor neurons during an example trial. The activity of the neurons (vertical axis) is shown as a color map against time (horizontal axis). The lateral connectivity ensures that there is a single bump of activity at every moment in time. The black line shows the direction of motion (right axis; arrows in panel B) chosen as a result of the neural activity. D: Maze trajectory corresponding to the trial shown in C. The numbered position markers match the times marked in C.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g003" position="float" xlink:type="simple"/></fig>
         <p>In reinforcement learning, a successful agent has to balance exploration of unvisited states and actions in the search for new rewards, and exploitation of previously successful strategies. In our network, the exploration/exploitation balance is the result of the bump dynamics. To see this, let us consider a naive agent, characterized by uniform connections from the place cells to the actor neurons. For this agent, the bump first forms at random and then drifts without preference in the action space. This corresponds to random action choices, or full exploration. After the agent has been rewarded for reaching the goal, synaptic weights linking particular place cells to a particular action will be strengthened. This will increase the probability that the bump forms for that action the next time over. Thus the action choice will become more deterministic, and the agent will exploit the knowledge it has acquired over previous trials.</p>
         <p>Here, we propose to use the same learning rule for the actor neurons' synapses as for those of the critic neurons. The reason is the following. Let us look at the case where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e201" xlink:type="simple"/></inline-formula>: the critic is signaling that the recent sequence of actions taken by the agent has caused an unexpected reward. This means that the association between the action neurons that have recently been active and the state neurons whose input they have received should be strengthened so that the same action is more likely to be taken again in the next occurrence of that state. In the contrary case of a negative reinforcement signal, the connectivity to recently active action neurons should be weakened so that recently taken action are less likely to be taken again, leaving the way to, hopefully, better alternatives. This is similar to the way in which the synapses from the state input to the critic neurons should be strengthened or weakened, depending on their pre- and postsynaptic activities. This suggests that the action neurons should use the same synaptic learning rule as the one in <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e202" xlink:type="simple"/></inline-formula> now denoting the activity of the action neurons, but the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e203" xlink:type="simple"/></inline-formula> signal still driven by the critic activity. This is biologically plausible and consistent with our assumption that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e204" xlink:type="simple"/></inline-formula> is communicated by a neuromodulator, which broadcasts information over a large fraction of the brain.</p>
         <p>There are two critical effects of our <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e205" xlink:type="simple"/></inline-formula>-winner-take-all lateral connectivity scheme. Firstly, it ensures that only neurons coding for similar actions can be active at the same time. Because of the Hebbian part of the learning rule, this means that only those which are directly responsible for the action choice are subject to reinforcement, positive or negative. Secondly, by forcing the activity of the action neurons to take the shape of a group of similarly tuned neurons, it effectively causes generalization across actions: neurons coding for actions similar to the one chosen will also be active, and thus will also be given credit for the outcome of the action <xref ref-type="bibr" rid="pcbi.1003024-Vasilaki1">[16]</xref>. This is similar to the way the actor learns in non-neural actor-critic algorithms <xref ref-type="bibr" rid="pcbi.1003024-Barto1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, where only actions actually taken are credited by the learning rule. Thus, although an infinite number of actions are possible at each position, the agent does not have to explore every single one of them (an infinitely long task!) to learn the right strategy.</p>
         <p>The fact that both the actor and the critic use the same learning rule is in contrast with the original formulation of the actor-critic network of Barto et al. <xref ref-type="bibr" rid="pcbi.1003024-Barto1">[18]</xref>, where the critic learning rule is of the form “TD error×presynaptic activity”. As discussed above, the “TD error×Hebbian LTP” form of the critic learning rule <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref> used here is a result of the exponential non-linearity used in the neuron model. Using the same learning rule for the critic and the actor has the interesting property that a single biological plasticity mechanism has to be postulated to explain learning in both structures.</p>
      </sec>
      <sec id="s2e">
         <title>Water-Maze Simulation</title>
         <p>In the Morris water-maze, a rat or a mouse swims in an opaque-water pool, in search of a submerged platform. It is assumed that the animal is mildly inconvenienced by the water, and is actively seeking refuge on the platform, the reaching of which it experiences as a positive (rewarding) event. In our simulated navigation task, the learning agent (modeling the animal) is randomly placed at one out of four possible starting locations and moves in the two-dimensional space representing the pool (<xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4A</xref>). Its goal is to reach the goal area (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e206" xlink:type="simple"/></inline-formula> of the total area) which triggers the delivery of a reward signal and the end of the trial. Because the attractor dynamics in the pool of actor neurons make it natural for the agent to follow a straight line, we made the problem harder by surrounding the goal with a U-shaped obstacle so that from three out of four starting positions, the agent has to turn at least once to reach the target. Obstacles in the maze cause punishment (negative reward) when touched. Similar to what is customary in animal experiments, unsuccessful trials were interrupted (without reward delivery) when they exceeded a maximum duration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e207" xlink:type="simple"/></inline-formula>.</p>
         <fig id="pcbi-1003024-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g004</object-id><label>Figure 4</label>
            <caption>
               <title>Maze navigation learning task.</title>
               <p>A: The maze consists of a square enclosure, with a circular goal area (green) in the center. A U-shaped obstacle (red) makes the task harder by forcing turns on trajectories from three out of the four possible starting locations (crosses). B: Color-coded trajectories of an example TD-LTP agent during the first 75 simulated trials. Early trials (blue) are spent exploring the maze and the obstacles, while later trials (green to red) exploit stereotypical behavior. C: Value map (color map) and policy (vector field) represented by the synaptic weights of the agent of panel B after 2000s simulated seconds. D: Goal reaching latency of agents using different learning rules. Latencies of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e208" xlink:type="simple"/></inline-formula> simulated agents per learning rule are binned by 5 trials (trials 1–5, trials 6–10, etc.). The solid lines shows the median of the latencies for each trial bin and the shaded area represents the 25th to 75th percentiles. For the R-max rule these all fall in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e209" xlink:type="simple"/></inline-formula> time limit after which a trial was interrupted if the goal was not reached. The R-max agent were simulated without a critic (see main text).</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g004" position="float" xlink:type="simple"/></fig>
         <p>During a trial, the synapses continually update their efficacies according to the learning rule, <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>. When a trial ends, we simulate the animal being picked up from the pool by suppressing all place cell activity. This results in a quick fading away of all neural activity, causing the filtered Hebbian term in the learning rule to vanish and learning to effectively stop. After an inter-trial interval of 3s, the agent was positioned in a new random position, starting a new trial.</p>
         <p><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4B</xref> shows color-coded trajectories for a typical simulated agent. The naive agent spends most of the early trials (blue traces) learning to avoid walls and obstacles. The agent then encounters the goal, first at random through exploration, then repeatedly through reinforcement of the successful trajectories. Later trials (yellow to red traces) show that the agent mostly exploits stereotypical trajectories it has learned to reach the target.</p>
         <p>We can get interesting insight into what was learned during the trials shown in <xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4B</xref> by examining the weight of the synapses from the place cells to actor or critic neurons. <xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4C</xref> shows the input strength to critic neurons as a color map for every possible position of the agent. This is in effect a “value map”: the value the agent attributes to each position in the maze. In the same graph, the synaptic weights to the actor neurons are illustrated by a vector field representing a “policy preference map”. It is only a preference map, not a real policy map because the input from the place cells (represented by the arrows) compete with the lateral dynamics of the actor network, which is history-dependent (not represented).</p>
         <p>The value and policy maps that were learned are experience-dependent and unique to each agent: the agent shown in <xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4B and C</xref> first discovered how to reach the target from the “north” (N) starting position. It then discovered how to get to the N position from starting positions E and W, and finally to get to W from S. It has not however discovered the way from S to E. For that reason the value it attributes to the SE quarter is lower than to the symmetrically equivalent quarter SW. Similarly the policy in the SE quarter is essentially undefined, whereas the policy in the SW quarter clearly points in the correct direction.</p>
         <p><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4D</xref> shows the distribution of latency – the time it takes to reach the goal – as a function of trials, for 100 agents. Trials of naive agents end after an average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e210" xlink:type="simple"/></inline-formula> (trials were interrupted after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e211" xlink:type="simple"/></inline-formula>). This value quickly decreases for agents using the TD-LTP learning rule (green), as they learn to reach the reward reliably in about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e212" xlink:type="simple"/></inline-formula> trials.</p>
         <p>We previously remarked that the TD-LTP rule of <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref> is similar to TD-STDP, the TD-modulated version of the R-STDP rule <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Izhikevich1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Legenstein1">[31]</xref>, at least in form. To see whether they are also similar in effect, in our context, we simulated agents using the TD-STDP learning rule (for both critic and actor synapses). The blue line in <xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4D</xref> show that the performance was only slightly worse than that of the TD-LTP rule, confirming our finding on the linear track that both rules are functionally equivalent.</p>
         <p>Policy gradient methods <xref ref-type="bibr" rid="pcbi.1003024-Baxter1">[5]</xref> follow a very different approach to reinforcement learning to TD methods. A policy gradient method for spiking neurons is R-max <xref ref-type="bibr" rid="pcbi.1003024-Xie1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Pfister1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Legenstein2">[39]</xref>. In short, R-max works by calculating the covariance between Hebbian pre-before-post activity and reward. Because this calculation relies on averaging those values over many trials, R-max is an inherently slow rule, typically learning on hundreds or thousands of trials. One would therefore expect that it can't match the speed of learning of TD-LTP or TD-STDP. Another difference of R-max with the other learning rules studied is that it does not need a critic <xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref>. Therefore we simulated an agent using R-max that only had an actor, and replaced the TD-signal by the reward, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e213" xlink:type="simple"/></inline-formula>. The red line of <xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4</xref> show that, as expected, R-max agents learn much slower than previously simulated agent, if at all: learning is actually so slow, consistent with the usual timescales for that learning rule, that it can't be seen in the graph because this would require much longer simulations.</p>
         <p>One might object that using the R-max rule without a critic is unfair, and that it might benefit from a translation into a R-max rule with R = TD, by replacing the reward term by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e214" xlink:type="simple"/></inline-formula> error, as we did for R-STDP. But this overlooks two points. Firstly, such a “TD-max” rule could not be used to learn the critic: by construction, it would tend to maximize the TD error, which is the opposite of what the critic has to achieve. Secondly, even if one were to use a different rule (e.g. TD-LTP) to learn the critic, this would not solve the slow timescale problem. We experimented with agents using a “TD-max” actor while keeping TD-LTP for the critic, but could not find notable improvement over agents with an R-max actor (data not shown).</p>
      </sec>
      <sec id="s2f">
         <title>Acrobot Task</title>
         <p>Having shown that our actor-critic system could learn a navigation task, we now address a task that requires higher temporal accuracy and higher dimensional state spaces. We focus on the acrobot swing-up task, a standard control task in the reinforcement control literature. Here, the goal is to lift the outermost tip of a double pendulum under the influence of gravity above a certain level, using only a weak torque at the joint (<xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5A</xref>). The problem is similar to that of a gymnast hanging below an horizontal bar: her hands rotate freely around the bar, and the only way to induce motion is by twist of her hips. While a strong athlete might be able to lift her legs above her head in a single motion, our acrobot is too weak to manage this. Instead, the successful strategy consists in moving the legs back and forth to start a swinging motion, building up energy, until the legs reach the sufficient height.</p>
         <fig id="pcbi-1003024-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g005</object-id><label>Figure 5</label>
            <caption>
               <title>Acrobot task.</title>
               <p>A: The acrobot swing-up task figures a double pendulum, weakly actuated by a torque <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e215" xlink:type="simple"/></inline-formula> at the joint. The state of the pendulum is represented by the two angles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e216" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e217" xlink:type="simple"/></inline-formula> and the corresponding angular velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e218" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e219" xlink:type="simple"/></inline-formula>. The goal is to lift the tip above a certain height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e220" xlink:type="simple"/></inline-formula> above the fixed axis of the pendulum, corresponding to the length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e221" xlink:type="simple"/></inline-formula> of the segments. B: Goal reaching latency of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e222" xlink:type="simple"/></inline-formula> TD-LTP agents. The solid line shows the median of the latencies for each trial number and the shaded area represents the 25th to 75th percentiles of the agents performance. The red line represents a near-optimal strategy, obtained by the direct search method (see <xref ref-type="sec" rid="s4">Models</xref>). The blue line show the trajectory of one of the best amongst the 100 agents. The dotted line shows the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e223" xlink:type="simple"/></inline-formula> limit after which a trial was interrupted if the agent did not reach the goal. C: Example trajectory of an agent successfully reaching the goal height (green line).</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g005" position="float" xlink:type="simple"/></fig>
         <p>The position of the acrobot is fully described by two angles, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e224" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e225" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5A</xref>). However, the swinging motion required to solve the task means that even in the same angular position, different actions (torque) might be required, depending on whether the system is currently swinging to the left or to the right. For this reason, the angular velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e226" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e227" xlink:type="simple"/></inline-formula> are also important variables. Together, these four variables represent the state of the agent, the four-dimensional equivalent of the x–y coordinates in the navigation task. Just as in the water-maze case, place cells firing rates were tuned to specific points in the 4-dimensional space.</p>
         <p>Again similar to the maze navigation, the choice of the action (in this case the torque exerted on the pendulum joint) is encoded by the population vector of the actor neurons. The only two differences to the actor in the water-maze are that (i) the action is described by a single scalar and (ii) the action neuron attractor network is not on a closed ring anymore, but rather an open segment, encoding torques <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e228" xlink:type="simple"/></inline-formula> in the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e229" xlink:type="simple"/></inline-formula>.</p>
         <p>Several factors make the acrobot task harder than the water-maze navigation task. First, the state space is larger, with four dimensions against two. Because the number of place cells we use to represent the state of the agent grows exponentially with the dimension of the state space, this is a critical point. A larger number of place cells means that each is visited less often by the agent, making learning slower. At even higher dimensions, at some point the place cells approach is expected to fail. However, we want to show that it can still succeed in four dimensions.</p>
         <p>A second difficulty arises from the faster dynamics of the acrobot system with respect to the neural network dynamics. Although in simulations we are in full control of the timescales of both the navigation and acrobot dynamics, we wish to keep them in range with what might naturally occur for animals. As such the acrobot model requires fast control, with precision on the order of 100ms. Finally, the acrobot exhibits complex dynamics, chaotic in the control-less case. Whereas the optimal strategy for the navigation task consists in choosing an action (i.e., a direction) and sticking to it, solving the acrobot task requires precisely timed actions to successfully swing the pendulum out of its gravity well.</p>
         <p>In spite of these difficulties, our actor-critic network using the TD-LTP learning rule is able to solve the acrobot task, as <xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5B</xref> shows. We compared the performance to a near-optimal trajectory <xref ref-type="bibr" rid="pcbi.1003024-Boone1">[40]</xref>: although our agents are typically twice as slow to reach the goal, they still learn reasonable solutions to the problem. Because the agents start with mildly random initial synaptic weights (see <xref ref-type="sec" rid="s4">Models</xref>) and are subject to stochasticity, their history, and thus their performance, vary; the best agents have performance approaching that of the optimal controller (blue trace in <xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5B</xref>).</p>
      </sec>
      <sec id="s2g">
         <title>Cartpole Task</title>
         <p>We next try our spiking neuron actor-critic network on a harder control task, the cartpole swing-up problem <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>. This is a more difficult extension of cartpole balancing, a standard task in machine learning <xref ref-type="bibr" rid="pcbi.1003024-Barto1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Michie1">[41]</xref>. Here, a pole is attached to a wheeled cart, itself free to move on a rail of limited length. The pole can swing freely around its axle (it doesn't collide with the rail). The goal is to swing the pole upright, and, ideally, to keep it in that position for as long as possible. The only control that can be exerted on the system is a force <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e230" xlink:type="simple"/></inline-formula> on the cart (<xref ref-type="fig" rid="pcbi-1003024-g006">Figure 6A</xref>). As in the acrobot task, four variables are needed to describe the system: the position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e231" xlink:type="simple"/></inline-formula> of the cart, its velocity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e232" xlink:type="simple"/></inline-formula>, and the angle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e233" xlink:type="simple"/></inline-formula> and angular velocity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e234" xlink:type="simple"/></inline-formula> of the pole. We define a successful trial as a trial where the pole was kept upright (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e235" xlink:type="simple"/></inline-formula>) for more than 10 s, out of a maximum trial length of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e236" xlink:type="simple"/></inline-formula>. A trial is interrupted and the agent is punished for either hitting the edges of the rail (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e237" xlink:type="simple"/></inline-formula>) or “over-rotating” (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e238" xlink:type="simple"/></inline-formula>). Agents are rewarded (or punished) with a reward rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e239" xlink:type="simple"/></inline-formula>.</p>
         <fig id="pcbi-1003024-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g006</object-id><label>Figure 6</label>
            <caption>
               <title>Cartpole task.</title>
               <p>A: Cartpole swing-up problem (schematic). The cart slides on a rail of length 5, while the pole of length 1 rotates around its axis, subject to gravity. The state of the system is characterized by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e240" xlink:type="simple"/></inline-formula>, while the control variable is the force <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e241" xlink:type="simple"/></inline-formula> exerted on the cart. The agent receives a reward proportional to the height of the pole's tip. B: Cumulative number of “successful” trials as a function of total trials. A successful trial is defined as a trial where the pole angle was maintained up (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e242" xlink:type="simple"/></inline-formula>) for more than 10s, out of a maximum trial length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e243" xlink:type="simple"/></inline-formula>. The black line shows the median, and the shaded area represents the quartiles of 20 TD-LTP agents' performance, pooled in bins of 10 trials. The blue line shows the number of successful trials for a single agent. C: Average reward in a given trial. The average reward rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e244" xlink:type="simple"/></inline-formula> obtained during each trial is shown versus the trial number. After a rapid rise (inset, vertical axis same as main plot), the reward rises in a much slower timescale as the agents learn the finer control needed to keep the pole upright. The line and the area represent the median and the quartiles, as in B. D: Example agent behavior after 4000 trials. The three diagrams show three examples of the same agent recovering from unstable initial conditions (top: pole sideways, center: rightward speed near rail edge, bottom: small angle near rail edge).</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g006" position="float" xlink:type="simple"/></fig>
         <p>The cartpole task is significantly harder than the acrobot task and the navigation task. In the two latter ones, the agent only has to reach a certain region of the state space (the platform in the maze, or a certain height for the acrobot) to be rewarded and to cause the end of the trial. In contrast, the agent controlling the cartpole system must reach the region of the state space corresponding to the pole being upright (an unstable manifold), and must learn to fight adverse dynamics to stay in that position.</p>
         <p>For this reason learning to successfully control the cartpole system takes a large number of trials. In <xref ref-type="fig" rid="pcbi-1003024-g006">Figure 6B</xref>, we show the number of successful trials as a function of trial number. It takes the “median agent” (black line) on the order of 3500 trials to achieve 100 successful trials. This is slightly worse but on the same order of magnitude as the (non-neural) actor-critic of <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, which needs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e245" xlink:type="simple"/></inline-formula> trials to reach that performance.</p>
         <p>The evolution of average reward by trial (<xref ref-type="fig" rid="pcbi-1003024-g006">Figure 6C</xref>) shows that agents start with a phase of relatively quick progression (inset), corresponding to the agents learning to avoid the immediate hazard of running into the edges of the rail. This is followed by slower learning, as the agents learn to swing and control the pole better and better. To ease the long learning process we resorted to variable learning rates for both the actor and critic on the cartpole task: we used the average recent rewards obtained to choose the learning rate (see <xref ref-type="sec" rid="s4">Models</xref>). More precisely, when the reward was low, agents used a large learning rate, but when performance improved, the agents were able to learn finer control strategies with a small learning rate. Eventually agents manage fine control and easily recover from unstable situations (<xref ref-type="fig" rid="pcbi-1003024-g006">Figure 6D</xref>). Detailed analysis of the simulation results showed that our learning agents suffered from noise in the actor part of the network, hampering the fine control needed to keep the pole upright. For example, the agent in <xref ref-type="fig" rid="pcbi-1003024-g006">Figure 6D</xref> has learned how to recover from a falling pole (top and middle plots) but will occasionally take more time than strictly necessary to bring the pole to a vertical standstill (bottom plot). The additional spike firing noise in our spiking neuron implementation could potentially explain the performance difference with the actor-critic in <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>.</p>
      </sec>
   </sec>
   <sec id="s3">
      <title>Discussion</title>
      <p>In this paper, we studied reward-modulated spike-timing-dependent learning rules, and the neural networks in which they can be used. We derived a spike-timing-dependent learning rule for an actor-critic network and showed that it can solve a water-maze type learning task, as well as acrobot and cartpole swing-up tasks that both require mastering a difficult control problem. The derived learning rule is of high biological plausibility and resembles the family of R-STDP rules previously studied.</p>
      <sec id="s3a">
         <title>Biological Plausibility</title>
         <p>Throughout this study we tried to keep a balance between model simplicity and biological plausibility. Our network model is meant to be as simple and general as possible for an actor-critic architecture. We don't want to map it to a particular brain structure, but candidate mappings have already been proposed <xref ref-type="bibr" rid="pcbi.1003024-Houk1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Joel1">[43]</xref>. Although they do not describe particular brain areas, most components of our network resemble brain structures. Our place cells are very close to – and indeed inspired by – hippocampal place cells <xref ref-type="bibr" rid="pcbi.1003024-OKeefe1">[22]</xref>. Here we assume that the information encoded in place cells is available to the rest of the brain. Actor neurons, tuned to a particular action and linked to the animal level action through population vector coding are similar to classical models of motor or pre-motor cortices <xref ref-type="bibr" rid="pcbi.1003024-Georgopoulos1">[37]</xref>. So-called “ramp” neurons of the ventral striatum have long been regarded as plausible candidates for critic neurons: their ramp activity in the approach of rewards matches that of the theoretical critic. If one compares experimental data (for example <xref ref-type="fig" rid="pcbi-1003024-g007">Figure 7A</xref>, adapted from van der Meer and Redish <xref ref-type="bibr" rid="pcbi.1003024-vanderMeer1">[44]</xref>) and the activity of a typical critic neuron (<xref ref-type="fig" rid="pcbi-1003024-g007">Figure 7B</xref>), the resemblance is striking. The prime neuromodulatory candidate to transmit the global TD error signal to the synapses is dopamine: dopaminergic neurons have long been known to exhibit TD-like activity patterns <xref ref-type="bibr" rid="pcbi.1003024-Schultz1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Hollerman1">[45]</xref>.</p>
         <fig id="pcbi-1003024-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g007</object-id><label>Figure 7</label>
            <caption>
               <title>Biological plausibility.</title>
               <p>A: Firing rate of rat ventral striatum “ramp cells” during a maze navigation task. In the original experiment, the rat was rewarded in two different places, first by banana flavored food pellets, corresponding to the big drop in activity, then by neutral taste food pellets, corresponding to the end of small ramp. Adapted from van der Meer and Redish <xref ref-type="bibr" rid="pcbi.1003024-vanderMeer1">[44]</xref>. B: Firing rate of a single critic neuron in our model from the linear track task in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref>. The dashed line indicates the firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e246" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eq. 12</xref>) corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e247" xlink:type="simple"/></inline-formula>. C: Putative network to calculate the TD error using synaptic delays. The lower right group of neurons corresponds to the critic neurons we considered in this paper. Each group of neurons gets its input delayed by the amount of the synaptic delay <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e248" xlink:type="simple"/></inline-formula>. Provided the synapses have the adequate efficacies (not shown), this allows the calculation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e249" xlink:type="simple"/></inline-formula> and the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e250" xlink:type="simple"/></inline-formula>.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g007" position="float" xlink:type="simple"/></fig>
         <p>A problem of representing the TD error by dopamine concentration is that while the theoretically defined <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e251" xlink:type="simple"/></inline-formula> error signal can be positive as well as negative, dopamine concentration values [DA] are naturally bound to positive values <xref ref-type="bibr" rid="pcbi.1003024-Potjans2">[46]</xref>. This could be circumvented by positing a non-linear relation between the two values (e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e252" xlink:type="simple"/></inline-formula>) at the price of sensitivity changes over the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e253" xlink:type="simple"/></inline-formula> range. Even a simpler, piecewise linear scheme <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e254" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e255" xlink:type="simple"/></inline-formula> is the baseline dopamine concentration) would be sufficient, because learning works as long as the <italic>sign</italic> of the TD error is correct.</p>
         <p>Another possibility would be for the TD error to be carried in the positive range by dopamine, and in the negative range by some other neuromodulator. Serotonin, which appears to play a role similar to negative TD errors in reversal learning <xref ref-type="bibr" rid="pcbi.1003024-Robbins1">[47]</xref>, is a candidate. On the other hand this role of serotonin is seriously challenged by experimental recordings of the activity of dorsal raphe serotonin neurons during learning tasks <xref ref-type="bibr" rid="pcbi.1003024-Nakamura1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Miyazaki1">[49]</xref>, which fail to show activity patterns corresponding to an inverse TD signal.</p>
         <p>One of the aspects of our actor-critic model that was not implemented directly by spiking neurons but algorithmically, is the computation of the TD signal which depends on the reward, the value function and its derivative. In our model, this computation is crucial to the functioning of the whole. Addition and subtraction of the reward and the value function could be done through concurrent excitatory and inhibitory input onto a group of neurons. Similarly, the derivative of the value function could be done by direct excitation by a signal and delayed (for example by a an extra synapse) inhibition by the same signal (see example in <xref ref-type="fig" rid="pcbi-1003024-g007">Figure 7C</xref>). It remains to be seen whether such a circuit can effectively be used to compute a useful TD error. At any rate, connections from the the ventral striatum (putative critic) to the substantia nigra pars compacta (putative TD signal sender) show many excitatory and inhibitory pathways, in particular through the globus pallidus, which could have precisely this function <xref ref-type="bibr" rid="pcbi.1003024-Cohen1">[50]</xref>.</p>
      </sec>
      <sec id="s3b">
         <title>Limitations</title>
         <p>A crucial limitation of our approach is that we rely on relatively low-dimensional state and action representations. Because both use similar tuning/place cells representations, the number of neurons to represent these spaces has to grow exponentially with the number of dimensions, an example of the curse of dimensionality. While we show that we can still successfully solve problems with four-dimensional state description, this approach is bound to fail sooner or later, as dimensionality increases. Instead, the solution probably lies in “smart” pre-processing of the state space, to delineate useful and reward-relevant low dimensional manifolds on which place cells could be tuned. Indeed, the representation by place cells can be learned from visual input with thousands of “retinal” pixels, using standard unsupervised Hebbian learning rules <xref ref-type="bibr" rid="pcbi.1003024-Arleo1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Stroesslin1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Sheynikhovich1">[52]</xref>.</p>
         <p>Moreover, TD-LTP is derived with the assumption of sparse neural coding, with neurons having narrow tuning curves. This is in contrast to covariance-based learning rules <xref ref-type="bibr" rid="pcbi.1003024-Loewenstein1">[53]</xref>, such as R-max <xref ref-type="bibr" rid="pcbi.1003024-Xie1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Pfister1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Legenstein2">[39]</xref> which can, in theory, work with any coding scheme, albeit at the price of learning orders of magnitude slower.</p>
      </sec>
      <sec id="s3c">
         <title>Synaptic Plasticity and Biological Relevance of the Learning Rule</title>
         <p>Although a number of experimental studies exist <xref ref-type="bibr" rid="pcbi.1003024-Reynolds3">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Pawlak2">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Seol1">[54]</xref> targeting the relation between STDP and dopamine neuromodulation, one is at pain to draw precise conclusions as to how these two mechanism interplay in the brain. As such, it is hard to extract a precise learning rule from the experimental data. On the other hand, we can examine our TD-LTP learning rule in the light of experimental findings and see whether they match, i.e., whether a biological synapse implementing TD-LTP would produce the observed results.</p>
         <p>Experiments combining various forms of dopamine or dopamine receptor manipulation with high-frequency stimulation protocols at the cortico-striatal synapses provide evidence of an interaction between dopamine and synaptic plasticity <xref ref-type="bibr" rid="pcbi.1003024-Wickens1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Reynolds3">[11]</xref>. While these experiments are too coarse to resolve the spike-timing dependence, they form a picture of the dopamine dependence: it appears that at high concentration the effect of dopamine paired with high-frequency stimulation is the induction of long-term potentiation (LTP), while at lower concentrations, long-term depression (LTD) is observed. At a middle “baseline” concentration, no change is observed. This picture is consistent with TD-LTP or TD-STDP if one assumes a relation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e256" xlink:type="simple"/></inline-formula> between the dopamine concentration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e257" xlink:type="simple"/></inline-formula> and the TD error.</p>
         <p>The major difference between TD-LTP and TD-STDP is the behavior of the rule on post-before-pre spike pairings. While TD-LTP ignores these, TD-STDP causes LTD (resp. LTP) for positive (resp. negative) neuromodulation. Importantly this doesn't seem to play a large role for the learning capability of the rule, i.e., the pre-before-post is the only crucial part. This is interesting in the light of the study by Zhang et al. <xref ref-type="bibr" rid="pcbi.1003024-Zhang1">[13]</xref> on hippocampal synapses, that finds that extracellular dopamine puffs reverse the post-before-pre side of the learning window, while strengthening the pre-before-post side. This is compatible with the fact that polarity of the post-before-pre side of the learning window is not crucial to reward-based learning, and might serve another function.</p>
         <p>One result predicted by both TD-LTP and TD-STDP and that has not, to our knowledge, been observed experimentally, is the sign reversal of the pre-before-post under negative reward-prediction-error signals. This could be a result of the experimental challenges required to lower dopamine concentrations without reaching pathological levels of dopamine depression. However high-frequency stimulation-based experiments show that a reversal of the global polarity of long-term plasticity indeed happens <xref ref-type="bibr" rid="pcbi.1003024-Wickens1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Reynolds3">[11]</xref>. Moreover, a study by Seol et al. <xref ref-type="bibr" rid="pcbi.1003024-Seol1">[54]</xref> of STDP induction protocols under different (unfortunately not dopaminergic) neuromodulators shows that both sides of the STDP learning window can be altered in both polarity and strength. This shows that a sign change of the effect of the pre-then-post spike-pairings is at least within reach of the synaptic molecular machinery.</p>
         <p>Another prediction that stems from the present work is the existence of eligibility traces, closing the temporal gap between the fast time requirements of STDP and delayed rewards. The concept of eligibility traces is well explored in reinforcement learning <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Baxter1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Klopf1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Sutton5">[56]</xref>, and has previously been proposed for reward-modulated STDP rules <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Izhikevich1">[30]</xref>. Although our derivation of TD-LTP reaches an eligibility trace by a different path (filtering of the spike train signal, rather than explicitly solving the temporal credit assignment problem), the result is functionally the same. In particular, the time scales of the eligibility traces we propose, on the order of hundreds of milliseconds, are of the same magnitude as those proposed in models of reward-modulated STDP <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Izhikevich1">[30]</xref>. Direct experimental evidence of eligibility traces still lacks, but they are reminiscent of the synaptic tagging mechanism <xref ref-type="bibr" rid="pcbi.1003024-Frey1">[57]</xref>. Mathematical models of tagging <xref ref-type="bibr" rid="pcbi.1003024-Clopath1">[58]</xref>, using molecular cascades with varying timescales, provide an example of how eligibility traces could be implemented physiologically.</p>
      </sec>
      <sec id="s3d">
         <title>Insights for Reward-Modulated Learning in the Brain</title>
         <p>One interesting result of our study, is the fact that although our TD signal properly “teaches” the critic neurons the value function and back-propagates the reward information to more distant points, it is difficult to see the back-propagation in the time course of the TD signal itself. The reason for this is that the signal is drowned in rapid fluctuations. If one were to record a single neuron representing the TD error, it would probably be impossible to reconstruct the noiseless signal, except with an extremely high number of repetitions under the same conditions. This might be an explanation for the fact that the studies by Schultz and colleagues (e.g., <xref ref-type="bibr" rid="pcbi.1003024-Hollerman1">[45]</xref>) repeatedly fail to show back-propagation of the TD error, even though dopamine neurons seem to encode such a signal.</p>
         <p>In this study, TD-STDP (and TD-LTP) is used in a “gated-Hebbian” way: a synapse between A and B should be potentiated if it witnessed pre-before-post pairings and the TD signal following later is positive. This is fundamentally different from the role of the reward-modulated version of that learning rule (R-STDP) in <xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref>, where it is used to do covariance-based learning: a synapse between A and B should be potentiated if it witnesses positive correlation between pre-before-post pairings and a success signal, <italic>on average</italic>. One consequence of this is the timescale of learning: while TD-based learning takes tens of trials, covariance based learning typically requires hundreds or thousands of trials. The other side of the coin is that covariance-based learning is independent of the neural coding scheme, while TD-based learning requires neural tuning curves to map the relevant features prior to learning. The fact that the mathematical structure of the learning rule (i.e., a three-factor rule where the third factor “modulates” the effect of pre-post coincidences <xref ref-type="bibr" rid="pcbi.1003024-Wickens2">[59]</xref>) is the same in both cases is remarkable, and one can see the advantage that the brain might have had to evolve such a multifunctional tool — a sort of “Swiss army knife” of synaptic plasticity.</p>
      </sec>
   </sec>
   <sec id="s4">
      <title>Models</title>
      <sec id="s4a">
         <title>Neuron Model</title>
         <p>For the actor and critic neurons we simulated a simplified spike response model (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e258" xlink:type="simple"/></inline-formula>, <xref ref-type="bibr" rid="pcbi.1003024-Gerstner2">[60]</xref>). This model is a stochastic variant of the leaky integrate-and-fire neuron, with the membrane potential of neuron of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e259" xlink:type="simple"/></inline-formula> given by<disp-formula id="pcbi.1003024.e260"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e260" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e261" xlink:type="simple"/></inline-formula> is the efficacy of the synapse from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e262" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e263" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e264" xlink:type="simple"/></inline-formula> is the set of firing times of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e265" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e266" xlink:type="simple"/></inline-formula> is the membrane time constant, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e267" xlink:type="simple"/></inline-formula> scales the refractory effect, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e268" xlink:type="simple"/></inline-formula> is the Heaviside step function and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e269" xlink:type="simple"/></inline-formula> is the last spike of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e270" xlink:type="simple"/></inline-formula> prior to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e271" xlink:type="simple"/></inline-formula>. The EPSP is described by the time course<disp-formula id="pcbi.1003024.e272"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e272" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e273" xlink:type="simple"/></inline-formula> is the synaptic rise time and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e274" xlink:type="simple"/></inline-formula> is a scaling constant, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e275" xlink:type="simple"/></inline-formula> is the membrane time constant, as in <xref ref-type="disp-formula" rid="pcbi.1003024.e260">Eq. 18</xref>. Given the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e276" xlink:type="simple"/></inline-formula>, spike firing in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e277" xlink:type="simple"/></inline-formula> is an inhomogeneous Poisson process: at every moment the neuron has a probability of emitting a spike, according to an instantaneous firing rate<disp-formula id="pcbi.1003024.e278"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e278" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e279" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e280" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e281" xlink:type="simple"/></inline-formula> are constants consistent with experimental values <xref ref-type="bibr" rid="pcbi.1003024-Jolivet1">[61]</xref>. In the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e282" xlink:type="simple"/></inline-formula>, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e283" xlink:type="simple"/></inline-formula> becomes a deterministic leaky integrate-and-fire neuron.</p>
      </sec>
      <sec id="s4b">
         <title>Navigation Task</title>
         <p>The Morris water-maze pool is modeled by a two-dimensional plane delimited by a square wall. The position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e284" xlink:type="simple"/></inline-formula> of the agent on the plane obeys<disp-formula id="pcbi.1003024.e285"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e285" xlink:type="simple"/><label>(21)</label></disp-formula>When the agent is within boundaries it moves with speed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e286" xlink:type="simple"/></inline-formula>, as defined by the actor neurons' activity (<xref ref-type="disp-formula" rid="pcbi.1003024.e411">Eq.29</xref>). Whenever the agent encounters a wall, it instantly “bounces” back a distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e287" xlink:type="simple"/></inline-formula> along unitary vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e288" xlink:type="simple"/></inline-formula>, which points inward, perpendicular to the obstacle surface. Every “bumping” against a wall is accompanied by a punishing, negative reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e289" xlink:type="simple"/></inline-formula> delivery (see reward delivery dynamics below).</p>
         <p>We used two variants of the navigation task. The linear track is a narrow rectangle of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e290" xlink:type="simple"/></inline-formula> centered around the origin, featuring a single starting position in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e291" xlink:type="simple"/></inline-formula> and a wide goal area (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e292" xlink:type="simple"/></inline-formula>) on the opposite side. Because the goal of this setup is to study critic learning, the action is clamped to a fixed value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e293" xlink:type="simple"/></inline-formula>, so that the agent runs toward the goal at a fixed speed.</p>
         <p>The second variant is the navigation maze with obstacle. It consists of a square area of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e294" xlink:type="simple"/></inline-formula> centered around the origin, with four starting positions at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e295" xlink:type="simple"/></inline-formula>. The goal area is a circle of radius <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e296" xlink:type="simple"/></inline-formula> centered in the middle of the maze. The goal is surrounded on three sides by a U-shaped obstacle (width of each segment: 2, length: 10).</p>
         <p>In both variants, place cells centers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e297" xlink:type="simple"/></inline-formula> are disposed on a grid (blue dots on <xref ref-type="fig" rid="pcbi-1003024-g001">Figure 1</xref>), with spacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e298" xlink:type="simple"/></inline-formula> coinciding with the width of the place fields. The outermost centers lie a distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e299" xlink:type="simple"/></inline-formula> outside the maze boundaries. This ensures a smooth coverage of the whole state space. In the case of the maze, the place cell grid consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e300" xlink:type="simple"/></inline-formula> centers. For the linear track setup, the grid has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e301" xlink:type="simple"/></inline-formula> centers.</p>
         <p>Trials start with the agent's position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e302" xlink:type="simple"/></inline-formula> being randomly chosen from one out of four possible starting positions. The place cells, indexed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e303" xlink:type="simple"/></inline-formula>, are inhomogeneous Poisson processes. After a trial starts, the place cells' instantaneous firing rates are updated to<disp-formula id="pcbi.1003024.e304"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e304" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e305" xlink:type="simple"/></inline-formula> is a constant regulating the activity of the place cells, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e306" xlink:type="simple"/></inline-formula> is the place cells separation distance and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e307" xlink:type="simple"/></inline-formula> are the place cells centers. The presynaptic activity in the place cells generates activity in the post-synaptic neurons of the critic and the actor with a small delay caused by the rise time of EPSPs.</p>
         <p>The value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e308" xlink:type="simple"/></inline-formula> is calculated according to <xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eqs 12</xref> and <xref ref-type="disp-formula" rid="pcbi.1003024.e115">13</xref>, with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e309" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e310" xlink:type="simple"/></inline-formula>. Because <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e311" xlink:type="simple"/></inline-formula> is delayed by the rise time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e312" xlink:type="simple"/></inline-formula> of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e313" xlink:type="simple"/></inline-formula> kernel, at the start of a trial the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e314" xlink:type="simple"/></inline-formula> is subject to large, boundary effect transients. To cancel these artifacts, we clamp the TD error to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e315" xlink:type="simple"/></inline-formula>, for the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e316" xlink:type="simple"/></inline-formula> of each trial. We use a reward discount time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e317" xlink:type="simple"/></inline-formula>.</p>
         <p>The goal of the agent is to reach the circular area which represents the submerged platform of the water-maze. When the agent reaches this platform, a positive reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e318" xlink:type="simple"/></inline-formula> is delivered, the trial ends and the agent is put in a so-called “neutral state”, which models the removal of the animal from the experiment area. The effects of this is (i) the place cells corresponding to the maze become silent, presumably replaced by other (not modeled) place cells, and (ii) the expectation of the animal becomes neutral, and therefore its value function goes to zero. So at the end of a trial, we turn off place cell activity (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e319" xlink:type="simple"/></inline-formula>), and the value function is no longer given by <xref ref-type="disp-formula" rid="pcbi.1003024.e115">Eq. 13</xref>, but decays exponentially to 0 with time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e320" xlink:type="simple"/></inline-formula> from its value at the time of the end of the trial. Importantly, synaptic plasticity continues after the end of the trial, so that the effect of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e321" xlink:type="simple"/></inline-formula> affects the synaptic weight even though its delivery takes place in the neutral state. Additionally, a trial can end without the platform being reached: if a trial exceeds the time limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e322" xlink:type="simple"/></inline-formula>, it is declared a failed trial, and interrupted with the agent put in the neutral state, just as in the successful case, but without reward being delivered.</p>
         <p>According to <xref ref-type="disp-formula" rid="pcbi.1003024.e021">Eq. 3</xref>, rewards are given to the agent as a reward rate. This reflects the fact that “natural” rewards, and reward consumption, are spread over time, rather than point-like events. So we translate absolute rewards (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e323" xlink:type="simple"/></inline-formula>) to a reward rate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e324" xlink:type="simple"/></inline-formula>), calculated as the difference of two decaying “traces” obeying dynamics<disp-formula id="pcbi.1003024.e325"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e325" xlink:type="simple"/><label>(23)</label></disp-formula>i.e.,<disp-formula id="pcbi.1003024.e326"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e326" xlink:type="simple"/><label>(24)</label></disp-formula>At most times, the reward is close to 0. Reward is delivered only when some event (goal reached or collision against an obstacle) occurs. The delivery of a reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e327" xlink:type="simple"/></inline-formula> happens through instantaneous update of the traces<disp-formula id="pcbi.1003024.e328"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e328" xlink:type="simple"/><label>(25)</label></disp-formula>The resulting effect is a subsequent positive excursion of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e329" xlink:type="simple"/></inline-formula>, with rise time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e330" xlink:type="simple"/></inline-formula> and fall time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e331" xlink:type="simple"/></inline-formula>, which, integrated over time, amounts to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e332" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4c">
         <title>Acrobot Task</title>
         <p>In the acrobot task, the position of the pendulum is described by two angles: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e333" xlink:type="simple"/></inline-formula> is the angle between the first segment of the pendulum and the vertical, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e334" xlink:type="simple"/></inline-formula> is the angle between the second segment and an imaginary prolongation of the first (<xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5A</xref>). When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e335" xlink:type="simple"/></inline-formula>, the pendulum hangs down. Critical to solving the task are also the angular velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e336" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e337" xlink:type="simple"/></inline-formula>. As in the maze navigation case, place cells tuned to specific centers are used to represent the state of the acrobot. We transform the angular velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e338" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e339" xlink:type="simple"/></inline-formula>. This allows a fine resolution over small velocities, while maintaining a representation of higher velocities with a small number of place cells. The state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e340" xlink:type="simple"/></inline-formula> is represented by the four variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e341" xlink:type="simple"/></inline-formula>.</p>
         <p>The place cells centers are disposed on a 4-dimensional grid defined by indexes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e342" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e343" xlink:type="simple"/></inline-formula> with<disp-formula id="pcbi.1003024.e344"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e344" xlink:type="simple"/><label>(26)</label></disp-formula>This yields a total of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e345" xlink:type="simple"/></inline-formula> centers. The activity of a place cell with center <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e346" xlink:type="simple"/></inline-formula> is defined by<disp-formula id="pcbi.1003024.e347"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e347" xlink:type="simple"/><label>(27)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e348" xlink:type="simple"/></inline-formula> is a function returning the difference between two angles modulo <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e349" xlink:type="simple"/></inline-formula> in the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e350" xlink:type="simple"/></inline-formula> and the place cell widths <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e351" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e352" xlink:type="simple"/></inline-formula> correspond to the grid spacing as in <xref ref-type="disp-formula" rid="pcbi.1003024.e344">Eq. 26</xref>.</p>
         <p>The acrobot dynamics obeys the following equations <xref ref-type="bibr" rid="pcbi.1003024-Sutton1">[1]</xref>:<disp-formula id="pcbi.1003024.e353"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e353" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e354"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e354" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e355"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e355" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e356"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e356" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e357"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e357" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e358"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e358" xlink:type="simple"/></disp-formula>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e359" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e360" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e361" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e362" xlink:type="simple"/></inline-formula> are convenience variables, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e363" xlink:type="simple"/></inline-formula> is the torque applied to the joint, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e364" xlink:type="simple"/></inline-formula> are the lengths of the segments, of mass <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e365" xlink:type="simple"/></inline-formula>, with moments of inertia <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e366" xlink:type="simple"/></inline-formula> and lengths to the centers of mass <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e367" xlink:type="simple"/></inline-formula>, under the influence of gravity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e368" xlink:type="simple"/></inline-formula>. All dimensions except time are unit-less.</p>
         <p>The goal is for the tip of the acrobot to reach a height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e369" xlink:type="simple"/></inline-formula> over the axis, i.e., fulfill the condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e370" xlink:type="simple"/></inline-formula>. Once this happens, or the maximum trial time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e371" xlink:type="simple"/></inline-formula> is reached, the trial ends. To entice the acrobot to do something, we give an ongoing punishment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e372" xlink:type="simple"/></inline-formula> to the agent for not reaching the reward, to be compared with the reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e373" xlink:type="simple"/></inline-formula> received at the goal. As in the water-maze case, we use a reward discount time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e374" xlink:type="simple"/></inline-formula>.</p>
         <p>Due to the larger number of place cells, we use less critic and actor neurons than in the maze case, respectively <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e375" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e376" xlink:type="simple"/></inline-formula>, to reduce the number of synapses and the computational load.</p>
         <p>To compare the performance of our agent against an “optimal” strategy, we use the direct search method <xref ref-type="bibr" rid="pcbi.1003024-Boone1">[40]</xref>. The main idea behind the method is to search for the sequence of action that will maximize the system's total energy, with knowledge of the acrobot dynamics. To make the search computationally tractable, a few simplifications are made: actions are limited to the alternative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e377" xlink:type="simple"/></inline-formula>, actions are only taken in steps of 100 ms, only a window of the next 10 steps is considered at a time, and the number of action switch in each window is limited to 2. Thus only 55 action sequences have to be examined, and the sequence that maximizes the total energy reached over the window, or reaches the goal height the sooner, is selected. The first action in that sequence is chosen as the action for the next step and the whole procedure is repeated with the window shifted by one step. The goal height reaching latency found with this method was 7.66s (red line in <xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5B</xref>).</p>
      </sec>
      <sec id="s4d">
         <title>Cartpole Task</title>
         <p>The position of the cartpole system is described by the cart position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e378" xlink:type="simple"/></inline-formula>, the cart velocity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e379" xlink:type="simple"/></inline-formula>, the angle of the pole with the vertical <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e380" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e381" xlink:type="simple"/></inline-formula> corresponds to the pole pointing upwards) and the angular velocity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e382" xlink:type="simple"/></inline-formula>; these form the state vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e383" xlink:type="simple"/></inline-formula>. Similar to the acrobot, the place cells for the cartpole problem are regularly disposed on a four-dimensional grid of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e384" xlink:type="simple"/></inline-formula> cells. The location of a place cell with index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e385" xlink:type="simple"/></inline-formula> is at location <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e386" xlink:type="simple"/></inline-formula> with<disp-formula id="pcbi.1003024.e387"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e387" xlink:type="simple"/><label>(28)</label></disp-formula>The activity of a place cell is defined in a way analog to <xref ref-type="disp-formula" rid="pcbi.1003024.e347">Eq. 27</xref>. The variance of the gaussian place fields is diagonal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e388" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e389" xlink:type="simple"/></inline-formula> corresponds to the grid spacing in dimension <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e390" xlink:type="simple"/></inline-formula>.</p>
         <p>The dynamics of the cartpole are <xref ref-type="bibr" rid="pcbi.1003024-Florian2">[62]</xref>:<disp-formula id="pcbi.1003024.e391"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e391" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e392"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e392" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003024.e393"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e393" xlink:type="simple"/></disp-formula>Here <italic>a = v</italic> is the acceleration of the cart, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e394" xlink:type="simple"/></inline-formula> is half the pole's length, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e395" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e396" xlink:type="simple"/></inline-formula> are coefficients of friction of the cart on the track and of pole rotation respectively. The cart, with mass <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e397" xlink:type="simple"/></inline-formula>, and the pole, with mass <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e398" xlink:type="simple"/></inline-formula>, are subject to the acceleration of gravity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e399" xlink:type="simple"/></inline-formula>. As in the acrobot case, all dimensions except time are unit-less.</p>
         <p>Following <xref ref-type="bibr" rid="pcbi.1003024-Doya1">[19]</xref>, the agent is rewarded continuously depending on the current height of the pole with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e400" xlink:type="simple"/></inline-formula>, and the reward discount time constant is set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e401" xlink:type="simple"/></inline-formula>. If the cart runs off its rail (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e402" xlink:type="simple"/></inline-formula>) or over-rotates (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e403" xlink:type="simple"/></inline-formula>) the trial is ended and a negative reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e404" xlink:type="simple"/></inline-formula> is given. A trial ends without reward after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e405" xlink:type="simple"/></inline-formula>. When a new trial starts, the position of the system is initialized with a random <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e406" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e407" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4e">
         <title>Actor Dynamics</title>
         <p>In population vector coding, each actor neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e408" xlink:type="simple"/></inline-formula> “votes” for its preferred action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e409" xlink:type="simple"/></inline-formula> in the action space, by firing an action potential. An action vector is obtained by averaging the product of the instantaneous firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e410" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eq. 12</xref>) and the action vector of each neuron, i.e.<disp-formula id="pcbi.1003024.e411"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e411" xlink:type="simple"/><label>(29)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e412" xlink:type="simple"/></inline-formula> is defined as<disp-formula id="pcbi.1003024.e413"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e413" xlink:type="simple"/><label>(30)</label></disp-formula>with filter<disp-formula id="pcbi.1003024.e414"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e414" xlink:type="simple"/></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e415" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e416" xlink:type="simple"/></inline-formula> being filtering time constants. The term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e417" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003024.e411">Eq. 29</xref> is a normalization term. In the case of the navigation task (two-dimensional action), it is equal to the number of actor neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e418" xlink:type="simple"/></inline-formula>. In the cases of the acrobot and the cartpole task (scalar action), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e419" xlink:type="simple"/></inline-formula>.</p>
         <p>We enforce a N-winner-takes-all mechanism on the action neurons by imposing “lateral” connectivity between the action neurons: action neurons coding for similar actions excite each other, while they inhibit the neurons coding for dissimilar actions. The synaptic weight between two action neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e420" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e421" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1003024.e422"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e422" xlink:type="simple"/><label>(32)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e423" xlink:type="simple"/></inline-formula> is a lateral connectivity function. This is zero for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e424" xlink:type="simple"/></inline-formula>, peaks for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e425" xlink:type="simple"/></inline-formula> and monotonously decreases towards 0 as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e426" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e427" xlink:type="simple"/></inline-formula> diverge. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e428" xlink:type="simple"/></inline-formula> is a normalization constant. The parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e429" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e430" xlink:type="simple"/></inline-formula> regulating the recurrent connections were manually tuned: the lateral connectivity has to be strong enough so that there is always exactly one “bump” of similarly tuned neurons active whenever the action neurons receive some excitation from the place cells, but not so strong that it completely dominates the feed-forward input from the place cells.</p>
         <p>The preferred vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e431" xlink:type="simple"/></inline-formula> of the action neurons and the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e432" xlink:type="simple"/></inline-formula> are dependent on the learning task. In the case of the maze navigation task, the preferred action vectors are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e433" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e434" xlink:type="simple"/></inline-formula> is a constant representing the agent velocity per rate unit and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e435" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e436" xlink:type="simple"/></inline-formula>. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e437" xlink:type="simple"/></inline-formula> function was chosen as<disp-formula id="pcbi.1003024.e438"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e438" xlink:type="simple"/><label>(33)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e439" xlink:type="simple"/></inline-formula>.</p>
         <p>In the case of the acrobot and cartpole tasks, the action vectors are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e440" xlink:type="simple"/></inline-formula>. For the acrobot <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e441" xlink:type="simple"/></inline-formula> represents the maximum torque that the agent can exert and for the cartpole task <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e442" xlink:type="simple"/></inline-formula> is the maximum force on the cart. The lateral connectivity function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e443" xlink:type="simple"/></inline-formula> in both cases was chosen as<disp-formula id="pcbi.1003024.e444"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e444" xlink:type="simple"/><label>(34)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e445" xlink:type="simple"/></inline-formula>. Additionally, we algorithmically constrain the torque exerted by the agent to the domain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e446" xlink:type="simple"/></inline-formula>. This models the limited strength of the agent's “muscles”.</p>
      </sec>
      <sec id="s4f">
         <title>Other Reward-Modulated Synaptic Learning Rules</title>
         <p>In R-STDP <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Izhikevich1">[30]</xref>–<xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref>, the effects of classic STDP are modulated by a neuromodulatory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e447" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e448" xlink:type="simple"/></inline-formula> is a constant baseline. We transformed the reward-modulated R-STDP into the TD-modulated rule TD-STDP by replacing the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e449" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e450" xlink:type="simple"/></inline-formula>. The TD-STDP rule can be written as<disp-formula id="pcbi.1003024.e451"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e451" xlink:type="simple"/><label>(35)</label></disp-formula>where the STDP learning window is<disp-formula id="pcbi.1003024.e452"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e452" xlink:type="simple"/></disp-formula>The eligibility trace kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e453" xlink:type="simple"/></inline-formula> is the result of an exponential decay, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e454" xlink:type="simple"/></inline-formula>, with time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e455" xlink:type="simple"/></inline-formula>. The positive constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e456" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e457" xlink:type="simple"/></inline-formula> govern the size of the pre-before-post and post-before-pre parts of the learning window respectively, and the time constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e458" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e459" xlink:type="simple"/></inline-formula> determine their timing requirement.</p>
         <p>R-max <xref ref-type="bibr" rid="pcbi.1003024-Xie1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Florian1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Frmaux1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Pfister1">[38]</xref> is a reward-modulated learning rule derived from policy gradient principles <xref ref-type="bibr" rid="pcbi.1003024-Baxter1">[5]</xref>. It can be written as<disp-formula id="pcbi.1003024.e460"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e460" xlink:type="simple"/><label>(36)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e461" xlink:type="simple"/></inline-formula> is the instantaneous firing rate of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e462" xlink:type="simple"/></inline-formula>, as defined in <xref ref-type="disp-formula" rid="pcbi.1003024.e278">Eq. 20</xref>.</p>
      </sec>
      <sec id="s4g">
         <title>Simulation Details</title>
         <p>Initial values of the synaptic weights to both critic and actor were randomly drawn from a normal distribution with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e463" xlink:type="simple"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e464" xlink:type="simple"/></inline-formula>. These values ensured an initial value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e465" xlink:type="simple"/></inline-formula> and reasonable action neuron activity prior to learning.</p>
         <p>For all learning rules, synaptic weights were algorithmically constrained to the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e466" xlink:type="simple"/></inline-formula>, to avoid negative or runaway weights. Learning rate values were manually adjusted (one value for actor and another one for critic synapses) to the value that yielded the best performance (as measured by the number of trials completed in 2.000s of simulated time). These values for the navigation and acrobot tasks are printed in <xref ref-type="table" rid="pcbi-1003024-t001">Table 1</xref>. For the cartpole task, somewhat faster learning was achieved by using a variable learning rate<disp-formula id="pcbi.1003024.e467"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e467" xlink:type="simple"/><label>(37)</label></disp-formula>for the critic, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e468" xlink:type="simple"/></inline-formula> is a running average of past reward rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e469" xlink:type="simple"/></inline-formula>, computed by filtering <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e470" xlink:type="simple"/></inline-formula> with an exponential window with time constant 50s. The actor learning rate was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e471" xlink:type="simple"/></inline-formula>.</p>
         <table-wrap id="pcbi-1003024-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.t001</object-id><label>Table 1</label>
            <caption>
               <title>Learning rates.</title>
            </caption><alternatives><graphic id="pcbi-1003024-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.t001" xlink:type="simple"/><table>
               <colgroup span="1">
                  <col align="left" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
               </colgroup>
               <thead>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Figure</td>
                     <td align="left" rowspan="1" colspan="1">Rule</td>
                     <td align="left" rowspan="1" colspan="1">Synapses</td>
                     <td align="left" rowspan="1" colspan="1">Value</td>
                     <td align="left" rowspan="1" colspan="1">Units</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C and D</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-LTP</td>
                     <td align="left" rowspan="1" colspan="1">critic</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e472" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e473" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4B, C and D</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-LTP</td>
                     <td align="left" rowspan="1" colspan="1">critic</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e474" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e475" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4B, C and D</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-LTP</td>
                     <td align="left" rowspan="1" colspan="1">actor</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e476" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e477" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4D</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-STDP</td>
                     <td align="left" rowspan="1" colspan="1">critic</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e478" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e479" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4D</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-STDP</td>
                     <td align="left" rowspan="1" colspan="1">actor</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e480" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e481" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g004">Figure 4D</xref></td>
                     <td align="left" rowspan="1" colspan="1">R-max</td>
                     <td align="left" rowspan="1" colspan="1">actor</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e482" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e483" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5B and C</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-LTP</td>
                     <td align="left" rowspan="1" colspan="1">critic</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e484" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e485" xlink:type="simple"/></inline-formula></td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"><xref ref-type="fig" rid="pcbi-1003024-g005">Figure 5B and C</xref></td>
                     <td align="left" rowspan="1" colspan="1">TD-LTP</td>
                     <td align="left" rowspan="1" colspan="1">actor</td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e486" xlink:type="simple"/></inline-formula></td>
                     <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e487" xlink:type="simple"/></inline-formula></td>
                  </tr>
               </tbody>
            </table></alternatives>
            <table-wrap-foot>
               <fn id="nt101">
                  <label/>
                  <p>Numerical values of the learning rates for the different learning rules used in simulations.</p>
               </fn>
            </table-wrap-foot>
         </table-wrap>
         <p>All simulations were ran using Euler's method with time-step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e488" xlink:type="simple"/></inline-formula>, except for the acrobot and cartpole dynamics, simulated using 4th order Runge-Kutta with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e489" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4h">
         <title>Derivation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e490" xlink:type="simple"/></inline-formula></title>
         <p>In this section we calculate the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e491" xlink:type="simple"/></inline-formula>, needed to derive <xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>. Using <xref ref-type="disp-formula" rid="pcbi.1003024.e109">Eqs 12</xref>–<xref ref-type="disp-formula" rid="pcbi.1003024.e115">13</xref>, and focusing on the synaptic weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e492" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e493" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e494" xlink:type="simple"/></inline-formula>, we find<disp-formula id="pcbi.1003024.e495"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e495" xlink:type="simple"/><label>(38)</label></disp-formula>where we used the fact that ρ<italic><sub>i</sub></italic>′<bold>(</bold><italic>t</italic><bold>)</bold> is independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e496" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e497" xlink:type="simple"/></inline-formula>. The derivative of the spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e498" xlink:type="simple"/></inline-formula> is ill-defined: in our stochastic neuron model, the spike train itself is independent of the synaptic weights. It is only the probability of the spike train actually being emitted by the neuron that depends on the weights. Therefore we replace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e499" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e500" xlink:type="simple"/></inline-formula>, the expected value of the spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e501" xlink:type="simple"/></inline-formula> conditional on the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e502" xlink:type="simple"/></inline-formula>. This yields<disp-formula id="pcbi.1003024.e503"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e503" xlink:type="simple"/><label>(39)</label></disp-formula>where the sum is over all possible spike trains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e504" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e505" xlink:type="simple"/></inline-formula> is the probability density of the spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e506" xlink:type="simple"/></inline-formula> being equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e507" xlink:type="simple"/></inline-formula>. The probability density of that spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e508" xlink:type="simple"/></inline-formula>, lasting from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e509" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e510" xlink:type="simple"/></inline-formula>, being produced by an SRM<sub>0</sub> neuron receiving inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e511" xlink:type="simple"/></inline-formula> is <xref ref-type="bibr" rid="pcbi.1003024-Pfister1">[38]</xref><disp-formula id="pcbi.1003024.e512"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e512" xlink:type="simple"/><label>(40)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e513" xlink:type="simple"/></inline-formula> is the membrane potential (<xref ref-type="disp-formula" rid="pcbi.1003024.e260">Eq. 18</xref>) and we have used <xref ref-type="disp-formula" rid="pcbi.1003024.e278">Eq. 20</xref>. Combining <xref ref-type="disp-formula" rid="pcbi.1003024.e503">Eqs 39</xref> and <xref ref-type="disp-formula" rid="pcbi.1003024.e512">40</xref> yields<disp-formula id="pcbi.1003024.e514"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e514" xlink:type="simple"/><label>(41)</label></disp-formula>The integration reflects the fact that the probability of a spike being emitted by the neuron at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e515" xlink:type="simple"/></inline-formula> is dependent not only on recent presynaptic spikes, but also on the time of the last spike of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e516" xlink:type="simple"/></inline-formula>, which in turn depends on its whole history.</p>
         <p>It is not clear that, in our context, this history dependence is a desirable outcome. Two devices already take the spike train history into account. Firstly, the definition of the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e517" xlink:type="simple"/></inline-formula> in the TD framework is conditional only on the current state, and not the long-term history. (This stems from the Markov decision process at the root of TD.) Secondly, the filtering of the spike train by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e518" xlink:type="simple"/></inline-formula> already ensures that the short-term history is remembered, making the integral over the history redundant.</p>
         <p>For these reasons, we choose to neglect the neuron's history, and to perform the following substitution<disp-formula id="pcbi.1003024.e519"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e519" xlink:type="simple"/><label>(42)</label></disp-formula>i.e., we take the last spike time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e520" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e521" xlink:type="simple"/></inline-formula> as given, and we ask how does the mean spiking at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e522" xlink:type="simple"/></inline-formula> vary as a function of the synaptic weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e523" xlink:type="simple"/></inline-formula>. Therefore we have<disp-formula id="pcbi.1003024.e524"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e524" xlink:type="simple"/><label>(43)</label></disp-formula>where we have used the definition of the neuron's firing rate, <xref ref-type="disp-formula" rid="pcbi.1003024.e278">Eq. 20</xref>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e525" xlink:type="simple"/></inline-formula> is the Dirac distribution. Using <xref ref-type="disp-formula" rid="pcbi.1003024.e260">Eqs 18</xref> yields<disp-formula id="pcbi.1003024.e526"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e526" xlink:type="simple"/><label>(44)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e527" xlink:type="simple"/></inline-formula> is the spike train of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e528" xlink:type="simple"/></inline-formula> culled to times posterior to the spikes of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e529" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e530" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e531" xlink:type="simple"/></inline-formula> denoting the Heaviside step function. Wrapping up the steps from <xref ref-type="disp-formula" rid="pcbi.1003024.e495">Eqs 38</xref> and <xref ref-type="disp-formula" rid="pcbi.1003024.e519">42</xref>–<xref ref-type="disp-formula" rid="pcbi.1003024.e526">44</xref>, we finally have<disp-formula id="pcbi.1003024.e532"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e532" xlink:type="simple"/><label>(45)</label></disp-formula></p>
      </sec>
      <sec id="s4i">
         <title>Derivation of the Squared TD Gradient Learning Rule</title>
         <p>In the Results section we derive a learning rule starting from <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref>. We also suggest that starting from a gradient descent on the squared TD error (<xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq.17</xref>) should yield a valid learning rule. Here we derive such a learning rule. Combining <xref ref-type="disp-formula" rid="pcbi.1003024.e059">Eq. 10</xref>, the definition of the TD error (<xref ref-type="disp-formula" rid="pcbi.1003024.e039">Eq. 5</xref>) and the result of the previous section (<xref ref-type="disp-formula" rid="pcbi.1003024.e524">Eq. 45</xref>), we find<disp-formula id="pcbi.1003024.e533"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e533" xlink:type="simple"/><label>(46)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e534" xlink:type="simple"/></inline-formula> is the spike train of presynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e535" xlink:type="simple"/></inline-formula>. This learning rule has the same general form as the TD-LTP rule (<xref ref-type="disp-formula" rid="pcbi.1003024.e133">Eq. 17</xref>): a “Hebbian” pre-before-post coincidence term is first temporally filtered, and then multiplied by the TD error with a term (<xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8A</xref>). The difference lies in the extra <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e536" xlink:type="simple"/></inline-formula> in the filter, which comes from a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e537" xlink:type="simple"/></inline-formula> term. As <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8</xref> suggests, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e538" xlink:type="simple"/></inline-formula> term largely dominates over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e539" xlink:type="simple"/></inline-formula>. This is the consequence of our choice of a long discount time constant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e540" xlink:type="simple"/></inline-formula>) with a short (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e541" xlink:type="simple"/></inline-formula>) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e542" xlink:type="simple"/></inline-formula> kernel.</p>
         <fig id="pcbi-1003024-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003024.g008</object-id><label>Figure 8</label>
            <caption>
               <title>Alternative learning rule and nuisance term.</title>
               <p>A: Schematic comparison of the squared TD gradient learning rule of <xref ref-type="disp-formula" rid="pcbi.1003024.e533">Eq. 46</xref> and TD-LTP, similar to <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2A</xref>. B: Linear track task using the squared TD gradient rule. Same conventions as in <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref>. C: linear track task using the TD-LTP rule (reprint of <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2C</xref> for comparison). D: Integrands of the disturbance term for Poisson spike train statistics. Top: squared TD gradient rule. Bottom: TD-LTP rule. In each plot the numerical value under the curve is given. This corresponds to the contribution of each presynaptic spike to the nuisance term. E: Disturbance term dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e543" xlink:type="simple"/></inline-formula> for the squared TD gradient rule. The mean weight change under initial conditions on an unrewarded linear track task with frozen weights, using the squared TD gradient learning rule, is plotted versus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e544" xlink:type="simple"/></inline-formula>, the number of neurons composing the critic. Each cross corresponds to the mean over a 200s simulation, the plot shows <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e545" xlink:type="simple"/></inline-formula> crosses for each condition. The line shows a fit of the data with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e546" xlink:type="simple"/></inline-formula>, the dependence form suggested by <xref ref-type="disp-formula" rid="pcbi.1003024.e574">Eq. 50</xref>. F: Same as E, for critic neurons using the TD-LTP learning rule. G, H: Same experiment as E and F, but using a rate neuron model with Gaussian noise of mean 0 and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e547" xlink:type="simple"/></inline-formula>. The line shows a fit with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e548" xlink:type="simple"/></inline-formula>, the dependence form suggested by <xref ref-type="disp-formula" rid="pcbi.1003024.e574">Eq. 50</xref>.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003024.g008" position="float" xlink:type="simple"/></fig>
      </sec>
      <sec id="s4j">
         <title>Noise Correlation Problem</title>
         <p>Here we show, both analytically and in simulations, that the squared TD gradient learning rule of <xref ref-type="disp-formula" rid="pcbi.1003024.e526">Eq. 46</xref> suffers from a noise bias problem. This arises from the noise in the individual neurons estimating the value function, and is serious enough to prevent learning. To see this, we start by decomposing the spike train <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e549" xlink:type="simple"/></inline-formula> of a neuron into a mean and a noise term, i.e.<disp-formula id="pcbi.1003024.e550"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e550" xlink:type="simple"/><label>(47)</label></disp-formula>where we have defined <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e551" xlink:type="simple"/></inline-formula>, with the brackets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e552" xlink:type="simple"/></inline-formula> denoting expectation, i.e., averaging over all possible outcomes of critic neurons activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e553" xlink:type="simple"/></inline-formula> conditioned on the presynaptic neural activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e554" xlink:type="simple"/></inline-formula>. With this definition, we can rewrite <xref ref-type="disp-formula" rid="pcbi.1003024.e533">Eq. 46</xref> as<disp-formula id="pcbi.1003024.e555"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e555" xlink:type="simple"/><label>(48)</label></disp-formula>where the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e556" xlink:type="simple"/></inline-formula> error has been spelled out explicitly (<xref ref-type="disp-formula" rid="pcbi.1003024.e039">Eqs 5</xref>, <xref ref-type="disp-formula" rid="pcbi.1003024.e115">13</xref> and <xref ref-type="disp-formula" rid="pcbi.1003024.e109">12</xref>). <xref ref-type="disp-formula" rid="pcbi.1003024.e555">Eq. 48</xref> suggests that quadratic terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e557" xlink:type="simple"/></inline-formula> in the noise might play a role in the learning rule. Indeed, distributivity and use of the facts <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e558" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e559" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e560" xlink:type="simple"/></inline-formula> gives<disp-formula id="pcbi.1003024.e561"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e561" xlink:type="simple"/><label>(49)</label></disp-formula>Here we have defined the autocorrelation of the noise terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e562" xlink:type="simple"/></inline-formula>, as well as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e563" xlink:type="simple"/></inline-formula> for brevity.</p>
         <p>The first term in the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1003024.e561">Eq. 49</xref> is analog to <xref ref-type="disp-formula" rid="pcbi.1003024.e533">Eq. 46</xref>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e564" xlink:type="simple"/></inline-formula> replacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e565" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e566" xlink:type="simple"/></inline-formula> replacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e567" xlink:type="simple"/></inline-formula>. In effect this is a “mean” version of the learning rule: this is what one would get by replacing the stochastic spiking neurons in the model by analog, noiseless units with a similar exponential activation function.</p>
         <p>The second term arises from the correlation of neuron noise in the TD term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e568" xlink:type="simple"/></inline-formula> and the Hebbian component of the learning rule. This term is a function of the autocorrelation function of the postsynaptic neuron. This carries only indirect information about the postsynaptic firing (and thus the current value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e569" xlink:type="simple"/></inline-formula>) and no information about the reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e570" xlink:type="simple"/></inline-formula>. For this reason, we conjecture that this second element is a potentially problematic term, which we refer to as the “nuisance” term. This hypothesis is confirmed by linear track simulations using the learning rule <xref ref-type="disp-formula" rid="pcbi.1003024.e526">Eq. 46</xref>, shown in <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8B</xref>. These indicate that the learning rule is unable to learn the task, contrary to TD-LTP (<xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8C</xref>, same as <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2B</xref>). More precisely, the value functions learned by the squared TD gradient rule suffer from a negative “drag” term.</p>
         <p>We next try to identify this negative “drag” with the nuisance term. Although there's no closed form expression for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e571" xlink:type="simple"/></inline-formula>, one can use the statistics of a Poisson process as a first order approximation. In that case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e572" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e573" xlink:type="simple"/></inline-formula> is the Dirac distribution) and <xref ref-type="disp-formula" rid="pcbi.1003024.e561">Eq. 49</xref> becomes<disp-formula id="pcbi.1003024.e574"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e574" xlink:type="simple"/><label>(50)</label></disp-formula></p>
         <p>The last term on the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1003024.e555">Eq. 50</xref> implies that, on average, each presynaptic spike in neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e575" xlink:type="simple"/></inline-formula> causes the synaptic weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e576" xlink:type="simple"/></inline-formula> to depress by a fixed amount. This quantity increases with the variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e577" xlink:type="simple"/></inline-formula> of the noise process, in this case the inhomogeneous Poisson process that drives the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e578" xlink:type="simple"/></inline-formula> neuron, and inversely to the number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e579" xlink:type="simple"/></inline-formula> of critic neurons. The time course of the presynaptic spike effect is ruled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e580" xlink:type="simple"/></inline-formula>, which is plotted in the top panel of <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8D</xref>. The aggregate nuisance effect on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e581" xlink:type="simple"/></inline-formula> of a single presynaptic spike is proportional to the integral of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e582" xlink:type="simple"/></inline-formula> over time.</p>
         <p>In <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8E</xref>, we explore the dependence of the nuisance term on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e583" xlink:type="simple"/></inline-formula> in numerical simulations. <xref ref-type="disp-formula" rid="pcbi.1003024.e574">Eq. 50</xref> suggests that the mean learning rule term should obey a relationship of the form<disp-formula id="pcbi.1003024.e584"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e584" xlink:type="simple"/><label>(51)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e585" xlink:type="simple"/></inline-formula> is the result of the “useful” part of the learning rule, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e586" xlink:type="simple"/></inline-formula> contains all the other dependencies of the nuisance term. We tested the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e587" xlink:type="simple"/></inline-formula> dependency by simulating agents with variable numbers of critic neurons in a linear track scenario. The setup was similar to that of <xref ref-type="fig" rid="pcbi-1003024-g002">Figure 2</xref>, except that the weights were frozen, i.e., we collected the value of the learning rule at each time step, but we didn't actually update the weights. The mean learning rule outcome for 200s of simulations are plotted in <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8E</xref> as crosses, against the number of critic neurons. The black line shows a fit of the data by <xref ref-type="disp-formula" rid="pcbi.1003024.e584">Eq. 51</xref>: both are in good agreement.</p>
         <p>From <xref ref-type="disp-formula" rid="pcbi.1003024.e574">Eq. 50</xref>, we see that the nuisance term also depends on the variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e588" xlink:type="simple"/></inline-formula> of the noise process. It is difficult to control the variance of our spiking neurons' noise process without also altering their firing rate and thus the result of the learning rule. To circumvent this difficulty, we turned to a rate model, where the single critic neuron's firing rate was<disp-formula id="pcbi.1003024.e589"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e589" xlink:type="simple"/><label>(52)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e590" xlink:type="simple"/></inline-formula> is a constant, the place cells rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e591" xlink:type="simple"/></inline-formula> are defined in <xref ref-type="disp-formula" rid="pcbi.1003024.e304">Eq. 22</xref> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e592" xlink:type="simple"/></inline-formula> is a white noise process. Similar to the steps above, a gradient descent on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e593" xlink:type="simple"/></inline-formula> yields a learning rule of the form<disp-formula id="pcbi.1003024.e594"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e594" xlink:type="simple"/><label>(53)</label></disp-formula>Due to the noise component in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e595" xlink:type="simple"/></inline-formula>, the learning rule suffers from the same noise-driven nuisance as the spiking version. This depends on the noise's variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e596" xlink:type="simple"/></inline-formula>, so that the mean weight change obeys<disp-formula id="pcbi.1003024.e597"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e597" xlink:type="simple"/><label>(54)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e598" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8F</xref>, we use the rate-based model and rule in the same “frozen weights” linear track scenario as in <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8E</xref>. This time we looked at how the mean weight change varied as a function of the noise variance. Again, the data is well matched by a fit with <xref ref-type="disp-formula" rid="pcbi.1003024.e597">Eq. 54</xref> (black line), suggesting that the nuisance term behaves as expected.</p>
      </sec>
      <sec id="s4k">
         <title>Noise Correlation in the TD-LTP Rule</title>
         <p>In the preceding section we found that a noise correlation nuisance in the squared TD gradient learning rule causes it to be ineffective. However, the same actually should apply to the TD-LTP rule. Indeed, if we repeat the steps above leading to <xref ref-type="disp-formula" rid="pcbi.1003024.e574">Eq. 50</xref> for the learning rule TD-LTP, we get<disp-formula id="pcbi.1003024.e599"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e599" xlink:type="simple"/><label>(55)</label></disp-formula></p>
         <p>The only difference is the time course of the nuisance term, which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e600" xlink:type="simple"/></inline-formula> for the squared TD gradient rule versus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e601" xlink:type="simple"/></inline-formula> for TD-LTP. <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8D</xref> shows a plot of both expressions: because the TD-LTP expression is much smaller, these are plotted on different axes. As noted before, the integral of the nuisance is proportional to these time courses (shown on <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8D</xref>). The term for TD-LTP is more than three orders of magnitude smaller than that of the square TD gradient rule.</p>
         <p>In <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8G and H</xref>, we repeat the experiments of <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8E and F</xref>, respectively. These show that the TD-LTP learning rule also suffers from a nuisance term, but that it is orders of magnitude smaller than for the squared TD gradient rule. As shown by <xref ref-type="fig" rid="pcbi-1003024-g008">Figure 8C</xref> and in the Results section, this nuisance is not sufficient to prevent TD-LTP from properly learning the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e602" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4l">
         <title>The Trouble with Continuous Q-Learning</title>
         <p>In the Results section, we claim that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e603" xlink:type="simple"/></inline-formula>-values based algorithms, such as Sarsa <xref ref-type="bibr" rid="pcbi.1003024-Sutton3">[24]</xref> and Q-Learning <xref ref-type="bibr" rid="pcbi.1003024-Watkins1">[23]</xref> are difficult to extend to continuous time in a neural network setting. Here we develop this argument.</p>
         <p>In the discrete Sarsa algorithm, the agent maintains an estimation of the state-action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e604" xlink:type="simple"/></inline-formula>-values. For an agent following the policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e605" xlink:type="simple"/></inline-formula>, starting at time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e606" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e607" xlink:type="simple"/></inline-formula> and executing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e608" xlink:type="simple"/></inline-formula>, this is defined as the discounted sum over future rewards <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e609" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003024.e610"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e610" xlink:type="simple"/><label>(56)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e611" xlink:type="simple"/></inline-formula> is a discount factor, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e612" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e613" xlink:type="simple"/></inline-formula> represent the future states and actions visited by the agent under policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e614" xlink:type="simple"/></inline-formula>. To learn <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e615" xlink:type="simple"/></inline-formula>-values approximations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e616" xlink:type="simple"/></inline-formula> to the real <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e617" xlink:type="simple"/></inline-formula>, Sarsa suggests the following update rule at time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e618" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003024.e619"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e619" xlink:type="simple"/><label>(57)</label></disp-formula>where the TD error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e620" xlink:type="simple"/></inline-formula> is defined as<disp-formula id="pcbi.1003024.e621"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e621" xlink:type="simple"/><label>(58)</label></disp-formula></p>
         <p>If one were to propose a continuous time version of Sarsa, one would start by redefining the state-action value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e622" xlink:type="simple"/></inline-formula> to continuous time t, similar to the value function of <xref ref-type="disp-formula" rid="pcbi.1003024.e021">Eq. 3</xref><disp-formula id="pcbi.1003024.e623"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e623" xlink:type="simple"/><label>(59)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e624" xlink:type="simple"/></inline-formula> now plays the role of the discount factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e625" xlink:type="simple"/></inline-formula>. As we did for <xref ref-type="disp-formula" rid="pcbi.1003024.e039">Eq. 5</xref>, we define the TD error on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e626" xlink:type="simple"/></inline-formula>-value by taking the derivative of <xref ref-type="disp-formula" rid="pcbi.1003024.e623">Eq. 59</xref><disp-formula id="pcbi.1003024.e627"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e627" xlink:type="simple"/><label>(60)</label></disp-formula>To calculate the TD error, one therefore needs to combine the three terms in <xref ref-type="disp-formula" rid="pcbi.1003024.e627">Eq. 60</xref>. We assume the reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e628" xlink:type="simple"/></inline-formula> is given by the environment. Typically <xref ref-type="bibr" rid="pcbi.1003024-Vasilaki1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003024-Arleo1">[20]</xref>, neural networks implementations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e629" xlink:type="simple"/></inline-formula>-values based reinforcement learning consist of a number “action cells” neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e630" xlink:type="simple"/></inline-formula>, each tuned to a specific action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e631" xlink:type="simple"/></inline-formula> and rate-coding for the state-action values<disp-formula id="pcbi.1003024.e632"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e632" xlink:type="simple"/><label>(61)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e633" xlink:type="simple"/></inline-formula> is neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e634" xlink:type="simple"/></inline-formula>'s firing rate. In that case, reading out the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e635" xlink:type="simple"/></inline-formula> is thus simply a matter of reading the activity of the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e636" xlink:type="simple"/></inline-formula> coding for the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e637" xlink:type="simple"/></inline-formula> selected at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e638" xlink:type="simple"/></inline-formula>.</p>
         <p>Reading out the temporal derivative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e639" xlink:type="simple"/></inline-formula> is harder to do in that context, because the currently chosen action is evolving all the time. For small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e640" xlink:type="simple"/></inline-formula>, we can approximate<disp-formula id="pcbi.1003024.e641"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003024.e641" xlink:type="simple"/><label>(62)</label></disp-formula>where we also used <xref ref-type="disp-formula" rid="pcbi.1003024.e632">Eq. 61</xref> and identified the action neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e642" xlink:type="simple"/></inline-formula> tuned to action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e643" xlink:type="simple"/></inline-formula>.</p>
         <p>The difficulty that arises in evaluating <xref ref-type="disp-formula" rid="pcbi.1003024.e641">Eq. 62</xref> is the following. It requires a system that can keep track of the two recent actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e644" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e645" xlink:type="simple"/></inline-formula>, identify the relevant neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e646" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e647" xlink:type="simple"/></inline-formula>, and calculate a difference of their firing rates. This is hard to envision in a biologically plausible setting. The use of an actor-critic architecture solves this problem by having a single population coding for the state-based value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003024.e648" xlink:type="simple"/></inline-formula> at all times.</p>
      </sec>
   </sec>
</body>
<back>
   <ref-list>
      <title>References</title>
      <ref id="pcbi.1003024-Sutton1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Sutton R, Barto A (1998) Reinforcement learning. Cambridge: MIT Press.</mixed-citation></ref>
      <ref id="pcbi.1003024-Sutton2"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name> (<year>1988</year>) <article-title>Learning to predict by the methods of temporal differences</article-title>. <source>Machine Learning</source> <volume>3</volume>: <fpage>9</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Williams1"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>R</given-names></name> (<year>1992</year>) <article-title>Simple statistical gradient-following methods for connectionist reinforcement learning</article-title>. <source>Machine Learning</source> <volume>8</volume>: <fpage>229</fpage>–<lpage>256</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Xie1"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xie</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Learning in neural networks by reinforcement of irregular spiking</article-title>. <source>Physical Review E</source> <volume>69</volume>: <fpage>41909</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Baxter1"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baxter</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bartlett</surname><given-names>P</given-names></name> (<year>2001</year>) <article-title>Infinite-horizon policy-gradient estimation</article-title>. <source>Journal of Artificial Intelligence Research</source> <volume>15</volume>: <fpage>319</fpage>–<lpage>350</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Florian1"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Florian</surname><given-names>RV</given-names></name> (<year>2007</year>) <article-title>Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity</article-title>. <source>Neural Computation</source> <volume>19</volume>: <fpage>1468</fpage>–<lpage>1502</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Schultz1"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name> (<year>1997</year>) <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source> <volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Wickens1"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Begg</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Arbuthnott</surname><given-names>GW</given-names></name> (<year>1996</year>) <article-title>Dopamine reverses the depression of rat corticostriatal synapses which normally follows high-frequency stimulation of cortex in vitro</article-title>. <source>Neuroscience</source> <volume>70</volume>: <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Reynolds1"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname><given-names>JNJ</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name> (<year>2000</year>) <article-title>Substantia nigra dopamine regulates synaptic plasticity and membrane potential uctuations in the rat neostriatum, in vivo</article-title>. <source>Neuroscience</source> <volume>99</volume>: <fpage>199</fpage>–<lpage>203</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Reynolds2"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname><given-names>JNJ</given-names></name>, <name name-style="western"><surname>Hyland</surname><given-names>BI</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name> (<year>2001</year>) <article-title>A cellular mechanism of reward-related learning</article-title>. <source>Nature</source> <volume>413</volume>: <fpage>67</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Reynolds3"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname><given-names>JNJ</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name> (<year>2002</year>) <article-title>Dopamine-dependent plasticity of corticostriatal synapses</article-title>. <source>Neural Netw</source> <volume>15</volume>: <fpage>507</fpage>–<lpage>521</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Pawlak1"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pawlak</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Kerr</surname><given-names>JND</given-names></name> (<year>2008</year>) <article-title>Dopamine receptor activation is required for corticostriatal spiketiming-dependent plasticity</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>2435</fpage>–<lpage>2446</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Zhang1"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Lau</surname><given-names>PM</given-names></name>, <name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name> (<year>2009</year>) <article-title>Gain in sensitivity and loss in temporal contrast of STDP by dopaminergic modulation at hippocampal synapses</article-title>. <source>PNAS</source> <volume>106</volume>: <fpage>13028</fpage>–<lpage>13033</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Pawlak2"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pawlak</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kirkwood</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kerr</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Timing is not everything: neuromodulation opens the STDP gate</article-title>. <source>Frontiers in Synaptic Neuroscience</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Potjans1"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potjans</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>A spiking neural network model of an actor-critic learning agent</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>301</fpage>–<lpage>339</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Vasilaki1"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Spike-based reinforcement learning in continuous state and action space: When policy gradient methods fail</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000586</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Gold1"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name> (<year>2007</year>) <article-title>The neural basis of decision making</article-title>. <source>Annual Review of Neuroscience</source> <volume>30</volume>: <fpage>535</fpage>–<lpage>574</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Barto1"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barto</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sutton</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>C</given-names></name> (<year>1983</year>) <article-title>Neuronlike adaptive elements that can solve difficult learning and control problems</article-title>. <source>IEEE transactions on systems, man, and cybernetics</source> <volume>13</volume>: <fpage>835</fpage>–<lpage>846</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Doya1"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2000</year>) <article-title>Reinforcement learning in continuous time and space</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>219</fpage>–<lpage>245</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Arleo1"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arleo</surname><given-names>A</given-names></name> (<year>2000</year>) <collab xlink:type="simple">GerstnerW</collab> (<year>2000</year>) <article-title>Spatial cognition and neuro-mimetic navigation: a model of hippocampal place cell activity</article-title>. <source>Biological Cybernetics</source> <volume>83</volume>: <fpage>287</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Foster1"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foster</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2000</year>) <article-title>Models of hippocampally dependent navigation using the temporal difference learning rule</article-title>. <source>Hippocampus</source> <volume>10</volume>: <fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-OKeefe1"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>L</given-names></name> (<year>1971</year>) <article-title>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain Res</source> <volume>34</volume>: <fpage>171</fpage>–<lpage>175</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Watkins1"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watkins</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>1992</year>) <article-title>Q-Learning</article-title>. <source>Machine Learning</source> <volume>8</volume>: <fpage>279</fpage>–<lpage>292</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Sutton3"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Sutton RS (1996) Generalization in reinforcement learning: Successful examples using sparse coarse coding. In: Advances in Neural Information Processing Systems 8. MIT Press, pp. 1038–1044.</mixed-citation></ref>
      <ref id="pcbi.1003024-Baird1"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Baird LC (1995) Residual algorithms: Reinforcement learning with function approximation. In: Prieditis A, Russell S, editors, Proceedings of the Twelfth International Conference on Machine Learning. San Francisco, CA.: Morgan Kaufmann., pp. 30–37.</mixed-citation></ref>
      <ref id="pcbi.1003024-Harmon1"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harmon</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Baird</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Klopf</surname><given-names>AH</given-names></name> (<year>1995</year>) <article-title>Reinforcement learning applied to a differential game</article-title>. <source>Adaptive Behavior</source> <volume>4</volume>: <fpage>3</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Sutton4"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Sutton RS (1984) Temporal credit assignment in reinforcement learning. Ph.D. thesis, UMass Amherst.</mixed-citation></ref>
      <ref id="pcbi.1003024-Dayan1"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>1992</year>) <article-title>The convergence of TD(λ) for general λ</article-title>. <source>Machine learning</source> <volume>8</volume>: <fpage>341</fpage>–<lpage>362</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Tsitsiklis1"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsitsiklis</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Van Roy</surname><given-names>B</given-names></name> (<year>1997</year>) <article-title>An analysis of temporal-difference learning with function approximation</article-title>. <source>Automatic Control, IEEE Transactions on</source> <volume>42</volume>: <fpage>674</fpage>–<lpage>690</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Izhikevich1"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>E</given-names></name> (<year>2007</year>) <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling</article-title>. <source>Cerebral Cortex</source> <volume>17</volume>: <fpage>2443</fpage>–<lpage>2452</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Legenstein1"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>A learning theory for reward-modulated spike-timingdependent plasticity with application to biofeedback</article-title>. <source>PLOS Comput Biol</source> <volume>4</volume>: <fpage>e1000180</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Frmaux1"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Functional requirements for reward-modulated spiketiming-dependent plasticity</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>13326</fpage>–<lpage>13337</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Gerstner1"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name>, <name name-style="western"><surname>van Hemmen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wagner</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>A neuronal learning rule for submillisecond temporal coding</article-title>. <source>Nature</source> <volume>383</volume>: <fpage>76</fpage>–<lpage>78</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Markram1"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lübke</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name> (<year>1997</year>) <article-title>Regulation of synaptic efficacy by coincidence of postysnaptic AP and EPSP</article-title>. <source>Science</source> <volume>275</volume>: <fpage>213</fpage>–<lpage>215</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Bi1"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>J Neurosci</source> <volume>18</volume>: <fpage>10464</fpage>–<lpage>10472</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Song1"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name> (<year>2000</year>) <article-title>Competitive Hebbian learning through spike-time-dependent synaptic plasticity</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>919</fpage>–<lpage>926</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Georgopoulos1"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Georgopoulos</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kettner</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>A</given-names></name> (<year>1988</year>) <article-title>Primate motor cortex and free arm movements to visual targets in three- dimensional space. II. Coding of the direction of movement by a neuronal population</article-title>. <source>J Neurosci</source> <volume>8</volume>: <fpage>2928</fpage>–<lpage>2937</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Pfister1"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Barber</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</article-title>. <source>Neural Comp</source> <volume>18</volume>: <fpage>1318</fpage>–<lpage>1348</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Legenstein2"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Chase</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>AB</given-names></name> (<year>2010</year>) <collab xlink:type="simple">MaassW</collab> (<year>2010</year>) <article-title>A reward-modulated hebbian learning rule can explain experimentally observed network reorganization in a brain control task</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>8400</fpage>–<lpage>8410</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Boone1"><label>40</label><mixed-citation publication-type="other" xlink:type="simple">Boone G (1997) Minimum-time control of the acrobot. In: Robotics and Automation, Proceedings, 1997 IEEE International Conference on. Volume 4, pp. 3281–3287. doi: 10.1109/ROBOT.1997.606789.</mixed-citation></ref>
      <ref id="pcbi.1003024-Michie1"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Michie D, Chambers R (1968) Boxes: An experiment in adaptive control. In: Dale E, Michie D, editors, Machine Intelligence 2. Edinburgh: Oliver and Boyd. pp. 137–152.</mixed-citation></ref>
      <ref id="pcbi.1003024-Houk1"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Houk J, Adams J, Barto A (1995) A model of how the basal ganglia generate and use neural signals that predict reinforcement. In: Houk JC, Davis JL, Beiser DG, editors, Models on Information Processing in the Basal Ganglia, Cambridge: MIT Press. pp. 249–270.</mixed-citation></ref>
      <ref id="pcbi.1003024-Joel1"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Ruppin</surname><given-names>E</given-names></name> (<year>2002</year>) <article-title>Actor–critic models of the basal ganglia: new anatomical and computational perspectives</article-title>. <source>Neural Networks</source> <volume>15</volume>: <fpage>535</fpage>–<lpage>547</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-vanderMeer1"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Meer</surname><given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name> (<year>2011</year>) <article-title>Theta phase precession in rat ventral striatum links place and reward information</article-title>. <source>The Journal of Neuroscience</source> <volume>31</volume>: <fpage>2843</fpage>–<lpage>2854</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Hollerman1"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hollerman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name> (<year>1998</year>) <article-title>Dopamine neurons report an error in the temporal prediction of reward during learning</article-title>. <source>Nature Neuroscience</source> <volume>1</volume>: <fpage>304</fpage>–<lpage>309</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Potjans2"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potjans</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>An imperfect dopaminergic error signal can drive temporal-difference learning</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1001133</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Robbins1"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robbins</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Differential regulation of fronto-executive function by the monoamines and acetylcholine</article-title>. <source>Cerebral Cortex</source> <volume>17</volume>: <fpage>i151</fpage>–<lpage>i160</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Nakamura1"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakamura</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name> (<year>2008</year>) <article-title>Reward-dependent modulation of neuronal activity in the primate dorsal raphe nucleus</article-title>. <source>The Journal of Neuroscience</source> <volume>28</volume>: <fpage>5331</fpage>–<lpage>5343</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Miyazaki1"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyazaki</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Miyazaki</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>Activation of dorsal raphe serotonin neurons underlies waiting for delayed rewards</article-title>. <source>The Journal of Neuroscience</source> <volume>31</volume>: <fpage>469</fpage>–<lpage>479</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Cohen1"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname><given-names>JY</given-names></name>, <name name-style="western"><surname>Haesler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vong</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Lowell</surname><given-names>BB</given-names></name>, <name name-style="western"><surname>Uchida</surname><given-names>N</given-names></name> (<year>2012</year>) <article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title>. <source>Nature</source> <volume>482</volume>: <fpage>85</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Stroesslin1"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stroesslin</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Sheynikhovich</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Chavarriaga</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2005</year>) <article-title>Robust self-localisation and navigation based on hippocampal place cells</article-title>. <source>Neural Networks</source> <volume>18</volume>: <fpage>1125</fpage>–<lpage>1140</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Sheynikhovich1"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sheynikhovich</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Chavarriaga</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Stroesslin</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Arleo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Is there a geometric module for spatial orientation? Insights from a rodent navigation model</article-title>. <source>Psychological Review</source> <volume>116</volume>: <fpage>540</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Loewenstein1"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name> (<year>2008</year>) <article-title>Robustness of learning that is based on covariance-driven synaptic plasticity</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000007</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Seol1"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seol</surname><given-names>GH</given-names></name>, <name name-style="western"><surname>Ziburkus</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Song</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>IT</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Neuromodulators control the polarity of spike-timing-dependent synaptic plasticity</article-title>. <source>Neuron</source> <volume>55</volume>: <fpage>919</fpage>–<lpage>929</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Klopf1"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">Klopf A (1982) The hedonistic neuron: a theory of memory, learning, and intelligence. Washington: Hemisphere.</mixed-citation></ref>
      <ref id="pcbi.1003024-Sutton5"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Beis</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Trainor</surname><given-names>LEH</given-names></name> (<year>1988</year>) <article-title>Hierarchical model of memory and memory loss</article-title>. <source>J Phys A</source> <volume>21</volume>: <fpage>4443</fpage>–<lpage>4454</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Frey1"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frey</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>R</given-names></name> (<year>1997</year>) <article-title>Synaptic tagging and long-term potentiation</article-title>. <source>Nature</source> <volume>385</volume>: <fpage>533</fpage>–<lpage>536</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Clopath1"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ziegler</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>Tag-trigger-consolidation: A model of early and late long-term-potentiation and depression</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000248</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Wickens2"><label>59</label><mixed-citation publication-type="other" xlink:type="simple">Wickens JR, Kotter R (1995) Cellular models of reinforcement. In: Houk J, Davis J, Beiser DG, editors, Models of information processing in basal ganglia, Cambridge: MIT-Press. pp. 187–214.</mixed-citation></ref>
      <ref id="pcbi.1003024-Gerstner2"><label>60</label><mixed-citation publication-type="other" xlink:type="simple">Gerstner W, Kistler WK (2002) Spiking Neuron Models. Cambridge UK: Cambridge University Press.</mixed-citation></ref>
      <ref id="pcbi.1003024-Jolivet1"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jolivet</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rauch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lüscher</surname><given-names>HR</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models</article-title>. <source>J Computational Neuroscience</source> <volume>21</volume>: <fpage>35</fpage>–<lpage>49</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003024-Florian2"><label>62</label><mixed-citation publication-type="other" xlink:type="simple">Florian RV (2007) Correct equations for the dynamics of the cart-pole system. Technical report, Center for Cognitive and Neural Studies (Coneural), Romania.</mixed-citation></ref>
   </ref-list>
</back>
</article>