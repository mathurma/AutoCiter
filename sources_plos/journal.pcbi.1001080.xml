<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2410R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1001080</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Spike-Based Population Coding and Working Memory</article-title><alt-title alt-title-type="running-head">Spike-Based Population Coding and Working Memory</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Boerlin</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Denève</surname><given-names>Sophie</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Group for Neural Theory, Département d'Études Cognitives, École Normale Supérieure, Paris, France</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Laboratoire de Neurosciences Cognitives, Inserm U960, Paris, France</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">sophie.deneve@ens.fr</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: MB SD. Performed the experiments: MB. Analyzed the data: MB. Wrote the paper: MB SD.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>2</month><year>2011</year></pub-date><pub-date pub-type="epub"><day>17</day><month>2</month><year>2011</year></pub-date><volume>7</volume><issue>2</issue><elocation-id>e1001080</elocation-id><history>
<date date-type="received"><day>20</day><month>6</month><year>2010</year></date>
<date date-type="accepted"><day>12</day><month>1</month><year>2011</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Boerlin, Denève</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Most of our daily actions are subject to uncertainty. Behavioral studies have confirmed that humans handle this uncertainty in a statistically optimal manner. A key question then is what neural mechanisms underlie this optimality, i.e. how can neurons represent and compute with probability distributions. Previous approaches have proposed that probabilities are encoded in the firing rates of neural populations. However, such rate codes appear poorly suited to understand perception in a constantly changing environment. In particular, it is unclear how probabilistic computations could be implemented by biologically plausible spiking neurons. Here, we propose a network of spiking neurons that can optimally combine uncertain information from different sensory modalities and keep this information available for a long time. This implies that neural memories not only represent the most likely value of a stimulus but rather a whole probability distribution over it. Furthermore, our model suggests that each spike conveys new, essential information. Consequently, the observed variability of neural responses cannot simply be understood as noise but rather as a necessary consequence of optimal sensory integration. Our results therefore question strongly held beliefs about the nature of neural “signal” and “noise”.</p>
</abstract><funding-group><funding-statement>This work was supported by the IST European consortium project BACS FP6-IST-027140, the Marie Curie Team of Excellence Grant BIND MECT-CT-20095-024831 and the Fondation pour la Recherche Médicale (FRM). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="18"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Our senses furnish us with information about the external world that is ambiguous and corrupted by noise. Taking this uncertainty into account is crucial for a successful interaction with our environment. Psychophysical studies have shown that animals and humans can behave as optimal Bayesian observers, i.e. they integrate noisy sensory cues, their own predictions and prior beliefs in order to maximize the expected outcome of their actions <xref ref-type="bibr" rid="pcbi.1001080-Ernst1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Kording1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Knill1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Todorov1">[4]</xref>.</p>
<p>Several theoretical investigations have explored the neural mechanisms that could underly such probabilistic computations <xref ref-type="bibr" rid="pcbi.1001080-Zemel1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Beck1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Deneve1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Rao1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Eliasmith1">[10]</xref>. In cortical areas, sensory and motor variables are encoded by the joint activity of populations of spiking neurons <xref ref-type="bibr" rid="pcbi.1001080-Barlow1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Georgopoulos1">[12]</xref> whose activity is highly variable and weakly correlated <xref ref-type="bibr" rid="pcbi.1001080-Shadlen1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Tolhurst1">[14]</xref>. The timing of individual spikes is unreliable while spike counts are approximately Poisson distributed <xref ref-type="bibr" rid="pcbi.1001080-Tolhurst1">[14]</xref>. These characteristics have inspired rate-based models that encode probability distributions in their average firing rates and spike count covariances. Previous studies have examined analytically and empirically how this information can be encoded in a population code <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Zemel1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Sahani1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Eliasmith1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Rao1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Jazayeri1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Beck2">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Natarajan1">[18]</xref>, how it can be decoded <xref ref-type="bibr" rid="pcbi.1001080-Seung1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Pouget1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Zemel1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Hinton1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Eliasmith1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Wu1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Huys1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Gerwinn1">[24]</xref> and how population codes can be combined optimally <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Deneve2">[25]</xref>. In particular, optimal cue combination reduces to a simple linear combination of neural activities for a broad family of neural variability, including Poisson or Gaussian noise <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>.</p>
<p>However, most of these studies neglect a crucial dimension of perception: time. Most sensory stimuli vary dynamically in a natural environment, which requires sensory representations to be constructed, integrated and combined on-line <xref ref-type="bibr" rid="pcbi.1001080-Huys1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Hinton1">[21]</xref>. Perceptual inference thus cannot be based on rates or spike counts measured during a “fixed” temporal window, as used in most previous population coding frameworks. At the same time, reliable decisions typically require an integration of sensory evidence over hundreds of milliseconds <xref ref-type="bibr" rid="pcbi.1001080-Rinberg1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Shadlen2">[27]</xref>, which largely exceeds the integrative time constant of single neurons. It is unclear how such leaky devices could compute sums of spike counts on the typical time scale of perceptual or motor tasks.</p>
<p>The problem is even more crucial if the decision is delayed compared to the presentation of sensory information. Sensory variables such as the direction of motion of a stimulus can be retained in “working memory” for significant periods of time even in the absence of sensory input. Neural correlates of this working memory appear as persistent neural activity in parietal and frontal brain areas and exhibit firing statistics similar to those found for sensory responses <xref ref-type="bibr" rid="pcbi.1001080-Funahashi1">[28]</xref>,<xref ref-type="bibr" rid="pcbi.1001080-Shadlen2">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1001080-Compte1">[29]</xref>. This persistent activity has been modeled as a stable state of recurrent neural network dynamics <xref ref-type="bibr" rid="pcbi.1001080-Compte2">[30]</xref>. However, such attractors correspond to stereotyped patterns of activity that can only represent a single stimulus value. For example, the memorized position of an object can be encoded by the position of a stable “bump” of activity <xref ref-type="bibr" rid="pcbi.1001080-Compte2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-BenYishai1">[31]</xref>. This would imply though that information about the reliability of the memorized cue is lost and cannot be used for delayed cue combination or decision making. We hypothesize instead that stimuli are memorized in the same format as sensory input, i.e. as a probability distribution. The question of how probability distributions can be memorized by a population of neurons remains largely unanswered.</p>
<p>Here, we approach these issues by using a new interpretation of population coding in the context of temporal sensory integration. We consider spikes, rather than rates, as the basic unit of probabilistic representation. We show how recurrent networks of leaky integrate-and-fire neurons can construct, combine and memorize probability distributions of dynamic sensory variables. Spike generation in these neurons results from a competition between an integration of evidence from feed-forward sensory inputs and a prediction from lateral connections. A neuron therefore acts as a “predictive encoder”, only spiking if its input cannot be predicted by its own or its neighbors' past activity.</p>
<p>We demonstrate that such networks integrate and combine sensory inputs optimally, i.e. without losing information, and track the stimulus dynamics spike-per-spike even in the absence of sensory input, over timescales much longer than the neural time constants. This framework thus provides a first comprehensive theory for optimal <italic>spike-based</italic> sensory integration and working memory. In contrast to rate models implemented with Poisson spiking neurons, this model does not require large levels of redundancy to compensate for the noise added by stochastic spike generation.</p>
<p>Similar to cortical sensory neurons, model neurons respond with sustained, asynchronous spiking activity. Spike times are variable and uncorrelated, despite the deterministic spike generation rule. However, in contrast to rate codes, each spike “counts”. The trial to trial variability of spike trains does not reflect an intrinsic source of noise that requires averaging, but is a consequence of predictive coding. While spike times are unpredictable at the level of a single neuron, they deterministically represent a probability distribution at the level of the population. This leads us to reinterpret the notions of signal and noise in cortical neural responses.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Goal of the model</title>
<p>In order to clarify the presentation, we will concentrate on the following general task. Imagine a cat chasing a mouse in your garden. The cat integrates auditory and visual information to locate the mouse. It will combine these cues according to their reliability. If for instance the mouse is partially covered by a bush, i.e. there is a high uncertainty associated with the visual cue, the cat will give a higher weight to its auditory information. If the mouse suddenly disappears behind a tree and cannot be heard or seen anymore, the cat should estimate the likely trajectory of the mouse in the absence of any relevant sensory input, in order to anticipate where the mouse is going to reappear. Finally, this information will need to be extracted when the cat eventually decides to catch the mouse.</p>
<p>The cat's task can thus be divided into three parts (<xref ref-type="fig" rid="pcbi-1001080-g001">figure 1A</xref>). First, during a sensory integration period, sensory cues about a dynamic stimulus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e001" xlink:type="simple"/></inline-formula>, are combined over modalities and time in order to get a more refined estimate about the stimulus. Second, during a memory period, the evolution of the stimulus is predicted and tracked while past information is kept available. Finally, during a decoding period, the position of the mouse is extracted from the memorized information.</p>
<fig id="pcbi-1001080-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g001</object-id><label>Figure 1</label><caption>
<title>Illustrations.</title>
<p>(A) Illustration of the network task. An auditory and a visual cue (cue 1 and 2) about a dynamic stimulus (e.g. the position of a mouse) are integrated and combined during the integration period. During the memory period, this information is kept available such that it can be read out over a timescale of order <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e002" xlink:type="simple"/></inline-formula> during the read-out period. (B) Schematic illustration of the network. The visual and the auditory cue about stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e003" xlink:type="simple"/></inline-formula> are encoded in two independent input populations that send feed-forward inputs to the output population. The output population is recurrently connected. The connection weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e004" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e005" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e006" xlink:type="simple"/></inline-formula> are functions of the input kernels <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e008" xlink:type="simple"/></inline-formula> as well as the output kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e009" xlink:type="simple"/></inline-formula>. (C) Illustration of the spike generation rule. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e010" xlink:type="simple"/></inline-formula> denotes the stimulus posterior given all inputs and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e011" xlink:type="simple"/></inline-formula> represents an approximation to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e012" xlink:type="simple"/></inline-formula> that is decoded from the output spike trains. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e013" xlink:type="simple"/></inline-formula> should be as close as possible to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e014" xlink:type="simple"/></inline-formula>. An output spike adds a kernel to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e015" xlink:type="simple"/></inline-formula>. If its effect is to reduce the mean squared distance between the curves (down right), the spike is fired. The spike is not generated however if it increases the distance between the two curves (top right).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g001" xlink:type="simple"/></fig>
<p>We assume that the dynamic stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e016" xlink:type="simple"/></inline-formula> evolves according to a drift-diffusion process of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e017" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e018" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e019" xlink:type="simple"/></inline-formula> are parameters and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e020" xlink:type="simple"/></inline-formula> is a Wiener process. The first term on the right-hand side of equation (1) describes the predictable drift of the stimulus. Intuitively, it describes the velocity of the stimulus. The second term describes stochastic and therefore unpredictable changes in the stimulus. This is the diffusive part of the stimulus dynamics.</p>
<p>Visual and auditory inputs are provided by two independent population of neurons on two input layers, a “visual” layer and an “auditory” layer. Input neurons respond to position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e021" xlink:type="simple"/></inline-formula> with noisy spike trains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e022" xlink:type="simple"/></inline-formula> (auditory) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e023" xlink:type="simple"/></inline-formula> (visual). We denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e024" xlink:type="simple"/></inline-formula> the auditory spike trains observed up to time t, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e025" xlink:type="simple"/></inline-formula> the number of spikes observed in a small temporal window <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e026" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e027" xlink:type="simple"/></inline-formula>. We assume that sensory input spikes depend instantaneously on the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e028" xlink:type="simple"/></inline-formula> and are conditionally independent of the past, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e029" xlink:type="simple"/></inline-formula>. Moreover, we consider sensory likelihoods that belong to the exponential family of probability distributions with linear sufficient statistics. In this case, the log probability of observing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e030" xlink:type="simple"/></inline-formula> spikes in the auditory layer can be written as a sum of spike counts<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e031" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e033" xlink:type="simple"/></inline-formula> are functions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e034" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e035" xlink:type="simple"/></inline-formula> acts as a normalization term. We will refer to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e036" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e037" xlink:type="simple"/></inline-formula> as the kernel and the bias of the auditory likelihood respectively. A similar equation holds for the visual likelihood. The family of distributions described by equation (2) captures most popular models of neural noise including Poisson noise, Gaussian or exponential noise, with or without correlations. In this article, we assume independent Poisson noise for simplicity. In this case, the kernels correspond to the log tuning curves, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e039" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e040" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e041" xlink:type="simple"/></inline-formula> are the visual and auditory tuning curves (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
<p>The two sensory input layers converge onto a recurrently connected output layer (<xref ref-type="fig" rid="pcbi-1001080-g001">figure 1B</xref>) that generates a set of output spike trains, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e042" xlink:type="simple"/></inline-formula>. We want these output spikes to represent the posterior probability of the position of the mouse given the visual and auditory spike trains. For this purpose, we define an “on-line decoder”, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e043" xlink:type="simple"/></inline-formula>, that reads out the information in the output population through a leaky integration of output spikes. The advantages of such a read-out function will be discussed shortly below. We define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e044" xlink:type="simple"/></inline-formula> such that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e045" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e046" xlink:type="simple"/></inline-formula> is a leak term, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e047" xlink:type="simple"/></inline-formula> defines a choice of output kernels, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e048" xlink:type="simple"/></inline-formula> stands for the temporal derivative of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e049" xlink:type="simple"/></inline-formula>. The network structure and dynamics shall ensure that this read-out approximates the log posterior of the combined inputs:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e050" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>If this equation holds, the output neurons are said to encode the stimulus “optimally”.</p>
<p>This decoder defines how the posterior probability is represented on-line (i.e. within time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e051" xlink:type="simple"/></inline-formula>) by the output spike trains. However, perceptual or motor tasks might never require an explicit read-out of probability distributions. The decoder is therefore a theoretical construct that does not have to be implemented in any specific neural structure.</p>
<p>The coding strategy for the output layer is chosen for self-consistency, i.e. it ensures that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e052" xlink:type="simple"/></inline-formula> can be used as input for further processing stages. Indeed, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e053" xlink:type="simple"/></inline-formula> is treated as a log-likelihood of output spike counts weighted by kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e054" xlink:type="simple"/></inline-formula> (compare equations 2 and 3). Furthermore, this coding strategy presents three additional advantages. First, it ensures that information about the stimulus can be read out on-line and spike-per-spike, each new spike of a neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e055" xlink:type="simple"/></inline-formula> adding a kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e056" xlink:type="simple"/></inline-formula>. Second, the leak term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e057" xlink:type="simple"/></inline-formula> implies that the position inferred from <italic>all</italic> past inputs (i.e. during seconds or minutes of sensory integrations or working memory) can be extracted within a time window of order <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e058" xlink:type="simple"/></inline-formula> (typically a few tens of milliseconds). This enables both long sensory integration as well as fast computation with leaky devices such as biological neurons. Finally, since the read-out is linear in log probability, combining information from multiple spike trains corresponds simply to using additional read-out kernels. For example, consider another network computing the position of the mouse based on olfactory cues. The total information can be read out by a single decoder applied to the output spike trains of both networks simultaneously. In effect, this performs a product of the two posterior probabilities.</p>
<p>We now derive the dynamics of the output neurons that will ensure that equation (4) holds approximately.</p>
</sec><sec id="s2b">
<title>Network dynamics</title>
<sec id="s2b1">
<title>Inference</title>
<p>In a first step, we need to know what an ideal observer, i.e. an observer that performs optimal inference on the input spikes, would know about the stimulus. We denote it as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e059" xlink:type="simple"/></inline-formula> which is the unnormalized log posterior probability of the stimulus given all inputs. Normalization can be neglected since the important information about the stimulus is contained in the shape and location of the distribution.</p>
<p>With the assumptions made in the previous section, we can derive an expression for the ideal observer of the stimulus in the limit of small <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e060" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e061" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>The ideal observer performs a linear integration of the input spikes weighted by the kernels of their likelihoods. The term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e062" xlink:type="simple"/></inline-formula> describes the evolution of the log posterior in the absence of input. As a consequence of the drift-diffusion dynamics of the stimulus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e063" xlink:type="simple"/></inline-formula> derives from a Fokker-Planck equation and takes the form <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e064" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details).</p>
</sec><sec id="s2b2">
<title>Output generation</title>
<p>Output spike trains shall be generated such that the output read-out, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e065" xlink:type="simple"/></inline-formula>, matches the ideal observer <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e066" xlink:type="simple"/></inline-formula>. We first discretize the stimulus space and evaluate the posterior at positions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e067" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e068" xlink:type="simple"/></inline-formula> corresponds to the preferred stimulus of output neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e069" xlink:type="simple"/></inline-formula>. Let us denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e070" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e071" xlink:type="simple"/></inline-formula>. Similarly, we denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e072" xlink:type="simple"/></inline-formula> the discretized version of the vector function kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e073" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e074" xlink:type="simple"/></inline-formula>.</p>
<p>We propose a spike generation criterion that minimizes the mean squared distance between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e075" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e076" xlink:type="simple"/></inline-formula>. It is schematically illustrated in <xref ref-type="fig" rid="pcbi-1001080-g001">figure 1C</xref>. The effect of a spike of output neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e077" xlink:type="simple"/></inline-formula> is to add a kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e078" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e079" xlink:type="simple"/></inline-formula>. A spike is generated whenever it has the effect of reducing the mean squared distance between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e080" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e081" xlink:type="simple"/></inline-formula>, i.e. if<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e082" xlink:type="simple"/><label>(6)</label></disp-formula></p>
<p>This criterion ensures that neurons only fire spikes to account for new information about the stimulus that has not previously been reported by their own or their neighbors' activity. Avoiding spike redundancies minimizes the metabolic cost of the code and increases the independence among output spikes.</p>
<p>In contrast to other error measures such as the Kullback-Leibler divergence, the squared distance results in a local integrate-and-fire spike generation rule. Indeed, let us now define the “membrane potential” <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e083" xlink:type="simple"/></inline-formula>, which simply is the difference between input and output log posterior, weighted by output kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e084" xlink:type="simple"/></inline-formula>. We can show that the temporal evolution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e085" xlink:type="simple"/></inline-formula> follows leaky integrate-and-fire dynamics (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e086" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>Output neurons integrate input spikes with feed-forward weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e087" xlink:type="simple"/></inline-formula> and output spikes with lateral weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e088" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e089" xlink:type="simple"/></inline-formula> denotes the matrix transpose. The constant bias term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e090" xlink:type="simple"/></inline-formula> contains information about how informative it is not to receive a spike. Neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e091" xlink:type="simple"/></inline-formula> fires a spike if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e092" xlink:type="simple"/></inline-formula>, with threshold <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e093" xlink:type="simple"/></inline-formula>. After firing a spike <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e094" xlink:type="simple"/></inline-formula> is reset to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e095" xlink:type="simple"/></inline-formula>.</p>
<p>The slow currents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e096" xlink:type="simple"/></inline-formula> are driven by output spikes and predict the dynamics of the stimulus. Their temporal evolution is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e097" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<p>The weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e098" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e099" xlink:type="simple"/></inline-formula> are functions of the output kernel, the leak and the stimulus dynamics: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e101" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e102" xlink:type="simple"/></inline-formula> denotes the partial derivative with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e103" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2b3">
<title>Roles of the different currents</title>
<p>An output neuron receives inputs through fast feed-forward connections (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e105" xlink:type="simple"/></inline-formula>), fast recurrent connections (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e106" xlink:type="simple"/></inline-formula>) as well as slow recurrent connections (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e107" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e108" xlink:type="simple"/></inline-formula>). Fast currents are “instantaneous” while slow currents are integrated with the time constant of the decoder <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e109" xlink:type="simple"/></inline-formula>. For the sake of simplicity we have assumed that the membrane time constant is the same as the time constant of the decoder. This predicts that fast postsynaptic potentials (PSPs) are exponentials with decay <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e110" xlink:type="simple"/></inline-formula> while slow PSPs are Gamma functions (an exponential of decay <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e111" xlink:type="simple"/></inline-formula> convolved by itself). In practice, the two time constants could differ significantly without affecting performance. In fact, leak currents scale with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e112" xlink:type="simple"/></inline-formula> and are in general much smaller than feed-forward and recurrent currents scaling with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e113" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e114" xlink:type="simple"/></inline-formula>. The contributions of leak currents to the network dynamics are therefore negligible (see <xref ref-type="fig" rid="pcbi-1001080-g002">figure 2</xref>). It follows that the membrane potential dynamics could be much faster than the slow currents, as would be the case for instance for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e115" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e116" xlink:type="simple"/></inline-formula> synapses.</p>
<fig id="pcbi-1001080-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g002</object-id><label>Figure 2</label><caption>
<title>Currents.</title>
<p>Averaged currents to a neuron with a preferred stimulus of 180 deg as a function of the presented stimulus location. (A) Currents during the integration period. Feed-forward input currents (blue) are excitatory for stimuli similar to the preferred stimulus of the neuron and inhibitory otherwise. The sum of fast and slow recurrent currents (red-green dashed line) follows an inverted profile of similar magnitude that counteracts the effect of the feed-forward input. The leak current (magenta) is small in magnitude compared to the synaptic currents. (B) Currents during the memory period. Feed-forward inputs are equal to zero. The individual lateral currents are enhanced with respect to the integration period. However, their total sum (red-green dashed line) is balanced and close to zero (see also the black dashed line in C). (C) Total currents (including leak) during the integration period (solid line) and during the memory period (dashed line). In both cases, the contributions of individual currents balance each other out such that the total current is small, slightly excitatory among neurons whose preferred stimuli are similar to the presented stimulus and inhibitory otherwise. The two maxima of the current during the memory period are due to the non-linear component of the slow recurrent currents (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e117" xlink:type="simple"/></inline-formula>) that codes for the stimulus diffusion. It has the effect of broadening the response during the memory period (see <xref ref-type="fig" rid="pcbi-1001080-g003">figure 3A</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g002" xlink:type="simple"/></fig>
<p>Example contributions of the different currents are depicted in <xref ref-type="fig" rid="pcbi-1001080-g002">figures 2A and 2B</xref>. Feed-forward inputs transmit new sensory evidence about a stimulus to the output neurons. Thus, feed-forward currents are globally positive for neurons whose preferred stimuli are similar to the presented stimulus, and negative for neurons whose preferred stimuli are different from it (<xref ref-type="fig" rid="pcbi-1001080-g002">figure 2A</xref>). In contrast, fast recurrent inputs subtract the output population's prediction from this sensory input and hence have opposite signs. Neurons with globally positive feed-forward currents receive negative fast recurrent currents, and vice-versa. Short-range fast inhibition and long-range fast excitation have the effect of avoiding redundancies by only letting one output neuron transmit unaccounted information at a time.</p>
<p>Slow recurrent connections, on the other hand, have two distinct roles. First, they “reintroduce” information that has leaked out, hence making past information available within the time window of integration of the decoder. It is this short-range slow excitation and long-range slow inhibition, mediated by the recurrent connections <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e118" xlink:type="simple"/></inline-formula> (or more precisely their subpart <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e119" xlink:type="simple"/></inline-formula>), that enables sustained bumps of activity in the output layer and therefore implements working memory. The second role of the slow currents is to take into account the non-stationary dynamics of the stimulus. For example, the stimulus drift is predicted by a spatial derivative of the feed-forward inputs (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e120" xlink:type="simple"/></inline-formula>, a component of the lateral connections <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e121" xlink:type="simple"/></inline-formula>), while the stimulus diffusion is predicted by a bimodal current peaking at the position of maximal slope in population response, contributed both by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e123" xlink:type="simple"/></inline-formula>. Slow currents hence maintain, shift and widen the global pattern of activity in order to predict the future state of the stimulus.</p>
<p>Altogether, spike generation in our model is deterministic and results from a competition between an integration of evidence from feed-forward and slow lateral inputs, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e124" xlink:type="simple"/></inline-formula>, and a prediction from fast lateral connections <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e125" xlink:type="simple"/></inline-formula>. A direct and important consequence of this competition is the maintenance of an almost perfect balance between the global excitatory and inhibitory currents received by each output neuron (<xref ref-type="fig" rid="pcbi-1001080-g002">figure 2C</xref>). Indeed, the total average current is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e126" xlink:type="simple"/></inline-formula>, since the network dynamics ensure that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e127" xlink:type="simple"/></inline-formula>. Different choices of kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e128" xlink:type="simple"/></inline-formula> can change the sign of excitatory and inhibitory interactions among output neurons, but total excitation and inhibition is always going to be balanced by the network dynamics. Spikes are caused by unpredictable fluctuations of this total balanced input. Even though output neurons share most of their feed-forward and lateral connections with their neighbors, the resulting output spike trains are asynchronous and have low firing rates (see section on network predictions and <xref ref-type="sec" rid="s3">discussion</xref>).</p>
<p>Finally, we assumed for the sake of simplicity that the same output neuron can both excite and inhibit different target neurons, which is clearly not realistic. A more realistic model can be constructed by using one purely excitatory neuron and another purely inhibitory neuron for each output kernel.</p>
</sec><sec id="s2b4">
<title>Roles of the output kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e129" xlink:type="simple"/></inline-formula> and leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e130" xlink:type="simple"/></inline-formula></title>
<p>The free parameters of our model are the leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e131" xlink:type="simple"/></inline-formula> and output kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e132" xlink:type="simple"/></inline-formula>. All other parameters are functions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e133" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e134" xlink:type="simple"/></inline-formula>, the stimulus dynamics (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e135" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e136" xlink:type="simple"/></inline-formula>) or the input response tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e137" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e138" xlink:type="simple"/></inline-formula> (or, more generally, the input kernels <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e139" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e140" xlink:type="simple"/></inline-formula>).</p>
<p>The kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e141" xlink:type="simple"/></inline-formula> determines the spatial impact or “meaning” of a spike. For example, we can adjust the kernel to give more or less “weight” to each output spike. A larger kernel results in lower activity as less spikes are needed to convey the same information. Thus, if the output kernels are multiplied by a constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e142" xlink:type="simple"/></inline-formula>, the output firing rates are roughly divided by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e143" xlink:type="simple"/></inline-formula>. This comes at the cost of fine precision, since changes in log-posterior smaller than the output kernel are not represented.</p>
<p>These output kernels do not necessarily need to be known in advance by the decoder, or any other neural structure extracting information about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e144" xlink:type="simple"/></inline-formula> from the output spike trains. They can be estimated (or “learnt”) directly from the tuning curves, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e145" xlink:type="simple"/></inline-formula>, and covariance matrix, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e146" xlink:type="simple"/></inline-formula>, of the output neurons <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e147" xlink:type="simple"/><label>(9)</label></disp-formula></p>
<p>This relationship holds if the spiking likelihood of the output neurons lies in the exponential family with linear sufficient statistics <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>. We found that decoders using the “true” kernels or kernels estimated using equation (9) were almost identical and performed equally well. Simulation results are reported for the learnt kernel. Equation (9) also shows that the choice of a specific output kernel constrains the tuning curves and covariances of the output neurons.</p>
<p>Similarly, the leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e148" xlink:type="simple"/></inline-formula> determines the temporal meaning of a spike. It sets the timescale over which information contained in a spike is meaningful. Shorter kernels (i.e. larger leaks) lead to higher firing rates but also more precise tracking of temporal changes in the stimulus. As described in the next section, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e149" xlink:type="simple"/></inline-formula> sets the slope of firing rate increase during sensory integration. Additionally, sustained firing rates during working memory are also proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e150" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2b5">
<title>Representation of prior beliefs</title>
<p>Let us briefly go back to our example of the cat and the mouse and say that the cat is looking around to find a mouse to chase. Even in the absence of the mouse, the cat's beliefs on where the mouse is likely to appear is not uniform. The cat might for instance know that there is a family of mice living in a specific bush. It will then base its search mainly on the area around that bush. In other words, the cat has a strong prior belief on where mice are likely to appear.</p>
<p>The prior belief corresponds to the initial value of the log posterior, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e151" xlink:type="simple"/></inline-formula>, at the onset of the stimulus, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e152" xlink:type="simple"/></inline-formula>. Thus, prior information can be “stored” by applying some external input and driving the output membrane potentials into a specific configuration given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e153" xlink:type="simple"/></inline-formula> before the start of a trial. The network activity will then maintain this information in memory in the form of a persistent pattern of activity, as it would for a sensory stimulus. Once the stimulus is presented, sensory information will be integrated starting from an initial state determined by this prior.</p>
</sec><sec id="s2b6">
<title>Approximating the nonlocal diffusion term</title>
<p>If the stimulus includes a diffusive component, the slow current <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e154" xlink:type="simple"/></inline-formula> contains a nonlocal and nonlinear term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e155" xlink:type="simple"/></inline-formula>. We could imagine that this term is computed by the dendritic trees of the output neurons. It has been shown that dendrites can implement nonlinear functions similar to a two layered neural network <xref ref-type="bibr" rid="pcbi.1001080-Poirazi1">[32]</xref>. Alternatively, we can approximate the nonlocal term by using the central limit theorem and approximating the posterior by a Gaussian distribution. The slow current <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e156" xlink:type="simple"/></inline-formula> is then given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e157" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e158" xlink:type="simple"/></inline-formula>. The time varying leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e159" xlink:type="simple"/></inline-formula> depends on the variance of the posterior distribution, which could be computed with a Kalman filter or directly estimated from the output spike trains. In this paper, we use a simpler approximation and replace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e160" xlink:type="simple"/></inline-formula> by a constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e161" xlink:type="simple"/></inline-formula>, resulting in a fully linear slow current. An example of this approximation will be shown in <xref ref-type="fig" rid="pcbi-1001080-g003">figure 3E</xref> of the next section. All other simulation are done using the full model.</p>
<fig id="pcbi-1001080-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g003</object-id><label>Figure 3</label><caption>
<title>Network performance.</title>
<p>(A) Input and output spike trains on a single trial. A stimulus with constant drift and diffusion is presented for 500 ms (gray area). (B) Time evolution of the stimulus posterior for the ideal observer (blue) and the network read-out (red). Thick lines show the mean of the posterior and narrow lines the corresponding width. The stimulus trajectory is shown in black. The dashed black line indicates the predictable (drift) part of the stimulus that the network is tracking during the memory period. (C) Snapshots of the posteriors, from left to right; after 500ms (end of integration period), after 2000 ms and after 5000 ms. (D) Coding performance measured as the standard deviation of the stimulus estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e162" xlink:type="simple"/></inline-formula> around its real value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e163" xlink:type="simple"/></inline-formula>. The blue and red curves depict the performance of the ideal observer and the network respectively and the green curve shows the performance of a network without slow currents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e164" xlink:type="simple"/></inline-formula>. (E) Width of the posterior decoded from the ideal observer (blue), the full network model (described in equations 7 and 8) (red), a network in which we approximate the nonlocal term in the slow currents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e165" xlink:type="simple"/></inline-formula> by a linear term (see equation 10) (green) and a network for which we completely remove the nonlocal term (magenta).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g003" xlink:type="simple"/></fig></sec></sec><sec id="s2c">
<title>Model predictions</title>
<p>We illustrate the network dynamics and model predictions using the general task outlined in <xref ref-type="fig" rid="pcbi-1001080-g001">figure 1A and 1B</xref>. Input neurons have bell-shaped tuning curves and generate Poisson spike trains in response to an angular stimulus with constant drift and diffusion. The output neurons follow the leaky integrate-and-fire dynamics of equation (7). The output kernels <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e166" xlink:type="simple"/></inline-formula> are chosen to be Gaussian shaped. Details of the simulation parameters can be found in the <xref ref-type="sec" rid="s4">Materials and Methods</xref> section. All model predictions described below are largely independent of the specific choices of input and output kernels.</p>
<sec id="s2c1">
<title>Network performance</title>
<p><xref ref-type="fig" rid="pcbi-1001080-g003">Figure 3A</xref> shows the input and output spike trains on an example trial. A stimulus with constant drift and diffusion is presented for 500 ms during which the output population receives feed-forward sensory input from the auditory and visual layer (top two panels of <xref ref-type="fig" rid="pcbi-1001080-g003">figure 3A</xref>). In the subsequent memory period, input stimulation ceases completely. The output population sustains spiking activity even in the absence of sensory input (bottom panel of 3A). This activity represents a working memory of the stimulus, i.e. a neural correlate of keeping past information available in the time window of integration of output neurons.</p>
<p>The response of the decoder closely matches the performance of an ideal observer (<xref ref-type="fig" rid="pcbi-1001080-g003">figure 3B and 3C</xref>), illustrating the optimality of the model network. This is true for both the decoded posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e167" xlink:type="simple"/></inline-formula> and the distribution of position estimates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e168" xlink:type="simple"/></inline-formula> (see methods). During the sensory integration period, the standard deviation of the estimator narrows, reflecting an accumulation of evidence about the stimulus (<xref ref-type="fig" rid="pcbi-1001080-g003">figure 3D</xref>). In the memory period, the sustained spiking activity keeps representing a probability distribution about the stimulus. This posterior tracks the drift of the stimulus, i.e. the predictable component of the stimulus dynamics (<xref ref-type="fig" rid="pcbi-1001080-g003">figure 3B</xref>). The diffusion however is unpredictable and therefore increases the uncertainty about the stimulus. As a result, the standard deviation of the decoded posterior increases over time (<xref ref-type="fig" rid="pcbi-1001080-g003">figure 3D</xref>). However, if we remove the diffusion term (i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e169" xlink:type="simple"/></inline-formula>), the standard deviation remains constant during the memory period (not shown). In all cases, the standard deviation of the network position estimates remains less than 2% above the standard deviation of an optimal estimator.</p>
<p>Slow currents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e170" xlink:type="simple"/></inline-formula> are essential to compensate for the leak in the decoder and predict the drift and diffusion of the stimulus. Without them, sensory integration is suboptimal and information quickly degrades during the memory period (<xref ref-type="fig" rid="pcbi-1001080-g003">figure 3D</xref>). This is a direct consequence of the limited time constant of integration of individual neurons. In fact, neurons lose information at a rate set by the leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e171" xlink:type="simple"/></inline-formula>. The slow currents compensate for this loss by reintroducing the information that has leaked out and hence making past information available within the time window of integration of a neuron. This turns the neurons into optimal integrators. The nonlinear part of the slow currents can be efficiently approximated by a linear term (equation 10). For an optimal choice of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e172" xlink:type="simple"/></inline-formula>, the linearized network performs very closely to the full network (<xref ref-type="fig" rid="pcbi-1001080-g003">figure 3E</xref>).</p>
<p>The network implements Bayesian inference and therefore combines visual and auditory cues optimally, weighting each sensory cue according to its accuracy. To illustrate this point, we plot the performance of the network in a bimodal case in which both input cues encode the stimulus with equal accuracy and two “unimodal” cases in which one of the inputs represents the stimulus much more accurately than the other. The accuracy of the sensory input was changed by multiplying the corresponding input tuning curves by a constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e173" xlink:type="simple"/></inline-formula>. In all three cases, the accuracy of the output estimator, measured by its standard deviation, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e174" xlink:type="simple"/></inline-formula>, lies within 2% of optimal performance (<xref ref-type="fig" rid="pcbi-1001080-g004">figure 4A</xref>). Thus, the network automatically adjusts to changes in cue reliability.</p>
<fig id="pcbi-1001080-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g004</object-id><label>Figure 4</label><caption>
<title>Cue combination and priors.</title>
<p>(A) Estimation accuracy for different reliabilities of the input cues: both input cues are equally reliable (bimodal) or one cue is more reliable than the other (cue 1 and cue 2). In each subgroup, bars depict from left to right the encoding accuracy of: cue 1, cue 2, the ideal observer, the network at the end of the integration period and the network after one second in the memory period. (B) Biasing effect of the prior measured as the difference between the real and the estimated stimulus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e175" xlink:type="simple"/></inline-formula>. The effect is stronger for short integration times (200 ms, left) than for long integration times (500 ms, right). Black bars show the bias expected for a Bayesian observer, white bars depict the network bias. (C) Standard deviation of the estimator with a Gaussian prior (solid lines) and with a flat prior (dashed lines). A structured prior narrows the width of the posterior. Blue lines denote the ideal observer, red lines the network performance. (D) Input and output spike trains on a single trial. A constant stimulus is presented for 500 ms (gray area). The spontaneous activity before stimulus onset encodes the prior belief about the stimulus. (E) Time evolution of the posterior for the ideal observer (blue) and the network (red). Thick lines show the mean of the posterior and narrow lines the corresponding width. The stimulus is shown in black.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g004" xlink:type="simple"/></fig>
<p>For the same reason, the network takes prior information into account accurately. <xref ref-type="fig" rid="pcbi-1001080-g004">Figures 4D and 4E</xref> illustrate the spike trains and the decoded posterior distribution on a single trial with a Gaussian prior centered at an orientation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e176" xlink:type="simple"/></inline-formula>. The prior is faithfully represented by the sustained spiking activity before stimulus onset (<xref ref-type="fig" rid="pcbi-1001080-g004">figure 4D</xref>). In this example, a static stimulus is presented to the network for 500 ms. As predicted for an optimal Bayesian observer, the prior biases the position estimates towards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e177" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1001080-g004">figure 4B</xref>) and narrows the posterior distribution (<xref ref-type="fig" rid="pcbi-1001080-g004">figure 4C</xref>). Moreover, the influence of the prior depends on the reliability of the sensory signal, i.e. the bias is stronger if the stimulus is presented for only 200 ms instead of 500 ms, as shown in <xref ref-type="fig" rid="pcbi-1001080-g004">figure 4B</xref>.</p>
</sec><sec id="s2c2">
<title>Output firing rates</title>
<p>The presentation of a stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e178" xlink:type="simple"/></inline-formula> results in a bell-shaped pattern of activity in the output population, peaking at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e179" xlink:type="simple"/></inline-formula>. Thus, output neurons are tuned to the position <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e180" xlink:type="simple"/></inline-formula> with bell-shaped tuning curves, similarly to the input neurons. However, the shape and amplitude of their tuning curves vary during the entire duration of the trial (<xref ref-type="fig" rid="pcbi-1001080-g005">figure 5B</xref>).</p>
<fig id="pcbi-1001080-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g005</object-id><label>Figure 5</label><caption>
<title>Output firing rates.</title>
<p>(A) Post-stimulus time histogram (PSTH) of the output activity in response to a stimulus with constant diffusion. Color indicates firing rates in Hz. The stimulus (magenta line) is presented during the first 500 ms. (B) Tuning curves of a sample neuron. Spikes are counted in 10ms bins centered at 50 ms (black), 200 ms (blue) and 500 ms (red) during the integration period and at 550 ms (green) and 2500 ms (magenta) during the memory period. (C) Traces of the average firing rate of a neuron whose preferred stimulus lies around the peak of the bump of activity. Different curves depict different levels of Fisher information in the input population codes: reference information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e181" xlink:type="simple"/></inline-formula> for the regular parameters (red), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e182" xlink:type="simple"/></inline-formula> (green) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e183" xlink:type="simple"/></inline-formula> (blue). (D) Traces of the average firing rate of three neurons whose preferred stimuli lie at the peak of the bump of activity (blue), the side of the bump (red) or far away from the bump (green). (E) PSTH of the output activity in response to a static stimulus presented for 500 ms. (F,G) Interspike interval (ISI) histogram during the integration period (F) and during the memory period (G) for a sample neuron. The red line shows the ISI histogram of a Poisson process with the same rate.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g005" xlink:type="simple"/></fig>
<p>The integration of sensory evidence and its maintenance in working memory is reflected by the instantaneous firing rates of the output neurons. <xref ref-type="fig" rid="pcbi-1001080-g005">Figure 5A</xref> depicts the post-stimulus time histogram (PSTH) of the output neurons in response to a stimulus with constant diffusion. The corresponding tuning curves are illustrated in <xref ref-type="fig" rid="pcbi-1001080-g005">figure 5B</xref>. During the integration period, the firing rates initially jump to a higher level of activity and subsequently ramp up. The gain of the tuning curves increases linearly with time, reflecting an accumulation of sensory evidence. Both the size of the initial response and the slope of the ramping increase in firing rate depends on the accuracy and quality of the sensory inputs. Thus, if we increase the Fisher information available in the input population codes (see methods), firing rates grow faster, reflecting a faster accumulation of evidence (<xref ref-type="fig" rid="pcbi-1001080-g005">figure 5C</xref>). This is reminiscent of neural responses in the parietal cortex during motion integration tasks <xref ref-type="bibr" rid="pcbi.1001080-Shadlen2">[27]</xref>. The slope of the ramp is also proportional to the leak term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e184" xlink:type="simple"/></inline-formula>. Thus, integrate-and-fire neurons with no leak (or with time constants significantly longer than the effective time constant of the dynamic stimulus) would have constant firing rates during sensory integration. This is predictable since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e185" xlink:type="simple"/></inline-formula> implies that the decoder is able to integrate output spike trains over the entire duration of the trial. It is therefore not necessary to represent accumulated sensory evidence on-line. In all cases, neural activities eventually saturate at a constant level, since the diffusive noise limits the precision with which the stimulus can be encoded (not shown here).</p>
<p>Firing rates during the memory period have a lower baseline activity but similar tuning as during the integration period. Over time, tuning curves and population activity decrease, broaden and eventually disappear (<xref ref-type="fig" rid="pcbi-1001080-g005">figure 5A and 5B</xref>). As a result, the instantaneous firing rates during the memory period are not constant but vary dynamically, ramping either up or down. <xref ref-type="fig" rid="pcbi-1001080-g005">Figure 5D</xref> shows the average firing rates of three neurons whose preferred stimuli are located around the peak of the persistent bump of activity (blue), the side of the bump (red) and far from the bump (green). Similar neural behavior has been observed in parietal and prefrontal brain areas during working memory tasks <xref ref-type="bibr" rid="pcbi.1001080-Shadlen2">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Romo1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Funahashi1">[28]</xref>. Our model suggests that such ramping behavior might reflect the widening of the posterior over time due to an accumulation of uncertainty about the represented variable. Thus, ramp-like changes in firing rates during working memory tasks could be a signature of a gradual decrease in confidence for this memory.</p>
<p>However, the behavior of the network is different in the absence of diffusion. The network is then able to maintain information about the stimulus over very long timescales, reflected by a neutrally stable bump of activity (<xref ref-type="fig" rid="pcbi-1001080-g005">figure 5E</xref>). The firing rates during the memory period are thus constant over time for a static stimulus. However, the amplitude of the sustained bump of activity depends on the amount of accumulated sensory evidence (<xref ref-type="fig" rid="pcbi-1001080-g005">figure 5C</xref>) as well as on the neural integration time constant. Indeed, the sustained firing rates necessary to maintain a constant log posterior, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e186" xlink:type="simple"/></inline-formula>, are proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e187" xlink:type="simple"/></inline-formula> multiplied by the leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e188" xlink:type="simple"/></inline-formula> (see equation 3). Thus, persistent activity is larger for more informative sensory inputs or stronger leaks. Notice that neurons and decoders without a leak would not exhibit any sustained activity.</p>
<p>This implies that our working memory model differs from previous models that are based on line attractor dynamics <xref ref-type="bibr" rid="pcbi.1001080-BenYishai1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Compte2">[30]</xref>. For these bump attractors, neural dynamics settle onto stereotyped activity profiles whose peak positions encode the most likely stimulus values. The probabilistic information associated with these values, however, is lost. In contrast, our network acts as an optimal integrator that maintains the sensory information it has received in the past. Consequently, various patterns of activity that differ in shape and amplitude can be sustained.</p>
<p>In particular, our network can maintain multi-modal posterior distributions reflected in multi-modal patterns of activity. <xref ref-type="fig" rid="pcbi-1001080-g006">Figure 6</xref> depicts a case in which two different stimuli are consecutively presented to the network with a delay interval of one second. Both stimuli are presented for equal time periods of 350 ms. Their representation depends on the relative distance between them. If the stimuli are presented far away from each other, the network sustains two spatially distinct bumps of activity (<xref ref-type="fig" rid="pcbi-1001080-g006">figure 6A</xref>). Both stimuli are also clearly represented in a bimodal log posterior distribution. However, if the two stimuli lay close together, individual bumps fuse into a single bump (<xref ref-type="fig" rid="pcbi-1001080-g006">figure 6B</xref>). As a consequence, the log posterior becomes unimodal, peaking in between the two stimuli. Thus, the accuracy at which information about individual stimuli can be resolved is limited by their spatial discrepancy.</p>
<fig id="pcbi-1001080-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g006</object-id><label>Figure 6</label><caption>
<title>Response to multiple stimuli.</title>
<p>Two static stimuli (red lines) are consecutively presented to the network for 350 ms each. They are separated by a delay time interval of one second. Their spatial distance is (A) 180 deg, and (B) 45 deg. Top row: Spike trains on a single trial. Bottom row: Time evolution of the unnormalized log posterior (gray scale representation). The simulated network contains 200 instead of 50 neurons for better visual clarity.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g006" xlink:type="simple"/></fig></sec><sec id="s2c3">
<title>Output spike train statistics</title>
<p>The resulting output spike trains are asynchronous and spike times are not reproducible from trial to trial. They exhibit properties very similar to Poisson processes. Thus, the interspike interval (ISI) distributions of the output spike trains are quasi-exponential in both integration and memory period (<xref ref-type="fig" rid="pcbi-1001080-g005">figure 5F and 5G</xref>). We find coefficients of variation (CV) of 0.97 in the integration and 1.06 in the memory period. Fano factors are about 1.4 in both periods. We also observe only small cross correlations between different neurons. Correlation coefficients never exceeded 0.001.</p>
<p>The sensory stage in our model is noisy, reflected by the Poisson firing of the input neurons. In contrast, output neurons generate spikes deterministically. Despite this fact, their spike trains resemble independent Poisson processes. This is true even during the memory period when the network activity is self-sustained and no noise is introduced by the external inputs. This eliminates the possibility that the output statistics are directly inherited from the Poisson distributed, feed-forward inputs and raises the question of where this variability comes from. In particular, can the responses of the network be considered to obey the predictions of a rate model?</p>
<p>We hence investigate the origin and role of this variability by using two approaches: A perturbation approach to study the dependency of output spike trains on initial conditions; and a decoding approach where we study how well the spike train of an output neuron can be predicted from the activity of the other neurons in the population.</p>
<p><italic>Perturbation approach</italic>. We consider the smallest possible perturbation; one additional output spike. The injection of only one extra spike disrupts the spike pattern and reshuffles the times of all subsequent spikes in the population (<xref ref-type="fig" rid="pcbi-1001080-g007">figure 7A</xref>). This effect is observed regardless of whether the extra spike is injected during the memory period or during the integration period. The average firing rates of the output neurons sharply increase directly after the perturbation, indicating that each extra spike produces many other extra spikes in its postsynaptic targets (<xref ref-type="fig" rid="pcbi-1001080-g007">figure 7C</xref>). This rise in firing rate quickly decays, such that the perturbed and unperturbed firing rates become indistinguishable within 10 ms after the injection of the extra spike. Such short-lived increase in population firing rate due to an added spike has recently been reported <italic>in vivo</italic> based on stimulation and recordings in rat barrel cortex <xref ref-type="bibr" rid="pcbi.1001080-London1">[34]</xref>.</p>
<fig id="pcbi-1001080-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g007</object-id><label>Figure 7</label><caption>
<title>Spike train variability.</title>
<p>(A) Output spike trains for two runs (blue and red) of activity starting with the same initial conditions. The red run is perturbed by the injection of one extra spike (shown by the red arrow). (B) Time course of the posterior of the two runs. (C) PSTH of the control (blue) and the perturbed (red) runs. The extra spike is injected at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e189" xlink:type="simple"/></inline-formula>. Spikes are counted in 2 ms time bins and averaged over all neurons and over 10000 trials. (D) Time course of the normalized cross-correlation between the two runs of activity. The vertical dotted line indicates the time at which the perturbation (one extra spike) was added. (E) Predictability (equation 33) of the activity of an output neuron if we record from a fraction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e190" xlink:type="simple"/></inline-formula> neurons of the output population. The predictability for neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e191" xlink:type="simple"/></inline-formula> is plotted for spikes that are generated from the deterministic network (blue) or from a Poisson process (red). The rightmost predictability (at a fraction of 1) corresponds to the predictability of the measured, i.e. not predicted, membrane potential. The inset shows the increase in predictability previous to a spike (for a fraction of recorded neurons of 0.8). (F) Schematic illustration of the error correcting properties of the network. The left panel shows a reference spike train. Each spike adds a kernel that when added together give the log posterior G (top). If an extra spike is added (middle panel, red spike), the spike train is reshuffled in a way that keeps the total log posterior constant. If the initial spike fails to be elicited (right panel, blue dotted spike), a neighboring neuron recognizes the “hole” of information transmission and fires a spike to fill it. This changes the initial condition (first firing neuron in black) and therefore shuffles the spike train. The total log posterior remains the same.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g007" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1001080-g007">Figure 7D</xref> shows the time course of the normalized cross-correlation between the perturbed and unperturbed spike trains. The addition of an extra spike induces a fast drop of this correlation. This is characteristic of a chaotic network <xref ref-type="bibr" rid="pcbi.1001080-vanVreeswijk1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Sompolinsky1">[36]</xref> in which two initially identical trajectories quickly diverge after a small perturbation.</p>
<p>The encoding properties of the output neurons are thereby not affected. The decoded posterior still matches the ideal observer closely (<xref ref-type="fig" rid="pcbi-1001080-g007">figure 7B</xref>). This shows that there is a multitude of spike patterns that can optimally encode the same information. Which pattern is chosen by the network strongly depends on initial conditions and small perturbations (see the schematic illustration in <xref ref-type="fig" rid="pcbi-1001080-g007">figure 7F</xref>).</p>
<p>We observed the same characteristics if a single output spike fails to be fired. Spike patterns are again completely reshuffled while coding performance is unaffected. Moreover, our model is robust to even frequent spike generation failure. The reason lies in the error correcting property of the code. If a spike generation fails it is compensated by a spike from another neuron that adds a similar kernel to the posterior, as illustrated in <xref ref-type="fig" rid="pcbi-1001080-g007">figure 7F</xref>.</p>
<p><italic>Decoding approach</italic>. We apply a decoding analysis during the memory period, in which the network relies only on its deterministic internal dynamics, and we consider a static stimulus without drift or diffusion.</p>
<p>Let us first assume that we record from the entire population of output neurons. We want to know how well the spike times of a single neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e192" xlink:type="simple"/></inline-formula> can be predicted by the responses of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e193" xlink:type="simple"/></inline-formula> other output neurons. Notice that if the spike trains were independent Poisson processes and hence completely uncorrelated, the spike times of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e194" xlink:type="simple"/></inline-formula> could not be predicted at all. In contrast, in our network, the membrane potential of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e195" xlink:type="simple"/></inline-formula> depends on the spikes from the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e196" xlink:type="simple"/></inline-formula> other neurons.</p>
<p>We can predict the spike times of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e197" xlink:type="simple"/></inline-formula> by estimating when its membrane potential (equation 7) will cross the firing threshold. This prediction will not be perfect since the initial state of the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e198" xlink:type="simple"/></inline-formula> is unknown. However, we can still predict spike times with millisecond accuracy with such a method.</p>
<p>Let us now suppose that we record (more realistically) from a subpopulation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e199" xlink:type="simple"/></inline-formula> output neurons. The responses of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e200" xlink:type="simple"/></inline-formula> other neurons in the full population is unknown. We want to know how well the spike times of recorded neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e201" xlink:type="simple"/></inline-formula> can still be predicted by the responses of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e202" xlink:type="simple"/></inline-formula> other recorded neurons. Our strategy is to treat the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e203" xlink:type="simple"/></inline-formula> recorded neurons as if they represented the whole output population, using their spike trains to predict the membrane potential of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e204" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e205" xlink:type="simple"/></inline-formula> (see methods). In this case, the spike times cannot be predicted with millisecond accuracy anymore. However, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e206" xlink:type="simple"/></inline-formula> is still correlated with the true membrane potential, and it increases shortly before an actual spike in neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e207" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1001080-g007">figure 7E</xref>, inset). We measured “predictability” by how significant this increase in predicted membrane potential is at the time of a spike (see methods). The predictability of an uncorrelated Poisson spike train would be zero.</p>
<p>As shown in <xref ref-type="fig" rid="pcbi-1001080-g007">figure 7E</xref>, the predictability is high when most of the population is taken into account. However, predictability decreases with the portion of output neurons that are recorded simultaneously. It becomes indistinguishable from a rate code with Poisson distributed, uncorrelated spike trains if less than 25% of the neurons in the population are recorded. In cases where it is possible to record from a large subpopulation, this analysis provides a specific, experimentally testable prediction.</p>
</sec><sec id="s2c4">
<title>Robustness</title>
<p>We have previously seen that our network is robust to small perturbations and spike generation failure. We are now going to show that it is also robust to synaptic noise. Synaptic background noise is a prominent source of neural noise <xref ref-type="bibr" rid="pcbi.1001080-Faisal1">[37]</xref>. Cortical neurons receive barrages of inputs that are largely uncorrelated with feed-forward stimuli <xref ref-type="bibr" rid="pcbi.1001080-Steriade1">[38]</xref> and this noisy input is sufficient to affect the spiking properties of these neurons <xref ref-type="bibr" rid="pcbi.1001080-Shu1">[39]</xref>. We model synaptic background noise as an additive white Gaussian noise term on the membrane potential of the output neurons. This noise current has a mean strength of zero and a standard deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e208" xlink:type="simple"/></inline-formula>. It increases the standard deviation of the total input that output neurons receive (including feed-forward and recurrent inputs) while letting the mean input unaffected. This results in a decrease of the signal-to-noise ratio of the total input, SNR = mean(input)/std(input), measured as the ratio of mean input to the standard deviation of the input. Thus, synaptic noise introduces additional uncertainty about the stimulus.</p>
<p><xref ref-type="fig" rid="pcbi-1001080-g008">Figure 8A</xref> shows the effect of different strengths of synaptic noise on the network. With increasing noise strength, the standard deviation of the stimulus estimator lies increasingly above its optimal value. However, even at a noise level that reduces the signal-to-noise ratio by 100%, the network performance at the end of the 500 ms integration period is only 15% worse than optimality. A SNR reduction of 20% only slightly affects the network performance. In the memory period, network performance decreases further although more slowly. This indicates that the network is most sensitive to noise at an early stage of the integration period. Once the stimulus posterior has sharpened, the network is more robust to noise perturbations. Altogether, our model is robust to even high levels of synaptic background noise.</p>
<fig id="pcbi-1001080-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001080.g008</object-id><label>Figure 8</label><caption>
<title>Robustness to noise.</title>
<p>(A) Coding performance of the network in the presence of synaptic background noise. The vertical axis plots the percentage excess of the standard deviation of the stimulus estimator above its optimal value. Results are reported for percentual decreases in the signal-to-noise ratio, SNR = mean(input)/std(input), of 0% (black), 20% (blue), 50% (red) and 100% (green). A static stimulus is presented during the first 500 ms (grey area). (B) Coding performance of a stochastic network for different output gains: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e209" xlink:type="simple"/></inline-formula> (green), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e210" xlink:type="simple"/></inline-formula> (magenta) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e211" xlink:type="simple"/></inline-formula> (cyan). The ideal observer is plotted in blue and the performance of the deterministic network in red. A static stimulus is presented during the entire 1500 ms. (C) Schematic illustration of the difference between deterministic and stochastic spike generation. The left and middle panel show two spike trains encoding the same information but starting from different initial conditions. However, neurons in the output population are recurrently connected and “know” therefore perfectly well, when to fire a spike such that the log posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e212" xlink:type="simple"/></inline-formula> is represented. If the lateral connections are removed, neurons fire stochastic spike trains that look similar to the deterministic ones but do not encode the same log posterior.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.g008" xlink:type="simple"/></fig>
<p>This robustness to even large levels of synaptic noise is another consequence of the error-correcting property of the code. Synaptic noise will lead neurons to reach their firing threshold even if their kernel does not decrease the mean squared distance between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e213" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e214" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1001080-g001">figure 1C</xref>). However, other output neurons will detect this temporary increase in prediction error in their membrane potential and fire spikes to compensate for it.</p>
<p>For a similar reason, our network is robust to changes in the connection strengths between neurons. Scaling all recurrent synapses by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e215" xlink:type="simple"/></inline-formula> from their optimal values leaves the network performance largely unaffected (figure not shown). This contrasts with networks based on line attractor dynamics (e.g. <xref ref-type="bibr" rid="pcbi.1001080-Seung2">[40]</xref>), which require connections to be tuned with better than 1% accuracy (see however <xref ref-type="bibr" rid="pcbi.1001080-Koulakov1">[41]</xref>).</p>
</sec><sec id="s2c5">
<title>Comparison to a rate model</title>
<p>Despite its deterministic nature, our model exhibits firing statistics comparable to a rate model with independent Poisson noise for which spike times do not carry information. Thus, the question arises whether we could implement the same computations equally efficiently with stochastically generated spikes? In particular, if we consider biological networks with thousands of neurons, averaging responses from large populations of neurons might render the contribution of each spike unimportant. In this case, spike-based and rate-based approaches might become equivalent. In the following, we show that this is not the case. A deterministic spike generation rule is crucial for efficient information transfer even in very large networks.</p>
<p>To show this, we started by implementing a version of the probabilistic population code of Ma et al. <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>. These authors have shown that optimal integration of information from two population codes reduces to a linear combination of their neural activities. In the context of temporal sensory integration, the predicted output firing rates, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e216" xlink:type="simple"/></inline-formula>, correspond to the cumulative spike counts <xref ref-type="bibr" rid="pcbi.1001080-Beck1">[7]</xref>. Thus, the output firing rates are given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e217" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e218" xlink:type="simple"/></inline-formula> represents the gain of the output neurons. As in our model, the output rates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e219" xlink:type="simple"/></inline-formula> represent the stimulus posterior distribution optimally and on-line. In particular, activities increase linearly over time to account for the accumulation of sensory evidence. In order to avoid saturations of neural activities, Beck et al. <xref ref-type="bibr" rid="pcbi.1001080-Beck1">[7]</xref> proposed a form of on-line normalization, effectively using a time varying gain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e220" xlink:type="simple"/></inline-formula>. This does not change the main conclusion of this section. For the sake of simplicity, we consider <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e221" xlink:type="simple"/></inline-formula> to be constant.</p>
<p>We now examine the consequence of firing spikes stochastically with rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e222" xlink:type="simple"/></inline-formula> rather than representing this accumulated evidence deterministically. We measured the performance of the stochastic network with the on-line decoder described in equation (3) and using the optimal output kernels <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e223" xlink:type="simple"/></inline-formula>.</p>
<p><xref ref-type="fig" rid="pcbi-1001080-g008">Figure 8B</xref> depicts the performance of the stochastic network for different values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e224" xlink:type="simple"/></inline-formula>. The stochastic network behaves qualitatively like an ideal observer, i.e. it accumulates evidence and its error decreases over time. Moreover, for large gains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e225" xlink:type="simple"/></inline-formula> and long integration times, the performance of the stochastic network approaches the performance of an ideal observer of the sensory input (i.e. about 10% above optimality for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e226" xlink:type="simple"/></inline-formula>). However, for shorter sensory integration periods (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e227" xlink:type="simple"/></inline-formula>500 ms), the performance is poor even for large gains. Moreover, the output gain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e228" xlink:type="simple"/></inline-formula> has to be much larger than one. This implies that the stochastic network requires many more output spikes than input spikes (about 15 times more in this example) in order to avoid destructive information losses between the input layers and the output layer. By contrast, our network fires half as many output spikes than input spikes. We found that we could even lower that amount to 5 times less spikes in the output layer than in the input layers by increasing the size of the output kernels without any significant degradation in network performance.</p>
<p>A neural system clearly cannot afford to spend 15 times more resources at each processing stage. Moreover, this cost of stochastic spike generation does not decrease with the size of the input and output neural populations. In the limit of large numbers of neurons/spikes, the variance of the stochastic network estimate approaches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e229" xlink:type="simple"/></inline-formula>, where the Cramer-Rao bound <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e230" xlink:type="simple"/></inline-formula> is the variance of an optimal estimator (see methods). Efficient information transfer can only be achieved at the cost of large values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e231" xlink:type="simple"/></inline-formula>, i.e. many more output spikes than input spikes.</p>
</sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>In this article, we have revisited population coding with spiking neurons in the context of dynamic stimuli. Starting from first principles, we have demonstrated that networks of laterally coupled integrate-and-fire neurons can integrate and combine sensory information about a dynamic stimulus in close approximation to an ideal observer. In the absence of sensory input, these networks either represent the stimulus prior probability in their spontaneous activity before stimulus onset or they represent a working memory of the inferred stimulus posterior in their sustained activity after integration. These memories thereby keep tracking the underlying stimulus dynamics.</p>
<p>An important innovation of our model is that it encodes working memories representing an entire stimulus distribution rather than only a single stimulus value. It thereby distinguishes itself from other working memory models in the literature. Most working memory models are bi-stable attractor models <xref ref-type="bibr" rid="pcbi.1001080-BenYishai1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Compte2">[30]</xref> in which the sustained activity settles to a stable pattern independently of integration time or stimulus contrast. It is clear that such a stereotyped activity profile can only code for the most likely stimulus. Information about the uncertainty associated with the stimulus is lost. In contrast, our model is not based on bi-stability or line attractor dynamics but on an integration of past sensory evidence. In the presence of diffusion (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e232" xlink:type="simple"/></inline-formula>), the only stable state is the quiescent state, which corresponds to a flat probability distribution. In the absence of diffusion, the network maintains any pattern of activity that is evoked by past sensory stimulation. However, sensory stimuli in the real world are never “truly” stable. Moreover, any form of stochasticity in neural processing will result in a slow but constant accumulation of errors (see for instance the progressive decrease in performance due to synaptic background noise in <xref ref-type="fig" rid="pcbi-1001080-g008">figure 8A</xref>). Both of these properties will lead to working memories that are not completely stable, but eventually relax towards a quiescent state, i.e. a flat posterior distribution. In agreement with this prediction, the precision of a working memory for static stimuli degrades with the duration of the delay <xref ref-type="bibr" rid="pcbi.1001080-Ploner1">[42]</xref>.</p>
<p>We propose that cortical neurons are primarily predictive encoders rather than stochastic spike generators. Integrate-and-fire dynamics as well as a competition between neurons only allows the generation of spikes that contain new information about the stimulus, i.e. information that has not yet been signaled by the neural population. Each spike therefore carries a precise meaning. As a consequence of the above mentioned properties, small networks of only tens of neurons can encode stable memories. Persistent, asynchronous memory states are notoriously difficult to achieve with small networks of integrate-and-fire neurons. Our model on the other hand is largely free from laborious fine tuning. It provides a functional interpretation of parameters such as lateral connections and synaptic dynamics, and could be used as a guideline to find optimal parameters in biophysically plausible networks. For instance, the slow currents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e233" xlink:type="simple"/></inline-formula> in our framework might be mediated by a combination of slow excitatory NMDA synapses and slow inhibitory <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e234" xlink:type="simple"/></inline-formula> synapses. NMDA synapses have been identified by previous studies as a potential requirement for robust working memory responses <xref ref-type="bibr" rid="pcbi.1001080-Compte2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Wang1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Seung2">[40]</xref>.</p>
<p>In our framework, prior beliefs correspond to setting the network into an initial state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e235" xlink:type="simple"/></inline-formula>. As an example, we proposed an implementation of a sustained pattern of baseline activity, equivalent to a working memory for an input provided before the start of the trial. Similar mechanisms for implementing priors using external inputs have been suggested in other theoretical studies <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>. This would predict that baseline firing rates are modulated by prior assumptions of a subject, for example by stimuli experienced in the recent past. However, “long-term” prior beliefs could also be implemented by the choice of output kernels. Thus, the density of preferred stimuli in the neural population could be chosen non-uniformly and such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e236" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001080-Simoncelli1">[44]</xref>. In this case, the prior would be represented by all neurons firing at a constant, low baseline firing rate. This predicts no structure in the baseline response prior to stimulus presentation, and no direct influence of the prior on the tuning curves of individual neurons. In support of such a mechanism, perceptual learning causes an increase in neural representation for more frequently experienced stimuli <xref ref-type="bibr" rid="pcbi.1001080-Tanaka1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Ohl1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Feldman1">[47]</xref>.</p>
<p>Another important aspect of our approach concerns its interpretation of neural variability. Traditional population coding approaches clearly separate “signal”, encoded in rate modulation, and “noise”, encoded in the spike count variance. Rate models, such as linear-nonlinear Poisson (LNP) neurons <xref ref-type="bibr" rid="pcbi.1001080-Pillow1">[48]</xref>, rely on stochastic spike generation for generating realistic spike trains. Individual spike times do not carry any meaning while spike train variability is interpreted as noise. A problem arises when such rate units are used to perform sensory integration. In this case, while output units can compensate for the neural noise by integrating information over cues and time, they “throw away” part of this information by firing spikes stochastically. Thus, Lochmann et al. <xref ref-type="bibr" rid="pcbi.1001080-Lochmann1">[49]</xref> have shown previously that stochastic firing strongly degrades the information transfer capacity of single neurons that represent a time varying binary stimulus. Here we show that this is also the case for continuous stimuli, except if the neural system is willing to largely increase the amount of resources (i.e. spikes, neurotransmitters) it devotes to each sensory variable.</p>
<p>Our approach provides an alternative account for the origin of neural variability observed in cortical networks. Stochastic firing is not a good description of noise in single neurons <xref ref-type="bibr" rid="pcbi.1001080-Mainen1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Reinagel1">[51]</xref>. Instead, it has been proposed that this variability originates in chaotic dynamics of recurrent networks of integrate-and-fire neurons with balanced excitation and inhibition <xref ref-type="bibr" rid="pcbi.1001080-vanVreeswijk1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Sompolinsky1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Banerjee1">[52]</xref>. This perfectly agrees with our findings since our network shows characteristics of a chaotic system in the absence of sensory input. However, we show that these dynamics cannot be equated to noise. They only reflect the fact that multiple deterministic trajectories (i.e. spike patterns) encode the same information (<xref ref-type="fig" rid="pcbi-1001080-g007">figure 7</xref>). Albeit chaotic, this network can conserve and transmit information perfectly. At the same time, the network is self-correcting and robust to types of noise that have been reported in cortical neurons, such as spike generation noise or synaptic noise <xref ref-type="bibr" rid="pcbi.1001080-Faisal1">[37]</xref>,<xref ref-type="bibr" rid="pcbi.1001080-Steriade1">[38]</xref>.</p>
<p>It might appear paradoxical to assume input neurons corrupted by Poisson noise while using perfectly deterministic output neurons. However, input noise in our model is meant to represent unavoidable sources of sensory noise, such as the stochasticity of our sensors in the first signal transduction stages (e.g. thermodynamical/quantum mechanical noise in the photoreceptors). This initial noise sets a bound on how much information is available for further processing stages. We used population codes with independent Poisson noise as inputs for the sake of convenience and because such variability is expected as a consequence of predictive coding. However, the same networks can process any noisy inputs whose log-likelihoods can be computed on-line. Our preliminary findings suggest indeed that our model can construct population codes with Poisson-like firing statistics for almost any type of noisy sensory input, including input that is not Poisson, not spiking or not a population code. Consequently, Poisson distributed input in our model does not represent noise in the input neurons but the outcome of previous optimal neural processing of the sensory input.</p>
<p>Our hypothesis can be tested experimentally in cases where one is able to record simultaneously from a significant portion of the population. Since our model assumes a strong level of inter-connectivity and shared input, a population could correspond to a local, relatively small network such as a micro-column, rather than a large and diffuse network containing millions of neurons. Our model predicts that the larger the simultaneously recorded population, the better one can predict individual spike times, using methods described in section “Output spike train statistics”. On the behavioral level, our model predicts that humans should be able to memorize entire probability distributions. This could be tested by a simple cue combination experiment, in which two cues about a stimulus (e.g. a visual and an auditory cue about the location of an object) are presented with a temporal delay. If subjects keep track of the uncertainty associated with the first cue, they should still behave like optimal Bayesian observers when combining information from the two cues after the delay period.</p>
<p>We are not the first authors to propose a spiking network for optimal cue combination and sensory integration. Ma et al. <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref> implemented probabilistic population codes for cue combination, and more recently for temporal integration of evidence in a motion integration tasks <xref ref-type="bibr" rid="pcbi.1001080-Beck1">[7]</xref> with either conductance-based integrate-and-fire neurons or stochastic LNP neurons. However, their theoretical approach is based on firing rates, and the simulated spiking networks are used to show that the sums of spike counts predicted by an ideal observer can also be implemented by spiking neurons. The authors show that the output layer behaves as an ideal observer when comparing uni-modal with bimodal cue combination or when observing how quickly information accumulates over time. However, they concentrate solely on the information contained in the output layer for the different conditions: unimodal versus bimodal or high versus low levels of sensory noise. They do not measure the performance of the spiking network in terms of how much information is conserved or lost in the transfer from input to output spike trains. Our results suggest that while their approach is indeed optimal if outputs are analog firing rates, it becomes suboptimal when translated into noisy spike trains (except if there are many more output spikes than input spikes). In contrast, our model can be used to implement a probabilistic population coding framework directly with spikes rather than with rates.</p>
<p>Other authors have considered log probability codes <xref ref-type="bibr" rid="pcbi.1001080-Rao1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Rao2">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Deneve1">[8]</xref>. For example, Rao <xref ref-type="bibr" rid="pcbi.1001080-Rao2">[53]</xref> proposed a network of integrate-and-fire neurons performing approximate Bayesian inference. Similar to our model, the membrane potentials were interpreted as log posteriors. However, this model encoded posterior probabilities in terms of instantaneous firing rates rather than considering spikes as deterministic prediction errors.</p>
<p>Our approach is similar to the “spiking Boltzmann machine” proposed by Hinton and Brown <xref ref-type="bibr" rid="pcbi.1001080-Hinton1">[21]</xref>. This model, however, performed approximate and not exact inference, and did not provide an explicit, local spike generation rule. Another related approach, termed fast population coding (FPC) <xref ref-type="bibr" rid="pcbi.1001080-Huys1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1001080-Natarajan1">[18]</xref>, was applied to more general stimulus dynamics described by Gaussian processes. This model is particularly relevant for very sparse input (few input spikes) and functions by adding more output spikes, hence rendering linear decoding easier. However its spike generation rule (using KL divergence) is non-local, requiring supervised learning of the lateral connections in order to approximate it. In contrast, our model works with a local spike generation rule, essentially compressing the code, but is optimal only for Markovian dynamics.</p>
<p>We assumed that output neurons “know” the parameters of the input noise and stimulus dynamics. Sensory noise, stimulus drift and diffusion are hard-wired in the weights of feed-forward and lateral connections. For the sake of simplicity, we considered simple stimulus dynamics with a constant drift <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e237" xlink:type="simple"/></inline-formula> and diffusion <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e238" xlink:type="simple"/></inline-formula>. However, our approach can be extended in a straightforward way to state dependent drifts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e239" xlink:type="simple"/></inline-formula> and diffusions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e240" xlink:type="simple"/></inline-formula>. We have seen that the input and output kernels can be learnt from the input and output tuning curves and covariance matrices. Thus, “slow” lateral connections predicting drifts and diffusions could be learnt using Hebbian-learning rules. However, a given network is designed for a specific set of stimulus parameters. Ideally, we would want output neurons to estimate these parameters online during the presentation of a stimulus, for example if the stimulus speed changes suddenly. This could be implemented by multi-dimensional networks representing dynamical parameters <xref ref-type="bibr" rid="pcbi.1001080-Denve1">[54]</xref>. Thus, the state variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e241" xlink:type="simple"/></inline-formula> could contain additional dimensions for velocity, acceleration, force, etc. The capacity of such networks to track their stimulus would only be limited by combinatorial explosions as more stimulus dimensions need to be represented.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Ideal observer</title>
<p>Here we derive an expression for the ideal observer of the log posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e242" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e243" xlink:type="simple"/></inline-formula> denotes the spike trains of the input neurons in population <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e244" xlink:type="simple"/></inline-formula> in response to dynamic stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e245" xlink:type="simple"/></inline-formula>. The ideal observer integrates the inputs from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e246" xlink:type="simple"/></inline-formula> populations that represent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e247" xlink:type="simple"/></inline-formula> different cues about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e248" xlink:type="simple"/></inline-formula>.</p>
<p>The total response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e249" xlink:type="simple"/></inline-formula> can be divided into the response at the current time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e250" xlink:type="simple"/></inline-formula> and the response history <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e251" xlink:type="simple"/></inline-formula>. The population response at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e252" xlink:type="simple"/></inline-formula> is a binary vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e253" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e254" xlink:type="simple"/></inline-formula> if an input neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e255" xlink:type="simple"/></inline-formula> fired a spike at time t and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e256" xlink:type="simple"/></inline-formula> otherwise.</p>
<p>We can use Bayes' rule to write the conditional probability of the stimulus given the past history of activity patterns,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e257" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>This equation expresses the current posterior stimulus probability as a spatially averaged version of the past stimulus probability, weighted by the current response probabilities and properly normalized by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e258" xlink:type="simple"/></inline-formula>. We have assumed that the response likelihoods are independent among input populations and only depend on the current stimulus location. We can turn the multiplications in equation (12) into sums by passing to the log domain,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e259" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p>The normalization term, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e260" xlink:type="simple"/></inline-formula>, corresponds to an additive constant that does not change the shape and therefore the information content of the log posterior. We will therefore neglect this term in what follows.</p>
<p>The response likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e261" xlink:type="simple"/></inline-formula> is assumed to belong to the exponential family with linear sufficient statistics, i.e. the firing probability of a neuron in a small time window <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e262" xlink:type="simple"/></inline-formula> can be written as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e263" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e264" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e265" xlink:type="simple"/></inline-formula> are arbitrary functions and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e266" xlink:type="simple"/></inline-formula> is a kernel that is related to the neurons' tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e267" xlink:type="simple"/></inline-formula> and their spike count covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e268" xlink:type="simple"/></inline-formula> through the relation <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e269" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e270" xlink:type="simple"/></inline-formula> denotes the derivative with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e271" xlink:type="simple"/></inline-formula>. We can then write the likelihood in its log form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e272" xlink:type="simple"/><label>(15)</label></disp-formula>Equation (15) takes a particularly easy form if we consider independent Poisson processes. In this case we find that the kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e273" xlink:type="simple"/></inline-formula> is linked to the logarithm of the tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e274" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e275" xlink:type="simple"/></inline-formula> and a bias term is given by the sum of tuning curves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e276" xlink:type="simple"/></inline-formula>. The term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e277" xlink:type="simple"/></inline-formula> acts as a normalization term and is neglected.</p>
<p>Let us now move to the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e278" xlink:type="simple"/></inline-formula>. The factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e279" xlink:type="simple"/></inline-formula> represents the probability that the stimulus moves from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e280" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e281" xlink:type="simple"/></inline-formula> in the small time interval dt. This probability is independent of the starting position, such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e282" xlink:type="simple"/></inline-formula>. This turns the term of interest into a convolution that we can expand and express as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e283" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e284" xlink:type="simple"/></inline-formula> denotes the probability that the stimulus moves by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e285" xlink:type="simple"/></inline-formula> in time interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e286" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e287" xlink:type="simple"/></inline-formula> is a probability density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e288" xlink:type="simple"/></inline-formula>. If we assume the stimulus to follow the drift-diffusion dynamics from equation (1), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e289" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e290" xlink:type="simple"/></inline-formula> is a Wiener process, we can express the remaining sums in equation (16) as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e291" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e292" xlink:type="simple"/></inline-formula>. Using these identities together with equation (16) and taking the log we find<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e293" xlink:type="simple"/><label>(17)</label></disp-formula>where we have Taylor expanded the log to first order. It can easily be verified that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e294" xlink:type="simple"/><label>(18)</label></disp-formula>We can use these identities and combine equations (13), (15) and (17) to find the temporal evolution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e295" xlink:type="simple"/></inline-formula> in the continuous limit <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e296" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e297" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e298" xlink:type="simple"/></inline-formula> denotes input spike trains with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e299" xlink:type="simple"/></inline-formula> the k<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e300" xlink:type="simple"/></inline-formula> spike of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e301" xlink:type="simple"/></inline-formula> in population <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e302" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b">
<title>Neural approximation to the ideal observer</title>
<p>Here we derive an approximation to the ideal observer that is implemented by the leaky integrate-and-fire neurons in the output population described in equations (7) and (8) of the main text.</p>
<p>We first introduce a discretization of the stimulus space given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e303" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e304" xlink:type="simple"/></inline-formula> corresponds to the preferred stimulus of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e305" xlink:type="simple"/></inline-formula>. Each neuron therefore codes for the value of the log posterior distribution at its preferred stimulus, which we denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e306" xlink:type="simple"/></inline-formula>. We want the output spike trains to encode a distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e307" xlink:type="simple"/></inline-formula> that closely approximates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e308" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e309" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e310" xlink:type="simple"/></inline-formula>. Additionally, following equation (3) the dynamics of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e311" xlink:type="simple"/></inline-formula> are given as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e312" xlink:type="simple"/><label>(20)</label></disp-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e313" xlink:type="simple"/></inline-formula> denotes a positive leak term and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e314" xlink:type="simple"/></inline-formula> is a freely chosen weighting kernel.</p>
<p>When inferring the input log posterior, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e315" xlink:type="simple"/></inline-formula>, in a neural system, one cannot simply use equation (19) because individual neurons do not have direct access to the spatial derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e316" xlink:type="simple"/></inline-formula>. However, if we choose a spike generation mechanism which ensures that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e317" xlink:type="simple"/></inline-formula> at all times, we can use the recurrent spikes to approximate the spatial derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e318" xlink:type="simple"/></inline-formula> and rewrite equation (19) in a discretized form as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e319" xlink:type="simple"/><label>(21)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e320" xlink:type="simple"/></inline-formula> denotes the input to neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e321" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e322" xlink:type="simple"/></inline-formula>. Notice that we have introduced a linear leak <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e323" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e324" xlink:type="simple"/></inline-formula> and compensated for it by adding a corresponding fraction of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e325" xlink:type="simple"/></inline-formula>.</p>
<p>We now define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e326" xlink:type="simple"/></inline-formula>. To find the time evolution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e327" xlink:type="simple"/></inline-formula> we need to calculate the time derivative of the spatial derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e328" xlink:type="simple"/></inline-formula>. Using equation (20) we get<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e329" xlink:type="simple"/><label>(22)</label></disp-formula></p>
<p>A similar equation is found for the second spatial derivative of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e330" xlink:type="simple"/></inline-formula>. Combining these equations with the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e331" xlink:type="simple"/></inline-formula> and denoting the spatial derivative with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e332" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e333" xlink:type="simple"/></inline-formula> we get<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e334" xlink:type="simple"/><label>(23)</label></disp-formula></p>
<p>Similarly we define <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e335" xlink:type="simple"/></inline-formula> so that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e336" xlink:type="simple"/><label>(24)</label></disp-formula></p>
<p>Finally, we can write our approximation to the ideal observer as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e337" xlink:type="simple"/><label>(25)</label></disp-formula></p>
<p>For this approximation to work, it is crucial that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e338" xlink:type="simple"/></inline-formula>. To ensure this condition to hold, we look at the squared distance between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e339" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e340" xlink:type="simple"/></inline-formula> and only let those neurons fire a spike, which add a kernel to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e341" xlink:type="simple"/></inline-formula> that moves it closer to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e342" xlink:type="simple"/></inline-formula>. Mathematically this means that a spike is fired if<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e343" xlink:type="simple"/><label>(26)</label></disp-formula></p>
<p>We can develop the squares in equation (26) to rewrite the spiking criterion as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e344" xlink:type="simple"/><label>(27)</label></disp-formula></p>
<p>We define the left hand side of this equation as the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e345" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e346" xlink:type="simple"/></inline-formula>. The temporal evolution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e347" xlink:type="simple"/></inline-formula> below threshold can be obtained by combining equations (25), (23), (24) and (20) with the left hand side of equation (27). It is then straight forward to find the final result<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e348" xlink:type="simple"/><label>(28)</label></disp-formula>where neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e349" xlink:type="simple"/></inline-formula> fires a spike if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e350" xlink:type="simple"/></inline-formula>, with threshold <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e351" xlink:type="simple"/></inline-formula>. After firing a spike <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e352" xlink:type="simple"/></inline-formula> is reset to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e353" xlink:type="simple"/></inline-formula>.</p>
<p>The dynamics of the slow currents <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e354" xlink:type="simple"/></inline-formula> are given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e355" xlink:type="simple"/><label>(29)</label></disp-formula>with weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e356" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e357" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c">
<title>Decoding</title>
<p>Decoding in our model reduces to a simple leaky integration of output spikes according to equation (3) of the main text. We can either assume that kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e358" xlink:type="simple"/></inline-formula> is known a-priori or we can learn it from the output tuning curves, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e359" xlink:type="simple"/></inline-formula>, and covariance matrix, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e360" xlink:type="simple"/></inline-formula> using the relation <xref ref-type="bibr" rid="pcbi.1001080-Ma1">[6]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e361" xlink:type="simple"/><label>(30)</label></disp-formula></p>
<p>The two methods give virtually identical results. All results reported in this paper use learnt kernels.</p>
<p>On every trial, we measure the mean and variance of the posterior that we decode from the output spike patterns. The estimator of the stimulus mean, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e362" xlink:type="simple"/></inline-formula> is its expected value: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e363" xlink:type="simple"/></inline-formula>. Its variance, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e364" xlink:type="simple"/></inline-formula> is computed as the second mode of the output posterior, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e365" xlink:type="simple"/></inline-formula>.</p>
<p>We measure coding accuracy over many trials as the variance, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e366" xlink:type="simple"/></inline-formula>, of the stimulus mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e367" xlink:type="simple"/></inline-formula> around the real value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e368" xlink:type="simple"/></inline-formula>. Notice, that variance of the estimator should equal the posterior variances averaged over many trials, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e369" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e370" xlink:type="simple"/></inline-formula> denotes average over trials. For simplicity, we only report the performance measured by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e371" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4d">
<title>Measuring predictability</title>
<p>We will use an indirect measure to assess the predictability of the response of a neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e372" xlink:type="simple"/></inline-formula> conditioned on the spike trains recorded from a subpopulation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e373" xlink:type="simple"/></inline-formula> neurons. Let us define the “predicted membrane potential” <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e374" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e375" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e376" xlink:type="simple"/><label>(31)</label></disp-formula>where the sum runs over all recorded neurons and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e377" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e378" xlink:type="simple"/><label>(32)</label></disp-formula></p>
<p>The predicted membrane potential depicts the total external “driving force” that neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e379" xlink:type="simple"/></inline-formula> receives from the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e380" xlink:type="simple"/></inline-formula> other neurons in the subpopulation. Neurons are generally strongly driven by external input right before they spike. Thus, a high predicted membrane potential and hence a high driving force is an indicator for an enhanced firing probability. We use this intuition to define the predictability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e381" xlink:type="simple"/></inline-formula>, of the activity of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e382" xlink:type="simple"/></inline-formula> on a given trial as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e383" xlink:type="simple"/><label>(33)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e384" xlink:type="simple"/></inline-formula> is the standard deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e385" xlink:type="simple"/></inline-formula> over the entire duration of the trial and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e386" xlink:type="simple"/></inline-formula> denotes a shuffled version of spike train <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e387" xlink:type="simple"/></inline-formula>. Thus, the predictability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e388" xlink:type="simple"/></inline-formula> measures the difference between the spike-triggered predicted membrane potentials computed from the recorded spike train and a random spike train with the same number of spikes. Normalizing by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e389" xlink:type="simple"/></inline-formula> turns <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e390" xlink:type="simple"/></inline-formula> into something like a signal-to-noise ratio.</p>
</sec><sec id="s4e">
<title>Encoding accuracy of the stochastic network</title>
<p>Here we derive an expression for the accuracy with which the stochastic network of section “Comparison to a rate model” can encode the underlying stimulus. The encoding accuracy of this network is limited by two factors: the initial accuracy with which the stimulus is encoded in the input populations and the additional uncertainty that stochastic spike generation adds on top of it.</p>
<p>The input accuracy is determined by the Cramer-Rao bound, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e391" xlink:type="simple"/></inline-formula>, which corresponds to the variance of an optimal estimator. It is related to the Fisher information in the inputs. For the case of uniformly arrayed tuning curves and Poisson firing statistics (as is the case for the input populations), Fisher information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e392" xlink:type="simple"/></inline-formula>, after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e393" xlink:type="simple"/></inline-formula> seconds of integration, can be calculated as <xref ref-type="bibr" rid="pcbi.1001080-Brunel1">[55]</xref>: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e394" xlink:type="simple"/></inline-formula>. The Cramer-Rao bound is then given as the inverse of Fisher information, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e395" xlink:type="simple"/></inline-formula>.</p>
<p>The output neurons in the stochastic network fire Poisson spikes from a rate, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e396" xlink:type="simple"/></inline-formula>, that corresponds to the sum of input spike counts scaled by gain factor K:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e397" xlink:type="simple"/><label>(34)</label></disp-formula></p>
<p>This corresponds to a mean rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e398" xlink:type="simple"/></inline-formula>. It is obvious, that an optimal estimator of the Poisson spike trains generated from these rates would have a variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e399" xlink:type="simple"/></inline-formula>.</p>
<p>The noise in input and output spike generation is independent from each other. The variances of input and output estimators therefore add up and we find that the accuracy of an optimal observer of the stochastic output spike trains is given as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e400" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4f">
<title>Simulation details</title>
<p>The network structure is outlined in <xref ref-type="fig" rid="pcbi-1001080-g001">figure 1B</xref>. Each neural layer contains <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e401" xlink:type="simple"/></inline-formula> neurons. Input tuning curves are circular Gaussians. For neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e402" xlink:type="simple"/></inline-formula> it would take the form <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e403" xlink:type="simple"/></inline-formula> where the preferred direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e404" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e405" xlink:type="simple"/></inline-formula>. We use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e406" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e407" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e408" xlink:type="simple"/></inline-formula> for the visual input and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e409" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e410" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e411" xlink:type="simple"/></inline-formula> for the auditory input population. The only exception is the simulation of the stochastic network (<xref ref-type="fig" rid="pcbi-1001080-g008">figure 8B</xref>), where we use identical tuning curves in the two inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e412" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e413" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e414" xlink:type="simple"/></inline-formula>.</p>
<p>The input kernels are given by the log tuning curves: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e415" xlink:type="simple"/></inline-formula>. Since we are interested in the log posterior up to an additive constant only, we are free to add or subtract a constant from the kernels. We therefore shift the input kernels, such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e416" xlink:type="simple"/></inline-formula>. In this way, each input spike adds on average zero to the log posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e417" xlink:type="simple"/></inline-formula>. A direct consequence of this shift is that the bias term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e418" xlink:type="simple"/></inline-formula> (see eq. 7) equals zero and hence disappears.</p>
<p>The output kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e419" xlink:type="simple"/></inline-formula> is also chosen to be a circular Gaussian with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e420" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e421" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e422" xlink:type="simple"/></inline-formula>. For <xref ref-type="fig" rid="pcbi-1001080-g003">figures 3E</xref> and <xref ref-type="fig" rid="pcbi-1001080-g004">4B–4E</xref> we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e423" xlink:type="simple"/></inline-formula> whereas all other parameters remained the same. In accordance with the input kernels, the baseline of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e424" xlink:type="simple"/></inline-formula> is set such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e425" xlink:type="simple"/></inline-formula>.</p>
<p>Parameters for the stimulus dynamics are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e426" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e427" xlink:type="simple"/></inline-formula>. These full dynamics are used in <xref ref-type="fig" rid="pcbi-1001080-g002">figure 2</xref> and <xref ref-type="fig" rid="pcbi-1001080-g003">3</xref>. <xref ref-type="fig" rid="pcbi-1001080-g005">Figure 5</xref> only uses the diffusion and the other figures use static stimuli. The neural leak is set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e428" xlink:type="simple"/></inline-formula>.</p>
<p>In order to change the reliability of the input cues (for the simulation in <xref ref-type="fig" rid="pcbi-1001080-g004">figures 4A</xref> and <xref ref-type="fig" rid="pcbi-1001080-g005">5C</xref>), we multiply the tuning curve of the input neurons in a population by a constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e429" xlink:type="simple"/></inline-formula>. This changes the Fisher information contained in this population by the multiplicative factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e430" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e431" xlink:type="simple"/></inline-formula>. The Cramer-Rao bound of an optimal estimator is therefore divided by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e432" xlink:type="simple"/></inline-formula>. Notice that the input kernels and therefore the feed-forward weights remain unchanged by this operation.</p>
<p>To test the robustness of our network to noise, we add a Gaussian white noise term to the membrane potential: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e433" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e434" xlink:type="simple"/></inline-formula> denotes the spiking input to neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e435" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e436" xlink:type="simple"/></inline-formula> is a white noise term with unit variance, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e437" xlink:type="simple"/></inline-formula>. Our simulations are done with noise strengths of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e438" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e439" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e440" xlink:type="simple"/></inline-formula>.</p>
<p>The differential equations of the membrane potentials are integrated using an Euler method with time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e441" xlink:type="simple"/></inline-formula>. As neighboring output neurons get highly similar input, it is often the case that various neurons cross their spiking threshold in the same time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e442" xlink:type="simple"/></inline-formula>. If this happens, we determine which neuron would cross the threshold first assuming a linear voltage increase during the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e443" xlink:type="simple"/></inline-formula>. We then let this neuron spike and reset its neighbors. Should there still be a neuron above threshold after this reset, we let it spike as well and so forth until no more neuron is above threshold. We then continue to the next integration step. In most cases however, only one neuron will spike per time interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001080.e444" xlink:type="simple"/></inline-formula>.</p>
</sec></sec></body>
<back>
<ack>
<p>We thank Brian Fischer for his constructive comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1001080-Ernst1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name>
<name name-style="western"><surname>Banks</surname><given-names>MS</given-names></name>
</person-group>             <year>2002</year>             <article-title>Humans integrate visual and haptic information in a statistically optimal fashion.</article-title>             <source>Nature</source>             <volume>415</volume>             <fpage>429</fpage>             <lpage>433</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Kording1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kording</surname><given-names>KP</given-names></name>
<name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>
</person-group>             <year>2004</year>             <article-title>Bayesian integration in sensorimotor learning.</article-title>             <source>Nature</source>             <volume>427</volume>             <fpage>244</fpage>             <lpage>247</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Knill1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="editor">
<name name-style="western"><surname>Knill</surname><given-names>DC</given-names></name>
<name name-style="western"><surname>Richards</surname><given-names>W</given-names></name>
</person-group>             <year>1996</year>             <source>Perception as Bayesian inference</source>             <publisher-loc>New York, NY, USA</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001080-Todorov1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Todorov</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>
</person-group>             <year>2002</year>             <article-title>Optimal feedback control as a theory of motor coordination.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <fpage>1226</fpage>             <lpage>1235</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Zemel1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zemel</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>1998</year>             <article-title>Probabilistic interpretation of population codes.</article-title>             <source>Neural Comput</source>             <volume>10</volume>             <fpage>403</fpage>             <lpage>430</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Ma1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name>
<name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>2006</year>             <article-title>Bayesian inference with probabilistic population codes.</article-title>             <source>Nat Neurosci</source>             <volume>9</volume>             <fpage>1432</fpage>             <lpage>1438</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Beck1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name>
<name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Hanks</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Churchland</surname><given-names>AK</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Probabilistic population codes for bayesian decision making.</article-title>             <source>Neuron</source>             <volume>60</volume>             <fpage>1142</fpage>             <lpage>1152</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Deneve1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name>
</person-group>             <year>2008</year>             <article-title>Bayesian spiking neurons i: inference.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>91</fpage>             <lpage>117</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Rao1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name>
</person-group>             <year>2004</year>             <article-title>Bayesian computation in recurrent neural circuits.</article-title>             <source>Neural Comput</source>             <volume>16</volume>             <fpage>1</fpage>             <lpage>38</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Eliasmith1"><label>10</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eliasmith</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Anderson</surname><given-names>CH</given-names></name>
</person-group>             <year>2003</year>             <source>Neural engineering: Computation, representation, and dynamics in neurobiological systems</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001080-Barlow1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name>
</person-group>             <year>1953</year>             <article-title>Summation and inhibition in the frog's retina.</article-title>             <source>J Physiol</source>             <volume>119</volume>             <fpage>69</fpage>             <lpage>88</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Georgopoulos1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Georgopoulos</surname><given-names>AP</given-names></name>
<name name-style="western"><surname>Kalaska</surname><given-names>JF</given-names></name>
<name name-style="western"><surname>Caminiti</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Massey</surname><given-names>JT</given-names></name>
</person-group>             <year>1982</year>             <article-title>On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex.</article-title>             <source>J Neurosci</source>             <volume>2</volume>             <fpage>1527</fpage>             <lpage>1537</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Shadlen1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>
<name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name>
</person-group>             <year>1994</year>             <article-title>Noise, neural codes and cortical organization.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>4</volume>             <fpage>569</fpage>             <lpage>579</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Tolhurst1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tolhurst</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name>
<name name-style="western"><surname>Dean</surname><given-names>AF</given-names></name>
</person-group>             <year>1983</year>             <article-title>The statistical reliability of signals in single neurons in cat and monkey visual cortex.</article-title>             <source>Vision Res</source>             <volume>23</volume>             <fpage>775</fpage>             <lpage>785</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Sahani1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2003</year>             <article-title>Doubly distributional population codes: simultaneous representation of uncertainty and multiplicity.</article-title>             <source>Neural Comput</source>             <volume>15</volume>             <fpage>2255</fpage>             <lpage>2279</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Jazayeri1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jazayeri</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name>
</person-group>             <year>2006</year>             <article-title>Optimal representation of sensory information by neural populations.</article-title>             <source>Nat Neuroscience</source>             <volume>9</volume>             <fpage>690</fpage>             <lpage>696</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Beck2"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Exact inferences in a neural implementation of a hidden markov model.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>1344</fpage>             <lpage>1361</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Natarajan1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Natarajan</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Huys</surname><given-names>QJM</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Zemel</surname><given-names>RS</given-names></name>
</person-group>             <year>2008</year>             <article-title>Encoding and decoding spikes for dynamic stimuli.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>2325</fpage>             <lpage>2360</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Seung1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
<name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name>
</person-group>             <year>1993</year>             <article-title>Simple models for reading neuronal population codes.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>90</volume>             <fpage>10749</fpage>             <lpage>10753</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Pouget1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Zemel</surname><given-names>R</given-names></name>
</person-group>             <year>2000</year>             <article-title>Information processing with population codes.</article-title>             <source>Nat Rev Neurosci</source>             <volume>1</volume>             <fpage>125</fpage>             <lpage>132</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Hinton1"><label>21</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>
<name name-style="western"><surname>Brown</surname><given-names>AD</given-names></name>
</person-group>             <year>1999</year>             <article-title>Spiking boltzmann machines.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>SA</surname><given-names>S</given-names></name>
<name name-style="western"><surname>TK</surname><given-names>L</given-names></name>
<name name-style="western"><surname>KR</surname><given-names>M</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems 12</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>122</fpage>             <lpage>128</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Wu1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wu</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Niranjan</surname><given-names>M</given-names></name>
<name name-style="western"><surname>ichi Amari</surname><given-names>S</given-names></name>
</person-group>             <year>2003</year>             <article-title>Sequential bayesian decoding with a population of neurons.</article-title>             <source>Neural Comput</source>             <volume>15</volume>             <fpage>993</fpage>             <lpage>1012</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Huys1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Huys</surname><given-names>QJM</given-names></name>
<name name-style="western"><surname>Zemel</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Natarajan</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2007</year>             <article-title>Fast population coding.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>404</fpage>             <lpage>441</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Gerwinn1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gerwinn</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Macke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>Bayesian population decoding of spiking neurons.</article-title>             <source>Front Comput Neurosci</source>             <volume>3</volume>             <fpage>21</fpage>          </element-citation></ref>
<ref id="pcbi.1001080-Deneve2"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>1999</year>             <article-title>Reading population codes: a neural implementation of ideal observers.</article-title>             <source>Nat Neurosci</source>             <volume>2</volume>             <fpage>740</fpage>             <lpage>745</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Rinberg1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rinberg</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Koulakov</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Gelperin</surname><given-names>A</given-names></name>
</person-group>             <year>2006</year>             <article-title>Speed-accuracy tradeoff in olfaction.</article-title>             <source>Neuron</source>             <volume>51</volume>             <fpage>351</fpage>             <lpage>358</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Shadlen2"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>
<name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name>
</person-group>             <year>2001</year>             <article-title>Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey.</article-title>             <source>J Neurophysiol</source>             <volume>86</volume>             <fpage>1916</fpage>             <lpage>1936</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Funahashi1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Funahashi</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Bruce</surname><given-names>CJ</given-names></name>
<name name-style="western"><surname>Goldman-Rakic</surname><given-names>PS</given-names></name>
</person-group>             <year>1989</year>             <article-title>Mnemonic coding of visual space in the monkey's dorsolateral prefrontal cortex.</article-title>             <source>J Neurophysiol</source>             <volume>61</volume>             <fpage>331</fpage>             <lpage>349</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Compte1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Compte</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Constantinidis</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Tegner</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Chafee</surname><given-names>MV</given-names></name>
<etal/></person-group>             <year>2003</year>             <article-title>Temporally irregular mnemonic persistent activity in prefrontal neurons of monkeys during a delayed response task.</article-title>             <source>J Neurophysiol</source>             <volume>90</volume>             <fpage>3441</fpage>             <lpage>3454</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Compte2"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Compte</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Goldman-Rakic</surname><given-names>PS</given-names></name>
<name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>
</person-group>             <year>2000</year>             <article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model.</article-title>             <source>Cereb Cortex</source>             <volume>10</volume>             <fpage>910</fpage>             <lpage>923</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-BenYishai1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ben-Yishai</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Bar-Or</surname><given-names>RL</given-names></name>
<name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name>
</person-group>             <year>1995</year>             <article-title>Theory of orientation tuning in visual cortex.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>92</volume>             <fpage>3844</fpage>             <lpage>3848</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Poirazi1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Poirazi</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Brannon</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Mel</surname><given-names>BW</given-names></name>
</person-group>             <year>2003</year>             <article-title>Pyramidal neuron as two-layer neural network.</article-title>             <source>Neuron</source>             <volume>37</volume>             <fpage>989</fpage>             <lpage>999</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Romo1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Romo</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name>
<name name-style="western"><surname>Hernndez</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Lemus</surname><given-names>L</given-names></name>
</person-group>             <year>1999</year>             <article-title>Neuronal correlates of parametric working memory in the prefrontal cortex.</article-title>             <source>Nature</source>             <volume>399</volume>             <fpage>470</fpage>             <lpage>473</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-London1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>London</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Roth</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Beeren</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Husser</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>
</person-group>             <year>2010</year>             <article-title>Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex.</article-title>             <source>Nature</source>             <volume>466</volume>             <fpage>123</fpage>             <lpage>127</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-vanVreeswijk1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>van Vreeswijk</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name>
</person-group>             <year>1998</year>             <article-title>Chaotic balanced state in a model of cortical circuits.</article-title>             <source>Neural Comput</source>             <volume>10</volume>             <fpage>1321</fpage>             <lpage>1371</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Sompolinsky1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Crisanti</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sommers</surname><given-names>HJ</given-names></name>
</person-group>             <year>1988</year>             <article-title>Chaos in random neural networks.</article-title>             <source>Phys Rev Lett</source>             <volume>61</volume>             <fpage>259</fpage>             <lpage>262</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Faisal1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Faisal</surname><given-names>AA</given-names></name>
<name name-style="western"><surname>Selen</surname><given-names>LPJ</given-names></name>
<name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>
</person-group>             <year>2008</year>             <article-title>Noise in the nervous system.</article-title>             <source>Nat Rev Neurosci</source>             <volume>9</volume>             <fpage>292</fpage>             <lpage>303</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Steriade1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Steriade</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Timofeev</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Grenier</surname><given-names>F</given-names></name>
</person-group>             <year>2001</year>             <article-title>Natural waking and sleep states: a view from inside neocortical neurons.</article-title>             <source>J Neurophysiol</source>             <volume>85</volume>             <fpage>1969</fpage>             <lpage>1985</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Shu1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shu</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Hasenstaub</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Badoual</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Bal</surname><given-names>T</given-names></name>
<name name-style="western"><surname>McCormick</surname><given-names>DA</given-names></name>
</person-group>             <year>2003</year>             <article-title>Barrages of synaptic activity control the gain and sensitivity of cortical neurons.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>10388</fpage>             <lpage>10401</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Seung2"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name>
<name name-style="western"><surname>Reis</surname><given-names>BY</given-names></name>
<name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name>
</person-group>             <year>2000</year>             <article-title>Stability of the memory of eye position in a recurrent network of conductance-based model neurons.</article-title>             <source>Neuron</source>             <volume>26</volume>             <fpage>259</fpage>             <lpage>271</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Koulakov1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Koulakov</surname><given-names>AA</given-names></name>
<name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Lisman</surname><given-names>JE</given-names></name>
</person-group>             <year>2002</year>             <article-title>Model for a robust neural integrator.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <fpage>775</fpage>             <lpage>782</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Ploner1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ploner</surname><given-names>CJ</given-names></name>
<name name-style="western"><surname>Gaymard</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Rivaud</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Agid</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Pierrot-Deseilligny</surname><given-names>C</given-names></name>
</person-group>             <year>1998</year>             <article-title>Temporal limits of spatial working memory in humans.</article-title>             <source>Eur J Neurosci</source>             <volume>10</volume>             <fpage>794</fpage>             <lpage>797</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Wang1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>
</person-group>             <year>1999</year>             <article-title>Synaptic basis of cortical persistent activity: the importance of nmda receptors to working memory.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>9587</fpage>             <lpage>9603</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Simoncelli1"><label>44</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
</person-group>             <year>2009</year>             <article-title>Optimal estimation in sensory systems.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Gazzaniga</surname><given-names>M</given-names></name>
</person-group>             <source>The Cognitive Neurosciences, IV</source>             <publisher-name>MIT Press</publisher-name>             <fpage>525</fpage>             <lpage>535</lpage>             <comment>chapter 36</comment>          </element-citation></ref>
<ref id="pcbi.1001080-Tanaka1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tanaka</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Ribot</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Imamura</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Tani</surname><given-names>T</given-names></name>
</person-group>             <year>2006</year>             <article-title>Orientation-restricted continuous visual exposure induces marked reorganization of orientation maps in early life.</article-title>             <source>Neuroimage</source>             <volume>30</volume>             <fpage>462</fpage>             <lpage>477</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Ohl1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ohl</surname><given-names>FW</given-names></name>
<name name-style="western"><surname>Scheich</surname><given-names>H</given-names></name>
</person-group>             <year>2005</year>             <article-title>Learning-induced plasticity in animal and human auditory cortex.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>15</volume>             <fpage>470</fpage>             <lpage>477</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Feldman1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name>
<name name-style="western"><surname>Brecht</surname><given-names>M</given-names></name>
</person-group>             <year>2005</year>             <article-title>Map plasticity in somatosensory cortex.</article-title>             <source>Science</source>             <volume>310</volume>             <fpage>810</fpage>             <lpage>815</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Pillow1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>
<name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Uzzell</surname><given-names>VJ</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>
<name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name>
</person-group>             <year>2005</year>             <article-title>Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>11003</fpage>             <lpage>11013</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Lochmann1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lochmann</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name>
</person-group>             <year>2008</year>             <article-title>Information transmission with spiking bayesian neurons.</article-title>             <source>New J Phys</source>             <volume>10</volume>             <fpage>055019 (19pp)</fpage>          </element-citation></ref>
<ref id="pcbi.1001080-Mainen1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mainen</surname><given-names>ZF</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1995</year>             <article-title>Reliability of spike timing in neocortical neurons.</article-title>             <source>Science</source>             <volume>268</volume>             <fpage>1503</fpage>             <lpage>1506</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Reinagel1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name>
</person-group>             <year>2000</year>             <article-title>Temporal coding of visual information in the thalamus.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>5392</fpage>             <lpage>5400</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Banerjee1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Banerjee</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Seris</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>2008</year>             <article-title>Dynamical constraints on using precise spike timing to compute in recurrent cortical networks.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>974</fpage>             <lpage>993</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Rao2"><label>53</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name>
</person-group>             <year>2005</year>             <article-title>Hierarchical bayesian inference in networks of spiking neurons.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Saul</surname><given-names>LK</given-names></name>
<name name-style="western"><surname>Weiss</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>
</person-group>             <source>Advances in Neural Information Processing Systems 17</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>1113</fpage>             <lpage>1120</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Denve1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Denve</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Duhamel</surname><given-names>JR</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Optimal sensorimotor integration in recurrent cortical networks: a neural implementation of kalman filters.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>5744</fpage>             <lpage>5756</lpage>          </element-citation></ref>
<ref id="pcbi.1001080-Brunel1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>
</person-group>             <year>1998</year>             <article-title>Mutual information, fisher information, and population coding.</article-title>             <source>Neural Comput</source>             <volume>10</volume>             <fpage>1731</fpage>             <lpage>1757</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>