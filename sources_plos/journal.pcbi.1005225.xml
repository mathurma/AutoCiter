<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005225</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00747</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Psychophysics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>An Extended Normalization Model of Attention Accounts for Feature-Based Attentional Enhancement of Both Response and Coherence Gain</article-title>
<alt-title alt-title-type="running-head">An Extended Attention Model</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7713-6745</contrib-id>
<name name-style="western">
<surname>Schwedhelm</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Krishna</surname>
<given-names>B. Suresh</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Treue</surname>
<given-names>Stefan</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Cognitive Neuroscience Laboratory, German Primate Center - Leibniz Institute for Primate Research, Goettingen, Germany</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Bernstein Center for Computational Neuroscience, Goettingen, Germany</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Faculty of Biology and Psychology, Goettingen University, Goettingen, Germany</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Leibniz-ScienceCampus Primate Cognition, Goettingen, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname>
<given-names>Matthias</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item><p><bold>Conceptualization:</bold> PS ST.</p></list-item> <list-item><p><bold>Data curation:</bold> PS.</p></list-item> <list-item><p><bold>Formal analysis:</bold> PS.</p></list-item> <list-item><p><bold>Funding acquisition:</bold> ST.</p></list-item> <list-item><p><bold>Investigation:</bold> PS.</p></list-item> <list-item><p><bold>Methodology:</bold> PS ST.</p></list-item> <list-item><p><bold>Project administration:</bold> ST.</p></list-item> <list-item><p><bold>Resources:</bold> ST.</p></list-item> <list-item><p><bold>Software:</bold> PS.</p></list-item> <list-item><p><bold>Supervision:</bold> BSK ST.</p></list-item> <list-item><p><bold>Validation:</bold> PS BSK.</p></list-item> <list-item><p><bold>Visualization:</bold> PS.</p></list-item> <list-item><p><bold>Writing – original draft:</bold> PS BSK.</p></list-item> <list-item><p><bold>Writing – review &amp; editing:</bold> PS BSK ST.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">pschwed@gwdg.de</email> (PS); <email xlink:type="simple">treue@gwdg.de</email> (ST)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>15</day>
<month>12</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>12</issue>
<elocation-id>e1005225</elocation-id>
<history>
<date date-type="received">
<day>9</day>
<month>5</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>10</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Schwedhelm et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005225"/>
<abstract>
<p>Paying attention to a sensory feature improves its perception and impairs that of others. Recent work has shown that a Normalization Model of Attention (NMoA) can account for a wide range of physiological findings and the influence of different attentional manipulations on visual performance. A key prediction of the NMoA is that attention to a visual feature like an orientation or a motion direction will increase the response of neurons preferring the attended feature (response gain) rather than increase the sensory input strength of the attended stimulus (input gain). This effect of feature-based attention on neuronal responses should translate to similar patterns of improvement in behavioral performance, with psychometric functions showing response gain rather than input gain when attention is directed to the task-relevant feature. In contrast, we report here that when human subjects are cued to attend to one of two motion directions in a transparent motion display, attentional effects manifest as a combination of input and response gain. Further, the impact on input gain is greater when attention is directed towards a narrow range of motion directions than when it is directed towards a broad range. These results are captured by an extended NMoA, which either includes a stimulus-independent attentional contribution to normalization or utilizes direction-tuned normalization. The proposed extensions are consistent with the feature-similarity gain model of attention and the attentional modulation in extrastriate area MT, where neuronal responses are enhanced and suppressed by attention to preferred and non-preferred motion directions respectively.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>We report a pattern of feature-based attentional effects on human psychophysical performance, which cannot be accounted for by the Normalization Model of Attention using biologically plausible parameters. Specifically, this prominent model of attentional modulation predicts that attention to a visual feature like a specific motion direction will lead to a response gain in the input-response function, rather than the input gain that we actually observe. In our data, the input gain is greater when attention is directed towards a narrow range of motion directions, again contrary to the model’s prediction. We therefore propose two physiologically testable extensions of the model that include direction-tuned normalization mechanisms of attention. Both extensions account for our data without affecting the previously demonstrated successful performance of the NMoA.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>CRC-889</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Treue</surname>
<given-names>Stefan</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>FOR-1847</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Treue</surname>
<given-names>Stefan</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by grants of the Deutsche Forschungsgemeinschaft through the Collaborative Research Center 889 “Cellular Mechanisms of Sensory Processing” and the Research Unit 1847 “Physiology of Distributed Computing Underlying Higher Brain Functions in Non-Human Primates”. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="1"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Attention to visual features like a specific orientation or motion direction has been shown to enhance visual responses to the attended feature across visual cortex in both monkey neurophysiology [<xref ref-type="bibr" rid="pcbi.1005225.ref001">1</xref>] and human fMRI data [<xref ref-type="bibr" rid="pcbi.1005225.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1005225.ref004">4</xref>]. Prior studies have reported that feature-based attention enhances responses in neurons tuned to the attended feature [<xref ref-type="bibr" rid="pcbi.1005225.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref006">6</xref>], privileges responses to the attended feature under competitive conditions [<xref ref-type="bibr" rid="pcbi.1005225.ref007">7</xref>] and induces shifts of the preferred feature [<xref ref-type="bibr" rid="pcbi.1005225.ref008">8</xref>]. Similarly, visual attention to a particular spatial location affects neuronal responses and improves perceptual performance at the attended location [reviewed in <xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>]. In particular, attention has been shown to enhance neuronal responses by increasing the effective sensory input strength (in our task: coherence gain: <xref ref-type="fig" rid="pcbi.1005225.g001">Fig 1A</xref>) and/or by scaling the responses of the neuron (response gain: <xref ref-type="fig" rid="pcbi.1005225.g001">Fig 1B</xref>) [<xref ref-type="bibr" rid="pcbi.1005225.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1005225.ref015">15</xref>].</p>
<fig id="pcbi.1005225.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Illustration of coherence response functions, relating behavioral and/or physiological responses to signal strength.</title>
<p>An attentional enhancement would be visible as a change in response gain (A) and/or coherence gain (B) on the psychometric, or neurometric function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g001" xlink:type="simple"/>
</fig>
<p>The Normalization Model of Attention [NMoA: <xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>] attempts to capture this variety of attentional effects in a single model. It proposes that attention multiplicatively scales the driving input to a neuronal population, and the response to this driving input of each individual neuron in the population is divisively normalized by the responses of all the neurons in the normalizing pool. Depending on the size of the visual stimulus and the spread of visual attention, the relative effects of sensory stimulation and visual attention on the individual neuron and the normalizing pool differ, leading to input-gain and/or response-gain effects that reproduce many of the effects of spatial attention on neuronal responses [<xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref016">16</xref>]. Further, fMRI measurements of the spatial spread of visual attention in human subjects provide support for this critical assumption of the NMoA by verifying the model’s predictions regarding the influence of the spatial spread of visual attention on behavioral performance [<xref ref-type="bibr" rid="pcbi.1005225.ref017">17</xref>], or voxel-averaged neurometric functions [<xref ref-type="bibr" rid="pcbi.1005225.ref018">18</xref>]. The NMoA also captures some of the reported effects of feature-based attention on neuronal responses [<xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>], using the same underlying mechanism of attentional scaling of sensory responses. Importantly, the NMoA predicts that, assuming biologically plausible parameters (see <xref ref-type="sec" rid="sec009">Materials and Methods</xref>) [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>], attention to a visual feature will impact neuronal responses mainly by increasing the effective response of neurons tuned to the attended feature (response gain), rather than by increasing the sensory input strength of the attended stimulus (input gain). This implies, given a quasi-linear linking-model relating neuronal responses to behavioral output [<xref ref-type="bibr" rid="pcbi.1005225.ref020">20</xref>], that attention to a visual feature will not produce input-gain effects, but only response-gain effects on psychometric functions. Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>] confirmed this prediction when they observed only response gain effects in an experiment where human subjects paid attention to either narrow or broad ranges of orientation.</p>
<p>In contrast, we report here that when human subjects are cued to attend to one of two motion directions in a transparent motion display, attentional effects manifest as a combination of input gain (in our task “coherence gain”) and response gain. Further, different from conclusions drawn by Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>], we observed a larger impact on input gain for a narrow focus of attention in feature space than for a broad focus, while the observed response gain effect was not significantly different between conditions. These results require either a revision of the assumptions linking neuronal activity to behavior, or extensions of the NMoA that include direction-tuned influences on the normalization pool. Since given the assumptions of the linking model, psychophysical performance can be used to estimate neuronal responses [<xref ref-type="bibr" rid="pcbi.1005225.ref020">20</xref>] as well as to deduce models of divisive normalization [<xref ref-type="bibr" rid="pcbi.1005225.ref021">21</xref>], we propose and compare two possible extensions to the NMoA, introducing either coherence-dependent or coherence-independent direction-tuned normalization. The extended normalization models are consistent with the feature-similarity gain model of attention [<xref ref-type="bibr" rid="pcbi.1005225.ref005">5</xref>] and the attentional modulation in extrastriate cortical area MT, where neuronal responses are enhanced and suppressed by attention to preferred and non-preferred motion directions respectively [<xref ref-type="bibr" rid="pcbi.1005225.ref006">6</xref>].</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>In this study, we measured human psychophysical performance in a direction discrimination task using transparent motion stimuli with varying motion coherence (<xref ref-type="fig" rid="pcbi.1005225.g002">Fig 2</xref>). We used endogenous cues of varying directional precision and validity to achieve two levels and two directional spreads of voluntary feature-based attention. Human perceptual performance was compared for strongly and weakly attended stimuli, directed by cues that were valid in 75% of all trials. In addition to these two validity conditions we employed two attentional conditions to generate narrow and wide feature-based attentional distributions to specifically test the critical role that the width of the attentional focus plays in the NMoA. For each of the four task constellations of the two cueing validities and the two widths of the attentional focus (valid-narrow, invalid-narrow, valid-broad and invalid-broad) we determined performance as a function of stimulus signal strength (motion coherence) and evaluated the effects of feature-based attention on the coherence response function.</p>
<fig id="pcbi.1005225.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Experimental protocol.</title>
<p>Human observers performed a direction discrimination task and reported the rotational direction change between the motion direction shown in stimulus display 2 and the corresponding motion component of stimulus 1. Black arrows indicate two example direction components embedded in the transparent motion display 1, one of which is slightly rotated and shown again in display 2. Subjects were cued to which one of the two motion directions of the transparent motion display was likely to be the relevant direction. Cues indicated either a relatively small range of possible directions (right panel, narrow focus cues), or a wide range of possibly relevant motion directions (broad focus cues). The actually displayed motion was always jittered around the cued direction, such that the cue itself was non-informative about the precise direction of the relevant motion. In addition, cues indicated the correct motion component with a 75% validity, making it worthwhile for subjects to process both motion components of stimulus display 1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g002" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Cue validity affects performance, especially when attention is focused</title>
<p>To validate whether our cueing paradigm was effective in causing differential attentional deployments, we computed each subject’s mean performance across coherences, separately for each subject, attentional condition (valid/invalid) and cue type (narrow/broad). We then performed four pair-wise comparisons (Bonferroni corrected <italic>α</italic> = 0.0125, paired t-tests, n = 6 subjects). <xref ref-type="fig" rid="pcbi.1005225.g003">Fig 3</xref> shows the average of these coherence-averaged performances across subjects. For both attentional conditions, subjects performed significantly better when the cue was valid than when it was invalid (narrow focus: mean Δ<italic>d</italic>′ = 0.958, p&lt;0.001, broad focus: mean Δ<italic>d</italic>′ = 0.358, p = 0.006). Further, the performance for the validly cued direction was significantly better in the narrow focus condition compared to the broad focus condition (mean Δ<italic>d</italic>′ = 0.513, p = 0.003). The performance in the invalidly cued direction was not significantly different between the two attentional conditions (mean Δ<italic>d</italic>′ = −0.087, p = 0.24). For the statistical tests performed above, we repeated all comparisons with paired, two-sided Wilcoxon signed rank tests. This did not qualitatively change our results (i.e. all statistically significant results remained significant and all non-significant results remained non-significant).</p>
<fig id="pcbi.1005225.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Attention improves performance, especially when it is focused on a small range of directions.</title>
<p>Bars indicate mean discrimination performance of all six observers, pooled across all levels of coherence. Colors indicate cue type. For each cue type, there is a significant difference between validly and invalidly cued trials, indicating that the cue lead to deployment of feature-based attention. In addition, the two types of cues (narrow and broad focus cues) lead to a significant difference in discrimination performance for validly, but not invalidly cued trials. Error bars indicate plus/minus one standard error. P values correspond to paired t-tests.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>A wide feature-focus causes pure response gain, while a narrow focus causes both coherence and response gain</title>
<p>The core aim of our study was to determine whether feature-based attention enhances performance by coherence or response gain and match our findings to the predictions of the NMoA. This was done by determining each subject’s coherence response function in each of our four task constellations by fitting Naka-Rushton equations (<xref ref-type="fig" rid="pcbi.1005225.g004">Fig 4</xref>) with a shared slope parameter for the four conditions. The task was tailored individually to each subject (see <xref ref-type="sec" rid="sec009">Materials and Methods</xref> section) leading to comparable performances across coherences and to comparable model results across subjects. Indeed, performing pairwise t-tests on R<sup>2</sup>-values obtained for each subject and attentional condition, we did not observe significant differences in the goodness of fits for the four task conditions. Mean R<sup>2</sup>-values (for 6 individually fitted subjects) were 0.98 (narrow-valid), 0.94 (narrow-invalid), 0.98 (broad-valid) and 0.91 (broad-invalid).</p>
<fig id="pcbi.1005225.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g004</object-id>
<label>Fig 4</label>
<caption>
<title>A narrow focus of attention causes coherence gain, while a broad focus does not.</title>
<p>Fits indicate coherence response functions for pooled performance across 6 subjects. Data points are the mean discrimination performance across subjects for each tested attentional condition, cue validity and coherence level. Panel A corresponds to the narrow focus cue type (single headed arrow) and panel B to the broad focus cue type (three headed arrow). Performance (broad and narrow conditions) was fitted with four dependent Naka-Rushton equations, sharing a jointly optimized slope. Significance values indicate differences in Naka-Rushton fit coefficients of per-subject fits (see also <xref ref-type="fig" rid="pcbi.1005225.g005">Fig 5</xref>). When comparing invalidly and validly cued trials, increases in the asymptotic performance at high levels of coherence indicate response gain effects, while decreases in coherence level at half maximum indicate coherence gain effects. Error bars of data points indicate plus/minus one standard error, crosses around coefficient indicators represent individual coefficients obtained from per-subject fittings of the coherence response function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g004" xlink:type="simple"/>
</fig>
<p>We then compared the fitted Naka-Rushton coefficients for validly and invalidly cued trials, to test if attention induced a reduction in <italic>c</italic><sub>50</sub> and/or an increase in <italic>d′</italic><sub><italic>max</italic></sub>. A decrease in <italic>c</italic><sub>50</sub> indicates an increase in coherence gain and an increase in <italic>d′</italic><sub><italic>max</italic></sub> indicates an increase in response gain. We performed four pair-wise comparisons (Bonferroni corrected <italic>α</italic> = 0.0125, paired, one-tailed t-tests, n = 6 subjects, we also performed this analysis with paired, two-tailed t-tests, which did not change our conclusions). For the narrow focus condition (<xref ref-type="fig" rid="pcbi.1005225.g004">Fig 4A</xref>), we find a significant cue-induced increase in coherence gain (mean Δ<italic>c</italic><sub>50</sub> = −0.179, p = 0.002, <xref ref-type="fig" rid="pcbi.1005225.g005">Fig 5A</xref>) as well as in response gain (mean Δ<italic>d′</italic><sub><italic>max</italic></sub> = 0.895, p = 0.001, <xref ref-type="fig" rid="pcbi.1005225.g005">Fig 5B</xref>). In the broad focus condition (<xref ref-type="fig" rid="pcbi.1005225.g004">Fig 4B</xref>), the response gain enhancement is of similar magnitude and also significant (mean Δ<italic>d′</italic><sub><italic>max</italic></sub> = 0.628, p = 0.004, <xref ref-type="fig" rid="pcbi.1005225.g005">Fig 5B</xref>) while the coherence gain enhancement is much smaller and narrowly misses significance (mean Δ<italic>c</italic><sub>50</sub> = −0.062, p = 0.047, evaluated at <italic>α</italic> = 0.0125, <xref ref-type="fig" rid="pcbi.1005225.g005">Fig 5A</xref>). These effects (averaged across subjects) are also evident in single subjects (<xref ref-type="fig" rid="pcbi.1005225.g004">Fig 4</xref> and <xref ref-type="fig" rid="pcbi.1005225.g005">Fig 5</xref>). As plotting performance as <italic>d</italic>′ might amplify differences at high coherences, we also performed the same analysis based on the proportion of correct responses. This did not change the pattern of results (i.e. response gain in the broad focus condition and a combination of coherence and response gain in the narrow focus condition).</p>
<fig id="pcbi.1005225.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Population effects are also evident in single subjects.</title>
<p>Data points indicate per subject fit coefficients <italic>c</italic><sub>50</sub> (A) and <italic>d</italic>′<sub><italic>max</italic></sub> (B), corresponding to coherence level at half maximum performance and asymptotic performance, respectively. For each subject, two Naka-Rushton equations per cue type were fit to the psychophysical data, revealing four informative coefficients. A decrease in <italic>c</italic><sub>50</sub> between validly and invalidly cued trials indicates a contrast gain effect and an increase in <italic>d</italic>′<sub><italic>max</italic></sub> a response gain effect. Dashed lines connect data points originating from the same subjects.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g005" xlink:type="simple"/>
</fig>
<p>Next, we tested whether the magnitude of coherence (<italic>c</italic><sub>50</sub>) and response gain (<italic>d</italic>′<sub><italic>max</italic></sub>) changes with attentional condition (i.e. with an increasing width of the feature-based attentional focus). We calculated a modulation index <italic>MI</italic><sub><italic>ζ</italic></sub> ((<italic>a</italic> − <italic>b</italic>)/(<italic>a</italic> + <italic>b</italic>), see <xref ref-type="sec" rid="sec009">Materials and Methods</xref> section) for each coefficient-condition pair and then performed paired comparisons of the distribution of indices across attentional conditions. We find that the magnitude of coherence gain is significantly different between attentional conditions (mean <inline-formula id="pcbi.1005225.e001"><alternatives><graphic id="pcbi.1005225.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.293</mml:mn><mml:mo>≜</mml:mo><mml:mn>82.9</mml:mn><mml:mi>%</mml:mi></mml:math></alternatives></inline-formula>, p = 0.007, paired t-test), while there is no significant change in response gain (mean <inline-formula id="pcbi.1005225.e002"><alternatives><graphic id="pcbi.1005225.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi>′</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.033</mml:mn><mml:mo>≜</mml:mo><mml:mn>6.8</mml:mn><mml:mi>%</mml:mi></mml:math></alternatives></inline-formula>, p = 0.136, paired t-test, Bonferroni corrected <italic>α</italic> = 0.025). For these statistical tests, we repeated all comparisons with paired, two-sided Wilcoxon signed rank tests. This did not qualitatively change our results.</p>
<p>We further addressed a potentially confounding ceiling effect of performances at high coherences by repeating the above analysis, leaving out the two highest coherences (i.e. the highest performances we measured in our task) of the valid condition in narrow focus trials, thereby disregarding data points that might have been affected by a ceiling effect of performance. With this reduced dataset, the increase in response gain narrowly misses significance in the narrow focus condition, however, a coherence gain change was still highly significant.</p>
</sec>
<sec id="sec005">
<title>Subjects used the sample, not the cue direction</title>
<p>The narrow focus cue did not signal the precise direction of the sample stimuli, but rather indicated that the relevant sample was likely to occur within a range of ±10 degree around the cued direction (heading of the arrow). Nonetheless, we tested whether subjects used the cued direction as sample and simply ignored the subsequently presented sample direction. If this were true, direction discrimination performance should increase once the test direction was far off from the cued direction. <xref ref-type="fig" rid="pcbi.1005225.g006">Fig 6A</xref> shows the performance across coherences for three groups of trials that differ in how far off the cued direction the test direction occurred. Groups were defined individually for each subject based on his/her individual direction change magnitude (see <xref ref-type="sec" rid="sec009">Materials and Methods</xref>) and we divided the possible range of absolute cue-test differences into three evenly spaced parts (close, medium and far). Since upcoming invalidly cued directions could also be inferred from the cue (since the uncued direction range centered ±135 degrees from the cued direction), we were able to define the same three groups for invalidly cued trials.</p>
<fig id="pcbi.1005225.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Task performance across coherences.</title>
<p>(A) Performance for groups of trials that differ in how far off the cued direction the test direction occurred. The possible range of test-cue differences was divided in three evenly spaced groups (close, medium, far). Lines above bars represent pairwise comparisons and stars indicate significant differences of adjacent bars. Error bars indicate plus/minus one standard error. (B) Like A, but groups were defined based on the differences between cue and sample.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g006" xlink:type="simple"/>
</fig>
<p>For each group we find significant effects of cue validity (paired t-tests, p&lt;0.001, p = 0.002, p&lt;0.001 for close, medium and far, respectively) while pairwise comparisons indicated that none of the three groups of validly cued trials was significantly different from the others. The same was true for the invalidly cued trials (all p&gt;0.027, Bonferroni corrected <italic>α</italic> = 0.0083, <italic>n</italic> = 6 comparisons). We thus find no evidence pointing towards subjects using the cue direction (rather than the sample direction) as a reference for the direction discrimination task in the narrow focus condition.</p>
<p>We also tested whether sample presentations occurring far from the cued direction resulted in improved task performance. For this purpose trial groups were defined as sample directions close (0–2 degrees), medium (3–6 degrees), and far (7–10 degrees) from the cued direction (or the inferred uncued direction). <xref ref-type="fig" rid="pcbi.1005225.g006">Fig 6B</xref> shows the performance across coherences for those three trial groups. Similar to the trial grouping by sample-test difference, we find significant effects of cue validity (paired t-tests, p = 0.001, p&lt;0.001, p = 0.001, for close, medium and far, respectively). Again, no pairwise comparison between groups was significant for either valid or invalid trials (all p&gt;0.02, Bonferroni corrected <italic>α</italic> = 0.0083, <italic>n</italic> = 6 comparisons). This suggests that in the narrow focus condition, the featural extent of attention covered at least a range of 20 degrees, centered on the attentional cue, which we also assumed in all model simulations.</p>
</sec>
<sec id="sec006">
<title>The current NMoA cannot plausibly account for these results</title>
<p>Our experimental results reveal a mixture of coherence and response gain enhancements when attention is focused on a narrow range of directions (narrow focus condition), and a pure response-gain enhancement when attention is focused on a broad range of directions (broad focus condition). As pointed out by Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>], a change in behavioral performance will mimic the underlying change in neuronal response functions, and therefore only a pure response gain for attention to motion directions will be visible in the neurometric function [<xref ref-type="bibr" rid="pcbi.1005225.ref020">20</xref>]. Further, even if any coherence gain effects were to arise, they would be found in the broad focus condition, which is the opposite of what our empirical data show. The intuition behind these statements has been presented in detail by Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>] as well as Reynolds and Heeger [<xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>], but we summarize it briefly here:</p>
<p>The NMoA computes the response of an arbitrary single neuron to a given set of stimuli as:
<disp-formula id="pcbi.1005225.e003">
<alternatives>
<graphic id="pcbi.1005225.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>R</italic>(<italic>x</italic>,θ;<italic>c</italic>) is the response of a neuron with its receptive field centered at <italic>x</italic> and its feature tuning centered at θ, receiving stimulus input with contrast <italic>c</italic>. <italic>A</italic><sub><italic>i</italic></sub>(<italic>x</italic>,θ)<italic>E</italic>(<italic>x</italic>,θ;<italic>c</italic><sup><italic>n</italic></sup>) is a term composed of the net excitatory input drive to the neuron <italic>E</italic>(<italic>x</italic>,θ;<italic>c</italic><sup><italic>n</italic></sup>) scaled by the attentional gain <italic>A</italic><sub><italic>i</italic></sub>(<italic>x</italic>,θ) ≥ 1, which varies with cue validity and attentional condition (i.e. narrow or broad focus). Further, <italic>E</italic>(<italic>x</italic>,θ;<italic>c</italic><sup><italic>n</italic></sup>) also depends on the stimulus contrast raised to an exponent (<italic>c</italic><sup><italic>n</italic></sup>) while both <italic>E</italic>(<italic>x</italic>,θ;<italic>c</italic><sup><italic>n</italic></sup>) and <italic>A</italic><sub><italic>i</italic></sub>(<italic>x</italic>,θ) depend on the similarity of the neuron’s receptive field and tuning properties with the driving stimulus and the attentional focus, respectively. <italic>S</italic>(<italic>x</italic>,θ;<italic>c</italic>) is the effect of the normalizing pool and represents the excitatory drive convolved by the suppressive surround:
<disp-formula id="pcbi.1005225.e004">
<alternatives>
<graphic id="pcbi.1005225.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>s</italic>(<italic>x</italic>,θ) is the suppressive filter (defining the spatial and feature tuning of the surround) and * indicates a convolution.</p>
<p>For the transparent motion stimuli with two component motion directions that we used, the response of one neuron with preferred direction centered at one of the component directions (from <xref ref-type="disp-formula" rid="pcbi.1005225.e003">Eq 1</xref>) can be simplified (without attention) as:
<disp-formula id="pcbi.1005225.e005">
<alternatives>
<graphic id="pcbi.1005225.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
with <italic>α</italic> as the (constant) gain of the neuron receiving it’s preferred input with contrast <italic>c</italic> and S representing the net normalizing effect of the neurons in the population. S is regulated by the width of <italic>s</italic>(<italic>x</italic>,θ) (see <xref ref-type="disp-formula" rid="pcbi.1005225.e004">Eq 2</xref>). When <italic>s</italic>(<italic>x</italic>,θ) is narrow (strongly tuned normalization), attention (<italic>γ</italic>) acts equally on the driving input and the normalizing factor S and this leads to a coherence-gain effect (Reynolds and Heeger 2009):
<disp-formula id="pcbi.1005225.e006">
<alternatives>
<graphic id="pcbi.1005225.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mi>γ</mml:mi><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>More explicitly, this happens because the normalizing pool is dominated by the inputs that excite the neuron and attention to the non-preferred feature is essentially invisible to the neuron since it lies outside both the excitatory and suppressive filters. In contrast, when <italic>s</italic>(<italic>x</italic>,θ) is broad, the impact of attention on the denominator <italic>S</italic> + σ is minimal (even if the attentional spread is broad) since the normalizing pool includes almost equal contributions from the neurons centered at the attended and unattended directions. Under these conditions,
<disp-formula id="pcbi.1005225.e007">
<alternatives>
<graphic id="pcbi.1005225.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mi>γ</mml:mi><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
which represents a response gain for the validly cued condition compared to the invalidly cued one. As a result, for the NMoA to predict a coherence-gain effect of attention, the normalizing pool (or suppressive surround) would have to be so narrow (see below) as to be physiologically implausible. Further, since the coherence-gain effect is facilitated when attention has a greater impact on the normalizing pool (by acting more broadly), it is the broad focus condition that should show a stronger coherence-gain effect of attention.</p>
<p>We confirm these statements by explicitly fitting the NMoA to our data. Free parameters, shared among attentional conditions, were the gain of attention (<italic>A</italic><sub><italic>i</italic></sub>), separately optimized for narrow and broad conditions, the normalization constant σ, the exponent n and a scaling parameter to linearly scale simulated values to <italic>d</italic>′ (for the values of the fixed parameters, see <xref ref-type="sec" rid="sec009">Materials and Methods</xref> section). The best fitting NMoA model shows a clear lack of fit to the empirical data (<xref ref-type="fig" rid="pcbi.1005225.g007">Fig 7</xref>), especially in the narrow focus condition, which is expected because that is where the coherence-gain effects manifest. The NMoA model’s best fit resembles a response gain in both attentional conditions, as expected. The observed lack of fit is not a result of our chosen fixed parameters: varying all but one of those parameters over a large range did not change our conclusions. The only critical parameter, as mentioned above, is the width of the suppressive filter in the feature dimension. We therefore redid the fits, but with the featural width of the suppressive filter as an additional free parameter (NMoA free model). This resulted in an optimal, yet biologically implausible, inhibitory tuning width of <italic>σ</italic> = 12.3 degrees and a model producing clear effects of coherence gain in both attentional conditions (<xref ref-type="fig" rid="pcbi.1005225.g007">Fig 7</xref>). This model accounts for the reduction of coherence gain in the broad-focus condition by proposing that the broader width of the attentional field is accompanied by a reduced attentional gain. While this is not an unreasonable assumption, it compromises the ability of the model to account for the observed response-gain changes, especially in the broad-focus condition (<xref ref-type="fig" rid="pcbi.1005225.g007">Fig 7B</xref>). Thus, even if the original NMoA is allowed to take on biologically implausible parameters, it still does not capture our data fully.</p>
<fig id="pcbi.1005225.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Model predictions of coherence response functions for individual fittings to the empirical performance of 6 subjects.</title>
<p>Data points with plus/minus one standard error are the mean discrimination performance across subjects for each tested attentional condition, cue validity and coherence level. Panel A corresponds to the narrow focus cue type (single headed arrow) and panel B to the broad focus cue type (three headed arrow). The two evaluated models are the original NMoA with 5 free parameters and a NMoA with optimal, yet biologically implausible suppressive tuning width (NMoA free, 6 free parameters). Note the prediction of reduced response gain for the broad focus condition (panel B) in both models.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Adding tuned normalization accounts for the empirical data</title>
<p>Since the original NMoA does not capture our observed effects of feature-based attention, we attempted to extend the NMoA in the simplest, yet most plausible manner in order to do so. The empirical data indicate that the coherence-gain effect of feature-based attention emerges for the validly-cued feature and is greater in the narrow focus condition. One way to incorporate a coherence-gain effect is to postulate that in addition to enhancing the input drive to the attended feature, feature-based attention reduces the coherence-independent normalization term <italic>σ</italic><sup><italic>n</italic></sup> (NMoA+ciN model) and that this reduction is greater when attention is more focused (as in the narrow focus condition). This reduction is independent of stimulus strength (coherence) and direction, but tuned to the attended direction such that attention to a particular motion direction reduces the normalizing effect on neurons tuned to that direction and potentially enhances the normalizing effect on neurons tuned to far-away directions. In other words, <xref ref-type="disp-formula" rid="pcbi.1005225.e003">Eq 1</xref> can be rewritten in an extended form as:
<disp-formula id="pcbi.1005225.e008">
<alternatives>
<graphic id="pcbi.1005225.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where 1≤<italic>N</italic>(<italic>θ</italic>) represents the direction-tuned effect of attention that is maximal for motion directions close to the attended feature.</p>
<p>Another way to incorporate a coherence-gain effect is to unify the NMoA with models utilizing previously proposed ideas of neuronal self-normalization [e.g. <xref ref-type="bibr" rid="pcbi.1005225.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref023">23</xref>]. Here, each neuron is normalized not only by its suppressive surround, but also by its own net-excitatory input. Such a coherence-dependent extension of the NMoA (NMoA+cdN model) can be written as:
<disp-formula id="pcbi.1005225.e009">
<alternatives>
<graphic id="pcbi.1005225.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mo>*</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">θ</mml:mi><mml:mo>;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where 0≤<italic>N</italic>≤1 is a single free parameter determining the balance between pure self-normalization (<italic>N</italic> = 1), predicting only coherence-gain, and the original NMoA (<italic>N</italic> = 0), predicting mainly response gain.</p>
<p>We examine the potential physiological bases of both extended versions of the NMoA in the Discussion section. In terms of capturing the coherence-gain effects of attention, both models effectively capture both the response-gain and coherence-gain effects evident in our empirical data (<xref ref-type="table" rid="pcbi.1005225.t001">Table 1</xref> and <xref ref-type="fig" rid="pcbi.1005225.g008">Fig 8</xref>).</p>
<fig id="pcbi.1005225.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Model predictions of coherence response functions for two extended Normalization Models.</title>
<p>The NMoA+ciN model (7 free parameters) includes a coherence independent contribution of feature-based attention to normalization while the NMoA+cdN model (6 free parameters) includes a weighted contribution of tuned-normalization. Panel and data points like in <xref ref-type="fig" rid="pcbi.1005225.g007">Fig 7</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.g008" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1005225.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005225.t001</object-id>
<label>Table 1</label> <caption><title>Model comparison</title></caption>
<alternatives>
<graphic id="pcbi.1005225.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005225.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify"/>
<th align="justify">NMoA</th>
<th align="justify">NMoA free</th>
<th align="justify">NMoA+cdN</th>
<th align="justify">NMoA+ciN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify">Free param.</td>
<td align="justify">5</td>
<td align="justify">6</td>
<td align="justify">6</td>
<td align="justify">7</td>
</tr>
<tr>
<td align="justify">Adj. R<sup>2</sup></td>
<td align="justify">0.8626</td>
<td align="justify">0.9017</td>
<td align="justify">0.9040</td>
<td align="justify">0.9070</td>
</tr>
<tr>
<td align="justify">AIC</td>
<td align="justify">-229.32</td>
<td align="justify">-275.59</td>
<td align="justify">-278.98</td>
<td align="justify">-282.42</td>
</tr>
<tr>
<td align="justify">BIC</td>
<td align="justify">-214.57</td>
<td align="justify">-257.89</td>
<td align="justify">-261.28</td>
<td align="justify">-261.78</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We fit both extended NMoAs (with one and two additional free parameters for the NMoA+cdN and NMoA+ciN model, respectively) and compared them to the previously computed best fits from the original NMoAs (fixed and free suppressive width, <xref ref-type="fig" rid="pcbi.1005225.g007">Fig 7</xref>). <xref ref-type="table" rid="pcbi.1005225.t001">Table 1</xref> summarizes the results. Both extensions fit the data significantly better than the original NMoA (F = 59.29, p&lt;0.001, between NMoA and NMoA+cdN; F = 33.20, p&lt;0.001, between NMoA and NMoA+ciN). Compared to the NMoA free model, only the NMoA+ciN model shows a significant advantage (F = 0.98, p = 0.56, between NMoA free and NMoA+cdN; F = 8.60, p = 0.004, between NMoA free and NMoA+ciN). However, AIC as well as BIC measures indicate both extended NMoAs as superior to the original NMoAs. Between extended models, we find that the NMoA+ciN model performs marginally better than the NMoA+cdN model (F = 5.23, p = 0.024) with both lower AIC and BIC metrics for the NMoA+ciN model, confirming that the use of one extra parameter was justified and the model with a coherence-independent influence of attention on normalization described the data better than the model incorporating neuronal self-normalization.</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>The Normalization Model of Attention [<xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>] has become the central model for capturing the known variety of attentional effects on neuronal responses, fMRI signals and behavioral performance. While the NMoA is powerful enough to explain a wide range of response patterns under physiologically plausible assumptions (see <xref ref-type="sec" rid="sec009">Materials and Methods</xref>), it is also limited in flexibility and cannot predict certain patterns of responses, such as a reduction of input gain, but not response gain, caused only by a widening of the attentional focus. Since many assumptions underlying the NMoA’s parameters are not easily verified, such predictions of “impossible results” are critical because they allow the model to be stringently tested against empirical data. Here, we report that human subjects show behavioral performance patterns that go against a prediction of the NMoA and suggest and compare two simple and testable extensions to the NMoA that can account for the findings.</p>
<p>As pointed out by Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>], the NMoA predicts that under biologically plausible parameter settings, attention to a visual feature like orientation or motion direction will only produce response-gain effects in neuronal response functions. Given that changes in the neuronal representation are assumed to scale quasi-lineary to behavioral performance [<xref ref-type="bibr" rid="pcbi.1005225.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref021">21</xref>], these effects imply that similarly, only response-gain effects will be found when comparing psychometric functions measuring performance on tasks involving attended and unattended features. Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>] went on to confirm this prediction by showing only response gain effects in psychometric functions when subjects paid attention to either narrow or broad ranges of orientation. Here, we built on this work by measuring the performance of human subjects on a task requiring them to discriminate a direction change in one of the two directions of a transparent motion display. Performance increased with motion coherence and was greater for validly cued stimuli. However, in contrast to Herrmann et al.’s [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>] results for attention directed to orientations, we found that attentional effects manifest as a combination of input gain and response gain on the psychometric function. Critically, when we compared the effects of attention directed towards either a narrow or broad range of motion directions, we found a significant decrease of input gain, but not response gain, for the broad focus, which cannot be readily accounted for by the original formulation of the NMoA.</p>
<p>Our results using a motion direction discrimination task differ from those of Herrmann et al.’s [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>] task using orientation discrimination, despite the fact that the two tasks are conceptually very similar. One difference is that we varied coherence rather than contrast to manipulate signal strength in order to obtain a sufficiently large dynamic range. Currently, there is only limited evidence describing the effect of coherence changes on neurometric functions. Available results indicate that, at least for non-transparent motion patterns, the coherence-response function in MT is much more linear than the contrast-response function [<xref ref-type="bibr" rid="pcbi.1005225.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1005225.ref026">26</xref>]. Sigmoidal coherence-response functions have also been reported in macaque MT [<xref ref-type="bibr" rid="pcbi.1005225.ref027">27</xref>]. It is not obvious why these differences between the coherence and contrast-response functions should cause the difference in our results. Our results show that adding either a coherence-independent contribution of attention to normalization or a coherence-dependent mechanism of self-normalization to the NMoA is sufficient to fully account for our data. This points to potential differences in the attentional contribution to normalization between our results and Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>]. Further research is needed to determine how different stimulus properties and task demands might lead to different amounts of stimulus-dependent and stimulus-independent feature-based attentional contributions to neuronal normalization.</p>
<p>We suggest two possible extensions of the NMoA both including direction-tuned influences on the normalization pool. The first model (NMoA+ciN) implements a coherence-independent, attentional contribution to normalization. Here, attention not only modulates the input drive to a neuronal population, but also reduces the impact of the normalization on the responses of neurons tuned for the attended direction. Further, the data indicate that such a tuned normalizing effect of attention would have to be greater when attention is more narrowly focused than when it is broadly distributed. To implement such a specific rescaling of the coherence-independent normalizing input in the brain, we suggest that since the NMoA can be considered a steady-state version of an unspecified network model with mutual competition, a stimulus at the preferred direction of the neuron could suppress the local population that is tuned to non-preferred directions and thereby reduce their contribution to the normalizing pool. Alternatively, we propose in the second model (NMoA+cdN) that each neuron preferentially weights its own contribution to the normalization pool (self-normalization) in comparison to the contribution of all suppressive neurons. Such a mechanism was previously shown to be a vital component in a model capturing the response properties of direction-selective neurons in extrastriate cortex [<xref ref-type="bibr" rid="pcbi.1005225.ref022">22</xref>]. The tuned normalization in another recent report [<xref ref-type="bibr" rid="pcbi.1005225.ref023">23</xref>] is also conceptually similar: here, the authors showed that MT neuronal responses to a pair of stimuli within the receptive field (one moving in the preferred direction and the other in the anti-preferred direction) were well explained by direction-tuned divisive normalization. The majority of neurons in their data showed a greater normalizing influence of the preferred stimulus. We show here that extending the NMoA with an explicit tuned-normalization component also captures our results in an attention task, despite the fact that this coherence-dependent mechanism is independent of the spread of attention. However, the difference between the two extensions is significant and the NMoA+cdN model described the data worse than the NMoA+ciN model.</p>
<p>The proposed NMoA+ciN model modifies the normalization mechanism to include a reduction by feature-based attention of the normalizing influence for neurons tuned to the attended direction. There are a variety of ways in which this modification could be implemented. For example, if feature-based attention suppresses the responses of neurons tuned to non-preferred directions, their contribution to the normalization pool could be reduced thereby reducing the coherence gain for neurons tuned to the attended direction (but increasing it for neurons tuned to the unattended direction, where the normalization pool will be enhanced). Alternatively, feature-based attention may enhance both the "stimulus drive" as well as the "normalization" for neurons tuned to the attended direction, and this effect may manifest as coherence gain. Importantly, here the direction selectivity of the normalization pool is not critical, but instead, attention has a selective effect on neurons tuned to the attended direction [<xref ref-type="bibr" rid="pcbi.1005225.ref012">12</xref>]. Thus, the mechanism works even if the normalization pool is untuned, but critically, it may also work when the normalization pool is tuned.</p>
<p>In a related framework, Boynton [<xref ref-type="bibr" rid="pcbi.1005225.ref028">28</xref>] proposed a normalization model with a stimulus independent contribution of attention to the normalization pool. This untuned normalization can account for attentional effects of input gain when attention is directed inside versus outside of a neuron’s receptive field. For non-spatial forms of attention, as described here, a feature-tuned input to normalization is necessary since attention does not shift out of the receptive field. It should be pointed out, however, that the proposed extension with a coherence-independent, tuned input to normalization (NMoA+ciN) can similarly be applied to this or other previously proposed models of attentional normalization [<xref ref-type="bibr" rid="pcbi.1005225.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1005225.ref030">30</xref>].</p>
<p>In addition to the extended normalization models considered above, one can imagine an important alternative to account for our empirical results. The hypothesized modifications all assume, that the behavioral effects of attention and its spread emerge from its effects on the neuronal representations of the stimulus (i.e. the perceptual representation). However, attention may also act by modifying the decisional mechanism, for example, through enhanced weighting of the cued stimuli [<xref ref-type="bibr" rid="pcbi.1005225.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1005225.ref038">38</xref>]. Specifically, the change in performance between validly and invalidly cued features could result from the differential weighting of inputs from the two motion directions, with greater weight given to the validly cued feature. With a lower weight to the unattended motion direction, the performance may only rise above chance once the coherence becomes sufficiently large. Similarly, the change in performance for validly cued motion directions between trials with focused or dispersed feature-based attention may be due to improved weighting of the same perceptual representation, rather than an effect of attention on the perceptual representation itself (as we assume here). Differentiating between these two alternatives may require physiological recordings that examine the effects of feature-based attention under our conditions in the dorsal motion-processing pathway in order to measure the underlying neuronal coherence-response functions.</p>
<p>Spatial attention has been shown to affect correlations within neuronal populations encoding visual features [<xref ref-type="bibr" rid="pcbi.1005225.ref039">39</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref040">40</xref>] and to reduce single-neuron variability [<xref ref-type="bibr" rid="pcbi.1005225.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1005225.ref042">42</xref>]. Such effects can cause improvements in psychophysical performance even without increases in neuronal responses. The NMoA does not consider such attentional effects and thus aims to account for changes in psychophysical performance by changes in mean spiking activity. Consequently, we have assumed that the attentional modulation of psychophysical performance is independent of changes in correlations between neuronal firing of individual neurons. Additional experiments are needed to clarify to which degree feature-based attention causes changes in both neuronal correlations and neuronal variability and how those potential effects translate into changes in psychophysical performance.</p>
<p>Attention to an anti-preferred motion direction suppresses the responses of MT neurons across the visual field in a multiplicative manner [<xref ref-type="bibr" rid="pcbi.1005225.ref005">5</xref>]. This finding inspired the feature-similarity gain model of attention which postulates that attending to a particular motion direction (or more generally, visual feature) enhances the responses of neurons tuned to the attended motion direction and suppresses the responses of neurons tuned to the opposite motion direction [<xref ref-type="bibr" rid="pcbi.1005225.ref006">6</xref>]. The NMoA can account for these findings by postulating that feature-based attention to the non-preferred direction increases its contrast or coherence-dependent contribution to the normalizing pool. Both of the proposed extensions to the NMoA do not compromise these previous predictions made by the NMoA, since they both contain the original model as a special case. However, the NMoA+ciN model has an additional mechanism whereby feature-based attention to the preferred direction has a coherence-independent “pure attentional” effect on the normalizing pool. This attentional influence can release a neuron from the suppressive effect of normalization when its preferred direction is attended. Measuring the extent to which these two effects contribute to the enhancing and suppressive effects of feature-based attention will require experiments specifically designed to tease apart these two effects.</p>
<p>In summary, our results support and extend the popular NMoA with a modulatory mechanism specific to feature-based attention. This will allow the NMoA and similar models of attention and divisive normalization to cover an even wider set of conditions. As our extensions generate testable predictions, they are well suited to guide further research into the mechanisms and phenomenology of feature-based attention.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec010">
<title>Human subjects</title>
<p>Eight subjects (ages 18–27 years) participated in the study, out of which 6 subjects (2 naive female, 3 naive male and 1 male lab member) reached a sufficient performance level for analysis (see section Data Analysis below). All subjects reported normal or corrected to normal vision. Prior to entering the main experiment four subjects participated in a pilot study to determine a suitable task timing. All naive participants received monetary compensation for each session. Subjects were verbally instructed about the task demands and received individual training before entering the main experiment (see section Pre-Tests). All experiments were in accordance with institutional guidelines for experiments with humans and adhered to the principles of the Declaration of Helsinki. Each subject gave informed written consent prior to participating in the study.</p>
</sec>
<sec id="sec011">
<title>Apparatus</title>
<p>Stimuli were presented on a LCD screen (SyncMaster 2233, Samsung) with a refresh rate of 120Hz and a background luminance of 20 cd/m<sup>2</sup>. The experiment was controlled by an Apple computer (MacPro 2010) running the open-source software MWorks version 0.5 (<ext-link ext-link-type="uri" xlink:href="http://mworks-project.org" xlink:type="simple">mworks-project.org</ext-link>). Subjects were seated in a dimly lit room at a viewing distance of 57cm from the screen, their head resting on a chin-rest. A gamepad (Precision, Logitech) was used for recording responses, such that a button press with the right index finger indicated a clockwise decision, and the left index finger a counter-clockwise decision. Each experimental trial was started by pressing a button with the right thumb. For three subjects, eye position was recorded monocularly (left eye) using a video-based eye tracker (IView X, SMI) sampling at 250Hz. For the remaining three subjects, eye position was recorded binocularly with a sampling frequency of 500Hz using an Eyelink-1000 system (SR Research). Both eye position systems were calibrated before each experimental session and the accuracy of the calibration confirmed by a custom calibration task.</p>
</sec>
<sec id="sec012">
<title>Stimuli and procedure</title>
<p><xref ref-type="fig" rid="pcbi.1005225.g002">Fig 2</xref> depicts the experimental paradigm. Subjects viewed moving random dot patterns (RDPs) through a stationary annulus-shaped virtual aperture with an inner diameter of 5 degrees and an outer diameter of 17.8 degrees of visual angle. The RDPs contained 4 dots/deg<sup>2</sup>, moving on individual linear paths at a speed of 15 deg/s. Each dot had a diameter of 0.252 degrees and a luminance of 70 cd/m<sup>2</sup>. Subjects had to maintain their gaze on a fixation point central to the RDP and to initiate each experimental trial by a thumb-button press. Then an attentional cue was presented (see section "Attentional Cues") for 500ms on top of the fixation point.</p>
<p>Following the cue and a 800ms delay, a RDP was displayed for 650ms. This first presentation of the RDP contained two superimposed groups of coherently moving dots (‘direction components’), as well as an additional number of randomly moving dots. The two motion directions of this transparent motion display were always 135±20 degrees apart, with each direction being sampled randomly from a ±10 degree range around a reference direction. Reference directions were +45, 0 and -45 degrees from straight left or rightward motion. The presentation of this first RDP was followed by a short delay of 100ms with only the fixation point present on the screen. Then the second RDP was displayed for 400ms, with a slightly rotated version of one of the two previously shown motion directions, as well as the same proportion of noise dots as in the first RDP. Subjects had to indicate whether the single motion direction of the second RDP was rotated clockwise or counter-clockwise relative to the closest motion direction of the first RDP (2 alternative-forced choice, <xref ref-type="fig" rid="pcbi.1005225.g002">Fig 2</xref>). Subjects received auditory feedback indicating correct or wrong judgments. The magnitude of the direction change was individually set for each subject to be the pooled just noticeable difference of all reference directions (see section Pre-Tests).</p>
<p>We varied the motion coherence on a trial-by-trial basis. Motion coherence was defined as the percentage of dots moving in signal directions. The remaining noise dots moved on linear paths in random directions. The coherence level was the same for both presentations of the RDP (i.e. regardless of how many motion directions were presented). We used 6 levels of coherences (1.6%, 6.4%, 12.8%, 25.6%, 51.2% and 100%) for each attentional condition. Throughout each session, all cue types and coherence levels were pseudo-randomly interleaved. One session consisted of 576 properly terminated trials, excluding fixation errors and erroneous early responses. Each subject participated in 5 sessions for a total of 2880 analyzed trials per subject. Trials in which eye-positions occurred outside a radius of 2.5 degrees around the fixation point, or eye blinks were considered fixation breaks. They caused trials to be aborted with an auditory feedback to the subjects. On average across all trials the subject’s eye positions during both stimulus presentations remained within a circular window with a radius of less than 0.6 degrees.</p>
</sec>
<sec id="sec013">
<title>Attentional cues</title>
<p>Previous studies aimed at developing or testing the NMoA have used spatially separated target and distractor stimuli, which could have been selected by spatial attention. We used a transparent motion display containing two spatially overlapping moving RDPs, leaving feature-based attentional mechanisms as the sole selection mechanism for behavioral enhancement. Two types of cues were used to direct subjects’ attention to one of the two motion directions of the transparent motion display. The <italic>narrow focus cue</italic> was a single arrow pointing in one of the six reference directions, indicating that the relevant motion signal of the first stimulus presentation was likely to occur within a range of ±10 degrees around its heading. The <italic>broad focus cue</italic> consisted of three arrows, all pointing either towards the left or the right side, indicating that the relevant motion was likely to be right- or leftwards. Both cues were valid (i.e. the relevant motion occurred within ±10 degrees of the narrow focus cue and towards the side of the broad focus cue) in 75% of all trials and all subjects were verbally instructed and frequently reminded to also pay some attention to the uncued directions. The narrow focus cue was designed to enable subjects to direct their attention onto a narrow range (ca. 20 degrees) of possible target directions, while the broad focus cue was used to induce a much wider focus (ca. 110 degrees) of the feature-based attention field. In both cases, attention helped the subjects to preferentially focus on one of the two directions of the transparent motion stimulus for subsequent comparison with the single motion.</p>
<p>The frequency of occurrence for the different types of cues was balanced between cue directions and cue types, such that no cue direction or cue type was overrepresented. We determined the influence of feature-based attention on psychophysical performance by comparing validly and invalidly cued trials.</p>
</sec>
<sec id="sec014">
<title>Pre-tests</title>
<p>Pre-testing consisted of 2 to 6 sessions of 450 valid trials each. Pre-test trials were identical to regular trials, but contained no attentional cues. Furthermore, the coherence level of all stimuli was set to 51.2%. To measure each subject’s individual just noticeable difference (JND), we varied the direction change magnitude in 15 discrete steps from -14 to 14 degrees. We then fitted a psychometric function (cumulative Gaussian) for each subject and each reference direction. Subjects started the main experiment once they reached a comparable performance for all six reference directions, with little to no bias in their discrimination thresholds. The subject JND was defined as the slope of the cumulative normal fit of the performance pooled over all reference directions. Subjects were trained to perform the pre-task until they reached a JND smaller than 16 degrees in one complete session of testing, or until they aborted the experiment. Altogether, 23 subjects entered the pre-testing phase, out of which 8 subjects continued to the main experiment. Subjects aborting the experiment mostly reported that they found the task too demanding to commit to further training or testing. For subjects reaching the criterion, their JND from the last session of pre-testing was used throughout the main experiment (mean JND = 12.86, standard-deviation = 1.94).</p>
</sec>
<sec id="sec015">
<title>Data analysis</title>
<p>To test whether the two types of attentional cues led to measurable attentional effects, we compared each subject’s mean performance over all levels of coherences between both attentional conditions. We calculated performance as <italic>d</italic>′ = <italic>zscore</italic>(<italic>p</italic><sub><italic>CWcorrect</italic></sub>) − <italic>zscore</italic>(<italic>p</italic><sub><italic>CCWfailure</italic></sub>), where ‘p<sub>CWcorrect</sub>’ is defined as the proportion of clockwise responses to clockwise changes, and ‘p<sub>CCWfailure</sub>’ as the proportion of clockwise responses to counter-clockwise changes. Using paired t-tests we determined whether performance differed between trials with narrow and broad focus cues and confirmed that attention was deployed in line with each cue type, as indicated by a significant difference between validly and invalidly cued trials.</p>
<p>In order to determine whether attention affected performance by response or coherence gain we investigated separately for each attentional condition, how each subject’s performance changes with motion coherence. To obtain the coherence response function, we fitted a Naka-Rushton equation [<xref ref-type="bibr" rid="pcbi.1005225.ref043">43</xref>–<xref ref-type="bibr" rid="pcbi.1005225.ref045">45</xref>]
<disp-formula id="pcbi.1005225.e010">
<alternatives>
<graphic id="pcbi.1005225.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
to each experimental condition using a non-linear least-squares procedure. Using this equation, psychophysical performance <italic>d</italic>′ for each level of coherence <italic>c</italic> can be described by the asymptotic performance at high levels of coherence <italic>d</italic>′<sub><italic>max</italic></sub>, the coherence level at half asymptotic performance <italic>c</italic><sub>50</sub> and the slope of the function <italic>n</italic>. We tested with one-tailed, paired t-tests whether changes in <italic>c</italic><sub>50</sub> and <italic>d</italic>′<sub><italic>max</italic></sub> occurred from invalidly to validly cued trials for each attentional condition. Significant increases in <italic>d</italic>′<sub><italic>max</italic></sub> represent response gain effects and significant decreases in <italic>c</italic><sub>50</sub> represent coherence gain effects. The slopes of the corresponding coherence response functions for each attentional condition were constrained to be equal in all four fits per subject to minimize the number of free parameters. We validated this choice by comparing this reduced model (with a single exponent per subject) to those with two exponents per subject (one for each attentional condition) and to those with four exponents per subject (one for each attentional condition and cue validity). The reduced model with a single exponent per subject produced almost identical fits and was clearly preferred (due to its lower number of parameters) by AIC and BIC measures. We evaluated further-reduced models with shared parameters (<italic>d</italic>′<sub><italic>max</italic></sub> or <italic>c</italic><sub>50</sub>) either across or within attentional conditions, but found that no simpler model was superior to the one described above. A robust fit of the coherence response functions requires that the asymptotic performance saturates at high levels of coherence. We therefore excluded two subjects with performance increases of Δ<italic>d</italic>′ ≥ 1 between the two highest coherence levels, leaving a total of 6 subjects for the final analysis.</p>
<p>To determine the coherence gain and response gain changes between attentional conditions, we computed a modulation index for each of the gain enhancements:
<disp-formula id="pcbi.1005225.e011">
<alternatives>
<graphic id="pcbi.1005225.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005225.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
where <italic>ζ</italic> corresponds to one of the two fitted coefficients <italic>c</italic><sub>50</sub> or <italic>d</italic>′<sub><italic>max</italic></sub>. We calculated the differences in modulation magnitude between conditions and tested with paired t-tests if the effect sizes of coherence and response gain varied significantly between the two attentional conditions. All statistical tests were Bonferroni corrected for multiple comparisons. Data analysis was done using custom scripts in Matlab R2014a (MathWorks). We used the Palamedes routines [<xref ref-type="bibr" rid="pcbi.1005225.ref046">46</xref>] for fitting psychometric functions and the Matlab Curve Fitting toolbox (MathWorks) for the non-linear fitting.</p>
</sec>
<sec id="sec016">
<title>Model simulations</title>
<p>To simulate our empirical data with the NMoA, we used custom Matlab scripts, based on the code of Reynolds and Heeger [<xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>]. We changed the original code to use a circular von Mises distribution for both the stimulation and the attention fields’ theta dimension. Therefore we express the width of the feature-attention spotlight in terms of parameter <italic>κ</italic>, which is the concentration of the distribution around it’s mean (1/<italic>κ</italic> is roughly equivalent to <italic>σ</italic><sup>2</sup> of a gaussian). We confirmed that this modified model produces similar results to the original NMoA by comparing our results with the outcome of the Matlab scripts available on the authors’ website.</p>
<p>We modeled our empirical results by defining a stimulus that is infinite in space, since no spatial position inside the annulus carried more relevant signal than any other and thus spatial attention could not have impacted psychophysical performance. Consequently we assumed that for modeling purposes, spatial attention was evenly distributed across all spatial locations. The two directions of the transparent motion display were modeled as two narrow bands in the theta dimension, each with a concentration of <italic>κ</italic> = 33, corresponding to roughly 10 degrees <italic>σ</italic>. The means of the two signals were 135 degrees apart from each other, corresponding to the mean difference in motion directions of the transparent motion display.</p>
<p>Assuming a quasi-optimal attentional allocation according to the task design we then simulated an attentional field with either a narrow or a broad focus of feature-based attention. The exact choice of field width turned out to be not critical for the main finding (see <xref ref-type="sec" rid="sec002">Results</xref> section for details). The narrow focus was an enhancement with a concentration (angular extent) of <italic>κ</italic> = 15 around one of the signals. The broad focus was centered on the same direction (i.e. as if it were a horizontal movement), but enhanced a much broader range of directions around it (<italic>κ</italic> = 0.5, which corresponds roughly to 90 degrees <italic>σ</italic>). Our model MT population was defined to have Gaussian receptive fields with a spatial extend of <italic>σ</italic> = 5 degrees and a tuning width of <italic>σ</italic> = 37 degrees. The suppressive field was defined to have a spatial kernel width of <italic>σ</italic> = 20 degrees and a feature tuning width of <italic>σ</italic> = 180 degrees. The latter parameter was used since it is known that in motion selective area MT, surround tuning is present, but is generally very broad [<xref ref-type="bibr" rid="pcbi.1005225.ref047">47</xref>]. Overall, this biologically plausible set of parameters is very similar to the one used in previous simulations by Herrmann et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref019">19</xref>] or Reynolds &amp; Heeger [<xref ref-type="bibr" rid="pcbi.1005225.ref009">9</xref>].</p>
<p>We modeled increasing levels of coherence by increasing the value of the sensory input strength parameter <italic>c</italic>. In the NMoA, this essentially equates increases in coherence to increases in contrast. This choice (also made by Jazayeri and Movshon [<xref ref-type="bibr" rid="pcbi.1005225.ref048">48</xref>] in a related context) is supported by the physiological finding that MT units do not change their tuning for linear motion with changes in motion coherence [<xref ref-type="bibr" rid="pcbi.1005225.ref049">49</xref>]. In order to convert the modeled population activity into a prediction of behavioral performance, we assumed that task performance is dominated by the quality of decoding of the two motion directions of stimulus display 1. Consequently, we selected two units of the simulated population with their tuning centered on the corresponding directions of stimulus display 1 (out of which one was previously cued and thus in the focus of attention). We assumed that task performance on validly and invalidly cued trials is proportional to the values of the neurometric function for the attended and unattended unit respectively. A large value of the neurometric function translates to a greater signal-to-noise ratio for the neural representation and a better identification of the stimulus directions. Since the direction-difference between the sample and test directions was small, units tuned to the sample directions also responded strongly to test directions and received levels of attentional enhancement similar to units tuned to the test directions. Therefore, their neurometric functions would also be proportional to detection performance for presented test stimuli.</p>
<p>In order to obtain the neurometric functions for relevant units, we repeated the simulation for varying values of <italic>c</italic> (i.e. signal to noise ratios of the two bands in theta). Through appropriate rescaling with just one additional parameter, we converted the neuronal activity of the relevant unit (depending on cue validity) into psychophysical performance. Importantly, as shown by Pestilli et al. [<xref ref-type="bibr" rid="pcbi.1005225.ref020">20</xref>], such a readout which equates attentional effects on neuronal response functions with those on behavioral psychometric functions (after a rescaling) leads to the same conclusions as those given by a more detailed implementation of an ideal likelihood-based observer [<xref ref-type="bibr" rid="pcbi.1005225.ref048">48</xref>]. Even when using this ideal observer to predict behavioral psychometric functions from the underlying modeled neuronal representation, the attentional effect on the behavioral psychometric function mimics the attentional effect on the underlying neuronal functions.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Geoffrey Boynton, Cliodhna Quigley and Janina Hüer for their comments on the manuscript and helpful discussions. We also acknowledge the work of Florian Kasten, who helped to collect the pilot data.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005225.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>, <name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>. <article-title>Feature-based attention in visual cortex</article-title>. <source>Trends in Neurosciences</source>. <year>2006</year>;<volume>29</volume>: <fpage>317</fpage>–<lpage>322</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2006.04.001" xlink:type="simple">10.1016/j.tins.2006.04.001</ext-link></comment> <object-id pub-id-type="pmid">16697058</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Craven</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Rosen</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Kwong</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Treisman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Savoy</surname> <given-names>RL</given-names></name>. <article-title>Voluntary attention modulates fMRI activity in human MT–MST</article-title>. <source>Neuron</source>. <year>1997</year>;<volume>18</volume>: <fpage>591</fpage>–<lpage>598</lpage>. <object-id pub-id-type="pmid">9136768</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saenz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Buracas</surname> <given-names>GT</given-names></name>, <name name-style="western"><surname>Boynton</surname> <given-names>GM</given-names></name>. <article-title>Global effects of feature-based attention in human visual cortex</article-title>. <source>Nature Neuroscience</source>. <year>2002</year>;<volume>5</volume>: <fpage>631</fpage>–<lpage>632</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn876" xlink:type="simple">10.1038/nn876</ext-link></comment> <object-id pub-id-type="pmid">12068304</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stoppel</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Boehler</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Strumpf</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Heinze</surname> <given-names>H-J</given-names></name>, <name name-style="western"><surname>Noesselt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hopf</surname> <given-names>J-M</given-names></name>, <etal>et al</etal>. <article-title>Feature-based attention modulates direction-selective hemodynamic activity within human MT</article-title>. <source>Hum Brain Mapp</source>. <year>2011</year>;<volume>32</volume>: <fpage>2183</fpage>–<lpage>2192</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hbm.21180" xlink:type="simple">10.1002/hbm.21180</ext-link></comment> <object-id pub-id-type="pmid">21305663</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name>. <article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title>. <source>Nature</source>. <year>1999</year>;<volume>399</volume>: <fpage>575</fpage>–<lpage>579</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/21176" xlink:type="simple">10.1038/21176</ext-link></comment> <object-id pub-id-type="pmid">10376597</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>. <article-title>Feature-based attention increases the selectivity of population responses in primate visual cortex</article-title>. <source>Current Biology</source>. <year>2004</year>;<volume>14</volume>: <fpage>744</fpage>–<lpage>751</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2004.04.028" xlink:type="simple">10.1016/j.cub.2004.04.028</ext-link></comment> <object-id pub-id-type="pmid">15120065</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khayat</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Niebergall</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name>. <article-title>Attention differentially modulates similar neuronal responses evoked by varying contrast and direction stimuli in area MT</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>: <fpage>2188</fpage>–<lpage>2197</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5314-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5314-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20147546</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Hayden</surname> <given-names>BY</given-names></name>, <name name-style="western"><surname>Mazer</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>59</volume>: <fpage>509</fpage>–<lpage>521</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.07.001" xlink:type="simple">10.1016/j.neuron.2008.07.001</ext-link></comment> <object-id pub-id-type="pmid">18701075</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>The normalization model of attention</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>61</volume>: <fpage>168</fpage>–<lpage>185</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.01.002" xlink:type="simple">10.1016/j.neuron.2009.01.002</ext-link></comment> <object-id pub-id-type="pmid">19186161</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McAdams</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JH</given-names></name>. <article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>: <fpage>431</fpage>–<lpage>441</lpage>. <object-id pub-id-type="pmid">9870971</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Pasternak</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name>. <article-title>Attention increases sensitivity of V4 neurons</article-title>. <source>Neuron</source>. <year>2000</year>;<volume>26</volume>: <fpage>703</fpage>–<lpage>714</lpage>. <object-id pub-id-type="pmid">10896165</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martínez-Trujillo</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>. <article-title>Attentional modulation strength in cortical area MT depends on stimulus contrast</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>35</volume>: <fpage>365</fpage>–<lpage>370</lpage>. <object-id pub-id-type="pmid">12160753</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williford</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>. <article-title>Effects of spatial attention on contrast response functions in macaque area V4</article-title>. <source>Journal of Neurophysiology</source>. <year>2006</year>;<volume>96</volume>: <fpage>40</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.01207.2005" xlink:type="simple">10.1152/jn.01207.2005</ext-link></comment> <object-id pub-id-type="pmid">16772516</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Basso</surname> <given-names>MA</given-names></name>. <article-title>Preparing to move increases the sensitivity of superior colliculus neurons</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>: <fpage>4561</fpage>–<lpage>4577</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5683-07.2008" xlink:type="simple">10.1523/JNEUROSCI.5683-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18434535</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thiele</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pooresmaeili</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Delicato</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Herrero</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>Additive effects of attention and stimulus contrast in primary visual cortex</article-title>. <source>Cerebral Cortex</source>. <year>2009</year>;<volume>19</volume>: <fpage>2970</fpage>–<lpage>2981</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhp070" xlink:type="simple">10.1093/cercor/bhp070</ext-link></comment> <object-id pub-id-type="pmid">19372142</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>. <article-title>A normalization model of attentional modulation of single unit responses</article-title>. <source>PLoS ONE</source>. <year>2009</year>;<volume>4</volume>: <fpage>e4651</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0004651" xlink:type="simple">10.1371/journal.pone.0004651</ext-link></comment> <object-id pub-id-type="pmid">19247494</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herrmann</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Montaser-Kouhsari</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>When size matters: attention affects performance by contrast or response gain</article-title>. <source>Nature Neuroscience</source>. <year>2010</year>;<volume>13</volume>: <fpage>1554</fpage>–<lpage>1559</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2669" xlink:type="simple">10.1038/nn.2669</ext-link></comment> <object-id pub-id-type="pmid">21057509</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hara</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Pestilli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>. <article-title>Differing effects of attention in single-units and populations are well predicted by heterogeneous tuning and the normalization model of attention</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>.</mixed-citation></ref>
<ref id="pcbi.1005225.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herrmann</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>. <article-title>Feature-based attention enhances performance by increasing response gain</article-title>. <source>Vision Research</source>. <year>2012</year>;<volume>74</volume>: <fpage>10</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2012.04.016" xlink:type="simple">10.1016/j.visres.2012.04.016</ext-link></comment> <object-id pub-id-type="pmid">22580017</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pestilli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ling</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>. <article-title>A population-coding model of attention’s influence on contrast response: Estimating neural effects from psychophysical data</article-title>. <source>Vision Research</source>. <year>2009</year>;<volume>49</volume>: <fpage>1144</fpage>–<lpage>1153</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2008.09.018" xlink:type="simple">10.1016/j.visres.2008.09.018</ext-link></comment> <object-id pub-id-type="pmid">18926845</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>DK</given-names></name>, <name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Braun</surname> <given-names>J</given-names></name>. <article-title>Attention activates winner-take-all competition among visual filters</article-title>. <source>Nature Neuroscience</source>. <year>1999</year>;<volume>2</volume>: <fpage>375</fpage>–<lpage>381</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/7286" xlink:type="simple">10.1038/7286</ext-link></comment> <object-id pub-id-type="pmid">10204546</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>How MT cells analyze the motion of visual patterns</article-title>. <source>Nature Neuroscience</source>. <year>2006</year>;<volume>9</volume>: <fpage>1421</fpage>–<lpage>1431</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1786" xlink:type="simple">10.1038/nn1786</ext-link></comment> <object-id pub-id-type="pmid">17041595</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ni</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Ray</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>. <article-title>Tuned normalization explains the size of attention modulations</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>73</volume>: <fpage>803</fpage>–<lpage>813</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.01.006" xlink:type="simple">10.1016/j.neuron.2012.01.006</ext-link></comment> <object-id pub-id-type="pmid">22365552</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Britten</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Responses of neurons in macaque MT to stochastic motion signals</article-title>. <source>Vis Neurosci</source>. <year>2009</year>;<volume>10</volume>: <fpage>1157</fpage>–<lpage>1169</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005225.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>A direct quantitative relationship between the functional properties of human and macaque V5</article-title>. <source>Nature Neuroscience</source>. <year>2000</year>;<volume>3</volume>: <fpage>716</fpage>–<lpage>723</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/76673" xlink:type="simple">10.1038/76673</ext-link></comment> <object-id pub-id-type="pmid">10862705</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakamura</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kashii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nagamine</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Matsui</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hashimoto</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Honda</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Human V5 demonstrated by magnetoencephalography using random dot kinematograms of different coherence levels</article-title>. <source>Neuroscience Research</source>. <year>2003</year>;<volume>46</volume>: <fpage>423</fpage>–<lpage>433</lpage>. <object-id pub-id-type="pmid">12871764</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daliri</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Kozyrev</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>. <article-title>Attention enhances stimulus representations in macaque visual cortex without affecting their signal-to-noise level</article-title>. <source>Nature Publishing Group</source>. <year>2016</year>;<volume>6</volume>: <fpage>27666</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep27666" xlink:type="simple">10.1038/srep27666</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005225.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boynton</surname> <given-names>GM</given-names></name>. <article-title>A framework for describing the effects of attention on visual responses</article-title>. <source>Vision Research</source>. <year>2009</year>;<volume>49</volume>: <fpage>1129</fpage>–<lpage>1143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2008.11.001" xlink:type="simple">10.1016/j.visres.2008.11.001</ext-link></comment> <object-id pub-id-type="pmid">19038281</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghose</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>. <article-title>Spatial summation can explain the attentional modulation of neuronal responses to multiple stimuli in area V4</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>: <fpage>5115</fpage>–<lpage>5126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0138-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0138-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18463265</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghose</surname> <given-names>GM</given-names></name>. <article-title>Attentional modulation of visual responses by flexible input gain</article-title>. <source>Journal of Neurophysiology</source>. <year>2009</year>;<volume>101</volume>: <fpage>2089</fpage>–<lpage>2106</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90654.2008" xlink:type="simple">10.1152/jn.90654.2008</ext-link></comment> <object-id pub-id-type="pmid">19193776</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pestilli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Carrasco</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>. <article-title>Attentional enhancement via selection and pooling of early sensory responses in human visual cortex</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>: <fpage>832</fpage>–<lpage>846</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.09.025" xlink:type="simple">10.1016/j.neuron.2011.09.025</ext-link></comment> <object-id pub-id-type="pmid">22153378</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zénon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Krauzlis</surname> <given-names>RJ</given-names></name>. <article-title>Attention deficits without cortical neuronal deficits</article-title>. <source>Nature</source>. <year>2012</year>;<volume>489</volume>: <fpage>434</fpage>–<lpage>437</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11497" xlink:type="simple">10.1038/nature11497</ext-link></comment> <object-id pub-id-type="pmid">22972195</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Peterson</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Pham</surname> <given-names>BT</given-names></name>, <name name-style="western"><surname>Droll</surname> <given-names>JA</given-names></name>. <article-title>Statistical decision theory to relate neurons to behavior in the study of covert visual attention</article-title>. <source>Vision Research</source>. <year>2009</year>;<volume>49</volume>: <fpage>1097</fpage>–<lpage>1128</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2008.12.008" xlink:type="simple">10.1016/j.visres.2008.12.008</ext-link></comment> <object-id pub-id-type="pmid">19138699</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eckstein</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Mack</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Liston</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Bogush</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Menzel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Krauzlis</surname> <given-names>RJ</given-names></name>. <article-title>Rethinking human visual attention: Spatial cueing effects and optimality of decisions by honeybees, monkeys and humans</article-title>. <source>Vision Research</source>. <year>2013</year>;<volume>85</volume>: <fpage>5</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2012.12.011" xlink:type="simple">10.1016/j.visres.2012.12.011</ext-link></comment> <object-id pub-id-type="pmid">23298793</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dosher</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Z-L</given-names></name>. <article-title>Mechanisms of perceptual attention in precuing of location</article-title>. <source>Vision Research</source>. <year>2000</year>;<volume>40</volume>: <fpage>1269</fpage>–<lpage>1292</lpage>. <object-id pub-id-type="pmid">10788639</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borji</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>. <article-title>Optimal attentional modulation of a neural population</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>: <fpage>358</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005225.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ames</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Lindsey</surname> <given-names>DT</given-names></name>. <article-title>Measuring the effect of attention on simple visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1993</year>;<volume>19</volume>: <fpage>108</fpage>–<lpage>130</lpage>. <object-id pub-id-type="pmid">8440980</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>J</given-names></name>. <article-title>Set-size effects in visual search: the effect of attention is independent of the stimulus for simple tasks</article-title>. <source>Vision Research</source>. <year>1994</year>;<volume>34</volume>: <fpage>1703</fpage>–<lpage>1721</lpage>. <object-id pub-id-type="pmid">7941377</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>JHR</given-names></name>. <article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title>. <source>Nature Neuroscience</source>. <year>2009</year>;<volume>12</volume>: <fpage>1594</fpage>–<lpage>1600</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2439" xlink:type="simple">10.1038/nn.2439</ext-link></comment> <object-id pub-id-type="pmid">19915566</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ruff</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>. <article-title>Attention can either increase or decrease spike count correlations in visual cortex</article-title>. <source>Nature Neuroscience</source>. <year>2014</year>;<volume>17</volume>: <fpage>1591</fpage>–<lpage>1597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3835" xlink:type="simple">10.1038/nn.3835</ext-link></comment> <object-id pub-id-type="pmid">25306550</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Sundberg</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>JH</given-names></name>. <article-title>Differential attention-dependent response modulation across cell classes in macaque visual area V4</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>55</volume>: <fpage>131</fpage>–<lpage>141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.06.018" xlink:type="simple">10.1016/j.neuron.2007.06.018</ext-link></comment> <object-id pub-id-type="pmid">17610822</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niebergall</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Khayat</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Treue</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name>. <article-title>Expansion of MT neurons excitatory receptive fields during covert attentive tracking</article-title>. <source>Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>: <fpage>15499</fpage>–<lpage>15510</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2822-11.2011" xlink:type="simple">10.1523/JNEUROSCI.2822-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22031896</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naka</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Rushton</surname> <given-names>WAH</given-names></name>. <article-title>S-potentials from luminosity units in the retina of fish (Cyprinidae)</article-title>. <source>The Journal of Physiology</source>. <year>1966</year>;<volume>185</volume>: <fpage>587</fpage>–<lpage>599</lpage>. <object-id pub-id-type="pmid">5918060</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Finn</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Priebe</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Ferster</surname> <given-names>D</given-names></name>. <article-title>The emergence of contrast-invariant orientation tuning in simple cells of cat visual cortex</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>54</volume>: <fpage>137</fpage>–<lpage>152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.02.029" xlink:type="simple">10.1016/j.neuron.2007.02.029</ext-link></comment> <object-id pub-id-type="pmid">17408583</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Busse</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wade</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>. <article-title>Representation of concurrent stimuli by population activity in visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>64</volume>: <fpage>931</fpage>–<lpage>942</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.11.004" xlink:type="simple">10.1016/j.neuron.2009.11.004</ext-link></comment> <object-id pub-id-type="pmid">20064398</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref046"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Prins N, Kingdom FAA. Kingdom FAA. Palamedes: Matlab routines for analyzing psychophysical data. 2009.</mixed-citation></ref>
<ref id="pcbi.1005225.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hunter</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Born</surname> <given-names>RT</given-names></name>. <article-title>Stimulus-dependent modulation of suppressive influences in MT</article-title>. <source>Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>: <fpage>678</fpage>–<lpage>686</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4560-10.2011" xlink:type="simple">10.1523/JNEUROSCI.4560-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21228177</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Optimal representation of sensory information by neural populations</article-title>. <source>Nature Neuroscience</source>. <year>2006</year>;<volume>9</volume>: <fpage>690</fpage>–<lpage>696</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1691" xlink:type="simple">10.1038/nn1691</ext-link></comment> <object-id pub-id-type="pmid">16617339</object-id></mixed-citation></ref>
<ref id="pcbi.1005225.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Britten</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Tuning bandwidths for near-threshold stimuli in area MT</article-title>. <source>Journal of Neurophysiology</source>. <year>1998</year>;<volume>80</volume>: <fpage>762</fpage>–<lpage>770</lpage>. <object-id pub-id-type="pmid">9705467</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>