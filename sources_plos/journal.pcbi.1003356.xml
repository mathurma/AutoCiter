<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00544</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003356</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Beyond GLMs: A Generative Mixture Modeling Approach to Neural System Identification</article-title>
<alt-title alt-title-type="running-head">Beyond GLMs: A Generative Approach</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Theis</surname><given-names>Lucas</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Chagas</surname><given-names>Andrè Maia</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Arnstein</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname><given-names>Cornelius</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Bethge</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref><xref ref-type="aff" rid="aff6"><sup>6</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Werner Reichardt Centre for Integrative Neuroscience, Tübingen, Germany</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Graduate School of Neural Information Processing, University of Tübingen, Tübingen, Germany</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Hertie Institute for Clinical Brain Research, Tübingen, Germany</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Graduate School of Neural and Behavioural Sciences, University of Tübingen, Tübingen, Germany</addr-line></aff>
<aff id="aff5"><label>5</label><addr-line>Bernstein Center for Computational Neuroscience, Tübingen, Germany</addr-line></aff>
<aff id="aff6"><label>6</label><addr-line>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Einhäuser</surname><given-names>Wolfgang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Philipps-University Marburg, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">lucas@bethgelab.org</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: LT AMC CS MB. Performed the experiments: LT AMC DA CS. Analyzed the data: LT AMC CS MB. Wrote the paper: LT MB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>11</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>21</day><month>11</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>11</issue>
<elocation-id>e1003356</elocation-id>
<history>
<date date-type="received"><day>1</day><month>4</month><year>2013</year></date>
<date date-type="accepted"><day>6</day><month>10</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Theis et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p><italic>Generalized linear models</italic> (GLMs) represent a popular choice for the probabilistic characterization of neural spike responses. While GLMs are attractive for their computational tractability, they also impose strong assumptions and thus only allow for a limited range of stimulus-response relationships to be discovered. Alternative approaches exist that make only very weak assumptions but scale poorly to high-dimensional stimulus spaces. Here we seek an approach which can gracefully interpolate between the two extremes. We extend two frequently used special cases of the GLM—a linear and a quadratic model—by assuming that the spike-triggered and non-spike-triggered distributions can be adequately represented using Gaussian mixtures. Because we derive the model from a generative perspective, its components are easy to interpret as they correspond to, for example, the spike-triggered distribution and the interspike interval distribution. The model is able to capture complex dependencies on high-dimensional stimuli with far fewer parameters than other approaches such as histogram-based methods. The added flexibility comes at the cost of a non-concave log-likelihood. We show that in practice this does not have to be an issue and the mixture-based model is able to outperform generalized linear and quadratic models.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>An essential goal of sensory systems neuroscience is to characterize the functional relationship between neural responses and external stimuli. Of particular interest are the nonlinear response properties of single cells. Inherently linear approaches such as generalized linear modeling can nevertheless be used to fit nonlinear behavior by choosing an appropriate feature space for the stimulus. This requires, however, that one has already obtained a good understanding of a cells nonlinear properties, whereas more flexible approaches are necessary for the characterization of unexpected nonlinear behavior. In this work, we present a generalization of some frequently used generalized linear models which enables us to automatically extract complex stimulus-response relationships from recorded data. We show that our model can lead to substantial quantitative and qualitative improvements over generalized linear and quadratic models, which we illustrate on the example of primary afferents of the rat whisker system.</p>
</abstract>
<funding-group><funding-statement>This work was partially supported by the German Ministry of Education, Science, Research and Technology through the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002), the German Excellency Initiative through the Centre for Integrative Neuroscience Tübingen (EXC307) and the German Research Foundation (SCHW 577/10-2). We also acknowledge support by the Open Access Publishing Fund of the University of Tübingen. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>To account for the stochasticity inherent to neural responses, single cells as well as populations of cells are often characterized in terms of a probabilistic model. A popular choice for this task are generalized linear models (GLMs) and related approaches <xref ref-type="bibr" rid="pcbi.1003356-McCullagh1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-Gerwinn1">[6]</xref>. These models can often be chosen such that the corresponding maximum likelihood problem is a convex optimization problem where a global optimum can be found. This guarantee comes at a price, as GLMs tightly constrain the computations which can be performed on the input. More complex computations can nevertheless be implemented by choosing a nonlinear feature representation of the input which is then fed into the linear model. In practice, however, it is typically very challenging to select the appropriate feature space because it presupposes a deeper understanding of the cell's nonlinear behavior or unfeasibly large amounts of data.</p>
<p>Several approaches have been suggested to overcome the limitations of the generalized linear model. A natural extension is given by <italic>generalized quadratic models</italic> <xref ref-type="bibr" rid="pcbi.1003356-Pillow2">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-Rajan1">[9]</xref>. While a quadratic model represents a true generalization of a linear model, it can also be viewed as a linear model with a quadratric extension of the feature space (and, depending on the parametrization, some additional constraints on the parameters). Consequently, it shares similar limitations. A linear combination of quadratic features might still fail to represent the kind of stimulus properties a neuron responds to, but going to higher-dimensional general-purpose feature spaces quickly leads to overfitting. The number of parameters which need to be estimated grows linearly with the stimulus dimensionality in a linear model, quadratically in a quadratic model, and correspondingly faster if one uses a feature space of higher order.</p>
<p>An alternative approach is offered by nonparametric methods such as <italic>maximally informative dimensions</italic> (MID) <xref ref-type="bibr" rid="pcbi.1003356-Sharpee1">[10]</xref>. Here, one first seeks a projection of the stimulus onto a lower-dimensional subspace such that as much information as possible is retained about the presence or absence of a spike. Afterwards, histograms are used to map out the nonlinear dependence of the neuron on the projected stimulus. This approach has the advantage that it can, at least in principle, capture arbitrary dependencies on the stimulus. However, the number of parameters that need to be estimated grows exponentially with the dimensionality of the stimulus subspace. This limits the approach to cells which are selective for only a few stimulus dimensions, although nonlinear extensions of this approach exist <xref ref-type="bibr" rid="pcbi.1003356-Rajan2">[11]</xref>.</p>
<p>Here, we explore a different tradeoff. We derive a much more flexible neuron model for single cells which can, at least in principle, approximate arbitrary dependencies on the stimulus. The model can be viewed as generalizing generalized linear and quadratic models, but in contrast to quadratic models cannot easily be reduced to a GLM by choosing a different representation of the stimulus. Nonlinear stimulus features are directly learned from the data by maximizing the model's likelihood and do not need to be hand picked. The number of parameters of the model still grows only quadratically with the dimensionality of the stimulus, and the complexity of the model can be tuned to take into account the cell's complexity and the amount of data available. We demonstrate that optimizing this model is feasible in practice and can lead to a better fit than either generalized linear or quadratic models.</p>
</sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<p>In the following—after briefly reviewing generalized linear and quadratic models—we introduce a new model for single cell responses and discuss its properties.</p>
<sec id="s2a">
<title>Ethics statement</title>
<p>All experimental and surgical procedures were carried out in accordance with the policy on the use of animals in neuroscience research of the Society for Neuroscience and the German law.</p>
</sec><sec id="s2b">
<title>Generalized linear and quadratic models</title>
<p>In a GLM it is assumed that the output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e001" xlink:type="simple"/></inline-formula> conditioned on some input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e002" xlink:type="simple"/></inline-formula> is distributed according to an exponential family and that the expected output is given by<disp-formula id="pcbi.1003356.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e003" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e004" xlink:type="simple"/></inline-formula> is an invertible nonlinearity. Parameters of the model are the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e005" xlink:type="simple"/></inline-formula> and potentially additional parameters of the exponential family. In the following, we will assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e006" xlink:type="simple"/></inline-formula> is a representation of the stimulus and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e007" xlink:type="simple"/></inline-formula> indicates the presence or absence of a spike.</p>
<p>A special case of the GLM applicable to our problem is, for instance, the linear-nonlinear-Bernoulli (LNB) model, where the exponential family is given by the Bernoulli distribution. As nonlinearity we might choose the sigmoidal logistic function,<disp-formula id="pcbi.1003356.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e008" xlink:type="simple"/><label>(1)</label></disp-formula>In the following, we will derive this linear model from a generative modeling point of view. This will help to motivate and see the connections to the extension presented later.</p>
<p>Let us consider the distribution over the stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e009" xlink:type="simple"/></inline-formula> conditioned on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e010" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e011" xlink:type="simple"/></inline-formula> equals 1, this distribution corresponds to the spike-triggered distribution. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e012" xlink:type="simple"/></inline-formula> equals 0, we will call it the <italic>non-spike-triggered distribution</italic>. At least for the moment, let us assume that both distributions are Gaussian, that is,<disp-formula id="pcbi.1003356.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e013" xlink:type="simple"/></disp-formula>with means <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e014" xlink:type="simple"/></inline-formula> and covariances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e015" xlink:type="simple"/></inline-formula>. Bayes' rule allows us to turn these assumptions into a probabilistic model of the neuron's behavior,<disp-formula id="pcbi.1003356.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e016" xlink:type="simple"/></disp-formula>Using a few simple calculations, the probability of observing a spike, or <italic>firing rate</italic>, can be seen to be<disp-formula id="pcbi.1003356.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e017" xlink:type="simple"/><label>(2)</label></disp-formula>where<disp-formula id="pcbi.1003356.e018"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e018" xlink:type="simple"/><label>(3)</label></disp-formula>Using our assumption of Gaussianity, this reduces to<disp-formula id="pcbi.1003356.e019"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e019" xlink:type="simple"/><label>(4)</label></disp-formula>where we have performed the reparametrization<disp-formula id="pcbi.1003356.e020"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e020" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003356.e021"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e021" xlink:type="simple"/></disp-formula>and the bias term is given by<disp-formula id="pcbi.1003356.e022"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e022" xlink:type="simple"/></disp-formula>If the spike-triggered and non-spike-triggered covariances are assumed identical, the quadratic term vanishes and we obtain the linear-nonlinear-Bernoulli model from above. Without this assumption, we are left with a quadratic model <xref ref-type="bibr" rid="pcbi.1003356-Pillow2">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-Rajan1">[9]</xref>.</p>
<p>The unconstrained quadratic model is equivalent to a GLM with a quadratic extension of the feature space, since<disp-formula id="pcbi.1003356.e023"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e023" xlink:type="simple"/><label>(5)</label></disp-formula>is linear in the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e024" xlink:type="simple"/></inline-formula>. In practice, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e025" xlink:type="simple"/></inline-formula> is often replaced by a low-rank approximation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e026" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003356-Pillow2">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-Rajan1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003356-Fitzgerald1">[12]</xref>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e027" xlink:type="simple"/></inline-formula> controls the rank. The quadratic form in this case is given by<disp-formula id="pcbi.1003356.e028"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e028" xlink:type="simple"/><label>(6)</label></disp-formula>When choosing this parametrization, the optimization is no longer a convex problem <xref ref-type="bibr" rid="pcbi.1003356-Rajan1">[9]</xref> and the model no longer a GLM. In the following, we will use “quadratic model” only to refer to the unconstrained version—a GLM with a quadratic feature space—and “linear model” to refer to the GLM without quadratic features.</p>
</sec><sec id="s2c">
<title>Spike-triggered mixture model</title>
<p>The generative point of view immediately suggests generalizations by loosening the assumptions of Gaussian distributed spike-triggered and non-spike-triggered stimuli. In the following, we consider mixtures of Gaussians as possible candidates,<disp-formula id="pcbi.1003356.e029"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e029" xlink:type="simple"/></disp-formula>Mixture models provide a good compromise between the assumptions of the tightly constrained generalized linear models and nonparametric approaches such as histograms. By plugging the mixture distributions into <xref ref-type="disp-formula" rid="pcbi.1003356.e018">Equation 3</xref>, we obtain a new neuron model whose complexity can be controlled by adjusting the number of mixture components. We dub this model the <italic>spike-triggered mixture model</italic> (STM).</p>
<p>In the same manner that we have derived a model for the neuron's dependency on the stimulus, we can incorporate dependence on other features as well. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e030" xlink:type="simple"/></inline-formula> be the time past since the neuron fired its last spike. Using Bayes' rule, we obtain<disp-formula id="pcbi.1003356.e031"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e031" xlink:type="simple"/></disp-formula>where here we have made the additional assumption that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e033" xlink:type="simple"/></inline-formula> are independent given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e034" xlink:type="simple"/></inline-formula>. This assumption is also known as the <italic>naive Bayes</italic> assumption and is often employed in classification. It has empirically been observed that naive Bayes classifiers often perform well even when the assumption of independence is not met <xref ref-type="bibr" rid="pcbi.1003356-Zhang1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003356-Bishop1">[14]</xref>.</p>
<p>Taken together, the input to the sigmoid nonlinearity (<xref ref-type="disp-formula" rid="pcbi.1003356.e017">Equation 2</xref>) is given by<disp-formula id="pcbi.1003356.e035"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e035" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e036" xlink:type="simple"/></inline-formula> represents the prior probability of observing a spike and we have used histograms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e037" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e038" xlink:type="simple"/></inline-formula> to represent the interval distributions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e039" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003356-g001">Figure 1</xref>). Note that if we do not constrain the parameters, there are several redundancies in this parametrization. For example, we can multiply both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e040" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e041" xlink:type="simple"/></inline-formula> by a common factor without changing the model's predictions. If we reparametrize the model to get rid of redundancies and in addition assume that one mixture component is enough to represent the non-spike-triggered distribution, the input to the sigmoid takes the much simpler form<disp-formula id="pcbi.1003356.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e042" xlink:type="simple"/><label>(8)</label></disp-formula>The assumption of Gaussian distributed non-spike-triggered stimuli is sensible, for instance, if an <italic>a priori</italic> Gaussian distributed stimulus is used to drive the neuron and the width of each bin of the spike train is small such that the posterior probability of observing a spike is generally also small, since in this case<disp-formula id="pcbi.1003356.e043"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e043" xlink:type="simple"/></disp-formula>The spike history dependent term on the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equation 8</xref> can also be written in terms of a linear filter,<disp-formula id="pcbi.1003356.e044"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e044" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e045" xlink:type="simple"/></inline-formula> represents the spike history, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e046" xlink:type="simple"/></inline-formula> is the unit vector with zeros everywhere except at the position of the most recent spike. That is, the only difference to a linear model with history dependent term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e047" xlink:type="simple"/></inline-formula> is that here all but one spike are suppressed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e048" xlink:type="simple"/></inline-formula>. In our experiments, we found that the two forms of spike history dependency worked equally well for most cells.</p>
<fig id="pcbi-1003356-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003356.g001</object-id><label>Figure 1</label><caption>
<title>Illustration of the spike-triggered mixture model (STM).</title>
<p><bold>A</bold>. A sigmoidal nonlinearity is applied to a log-likelihood ratio of two mixtures of Gaussians to determine the firing rate of the model, which is then used to generate spikes. <bold>B</bold>. By making a naive Bayes assumption, additional information and measurements such as interspike interval distributions can easily be incorporated into the model in the form of additional log-likelihood ratios.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003356.g001" position="float" xlink:type="simple"/></fig>
<p>It is instructive to compare <xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equation 8</xref> with <xref ref-type="disp-formula" rid="pcbi.1003356.e019">Equation 4</xref>. While the quadratic model can be cast into the form of a linear model with a quadratic feature space, this is in general not possible for the STM. The function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e049" xlink:type="simple"/></inline-formula> is also known as <italic>soft maximum</italic>, since it can be viewed as a smooth approximation to the maximum of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e050" xlink:type="simple"/></inline-formula>. Our model is thus effectively taking the maximum of the responses of a number of quadratic models. Also note that the number of parameters is only a constant times the number of parameters of the quadratic model, which means it still grows only quadratically in the number of stimulus dimensions. But the number of parameters can be reduced further, as discussed in the next section.</p>
</sec><sec id="s2d">
<title>Reducing the number of parameters</title>
<p>Assuming a single non-spike-triggered mixture component as in <xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equation 8</xref> and ignoring the spike history for the moment, the number of parameters of the STM grows as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e051" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e052" xlink:type="simple"/></inline-formula> is the stimulus dimensionality and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e053" xlink:type="simple"/></inline-formula> is the number of mixture components. This growth might still be impractical where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e054" xlink:type="simple"/></inline-formula> is large or the amount of available data is small, as is often the case with neural data.</p>
<p>To reduce the number of parameters, we can employ the same trick as for the quadratic model and replace the matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e055" xlink:type="simple"/></inline-formula> by low-rank approximations (<xref ref-type="disp-formula" rid="pcbi.1003356.e028">Equation 6</xref>). If we additionally share features <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e056" xlink:type="simple"/></inline-formula> between the different components, we obtain<disp-formula id="pcbi.1003356.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e057" xlink:type="simple"/><label>(9)</label></disp-formula>The number of parameters now grows as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e058" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e059" xlink:type="simple"/></inline-formula> is the number of quadratic features <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e060" xlink:type="simple"/></inline-formula> contributing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e061" xlink:type="simple"/></inline-formula> parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e062" xlink:type="simple"/></inline-formula> is the number of coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e063" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e064" xlink:type="simple"/></inline-formula> is the number of parameters added by the linear features <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e065" xlink:type="simple"/></inline-formula>. That is, for fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e066" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e067" xlink:type="simple"/></inline-formula>, the number of parameters is linear in the number of stimulus dimensions. We will refer to this variant of the model as the <italic>factored STM</italic>.</p>
</sec><sec id="s2e">
<title>Experimental setup</title>
<p>We tested our model on spike trains obtained from 18 whisker-sensitive trigeminal ganglion cells of adult Sprague-Dawley rats. Recordings were made with a single electrode (sampling frequency: 20 kHz, bandpass filter: 300–5000 Hz). Manual stimulation was used to identify which whisker the neuron innervated as well as the approximate preferred direction of the whisker, after which the whisker was placed inside a plastic tube driven by a metal stimulator arm. The stimulator arm was programmed to deliver low-pass filtered (100 Hz) Gaussian white noise stimulation along the neuron's preferred movement direction. Stimulation was divided into 50 <italic>unfrozen trials</italic> in which the stimulation sequence varied between trials, and 50 <italic>frozen trials</italic> in which a Gaussian white noise sequence was generated for the first trial only and then repeated for each subsequent trial. Spikes were extracted offline on the basis of waveform shape and all cells were categorized as either <italic>slowly adapting</italic> (SA) or <italic>rapidly adapting</italic> (RA). Example spike trains of two cells for frozen stimuli are shown in <xref ref-type="fig" rid="pcbi-1003356-g002">Figure 2</xref>.</p>
<fig id="pcbi-1003356-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003356.g002</object-id><label>Figure 2</label><caption>
<title>Spike trains generated by real and model neurons.</title>
<p>Stimuli corresponding to the spike trains are shown at the top. The first row below the stimulus shows spike trains and interspike interval distributions generated by one <italic>slowly adapting</italic> (<bold>A</bold>) and one <italic>rapidly adapting</italic> cell (<bold>B</bold>) of the rat's whisker system. The two cells shown are the SA cell and RA cell where the quantitative improvement in performance gained by using an STM over a quadratic model was largest.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003356.g002" position="float" xlink:type="simple"/></fig>
<p>We extracted 10 ms windows from the stimulus and reduced their dimensionality by keeping the first 10 principal components (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e068" xlink:type="simple"/></inline-formula> explained variance). We also extracted 25 ms of the spike history preceding each bin of the spike train. The dimensionality of the spike history was reduced to 100 by binning spikes into 100 equally sized bins of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e069" xlink:type="simple"/></inline-formula> width (no bin contained more than 1 spike). We then removed all but the most recent spike from the spike history and used this as input to all models. A linear projection of this vector is equivalent to the form of spike history dependency in <xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equation 8</xref>.</p>
<p>Filters of generalized linear models were first trained assuming a sigmoid nonlinearity. Together with a Bernoulli output distribution, this guarantees a concave log-likelihood such that an optimal solution can be found. Afterwards, we replaced the sigmoid nonlinearity with a more flexible nonlinearity consisting of a sum of Gaussian blobs,<disp-formula id="pcbi.1003356.e070"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e070" xlink:type="simple"/></disp-formula>where the hyperbolic tangent ensures that the predicted probability of a spike does not exceed 1. We jointly optimized the parameters of this nonlinearity and the linear filter by alternately maximizing the average log-likelihood of the linear-nonlinear model using limited-memory BFGS <xref ref-type="bibr" rid="pcbi.1003356-Byrd1">[15]</xref>, a standard quasi-Newton method (see <xref ref-type="supplementary-material" rid="pcbi.1003356.s001">Text S1</xref> of the supporting information for gradients of the parameters). In a final step, we used a nonparametric histogram estimate (150 bins) to map out the nonlinearity. Through this multi-step procedure we tried to maximize the chances of finding a linear-nonlinear description of a neuron's behavior where one exists. Note that strictly speaking, this model is no longer a generalized linear model (since the nonlinearities used are not invertible and the nonlinearities' parameters are optimized). Quadratic models were optimized using the same procedure after extending the input by quadratic features.</p>
<p>The parameters of the STM (<xref ref-type="disp-formula" rid="pcbi.1003356.e035">Equation 7</xref>) were initialized by estimating the spike-triggered, non-spike-triggered, and interspike interval distributions. Mixtures of Gaussians were fitted using standard expectation maximization <xref ref-type="bibr" rid="pcbi.1003356-Bishop1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003356-Dempster1">[16]</xref> and interval distributions were estimated using histograms. While naive Bayes classifiers often already work well, it can be beneficial to directly optimize the conditional log-likelihood <xref ref-type="bibr" rid="pcbi.1003356-Ng1">[17]</xref>. After initializing the parameters, we thus discriminatively finetuned the parameters using BFGS <xref ref-type="bibr" rid="pcbi.1003356-Nocedal1">[18]</xref>. We found that this indeed helped to substantially improve the performance where the model depended on both the stimulus and the spike history.</p>
<p>We used between three and five components for the spike-triggered distribution and one and two components for the non-spike-triggered distribution, which was found to work well in preliminary runs on a different but related dataset with similar stimuli. Using two non-spike-triggered components increased the stability of the optimization for some cells. Finally, factored STMs were trained discriminatively using limited-memory BFGS with randomly initialized parameters.</p>
<p>All models were trained on the 50 unfrozen trials and performance was evaluated based on the 50 frozen trials.</p>
</sec></sec><sec id="s3">
<title>Results</title>
<p>We qualitatively and quantitatively compare the performance of the generalized linear, quadratic and spike-triggered mixture model (STM) for different cells and find in both cases that the STM can lead to substantial improvements.</p>
<sec id="s3a">
<title>Qualitative comparison</title>
<p><xref ref-type="fig" rid="pcbi-1003356-g002">Figure 2</xref> shows spike trains generated by the different models when fitted to one SA cell and one RA cell. The trial-to-trial variability of the responses of most cells in the dataset is quite low. This behavior is well captured by the STM, while the responses of the generalized linear and quadratic models generally seem to be noisier. This difference is more pronounced for SA cells than for RA cells, where all models appear to give a reasonably good fit. Corresponding peristimulus time histograms (PSTHs) can be seen in <xref ref-type="fig" rid="pcbi-1003356-g003">Figure 3</xref> (details on how the PSTHs were computed are given in the next section).</p>
<fig id="pcbi-1003356-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003356.g003</object-id><label>Figure 3</label><caption>
<title>Peristimulus time histograms.</title>
<p>The insets show peristimulus time histograms (PSTHs) corresponding to the spike trains in <xref ref-type="fig" rid="pcbi-1003356-g002">Figure 2</xref> (best viewed on a computer screen). PSTHs were estimated from 50 trials for real cells, 1000 trials for model cells, and smoothed using a Gaussian kernel. The kernel width was chosen automatically (see text) except for the zoomed-in excerpt of the PSTH in <bold>B</bold>, where for better visibility we used a slightly wider kernel (FWHM: 0.15 ms). The variance explained (R<sup>2</sup>) by the generalized linear model, quadratic model and STM was 0.15, 0.26, and 0.47 (<bold>A</bold>), and 0.19, 0.41, and 0.5 (<bold>B</bold>), respectively.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003356.g003" position="float" xlink:type="simple"/></fig>
<p>Similar conclusions can be drawn by looking at spike-triggered distributions (<xref ref-type="fig" rid="pcbi-1003356-g004">Figure 4</xref>). Ensembles of spike-triggered positions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e071" xlink:type="simple"/></inline-formula> and velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e072" xlink:type="simple"/></inline-formula> of the time-varying stimulus suggest a complex dependency of the responses on the stimulus for at least some cells. Note, however, that even a linear neuron can produce non-Gaussian spike-triggered distributions when the stimulus is correlated over time and the cell's firing depends on its history of generated spikes. Also note that while here we show 2-dimensional spike-triggered distributions, the input to the models was a 10-dimensional stimulus (and a 100-dimensional spike history), as described above.</p>
<fig id="pcbi-1003356-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003356.g004</object-id><label>Figure 4</label><caption>
<title>Spike-triggered distributions of real and model neurons.</title>
<p>The plots show spike-triggered histograms of positions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e073" xlink:type="simple"/></inline-formula> (horizontal axis) and velocities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e074" xlink:type="simple"/></inline-formula> (vertical axis) of the stimulus for the same neurons displayed in <xref ref-type="fig" rid="pcbi-1003356-g002">Figure 2</xref>, that is, for one SA cell (<bold>A</bold>) and one RA cell (<bold>B</bold>). Stimuli were measured 1.5 ms before a spike occured. Note that these are not the stimuli the STM was trained on, which were 10-dimensional. Solid lines indicate one and two standard deviations of the Gaussian prior stimulus distribution.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003356.g004" position="float" xlink:type="simple"/></fig>
<p>To get a better intuition for the degree of nonlinearity of a cell, we can compare the cell's spike-triggered distribution with the spike-triggered distribution of the best matching linear model. In the given examples, the linear model is unable to reproduce the spike-triggered distributions of the cells displayed in <xref ref-type="fig" rid="pcbi-1003356-g004">Figure 4</xref>. For the SA cell, even the quadratic model fails to reproduce many of the features of the neuron's spike-triggered distribution, while the STM's behavior much more closely resembles that of the real cell.</p>
</sec><sec id="s3b">
<title>Quantitative comparison</title>
<p>To quantify the performance of the different models, we estimate the <italic>cross-entropy</italic> or negative log-likelihood,<disp-formula id="pcbi.1003356.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e075" xlink:type="simple"/><label>(10)</label></disp-formula>where the expectation is taken over stimuli <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e076" xlink:type="simple"/></inline-formula> and spikes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e077" xlink:type="simple"/></inline-formula> generated by the real neuron. We estimate this quantity using 50 frozen trials not used during training of the model. The cross-entropy is a natural measure for comparing different models, as it is the measure which is optimized during maximum likelihood estimation of the parameters, and many other system-identification approaches such as spike-triggered averaging can often be viewed as performing maximum likelihood or penalized maximum likelihood learning <xref ref-type="bibr" rid="pcbi.1003356-Wu1">[19]</xref>.</p>
<p>The cross-entropy can be used to lower-bound the mutual information between stimuli and spikes,<disp-formula id="pcbi.1003356.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e078" xlink:type="simple"/></disp-formula>The better a model distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e079" xlink:type="simple"/></inline-formula> approximates a cell's behavior, the smaller the difference will be between the lower bound and the true information transmitted by the cell. Note that this mutual information only quantifies the information carried by one bin of the spike train while we are generally more interested in the information carried by an entire spike train, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e080" xlink:type="simple"/></inline-formula>.</p>
<p>The spike train's mutual information with the stimulus can be decomposed as follows<disp-formula id="pcbi.1003356.e081"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e081" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e082" xlink:type="simple"/></inline-formula> denotes the history of spikes preceding time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e083" xlink:type="simple"/></inline-formula>. To correctly quantify the mutual information between the spike train and the stimulus, it is thus imporant to take spike history effects into account. If we also use the fact that a neuron's firing will only be affected by the stimulus preceding a spike, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e084" xlink:type="simple"/></inline-formula>, we get<disp-formula id="pcbi.1003356.e085"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e085" xlink:type="simple"/></disp-formula>for the mutual information of the spike train per time bin. Dividing by the bin width yields an information rate (measured in bits per second or similar). Estimating this quantity requires two models: one for approximating the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e086" xlink:type="simple"/></inline-formula> and one for approximating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e087" xlink:type="simple"/></inline-formula>. A model for the former can take the form of <xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equation 8</xref> but with the stimulus-dependent terms dropped.</p>
<p><xref ref-type="sec" rid="s3">Results</xref> averaged over all recorded SA cells (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e088" xlink:type="simple"/></inline-formula>) and all RA cells (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e089" xlink:type="simple"/></inline-formula>) are given in <xref ref-type="fig" rid="pcbi-1003356-g005">Figure 5</xref>. The average improvement of the STM over the quadratic model is 45.40 bit/s for SA cells and 15.48 bit/s for RA cells (for models taking into account spike history). The improvement for the cell with the largest difference to the quadratic model is 95.15 bit/s for SA cells and 43.05 bit/s for RA cells (the cells displayed in <xref ref-type="fig" rid="pcbi-1003356-g002">Figures 2</xref> to <xref ref-type="fig" rid="pcbi-1003356-g004">4</xref>). The firing rates of these two neurons were 117 Hz and 52.6 Hz, respectively, so that both numbers roughly correspond to 0.8 bit/spike improvement. These improvements correspond to the amount of information carried by the cells that would have been missed if a quadratic model was used to estimate mutual information instead of an STM. The average differences between the quadratic and the linear model, and the STM and the quadratic model (with and without including spike history) were all significant (one-tailed Wilcoxon signed-rank test, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e090" xlink:type="simple"/></inline-formula>; <xref ref-type="fig" rid="pcbi-1003356-g005">Figure 5C and D</xref>).</p>
<fig id="pcbi-1003356-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003356.g005</object-id><label>Figure 5</label><caption>
<title>Quantitative model comparison.</title>
<p>Linear, quadratic and spike-triggered mixture models (STM) were evaluated on 8 slowly adapting cells (<bold>A</bold>) and 10 rapidly adapting cells (<bold>B</bold>). The performance of each model is measured in terms of the cross-entropy (negative log-likelihood) averaged over all cells (smaller is better). Light bars correspond to models which ignore the spike history, dark bars correspond to models which explicitly take the spike history into account. By subtracting the cross-entropy from the estimated entropy of the spike trains (“Prior”), an estimate of mutual information (MI) between stimuli and spike trains is obtained. The bars in <bold>C</bold> and <bold>D</bold> show (from left to right) the differences in performance between the linear model and the prior, the quadratic model and the linear model, and the STM and the quadratic model (with and without spike history dependency, respectively). The two right most bars show the improvement of the PSTH over the STM with and without spike history dependency.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003356.g005" position="float" xlink:type="simple"/></fig>
<p>In addition to comparing different models, we can also compute and compare our model's performance to the cross-entropy of a PSTH, which has also been called <italic>oracle model</italic> <xref ref-type="bibr" rid="pcbi.1003356-Vintch1">[20]</xref>. We computed PSTHs by convolving the average response to the frozen stimulus with a Gaussian kernel. We took all but one trial to compute the PSTH and the remaining trial for prediction. That is, the probability of a spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e091" xlink:type="simple"/></inline-formula> in trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e092" xlink:type="simple"/></inline-formula> was predicted to be<disp-formula id="pcbi.1003356.e093"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e093" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e094" xlink:type="simple"/></inline-formula> is the number of trials and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e095" xlink:type="simple"/></inline-formula> is a normalized Gaussian kernel of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e096" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003356.e097"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e097" xlink:type="simple"/></disp-formula>For spike counts larger than 1, the same approach could be taken by using the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1003356.e093">Equation 11</xref> as the rate parameter of a Poisson distribution. We found it was necessary to add a small offset to the PSTH to achieve good results. Both the offset and the kernel width were automatically chosen from a prespecified set of parameters to minimize the cross-entropy averaged over all trials. That is, for each individual cell, we chose the kernel width with the best prediction performance. The optimal kernel widths were found to be around 0.12 ms and 0.09 ms (full width at half maximum, FWHM) for the SA and the RA cell displayed in <xref ref-type="fig" rid="pcbi-1003356-g003">Figure 3</xref>, respectively.</p>
<p>While the performance of the PSTH does not give us a guaranteed lower bound on the achievable cross-entropy, it gives us a very optimistic estimate of the performance that can be achieved by a model which does not take spike history into account. We found that the PSTH yielded a significantly lower cross-entropy than an STM without history dependency (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e098" xlink:type="simple"/></inline-formula>), but not significantly lower than an STM which takes spike history into account (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e099" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e100" xlink:type="simple"/></inline-formula>, respectively; <xref ref-type="fig" rid="pcbi-1003356-g005">Figure 5C and D</xref>).</p>
<p>PSTHs for model cells were estimated from 1000 spike trains (sampling spike trains was necessary since the models depend on the spike history) using the same kernel as for the real cell. The variance explained (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e101" xlink:type="simple"/></inline-formula>) by the generalized linear model, quadratic model and STM was 0.15, 0.26, and 0.47 for the SA cell, and 0.19, 0.41, and 0.5 for the RA cell (<xref ref-type="fig" rid="pcbi-1003356-g003">Figure 3</xref>), respectively. Note that the explained variance depends heavily on the chosen kernel width and wider kernels would yield larger coefficients.</p>
</sec><sec id="s3c">
<title>How much data is enough?</title>
<p>The high firing rate of the cells and the resulting abundance of data allowed us to neglect regularization and overfitting issues. The training set contained on average about 25,000 spikes for SA cells and 6,700 spikes for RA cells. However, typically much less data is available.</p>
<p>To counter overfitting, different approaches to regularization can be taken. We already suggested reducing the number of parameters of the STM via factorization and parameter sharing (<xref ref-type="disp-formula" rid="pcbi.1003356.e057">Equation 9</xref>). To get an idea of how the factored STM's performance behaves as a function of the available data, we artificially reduced the amount of data by randomly picking a subset of the 50 training trials. Of that subset, we used 50% for validation and 50% for optimization. During optimization, the performance on the validation set was tested every 5 iterations. If it decreased 50 times in a row, training was stopped and the parameters with the lowest validation error until then were kept. Other than early stopping, no other form of regularization was used. The test set was the same as the one used in <xref ref-type="fig" rid="pcbi-1003356-g005">Figure 5</xref>.</p>
<p><xref ref-type="fig" rid="pcbi-1003356-g006">Figure 6</xref> shows the performance of the factored STM for different amounts of spikes in the training set. The factored STM used 6 components and 5 quadratic features (246 parameters in total) for the SA cell, and 3 components and 5 quadratic features (198 parameters) for the RA cell. For comparison, we also plot the performance of a generalized linear model (111 parameters) trained with early stopping on a subset of the training data, as well as the performance of non-factored STMs (532 parameters and 400 parameters, respectively) and quadratic (156 parameters) models trained on the entire training set.</p>
<fig id="pcbi-1003356-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003356.g006</object-id><label>Figure 6</label><caption>
<title>Performance as a function of available data.</title>
<p>The factored STM was trained with different random subsets of the training trials and evaluated on all test trials for one SA cell (<bold>A</bold>) and one RA cell (<bold>B</bold>). The horizontal axis represents the number of spikes in the training set. Shown are the average performances (solid blue line) along with 90% confidence intervals (5th and 95th percentile). For comparison, we also show the performance of the linear model trained with different subsets of the data, the average performances of the non-factored STM, and the quadratic model trained on the entire training set. Note that the factored STM outperforms the generalized linear model even when only a small fraction of the dataset is used.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003356.g006" position="float" xlink:type="simple"/></fig>
<p>For the SA cell, the performance of the factored STM started to decrease more rapidly as soon as less than 5,000 spikes were present in the training set. However, even with 2,500 spikes the average performance was still much better than the performance of a quadratic model trained on the entire dataset. For the RA cell, the performance started to deteriorate at about 2,000 spikes. Note that the performance of the linear model worsened at a similar rate. Reducing the number of parameters further by using half the spike history or six instead of ten principal componets to represent the stimulus did not help to improve performance in the regime of few data points. The performance might however be improved by choosing suitable priors for the parameters, which we did not explore here.</p>
<p>Training with half the dataset of the RA cell (about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e102" xlink:type="simple"/></inline-formula> data points) on average took 9.4 minutes for the factored STM and 2.7 minutes for the linear model with parallelized implementations written in C++ when run on a machine with 12 Intel Xeon E5-2630 cores (2.3 GHz).</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<p>We have shown that a spike-triggered mixture model can lead to better performance than either linear or quadratic models, which we illustrated on the example of rat primary afferents. A possible explanation for the improved performance might be that our model can better cope with a cell's adaptation to the stimulus. Because the firing rate of our model is effectively a maximum over a number of quadratic models, the model is able to respond differently in different regions of the stimulus space. Our model may yield even bigger improvements when applied to cells higher up the hierarchy—such as cortical cells—where highly nonlinear dependencies on the stimulus are to be expected <xref ref-type="bibr" rid="pcbi.1003356-Carandini1">[21]</xref>. In particular, an interesting empirical question is whether STMs will be able to improve upon quadratic models in modeling complex cells <xref ref-type="bibr" rid="pcbi.1003356-Hubel1">[22]</xref>. As a generalization, the STM can capture the same kind of invariances that the quadratic model can capture, but in addition allows us to spend parameters in different ways by adding components instead of quadratic feature dimensions.</p>
<p>Here, we chose to give up on the constraint of convexity to be able to build a more flexible neuron model. In practice, non-convex or even multimodal likelihoods do not have to be an issue. Many local optima of the STM likelihood are created simply by permutations of the parameters of the different mixture components and are therefore unproblematic. We found that initializing mixture models with EM and fine-tuning with an off-the-shelf optimizer worked well for our data and the performance of the resulting model was stable across different intializations. The parameters of the factored variant of the STM (<xref ref-type="disp-formula" rid="pcbi.1003356.e057">Equation 9</xref>) were randomly initialized and gave comparable results (<xref ref-type="fig" rid="pcbi-1003356-g006">Figure 6</xref>).</p>
<p>Alternatively, we could have used support vector machines, kernel logistic regression (KLR) <xref ref-type="bibr" rid="pcbi.1003356-Zhu1">[23]</xref> or other kernel based approaches <xref ref-type="bibr" rid="pcbi.1003356-Truccolo2">[24]</xref> for gaining flexibility while retaining convexity. In KLR, the input to the sigmoid (<xref ref-type="disp-formula" rid="pcbi.1003356.e017">Equation 2</xref>) determining the firing rate takes the form<disp-formula id="pcbi.1003356.e103"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003356.e103" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e104" xlink:type="simple"/></inline-formula> indexes training points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003356.e106" xlink:type="simple"/></inline-formula> is a kernel measuring the similarity between stimuli or, more generally, inputs to the neuron. If a Gaussian RBF kernel is used, KLR becomes similar to an STM with all covariance matrices constrained to a multiple of the identity matrix and one mixture component placed on top of each data point (<italic>cf.</italic> <xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equation 8</xref>).</p>
<p>KLR is equivalent to a linear-nonlinear-Bernoulli model with a cleverly chosen feature space whose dimensionality grows with the number of data points. Hence, one advantage of KLR is that its objective function is convex. Advantages of a parametric model like the one presented in this paper are more readily interpretable parameters and lower computational costs when the number of training points is large. Ultimately, whether kernel based methods or a generative approach should be preferred presumably depends on whether one has a better intuition of what represents a good kernel for the input space, or a better intuition of what represents a good characterization of the spike-triggered distribution.</p>
<p>The idea of using spike-triggered distributions to construct and motivate neuron models is not new. However, most work in this direction has focused on spike-triggered averages and covariances <xref ref-type="bibr" rid="pcbi.1003356-deRuytervanSteveninck1">[25]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-Fairhall1">[29]</xref>. Here we used mixtures of Gaussians and histograms to derive a new neuron model, but other distributions might work better in a different context and might be worth exploring.</p>
<p>Yet another related approach is to use feed-forward neural networks <xref ref-type="bibr" rid="pcbi.1003356-Lehky1">[30]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-Prenger1">[32]</xref>. While standard feed-forward neural networks are in principle also able to represent arbitrarily complex stimulus-response relationships <xref ref-type="bibr" rid="pcbi.1003356-Cybenko1">[33]</xref>, one can hope to get away with fewer parameters, less data, or simpler optimization schemes when using a model tailored to the task at hand. In contrast to general nonlinear regression strategies, a generative approach can lead to much more problem-specific architectures and nonlinearities (<xref ref-type="disp-formula" rid="pcbi.1003356.e042">Equations 8</xref> and <xref ref-type="disp-formula" rid="pcbi.1003356.e057">9</xref>). Similar cascades of linear-nonlinear units have been proposed but motivated by physiological rather than statistical considerations <xref ref-type="bibr" rid="pcbi.1003356-Vintch1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003356-Gollisch1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003356-McFarland1">[36]</xref>.</p>
<p>STMs can easily be extended to model populations of neurons similar to how GLMs are extended to populations by introducing coupling filters <xref ref-type="bibr" rid="pcbi.1003356-Pillow1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003356-Stevenson1">[37]</xref>. Analogous to how we incorporated dependency on the spike history of a single neuron, a form for the dependency between neurons can also be motivated via a log-likelihood ratio for the distribution of cross-interspike intervals.</p>
<p>Code for fitting STMs is provided at <ext-link ext-link-type="uri" xlink:href="http://bethgelab.org/code/theis2013a/" xlink:type="simple">http://bethgelab.org/code/theis2013a/</ext-link>.</p>
</sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003356.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003356.s001" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>Gradients for the log-likelihoods of the STM and factored STM.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003356-McCullagh1"><label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">McCullagh P, Nelder JA (1989) Generalized linear models. Chapman &amp; Hall, second edition.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Nelder1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nelder</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Wedderburn</surname><given-names>RWM</given-names></name> (<year>1972</year>) <article-title>Generalized linear models</article-title>. <source>Journal of the Royal Statistical Society, Series A (General)</source> <volume>135</volume>: <fpage>370</fpage>–<lpage>384</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Paninski1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2004</year>) <article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title>. <source>Network: Computation in Neural Systems</source> <volume>15</volume>: <fpage>243</fpage>–<lpage>262</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Truccolo1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Eden</surname><given-names>UT</given-names></name>, <name name-style="western"><surname>Fellows</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Donoghu</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>EN</given-names></name> (<year>2004</year>) <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title>. <source>Journal of Neurophysiology</source> <volume>93</volume>: <fpage>1074</fpage>–<lpage>1089</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Pillow1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Spatio-temporal correlations and visual signaling in a complete neuronal population</article-title>. <source>Nature</source> <volume>454</volume>: <fpage>995</fpage>–<lpage>999</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Gerwinn1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerwinn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Macke</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Bayesian inference for generalized linear models for spiking neurons</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>4</volume>: <fpage>12</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Pillow2"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2006</year>) <article-title>Dimensionality reduction in neural models: An information-theoretic generalization of spike-triggered average and covariance analysis</article-title>. <source>Journal of Vision</source> <volume>6</volume>: <fpage>414</fpage>–<lpage>428</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Park1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Park IM, Pillow JW (2011) Bayesian spike-triggered covariance. In: Advances in Neural Information Processing Systems 24. pp. 1692–1700.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Rajan1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rajan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name> (<year>2013</year>) <article-title>Learning quadratic receptive fields from neural responses to natural stimuli</article-title>. <source>Neural Computation</source> <volume>25</volume>: <fpage>1661</fpage>–<lpage>1692</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Sharpee1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sharpee</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2004</year>) <article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title>. <source>Neural Computation</source> <volume>16</volume>: <fpage>223</fpage>–<lpage>250</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Rajan2"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Rajan K, Bialek W (2012). Maximally informative stimulus energies in the analysis of neural responses to natural signals. arXiv:1201.0321.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Fitzgerald1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitzgerald</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Rowekamp</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name> (<year>2011</year>) <article-title>Second order dimensionality reduction using minimum and maximum mutual information models</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume> (<issue>10</issue>)  <fpage>e1002249</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Zhang1"><label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhang H (2004) The optimality of naive Bayes. In: Proceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference. AAAI Press, pp. 562–567.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Bishop1"><label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Bishop CM (2006) Pattern Recognition and Machine Learning. Springer.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Byrd1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Byrd</surname><given-names>RH</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Nocedal</surname><given-names>J</given-names></name> (<year>1995</year>) <article-title>A limited memory algorithm for bound constrained optimization</article-title>. <source>SIAM Journal on Scientific and Statistical Computing</source> <volume>16</volume>: <fpage>1190</fpage>–<lpage>1208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Dempster1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dempster</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>DB</given-names></name> (<year>1977</year>) <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the Royal Statistical Society, Series B</source> <volume>39</volume>: <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Ng1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Ng AY, Jordan MI (2002) On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. In: Advances in Neural Information Processing Systems 15. pp. 841–848.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Nocedal1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Nocedal J, Wright SJ (2006) Numerical Optimization. Springer, second edition, 136–143 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Wu1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>MCK</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2006</year>) <article-title>Complete functional characterization of sensory neurons by system identification</article-title>. <source>Annual Review of Neuroscience</source> <volume>29</volume>: <fpage>477</fpage>–<lpage>505</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Vintch1"><label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Vintch B, Zaharia AD,Movshon JA, Simoncelli EP (2012) Efficient and direct estimation of a neural subunit model for sensory coding. In: Advances in Neural Information Processing Systems 25. pp. 3113–3121.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Carandini1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Do we know what the early visual system does?</article-title> <source>The Journal of Neuroscience</source> <volume>25</volume>: <fpage>10577</fpage>–<lpage>10597</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Hubel1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1962</year>) <article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title>. <source>Journal of Physiology</source> <volume>160</volume>: <fpage>106</fpage>–<lpage>154</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Zhu1"><label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhu J, Hastie T (2001) Kernel logistic regression and the import vector machine. In: Advances in Neural Information Processing Systems 14. pp. 1081–1088.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Truccolo2"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name> (<year>2007</year>) <article-title>Nonparametric modeling of neural point processes via stochastic gradient boosting regression</article-title>. <source>Neural Computation</source> <volume>19</volume>: <fpage>672</fpage>–<lpage>705</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-deRuytervanSteveninck1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>1988</year>) <article-title>Real-time performance of a movement-sensitive neuron in the blowfly visual system: Coding and information transfer in short spike sequences</article-title>. <source>Proceedings of the Royal Society of London Series B, Biological Sciences</source> <volume>234</volume>: <fpage>379</fpage>–<lpage>414</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Brenner1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brenner</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name> (<year>2000</year>) <article-title>Adaptive rescaling maximizes information transmission</article-title>. <source>Neuron</source> <volume>26</volume>: <fpage>695</fpage>–<lpage>702</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Simoncelli1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Simoncelli EP, Paninski L, Pillow J, Schwartz O (2004) The Cognitive Neurosciences, MIT Press, chapter Characterization of Neural Responses with Stochastic Stimuli. third edition, pp. 327–338.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Schwartz1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2006</year>) <article-title>Spike-triggered neural characterization</article-title>. <source>Journal of Vision</source> <volume>6</volume>: <fpage>484</fpage>–<lpage>507</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Fairhall1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Burlingame</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Narasimhan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Selectivity for multiple stimulus features in retinal ganglion cells</article-title>. <source>Journal of Neurophysiology</source> <volume>96</volume>: <fpage>2724</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Lehky1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lehky</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name> (<year>1992</year>) <article-title>Predicting responses of nonlinear neurons in monkey striate cortex to complex patterns</article-title>. <source>Journal of Neuroscience</source> <volume>12</volume>: <fpage>3568</fpage>–<lpage>3581</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Lau1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Stanley</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2002</year>) <article-title>Computational subunits of visual cortical neurons revealed by artificial neural networks</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>99</volume>: <fpage>8974</fpage>–<lpage>8979</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Prenger1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prenger</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>MCK</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2004</year>) <article-title>Nonlinear V1 responses to natural scenes revealed by neural network analysis</article-title>. <source>Neural Networks</source> <volume>17</volume>: <fpage>663</fpage>–<lpage>679</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Cybenko1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cybenko</surname><given-names>G</given-names></name> (<year>1989</year>) <article-title>Approximations by superpositions of sigmoidal functions</article-title>. <source>Mathematics of Control, Signals, and Systems</source> <volume>2</volume>: <fpage>303</fpage>–<lpage>314</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Gollisch1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gollisch</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Herz</surname><given-names>AVM</given-names></name> (<year>2005</year>) <article-title>Disentangling sub-millisecond processes within an auditory transduction chain</article-title>. <source>PLoS Biology</source> <volume>3</volume>: <fpage>e8</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Butts1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Weng</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Alonso</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2011</year>) <article-title>Temporal precision in the visual pathway through the interplay of excitation and stimulus-driven suppression</article-title>. <source>The Journal of Neuroscience</source> <volume>31</volume>: <fpage>11313</fpage>–<lpage>11327</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-McFarland1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McFarland</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Cui</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name> (<year>2013</year>) <article-title>Inferring nonlinear neuronal computation based on physiologically plausible inputs</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>: <fpage>e1003143</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003356-Stevenson1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevenson</surname><given-names>IH</given-names></name>, <name name-style="western"><surname>Rebesco</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>LE</given-names></name>, <name name-style="western"><surname>Klörding</surname><given-names>KP</given-names></name> (<year>2008</year>) <article-title>Inferring functional connections between neurons</article-title>. <source>Current Opinion in Neurobiology</source> <volume>18</volume>: <fpage>582</fpage>–<lpage>588</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>