<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004896</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01441</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Confidence intervals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep Neural Networks as a Computational Model for Human Shape Sensitivity</article-title>
<alt-title alt-title-type="running-head">Deep Nets Represent Perceived Shape</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8843-3147</contrib-id>
<name name-style="western">
<surname>Kubilius</surname>
<given-names>Jonas</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1435-2536</contrib-id>
<name name-style="western">
<surname>Bracci</surname>
<given-names>Stefania</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Op de Beeck</surname>
<given-names>Hans P.</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname>
<given-names>Matthias</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: JK HPOdB. Performed the experiments: JK. Analyzed the data: JK SB HPOdB. Contributed reagents/materials/analysis tools: JK SB HPOdB. Wrote the paper: JK SB HPOdB.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">jonas.kubilius@ppw.kuleuven.be</email> (JK); <email xlink:type="simple">hans.opdebeeck@ppw.kuleuven.be</email> (HPOdB)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>28</day>
<month>4</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>4</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>4</issue>
<elocation-id>e1004896</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>8</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>3</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Kubilius et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004896"/>
<abstract>
<p>Theories of object recognition agree that shape is of primordial importance, but there is no consensus about how shape might be represented, and so far attempts to implement a model of shape perception that would work with realistic stimuli have largely failed. Recent studies suggest that state-of-the-art convolutional ‘deep’ neural networks (DNNs) capture important aspects of human object perception. We hypothesized that these successes might be partially related to a human-like representation of object shape. Here we demonstrate that sensitivity for shape features, characteristic to human and primate vision, emerges in DNNs when trained for generic object recognition from natural photographs. We show that these models explain human shape judgments for several benchmark behavioral and neural stimulus sets on which earlier models mostly failed. In particular, although never explicitly trained for such stimuli, DNNs develop acute sensitivity to minute variations in shape and to non-accidental properties that have long been implicated to form the basis for object recognition. Even more strikingly, when tested with a challenging stimulus set in which shape and category membership are dissociated, the most complex model architectures capture human shape sensitivity as well as some aspects of the category structure that emerges from human judgments. As a whole, these results indicate that convolutional neural networks not only learn physically correct representations of object categories but also develop perceptually accurate representational spaces of shapes. An even more complete model of human object representations might be in sight by training deep architectures for multiple tasks, which is so characteristic in human development.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Shape plays an important role in object recognition. Despite years of research, no models of vision could account for shape understanding as found in human vision of natural images. Given recent successes of deep neural networks (DNNs) in object recognition, we hypothesized that DNNs might in fact learn to capture perceptually salient shape dimensions. Using a variety of stimulus sets, we demonstrate here that the output layers of several DNNs develop representations that relate closely to human perceptual shape judgments. Surprisingly, such sensitivity to shape develops in these models even though they were never explicitly trained for shape processing. Moreover, we show that these models also represent categorical object similarity that follows human semantic judgments, albeit to a lesser extent. Taken together, our results bring forward the exciting idea that DNNs capture not only objective dimensions of stimuli, such as their category, but also their subjective, or perceptual, aspects, such as shape and semantic similarity as judged by humans.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>ERC-2011-Stg-284101</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Op de Beeck</surname>
<given-names>Hans P.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002749</institution-id>
<institution>Federaal Wetenschapsbeleid</institution>
</institution-wrap>
</funding-source>
<award-id>IUAP P7/11</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Op de Beeck</surname>
<given-names>Hans P.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003130</institution-id>
<institution>Fonds Wetenschappelijk Onderzoek</institution>
</institution-wrap>
</funding-source>
<award-id>PhD fellowship</award-id>
<principal-award-recipient>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8843-3147</contrib-id>
<name name-style="western">
<surname>Kubilius</surname>
<given-names>Jonas</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004040</institution-id>
<institution>KU Leuven</institution>
</institution-wrap>
</funding-source>
<award-id>Postdoctoral mandate</award-id>
<principal-award-recipient>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8843-3147</contrib-id>
<name name-style="western">
<surname>Kubilius</surname>
<given-names>Jonas</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work has been funded by the The Belgian Science Policy (IUAP P7/11, <ext-link ext-link-type="uri" xlink:href="http://www.belspo.be/iap/" xlink:type="simple">http://www.belspo.be/iap/</ext-link>) and the European Research Council (ERC-2011-Stg-284101; <ext-link ext-link-type="uri" xlink:href="http://erc.europa.eu/" xlink:type="simple">http://erc.europa.eu/</ext-link>) grants awarded to HPOdB. JK is a research assistant of the Research Foundation—Flanders (FWO; <ext-link ext-link-type="uri" xlink:href="http://www.fwo.be/" xlink:type="simple">http://www.fwo.be/</ext-link>) and holds a Postdoctoral Mandate from the Internal Funds KU Leuven (<ext-link ext-link-type="uri" xlink:href="http://www.kuleuven.be/research/support/" xlink:type="simple">http://www.kuleuven.be/research/support/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="26"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files are available from the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/jf42a" xlink:type="simple">https://osf.io/jf42a</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Understanding how the human visual system processes visual information involves building models that would account for human-level performance on a multitude of tasks. For years, despite the best efforts, computational understanding of even the simplest everyday tasks such as object and scene recognition have been limited to toy datasets and poor model performances. For instance, hierarchical architecture HMAX [<xref ref-type="bibr" rid="pcbi.1004896.ref001">1</xref>], once known as “the standard model” of vision [<xref ref-type="bibr" rid="pcbi.1004896.ref002">2</xref>], worked successfully on a stimulus set of paper clips and could account for some rapid categorization tasks [<xref ref-type="bibr" rid="pcbi.1004896.ref003">3</xref>] but failed to capture shape and object representations once tested more directly against representations in the visual cortex (e.g., [<xref ref-type="bibr" rid="pcbi.1004896.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>]).</p>
<p>Recently, however, deep neural networks (DNNs) brought a tremendous excitement and hope to multiple fields of research. For the first time, a dramatic increase in performance has been observed on object and scene categorization tasks [<xref ref-type="bibr" rid="pcbi.1004896.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref008">8</xref>], quickly reaching performance levels rivaling humans [<xref ref-type="bibr" rid="pcbi.1004896.ref009">9</xref>]. More specifically in the context of object recognition, stimulus representations developed by the deep nets have been shown to account for neural recordings in monkey inferior temporal cortex and functional magnetic resonance imaging data throughout the human ventral visual pathway (e.g., [<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref011">11</xref>]), suggesting that some fundamental processes, shared across different hardware, have been captured by deep nets.</p>
<p>The stimulus sets on which DNNs have been tested in these previous studies allow the inference that there is a general correspondence between the representations developed within DNNs and important aspects of human object representations at the neural level. However, these stimulus sets were not designed to elucidate specific aspects of human representations. In particular, a long tradition in human psychophysics and primate physiology has pointed towards the processing of shape features as the underlying mechanism behind human object recognition (e.g., [<xref ref-type="bibr" rid="pcbi.1004896.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004896.ref015">15</xref>]). Cognitive as well as computational models of object recognition have mainly focused upon the hierarchical processing of shape (e.g., [<xref ref-type="bibr" rid="pcbi.1004896.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref017">17</xref>]). There are historical and remaining controversies about the exact nature of these shape representations, such as about the degree of viewpoint invariance and the role of structural information in the higher levels of representation (e.g., [<xref ref-type="bibr" rid="pcbi.1004896.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref019">19</xref>]). Still, all models agree on the central importance of a hierarchical processing of shape. For this reason we hypothesized that the general correspondence between DNNs representations and human object representations might be related to a human-like sensitivity for shape properties in the DNNs.</p>
<p>Here we put this hypothesis to the test through a few benchmark stimulus sets, which have highlighted particular aspects of human shape perception in the past. We first demonstrate that convolutional neural networks (convnets), the most common kind of DNN models in image processing, can recognize objects based upon shape also when all other cues are removed, as humans can. Moreover, we show that despite being trained solely for object categorization, higher layers of convnets develop a surprising sensitivity for shape that closely follows human perceptual shape judgments. When we dissociate shape from category membership, then abstract categorical information is available to a limited extent in these networks, suggesting that a full model of shape and category perception might require richer training regimes for convnets.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Experiment 1: Recognition from shape</title>
<p>If convnets are indeed extracting perceptually relevant shape dimensions, they should be able to utilize shape for object recognition. This ability should extend to specific stimulus formats that highlight shape and do not include many other cues, such as silhouettes. The models have been trained for object recognition with natural images, how would they perform when all non-shape cues are removed? In order to systematically evaluate how convnet recognition performance depends on the amount of available shape and non-shape (e.g., color or texture) information, we employed the colorized version of the Snodgrass and Vanderwart stimulus set of common everyday objects [<xref ref-type="bibr" rid="pcbi.1004896.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref021">21</xref>]. This stimulus set consists of 260 line drawings of common objects that are easily recognizable to human observers and has been used extensively in a large number of studies (Google Scholar citations: over 4000 to [<xref ref-type="bibr" rid="pcbi.1004896.ref020">20</xref>]; over 500 to [<xref ref-type="bibr" rid="pcbi.1004896.ref021">21</xref>]). In our experiments, we used a subset of this stimulus set (see <xref ref-type="sec" rid="sec013">Methods</xref>), consisting of 61 objects (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1A</xref>). Three variants of the stimulus set were used: original color images, greyscale images, and silhouettes.</p>
<fig id="pcbi.1004896.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004896.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Object categorization.</title>
<p>(a) Examples of stimuli from the modified Snodgrass and Vanderwart stimulus set [<xref ref-type="bibr" rid="pcbi.1004896.ref021">21</xref>]. Stimulus images courtesy of Michael J. Tarr, Center for the Neural Basis of Cognition and Department of Psychology, Carnegie Mellon University, <ext-link ext-link-type="uri" xlink:href="http://www.tarrlab.org/" xlink:type="simple">http://www.tarrlab.org/</ext-link>. (b) Human (<italic>n</italic> = 10 for each variant of the stimulus set) and convnet (CaffeNet, VGG-19, GoogLeNet) accuracy in naming objects. For each stimulus set variant, mean human performance is indicated by a gray horizontal line, with the gray surrounding band depicting 95% bootstrapped confidence intervals. Error bars on model performance also depict 95% bootstrapped confidence intervals. (c) A consistency between human and convnet naming of objects. A consistency of .5 means that about half of responses (whether correct or not) we consistent between a model and an average of humans. Error bars indicate 95% bootstrapped confidence intervals. Gray bands indicate estimated ceiling performance based on between-human consistency. (d) Correlation with human performance on the silhouette stimulus set. The x-axis depicts an average human accuracy for a particular silhouette [<xref ref-type="bibr" rid="pcbi.1004896.ref015">15</xref>] and the y-axis depicts GoogLeNet performance on the same silhouette (either correct (value 1.0) or incorrect (value 0.0)). Model’s performance is jittered on the x- and y-axis for better visibility. Dark gray bubbles indicate average model’s performance for 11 bins of human performance (i.e., 0–5%, 5–15%, 15–25%, etc.) with the size of each bubble reflecting the number of data points per bin. The orange line shows the logistic regression fit with a 95% bootstrapped confidence interval (light orange shaded). The slope of the logistic regression is reliably different from zero.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004896.g001" xlink:type="simple"/>
</fig>
<p>First, we asked 30 human observers (10 per variant of the stimulus set) to choose a name of each object, presented for 100 ms, from a list of 657 options, corresponding to the actual of these objects and their synonyms as defined by observers in [<xref ref-type="bibr" rid="pcbi.1004896.ref020">20</xref>]. Consistent with previous studies [<xref ref-type="bibr" rid="pcbi.1004896.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref021">21</xref>], participants were nearly perfect in naming color objects, slightly worse for grayscale objects, and considerably worse for silhouettes (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1B</xref>, gray bands). Moreover, we found that participants were very consistent in their responses (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1C</xref>, gray bands).</p>
<p>We then presented three convnets with the stimuli and asked them to produce a single best guess of what might be depicted in the image. A correct answer was counted if the label exactly matched the actual label. We found that all deep nets exhibited a robust categorization performance on the original color stimulus set, reaching about 80–90% accuracy (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1B</xref>, with the best model (GoogLeNet) reaching human level of performance. Given that the models have not been trained at all on abstract line drawings, we found it an impressive demonstration of convnet feature generalization.</p>
<p>As textural cues were gradually removed, convnets still performed reasonably well. In particular, switching to grayscale decreased the performance by about 15%, whereas a further decrease by 30% occurred when inner gradients were removed altogether (silhouette condition). In other words, even when an object is defined solely by its shape, convnets maintain a robust and highly above-chance performance. Notably, a similar pattern of performance was observed when humans were asked to categorize these objects, suggesting that models are responding similarly to humans but are overall less accurate (irrespective of stimulus variant).</p>
<p>To investigate the consistency between human and model responses in more detail, we computed a squared Euclidean distance between the average human accuracy and a model accuracy, and normalized it to the range [0, 1], such that a consistency of .5 means that a model responded correctly where a human responded correctly and made a mistake where a human made a mistake about half of the time (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1C</xref>; see <xref ref-type="sec" rid="sec013">Methods</xref> for reasoning behind this choice of consistency). Overall, the consistency was substantial and nearly reached between-human consistency for color objects for our best model (GoogLeNet). To visualize the amount of consistency, we depicted The best model’s (GoogLeNet) performance on silhouettes against human performance (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1D</xref>). The performances are well correlated as indicated by the slope of the logistic regression being reliably above zero (<xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1D</xref>; z-test on GoogLeNet: <italic>z</italic> = 2.549, <italic>p</italic> = .011; CaffeNet: <italic>z</italic> = 2.393, <italic>p</italic> = .017; VGG-19: <italic>z</italic> = 2.323, <italic>p</italic> = .020). Furthermore, we computed consistency between models and found that for each variant of the stimulus set, the models appear to respond similarly and commit similar mistakes (the between-model consistency is about .8 for each pairwise comparison), indicating that the models learn similar features.</p>
<p><xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1D</xref> also shows that the models sometimes outperformed humans, seemingly in those situations where a model could take an advantage of a limited search space (e.g., it is much easier to say there is an iron when you do not know about hats). Overall, however, despite the moderate yet successful performance on silhouettes, it is obvious from <xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1D</xref> that there are quite some stimuli on which the models fail but which are recognized perfectly by human observers. Common mistakes could be divided into two groups: (i) similar shape (<italic>grasshopper</italic> instead of <italic>bee</italic>), and (ii) completely wrong answers where the reason behind model’s response is not so obvious (<italic>whistle</italic> instead of <italic>lion</italic>). We think that the former scenario further supports the idea that models base their decisions primarily on shape and are not easily distracted by the lack of other features. In either case, the errors might be remedied by model exposure to cartoons and drawings. Moreover, we do not think that these discrepancies might be primarily due to the lack of recurrent processes in these models since we tried to minimize influences of possible recurrent processes during human categorization by presenting stimuli for 100 ms to human observers. It is also possible that better naturalistic training sets in general are necessary where objects would be decoupled from background. For instance, lions always appear in savannahs, so models might be putting too much weight on savannah’s features for detecting a lion, which would be a poor strategy in the case of this stimulus set. Nonetheless, even in the absence of such training, convnets generalize well to such unrealistic stimuli, demonstrating that they genuinely learn some abstract shape representations.</p>
</sec>
<sec id="sec004">
<title>Experiment 2: Physical vs. perceived shape</title>
<p>In Experiment 2, we wanted to understand whether convolutional neural networks develop representations that capture the shape dimensions that dominate perception, the so-called “perceived” shape dimensions, rather than the physical (pixel-based) form. In most available stimulus sets these two dimensions are naturally correlated because the physical form and the perceived shape are nearly or completely identical. In order to disentangle the relative contributions of each of these dimensions, we needed stimulus sets where a great care was taken to design perceptual dimensions that would differ from physical dimensions.</p>
<sec id="sec005">
<title>Experiment 2a</title>
<p>First, we used a stimulus set where the physical and perceptual dimensions were specifically designed to be orthogonal. Building on their earlier stimulus set [<xref ref-type="bibr" rid="pcbi.1004896.ref022">22</xref>], Op de Beeck and colleagues [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>] created a stimulus set of nine novel shapes (<xref ref-type="fig" rid="pcbi.1004896.g002">Fig 2A</xref>) that can be characterized either in terms of their overall shape envelope / aspect ratio (vertical, square, horizontal), which we refer to as a physical form, or their perceptual features (spiky, smoothie, cubie), which we refer to as a perceived shape since humans base their shape judgments on these features, as explained below. They then computed the physical form dissimilarity matrix by taking the difference in the pixels, whereas the perceptual shape dissimilarity matrix was obtained in a behavioral experiment by asking participants to judge shape similarity ([<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>]; see <xref ref-type="sec" rid="sec013">Methods</xref> for details). They reported that participants typically grouped these stimuli based on the perceived shape and not shape envelope (<xref ref-type="fig" rid="pcbi.1004896.g002">Fig 2B</xref> shows the multidimensional scaling plot of these dissimilarity matrices), and also provided neural evidence for such representations in the higher shape-selective visual area known as the lateral occipital complex (LOC). The most common hierarchical model of object recognition available at that time, the original HMAX [<xref ref-type="bibr" rid="pcbi.1004896.ref001">1</xref>], did not capture perceived shape with this stimulus set.</p>
<fig id="pcbi.1004896.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004896.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Model preference for shape.</title>
<p>(a) Stimulus set with physical and perceived shape dimensions manipulated orthogonally [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>]. (b) Multidimensional scaling plots for the dissimilarity matrices of physical form, perceived shape, and the GoogLeNet outputs (at the top layer). The separation between shapes based on their perceived rather than physical similarity is evident in the GoogLeNet outputs (for visualization purposes, indicated by the lines separating the three clusters). (c) A correlation between model outputs and the physical form similarity of stimuli. Most shallow models are capturing physical similarity reasonably well, whereas HMAX and deep models are largely less representative of the physical similarity. (d) A correlation between model outputs and the perceived shape similarity of stimuli. Here, in contrast, deep models show a tendency of capturing perceived shape better than shallow and HMAX models. Gray band indicates estimated ceiling correlation based on human performance. (e) Correlation with physical (green) and perceived (orange) shape similarity across the layers of HMAX models and convnets. A preference for the perceived shape emerges in the upper layers. Vertical dotted lines indicate where fully-connected layers start. In all plots, error bars (or bands) indicate the 95% bootstrapped confidence intervals.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004896.g002" xlink:type="simple"/>
</fig>
<p>In order to investigate the relative importance of the physical form and perceived shape for various computer vision models, we used this stimulus set to compute the outputs of five shallow (single layer) models (pixelwise, GaborJet, HOG, PHOG, and PHOW), three versions of HMAX of varying complexity (HMAX ‘99, HMAX-HMIN, HMAX-PNAS), and three deep convnets (CaffeNet, VGG-19, and GoogLeNet; see <xref ref-type="sec" rid="sec013">Methods</xref> for details). Next, we computed the dissimilarity between each pair of the nine stimuli, resulting in a 9x9 dissimilarity matrix, and we applied multidimensional scaling on these matrices. <xref ref-type="fig" rid="pcbi.1004896.g002">Fig 2B</xref> shows the two-dimensional arrangement derived from multidimensional scaling for CaffeNet. This procedure revealed that the representations of these stimuli in the output of deep models tended to cluster based on their perceived rather than physical similarity, comparable to human judgments and neural representations in the higher visual areas in human cortex [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>].</p>
<p>To quantify how well the computed stimulus dissimilarities overall corresponded to the physical form or the perceived shape dissimilarity, we correlated physical form and perceived shape dissimilarity matrices with model’s output. We found that shallow models were mostly better at capturing physical dissimilarity than the output layers of deep models (<xref ref-type="fig" rid="pcbi.1004896.g002">Fig 2C</xref>; bootstrapped related samples one-tailed significance test for shallow vs. HMAX and for shallow vs. deep: p &lt; .001), whereas perceived shape was better captured by most deep models (<xref ref-type="fig" rid="pcbi.1004896.g002">Fig 2D</xref>; bootstrapped related samples one-tailed significance test for deep vs. HMAX: <italic>p</italic> = .010; deep vs. shallow: <italic>p</italic> = .002). (But note that, obviously, early layers of deep nets typically can reflect physical dissimilarities too.) Correlation layer-by-layer revealed that preference for shape gradually increased throughout the layers of all convnets, whereas physical similarity abruptly decreased at their upper layers (<xref ref-type="fig" rid="pcbi.1004896.g002">Fig 2E</xref>).</p>
</sec>
<sec id="sec006">
<title>Experiment 2b</title>
<p>So far, we found that deeper networks generally reflect perceive shape similarity better than shallower ones. Is this effect specific to the two dimensions used in [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>] or does it reflect a broader tendency for deeper nets to develop perceptually-relevant representations? In Experiment 2b, we constructed a new stimulus set where perceptually relevant dimensions were no longer explicitly defined. In particular, it was composed of six letters from six novel font families (<xref ref-type="fig" rid="pcbi.1004896.g003">Fig 3A</xref>). Even though the letters were unrecognizable (e.g., a letter ‘a’ in these fonts looked nothing like a typical ‘a’) and varied substantially within a font family, implicitly the letters in each font shared the same style (size was matched across fonts as much as possible). It should be noted though that even for human observers detecting these font families is not straightforward (note mistakes in <xref ref-type="fig" rid="pcbi.1004896.g003">Fig 3B</xref>).</p>
<fig id="pcbi.1004896.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004896.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Model preference for shape.</title>
<p>(a) Stimulus set of six letters from six constructed fonts. (b) Multidimensional scaling plots for the dissimilarity matrices of shape judgments by humans and the GoogLeNet outputs (at the top layer). Notice how humans and GoogLeNet are good at clustering the ULOG font but both struggle with Futurama. (c-d) A correlation between model outputs and physical form similarity (c) and perceived shape similarity (d) of stimuli. Whereas shallow models only capture physical shape, deep models capture perceived shape significantly better than shallow and HMAX models. Gray band indicates the estimated ceiling correlation based on human performance. (e) Correlation with physical (green) and perceived (orange) shape similarity across the layers of HMAX models and convnets. Vertical dotted lines indicate where fully-connected layers start. A preference for the perceived shape emerges in the upper layers. In all plots, error bars (or bands) indicate the 95% bootstrapped confidence intervals.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004896.g003" xlink:type="simple"/>
</fig>
<p>As seen in <xref ref-type="fig" rid="pcbi.1004896.g003">Fig 3C</xref>, deep models are sensitive to differences between fonts, with the letters presented in the same font tending to be clustered in the same part of the representational space. To quantify the similarity of model representations to human judgments, as before, we asked participants to rate the similarity between all these letters, and correlated their judgments with model outputs (<xref ref-type="fig" rid="pcbi.1004896.g003">Fig 3D</xref>). We found that deep models captured perceived similarity of letters significantly better than shallow models (bootstrapped related samples significance test, one-tailed: <italic>p</italic> &lt; .001), whereas shallow models correlated significantly better with physical form (bootstrapped related samples significance test, one-tailed: <italic>p</italic> &lt; .001), consistent with our hypothesis that the formation of perceptually-relevant representational spaces is a general property of convnets. In fact, convnets captured all explainable variance in human data, demonstrating that in certain scenarios convnets are already sufficiently robust.</p>
<p>Moreover, we observed that convnets exhibited significantly stronger correlations with behavioral similarity than HMAX family of models (bootstrapped related samples significance test, one-tailed: <italic>p</italic> &lt; .001). This tendency was already visible in Exp. 2a, but did not reach statistical significance yet. With this larger stimulus set that contains less pronounced perceptual dimensions, the limitations of HMAX models in shape processing and their relevance in understanding human perception became prominent, even for the most complex version HMAX-PNAS. These results corroborate with earlier observations that HMAX family of models are not sufficient for explaining categorical data dimensions [<xref ref-type="bibr" rid="pcbi.1004896.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>].</p>
</sec>
</sec>
<sec id="sec007">
<title>Experiment 3: Non-accidental properties</title>
<p>In 1987, Biederman put forward the Recognition-by-Components (RBC) theory [<xref ref-type="bibr" rid="pcbi.1004896.ref016">16</xref>] that proposed that objects recognition might be based on shape properties known as non-accidental. Under natural viewing conditions, many object’s properties are changing, depending on lighting, clutter, viewpoint and so on. In order to recognize objects robustly, Biederman proposed that the visual system might utilize those properties that remain largely invariant under possible natural variations. In particular, Biederman focused on those properties of object shape that remain unchanged when the three-dimensional shape of an object is projected to the two-dimensional surface on the eye’s retina, such as curved versus straight object axis, parallel versus converging edges, and so on [<xref ref-type="bibr" rid="pcbi.1004896.ref023">23</xref>]. Importantly, RBC theory predicts that observers should notice a change in a non-accidental property more readily than an equivalent change in a metric property. Consider, for example, geons shown in <xref ref-type="fig" rid="pcbi.1004896.g004">Fig 4A</xref>, top row. Both the non-accidental and the metric variant differ by the same amount from the base geon (as measured by some linear metric, such a pixelwise or GaborJet difference), yet the non-accidental one appears more distinct to us.</p>
<fig id="pcbi.1004896.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004896.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>(a) Examples of geons [<xref ref-type="bibr" rid="pcbi.1004896.ref024">24</xref>]. In order to measure model’s sensitivity to changes in non-accidental properties, model’s output is computed for a particular stimulus (middle column) and compared to the output when another variant of the same kind of stimulus is presented (right column) and when a non-accidental change in the stimulus is introduced (left column) that is physically (in the metric space) just as far from the base as the metric variant. We used 22 such triplets in total. (b) Model performance on discriminating between stimuli. For each triplet, model’s output is counted as accurate if the non-accidental variant is more dissimilar from the base stimulus than the metric variant is from the base. Chance level (50%) is indicated by a dashed line. (c) HMAX and convnet performance on the task at different layers. Non-accidental stimuli appear to be closer to the base in the early layers, which is consistent with a conservative design of the stimuli [<xref ref-type="bibr" rid="pcbi.1004896.ref024">24</xref>]. Best performance is observed at the upper layers of convnets with a slight dip at the output layer. Vertical dotted lines indicate where fully-connected layers start. In both plots, error bars (or bands) depict 95% bootstrapped confidence intervals.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004896.g004" xlink:type="simple"/>
</fig>
<p>Over years, Biederman and others consistently found such preference to hold in a large number of studies across species [<xref ref-type="bibr" rid="pcbi.1004896.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1004896.ref028">28</xref>], age groups [<xref ref-type="bibr" rid="pcbi.1004896.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1004896.ref031">31</xref>], non-urban cultures [<xref ref-type="bibr" rid="pcbi.1004896.ref032">32</xref>], and even in the selectivity of inferior temporal neurons in monkeys [<xref ref-type="bibr" rid="pcbi.1004896.ref024">24</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref033">33</xref>]. This idea of invariants has also been shown to play an important role in scene categorization [<xref ref-type="bibr" rid="pcbi.1004896.ref034">34</xref>] and famously penetrated computer vision literature when David Lowe developed his SIFT (Scale-Invariant Feature Transform) descriptor that attempted to capture invariant features in an image [<xref ref-type="bibr" rid="pcbi.1004896.ref035">35</xref>].</p>
<p>Thus, the sensitivity for non-accidental properties presents an important and well-tested line of research where the physical size of differences between shapes is dissociated from the effect of specific shape differences on perception. We tested the sensitivity for non-accidental properties using a previously developed stimulus set of geon triplets where the metric variant is as distinct or, typically, even more distinct from the base than the non-accidental variant as measured in the metric (physical) space. Nevertheless, humans and other species report perceiving non-accidental shapes as more dissimilar from the base than the metric ones, presenting us with a perfect test case where, similar to Exp. 2, physical shape similarity is different from the perceived one.</p>
<p>We evaluated model performance on this set of 22 geons (<xref ref-type="fig" rid="pcbi.1004896.g004">Fig 4A</xref>) that have been used previously in behavioral [<xref ref-type="bibr" rid="pcbi.1004896.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref036">36</xref>] and neurophysiological studies. A model’s response was counted as accurate if the response to a non-accidental stimulus was more dissimilar from the base than the metric one.</p>
<p>We found that all deep but not shallow or HMAX models (except for HMAX’99) showed a higher than chance performance (<xref ref-type="fig" rid="pcbi.1004896.g004">Fig 4B</xref>) with performance typically improving gradually throughout the architecture (<xref ref-type="fig" rid="pcbi.1004896.g004">Fig 4C</xref>; bootstrapped related samples significance test for deep vs. shallow, one-tailed: <italic>p</italic> &lt; .001; deep vs. HMAX: <italic>p</italic> = .011). Moreover, deeper networks tended to perform slightly better than shallower ones, in certain layers even achieving perfect performance. Overall, there was not any clear pattern in mistakes across convnets, except for a tendency towards mistakes in the main axis curvature, that is, convnets did not seem to treat straight versus curved edges as very distinct. In contrast, humans consistently show a robust sensitivity to changes in the main axis curvature [<xref ref-type="bibr" rid="pcbi.1004896.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref036">36</xref>]. Note that humans are also not perfect at detecting NAPs as reported by [<xref ref-type="bibr" rid="pcbi.1004896.ref036">36</xref>]. Thus, we do not go further into these differences because the RBC theory and most previous behavioral and neural studies only address a general preference for NAP changes, and hence do not provide a systematic framework for interpreting the presence or absence of such preference for specific NAPs.</p>
</sec>
<sec id="sec008">
<title>Experiment 4: The role of category when dissociated from shape</title>
<p>In the first three experiments, we demonstrated convnet sensitivity to shape properties. However, these convnets have been explicitly trained to optimize not for shape but rather category, that is, to provide a correct semantic label. Apparently, categorization is aided by developing sensitivity to shape. But is there anything beyond sensitivity to shape then that convnets develop? In other words, to what extent do these networks develop semantic representations similar to human categorical representations over and above mere shape information?</p>
<p>Typically, object shape and semantic properties are correlated, such that objects from the same category (e.g., fruits) share some shape properties as well (all have smooth roundish shape) that may distinguish them from another category (e.g., cars that have more corners), making it difficult to investigate the relative contributions of these two dimensions. To overcome these limitations, Bracci and Op de Beeck [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>] recently designed a new stimulus set, comprised of 54 photographs of objects, where shape and category dimensions are orthogonal to each other as much as possible (<xref ref-type="fig" rid="pcbi.1004896.g005">Fig 5A</xref>). In particular, objects from six categories have been matched in such a way that any one exemplar from a particular category would have a very similar shape to an exemplar from another category. Thus, the dissociation between shape and category is more prominent and can be meaningfully measured by asking participants to judge similarity between these objects based either on their shape or on their category. By correlating the resulting dissimilarity matrices to human neural data, Bracci and Op de Beeck [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>]found that perceived shape and semantic category are represented in parallel in the visual cortex.</p>
<fig id="pcbi.1004896.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004896.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Categorical representations in convnets.</title>
<p>(a) Stimulus set from [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>] with shape and category information largely orthogonal. (Adapted with permission from [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>].) (b) Multidimensional scaling plot representing object dissimilarity in the output layer of GoogLeNet. The black line indicates a clear separation between natural (brown, orange, yellow) and man-made (blue, gray, pink) objects. (c-d) A correlation between model representations and human shape (c) and category (d) judgments. Gray band indicates estimated ceiling correlation based on human performance. (e) A correlation with human shape (orange) and category (blue) judgments across the layers of HMAX models and convnets. Vertical dotted lines indicate where fully-connected layers start. In all plots error bars (or bands) represent 95% bootstrapped confidence intervals.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004896.g005" xlink:type="simple"/>
</fig>
<p>We employed this stimulus set to explore how categorical information is represented by convnets. As before, participants were asked to judge similarity among stimuli based either on their shape or on their category. Note that even for categorical judgments, participants were asked to rate categorical similarity rather than divide stimulus set into six categories, resulting in idiosyncratic categorical judgments and consistency between humans not reaching ceiling.</p>
<p>First, we found that convnets represented shape fairly well, correlating with perceptual human shape judgments between .3 and .4, nearly reaching the human performance limit (<xref ref-type="fig" rid="pcbi.1004896.g005">Fig 5C and 5D</xref>). Unlike before, the effect was not specific to deep models but was also observed in HMAX and even shallow models. This observation is expected because, unlike in previous experiments, in this stimulus set physical form and perceived shape are well correlated. Instead, the purpose of this stimulus set was to investigate to what extent semantic human category judgments are captured by convnets, since here category is dissociated from shape. We found that all deep but not shallow or HMAX models captured at least some semantic structure in our stimuli (<xref ref-type="fig" rid="pcbi.1004896.g005">Fig 5D and 5E</xref>; bootstrapped related samples significance test for deep vs. shallow and deep vs. HMAX: <italic>p</italic> &lt; .001), indicating that representations in convnets contain both shape and category information. Similar to Exp. 1, comparable correlations were observed even when the models were provided only with silhouettes of the objects (no texture), indicating that such categorical decisions appear to rely mainly on the shape contour and not internal features.</p>
<p>The abundance of categorical information in convnet outputs is most strikingly illustrated in <xref ref-type="fig" rid="pcbi.1004896.g005">Fig 5B</xref> where a multidimensional scaling plot depicts overall stimulus similarity. A nearly perfect separation between natural and manmade objects is apparent. Note that less than a half of these objects (23 out of 54) were known to GoogLeNet, but even completely unfamiliar objects are nonetheless correctly situated. This is quite surprising given that convnets were never trained to find associations between different categories. In other words, there is no explicit reason why a convnet should learn to represent guitars and flutes similarly (the category of “musical instruments” is not known to the model). We speculate that these associations might be learned implicitly, since during training objects of the same superordinate category (“musical instruments”) might co-occur in images. Further tests would be necessary to establish the extent of such implicit learning in convnets.</p>
<p>Despite its significance, the correlation with categorical judgments was much weaker than with shape, even after we restricted stimuli to the 23 objects in the ImageNet, meaning that the learned representations in convnets are largely based on shape and not category. In other words, categorical information is not as dominant in convnets as in humans, in agreement with [<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>] where deep nets were shown to account for categorical representations in humans only when categorical training was introduced on top of the outputs of convnets. (See also <xref ref-type="sec" rid="sec009">Discussion</xref> where we talk about the availability of information in models.)</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>Here we demonstrated that convolutional neural networks are not only superior in object recognition but also reflect perceptually relevant shape dimensions. In particular, convnets demonstrated robust similarities with human shape sensitivities in three demanding stimulus sets: (i) object recognition based solely on shape (Exp. 1), (ii) correlations with perceived rather than physical shape dimensions (Exp. 2), and (iii) sensitivity to non-accidental shape properties (Exp. 3). Notably, these shape-based representations emerged without convnets being trained for shape processing or recognition and without any explicit knowledge of our stimuli.</p>
<p>Furthermore, we demonstrated that convnets also develop abstract, or superordinate, category representations, but to a much smaller extent than shape (Exp. 4). In particular, we found that objects belonging to the same superordinate category (e.g., bananas and oranges belong to fruits) are represented more similarly than objects from different superordinate categories (e.g., bananas and violins).</p>
<p>These results expand the growing literature that convnets reflect human visual processing [<xref ref-type="bibr" rid="pcbi.1004896.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref011">11</xref>]. More specifically, the correspondence between object representations in primates and convnets has been investigated in the context of object recognition. Our study adds the new information that the powerful object recognition performance of convnets is related to a human-like sensitivity for shape and, to a lesser extent, perceptually-adequate semantic spaces of the objects they learn.</p>
<p>We emphasize that the reported sensitivity to shape reflects the representational spaces learned in convnets, as opposed to the information that is in principle available in them. Both approaches are useful and valid for evaluating how good a particular model is in processing visual information, but they provide different kinds of information. “Available information” means that a linear classifier, such as a Support Vector Machine, can learn (with supervision) to correctly classify stimuli into predefined categories. Thus, if an above-chance classification can be performed from a model’s output, it means that a model is making this the information necessary to perform a task explicit (object manifolds become untangled, as described in [<xref ref-type="bibr" rid="pcbi.1004896.ref038">38</xref>]). For instance, object categorization based on their pixel values does not work but after a series of transformations in convnets, categorical information becomes available. A better model for a particular task is then the one that has task-relevant information explicitly available.</p>
<p>While this approach provides a valuable information about model’s behavior, it does not directly provide evidence if it is a good model of human visual perception. In other words, the fact that a model is categorizing objects very well does not imply that it represents these categories similarly to humans. An explicit evaluation how well a model explains human data needs to be performed. One approach is to use model outputs to predict neural data, as used in [<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref010">10</xref>]. However, in this case model outputs are combined in a supervised fashion, so this approach also resembles the “available information” approach in that we use external knowledge to investigate the amount of information available in the model outputs.</p>
<p>Another approach is to compare the representational spaces in models and humans, as proposed in [<xref ref-type="bibr" rid="pcbi.1004896.ref039">39</xref>] and used in this study. By computing the similarity of stimulus representations in models and humans we can understand if models implicitly develop representations that match human representations. Critically, we are not asking if a model contains necessary information that could be used for a particular task. Rather, we are asking if a model is already representing this information in a similar way independent of a task. To illustrate this difference, consider, for instance, stimuli in Exp. 2a. Even a very simple shallow model, such as GaborJet, can learn to classify these nine stimuli into three perceptual categories (spikies, smoothies, and cubies) correctly because these dimensions are very simple and are easily accessible. Nonetheless, we found that representations in such simple models do not match human perception. Even though one could decode all the necessary information in stimuli from Exp. 2a to perform classification into perceptual categories, this kind of information is not made dominant in shallow models.</p>
<p>Again, both approaches are valid and meaningful, but it is important to be explicit about the implications that they bring. We emphasize that, despite only being trained on a task that has a clear correct answer to it (i.e., object recognition), convnets also develop representations that reflect subjective judgments of human observers. In our opinion, this difference is critical to the success of deep nets. Observe that in the typical convnet training, a particular image can have only a cat or a dog, or a car, or something else, always with a correct answer for any given image, and deep nets have been shown to learn this correspondence very well. However, this is very different from what humans typically do. The human visual system is particularly adept at making stable judgments about the environment when there is no clear or single answer to a given problem. For example, most individuals would report seeing a house rather that a stack of bricks even though both answers are technically correct. As a recent blunder of Google Photos app that labelled black people as gorillas [<xref ref-type="bibr" rid="pcbi.1004896.ref040">40</xref>] illustrates, a machine that is incapable of arriving to perceptually congruent decisions might be unacceptable in social settings where such common sense is expected. Although more data would clearly help, it is hard if not impossible to have training data for every possible situation. Uncertainty is bound to occur in natural scenarios (e.g., due to the lack of training samples or poor visibility), so a more error-prone strategy is making sure that machines are learning perceptually relevant dimensions that will generalize properly to unfamiliar settings. Such machines can become better than humans, but critically when they are wrong, their mistakes will be human-like. We therefore expect that further advancement of computer vision and artificial intelligence at large are critically dependent not only on improvement in benchmark performance but also in matching human perceptual judgments.</p>
<p>Our data also provide insights how convnets relate to the information processing across the visual cortex. First, we observed that early layers of convnets tended to relate to the physical stimulus dimensions, consistent with the known properties and models of early visual cortex [<xref ref-type="bibr" rid="pcbi.1004896.ref041">41</xref>]. Second, the output layer of convnets related to the perceived shape properties. Earlier human neuroimaging studies and monkey electrophysiological recordings revealed that these perceptual shape representations are implemented in human occipitotemporal cortex and in monkey inferotemporal cortex. Specifically, fMRI experiments with the stimulus set used in our Exp. 2a have shown that the shape-selective lateral occipital area might be mostly involved in this shape processing [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>]. Moreover, the geon stimulus set used in Exp. 3 also showed disproportionate sensitivity to non-accidental properties in monkey physiological recordings [<xref ref-type="bibr" rid="pcbi.1004896.ref024">24</xref>]. Finally, the stimulus set used in Exp. 4 showed a co-existence of shape and category information in human visual cortex and the dominance of categorical representations in the most anterior parts of it [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>]. Taken together, these results suggest that the shape representations in output layers of convnets relate to shape processing in higher visual areas in primates and their behavioral responses.</p>
<p>However, note that it is not necessarily the output layer which provides the best fit with shape representations in the primate brain. Given that the output layer is directly optimized to produce a correct category label rather than to represent shapes, it is possible that earlier layers are in fact better at capturing shape dimensions. Our results appear to be broadly consistent with this notion. However, these differences between layers seem to be small and the best intermediate layer is not consistent across experiments. Moreover, shape itself is a hierarchical concept that can be understood at multiple scales of analysis (e.g., local, global) and at multiple layers of abstraction [<xref ref-type="bibr" rid="pcbi.1004896.ref042">42</xref>], so it may not be possible to pinpoint an exact locus of shape representations neither in convnets nor in the human visual system. Rather, different dimensions of shape features might be distributed across multiple areas.</p>
<sec id="sec010">
<title>Causes for shape sensitivity in convnets</title>
<p>Our results suggest that a human-like sensitivity to shape features is a quite common property shared by different convnets, at least of the type that we tested. However, the three convnets were also very similar, since all of them very trained on the same dataset and used the same training procedure. Which convnet properties are important in developing such shape sensitivity?</p>
<p>One critical piece of information is offer by the comparison to HMAX models. Despite a similar architecture, in most experiments we observed that overall HMAX models failed to capture shape sensitivity to the same extent as convnets. The most obvious difference lies in the depth of the architecture. There are at most four layers in HMAX models but at least eight layers in the simplest of our convnets, CaffeNet. However, HMAX’99 (that has two layers) did not seem to perform consistently worse than HMAX-PNAS (that has four layers). Another important difference is the lack of supervision during training. As has been demonstrated before with object categorization [<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>], unsupervised training does not seem to be sufficiently robust, at least the way it is implemented in HMAX.</p>
<p>Another hint that supervision might be the critical component in learning universal shape dictionaries comes from comparing our results to the outputs obtained via the Hierarchical Modular Optimization (HMO) that was recently reported to correspond well to primate neural responses [<xref ref-type="bibr" rid="pcbi.1004896.ref010">10</xref>]. For Exps. 2a and 4, where we could obtain the outputs of the HMO layer that corresponds best to monkey neural data, we found largely similar pattern of results, despite differences in depth, training procedure, and training dataset. The only clear similarity between the tested convnets and HMO was supervised learning.</p>
<p>Finally, part of convnet power might also be attributed to the fully-connected layers. Both in CaffeNet and VGG-19, the critical preference for perceived shape emerges at the fully-connected layers. In GoogLeNet, the preference to perceptual dimensions is typically the strongest at the last layer that is also fully-connected, though earlier layers that are not fully-connected also exhibit a robust preference for perceived shape.</p>
<p>Other parameters, such as the naturalness of the training dataset or the task that convnet is optimized for, might also contribute to the representations that convnets develop. In short, the tests and the models that we have included in the present paper provide a general answer to our hypotheses about shape representations in convnets, but there are many specific questions about the role of individual variables that remain to be answered.</p>
</sec>
<sec id="sec011">
<title>Relation to theories of shape processing</title>
<p>In the literature, at least two theoretical approaches to shape processing have played an important role: image-based theories [<xref ref-type="bibr" rid="pcbi.1004896.ref019">19</xref>], which capitalize on processing image features without an explicit encoding of the relation between them, and structure-based theories [<xref ref-type="bibr" rid="pcbi.1004896.ref018">18</xref>], which emphasize the role of explicit structural relations in shape processing. Our results do not necessarily provide support for particular theories of shape processing. Of course, in their spirit convnets are closer to image-based theories since there is no explicit shape representation computed. On the other hand, in Exp. 3 we also found that convnets were sensitive to non-accidental properties even without ever being trained to use these properties. While in principle HMAX architectures can also develop sensitivity to non-accidental properties when a temporal association rule is introduced [<xref ref-type="bibr" rid="pcbi.1004896.ref043">43</xref>], the fact that such sensitivity automatically emerges in convnets when training for object categorization provides indirect support that non-accidental properties are diagnostic in defining object categories, as proposed by the RBC theory [<xref ref-type="bibr" rid="pcbi.1004896.ref016">16</xref>].</p>
<p>Of course, a mere sensitivity to non-accidental properties does not imply that convnets must actually utilize the object recognition scheme proposed by the RBC theory [<xref ref-type="bibr" rid="pcbi.1004896.ref016">16</xref>]. For instance, according to this theory, objects are subdivided into sets of shape primitives, known as geons, and recognized based on which geons compose that particular object, referred to as a “structural description” of the object. Finding an increased sensitivity for non-accidental properties does not necessarily imply that all these other assertions of the RBC theory are correct, and it does not by itself settle the controversy between image-based and structure-based models of object recognition.</p>
</sec>
<sec id="sec012">
<title>Are convnets <italic>the</italic> model of human visual processing?</title>
<p>While we demonstrate an unprecedented match between convnet representations and human shape perception, our experiments only capture a tiny fraction of the rich repertoire of human shape processing. It is clear from Exp. 1 that despite a strong performance, convnets remain about 20% worse than human observers at object recognition on silhouettes. Given that convnets are already very deep and were trained exhaustively, it may be a sign that in order to bridge this gap, convnets need additional layers dedicated to developing more explicit structural representations.</p>
<p>Another, more fundamental limitation is their feedforward architecture. Whereas humans are thought to be able to perform many object and scene recognition tasks in a feedforward manner [<xref ref-type="bibr" rid="pcbi.1004896.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1004896.ref046">46</xref>], they are certainly not limited to feedforward processing and in many scenarios will benefit from recurrent processing [<xref ref-type="bibr" rid="pcbi.1004896.ref047">47</xref>]. The role of such recurrent processes has been particularly debated in understanding perceptual organization, where the visual system is actively organizing the incoming information into larger entities [<xref ref-type="bibr" rid="pcbi.1004896.ref048">48</xref>]. For instance, monkey neurophysiology revealed that figure-ground segmentation benefits both from feedforward and feedback processes [<xref ref-type="bibr" rid="pcbi.1004896.ref049">49</xref>], and many models of segmentation utilize recurrent loops (for an in-depth discussion, see [<xref ref-type="bibr" rid="pcbi.1004896.ref050">50</xref>]). In contrast, despite their superior object categorization abilities, vanilla convnets show rather poor object localization results, with the top-performing model (GoogLeNet) in the ImageNet Large Scale Visual Recognition Challenge 2014 scoring 93.3% on a single object categorization task, yet localizing that object with only 73.6% accuracy [<xref ref-type="bibr" rid="pcbi.1004896.ref051">51</xref>].</p>
<p>In other words, we showed that convnets sensitivity to shape that reflects human judgments once the object itself can be easily extracted from the image. However, as soon as segmentation and other perceptual organization processes become more complicated, humans but not convnets can benefit from recurrent connections. Thus, recurrent neural networks, which incorporate the feedforward complexity of the tested convnets, might provide an even better fit to human perception than purely feedforward convnets.</p>
<p>Finally, we have also argued in [<xref ref-type="bibr" rid="pcbi.1004896.ref050">50</xref>] that feedforward architectures such as convnets might be lacking critical mechanisms that could contribute to the initial image segmentation. In our view, high performance at object recognition and shape processing tasks should not be taken as evidence that the “convolution-non-linearity-pooling” stack at the heart of convnets is necessarily the right or the full solution of feedforward visual processing yet. Small modifications to this architecture, such as adding feature map correlations [<xref ref-type="bibr" rid="pcbi.1004896.ref052">52</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref053">53</xref>] or performing VLAD [<xref ref-type="bibr" rid="pcbi.1004896.ref054">54</xref>] or Fisher Vector [<xref ref-type="bibr" rid="pcbi.1004896.ref055">55</xref>] pooling already provides convnets with the ability to segment input images and represent textures and artistic style, all of which might be the part of feedforward computations in human visual cortex.</p>
<p>Taken together, we demonstrated that convolutional neural networks trained for multiclass object categorization implicitly learn representations of shape that reflect human shape perception. Moreover, we showed that convnets also develop abstract semantic spaces independent of shape representations that provide a good, albeit weaker, match to human categorical judgments. Overall, our results provide an important demonstration that convnets are not limited to only extracting objective information from the visual inputs (such as object category) but can also represent the subjective aspects of visual information in accordance to human judgments. In other words, our work suggests that convnets might be a good candidate model for understanding various perceptual qualities of visual information.</p>
</sec>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Ethics statement</title>
<p>Studies reported here were approved by the Social and Societal Ethic Committee at KU Leuven (Exp. 1 and 2b) and the Massachusetts Institute of Technology’s Committee on the Use of Humans as Experimental Subjects (Exp. 1).</p>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>Materials</title>
<p>Almost all simulations were run with Python using the <italic>psychopy-ext</italic> package [<xref ref-type="bibr" rid="pcbi.1004896.ref056">56</xref>] that provides several simple shallow models and bindings to the Caffe library [<xref ref-type="bibr" rid="pcbi.1004896.ref057">57</xref>] and to several popular computer vision models (PHOG, PHOW, HMAX-HMIN, and HMAX-PNAS), written in MATLAB/C. For online data collection, we used <italic>mturkutils</italic>, a Python interface package to Amazon Mechanical Turk. For data collection in in Exp. 2b, we used similarity rating interface in MATLAB from [<xref ref-type="bibr" rid="pcbi.1004896.ref058">58</xref>].</p>
<p>For data analysis, we used several popular free and open source Python packages, including <italic>numpy</italic>, <italic>scipy</italic>, <italic>scikits-learn</italic>, <italic>scikits-image</italic> [<xref ref-type="bibr" rid="pcbi.1004896.ref059">59</xref>], <italic>pandas</italic>, <italic>seaborn</italic>, <italic>statsmodels</italic>, and <italic>NLTK</italic> [<xref ref-type="bibr" rid="pcbi.1004896.ref060">60</xref>]. The code and stimulus sets for all of our simulations are available publicly at <ext-link ext-link-type="uri" xlink:href="https://osf.io/jf42a" xlink:type="simple">https://osf.io/jf42a</ext-link>, except in the cases when the stimulus set is already available online (links to these stimulus sets are provided in the repository and in the text) or subject to copyright restrictions (stimulus set for Exp. 4). For a maximal reproducibility, all results reported in this manuscript can be generated with a single command: python run.py report—bootstrap.</p>
</sec>
<sec id="sec016">
<title>Stimulus sets</title>
<p>All stimuli were scaled to 256×256 px size. Convnets further downsampled these images to their own predefined image sizes (typically around 224×224 px). Stimuli pixel intensities were rescaled to the range between 0 and 1, where 0 corresponds to a black pixel and 1 corresponds to a white pixel, and, for deep models, the mean of the ImageNet training set was subtracted. No further processing was done.</p>
<sec id="sec017">
<title>Snodgrass and Vanderwart stimulus set of common objects [<xref ref-type="bibr" rid="pcbi.1004896.ref020">20</xref>]</title>
<p>This is a popular set of 260 common everyday objects (one exemplar per category). The complete stimulus set is available at <ext-link ext-link-type="uri" xlink:href="http://wiki.cnbc.cmu.edu/images/SVLO.zip" xlink:type="simple">http://wiki.cnbc.cmu.edu/images/SVLO.zip</ext-link>. In our experiments, we used its updated version that is freely available and contains images in [<xref ref-type="bibr" rid="pcbi.1004896.ref021">21</xref>]. Moreover, since not all object categories in this stimulus set were contained in the training set of convnets, we selected a subset of categories for our experiments. First, for each category, we determined their WordNet synsets [<xref ref-type="bibr" rid="pcbi.1004896.ref061">61</xref>], so that we could directly relate them to the ImageNet categories used for convnet training. We selected only those categories that exactly matched the ones in the ImageNet training set. While we could have attempted to include more categories by allowing more specific terms (e.g., <italic>apple</italic> could have been included since <italic>Granny Smith</italic> is in the ImageNet training set), it is not clear whether such an inclusion would be valid. For instance, it is possible that only a green apple would be labeled as an apple whereas a red one would be more often labeled as a peach or a cherry simply because the models were trained on green apples only. Thus, we opted for a conservative set of 61 categories.</p>
<p>Starting from the color images, we produced two further variants of the stimulus set:</p>
<list list-type="bullet">
<list-item><p>Grayscaled images by converting the original images to 256 shades of gray.</p></list-item>
<list-item><p>Silhouettes by filling the entire object with black. Note that this procedure also fills in any cavities inside objects. For example, the silhouette of a donut would appear as a disk. We used this procedure to be consistent with the stimuli used by [<xref ref-type="bibr" rid="pcbi.1004896.ref015">15</xref>].</p></list-item>
</list>
<p>Behavioral human responses were collected online using Amazon Mechanical Turk platform. For each variant of the stimulus set, ten participants were recruited to type the names of each object. The full stimulus set of 260 stimuli was used in this task. Each participant completed only one variant of the stimulus set. An image was shown for 100 ms, followed by a gap of 500 ms until a response field appeared. The participants were asked to type the name of the object they saw. To make the task more similar to model evaluation where a model is choosing from a list of available nouns, participants could only type answers from the list of 260 categories, their synonyms (as reported in [<xref ref-type="bibr" rid="pcbi.1004896.ref020">20</xref>]), and several extra synonyms that appeared to be valid but were not present in[<xref ref-type="bibr" rid="pcbi.1004896.ref020">20</xref>] (namely, <italic>stroller</italic> and <italic>pram</italic> for <italic>baby carriage</italic>, <italic>doorhandle</italic> for <italic>doorknob</italic>, <italic>fridge</italic> for <italic>refrigerator</italic>, and <italic>bunny</italic> and <italic>hare</italic> for <italic>rabbit</italic>). In total, there were 657 options. Initially, the response field was empty but as participants typed in the first three characters, a list of suggestions appeared, and the participants were forced to choose from them. A correct response was counted if participant’s response either exactly matched the original name or was in the list of synonyms. Note that for several categories some synonyms overlapped.</p>
<p>We requested that only participants fluent in English completed the task, and participants could stop doing the task at any point if they felt it was too difficult for them. However, as there is no simple way to enforce this requirement in Amazon Mechanical Turk, it is possible that some of the reported mistakes resulted from the lack of fluency in English. On the other hand, on average participants performed slightly better than reported in previous well-controlled studies [<xref ref-type="bibr" rid="pcbi.1004896.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref021">21</xref>] despite much shorter stimulus presentation times, though this may also be driven by the differences in naming procedure (free labelling in those studies as compared to a choice task in our case). Moreover, there was a substantial agreement in naming patterns between [<xref ref-type="bibr" rid="pcbi.1004896.ref015">15</xref>] on silhouettes and our study (correlation = .79, consistency = .94). The study took about half an hour to complete and participants were paid 50 cents for their help.</p>
<p>CaffeNet, VGG-19, and GoogLeNet were provided with the images from these three variants to produce a single best guess for each image. To match this task to human responses, the models could only choose from one of 61 possible answers (that is, responses not in the list of our 61 categories were not permitted). A correct answer was recorded that label matched the original synset. While a smaller search space may give an advantage to models as compared human observers who had a much larger search space (657 concepts), we note that humans (but not models) were also permitted to provide rather generic responses (such as <italic>bird</italic> instead of <italic>peacock</italic>), which should have made the task easier. However, it is possible that human performance would be even better if they were to choose among 61 categories only. (that is, the original synset or one of its synonyms).</p>
</sec>
<sec id="sec018">
<title>Op de Beeck et al. stimulus set of spiky, smoothie, cubie [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>]</title>
<p>Op de Beeck and colleagues [<xref ref-type="bibr" rid="pcbi.1004896.ref022">22</xref>] investigated shape representations in human visual cortex using a novel generative stimulus set of three-dimensional shapes, belonging to one of three categories:</p>
<list list-type="bullet">
<list-item><p>Spikies–shapes with spiky appearance and sharp local features</p></list-item>
<list-item><p>Smoothies–shapes defined my smooth surfaces</p></list-item>
<list-item><p>Cubies–shapes consisting of largely parallel and orthogonal surfaces</p></list-item>
</list>
<p>Additionally, several parameters controlling the overall shape properties, such as aspect ratio and symmetry, can be manipulated. For their later study, Op de Beeck and colleagues [<xref ref-type="bibr" rid="pcbi.1004896.ref005">5</xref>] created a stimulus set of nine classes where the shape envelope (physical form) and stimulus category (or “perceived shape”) were manipulated orthogonally. They presented the 20 exemplars from each class in various sequences and asked eight participants to rate shape similarity of the current shape to the preceding one. These shapes were also used in a separate functional magnetic resonance imaging (fMRI) experiment.</p>
<p>For our experiments, we used the behavioral data and the prototype (average exemplars) from the nine shape classes, as reported in the fMRI experiment. All resources are shared in the code repository of this project.</p>
</sec>
<sec id="sec019">
<title>Fonts stimulus set</title>
<p>We compiled a new stimulus set specifically for this paper using the first six letters from six constructed fonts: Arcadian, Atlantean, Dovahzul, Futurama Alien Alphabet, Hymmnos, and ULOG (fonts were downloaded from the internet; the resulting stimuli are provided in the code repository of the project). The specific fonts were chosen arbitrarily. The letters were scaled to be approximately the same size, and presented in black on a white background. We asked eight participants (2 males, 6 right-handed) to arrange these 36 shapes on the screen according to their similarity using the interface from [<xref ref-type="bibr" rid="pcbi.1004896.ref058">58</xref>]. The resulting distances in this two-dimensional space were projected into a multidimensional dissimilarity matrix using the method proposed by [<xref ref-type="bibr" rid="pcbi.1004896.ref058">58</xref>]. Similarity judgments took about 50 minutes to complete and participants received course credit for their help.</p>
</sec>
<sec id="sec020">
<title>Kayaert et al. geon stimulus set [<xref ref-type="bibr" rid="pcbi.1004896.ref024">24</xref>]</title>
<p>Kayaert and colleagues [<xref ref-type="bibr" rid="pcbi.1004896.ref024">24</xref>] created a stimulus set of 22 geon triplets, where each triplet consists of a non-accidental and a metric geon that each differ slightly from the base geon but in opposite directions. Whereas the metric variant appears rather similar to the base stimulus, the non-accidental variant differs from the base stimulus in terms of a particular ‘non-accidental’ feature. Such features remain stable in their two-dimensional retinal projection independent of the viewing angle and have been postulated as a basis of object recognition [<xref ref-type="bibr" rid="pcbi.1004896.ref016">16</xref>]. For example, a curved stimulus axis remains curved from nearly all viewing angles (except for several very specific cases, known as ‘accidental views’, where it is seen straight). In contrast, a right angle does not remain right in a two-dimensional projection, and thus is not a non-accidental feature. Importantly, despite the fact that perceptually non-accidental variants appear more dissimilar from the base that the metric ones, when measured by their pixelwise or gaborjet difference, that is, using some simple linear metric, non-accidental variants are in fact more similar to the base geons than the metric ones.</p>
<p>This stimulus set has been used in several subsequent studies [<xref ref-type="bibr" rid="pcbi.1004896.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1004896.ref036">36</xref>] and is freely available online at <ext-link ext-link-type="uri" xlink:href="http://geon.usc.edu/~ori/" xlink:type="simple">http://geon.usc.edu/~ori/</ext-link>.</p>
</sec>
<sec id="sec021">
<title>Bracci and Op de Beeck stimulus set [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>]</title>
<p>In order to study how the visual system represents shape and category and whether the two dimensions are processed by the same visual areas, [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>] created a stimulus set where these two dimensions are orthogonal. In particular, for each stimulus in a particular category there is another stimulus in another category that has been carefully matched in terms of its shape. The stimulus set consists of six categories (minerals, animals, fruits/vegetables, music, sport equipment, and tools) that could be further divided into natural (minerals, animals, fruits/vegetables) and manmade (music, sport equipment, tools). Each category is composed of nine grayscale objects, each with a unique shape within a category, thus the stimulus set could also be divided into nine shape groups.</p>
<p>In a behavioral experiment, [<xref ref-type="bibr" rid="pcbi.1004896.ref037">37</xref>] asked 16 participants to judge object similarity by dragging stimuli on the screen such that more similar stimuli were closer (multiple object arrangement method; [<xref ref-type="bibr" rid="pcbi.1004896.ref052">52</xref>]). For shape similarity, participants were asked to arrange the images based on perceived object shape similarity. For semantic category similarity, participants were asked to arrange the images based on the semantic similarity among objects. The resulting distances in this two-dimensional space were projected into a multidimensional dissimilarity matrix using the method proposed by [<xref ref-type="bibr" rid="pcbi.1004896.ref052">52</xref>], and averaged across participants for correlational analyses in this manuscript.</p>
<p>Due to copyright restrictions, this stimulus set is only available upon request from the authors.</p>
</sec>
</sec>
<sec id="sec022" sec-type="materials|methods">
<title>Models</title>
<p>We used three groups of models: shallow, HMAX, and deep. Shallow models consist of a single layer of processing, and all features are built manually (i.e., there is no training). In contrast, HMAX and deep networks have a hierarchical feedforward architecture and have been trained for object categorization. However, HMAX models are not as deep (up to four layers) and have not been trained very optimally (either by manual feature selection or by imprinting stimulus selectivity), whereas deep nets acquire their features through training by backpropagation, which operates on all the weights in the network.</p>
<sec id="sec023">
<title>Shallow models</title>
<p>To compare deep net performance to several shallow (single-layer) architectures, we ran our experiments on the following popular models:</p>
<list list-type="bullet">
<list-item><p><bold>Pixelwise</bold> model that simply provides the values of all pixels as an output. This model provides a useful baseline for controlling how much low-level information is available in a raw image prior to any processing. In our tests, we used a Python implementation of the pixelwise model, available in the <italic>psychopy_ext</italic> package [<xref ref-type="bibr" rid="pcbi.1004896.ref056">56</xref>].</p></list-item>
<list-item><p><bold>GaborJet</bold> model [<xref ref-type="bibr" rid="pcbi.1004896.ref062">62</xref>], a simplistic V1-like model used as a baseline in many studies by Biederman and others (e.g., [<xref ref-type="bibr" rid="pcbi.1004896.ref031">31</xref>]). This model computes a convolution of an image with a set of Gabor filters of five spatial frequencies and eight orientations, placed on a regular 10x10 grid. In our tests, we used a Python implementation of GaborJet, available in the <italic>psychopy_ext</italic> package [<xref ref-type="bibr" rid="pcbi.1004896.ref056">56</xref>].</p></list-item>
<list-item><p><bold>Histogram of Oriented Gradient (HOG)</bold> model [<xref ref-type="bibr" rid="pcbi.1004896.ref063">63</xref>] that produces a histogram of normalized gradients (i.e., orientations) in local image patches. Thus, it computes the distribution of orientations present in the image without caring much where these orientations are located in the image. In our experiments, we used a HOG implementation available from the <italic>scikit-image</italic> Python package [<xref ref-type="bibr" rid="pcbi.1004896.ref059">59</xref>] with the default parameters of nine gradient orientations and 3x3 image patches of 8x8 px each.</p></list-item>
<list-item><p><bold>Pyramid Histogram of Oriented Gradient (PHOG)</bold> model, introduced by [<xref ref-type="bibr" rid="pcbi.1004896.ref064">64</xref>] and inspired by the PHOW descriptor [<xref ref-type="bibr" rid="pcbi.1004896.ref065">65</xref>], that computes the HOG descriptor over several spatial scales and concatenates them. The resulting PHOG descriptor is similar to PHOW but is much simpler (and presumably better) and similar to SIFT but is much denser since features are extracted at all locations. We used a MATLAB implementation available from the authors’ website (<ext-link ext-link-type="uri" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/caltech/phog.html" xlink:type="simple">http://www.robots.ox.ac.uk/~vgg/research/caltech/phog.html</ext-link>) with four spatial scales (dividing the image into 4, 16, and 64 bins, as well as using the whole image), eight quantization bins, and the range of orientations from 0° to 360°.</p></list-item>
<list-item><p><bold>Pyramid Histogram of Visual Words (PHOW)</bold> model [<xref ref-type="bibr" rid="pcbi.1004896.ref065">65</xref>] that computes a dense SIFT descriptor (that captures local orientations) over multiple scales [<xref ref-type="bibr" rid="pcbi.1004896.ref035">35</xref>] and quantizes the computed features across all images using k-means, thus learning a multiscale bag of visual words for that stimulus set. Then feature matching to this dictionary is performed for all those scales in the image, and the resulting histograms are concatenated. We used the authors’ MATLAB implementation (<ext-link ext-link-type="uri" xlink:href="http://web.engr.illinois.edu/~slazebni/research/SpatialPyramid.zip" xlink:type="simple">http://web.engr.illinois.edu/~slazebni/research/SpatialPyramid.zip</ext-link>) with default parameters (three spatial scales, corresponding to the whole image and its division into 4 and 16 boxes, and a dictionary of 200 words).</p></list-item>
</list>
</sec>
<sec id="sec024">
<title>HMAX models</title>
<p>HMAX is a family of architectures that consist of a hierarchy of simple and complex layers. Simple layers are built to respond to particular features in the input (e.g., edges, corners), whereas complex layers max pool over outputs of the simple units to introduce invariance to feature transformations (e.g., translation, scale). Over the years, several generations of HMAX models have been proposed:</p>
<list list-type="bullet">
<list-item><p><bold>HMAX’99</bold> is the original HMAX architecture proposed by [<xref ref-type="bibr" rid="pcbi.1004896.ref001">1</xref>]. The first layer, S1, features Gabor filters of four spatial frequency bands and four orientations. Locally maximally responding units are then pooled in layer C1. Next, combinations of these edge filters are produced in layer S2, roughly mimicking corner detection, and a maximum in computed in layer C2. A final View-Tuned Unit (VTU) layer can be introduced to perform template matching to a specific dataset (not used in our experiments). This model is no longer actively used, but remains relevant for a comparison to earlier studies that used it as a benchmark. In our tests, we used a Python port of this model, available in the <italic>psychopy_ext</italic> package [<xref ref-type="bibr" rid="pcbi.1004896.ref056">56</xref>]. Since convnets usually treat convolution and pooling as a part of a single layer, here (and in other HMAX models) we only report outputs of the C layers and not both S and C. The C layers were chosen because (i) C2 layer but not S2 layer is also the output layer, (ii) HMIN only provides access to the C2 layer, and (iii) HMAX-PNAS by default only reports C layers, so for comparison with previous results C layers are more important. The VTU layer was not used.</p></list-item>
<list-item><p><bold>HMIN</bold> is the minimal implementation of the current HMAX model. It is still composed of the four layers as HMAX’99 but, critically, layer S2 is no longer manually crafted but trained on natural images by imprinting unit responses to these images (see the Supplementary text of [<xref ref-type="bibr" rid="pcbi.1004896.ref003">3</xref>] for details). We used the model from the authors’ website (<ext-link ext-link-type="uri" xlink:href="http://cbcl.mit.edu/jmutch/hmin/" xlink:type="simple">http://cbcl.mit.edu/jmutch/hmin/</ext-link>) with the default parameters and the default dictionary of 4075 features. Only data from layer C2 is available.</p></list-item>
<list-item><p><bold>HMAX-PNAS</bold> is the most elaborated version of HMAX that has been shown to perform rapid animal detection is a similar fashion to humans [<xref ref-type="bibr" rid="pcbi.1004896.ref003">3</xref>]. It is a deeper hierarchy than the original model, composed of nine layers (S1, C1, S2, C2, S2b, C2b, S3, C3, S4), and features bypass routes as well as training of S2, S2b, and S3 layers on natural images, as described for HMIN. We used the model from the authors’ website (<ext-link ext-link-type="uri" xlink:href="http://cbcl.mit.edu/software-datasets/pnas07/index.html" xlink:type="simple">http://cbcl.mit.edu/software-datasets/pnas07/index.html</ext-link>) with default parameters, except that all available features were used (C1: 78244, C2: 3124000, C2b: 2000, C3: 2000), whereas the default was selecting 1500 random features for all layers.</p></list-item>
</list>
</sec>
<sec id="sec025">
<title>Deep models</title>
<p>In order to explore the generality of shape processing in convnets, the experiments were conducted using three popular convolutional neural networks (convnets). In a nutshell, convnets are simply multilayer perceptrons with two unique properties: (i) convolutions, such that inputs are convolved with a small kernel, typically around 3×3 px, which means that computations are restricted to local neighborhoods only and that massively reduces the number of computations, and (ii) weight sharing, such that all units (“neurons”) in a particular layer have the same weights, which massively reduces the number of parameters in the model. These simplifications are critical in building models that a manageable, that is, fit in computer memory and can be trained. To increase representational power, each layer contains multiple feature maps (typically in the order of hundreds) each trained to respond to different features in the input.</p>
<p>A layer in a convnet typically is a stack of thee operations: (i) convolution, (ii) non-linearity (usually rectified linear unit (ReLU), and (iii) max and/or average pooling that can be thought of as analogous to the increasing receptive field sizes and tolerance to transformations in biological networks. Note that not all of these operations need to be present in a layer, and it is not always clear what exactly constitutes a layer. Often the last few layers are fully-connected, which is a particular type of convolutional layer with a kernel size of 1×1 px and a feature map usually in the order of several thousands. The output layer is the best example of such fully-connected scheme, where the inputs from the penultimate layer are collapsed to the number of output categories. Thus, in a convnet architecture, an input image is gradually and non-linearly transformed into an output vector that corresponds to the probability of each category known to the model being present in the input.</p>
<p>The resulting architectures have several tens of millions of parameters that need to be learned by the model in order to perform recognition tasks. In a nutshell, first the model is initialized with random weights. An input image is passed through the network, and the resulting category probabilities are compared to the correct label provided by the experimenter. Since the goal is to maximize the chance of a correct response, all incorrect responses count towards the loss of the model, and the weights in the model are updated so as to minimize this loss. As the update happens from top (output) to bottom (input), this procedure is referred to as backpropagation.</p>
<p>In our experiments, all models have been trained on the typical single object categorization task using the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2012 dataset containing about 1.2 million images, divided into 1000 categories [<xref ref-type="bibr" rid="pcbi.1004896.ref066">66</xref>]. The particular implementation of these models is provided by the Caffe library [<xref ref-type="bibr" rid="pcbi.1004896.ref067">67</xref>]. We used three models that varied in their architecture and depth:</p>
<list list-type="bullet">
<list-item><p><bold>CaffeNet</bold>, a canonical (“vanilla”) implementation of convolutional neural networks offered by the Caffe library that closely resembles the original architecture used by [<xref ref-type="bibr" rid="pcbi.1004896.ref007">7</xref>] in the ILSVRC-2012. The model consists of five convolutional and three fully-connected layers. The particular model available from Caffe achieves 80.1% top-5 accuracy on the validation set [<xref ref-type="bibr" rid="pcbi.1004896.ref068">68</xref>].</p></list-item>
<list-item><p><bold>VGG-19 (referred to as VGG-CNN-D in Caffe;</bold> [<xref ref-type="bibr" rid="pcbi.1004896.ref069">69</xref>]), a runner-up at the ILSVRC 2014 that provides a clean and a very deep implementation of 19 layers. The whole network is analogous to the CaffeNet, but each convolutional layer is split into several convolutional layers (i.e., first and second layers are each split into two, the rest into four) of fixed kernel size (3×3 px) and increasing number of feature maps (64, 128, 256, 512, 512). The particular implementation available from Caffe achieves 88.5% top-5 accuracy on the validation dataset [<xref ref-type="bibr" rid="pcbi.1004896.ref068">68</xref>].</p></list-item>
<list-item><p><bold>GoogLeNet</bold> [<xref ref-type="bibr" rid="pcbi.1004896.ref009">9</xref>], a reimplementation of the winning architecture of the ILSVRC 2014 [<xref ref-type="bibr" rid="pcbi.1004896.ref051">51</xref>], available as part of the Caffe library and trained similarly to CaffeNet. Unlike previous models, this model is composed of the so-called inception modules that compute several convolutions and a max pooling in parallel using different kernel sizes, resulting in a complex non-sequential architecture, which is the reason behind the “jittery” look in our plots. Inception modules have been argued to allow for an increase in available units with little computational cost and to provide access to multiscale representations at each layer [<xref ref-type="bibr" rid="pcbi.1004896.ref009">9</xref>]. The model has 22 layers that have parameters, thus constituting a very deep model. Given the depth and complexity of the model, there are three auxiliary classifiers spread at mid- and higher-levels of the architecture that aid in training the entire network. The particular model available from Caffe achieves 89.0% top-5 accuracy on the validation set [<xref ref-type="bibr" rid="pcbi.1004896.ref068">68</xref>].</p></list-item>
</list>
<p>In our experiments, the final fully-connected layer with 1000 units was reported for all models. In the plots that show all layers, outputs of convolutional and fully-connected layers were reported (thus, no ReLU or pooling layers were reported).</p>
</sec>
</sec>
<sec id="sec026">
<title>Correlational analyses</title>
<p>We computed correlations between a particular layer of a model and behavioral data by first computing a dissimilarity matrix between stimuli and then correlating the resulting dissimilarity matrices. We also used a one minus a Pearson correlation as a distance between stimuli as a metric. Since these dissimilarity matrices are symmetric, we used only the upper triangle to compute correlations. Both correlations were computed using the following formula:
<disp-formula id="pcbi.1004896.e001">
<alternatives>
<graphic id="pcbi.1004896.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004896.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <italic>x</italic> and <italic>y</italic> correspond to either the outputs of a given model to two different stimuli (for the dissimilarity matrix computation) or the values in the upper triangle of these dissimilarity matrices (for correlational analyses). We also conducted the analyses using a normalized Euclidean distance as a metric for producing dissimilarity matrices, but pattern of results remained the same, indicating that the choice of a metric has little effect on our findings.</p>
<p>The upper and lower bounds of the ceiling performance (shown as a gray band in figures) were estimated using the procedure described in [<xref ref-type="bibr" rid="pcbi.1004896.ref070">70</xref>]. The upper bound was estimated by computing the average Pearson correlation between each participant’s dissimilarity matrix and the average across all participants after z-scoring data. The lower bound was estimated by computing the average Pearson correlation between each participant’s dissimilarity matrix and the average across the remaining participants after z-scoring data.</p>
</sec>
<sec id="sec027">
<title>Consistency analyses</title>
<p>In Exp. 1, the consistency between human and model (or between two models) was computed as one minus a normalized squared Euclidean distance between the corresponding accuracy vectors <italic>x</italic> and <italic>y</italic>:
<disp-formula id="pcbi.1004896.e002">
<alternatives>
<graphic id="pcbi.1004896.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004896.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>This metric is a version of the Matching distance that is used for estimating dissimilarity in binary data generalized to the non-logistic case. We chose this metric due to the largely (but not fully) logistic nature of our data (which was not the case in correlational analyses). In particular, this consistency measure is high when most of the responses match, whereas a correlation is very low as there is little variance in the data (i.e., mostly 1’s). The same consistency measure was used for estimating upper and lower bounds of the ceiling performance based on human accuracy instead of the Pearson correlation as described in the correlational analyses.</p>
</sec>
<sec id="sec028">
<title>Bootstrapping</title>
<p>In order to estimate the reliability of effects in our experiments, we used bootstrapping approach (number of iterations was always 1000). In <xref ref-type="fig" rid="pcbi.1004896.g001">Fig 1C</xref> and <xref ref-type="fig" rid="pcbi.1004896.g004">Fig 4</xref>, we computed 95% confidence intervals by resampling with replacement model responses (that is, the vector of 0’s and 1’s that indicates whether model’s response was correct or not), and computing the mean accuracy at each iteration. The 95% confidence interval was the reported as the 2.5% and 97.5% percentiles of the bootstrapped accuracy distribution</p>
<p>To estimate the reliability of correlations in the correlational analyses (all other figures), we computed bootstrapped confidence intervals as described in [<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>]. In particular, dissimilarity matrices were resampled with replacement, and the resulting matrices where correlated for a total of 1000 iterations. Note that diagonal entries in the original dissimilarity matrix were undefined, so these entries were removed from the resampled matrices as well. Again, as done in [<xref ref-type="bibr" rid="pcbi.1004896.ref070">70</xref>], only the upper triangle was used for correlations. In Exp. 2b and Exp. 4, the bootstrapping procedure was carried out in a stratified manner, such that a resampling was done only within each class of stimuli, as done in [<xref ref-type="bibr" rid="pcbi.1004896.ref006">6</xref>]. In particular, for shape, stimuli were resampled from the same shape class (e.g., a new sample for elongated vertical shapes was sampled from the nine available elongated vertical shapes only), whereas for semantical category, stimuli were resampled from the same category (e.g., a new sample for fruits category was sampled from the six available fruit images only). Resampling without stratification yielded qualitatively comparable results. In Exp. 2a and Exp. 3, no stratification was used because there were too few stimuli (Exp. 2a) or no categories (Exp. 3). Confidence intervals were computed as the 2.5% and 97.5% percentiles of the bootstrap distribution.</p>
<p>To estimate whether shallow, HMAX, and deep models differed, we used a bootstrapped paired-samples significance test (independent-samples significance test gave largely similar results). For each bootstrap resampling, model performance (correlation with the behavioral or pixelwise dissimilarity matrices) was averaged across models within a group, and the difference in average performance was computed for each pair of groups (3 pairwise comparisons in total). Across iterations, this yielded many such differences, which together formed the distribution used for statistical inference (one for each pair of groups). The percentile of scores below zero in each such distribution of differences was reported as the <italic>p</italic>-value.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Dan Yamins for useful discussions and assistance in obtaining the HMO outputs, Ori Amir for his assistance in providing the behavioral data for non-accidental properties, and Pieter Moors for providing insights into computing statistics. We also thank Kęstutis Kubilius and Vilnius University Institute of Mathematics and Informatics (Vilnius, Lithuania) for technical support.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004896.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source>. <year>1999</year> <month>Nov</month>;<volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>–<lpage>25</lpage>. <object-id pub-id-type="pmid">10526343</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref002"><label>2</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <chapter-title>How the visual cortex recognizes objects: The tale of the standard model</chapter-title>. In: <name name-style="western"><surname>Werner</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Chalupa</surname> <given-names>LM</given-names></name>, editors. <source>The Visual Neurosciences</source>. <publisher-name>MIT Press</publisher-name>; <year>2004</year>. p. <fpage>1640</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2007</year> <month>Apr</month> <day>10</day>;<volume>104</volume>(<issue>15</issue>):<fpage>6424</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">17404214</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Esteky</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source>. <year>2008</year> <month>Dec</month> <day>26</day>;<volume>60</volume>(<issue>6</issue>):<fpage>1126</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.10.043" xlink:type="simple">10.1016/j.neuron.2008.10.043</ext-link></comment> <object-id pub-id-type="pmid">19109916</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Op de Beeck</surname> <given-names>HP</given-names></name>, <name name-style="western"><surname>Torfs</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>. <article-title>Perceived shape similarity among unfamiliar objects and the organization of the human object vision pathway</article-title>. <source>J Neurosci</source>. <year>2008</year> <month>Oct</month>;<volume>28</volume>(<issue>40</issue>):<fpage>10111</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2511-08.2008" xlink:type="simple">10.1523/JNEUROSCI.2511-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18829969</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year> <month>Nov</month> <day>6</day>;<volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <chapter-title>ImageNet classification with deep convolutional neural networks</chapter-title>. In: <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burges</surname> <given-names>CJC</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>25</volume> [Internet]. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2012</year>. p. <fpage>1097</fpage>–<lpage>105</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" xlink:type="simple">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lapedriza</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Xiao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <chapter-title>Learning Deep Features for Scene Recognition using Places Database</chapter-title>. In: <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Welling</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cortes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lawrence</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>27</volume> [Internet]. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2014</year> [cited 2015 Jul 24]. p. <fpage>487</fpage>–<lpage>95</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf" xlink:type="simple">http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref009"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, et al. Going deeper with convolutions. ArXiv14094842 Cs [Internet]. 2014 Sep 16 [cited 2014 Sep 28]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.4842" xlink:type="simple">http://arxiv.org/abs/1409.4842</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2014</year> <month>May</month> <day>8</day>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment> <object-id pub-id-type="pmid">24812127</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MAJ</given-names></name>. <article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title>. <source>J Neurosci</source>. <year>2015</year> <month>Jul</month> <day>8</day>;<volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>K</given-names></name>. <article-title>Inferotemporal cortex and object vision</article-title>. <source>Annu Rev Neurosci</source>. <year>1996</year> <month>Jan</month>;<volume>19</volume>:<fpage>109</fpage>–<lpage>39</lpage>. <object-id pub-id-type="pmid">8833438</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname> <given-names>CE</given-names></name>. <article-title>Shape Representation in Area V4: Position-Specific Tuning for Boundary Conformation</article-title>. <source>J Neurophysiol</source>. <year>2001</year> <month>Nov</month> <day>1</day>;<volume>86</volume>(<issue>5</issue>):<fpage>2505</fpage>–<lpage>19</lpage>. <object-id pub-id-type="pmid">11698538</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Reddy</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name>. <article-title>Invariant visual representation by single neurons in the human brain</article-title>. <source>Nature</source>. <year>2005</year> <month>Jun</month> <day>23</day>;<volume>435</volume>(<issue>7045</issue>):<fpage>1102</fpage>–<lpage>7</lpage>. <object-id pub-id-type="pmid">15973409</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Winter</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Beeck</surname> <given-names>HO de</given-names></name>, <name name-style="western"><surname>Ploeger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beckers</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Vanroose</surname> <given-names>P</given-names></name>. <article-title>Identification of everyday objects on the basis of silhouette and outline versions</article-title>. <source>Perception</source>. <year>2008</year>;<volume>37</volume>(<issue>2</issue>):<fpage>207</fpage>–<lpage>44</lpage>. <object-id pub-id-type="pmid">18456925</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Recognition-by-components: A theory of human image understanding</article-title>. <source>Psychol Rev</source>. <year>1987</year>;<volume>94</volume>(<issue>2</issue>):<fpage>115</fpage>–<lpage>47</lpage>. <object-id pub-id-type="pmid">3575582</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Dynamic binding in a neural network for shape recognition</article-title>. <source>Psychol Rev</source>. <year>1992</year>;<volume>99</volume>(<issue>3</issue>):<fpage>480</fpage>–<lpage>517</lpage>. <object-id pub-id-type="pmid">1502274</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Gerhardstein</surname> <given-names>PC</given-names></name>. <article-title>Recognizing depth-rotated objects: evidence and conditions for three-dimensional viewpoint invariance</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1993</year> <month>Dec</month>;<volume>19</volume>(<issue>6</issue>):<fpage>1162</fpage>–<lpage>82</lpage>. <object-id pub-id-type="pmid">8294886</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>. <article-title>Is human object recognition better described by geon structural descriptions or by multiple views? Comment on Biederman and Gerhardstein (1993)</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1995</year> <month>Dec</month>;<volume>21</volume>(<issue>6</issue>):<fpage>1494</fpage>–<lpage>505</lpage>. <object-id pub-id-type="pmid">7490590</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Snodgrass</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Vanderwart</surname> <given-names>M</given-names></name>. <article-title>A standardized set of 260 pictures: Norms for name agreement, image agreement, familiarity, and visual complexity</article-title>. <source>J Exp Psychol [Hum Learn]</source>. <year>1980</year> <month>Mar</month>;<volume>6</volume>(<issue>2</issue>):<fpage>174</fpage>–<lpage>215</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rossion</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pourtois</surname> <given-names>G</given-names></name>. <article-title>Revisiting Snodgrass and Vanderwart’s object pictorial set: The role of surface detail in basic-level object recognition</article-title>. <source>Perception</source>. <year>2004</year>;<volume>33</volume>(<issue>2</issue>):<fpage>217</fpage>–<lpage>36</lpage>. <object-id pub-id-type="pmid">15109163</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Op de Beeck</surname> <given-names>HP</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>NG</given-names></name>. <article-title>Discrimination Training Alters Object Representations in Human Extrastriate Cortex</article-title>. <source>J Neurosci</source>. <year>2006</year> <month>Dec</month> <day>13</day>;<volume>26</volume>(<issue>50</issue>):<fpage>13025</fpage>–<lpage>36</lpage>. <object-id pub-id-type="pmid">17167092</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref023"><label>23</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lowe</surname> <given-names>DG</given-names></name>. <source>Perceptual Organization and Visual Recognition</source>. <publisher-loc>Norwell, MA, USA</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>; <year>1985</year>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayaert</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Shape Tuning in Macaque Inferior Temporal Cortex</article-title>. <source>J Neurosci</source>. <year>2003</year> <month>Apr</month> <day>1</day>;<volume>23</volume>(<issue>7</issue>):<fpage>3016</fpage>–<lpage>27</lpage>. <object-id pub-id-type="pmid">12684489</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peissig</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Wasserman</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Seeing things from a different angle: The pigeon’s recognition of single geons rotated in depth</article-title>. <source>J Exp Psychol Anim Behav Process</source>. <year>2000</year>;<volume>26</volume>(<issue>2</issue>):<fpage>115</fpage>–<lpage>32</lpage>. <object-id pub-id-type="pmid">10782428</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lazareva</surname> <given-names>OF</given-names></name>, <name name-style="western"><surname>Wasserman</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Pigeons and humans are more sensitive to nonaccidental than to metric changes in visual objects</article-title>. <source>Behav Processes</source>. <year>2008</year> <month>Feb</month>;<volume>77</volume>(<issue>2</issue>):<fpage>199</fpage>–<lpage>209</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.beproc.2007.11.009" xlink:type="simple">10.1016/j.beproc.2007.11.009</ext-link></comment> <object-id pub-id-type="pmid">18248918</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amir</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hayworth</surname> <given-names>KJ</given-names></name>. <article-title>The neural basis for shape preferences</article-title>. <source>Vision Res</source>. <year>2011</year>;<volume>51</volume>(<issue>20</issue>):<fpage>2198</fpage>–<lpage>206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2011.08.015" xlink:type="simple">10.1016/j.visres.2011.08.015</ext-link></comment> <object-id pub-id-type="pmid">21906615</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Greater sensitivity to nonaccidental than metric changes in the relations between simple shapes in the lateral occipital cortex</article-title>. <source>NeuroImage</source>. <year>2012</year>;<volume>63</volume>(<issue>4</issue>):<fpage>1818</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2012.08.066" xlink:type="simple">10.1016/j.neuroimage.2012.08.066</ext-link></comment> <object-id pub-id-type="pmid">22960149</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayaert</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>. <article-title>Infants and toddlers show enlarged visual sensitivity to nonaccidental compared with metric shape changes</article-title>. <source>-Percept</source>. <year>2010</year> <month>Dec</month> <day>20</day>;<volume>1</volume>(<issue>3</issue>):<fpage>149</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ons</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>. <article-title>Development of differential sensitivity for shape changes resulting from linear and nonlinear planar transformations</article-title>. <source>-Percept</source>. <year>2011</year> <month>May</month> <day>19</day>;<volume>2</volume>(<issue>2</issue>):<fpage>121</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amir</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Herald</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Shah</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Mintz</surname> <given-names>TH</given-names></name>. <article-title>Greater sensitivity to nonaccidental than metric shape properties in preschool children</article-title>. <source>Vision Res</source>. <year>2014</year> <month>Apr</month>;<volume>97</volume>:<fpage>83</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2014.02.006" xlink:type="simple">10.1016/j.visres.2014.02.006</ext-link></comment> <object-id pub-id-type="pmid">24582797</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Yue</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Davidoff</surname> <given-names>J</given-names></name>. <article-title>Representation of Shape in Individuals From a Culture With Minimal Exposure to Regular, Simple Artifacts: Sensitivity to Nonaccidental Versus Metric Properties</article-title>. <source>Psychol Sci</source>. <year>2009</year> <month>Dec</month> <day>1</day>;<volume>20</volume>(<issue>12</issue>):<fpage>1437</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9280.2009.02465.x" xlink:type="simple">10.1111/j.1467-9280.2009.02465.x</ext-link></comment> <object-id pub-id-type="pmid">19883490</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lorincz</surname> <given-names>A</given-names></name>. <article-title>Inferior temporal neurons show greater sensitivity to nonaccidental than to metric shape differences</article-title>. <source>J Cogn Neurosci</source>. <year>2001</year>;<volume>13</volume>(<issue>4</issue>):<fpage>444</fpage>–<lpage>53</lpage>. <object-id pub-id-type="pmid">11388918</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walther</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>D</given-names></name>. <article-title>Nonaccidental Properties Underlie Human Categorization of Complex Natural Scenes</article-title>. <source>Psychol Sci</source>. <year>2014</year> <month>Jan</month> <day>28</day>;0956797613512662.</mixed-citation></ref>
<ref id="pcbi.1004896.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lowe</surname> <given-names>DG</given-names></name>. <article-title>Distinctive image features from scale-invariant keypoints</article-title>. <source>Int J Comput Vis</source>. <year>2004</year>;<volume>60</volume>(<issue>2</issue>):<fpage>91</fpage>–<lpage>110</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amir</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hayworth</surname> <given-names>KJ</given-names></name>. <article-title>Sensitivity to nonaccidental properties across various shape dimensions</article-title>. <source>Vision Res</source>. <year>2012</year>;<volume>62</volume>(<issue>1</issue>):<fpage>35</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bracci</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Op de Beeck</surname> <given-names>H</given-names></name>. <article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title>. <source>J Neurosci</source>. <year>2016</year> <month>Jan</month> <day>13</day>;<volume>36</volume>(<issue>2</issue>):<fpage>432</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2314-15.2016" xlink:type="simple">10.1523/JNEUROSCI.2314-15.2016</ext-link></comment> <object-id pub-id-type="pmid">26758835</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>. <article-title>Untangling invariant object recognition</article-title>. <source>Trends Cogn Sci</source>. <year>2007</year> <month>Aug</month>;<volume>11</volume>(<issue>8</issue>):<fpage>333</fpage>–<lpage>41</lpage>. <object-id pub-id-type="pmid">17631409</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Representational similarity analysis—Connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year> <month>Jan</month>;<volume>2</volume>:<fpage>4</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment> <object-id pub-id-type="pmid">19104670</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref040"><label>40</label><mixed-citation publication-type="other" xlink:type="simple">Google apologises for Photos app’s racist blunder [Internet]. BBC News. [cited 2015 Nov 28]. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.bbc.com/news/technology-33347866" xlink:type="simple">http://www.bbc.com/news/technology-33347866</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Do we know what the early visual system does?</article-title> <source>J Neurosci</source>. <year>2005</year> <month>Nov</month> <day>16</day>;<volume>25</volume>(<issue>46</issue>):<fpage>10577</fpage>–<lpage>97</lpage>. <object-id pub-id-type="pmid">16291931</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>EJ</given-names></name>. <article-title>A Layered View of Shape Perception</article-title>. <source>Br J Philos Sci</source>. <year>2015</year> <month>Aug</month> <day>9</day>;axv042.</mixed-citation></ref>
<ref id="pcbi.1004896.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parker</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>. <article-title>Unsupervised invariance learning of transformation sequences in a model of object recognition yields selectivity for non-accidental properties</article-title>. <source>Front Comput Neurosci</source>. <year>2015</year>;<fpage>115</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2015.00115" xlink:type="simple">10.3389/fncom.2015.00115</ext-link></comment> <object-id pub-id-type="pmid">26500528</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname> <given-names>C</given-names></name>. <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6582</issue>):<fpage>520</fpage>–<lpage>2</lpage>. <object-id pub-id-type="pmid">8632824</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>FF</given-names></name>, <name name-style="western"><surname>VanRullen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Rapid natural scene categorization in the near absence of attention</article-title>. <source>Proc Natl Acad Sci</source>. <year>2002</year> <month>Jul</month> <day>9</day>;<volume>99</volume>(<issue>14</issue>):<fpage>9596</fpage>–<lpage>601</lpage>. <object-id pub-id-type="pmid">12077298</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greene</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>The Briefest of Glances The Time Course of Natural Scene Understanding</article-title>. <source>Psychol Sci</source>. <year>2009</year> <month>Apr</month> <day>1</day>;<volume>20</volume>(<issue>4</issue>):<fpage>464</fpage>–<lpage>72</lpage>. <object-id pub-id-type="pmid">19399976</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>Cortical algorithms for perceptual grouping</article-title>. <source>Annu Rev Neurosci</source>. <year>2006</year> <month>Jul</month>;<volume>29</volume>(<issue>1</issue>):<fpage>203</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Feldman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gepshtein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kimchi</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pomerantz</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>van der Helm</surname> <given-names>PA</given-names></name>, <etal>et al</etal>. <article-title>A century of Gestalt psychology in visual perception: II. Conceptual and theoretical foundations</article-title>. <source>Psychol Bull</source>. <year>2012</year>;<volume>138</volume>(<issue>6</issue>):<fpage>1218</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0029334" xlink:type="simple">10.1037/a0029334</ext-link></comment> <object-id pub-id-type="pmid">22845750</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poort</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Raudies</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wannig</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Neumann</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>. <article-title>The Role of Attention in Figure-Ground Segregation in Areas V1 and V4 of the Visual Cortex</article-title>. <source>Neuron</source>. <year>2012</year> <month>Jul</month>;<volume>75</volume>(<issue>1</issue>):<fpage>143</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.04.032" xlink:type="simple">10.1016/j.neuron.2012.04.032</ext-link></comment> <object-id pub-id-type="pmid">22794268</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Op de Beeck</surname> <given-names>HP</given-names></name>. <article-title>A conceptual framework of computations in mid-level vision</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>158</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2014.00158" xlink:type="simple">10.3389/fncom.2014.00158</ext-link></comment> <object-id pub-id-type="pmid">25566044</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref051"><label>51</label><mixed-citation publication-type="other" xlink:type="simple">Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, et al. ImageNet Large Scale Visual Recognition Challenge. ArXiv14090575 Cs [Internet]. 2014 Sep 1 [cited 2014 Sep 15]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.0575" xlink:type="simple">http://arxiv.org/abs/1409.0575</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref052"><label>52</label><mixed-citation publication-type="other" xlink:type="simple">Gatys LA, Ecker AS, Bethge M. Texture Synthesis Using Convolutional Neural Networks. ArXiv150507376 Cs Q-Bio [Internet]. 2015 May 27 [cited 2015 Nov 23]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1505.07376" xlink:type="simple">http://arxiv.org/abs/1505.07376</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref053"><label>53</label><mixed-citation publication-type="other" xlink:type="simple">Gatys LA, Ecker AS, Bethge M. A Neural Algorithm of Artistic Style. ArXiv150806576 Cs Q-Bio [Internet]. 2015 Aug 26 [cited 2015 Nov 23]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.06576" xlink:type="simple">http://arxiv.org/abs/1508.06576</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref054"><label>54</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gong</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lazebnik</surname> <given-names>S</given-names></name>. <chapter-title>Multi-scale Orderless Pooling of Deep Convolutional Activation Features</chapter-title>. In: <name name-style="western"><surname>Fleet</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pajdla</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schiele</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tuytelaars</surname> <given-names>T</given-names></name>, editors. <source>Computer Vision–ECCV</source> <year>2014</year> [Internet]. <publisher-name>Springer International Publishing</publisher-name>; 2014 [cited 2015 Oct 9]. p. <fpage>392</fpage>–<lpage>407</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/chapter/10.1007/978-3-319-10584-0_26" xlink:type="simple">http://link.springer.com/chapter/10.1007/978-3-319-10584-0_26</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref055"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">Cimpoi M, Maji S, Vedaldi A. Deep convolutional filter banks for texture recognition and segmentation. ArXiv14116836 Cs [Internet]. 2014 Nov 25 [cited 2015 Oct 9]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1411.6836" xlink:type="simple">http://arxiv.org/abs/1411.6836</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>. <article-title>A framework for streamlining research workflow in neuroscience and psychology</article-title>. <source>Front Neuroinformatics</source>. <year>2014</year>;<volume>7</volume>:<fpage>52</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref057"><label>57</label><mixed-citation publication-type="other" xlink:type="simple">Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R, et al. Caffe: Convolutional architecture for fast feature embedding. ArXiv Prepr ArXiv14085093. 2014;</mixed-citation></ref>
<ref id="pcbi.1004896.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>. <article-title>Inverse MDS: Inferring Dissimilarity Structure from Multiple Item Arrangements</article-title>. <source>Front Psychol</source> [Internet]. <year>2012</year> <month>Jul</month> <day>25</day> [cited 2015 Apr 1];3. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3404552/" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3404552/</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Walt</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schönberger</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Boulogne</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Warner</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Yager</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source>. <year>2014</year> <month>Jun</month> <day>19</day>;<volume>2</volume>:<fpage>e453</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7717/peerj.453" xlink:type="simple">10.7717/peerj.453</ext-link></comment> <object-id pub-id-type="pmid">25024921</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref060"><label>60</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bird</surname> <given-names>S</given-names></name>. <chapter-title>NLTK: The Natural Language Toolkit</chapter-title>. In: <source>Proceedings of the COLING/ACL on Interactive Presentation Sessions</source> [Internet]. <publisher-loc>Stroudsburg, PA, USA</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2006</year> [cited 2015 Jul 22]. p. <fpage>69</fpage>–<lpage>72</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3115/1225403.1225421" xlink:type="simple">http://dx.doi.org/10.3115/1225403.1225421</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref061"><label>61</label><mixed-citation publication-type="other" xlink:type="simple">Princeton University. About WordNet [Internet]. Princeton University; 2010 [cited 2015 Nov 11]. Available from: <ext-link ext-link-type="uri" xlink:href="http://wordnet.princeton.edu" xlink:type="simple">http://wordnet.princeton.edu</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lades</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vorbruggen</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Buhmann</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lange</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>von der Malsburg</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wurtz</surname> <given-names>RP</given-names></name>, <etal>et al</etal>. <article-title>Distortion invariant object recognition in the dynamic link architecture</article-title>. <source>IEEE Trans Comput</source>. <year>1993</year>;<volume>42</volume>(<issue>3</issue>):<fpage>300</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004896.ref063"><label>63</label><mixed-citation publication-type="other" xlink:type="simple">Dalal N, Triggs B. Histograms of oriented gradients for human detection. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005 CVPR 2005. San Diego, CA; 2005. p. 886–93 vol. 1.</mixed-citation></ref>
<ref id="pcbi.1004896.ref064"><label>64</label><mixed-citation publication-type="other" xlink:type="simple">Bosch A, Zisserman A, Munoz X. Representing shape with a spatial pyramid kernel. In: Proceedings of the 6th ACM International Conference on Image and Video Retrieval [Internet]. New York, NY, USA: ACM; 2007 [cited 2014 Sep 5]. p. 401–8. Available from: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/1282280.1282340" xlink:type="simple">http://doi.acm.org/10.1145/1282280.1282340</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref065"><label>65</label><mixed-citation publication-type="other" xlink:type="simple">Lazebnik S, Schmid C, Ponce J. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2006. p. 2169–78.</mixed-citation></ref>
<ref id="pcbi.1004896.ref066"><label>66</label><mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L. ImageNet: A large-scale hierarchical image database. In: IEEE Conference on Computer Vision and Pattern Recognition, 2009 CVPR 2009. 2009. p. 248–55.</mixed-citation></ref>
<ref id="pcbi.1004896.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>An</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Gong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Pan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <etal>et al</etal>. <article-title>Orientation-Cue Invariant Population Responses to Contrast-Modulated and Phase-Reversed Contour Stimuli in Macaque V1 and V2</article-title>. <source>PLoS ONE</source>. <year>2014</year> <month>Sep</month> <day>4</day>;<volume>9</volume>(<issue>9</issue>):<fpage>e106753</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0106753" xlink:type="simple">10.1371/journal.pone.0106753</ext-link></comment> <object-id pub-id-type="pmid">25188576</object-id></mixed-citation></ref>
<ref id="pcbi.1004896.ref068"><label>68</label><mixed-citation publication-type="other" xlink:type="simple">BVLC/caffe [Internet]. GitHub. [cited 2015 Nov 23]. Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/BVLC/caffe" xlink:type="simple">https://github.com/BVLC/caffe</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref069"><label>69</label><mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. ArXiv14091556 Cs [Internet]. 2014 Sep 4 [cited 2015 Jul 22]; Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556" xlink:type="simple">http://arxiv.org/abs/1409.1556</ext-link></mixed-citation></ref>
<ref id="pcbi.1004896.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>A Toolbox for Representational Similarity Analysis</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year> <month>Apr</month> <day>17</day>;<volume>10</volume>(<issue>4</issue>):<fpage>e1003553</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment> <object-id pub-id-type="pmid">24743308</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>