<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00127</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003511</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Circuit models</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural homeostasis</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>STDP Installs in Winner-Take-All Circuits an Online Approximation to Hidden Markov Model Learning</article-title>
<alt-title alt-title-type="running-head">STDP Approximates Hidden Markov Model Learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kappel</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Nessler</surname><given-names>Bernhard</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>Wolfgang</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Institute for Theoretical Computer Science, Graz University of Technology, Graz, Austria</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sprekeler</surname><given-names>Henning</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Cambridge, UK and Humboldt-Universität zu Berlin, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">kappel@igi.tugraz.at</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: DK BN WM. Performed the experiments: DK. Analyzed the data: DK BN WM. Wrote the paper: DK BN WM.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>3</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>27</day><month>3</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>3</issue>
<elocation-id>e1003511</elocation-id>
<history>
<date date-type="received"><day>22</day><month>1</month><year>2013</year></date>
<date date-type="accepted"><day>24</day><month>1</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Kappel et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="RA1" related-article-type="companion" ext-link-type="uri" page="e1002211" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1002211"> <article-title>Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons</article-title></related-article><related-article id="RA2" related-article-type="companion" ext-link-type="uri" page="e1002294" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1002294"> <article-title>Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons</article-title></related-article><related-article id="RA3" related-article-type="companion" ext-link-type="uri" page="e1003037" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1003037"> <article-title>Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity</article-title></related-article>
<abstract>
<p>In order to cross a street without being run over, we need to be able to extract very fast hidden causes of dynamically changing multi-modal sensory stimuli, and to predict their future evolution. We show here that a generic cortical microcircuit motif, pyramidal cells with lateral excitation and inhibition, provides the basis for this difficult but all-important information processing capability. This capability emerges in the presence of noise automatically through effects of STDP on connections between pyramidal cells in Winner-Take-All circuits with lateral excitation. In fact, one can show that these motifs endow cortical microcircuits with functional properties of a hidden Markov model, a generic model for solving such tasks through probabilistic inference. Whereas in engineering applications this model is adapted to specific tasks through offline learning, we show here that a major portion of the functionality of hidden Markov models arises already from online applications of STDP, without any supervision or rewards. We demonstrate the emergent computing capabilities of the model through several computer simulations. The full power of hidden Markov model learning can be attained through reward-gated STDP. This is due to the fact that these mechanisms enable a rejection sampling approximation to theoretically optimal learning. We investigate the possible performance gain that can be achieved with this more accurate learning method for an artificial grammar task.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>It has recently been shown that STDP installs in ensembles of pyramidal cells with lateral inhibition networks for Bayesian inference that are theoretically optimal for the case of stationary spike input patterns. We show here that if the experimentally found lateral excitatory connections between pyramidal cells are taken into account, theoretically optimal probabilistic models for the prediction of time-varying spike input patterns emerge through STDP. Furthermore a rigorous theoretical framework is established that explains the emergence of computational properties of this important motif of cortical microcircuits through learning. We show that the application of an idealized form of STDP approximates in this network motif a generic process for adapting a computational model to data: expectation-maximization. The versatility of computations carried out by these ensembles of pyramidal cells and the speed of the emergence of their computational properties through STDP is demonstrated through a variety of computer simulations. We show the ability of these networks to learn multiple input sequences through STDP and to reproduce the statistics of these inputs after learning.</p>
</abstract>
<funding-group><funding-statement>Written under partial support by the European Union project FP7-248311 (AMARSI) and project FP7-269921 (BRAINSCALES). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="22"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>An ubiquitous motif of cortical microcircuits is ensembles of pyramidal cells (in layers 2/3 and in layer 5) with lateral inhibition <xref ref-type="bibr" rid="pcbi.1003511-Berger1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Avermann1">[3]</xref>. This network motif is called a <italic>winner-take-all</italic> (WTA) circuit, since inhibition induces competition between pyramidal neurons <xref ref-type="bibr" rid="pcbi.1003511-Douglas1">[4]</xref>. We investigate in this article which computational capabilities emerge in WTA circuits if one also takes into account the existence of lateral excitatory synaptic connections within such ensembles of pyramidal cells (<xref ref-type="fig" rid="pcbi-1003511-g001">Fig. 1A</xref>). This augmented architecture will be our default notion of a WTA circuit throughout this paper.</p>
<fig id="pcbi-1003511-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g001</object-id><label>Figure 1</label><caption>
<title>Illustration of the network model.</title>
<p>(A) The structure of the network. It consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e001" xlink:type="simple"/></inline-formula> excitatory neurons (blue) that receive feedforward inputs (green synapses) and lateral excitatory all-to-all connections (blue synapses). Interneurons (red) install soft winner-take-all behavior by injecting a global inhibition to all neurons of the circuit in response to the network's spiking activity. (B) The Bayesian network representing the HMM over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e002" xlink:type="simple"/></inline-formula> time steps. The prediction model (blue arrows) is implemented by the lateral synapses. It determines the evolution of the hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e003" xlink:type="simple"/></inline-formula> over time. The observation model (green arrows) is implemented by feedforward connections. The inference task for the HMM is to determine a sequence of hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e004" xlink:type="simple"/></inline-formula> (white), given the afferent activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e005" xlink:type="simple"/></inline-formula> (gray). (C) The STDP window that is used to update the excitatory synapses. The synaptic weight change is plotted against the time difference between pre- and postsynaptic spike events.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g001" position="float" xlink:type="simple"/></fig>
<p>We show that this network motif endows cortical microcircuits with the capability to encode and process information in a highly dynamic environment. This dynamic environment of generic cortical mircocircuits results from quickly varying activity of neurons at the sensory periphery, caused for example by visual, auditory, and somatosensory stimuli impinging on a moving organism that actively probes the environment for salient information. Quickly changing sensory inputs are also caused by movements and communication acts of other organisms that need to be interpreted and predicted. Finally, a generic cortical microcircuit also receives massive inputs from other cortical areas. Experimental data with simultaneous recordings of many neurons suggest that these internal cortical codes are also highly dynamic, and often take the form of characteristic assembly sequences or trajectories of local network states <xref ref-type="bibr" rid="pcbi.1003511-Han1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Harvey1">[10]</xref>. We show in this article that WTA circuits have emergent coding and computing capabilities that are especially suited for this highly dynamic context of cortical microcircuits.</p>
<p>We show that spike-timing-dependent plasticity (STDP) <xref ref-type="bibr" rid="pcbi.1003511-Caporale1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Markram1">[12]</xref>, applied on both the lateral excitatory synapses and synapses from afferent neurons, implements in these networks the capability to represent the underlying statistical structure of such spatiotemporal input patterns. This implies the challenge to solve two different learning tasks in parallel. First it is necessary to learn to recognize the salient high-dimensional patterns from the afferent neurons, which was already investigated in <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref>. The second task consists in learning the temporal structure underlying the input spike sequences. We show that augmented WTA circuits are able to detect the sequential arrangements of the learned salient patterns. Synaptic plasticity for lateral excitatory connections provides the ability to discriminate even identical input patterns according to the temporal context in which they appear. The same STDP rule, that leads to the emergence of sparse codes for individual input patterns in the absence of lateral excitatory connections <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref> now leads to the emergence of context specific neural codes and even predictions for temporal sequences of such patterns. The resulting neural codes are sparse with respect to the number of neurons that are tuned for a specific salient pattern and the temporal context in which it appears.</p>
<p>The basic principles of learning sequences of forced spike activations in general recurrent networks were studied in previous work <xref ref-type="bibr" rid="pcbi.1003511-Rezende1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Brea1">[15]</xref> and resulted in the finding that an otherwise local learning rule (like STDP) has to be enhanced by a global third factor which acts as an <italic>importance weight</italic>, in order to provide a – theoretically provable – approximation to temporal sequence learning. The possible role of such importance weights for probabilistic computations in spiking neural networks with lateral inhibition was already investigated earlier in <xref ref-type="bibr" rid="pcbi.1003511-Shi1">[16]</xref>.</p>
<p>In this article we establish a rigorous theoretical framework which reveals that each spike train generated by WTA circuits can be viewed as a sample from the state space of a <italic>hidden Markov model</italic> (HMM). The HMM has emerged in machine learning and engineering applications as a standard probabilistic model for detecting hidden regularities in sequential input patterns, and for learning to predict their continuation from initial segments <xref ref-type="bibr" rid="pcbi.1003511-Rabiner1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>. The HMM is a generative model which relies on the assumption that the statistics of input patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e006" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e007" xlink:type="simple"/></inline-formula> time steps is governed by a sequence of hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e008" xlink:type="simple"/></inline-formula>, such that the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e009" xlink:type="simple"/></inline-formula> hidden state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e010" xlink:type="simple"/></inline-formula> “explains” or generates the input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e011" xlink:type="simple"/></inline-formula>. We show that the instantaneous state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e012" xlink:type="simple"/></inline-formula> of the HMM is realized by the joint activity of all neurons of a WTA circuit, i.e. the spikes themselves and their resulting postsynaptic potentials. The stochastic dynamics of the WTA circuit implements a <italic>forward sampler</italic> that approximates exact HMM inference by propagating a single sample from the hidden state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e013" xlink:type="simple"/></inline-formula> forward in time <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>.</p>
<p>We show analytically that a suitable STDP rule in the WTA circuit – notably the same rule on both the recurrent and the feedforward synaptic connections – realizes theoretically optimal parameter acquisition in terms of an online <italic>expectation-maximization</italic> (EM) algorithm <xref ref-type="bibr" rid="pcbi.1003511-Celeux1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Neal1">[22]</xref>, for a certain pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e014" xlink:type="simple"/></inline-formula> <italic>if</italic> the stochastic network dynamics describes the state sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e015" xlink:type="simple"/></inline-formula> upon the input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e016" xlink:type="simple"/></inline-formula>. We further show that when the STDP rule is applied within the approximative forward sampling network dynamics of the WTA circuit, it instantiates a weak but well defined approximation of theoretically optimal HMM learning through EM. This is remarkable insofar as no additional mechanisms are needed for this approximation – it is automatically implemented through the stochastic dynamics of the WTA circuit, in combination with STDP. In this paper we focus on the analysis of this approximation scheme, its limits and its behavioral relevance.</p>
<p>We test this model in computer simulations that duplicate a number of experimental paradigms for evaluating emergent neural codes and behavioral performance in recognizing and predicting temporal sequences. We analyze evoked and spontaneous dynamics that emerges in our model network after learning an object sequence memory task as in the experiments of <xref ref-type="bibr" rid="pcbi.1003511-Berdyyeva1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Warden1">[24]</xref>. We show that the pyramidal cells of a WTA circuit learn through STDP to encode the hidden states that underlie the input statistics in such tasks, which enables these cells to recognize and distinguish multiple pattern sequences and to autonomously predict their continuation from initial segments. Furthermore, we find neural assemblies emerging in neighboring interconnected WTA circuits that encode different abstract features underlying the task. The resulting neural codes resemble the highly heterogeneous codes found in the cortex <xref ref-type="bibr" rid="pcbi.1003511-Rigotti1">[25]</xref>. Furthermore, neurons often learn to fire preferentially after specific predecessors, building up stereotypical neural trajectories within neural assemblies, that are also commonly observed in cortical activity <xref ref-type="bibr" rid="pcbi.1003511-Han1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Luczak2">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Xu1">[26]</xref>.</p>
<p>Our generative probabilistic perspective of synaptic plasticity in WTA circuits naturally leads to the question whether the proposed learning approximation is able to solve complex problems beyond simple sequence learning. Therefore we reanalyze data on artificial grammar learning experiments from cognitive science <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref>, where subjects were exposed to sequences of symbols generated by some hidden artificial grammar, and then had to judge whether subsequently presented unseen test sequences had been generated by the same grammar. We show that STDP learning in our WTA circuits is able to infer the underlying grammar model from a small number of training sequences.</p>
<p>The simple approximation by forward sampling, however, clearly limits the learning performance. We show that the full power of HMM-learning can be attained in a WTA circuit based on the <italic>rejection sampling</italic> principle <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>. A binary factor is added to the STDP learning rule, that gates the expression of synaptic plasticity through a subsequent global modulatory signal. The improvement in accuracy of this more powerful learning method comes at the cost that every input sequence has to be repeated a number of times, until one generated state sequence is accepted. We show that a significant performance increase can be achieved already with a small number of repetitions. We demonstrate this for a simple and a more complex grammar learning task.</p>
</sec><sec id="s2">
<title>Results</title>
<p>We first define the spiking neural network model for the winner-take-all (WTA) circuit considered throughout this paper. The architecture of the network is illustrated in <xref ref-type="fig" rid="pcbi-1003511-g001">Fig. 1A</xref>. It consists of stochastic spiking neurons, which receive excitatory input from an afferent population (green synapses) and from lateral excitatory connections (blue synapses) between neighboring pyramidal neurons. To clarify the distinction between these connections, we denote the synaptic efficacies of feedforward and lateral synapses by different weight matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e017" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e018" xlink:type="simple"/></inline-formula>, respectively, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e019" xlink:type="simple"/></inline-formula> denotes the number of afferent neurons and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e020" xlink:type="simple"/></inline-formula> the size of the circuit (i.e., the number of pyramidal cells in the circuit). In addition, all neurons within the WTA circuit project to interneurons and in turn all receive the same common inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e021" xlink:type="simple"/></inline-formula>. Thus the membrane potential of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e022" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e023" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1003511.e024"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e024" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pcbi.1003511.e025"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e025" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e026" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e027" xlink:type="simple"/></inline-formula> denote the time courses of the excitatory postsynaptic potentials (EPSP) under the feedforward and lateral synapses, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e028" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e029" xlink:type="simple"/></inline-formula> are the elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e030" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e031" xlink:type="simple"/></inline-formula> respectively, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e032" xlink:type="simple"/></inline-formula> is a parameter that controls the excitability of the neuron. The two sums in (1) describe the time courses of the membrane potential in response to synaptic inputs from feedforward and lateral synapses. In <xref ref-type="disp-formula" rid="pcbi.1003511.e025">equation (2)</xref> we used the assumption of additive EPSPs, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e033" xlink:type="simple"/></inline-formula> denotes a kernel function that determines the time course of an EPSP <xref ref-type="bibr" rid="pcbi.1003511-Gerstner1">[28]</xref>. The sums run over all spike times of the presynaptic neuron. For the theoretical analysis we used a single exponential decay for the sake of simplicity, throughout the simulations we used double exponential kernels, if not stated otherwise. Our theoretical model can be further extended to other EPSP shapes (see the <xref ref-type="sec" rid="s4">Methods</xref> section for details).</p>
<p>As proposed in <xref ref-type="bibr" rid="pcbi.1003511-Jolivet1">[29]</xref>, we employ an exponential dependence between the membrane potential and the firing probability. Therefore the instantaneous rate of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e034" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e035" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e036" xlink:type="simple"/></inline-formula> is a constant that scales the firing rate. The inhibitory feedback loop <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e037" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003511.e024">equation (1)</xref>, that depresses the membrane potentials whenever the network activity rises, has a normalizing effect on the circuit-wide output rate. Although, each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e038" xlink:type="simple"/></inline-formula> generates spikes according to an individual Poisson process, this inhibition couples the neural activities and thereby installs the required competition between all cells in the circuit. We model the effect of this inhibition in an abstract way, where we assume, that all WTA neurons receive the same inhibitory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e039" xlink:type="simple"/></inline-formula> such that the overall spiking rate of the WTA circuit stays approximately constant. Ideal WTA behavior is attained if the network rate is normalized to the same value at any point in time, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e040" xlink:type="simple"/></inline-formula>. Using this, we find the circuit dynamics to be determined by<disp-formula id="pcbi.1003511.e041"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e041" xlink:type="simple"/><label>(3)</label></disp-formula>This ideal WTA circuit realizes a soft-max or soft WTA function, granting the highest firing rate to the neuron with the highest membrane potential, but still allowing all other neurons to fire with non-zero probability.</p>
<sec id="s2a">
<title>Recapitulation of hidden Markov model theory</title>
<p>In this section we briefly summarize the relevant concepts for deriving our theoretical results. An exhaustive discussion on hidden Markov model theory can be found in <xref ref-type="bibr" rid="pcbi.1003511-Rabiner1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>. Throughout the paper, to keep the notation uncluttered we use the common short-hand notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e042" xlink:type="simple"/></inline-formula> to denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e043" xlink:type="simple"/></inline-formula>, i.e. the probability that the random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e044" xlink:type="simple"/></inline-formula> takes on the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e045" xlink:type="simple"/></inline-formula>. If it is not clear from the context, we will use the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e046" xlink:type="simple"/></inline-formula> to remind the reader of the underlying random variable, that is only implicitly defined.</p>
<p>The HMM is a generative model for input pattern sequences over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e047" xlink:type="simple"/></inline-formula> time steps <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e048" xlink:type="simple"/></inline-formula> (the input patterns are traditionally called observations in the context of HMMs). It relies on the assumption that a sequence of hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e049" xlink:type="simple"/></inline-formula> and a set of parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e050" xlink:type="simple"/></inline-formula> exist, which govern the statistics of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e051" xlink:type="simple"/></inline-formula>. This assumption allows to write the joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e052" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e053" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003511.e054"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e054" xlink:type="simple"/><label>(4)</label></disp-formula>where we suppress an explicit representation of the initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e055" xlink:type="simple"/></inline-formula>, for the sake of brevity. The joint distribution (4) factorizes in each time step into the <italic>observation model</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e056" xlink:type="simple"/></inline-formula> and the state transition or <italic>prediction model</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e057" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>. This independence property is illustrated by the Bayesian network for a HMM in <xref ref-type="fig" rid="pcbi-1003511-g001">Fig. 1B</xref>.</p>
<p>The HMM is a generative model and therefore we can recover the distribution over input patterns by marginalizing out the hidden state sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e058" xlink:type="simple"/></inline-formula>. Learning in this model means to adapt the model parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e059" xlink:type="simple"/></inline-formula> such that this marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e060" xlink:type="simple"/></inline-formula> comes as close as possible to the empirical distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e061" xlink:type="simple"/></inline-formula> of the observable input sequences. A generic method for learning in generative models with hidden variables is the <italic>expectation-maximization</italic> (EM) algorithm <xref ref-type="bibr" rid="pcbi.1003511-Dempster1">[30]</xref>, and its application to HMMs is known as the Baum-Welch algorithm <xref ref-type="bibr" rid="pcbi.1003511-Baum1">[31]</xref>. This algorithm consists of iterating two steps, the <italic>E-step</italic> and the <italic>M-step</italic>, where the model parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e062" xlink:type="simple"/></inline-formula> are adjusted at each M-step (for the updated posterior generated at the preceding E-step). A remarkable feature of the algorithm is that the fitting of the model to the data is guaranteed to improve at each M-step of this iterative process. Whereas the classical EM algorithm is restricted to offline learning (where all training data are available right at the beginning), there exist also stochastic online versions of EM learning.</p>
<p>In its stochastic online variant <xref ref-type="bibr" rid="pcbi.1003511-Celeux1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Neal1">[22]</xref> the E-step consists of generating one sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e063" xlink:type="simple"/></inline-formula> from the <italic>posterior distribution</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e064" xlink:type="simple"/></inline-formula>, given one currently observed input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e065" xlink:type="simple"/></inline-formula>. Given these sampled values for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e066" xlink:type="simple"/></inline-formula>, the subsequent M-step adapts the model parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e067" xlink:type="simple"/></inline-formula> such that the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e068" xlink:type="simple"/></inline-formula> increases. The adaptation is confined to acquiring the conditional probabilities that govern the observation and the prediction model.</p>
<p>It would be also desirable to realize the inference and sampling of one such posterior sample sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e069" xlink:type="simple"/></inline-formula> in a fully online processing, i.e. generating each state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e070" xlink:type="simple"/></inline-formula> in parallel to the arrival of the corresponding input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e071" xlink:type="simple"/></inline-formula>. Yet this seems to be impossible as the probabilistic model according to (4) implies a statistical dependence between any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e072" xlink:type="simple"/></inline-formula> and the whole future observation sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e073" xlink:type="simple"/></inline-formula>. However, it is well known that the inference of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e074" xlink:type="simple"/></inline-formula> can be approximated by a so-called <italic>forward sampling</italic> process <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>, where every single time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e075" xlink:type="simple"/></inline-formula> of the sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e076" xlink:type="simple"/></inline-formula> is sampled online, based solely on the knowledge of the observations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e077" xlink:type="simple"/></inline-formula> received so far, rather than the observation of the complete sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e078" xlink:type="simple"/></inline-formula>. Hence sampling the sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e079" xlink:type="simple"/></inline-formula> is approximated by propagating a single sample from the HMM state space forward in time.</p>
</sec><sec id="s2b">
<title>Forward sampling in WTA circuits</title>
<p>In this section we show that the dynamics of the network realizes a forward sampler for the HMM. We make use of the fact that <xref ref-type="disp-formula" rid="pcbi.1003511.e024">equations (1)</xref>, <xref ref-type="disp-formula" rid="pcbi.1003511.e025">(2)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003511.e041">(3)</xref> realize a Markov process, in the sense that future network dynamics is independent from the past, given the current network state (for a suitable notion of network state). This property holds true for most reasonable choices of EPSP kernels. For the sake of brevity we focus in the theoretical analysis on the simple case of a single exponential decay with time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e080" xlink:type="simple"/></inline-formula>.</p>
<p>We seek a description of the continuous-time network dynamics in response to afferent spike trains over a time span of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e081" xlink:type="simple"/></inline-formula> that can be mapped to the state space of a corresponding HMM with discrete time steps. Although the network works in continuous time, its dynamics can be fully described taking only those points in time into account, where one of the neurons in the recurrent circuit produces a spike. This allows to directly link spike trains generated by the network to a sequence of samples from the state space of a corresponding HMM.</p>
<p>Let the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e082" xlink:type="simple"/></inline-formula> spike times produced during this time window be given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e083" xlink:type="simple"/></inline-formula>. The neuron dynamics are determined by the membrane time courses (2). For convenience let us introduce the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e084" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e085" xlink:type="simple"/></inline-formula> and by analogy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e086" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e087" xlink:type="simple"/></inline-formula>.</p>
<p>Due to the exponentially decaying EPSPs the synaptic activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e088" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e089" xlink:type="simple"/></inline-formula> is fully defined by the synaptic activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e090" xlink:type="simple"/></inline-formula> at the time of the previous spike <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e091" xlink:type="simple"/></inline-formula>, and the identity of the neuron that spiked in that previous time step, which we denote by a discrete variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e092" xlink:type="simple"/></inline-formula>. We thus conclude that the sequence of tuples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e093" xlink:type="simple"/></inline-formula> (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e094" xlink:type="simple"/></inline-formula>) fulfills the Markov condition, i.e. the conditional independence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e095" xlink:type="simple"/></inline-formula> and thus fully represents the continuous dynamics of the network (see <xref ref-type="sec" rid="s4">Methods</xref>). We call <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e096" xlink:type="simple"/></inline-formula> the <italic>network state</italic>. The corresponding HMM forward sampler follows a simple update scheme that samples a new state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e097" xlink:type="simple"/></inline-formula> given the current observation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e098" xlink:type="simple"/></inline-formula> and the previous state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e099" xlink:type="simple"/></inline-formula>. This dynamic is equivalent to the WTA network model.</p>
<p>This state representation allows us to update the network dynamics online, jumping from one spike time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e100" xlink:type="simple"/></inline-formula> to the next. Using this property, we find that the dynamics of the network realizes a probability distribution over state sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e101" xlink:type="simple"/></inline-formula>, given an afferent sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e102" xlink:type="simple"/></inline-formula>, which can be written as<disp-formula id="pcbi.1003511.e103"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e103" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e104" xlink:type="simple"/></inline-formula> is the set of network parameters. The factorization and independence properties in (5) are induced by the state representation and the circuit dynamics. We assume here that the lateral inhibition within the WTA circuit ensures that the output rate of the whole circuit is normalized, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e105" xlink:type="simple"/></inline-formula> at all times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e106" xlink:type="simple"/></inline-formula>. This allows to introduce the distribution over the inter-spike-time intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e107" xlink:type="simple"/></inline-formula> independent from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e108" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Note, that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e109" xlink:type="simple"/></inline-formula> determines the interval between spikes of <italic>all</italic> circuit neurons, realized by a <italic>homogeneous</italic> Poisson process with a constant rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e110" xlink:type="simple"/></inline-formula>. The second term in the second line of (5) determines the course of the membrane potential, i.e. it assures that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e111" xlink:type="simple"/></inline-formula> follows the membrane dynamics. Since the EPSP kernels are deterministic functions this distribution has a single mass point, where (2) is satisfied. The first factor in the second line of (5) is given by the probability of each individual neuron to spike. This probability depends on the membrane potential (1), which in turn is determined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e112" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e113" xlink:type="simple"/></inline-formula> and the network parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e114" xlink:type="simple"/></inline-formula>. Given that the circuit spikes at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e115" xlink:type="simple"/></inline-formula>, the firing probability of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e116" xlink:type="simple"/></inline-formula> can be expressed as a conditional distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e117" xlink:type="simple"/></inline-formula>. The lateral inhibition in (1) ensures that this probability distribution is correctly normalized. Therefore, the winner neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e118" xlink:type="simple"/></inline-formula> is drawn from a multinomial distribution at each spike time.</p>
<p>For the given architecture the functional parts of the network can be related directly to hidden Markov model dynamics. In the <xref ref-type="sec" rid="s4">Methods</xref> section we show in detail that by rewriting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e119" xlink:type="simple"/></inline-formula> the membrane potential (1) can be decomposed into three functional parts<disp-formula id="pcbi.1003511.e120"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e120" xlink:type="simple"/><label>(6)</label></disp-formula>The lateral excitatory connections predict a prior belief about the current network activity and the feedforward synapses match this prediction against the afferent input. The inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e121" xlink:type="simple"/></inline-formula> implements the normalization that is required to make (6) a valid multinomial distribution. The functional parts of the membrane potential can be directly linked to the prediction and observation models of a HMM, where the network state is equivalent to the hidden state of this HMM. The WTA circuit realizes a forward-sampler for this HMM, which approximates sampling from the posterior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e122" xlink:type="simple"/></inline-formula> in an online fashion <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>. Its sampling is carried out step by step, i.e. it generates through each spike a new sample from the network state space, taking only the previous time step sample into account. Furthermore this forward sampling requires no additional computational organization, but is achieved by the inherent dynamics of the stochastically firing WTA circuit.</p>
</sec><sec id="s2c">
<title>STDP instantiates a stochastic approximation to EM parameter learning</title>
<p>Formulating the network dynamics in terms of a probabilistic model is beneficial for two reasons: First, it gives rise to a better understanding of the network dynamics by relating it to samples from the HMM state space. Second, the underlying model allows us to derive parameter estimation algorithms and to compare them with biological mechanisms for synaptic plasticity. For the HMM, this approach results in an instantiation of the EM algorithm <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Dempster1">[30]</xref> in a network of spiking neurons (stochastic WTA circuit). In the <xref ref-type="sec" rid="s4">Methods</xref> section we derive this algorithm for the WTA circuit and show that the M-step evaluates to weight updates that need to be applied whenever neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e123" xlink:type="simple"/></inline-formula> emits a spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e124" xlink:type="simple"/></inline-formula>, according to<disp-formula id="pcbi.1003511.e125"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e125" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e126" xlink:type="simple"/></inline-formula> is a positive constant that controls the learning rate. Note that the update rules for the feedforward and the recurrent connections are identical, and thus all excitatory synapses in the network are handled uniformly. These plasticity rules (7) are equivalent to the updates that previously emerged as theoretically optimal synaptic weight changes, for learning to recognize repeating high-dimensional patterns in spike trains from afferent neurons, in related studies <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Nessler2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss1">[33]</xref>. The update rules consist of two parts: A Hebbian long-term potentiating (LTP) part that depends on presynaptic activity and a constant depression term. The dependence on the EPSP time courses (2) makes the first part implicitly dependent on the history of presynaptic spikes. The STDP window is shown in <xref ref-type="fig" rid="pcbi-1003511-g001">Fig. 1C</xref> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e127" xlink:type="simple"/></inline-formula>-shaped EPSPs. Potentiation is triggered when the postsynaptic neuron fires after the presynaptic neuron. This term is commonly found in synaptic plasticity measured in biological neurons, and for common EPSP windows it closely resembles the shape of the pre-before-post part of standard forms of STDP <xref ref-type="bibr" rid="pcbi.1003511-Caporale1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Markram1">[12]</xref>. The dependence on the current value of the synaptic weight has a local stabilizing effect on the synapse. The depressing part of the update rule is triggered whenever the postsynaptic neuron fires independent of presynaptic activity. It contrasts LTP and assures that the synaptic weights stay globally in a bounded regime. It is shown in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4</xref> of <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref> that the simple rule (7) reproduces the standard form of STDP curves when it is applied with an intermediate pairing rate.</p>
<p>While these M-step updates emerge as exact solutions for the underlying HMM, the WTA circuit implements an approximation of the <italic>E-step</italic>, using forward sampling from the distribution in <xref ref-type="disp-formula" rid="pcbi.1003511.e103">equation (5)</xref>. In the following experiments we will first focus on this simple approximation, and analyze what computational function emerges in the network using the STDP updates (7) without any third signal related to reward or a “teacher”. In the last part of the <xref ref-type="sec" rid="s2">Results</xref> section we will introduce a possible implementation of a refined approximation, and assess the advantages and disadvantages of this method.</p>
</sec><sec id="s2d">
<title>Learning to predict spike sequences through STDP</title>
<p>In this section we show through computer simulations that our WTA circuits learn to encode the hidden state that underlies the input statistics via the STDP rule (7). We demonstrate this for a simple sequence memory task and analyze in detail how the hidden state underlying this task is represented in the network. The experimental paradigm reproduces the structure of object sequence memory tasks, where monkeys had to memorize a sequence of movements and reproduce it after a delay period <xref ref-type="bibr" rid="pcbi.1003511-Berdyyeva1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Warden1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Shima1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Isoda1">[35]</xref>. The task consisted of three phases: An initial cue phase, a delay phase and a recall phase. Each phase is characterized by a different input sequence, where the cue sequence defines the identity of the recall sequence. We used four cue/recall pairs in this experiment.</p>
<p>The structure of this task is illustrated in <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2A</xref>. The graph represents a finite state grammar that can be used to generate symbol sequences by following a path from <italic>Start</italic> to <italic>Exit</italic>. In this first illustrative example the only stochastic decision is made at the beginning, randomly choosing one of the four cue phases with equal probabilities while the rest of the sequence is deterministic. On each arc that is passed, the symbol next to the arc is generated, e.g. <italic>AB-delay-ab</italic> is one possible symbolic sequence. Note that all symbols can appear in different temporal contexts, e.g. <italic>A</italic> appears in sequence <italic>AB-delay-ab</italic> and in <italic>BA-delay-ba</italic>. The <italic>delay</italic> symbol is completely unspecific since it appears in all four possible sequences. Therefore this task does not fulfill the Markov condition with respect to the input symbols, e.g. knowing that the current symbol is <italic>delay</italic> does not identify the next one as it might be any of <italic>a,b,c,d</italic>. Only additional knowledge about the temporal context of the symbol allows to uniquely identify the continuation of the sequence.</p>
<fig id="pcbi-1003511-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g002</object-id><label>Figure 2</label><caption>
<title>Emergence of working memory encoded in neural assemblies through weak HMM learning in a WTA circuit through STDP.</title>
<p>(A) Illustration of the input encoding for sequence <italic>AB-delay-ab</italic>. The upper plot shows one example input spike train (blue dots) plotted on top of the mean firing rate (100 out of 200 afferent neurons shown). The lower panel shows the finite state grammar graph that represents the simple working memory task. The graph can be used to generate symbol sequences by following any path from <italic>Start</italic> to <italic>Exit</italic>. In the first state (<italic>Start</italic>) a random decision is made, which of the four paths to take. This decision determines all arcs that are passed throughout the sequence. On each arc that is passed the symbol next to the arc is emitted (and provided as input to the WTA circuit in the form of some 200-dimensional rate pattern). (B,C) Evoked activity of the WTA circuit for one example input sequence before learning (B) and for each of the four sequences after learning (C). The network activity is averaged and smoothed over 100 trial runs (gray traces), the blue dots show the spiking activity for one trial run. The input sequences are labeled by their pattern symbols on top of each plot. The neurons are sorted by the time of their highest average activity over all four sequences, after learning. For each sequence a different assembly of neurons becomes active in the WTA circuit. Dotted black lines indicate the boundaries between assemblies. Since the 4 assemblies that emerged have virtually no overlap, the WTA circuit has recovered the structure of the hidden states that underlie the task. (D) The lateral weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e128" xlink:type="simple"/></inline-formula> that emerged through STDP. The neurons are sorted using the same sorting algorithm as in (B,C). The black dotted lines correspond to assembly boundaries, neurons that fired on average less than one spike per sequence are not shown. Each neuron has learned to fire after a distinct set of predecessors, which reflects the sequential order of assembly firing. The stochastic switches between sequences are represented by enhanced weights between neurons active at the sequence onsets.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g002" position="float" xlink:type="simple"/></fig>
<p>This additional knowledge can be represented in a hidden state that encodes the required information, which renders this task a simple example of a HMM. The hidden states of this HMM have to encode the input patterns and the temporal context in which they appear in order to maintain the Markov property throughout the sequences, e.g. a distinct state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e129" xlink:type="simple"/></inline-formula> encodes pattern <italic>B</italic> when it appears in sequence <italic>AB-delay-ab</italic>. The temporal structure of the hidden state can be related to the finite state grammar in <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2A</xref>. The arcs of the grammar directly correspond to the hidden states, i.e. given knowledge about the currently visited arc allows us to complete the sequence. The symbols next to the arcs define the observation model, i.e. the most likely symbol throughout each state. In this simple symbolic HMM the observation model is in fact deterministic, since exactly one symbol is allowed in each state.</p>
<p>In the neural implementation of this task, the symbolic sequences are presented to the WTA circuit encoded by afferent spike trains. Every symbol <italic>A,B,C,D,a,b,c,d,delay</italic> is represented by a rate pattern with fixed length of 50 ms, during which each afferent neuron emits spikes with a symbol-specific, fixed Poisson rate (see <xref ref-type="sec" rid="s4">Methods</xref>). One example input spike train encoding the symbolic sequence <italic>AB-delay-ab</italic> is shown in the top panel of <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2A</xref>. The input spike times are not kept fixed but newly drawn for each pattern presentation. This input encoding adds extra variability to the task, which is not directly reflected by the simple symbolic finite state grammar. Still, the statistics underlying the input sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e130" xlink:type="simple"/></inline-formula> follow the dynamics of a HMM of the form (4), and therefore our WTA circuit and the spike trains that encode sequences generated by the artificial grammar share a common underlying model.</p>
<p>The observation model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e131" xlink:type="simple"/></inline-formula> of that HMM covers the uncertainty induced by the noisy rate patterns by assigning a certain likelihood to each observed input activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e132" xlink:type="simple"/></inline-formula>. The hidden state representation has to encode the context-dependent symbol identity and the temporal structure of the sequences, i.e. the duration of each individual symbol. In our continuous-time formulation the hidden state is updated at the time points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e133" xlink:type="simple"/></inline-formula>. Therefore, throughout the presentation of a rate pattern of 50 ms length, several state updates are encountered during which the hidden state has to be maintained. In principle this can be done by allowing each hidden state to persist over multiple update steps by assigning non-zero probabilities to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e134" xlink:type="simple"/></inline-formula>. However, this approach is well known to result in a poor representation of time as it induces an exponential distribution over the state durations, which is inappropriate in most physical systems and obviously also for the case of deterministic pattern lengths, considered here <xref ref-type="bibr" rid="pcbi.1003511-Rabiner1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>. The accuracy of the model can be increased at the cost of a larger state space by introducing intermediate states, e.g. by representing pattern <italic>B</italic> in sequence <italic>AB-delay-ab</italic> by an assembly of states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e135" xlink:type="simple"/></inline-formula> that form an ordered state sequence throughout the pattern presentation. Each of these assemblies encodes a specific input pattern, the temporal context and its sequential structure throughout the pattern, and with sufficiently large assemblies the temporal resolution of the model achieves reasonable accuracy. We found that this coding strategy emerges unsupervised in our WTA circuits through the STDP rule (7).</p>
<p>To show this, we trained a WTA circuit with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e136" xlink:type="simple"/></inline-formula> afferent cells and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e137" xlink:type="simple"/></inline-formula> circuit neurons by randomly presenting input spike sequences until convergence. In this experiment, the patterns were presented as a continuous stream of input spikes, without intermediate pauses or resetting the network activity at the beginning of the sequences. Training started from random initial weights, and therefore the observation and prediction model had to be learned from the presented spike sequences. Prior to learning the neural activity was unspecific to the patterns and their temporal context (see <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2B</xref>). <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2C</xref> shows the evoked activities for all four sequences after training. The output of the network is represented by the perievent time histogram (PETH) averaged over 100 trial runs and a single spike train that is plotted on top. To simplify the interpretation of the network output we sorted the neurons according to their preferred firing times (see <xref ref-type="sec" rid="s4">Methods</xref>). Each sequence is encoded by a different assembly of neurons. This reflects the structure of the hidden state that underlies the task. Since the input is presented as continuous spike train, the network has also learned intermediate states that represent a gradual blending between patterns. About 25 neurons were used to encode the information required to represent the hidden state of each sequence.</p>
<p>This coding scheme installs different representations of the patterns depending on the temporal context they appeared in, e.g. the pattern <italic>delay</italic> within the sequence <italic>AB-delay-ab</italic> was represented by another assembly of neurons than the one in the sequence <italic>BA-delay-ba</italic>. Small assemblies of about five neurons became tuned for each pattern and temporal context. This sparse representation emerged through learning and is not merely a consequence of the inherent sparseness of the WTA dynamics. Prior to learning all WTA neurons are broadly tuned and show firing patterns that are unordered and nonspecific (see <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2B</xref>). After learning their afferent synapses are tuned for specific input patterns, whereas the temporal contexts in which they appear are encoded in the excitatory lateral synapses. The latter can be seen by inspecting the synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e138" xlink:type="simple"/></inline-formula> shown in <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2D</xref>. They reflect the sparse code and also the sequential order in which the neurons are activated. They also learned to encode the stochastic transitions at the beginning of the cue phase, where randomly one of the four sequences is selected. These stochastic switches are reflected in increased strength of synapses that connect neurons activated at the end and the beginning of the sequences.</p>
<p>The behavior of the circuit is further examined in <xref ref-type="fig" rid="pcbi-1003511-g003">Fig. 3</xref>. The average network activity over 100 trial runs of the neurons that became most active during sequence <italic>AB-delay-ab</italic> are shown in <xref ref-type="fig" rid="pcbi-1003511-g003">Fig. 3A</xref>. In addition the spike trains for 20 trials are shown for three example neurons. The same sorting was applied as in <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2</xref>. Using the hidden state encoded by the network it should be possible to predict the recall patterns after seeing the cue, if it correctly learned the input statistics. We demonstrate this by presenting incomplete inputs to the network. After presentation of the delay pattern the input was turned off and the network was allowed to run freely. The delay pattern was played three times longer than in the training phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e139" xlink:type="simple"/></inline-formula>. During this time the network was required to store its current state (the identity of the cue sequence). After this delay time the input was turned off – no spikes were generated by the afferent neurons during this phase, the network was purely driven by the lateral connections. Since the delay time was much longer than the EPSP windows the network had to keep track of the sequence identity in its activity pattern throughout this time to solve the task. <xref ref-type="fig" rid="pcbi-1003511-g003">Fig. 3B</xref> shows the output behavior of the network for sequence <italic>AB-delay-free</italic> (where <italic>free</italic> denotes a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e140" xlink:type="simple"/></inline-formula> time window with no external input). After the initial sequence <italic>AB</italic> was presented, a small assembly of neurons became active that represents the delay pattern that was associated with that specific sequence. After the delay pattern was turned off, the network completed the hidden state sequence using its memorized activity, which can be seen by comparing the evoked and spontaneous spike trains in <xref ref-type="fig" rid="pcbi-1003511-g003">Fig. 3A and B</xref>, respectively.</p>
<fig id="pcbi-1003511-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g003</object-id><label>Figure 3</label><caption>
<title>Spontaneous replay of pattern sequences.</title>
<p>(A,B) The output behavior of a trained network for sequence <italic>AB-delay-ab</italic>. The network input is indicated by pattern symbols on top of the plot and pattern borders (gray vertical lines). (A) The average firing behavior of the network during evoked activity. The 30 circuit neurons that showed highest activity for this sequence are shown. The remaining neurons were almost perfectly silent. The network activity is averaged over 100 trial runs and neurons are sorted by the time of maximum average activity. Detailed spiking activities for three example neurons that became active after the delay pattern are shown. Each plot shows 20 example spike trains. (B) Spontaneous completion of sequence <italic>AB-delay-free</italic>. After presenting the cue sequence <italic>AB</italic> and the delay pattern for 150 ms the afferent input was turned off, letting the network run driven solely by lateral connections. During this spontaneous activity, the neurons are activated in the same sequential order as in the evoked trials. Detailed spiking activity is shown for the same three example neurons as in (A). (C) Histograms of the rank order correlation between the evoked and spontaneous network activity for all four sequences, computed over 100 trial runs. The sequential order of neural firing is reliably reproduced during the spontaneous activity and thus the structure of the hidden state is correctly completed.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g003" position="float" xlink:type="simple"/></fig>
<p>In order to quantify the ability of the network to reproduce the structure of the hidden state, we evaluated the similarity between the spontaneous and evoked network activity using the rank order correlation coefficient, which is a similarity measure normalized between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e141" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e142" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e143" xlink:type="simple"/></inline-formula> means that the order is perfectly preserved. This measure has been previously proposed to detect stereotypical temporal order in neural firing patterns <xref ref-type="bibr" rid="pcbi.1003511-Luczak1">[6]</xref>. <xref ref-type="fig" rid="pcbi-1003511-g003">Fig. 3C</xref> shows the histograms over the correlation coefficients for all four sequences. The histograms were created by calculating the rank order correlation between the spontaneous sequences and the PETH of the evoked sequences. It can be seen that the temporal order of the evoked sequence was reliably reproduced during the free run. To that end, for each of the input sequences, a stable representation has been trained into the network, that is encoded in the lateral synapses. This structure emerged completely unsupervised using the local STDP rule, solely from the intrinsic dynamics of the network.</p>
</sec><sec id="s2e">
<title>Mixed selectivity emerges in multiple interconnected WTA circuits</title>
<p>The first experiment demonstrated that through STDP, single neurons of a WTA circuit get tuned for distinct input patterns and the temporal context in which they appear. The neural code that emerged is reminiscent of some features found in cortical activity of monkeys solving similar tasks, namely the emergence of context cells that respond specifically to certain symbols when they appear in a specific temporal context <xref ref-type="bibr" rid="pcbi.1003511-Shima1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Barone1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Shima2">[37]</xref>. However, the overall competition of a single WTA circuit hinders the building of codes for more abstract features, which are also found in the cortex in the very same experiments where neurons in the same cortical area encode different functional aspects of stimuli and actions. They seem to integrate information on different levels of abstraction which results in a diverse and rich neural code, where close-by neurons are often tuned to different task-related features <xref ref-type="bibr" rid="pcbi.1003511-Rigotti1">[25]</xref>.</p>
<p>We show that our model reproduces this mixed selectivity of cortical neurons if multiple interconnected WTAs are trained on a common input. The strong competition is restricted to neurons within every single WTA, whereas there is no competition between neurons of different circuits and lateral connections allow full information exchange between the circuits. Therefore, the model is extended by splitting the network into smaller WTA groups, each of which receives input from a distinct inhibitory feedback loop that implements competition between members of that group. In addition all neurons receive lateral excitatory input from the whole network. Every WTA group still follows the dynamics of a forward sampler for a HMM. Each of these WTA circuits adapts its synaptic weights through STDP to best represent the observed input spike sequences. In addition, the lateral connections between WTA groups introduce a coupling between the network states of individual groups. The dynamics of the whole network of WTA circuits can be understood as a forward sampler for a coupled HMM <xref ref-type="bibr" rid="pcbi.1003511-Brand1">[38]</xref>, where every WTA group encodes one multinomial variable of a compound state such that from one time step to the next all single state variables have influence on each other <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Brand1">[38]</xref>.</p>
<p>In the first experiment we have seen that the WTA circuit learned to use about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e144" xlink:type="simple"/></inline-formula> of the available neurons to encode each of the four sequences. We have also seen that the network used small assemblies of neurons to represent each of the patterns in favor of a finer temporal resolution. This implies that WTA circuits of different size can learn to decode the input sequence on different levels of detail, where small circuits only learn the most salient features of the input sequences. To show this we trained a network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e145" xlink:type="simple"/></inline-formula> WTA groups of random size between 10 and 50 units, giving a total network size of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e146" xlink:type="simple"/></inline-formula>, on the simple object sequence memory task (<xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2A</xref>). The neural code that emerges in this network after training is shown in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4</xref>. The output rates of the circuit neurons were measured during the presentation of pattern <italic>a</italic> appearing in the sequence <italic>AB-delay-ab</italic>, <italic>BA-delay-ba</italic>, shown in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4A,B</xref> respectively. Three classes of neurons can be distinguished: 10 neurons were tuned to pattern <italic>a</italic> in the context <italic>AB-delay-ab</italic> only (shown in red), 12 neurons were tuned to pattern <italic>a</italic> exclusievly in the context <italic>BA-delay-ba</italic> (shown in blue) and 5 additional neurons encode pattern <italic>a</italic> independent of its context (green), i.e. they get activated by the pattern <italic>a</italic> in both sequences <italic>AB-delay-ab</italic> and <italic>BA-delay-ba</italic>. The remaining neurons were not significantly tuned for pattern <italic>a</italic> (average firing rate during pattern <italic>a</italic> was less than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e147" xlink:type="simple"/></inline-formula>, not shown in the plot).</p>
<fig id="pcbi-1003511-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g004</object-id><label>Figure 4</label><caption>
<title>Mixed selectivity in networks of multiple interconnected WTA circuits.</title>
<p>(A,B) Mean firing rate of the circuit neurons for evoked activity during pattern <italic>a</italic> in sequence <italic>AB-delay-ab</italic> (A) and <italic>BA-delay-ba</italic> (B). A threshold of 10 Hz (dashed line) was used to distinguish between neurons that were active or inactive during the pattern. Firing rates of neurons that were not context selective are shown in green, that of neurons selective for starting sequences <italic>AB</italic> and <italic>BA</italic> are shown in red and blue, respectively. Neurons that did not fall in one of these groups are not shown. Spike trains of one context selective (C) and one non-selective (D) neuron are presented for spontaneous completion of sequence <italic>AB-delay-ab</italic> (upper) and <italic>BA-delay-ba</italic> (lower) (cue phase is not shown). Spike raster plots over 20 trial runs and corresponding averaged neural activity (PETH) are shown. The two neurons encode the input on different levels of abstraction. The neuron in panel (D) shows context cell behavior, since it encodes pattern <italic>a</italic> only if it occurs in the context of sequence <italic>ab</italic>. During <italic>ba</italic> it remains (almost) perfectly silent. The neuron in (C) is not context selective, but nevertheless fires reliably during the time slot of pattern <italic>a</italic> during the free run by integrating information from other (context selective) neurons. It belongs to a WTA circuit with 15 neurons, for which the network state projection is shown in panel (E). (E,F) Linear projection of the network activity during the delay phase to the first two components of the jPCA, for a single WTA circuit with 15 neurons (E) and for the whole network (F). 10 trajectories are plotted for each sequence (<italic>AB-delay-ab</italic> red, <italic>BA-delay-ba</italic> green, <italic>CD-delay-cd</italic> blue, <italic>DC-delay-dc</italic> yellow). The dots at the beginning of each line, indicate the onsets of the delay state, i.e. the beginning of the trajectories. The plots have arbitrary scale. The projection of the WTA circuit in (E) does not allow a linear separation between all four sequences, whereas the activity of the whole network (F) clusters into four sequence-specific regions. The network neurons use this state representation to modulate their behavior during spontaneous activity.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g004" position="float" xlink:type="simple"/></fig>
<p>To pinpoint the computational function that emerged in the network we compared the spontaneous activity of individual neurons from different WTA circuits. Spike trains for one context-specific and one non-specific neuron are compared in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4C and D</xref>, respectively. Both panels show spike raster plots over 20 trial runs and averaged neuron activities (PETH) for sequences <italic>AB-delay-free</italic> and <italic>BA-delay-free</italic>. The neuron in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4C</xref> belongs to a small WTA group with a total size of 15 neurons and shows context unspecific behavior, whereas the neuron in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4D</xref> which belongs to a larger WTA group (42 neurons) is context specific (see <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4A,B</xref>). This behavior is also reproduced during the free run, when the neurons are only driven by their lateral synapses. The neuron in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4D</xref> remains silent during <italic>BA-delay-free</italic> and thus shows the properties of context cells observed in the cortex, whereas the neuron in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4C</xref> is active during both sequences. Still, during spontaneous replay that neuron correctly reproduces the temporal structure of the input sequences. In sequences starting with <italic>AB</italic> the neural activity peaks at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e148" xlink:type="simple"/></inline-formula> after the onset of the free run – the time pattern <italic>a</italic> was presented in the evoked phase. If the sequence starts with <italic>BA</italic> this behavior is modulated and the activity is delayed by roughly 50 ms, to the time point <italic>a</italic> would appear in the recall phase. The required information to control this modulation was not available within the small WTA group the neuron belongs to, but provided by neighboring context-specific neurons from other groups.</p>
<p>To see this we trained a linear classifier on the evoked activity during the delay phase of <italic>AB-delay-ab</italic> and <italic>BA-delay-ba</italic> (see <xref ref-type="sec" rid="s4">Methods</xref> for details). If the neurons reliably encode the sequence identity a separating plane should divide the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e149" xlink:type="simple"/></inline-formula>-dimensional space of network activities between the sequences. Training the classifier only on the 15-dimensional state space of the group the neuron in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4C</xref> belongs to, did not reveal such a plane (the classification performance was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e150" xlink:type="simple"/></inline-formula>). Therefore, this small WTA circuit did not encode the required memory item to distinguish between the two sequences after the delay phase. However, the whole network of all WTA groups reliably encoded this information and the classifier trained on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e151" xlink:type="simple"/></inline-formula>-dimensional state space could distinguish between the delay phases of <italic>AB-delay-ab</italic> and <italic>BA-delay-ba</italic> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e152" xlink:type="simple"/></inline-formula> accuracy.</p>
<p>To illustrate the different emergent representations, we compared linear projections of the state of the small WTA group with 15 neurons and the state of the whole network in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4E,F</xref>, respectively. The plots show the network activity during the delay phase for all four sequences. Each line corresponds to a trajectory of the evoked network activity, where the line colors indicate the sequence identity. The state trajectories were projected onto the first two dimensions of the dynamic principal component analysis (jPCA), that was recently introduced as an alternative to normal PCA that is applicable to data with rotational dynamics <xref ref-type="bibr" rid="pcbi.1003511-Churchland1">[39]</xref>. Empirically, we found this analysis method superior to normal PCA in finding linear projections that separate the network states for different input sequences. One explanation for this lies in the dynamical properties of WTA circuits. Due to the global normalization which induces a constant network rate, the dynamics of the network are roughly energy-preserving. Since this implies that the corresponding linear dynamical system is largely non-expanding/contracting, a method that identifies purely rotational dynamics such as the jPCA was found to be beneficial here.</p>
<p><xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4E</xref> shows the first two jPCA components of the neural activities during the delay phase for the WTA circuit with 15 neurons, which the neuron in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4C</xref> belongs to. This circuit was not able to distinguish between all four input sequences, since it activated the same neurons to encode them. This is also reflected in the jPCA projections shown in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4E</xref>, which show a large overlap for sequences <italic>AB-delay-ab</italic> and <italic>BA-delay-ba</italic>. On the other hand, the network state comprising all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e153" xlink:type="simple"/></inline-formula> neurons reliably encoded the sequence identities (see <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4F</xref>). The delay state for each sequence spans an area in the 2-D projection and therefore the network found a state space that allows a linear separation between the sequences. Such a representation is important since the neuron model employs a linear combination of the network state in the membrane dynamics (1) and therefore provides the information required by the neurons in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4C,D</xref> to modulate their spontaneous behavior.</p>
</sec><sec id="s2f">
<title>Trajectories in network assemblies emerge for stationary input patterns</title>
<p>Information about transient stimuli is often kept available over long time spans in trajectories of neural activity in the mammalian cortex <xref ref-type="bibr" rid="pcbi.1003511-Han1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Luczak2">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Xu1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Jin1">[40]</xref> and in songbirds <xref ref-type="bibr" rid="pcbi.1003511-Fiete1">[41]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Hahnloser1">[43]</xref>. In the previous experiment we saw that our model is in principle capable to develop such trajectories in neural assemblies (see <xref ref-type="fig" rid="pcbi-1003511-g003">Fig. 3B</xref>), which emerged to encode salient input patterns and the temporal structure throughout them. However, in that experiment the input sequences comprised a rich temporal structure, since each pattern was only shown for a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e154" xlink:type="simple"/></inline-formula> time bin which might have facilitated the development of these activity patterns. In this section we study whether a similar behavior also emerges when the input signal is stationary over long time spans.</p>
<p>In analogy to the previous experiment we generated two input sequences <italic>A-delay</italic> and <italic>B-delay</italic>. The patterns <italic>A</italic>, <italic>B</italic> were played for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e155" xlink:type="simple"/></inline-formula> and the pattern <italic>delay</italic> for 500 ms. As in all other experiments, the patterns were rate patterns, i.e. each input neuron fired with a constant Poisson rate during the pattern and spike times were not kept fixed throughout trials. One example input spike train is shown in <xref ref-type="fig" rid="pcbi-1003511-g005">Fig. 5A</xref>.</p>
<fig id="pcbi-1003511-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g005</object-id><label>Figure 5</label><caption>
<title>Neural trajectories emerge for stationary input patterns.</title>
<p>(A) A network was trained with an extended delay phase of 500 ms. Input spike trains of a single run for sequence <italic>A-delay</italic> (25 out of 100 afferent neurons). Throughout the delay phase the afferent neurons fire with fixed stationary Poisson rates. (B) The output behavior for sequence <italic>A-delay</italic> averaged over 100 trial runs. The circuit neurons are sorted according to their mean firing time within the sequences (120 out of 704 neurons are shown). (C) Histograms of the rank order correlation between the evoked and spontaneous network activity. The sequential order of neural firing is preserved during spontaneous activity. (D,E) Homeostatic plasticity enhances the formation of this sequential structure. The output behavior of the network trained with STDP and the homeostatic plasticity mechanism is shown. Approximately 50% of the neurons encode each of the two sequence. The neurons learn to fire at a specific point in time within the delay patterns, building up stable trajectories.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g005" position="float" xlink:type="simple"/></fig>
<p>Although the input was stationary for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e156" xlink:type="simple"/></inline-formula> during the <italic>delay</italic> pattern, we could still observe the emergence of neural trajectories in the network after training. Again, we used a network composed of multiple interconnected WTA circuits to learn these patterns. We employed a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e157" xlink:type="simple"/></inline-formula> WTA groups of random size in the range from 10 to 100 neurons. The total network had a size of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e158" xlink:type="simple"/></inline-formula> circuit neurons and we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e159" xlink:type="simple"/></inline-formula> afferent cells. <xref ref-type="fig" rid="pcbi-1003511-g005">Fig. 5B</xref> shows the sorted average output activity after training. For each of the two sequences a distinct assembly of neurons emerged and the neurons composing these assemblies fired in a distinct sequential order. <xref ref-type="fig" rid="pcbi-1003511-g005">Fig. 5C</xref> shows the rank order correlations between the evoked and spontaneous activities. The trajectories of neural firing were reliably reproduced during spontaneous activity, but only about 100 neurons were used for each of the two assemblies, leaving the remaining 500 neurons (almost) perfectly silent.</p>
<p>The emergence of these trajectories can be further enhanced using a homeostatic intrinsic plasticity mechanism which enforces that on average all network neurons participate equally in the representation of the hidden state. This can be achieved by a mechanism that regulates the excitability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e160" xlink:type="simple"/></inline-formula> of each neuron, such that the overall output rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e161" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e162" xlink:type="simple"/></inline-formula> (measured over a long time window) converges to a given target rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e163" xlink:type="simple"/></inline-formula>. (see <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss2">[44]</xref> and the <xref ref-type="sec" rid="s4">Methods</xref> section). Augmenting the dynamics of the network with this intrinsic plasticity rule prevents neurons from becoming inactive if their synaptic weights decrease and by that assures that each neuron joins one of the assemblies. This can be seen in <xref ref-type="fig" rid="pcbi-1003511-g005">Fig. 5C,D</xref> which shows the output activity after training with STDP augmented with the homeostatic mechanism. The neurons formed a fixed ordered sequence and thus showed a clear preference for a certain point in time within the pattern. Even though the delay pattern had no salient temporal structure (the rates of all afferent neurons were constant throughout the pattern) these trajectories were formed by imprinting the sequential order of the neural activity into the lateral excitatory connections. As in the first experiment each neuron has learned to fire after a distinct group of preferred predecessors, resulting in neural trajectories through the network. Therefore, the time that has elapsed since the <italic>delay</italic> pattern started could be inferred from the neural population activity. In addition the identity of the initial pattern was also memorized, since about half of the population became active for each of the two sequences.</p>
</sec><sec id="s2g">
<title>Learning the temporal structure of an artificial grammar model</title>
<p>The finite state grammar used in the previous experiments (<xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2A</xref>) did not utilize the full expressive power of HMMs since it only allowed stochastic switches at the beginning of each sequence. In this section we consider the problem of learning more general finite state grammars in WTA circuits, a problem that has also been extensively studied in cognitive science in artificial grammar learning (AGL) experiments <xref ref-type="bibr" rid="pcbi.1003511-Reber1">[45]</xref>. <xref ref-type="fig" rid="pcbi-1003511-g006">Fig. 6A</xref> shows the artificial grammar that was used in <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref> to train subjects using different stimulus modalities (visual, auditory and tactile). There it was shown that humans can acquire the basic statistics of such grammars extremely fast. On this particular task humans showed a performance of 62% to 75% percent (depending on the stimulus modality that was used) after only a few dozens of stimulus presentations <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref>.</p>
<fig id="pcbi-1003511-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g006</object-id><label>Figure 6</label><caption>
<title>Fast learning of an artificial grammar.</title>
<p>(A) The artificial grammar from <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Gomez1">[74]</xref> represented as a finite state grammar graph. Grammatical sequences are generated by following a path from <italic>Start</italic> to <italic>Exit</italic>. If a node has more than one outgoing arc one is chosen at random with equal probability to continue the path. (B) Convergence of the network performance on that task. The blue curve shows the evolution of the mean classification performance against the number of training samples, when forward sampling was used. The blue shaded area indicates the standard deviation over 20 trial runs. After 80 training samples the network exceeds human performance reported in <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref>. Using rejection sampling with 10 samples on average (red curve) does not significantly outperform forward sampling on this task.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g006" position="float" xlink:type="simple"/></fig>
<p>We show that our network model can extract the basic structure of this grammar. This internal representation can be subsequently used to classify unseen sequences as grammatical or not. Through STDP the network adapts the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e164" xlink:type="simple"/></inline-formula> such that they reflect the statistics underlying the training sequences, and the emergent HMM can then be used to evaluate the sequence likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e165" xlink:type="simple"/></inline-formula>. The ability of the network to distinguish between grammatical and ungrammatical sequences was assessed by applying a threshold on the sequence log-likelihood, an approximation of which was computed over a single sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e166" xlink:type="simple"/></inline-formula> from (5) (see <xref ref-type="sec" rid="s4">Methods</xref>). The threshold was assigned to the mean of the log-likelihood values computed for all test sequences. Likelihoods that laid above that threshold were reported as grammatical.</p>
<p>In this experiment we used a sparse input coding, where only a small subset of afferent neurons is activated for each of the symbols. This representation could be realized by another WTA circuit used as input for the network to decode more complex input patterns. We trained a single WTA circuit with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e167" xlink:type="simple"/></inline-formula> neurons on this sparse input. Using this model, we were able to achieve high learning speeds. In each training iteration one of the 12 training data sets from <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref> (using only the first sequence of each match/mismatch pair) was chosen at random and presented to the network. For testing we used the 20 test sequences from <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref> to evaluate the learning performance. Training was interrupted after every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e168" xlink:type="simple"/></inline-formula> sequence presentation to assess the classification performance. The resulting learning curve is shown in <xref ref-type="fig" rid="pcbi-1003511-g006">Fig. 6B</xref>. The classification rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e169" xlink:type="simple"/></inline-formula> that was reported in the behavioral experiment was exceeded after only 80 iterations. By training the network beyond this point performances up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e170" xlink:type="simple"/></inline-formula> were reached. Note that none of the training sequences appeared in the test set. Therefore the network has not just learned a fixed set of sequences, but extracted relevant statistical features that allowed it to generalize to new data.</p>
</sec><sec id="s2h">
<title>A refined EM approximation using rejection sampling</title>
<p>So far in all experiments the simple forward sampling approximation was used for learning the model parameters. Although this learning paradigm has shown to be surprisingly powerful, it is limited and will not be sufficient if the network is required to learn more complex tasks or acquire probabilistic models with a high level of detail. In this section we derive the refined approximation toward evaluating the HMM E-Step in a recurrent WTA circuit based on rejection sampling.</p>
<p>Exactly solving the E-step requires to evaluate the posterior probability of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e171" xlink:type="simple"/></inline-formula>, given by<disp-formula id="pcbi.1003511.e172"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e172" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e173" xlink:type="simple"/></inline-formula> is the HMM joint distribution, given by <xref ref-type="disp-formula" rid="pcbi.1003511.e054">equation (4)</xref>. A stochastic EM update is realized by drawing a state sequence from the posterior for which the M-step parameter updates are performed. However, directly sampling from (8) is not possible for a spiking neural network, since it requires the integration of information over the whole state sequence and thus, looking into the future. This can be seen by noting that the integral in (8) runs over the state space of the whole sequence. To that end, the network is not able to sample from this distribution directly. Nevertheless, it is possible to indirectly evaluate (8) using samples generated from (5), which can be expressed by<disp-formula id="pcbi.1003511.e174"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e174" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e175" xlink:type="simple"/></inline-formula> denotes the expected value over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e176" xlink:type="simple"/></inline-formula>, which in this context is called a <italic>proposal distribution</italic> since it is used to propose samples, which are then used to indirectly evaluate the target distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e177" xlink:type="simple"/></inline-formula>. The scalar <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e178" xlink:type="simple"/></inline-formula> is the <italic>importance weight</italic> between the target and the proposal distribution, which is used to scale the influence of the sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e179" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Neal2">[46]</xref>.</p>
<p>The expectation in the denominator of (9) is again not easy to evaluate, since it requires us to integrate over multiple sequences. The most pragmatic solution to this problem is to approximate this term using a single sample from the proposal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e180" xlink:type="simple"/></inline-formula>. Under this approximation the importance weight in (9) cancels out and we arrive at the trivial approximation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e181" xlink:type="simple"/></inline-formula>, i.e. each sample from the proposal distribution is accepted as a valid sample from the posterior. This is the forward sampling approximation that was used so far throughout all experiments.</p>
<p>In order to improve this approximation we use the stochasticity of the network, which assures that different state sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e182" xlink:type="simple"/></inline-formula> are proposed if the same input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e183" xlink:type="simple"/></inline-formula> is presented several times. Rejection sampling utilizes this stochasticity and preferentially selects sequences with high likelihood throughout the whole input. The required information to do this selection is a global quantity that must be tracked over the whole sequence. The probability to accept a state sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e184" xlink:type="simple"/></inline-formula> is directly proportional to the importance weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e185" xlink:type="simple"/></inline-formula>, which computes to<disp-formula id="pcbi.1003511.e186"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e186" xlink:type="simple"/><label>(10)</label></disp-formula>Note that (10) can be easily computed forward in time, since in each time step, it only needs to be updated using the instantaneous input likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e187" xlink:type="simple"/></inline-formula>. Further note that this is a measure for surprise or prediction error – the probability of observing the current input given the previous state. The information to decide whether to accept <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e188" xlink:type="simple"/></inline-formula> is the accumulated prediction error over the whole sequence. This approach also naturally extends to the case of multiple interconnected WTAs. There, the contributions to the importance weight of every single circuit have to be multiplied in every time step and therefore, a possible rejection is in that case effective for the whole network of all WTAs at once.</p>
<p>Since the importance weights need to be accumulated over the whole sequence of spike events of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e189" xlink:type="simple"/></inline-formula>, the weight update rules (7) can not be applied instantaneously. In the neural implementation we achieved this using a synaptic eligibility trace as proposed in <xref ref-type="bibr" rid="pcbi.1003511-Izhikevich1">[47]</xref>. Instead of updating the weights directly they are tagged and consolidation of the tags is delayed until the whole sequence is read. The probability to accept these tags is proportional to the importance weights, i.e.<disp-formula id="pcbi.1003511.e190"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e190" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e191" xlink:type="simple"/></inline-formula> is a constant that scales the acceptance rate. If a sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e192" xlink:type="simple"/></inline-formula> is accepted, the synaptic tags are consolidated. If the circuit decides not to accept, the synaptic weight changes for the whole sequence have to be discarded. This result is analytically similar to <xref ref-type="bibr" rid="pcbi.1003511-Brea1">[15]</xref>, where the importance weights (10) were introduced by weighting the eligibility traces with a deterministic scalar factor (importance sampling). Here, in the rejection sampling framework a stochastic variant of this method is used. The advantage of the rejection sampling method is that it is not necessary to explicitly compute the normalization in (8). The normalization can be approximated by replaying in every training iteration the input sequence multiple times until it gets accepted once, instead of using a constant number of replays as with importance sampling. In practice however it is necessary to adapt the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e193" xlink:type="simple"/></inline-formula> throughout learning in order to get a reasonable number of replays. We used a simple linear tracking mechanism for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e194" xlink:type="simple"/></inline-formula> throughout the experiments (see <xref ref-type="sec" rid="s4">Methods</xref>). A performance comparison of these different sampling approximations is provided at the end of the <xref ref-type="sec" rid="s2">Results</xref> section.</p>
<p>We assume that the circuit interacts with a mechanism that allows the replay of the afferent stimulus multiple times. By enforcing that each input is accepted once, we guarantee that the network learns the statistics of all input sequences with equal accuracy. This view allows us to make an interesting theoretical prediction: when an input is not well represented by the network it is more likely to be rejected and therefore, the number of rejected and resampled sequences represents a notion of novelty. Literally speaking, the network pays more attention to novel inputs, by resampling them multiple times (see <xref ref-type="sec" rid="s4">Methods</xref> for details).</p>
</sec><sec id="s2i">
<title>Rejection sampling enhances the learning capabilities of STDP</title>
<p>In the following experiments we investigate the possible performance gain that can be achieved if the network has access to this rejection sampling mechanism. We have previously seen that the grammar from <xref ref-type="fig" rid="pcbi-1003511-g001">Fig. 1</xref> in <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref> can be learned almost perfectly using pure forward sampling. However, this data set had a very simple structure. To distinguish between grammatical and ungrammatical sequences only required the analysis of the local statistics of the input. E.g. it is easy to see that the sequence <italic>DEAC</italic> is not grammatical since it contains the bigram <italic>DE</italic>, which never appears in the training data. Each of the ungrammatical sequences contains at least one illegal bigram and thus can be classified based on a simple model of symbol transitions. This simple structure was already recovered with the online learning scheme and therefore using rejection sampling on that task did not result in a significant performance increase (see <xref ref-type="fig" rid="pcbi-1003511-g006">Fig. 6</xref>).</p>
<p>To demonstrate the advantage of rejection sampling, we created a grammar that required integration of information over a longer time span, shown in <xref ref-type="fig" rid="pcbi-1003511-g007">Fig. 7A</xref>. Although this grammar only allows to create four sequences <italic>AABC</italic>, <italic>BBAC</italic>, <italic>ABAD</italic> and <italic>BABD</italic>, the underlying structure is more complex than in the previous tasks. The identity of the last symbol can only be inferred if the identity and context of the first symbol is integrated and memorized over the whole sequence. To that end, the rejection sampling algorithm that allows the network to propagate information over the whole sequence, should bring a definite benefit over forward sampling for this task.</p>
<fig id="pcbi-1003511-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g007</object-id><label>Figure 7</label><caption>
<title>Rejection sampling enhances the classification performance of the network.</title>
<p>(A) The grammar graph used for this task. A three letter sequence composed of <italic>A</italic>s and <italic>B</italic>s identifies the last symbol, <italic>C</italic> or <italic>D</italic>. Therefore, the most salient information is provided at the end of the sequence. (B) The classification rate on this task is plotted for forward (green) and rejection sampling (red). The error bars indicate the standard deviation over 10 trial runs. Rejection sampling significantly increases the classification performance on this task. (C,D) Comparison of the time courses of the instantaneous input log likelihood for a legal input sequence <italic>BBAC</italic> (C) and an illegal sequence <italic>BBAD</italic> (D). Input patterns are indicated by the pattern symbols on top of the plots. The upper plot shows the output spike trains of the network, the lower plot shows the traces of the instantaneous input likelihood plotted in the log domain, which indicates the ability of the network to predict the continuation of the afferent spike train. The trace in (D) shows a strong negative peak at the illegal transition at 150 ms. The prediction model that emerged through STDP augmented with rejection sampling, enables the network to detect illegal sequences.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g007" position="float" xlink:type="simple"/></fig>
<p>The quantity that is needed to update the importance weights (10) and also to estimate the sequence likelihood for classifying grammatical against ungrammatical inputs, is given by the instantaneous input likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e195" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref>). As pointed out earlier, this quantity is a measure for surprise, i.e. the probability of observing the current input pattern given the network state. The ability of the network to exploit this prediction error to classify sequences is illustrated in <xref ref-type="fig" rid="pcbi-1003511-g007">Fig. 7</xref>. The input-output behavior of a network after training with rejection sampling is shown for the grammatical sequence <italic>BBAC</italic> and the ungrammatical sequence <italic>BBAD</italic>, in <xref ref-type="fig" rid="pcbi-1003511-g007">Fig. 7C,D</xref> respectively. The bottom plots show traces of the instantaneous input log-likelihood. Throughout the grammatical sequence in <xref ref-type="fig" rid="pcbi-1003511-g007">Fig. 7C</xref> the trace stays near baseline, which indicates that the network is capable of predicting the sequence. Within the patterns, the trace only shows small deviations due to input noise. Switches between the input patterns e.g. at the border from pattern <italic>A</italic> to <italic>C</italic> cause modest levels of surprise, due to the sudden change of the network state. However, the illegal transition to pattern <italic>D</italic> in <xref ref-type="fig" rid="pcbi-1003511-g007">Fig. 7D</xref> causes a strong negative peak. At this point the network is not capable of predicting the final pattern. Thus the input is assigned to a low overall sequence likelihood and will therefore be classified as ungrammatical.</p>
<p>In the rejection sampling algorithm this quantity is also used throughout training, to learn preferably from sequences that are best capable of predicting the input sequences. To quantify the advantage of this method over online learning we compared the performance on the AGL task. As in the previous experiment, the ability of the network to distinguish between grammatical and ungrammatical sequences was evaluated by applying a threshold on the sequence likelihood. The threshold was assigned to the mean of the log-likelihood values computed for all tested sequences. The network parameters were tuned such that the number of rejected samples in each iteration, averaged over the whole training session was equal to the desired number of samples (see <xref ref-type="sec" rid="s4">Methods</xref>). The classification errors are compared in <xref ref-type="fig" rid="pcbi-1003511-g007">Fig. 7B</xref> for learning with forward and rejection sampling. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e196" xlink:type="simple"/></inline-formula> that scales the number of rejected samples was tracked to give an average number of 10 rejected samples per iteration. Despite this relatively small number of times the sequences is resampled, it can be seen that the performance on this task significantly increased with rejection sampling. Online learning achieved a classification rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e197" xlink:type="simple"/></inline-formula>. With rejection sampling the network achieved <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e198" xlink:type="simple"/></inline-formula> classification rate. Hence we confirmed, that having access to the rejection sampling mechanism allows the network to learn the input statistics with higher levels of accuracy. Furthermore, for the example given here, this was achieved with a relatively small average number of resampled state sequences.</p>
</sec><sec id="s2j">
<title>Comparison of the convergence speed and performance of the approximate algorithms</title>
<p>In order to give a quantitative notion of how the sampling approximations affect the learning performance, we applied the methods to solve a generic HMM learning task. To allow a direct comparison with standard machine learning algorithms for HMMs, we used a time-discrete version of our model in this section. Therefore, we set the inter-spike-intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e199" xlink:type="simple"/></inline-formula> to a fixed constant value and used rectangular EPSP kernels of the same length. With this modification our model is equivalent to a discrete input, discrete state HMM, commonly considered in the machine learning literature <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>. We created random HMMs and used them to generate a training and a test data set. Using this data we compared the training performance of different approximation algorithms.</p>
<p>The accuracy of the rejection sampling algorithm crucially depends on how the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e200" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003511.e190">equation (11)</xref> is selected. If it is set to a very large constant value, every sample gets accepted and we arrive at the simple forward sampling approximation. We compared this forward sampling algorithm with the simple tracking algorithm that was used in the previous experiment and with the optimal mechanism, which computes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e201" xlink:type="simple"/></inline-formula> over a batch of sampled sequences (see <xref ref-type="sec" rid="s4">Methods</xref>). In addition we compared these methods with the importance sampling algorithm considered in <xref ref-type="bibr" rid="pcbi.1003511-Brea1">[15]</xref>, where the scalar values of the importance weights were directly used to weight synaptic tags. All sampling methods were compared for an average number of 10 and 100 resampled sequences. Furthermore we applied standard EM learning for HMMs (the Baum-Welch algorithm) as reference method <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Baum1">[31]</xref>.</p>
<p>The results of the eight different training algorithms are compared in <xref ref-type="fig" rid="pcbi-1003511-g008">Fig. 8</xref>. The figure shows the log-likelihood on the test data averaged over the 50 learning trials. As can be seen, pure forward sampling shows poor performance on this task compared with Baum-Welch learning, but with increasing number of samples the approximation approaches the performance of the exact EM updates. Interestingly we found that importance sampling and rejection sampling show almost the same performance. We believe that the reason for this lies in the high variance of the importance weights. The weights of consecutive samples can differ several orders of magnitude. After normalization, effectively only the sample with the highest importance weight has non-zero influence on the weight updates. Therefore the two algorithms are numerically almost identical for the task considered here. Using the tracking mechanism for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e202" xlink:type="simple"/></inline-formula> resulted in decreased performance compared to the exact algorithm. Still, a significant performance gain can be observed with increased average number of samples.</p>
<fig id="pcbi-1003511-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003511.g008</object-id><label>Figure 8</label><caption>
<title>Comparison of the convergence speed and learning performance of different sampling methods.</title>
<p>Comparison of the sampling approximations to standard HMM learning. The performance is assessed by the log likelihood averaged over 50 trial runs. The plots show average convergence properties of: forward sampling (solid blue), importance sampling over 10 (dashed yellow) and 100 trials on average (solid yellow), rejection sampling over 10 (dashed red) and 100 trials (solid red), rejection sampling with the simple linear tracking of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e203" xlink:type="simple"/></inline-formula> over 10 (dashed green) and 100 trials on average (solid green), and the Baum-Welch algorithm (solid black). With increased number of samples the performance of the algorithm converges towards the solution of the standard EM algorithm. There was no significant performance difference between rejection and importance sampling. The simple tracking mechanism for the rejection sampler is outperformed by the exact algorithm, but still a significant performance gain with increased number of samples can be observed.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003511.g008" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We have shown that STDP in WTA circuits with lateral excitatory connections implements the capability to represent the statistical structure underlying time-varying input patterns. The different types of excitatory synapses in the network serve different computational functions. Lateral connections recurrently feed back past network spikes which are used to predict a prior belief about the current network activity. The feedforward synapses match this prediction against the belief inferred from afferent inputs. The sparse code that emerges in this circuit allows to represent the activity of the whole network as samples from the state space of a HMM, implementing a forward sampler, which provides the circuit with a simple online approximation to exact HMM inference.</p>
<p>We have focused in this article on an idealized version of the STDP rule that implements the maximization step of the EM algorithm for the HMM. Similar rules also emerged in earlier studies as stochastic approximations to EM implemented in networks of spiking neurons <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Nessler2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Keck1">[48]</xref>, for learning instantaneous afferent spike patterns. The only structural difference in the network architecture for temporal models is the presence of lateral excitatory connections. We have shown that if a WTA circuit is passively exposed to spatiotemporal rate patterns STDP implements a crude online approximation of EM. The emerging neural codes represent the hidden states that underlie the spatiotemporal input patterns. Different neurons are activated for the same input pattern if it appears in different temporal contexts. Furthermore, we have shown that if multiple WTA circuits are recurrently interconnected the network activity becomes more diverse and encodes various abstract features.</p>
<p>Throughout our analysis we realized the WTA dynamics using a feedback loop, where the required inhibition was given in its theoretically optimal form, according to <xref ref-type="disp-formula" rid="pcbi.1003511.e041">equation (3)</xref>. This optimal inhibition predicted by our model is strongly correlated with the activation of excitatory neurons within the WTA circuit. Such strong balance and correlation between excitation and inhibition has been observed in the cortex in vivo <xref ref-type="bibr" rid="pcbi.1003511-Okun1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Haider1">[49]</xref>. A consequence of this inhibitory feedback in our network model is that the total output rate is constant. Yet individual neurons in the network may exhibit complex behavior, and our experimental results have shown that they exploit a wide dynamical range. Furthermore it has been shown in <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref> that the assumption of a constant overall output rates can be lifted for the case of WTA circuits without lateral synapses. The only requirement identified there was that the circuit-wide output rate and the input had to be stochastically independent, which was theoretically shown and experimentally verified in an experiment where the network rate was modulated with a global oscillation throughout learning. The constant output rate considered in the present study is the simplest case which is compatible with our model. Identifying a more general class of rate functions which can be incorporated into our theoretical framework will be the subject of future work.</p>
<p>The activity patterns that emerge in our WTA circuits share important features with experimentally observed cortical dynamics. One feature is that neurons become tuned for mixtures of different task-relevant aspects as commonly observed in cortical neurons <xref ref-type="bibr" rid="pcbi.1003511-Rigotti1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Shima1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Barone1">[36]</xref>. The neural assemblies that encode these temporal features imprint their stereotypical sequential activation pattern within the lateral synapses. Another common feature is the emergence of stereotypical firing sequences during evoked and spontaneous activity, which is also found in cortical activity <xref ref-type="bibr" rid="pcbi.1003511-Han1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Luczak2">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Xu1">[26]</xref>. This analysis also provides a theoretical foundation for the results that were reported recently in <xref ref-type="bibr" rid="pcbi.1003511-Klampfl1">[50]</xref>. There, a similar network of stochastic WTA circuits was used to learn spike patterns superimposed with Poisson noise, and similar stimulus-specific assemblies emerged. But no theoretical framework was provided there.</p>
<p>The network of multiple interconnected WTA circuits has a very interesting theoretical interpretation as it implements in that case a forward sampler for a coupled HMM <xref ref-type="bibr" rid="pcbi.1003511-Brand1">[38]</xref>, where multiple HMMs run in parallel to jointly encode the hidden state. In our experiment this coupling between neighboring WTA circuits allowed them to reproduce typical sequences of hidden states in the absence of input, even if some circuits did not have enough expressive power to store this information. An interesting future extension of this model would be to present different coupled stimuli (e.g. speech and audio from a common source) to different WTA groups in this circuit. Individual WTA circuits would then learn the temporal structure of these stimuli and the lateral excitatory synapses between WTA circuits would detect relevant correlations between them.</p>
<p>We have also shown that STDP installs in WTA circuits capabilities that go beyond just learning afferent sequences. From few presentations the network extracted relevant statistical properties underlying the afferent patterns. We demonstrated this on an artificial grammar learning task. The network extracted parts of the structure of this grammar, which allowed it to subsequently classify unseen sequences as stemming from the same grammar or not. Interestingly, the learning speed and classification performance achieved with the forward sampling approximation, in the early learning phase, is comparable to the performance reported for humans on the same task <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref>. This is also interesting because the network considered here is similar to the single recurrent network (SRN) previously suggested as a model for artificial grammar learning <xref ref-type="bibr" rid="pcbi.1003511-Elman1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Jordan1">[52]</xref>. The context layer, that is used in the SRN to store the hidden layer activity from previous time steps, is implicitly implemented in the lateral synapses of our WTA circuit. The SRN was successfully used to model human capabilities in artificial grammar learning tasks <xref ref-type="bibr" rid="pcbi.1003511-Cleeremans1">[53]</xref> (but see <xref ref-type="bibr" rid="pcbi.1003511-Boucher1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Pothos1">[55]</xref>, for alternative theories and models of artificial grammar learning).</p>
<p>We have also exhibited a strategy to increase the computational power of WTA circuits by using more advanced learning methods. The rejection sampling algorithm that was proposed here is one possible solution to this problem. It enables the network to learn the temporal statistics with a much higher degree of accuracy, but at the same time it considerably increases the complexity of learning. Each input sequence must be replayed multiple times and thus, the convergence speed is decreased since many sampled paths will be rejected (in the experiment we resampled each path 10 times on average, therefore the learning time increased 10-fold). This makes learning possible on a long time scale only. However, the two mechanisms – pure forward sampling and rejection sampling – should not be seen as mutual exclusive strategies. Possibly both mechanisms could be found in biological systems. STDP might subserve to learn a quick preliminary representation of novel input statistics, while more complex models could emerge on a long time scale by selectively modulating the learning rate with global information. We demonstrated that in some cases a significant increase in learning performance can be achieved with only a small average number of resampled sequences. The experimental results suggest that for learning temporal sequences and simple grammars the pure implementation of STDP in WTA circuits is sufficient, whereas third-factor STDP rules become relevant for learning complex temporal structures.</p>
<sec id="s3a">
<title>Related work</title>
<p>The close relation between HMMs and recurrent neural networks was previously discovered and employed for deriving models for Bayesian computation in the cortex. These studies targeted the implementation of Bayesian filtering <xref ref-type="bibr" rid="pcbi.1003511-Rao1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Bobrowski1">[57]</xref>, capturing the forward message of the belief propagation algorithm in a rate-based neural code, or using a two-state HMM to capture the dynamics of single neurons <xref ref-type="bibr" rid="pcbi.1003511-Denve1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Boerlin1">[59]</xref>. In the present study we directly analyzed spikes produced by WTA circuits in terms of samples from the state space of a HMM. For the HMM this results in an arguably weaker form of inference than belief propagation, but led in a straightforward manner to an analysis of learning in the network.</p>
<p>The emergence of predictive population codes in recurrent networks through synaptic plasticity and their importance for sequence learning was previously suggested and experimentally verified <xref ref-type="bibr" rid="pcbi.1003511-Abbott1">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Rao2">[61]</xref>. In <xref ref-type="bibr" rid="pcbi.1003511-Denve2">[62]</xref> it was shown that spiking neurons can learn the parameters of a 2-state HMM using synaptic plasticity, thereby implementing an online EM algorithm <xref ref-type="bibr" rid="pcbi.1003511-Stiller1">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Mongillo1">[64]</xref>. In <xref ref-type="bibr" rid="pcbi.1003511-Rezende1">[14]</xref> learning of temporal models was implemented through a variational approximation, and revealed STDP-like learning rules. In <xref ref-type="bibr" rid="pcbi.1003511-Brea1">[15]</xref> it was shown that a network of neurons can learn to encode and reproduce a sequence of fixed spike times. The learning rules were derived using an importance sampling algorithm that yielded synaptic updates similar to the third-factor STDP rule presented here.</p>
<p>The crucial difference between <xref ref-type="bibr" rid="pcbi.1003511-Brea1">[15]</xref> and our approach is the usage of WTA circuits as building blocks for the recurrent network instead of individual neurons. Due to the possibility to use multiple WTAs our model has the freedom to factorize the multinomial HMM state space into smaller coupled variables, whereas <xref ref-type="bibr" rid="pcbi.1003511-Brea1">[15]</xref> always fully factorizes the state space down to single binary variables. However, under the assumption of linear neurons the state-transition probabilities in all these models are always represented by only <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e204" xlink:type="simple"/></inline-formula> recurrent synapses. Thus the expressive power of all these models (with the same number of neurons) should be more or less identical. The optimal factorization of the state space may strongly depend on the task. Our experiments suggest that the restriction on the number of possible activity patterns due to the usage of WTAs seems minor compared to the crucial advantage of their intrinsic stabilizing effects of the network's activity. To the best of our knowledge this stabilization is the reason why the pure forward sampling learning approach performed so well in our experiments.</p>
</sec><sec id="s3b">
<title>Contribution to a principled understanding of computation and plasticity in cortical microcircuits</title>
<p>The theoretical framework that we have introduced in this article provides a new and more principled understanding for the role of STDP in a generic cortical microcircuit motif (ensembles of pyramidal cells with lateral excitation and inhibition): Even in the absence of global signals related to reward, STDP installs in these microcircuit motifs an approximation to a HMM through forward sampling. The underlying theoretical analysis provides a new understanding of the role of spikes in such WTA circuits as samples from a (potentially very large) set of hidden states that enable generic cortical microcircuits to detect generic neural codes for afferent spike patterns that can reflect their temporal context and support predictions of future stimuli.</p>
<p>A remarkable feature of our model is that it postulates that noise in neural responses plays a very important role for the emergence of such “intelligent” temporal processing: We have shown that it provides in WTA circuits the basis for enabling probabilistic inference and learning through sampling, i.e. through an “embodiment” of probability distributions through neural activity. Thus stochasticity of neural responses provides an interesting alternative to models for probabilistic inference in biological neural systems through belief propagation (see <xref ref-type="bibr" rid="pcbi.1003511-Lochmann1">[65]</xref> for a review), i.e. through an emulation of an inherently deterministic calculation.</p>
<p>The rejection sampling algorithm that was proposed here as a method for emulating the full power of HMM learning requires in addition a mechanism that allows to replay input patterns multiple times. Such replay of complex spatiotemporal patterns is well documented in the hippocampus and was proposed as a mechanism for memory consolidation in the cortex <xref ref-type="bibr" rid="pcbi.1003511-Buhry1">[66]</xref>. This view is also supported by findings that showed that coordinated reactivation of temporal patterns can be observed in the cortex <xref ref-type="bibr" rid="pcbi.1003511-Ji1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Fujisawa1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Hoffman1">[67]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Peyrache1">[68]</xref>. In our framework, samples generated by the WTA circuit must be replayed several times until the network produces a spike train that provides a sequence of hidden states that gives satisfactory explanations and predictions for all segments of the sequence. The number of times a sequence is replayed is proportional to the prediction error accumulated over the sequence, which is a measure for the sample quality. Thus, sequences that are novel and to that end not well represented in the network should be replayed more often and thus, they get more attention in the learning process. This view is supported by experimental data that revealed that transient novel experiences are replayed more prominently than familiar stimuli <xref ref-type="bibr" rid="pcbi.1003511-Ribeiro1">[69]</xref>–<xref ref-type="bibr" rid="pcbi.1003511-Xu2">[71]</xref>.</p>
<p>Altogether our results show that hidden Markov models provide a promising theoretical framework for understanding the emergence of all-important capabilities of the brain to understand and predict hidden states of complex time-varying sensory stimuli.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Spiking network model</title>
<p>In this section we provide additional details to the derivations of the network model and its stochastic dynamics. For the sake of simplicity, throughout the theoretical analysis we use a simple EPSP kernel of the form<disp-formula id="pcbi.1003511.e205"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e205" xlink:type="simple"/><label>(12)</label></disp-formula>Thus, a kernel with a single exponential decay with time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e206" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e207" xlink:type="simple"/></inline-formula> determines the Heaviside step function which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e208" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e209" xlink:type="simple"/></inline-formula> and zero else.</p>
<p>The derivation provided here can be extended to more complex EPSP shapes, if two prerequisites are fulfilled. First, a suitable Markov state must be found that describes the dynamics of the EPSP kernel, i.e. a state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e210" xlink:type="simple"/></inline-formula> must exist for which we can write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e211" xlink:type="simple"/></inline-formula>. In fact, this property holds true for any deterministic function, although the required Markov state can be very complex. Second, the statistics of the EPSPs induced by the kernel must be readily described by an exponential family distribution. For this latter requirement the same considerations as for the afferent synapses apply, which have been addressed in <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Nessler2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss1">[33]</xref>. The simplest case for which these conditions are fulfilled is the one considered in the last experiment where rectangular EPSPs and constant inter-spike intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e212" xlink:type="simple"/></inline-formula> of the same length were used. In that case the network state collapses to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e213" xlink:type="simple"/></inline-formula>, which follows a multinomial distribution as considered in <xref ref-type="bibr" rid="pcbi.1003511-Nessler2">[32]</xref>.</p>
<sec id="s4a1">
<title>Details to <italic>Forward sampling in WTA circuits</italic></title>
<p>We show here that the WTA circuit correctly implements forward sampling in a HMM. In particular we show that a HMM with an observation model from the exponential family can be directly mapped to the network dynamics. Many of the theoretical details for the special case of stationary input patterns were analyzed in <xref ref-type="bibr" rid="pcbi.1003511-Nessler1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Nessler2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss1">[33]</xref>, here we focus on the derivations specific for the network with lateral excitatory connections.</p>
<p>First we define a HMM with observations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e214" xlink:type="simple"/></inline-formula> and hidden state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e215" xlink:type="simple"/></inline-formula> to reflect the dynamics of the WTA circuit. The HMM joint distribution is given by <xref ref-type="disp-formula" rid="pcbi.1003511.e054">equation (4)</xref>. Each time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e216" xlink:type="simple"/></inline-formula> factorizes into the <italic>observation model</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e217" xlink:type="simple"/></inline-formula> and the <italic>prediction model</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e218" xlink:type="simple"/></inline-formula>. We assume a mixture of exponential family distributions for the observation model. Many interesting distributions are members of this family, e.g. the Poisson or the Normal distribution. The network output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e219" xlink:type="simple"/></inline-formula> determines which mixture component is responsible for the observation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e220" xlink:type="simple"/></inline-formula>. In its generic form the likelihood of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e221" xlink:type="simple"/></inline-formula>-dimensional observations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e222" xlink:type="simple"/></inline-formula> given mixture component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e223" xlink:type="simple"/></inline-formula> can be written as<disp-formula id="pcbi.1003511.e224"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e224" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e225" xlink:type="simple"/></inline-formula> is a base measure and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e226" xlink:type="simple"/></inline-formula> is the log-partition function, which assure that (13) is correctly normalized. In this framework <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e227" xlink:type="simple"/></inline-formula> determines the sufficient statistics of the input distribution, e.g. the current input rate for the Poisson distribution, which is estimated by filtering the input spike train with the EPSP kernel. Since the input and output spike times are independent, given optimal WTA behavior, we exploit the conditional independence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e228" xlink:type="simple"/></inline-formula>. We assume that inputs are homogeneous, meaning that the sums over all input channels are constant. More precisely, we assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e229" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e230" xlink:type="simple"/></inline-formula> holds true at all times. These assumptions were never perfectly fulfilled in the simulations, but nevertheless the algorithm was robust against deviations from these constraints throughout all experiments. The choice of the log-partition function determines the member from the exponential family. The derivations here were done for Poisson distributed inputs but they equally apply to other members. Given the homogeneity assumption for the input we find the log-partition to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e231" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss1">[33]</xref>.</p>
<p>The prediction model has to reflect the dynamics of the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e232" xlink:type="simple"/></inline-formula>. At each time point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e233" xlink:type="simple"/></inline-formula> the spiking output projects the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e234" xlink:type="simple"/></inline-formula>-dimensional state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e235" xlink:type="simple"/></inline-formula> to the discrete value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e236" xlink:type="simple"/></inline-formula>, which is then projected in the next step to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e237" xlink:type="simple"/></inline-formula>. Using the independence properties, that emerge from these dynamics, the prediction model factorizes to<disp-formula id="pcbi.1003511.e238"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e238" xlink:type="simple"/><label>(14)</label></disp-formula>The last term determines the distribution over the inter-spike intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e239" xlink:type="simple"/></inline-formula>. Assuming Poisson distributed spike times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e240" xlink:type="simple"/></inline-formula> this is given by an exponential distribution with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e241" xlink:type="simple"/></inline-formula>, i.e.<disp-formula id="pcbi.1003511.e242"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e242" xlink:type="simple"/><label>(15)</label></disp-formula>The second part of (14) deterministically updates the EPSPs. Using the simple kernel function (12) the lateral EPSPs can be updated online<disp-formula id="pcbi.1003511.e243"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e243" xlink:type="simple"/><label>(16)</label></disp-formula>Since this is a deterministic function the probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e244" xlink:type="simple"/></inline-formula> in (14) collapses to a single mass point, where the update <xref ref-type="disp-formula" rid="pcbi.1003511.e243">equation (16)</xref> is fulfilled. The second and third parts of (14) project the spiking network output to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e245" xlink:type="simple"/></inline-formula>-dimensional space of the EPSP time courses (2). The first part of the prediction model projects it back to a discrete variable drawn from a multinomial distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e246" xlink:type="simple"/></inline-formula>. Using Bayes rule, we can decompose this into<disp-formula id="pcbi.1003511.e247"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e247" xlink:type="simple"/><label>(17)</label></disp-formula>The likelihood term can again be expressed in terms of an exponential family distribution<disp-formula id="pcbi.1003511.e248"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e248" xlink:type="simple"/><label>(18)</label></disp-formula>For each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e249" xlink:type="simple"/></inline-formula> the prior probability to fire is determined by the excitability parameter, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e250" xlink:type="simple"/></inline-formula>. In <xref ref-type="bibr" rid="pcbi.1003511-Nessler2">[32]</xref> a learning rule was presented for these network parameters, which equally applies to the framework presented here. For simplicity however, we can assume that all neurons have the same prior probability to fire, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e251" xlink:type="simple"/></inline-formula>.</p>
<p>Under the homogeneity condition and if the synaptic weights obey <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e252" xlink:type="simple"/></inline-formula>, the log-partition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e253" xlink:type="simple"/></inline-formula> becomes constant over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e254" xlink:type="simple"/></inline-formula>. It has been shown that this condition emerges automatically from the STDP rules (7) <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss1">[33]</xref>. Using this, the probability of generating a state sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e255" xlink:type="simple"/></inline-formula> using forward sampling can be directly linked to the network dynamics. The true posterior distribution (8) and the proposal distribution for forward sampling only differ in the normalization. Forward sampling is done, by explicitly normalizing the state update in (4) at each time point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e256" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003511-Koller1">[20]</xref>. This normalization is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e257" xlink:type="simple"/></inline-formula>, from which we find the proposal distribution to be given by<disp-formula id="pcbi.1003511.e258"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e258" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pcbi.1003511.e259"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e259" xlink:type="simple"/><label>(20)</label></disp-formula>This recovers the result of <xref ref-type="disp-formula" rid="pcbi.1003511.e103">equation (5)</xref>. The first term of the second line can be written using (13), (17) and (18)<disp-formula id="pcbi.1003511.e260"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e260" xlink:type="simple"/><label>(21)</label></disp-formula><disp-formula id="pcbi.1003511.e261"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e261" xlink:type="simple"/><label>(22)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e262" xlink:type="simple"/></inline-formula> given by (3). Here we have used that the marginal in the denominator of (17) does not depend on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e263" xlink:type="simple"/></inline-formula> and so do <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e264" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e265" xlink:type="simple"/></inline-formula> under the conditions described above. Therefore they cancel out through the normalization (3). Comparing this result with the neuron dynamics (6) and (5) it is easy to verify that the WTA circuit correctly realizes the HMM forward sampler (19).</p>
</sec><sec id="s4a2">
<title>Details to <italic>STDP installs a stochastic approximation to EM parameter learning</italic></title>
<p>In this section we derive the optimal updates for the model parameters in terms of the expectation-maximization (EM)-algorithm and show that the STDP rules (7) are stochastic approximations. The goal of the EM optimization is to minimize the error between the model likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e266" xlink:type="simple"/></inline-formula> and the empirical distribution over input sequences, which we denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e267" xlink:type="simple"/></inline-formula>. A natural way to express this error is the Kullback-Leibler divergence. Thus, the update can be derived by minimizing<disp-formula id="pcbi.1003511.e268"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e268" xlink:type="simple"/><label>(23)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e269" xlink:type="simple"/></inline-formula> is the entropy of the true input distribution and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e270" xlink:type="simple"/></inline-formula> denotes the expectation with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e271" xlink:type="simple"/></inline-formula>. We are interested in a solution to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e272" xlink:type="simple"/></inline-formula> that minimizes (23). Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e273" xlink:type="simple"/></inline-formula> is constant for a given input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e274" xlink:type="simple"/></inline-formula>, it can be ignored and minimizing (23) becomes equivalent to maximizing the expected log-likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e275" xlink:type="simple"/></inline-formula>. The derivative of the log-likelihood can be simplified to<disp-formula id="pcbi.1003511.e276"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e276" xlink:type="simple"/><label>(24)</label></disp-formula>The integral can again be written in terms of an expectation. The condition for the maximum likelihood becomes<disp-formula id="pcbi.1003511.e277"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e277" xlink:type="simple"/><label>(25)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e278" xlink:type="simple"/></inline-formula> denotes the expectation with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e279" xlink:type="simple"/></inline-formula>. The derivative in this last form can be easily calculated. By inserting the model joint distribution (4) it yields for the model parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e280" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e281" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003511.e282"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e282" xlink:type="simple"/><label>(26)</label></disp-formula>A similar result can be found for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e283" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e284" xlink:type="simple"/></inline-formula> is the Kronecker delta, which is one if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e285" xlink:type="simple"/></inline-formula> and zero otherwise. Inserting this result into (25), setting the derivative to zero and rearranging the terms, we identify the optimal model parameters<disp-formula id="pcbi.1003511.e286"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e286" xlink:type="simple"/><label>(27)</label></disp-formula>In the E-step the expectations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e287" xlink:type="simple"/></inline-formula> are evaluated. Note that the expectations are taken over the whole sequence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e288" xlink:type="simple"/></inline-formula> output spikes. In the M-step the parameters are updated to their new values. An estimate of these expectations is computed by the network generating output spike sequences. A local minimum of (23) can be found by iteratively evaluating the E- and M-step.</p>
<p>We will now show that the STDP protocol introduced here converges stochastically to the same result as the EM updates (27). We will derive this results for the lateral weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e289" xlink:type="simple"/></inline-formula> only, since adaption for other parameters is straightforward. Including the reward mechanism, the weight update consists of two stochastic processes: the forward sampling and the stochastic decision for the rejection step (11). The updates are made for each output spike of the network and therefore they will always fluctuate. In our analysis we are interested in the equilibrium point of these fluctuations for some given target distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e290" xlink:type="simple"/></inline-formula>. This can be expressed for the expected weight update using (7),(9) and (11), for which we get<disp-formula id="pcbi.1003511.e291"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e291" xlink:type="simple"/><label>(28)</label></disp-formula>which by inserting <xref ref-type="disp-formula" rid="pcbi.1003511.e186">equation (10)</xref> yields<disp-formula id="pcbi.1003511.e292"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e292" xlink:type="simple"/></disp-formula>which is equivalent to the solution of the EM-algorithm (27).</p>
</sec><sec id="s4a3">
<title>Details to <italic>A refined EM approximation using rejection sampling</italic></title>
<p>Here we present additional details to the rejection sampling algorithm that was used throughout the numerical experiments. The algorithm requires to evaluate two quantities that evolve on different time scales. The synaptic weight updates need to be updated on each spike, whereas the importance weights (10) need to be tracked over a whole input sequence.</p>
<p>The importance weight over sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e293" xlink:type="simple"/></inline-formula> is given by (10) which can be verified by inserting <xref ref-type="disp-formula" rid="pcbi.1003511.e258">equation (19)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003511.e054">(4)</xref> into <xref ref-type="disp-formula" rid="pcbi.1003511.e186">(10)</xref>. Using (3) we find that this quantity computes to<disp-formula id="pcbi.1003511.e294"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e294" xlink:type="simple"/><label>(29)</label></disp-formula>where the marginals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e295" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e296" xlink:type="simple"/></inline-formula> are given by<disp-formula id="pcbi.1003511.e297"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e297" xlink:type="simple"/><label>(30)</label></disp-formula>The term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e298" xlink:type="simple"/></inline-formula> is arbitrary since it cancels out in the rejection sampling algorithm, but we found that (30) achieves better performance including the dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e299" xlink:type="simple"/></inline-formula>, when using the the rejection sampler with the simple tracking mechanism for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e300" xlink:type="simple"/></inline-formula>. The performance of the rejection sampling algorithm essentially depends on the variance of the importance weights. The lower this variance is, the more generated sequences will be accepted. Since the importance weights are only needed to compare the quality of different proposed hidden state trajectories, all fluctuations that depend on the feedforward weights and inputs only, can be discarded. Explicitly subtracting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e301" xlink:type="simple"/></inline-formula> allows to minimize the fluctuations injected by the feedforward synapses. This modification had a large impact on reducing the number of rejected trajectories and therefore increased learning speed.</p>
<p>The likelihood of an input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e302" xlink:type="simple"/></inline-formula> can be approximated using a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e303" xlink:type="simple"/></inline-formula> paths <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e304" xlink:type="simple"/></inline-formula> sampled from (5) given by<disp-formula id="pcbi.1003511.e305"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e305" xlink:type="simple"/><label>(31)</label></disp-formula>In the simplest approximation the expectation can be taken over a single path. Thus, we find that the sequence log-likelihood can be directly approximated by (29), i.e.<disp-formula id="pcbi.1003511.e306"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e306" xlink:type="simple"/><label>(32)</label></disp-formula>In the AGL experiments this simple approximate likelihood was used to distinguish between grammatical and non-grammatical sequences.</p>
<p>The probability of generating a sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e307" xlink:type="simple"/></inline-formula> through the state space is given by the proposal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e308" xlink:type="simple"/></inline-formula>, which was defined in <xref ref-type="disp-formula" rid="pcbi.1003511.e103">equation (5)</xref>. The bias between this and the model distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e309" xlink:type="simple"/></inline-formula>, is given by the importance weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e310" xlink:type="simple"/></inline-formula>, which we have derived earlier (10). This bias can be eliminated using rejection sampling, i.e. accepting the sampled sequences based on a stochastic decision proportional to the importance weight. In the neural network we implemented this using an eligibility trace of synaptic weight changes <xref ref-type="bibr" rid="pcbi.1003511-Izhikevich1">[47]</xref>. The weight updates were accumulated over the whole input sequence<disp-formula id="pcbi.1003511.e311"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e311" xlink:type="simple"/><label>(33)</label></disp-formula>The synaptic weights can be learned by modulating the learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e312" xlink:type="simple"/></inline-formula> when incorporating the synaptic weight changes (33) at the end of a sequence. The learning rate must be modulated according to the importance weights. In the simulations we used a stochastic binary decision, whether to accept or reject the sampled sequence<disp-formula id="pcbi.1003511.e313"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e313" xlink:type="simple"/><label>(34)</label></disp-formula>where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e314" xlink:type="simple"/></inline-formula> is a constant learning rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e315" xlink:type="simple"/></inline-formula> is a constant that scales the average number of rejected samples. The probability of accepting a path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e316" xlink:type="simple"/></inline-formula> is directly proportional to the importance weights. Using this, we immediately find that the mean number of rejected samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e317" xlink:type="simple"/></inline-formula> for an input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e318" xlink:type="simple"/></inline-formula> is inversely proportional to the sequence likelihood, i.e.<disp-formula id="pcbi.1003511.e319"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e319" xlink:type="simple"/><label>(35)</label></disp-formula>The average learning rate assigned to a sampled sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e320" xlink:type="simple"/></inline-formula> depends on the probability of sampling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e321" xlink:type="simple"/></inline-formula> from the proposal distribution and the number of times the sequence is resampled. Using this, (35) and (34) we find the expected learning rate associated with a state sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e322" xlink:type="simple"/></inline-formula> to be given by<disp-formula id="pcbi.1003511.e323"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e323" xlink:type="simple"/><label>(36)</label></disp-formula></p>
<p>The constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e324" xlink:type="simple"/></inline-formula> in (34) can be used to control the average number of rejected samples. We used a simple linear tracking algorithm for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e325" xlink:type="simple"/></inline-formula> in the logarithmic domain. Whenever a path was accepted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e326" xlink:type="simple"/></inline-formula> was decreased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e327" xlink:type="simple"/></inline-formula>, if the path was rejected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e328" xlink:type="simple"/></inline-formula> was increased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e329" xlink:type="simple"/></inline-formula>. As learning proceeds the network converges to an equilibrium acceptance rate, determined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e330" xlink:type="simple"/></inline-formula>. Throughout the experiments this parameter was tuned to achieve the desired mean number of samples over the whole training session. A quantitative comparison between the learning performances achieved with the batch algorithm and this tracking mechanism, is given in <xref ref-type="fig" rid="pcbi-1003511-g008">Fig. 8</xref>.</p>
<p>In the batch version of the algorithm a set of sampled paths with a fixed size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e331" xlink:type="simple"/></inline-formula> was used to compute <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e332" xlink:type="simple"/></inline-formula> directly, which was chosen such that the distribution over the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e333" xlink:type="simple"/></inline-formula> paths was correctly normalized. Using (34) we find this to be fulfilled for<disp-formula id="pcbi.1003511.e334"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e334" xlink:type="simple"/><label>(37)</label></disp-formula>A sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e335" xlink:type="simple"/></inline-formula> was then chosen at random from the set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e336" xlink:type="simple"/></inline-formula> sampled sequences. The importance sampler was realized by directly weighting the synaptic changes by the scalar value of the normalized importance weight, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e337" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s4b">
<title>Simulations and data analysis</title>
<p>All simulations were done in Matlab (Mathworks), directly implementing the derived equations without discrete time approximations. The population output rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e338" xlink:type="simple"/></inline-formula> was tuned to give an average output rate of 5–20 Hz per neuron. Prior to learning all weights were set to small equally distributed random values. The weight updates were incorporated using a constant learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e339" xlink:type="simple"/></inline-formula>.</p>
<p>Other than in the theoretical analysis where synaptic delays were neglected for the sake of simplicity, we used synaptic delays of 5 ms for the lateral excitatory synapses in the numerical experiments. We also used a more realistic double exponential EPSP kernel of the form <xref ref-type="bibr" rid="pcbi.1003511-Gerstner1">[28]</xref><disp-formula id="pcbi.1003511.e340"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e340" xlink:type="simple"/><label>(38)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e341" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e342" xlink:type="simple"/></inline-formula> are the time constants of the falling and rising edges of the EPSP kernel, respectively. The above theoretical analysis applies equally to this kernel, but would be slightly more complex since each of the two exponential decay terms comprises a piece of memory which has to be reflected in the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e343" xlink:type="simple"/></inline-formula>.</p>
<p>The diagonal of the weight matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e344" xlink:type="simple"/></inline-formula> was set to zero and these weights were excluded from learning. Instead, a refractory mechanism was used with a kernel given by <xref ref-type="bibr" rid="pcbi.1003511-Gerstner1">[28]</xref><disp-formula id="pcbi.1003511.e345"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e345" xlink:type="simple"/><label>(39)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e346" xlink:type="simple"/></inline-formula> is the maximum amplitude of the refractory kernel, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e347" xlink:type="simple"/></inline-formula> is the refractory time constant and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e348" xlink:type="simple"/></inline-formula> is the time elapsed since the last output spike. <xref ref-type="disp-formula" rid="pcbi.1003511.e345">Equation (39)</xref> was subtracted from the membrane potential (1).</p>
<sec id="s4b1">
<title>Details to <italic>Learning to predict spike sequences through STDP</italic></title>
<p>The input patterns were generated by drawing for each afferent neuron and each pattern a value from the Beta distribution with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e349" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e350" xlink:type="simple"/></inline-formula> and multiplying this value with the maximum rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e351" xlink:type="simple"/></inline-formula>. Using these rate patterns, input spikes were then generated by creating independent spike events from a Poisson process.</p>
<p>To facilitate the interpretability of the network output, we applied a smoothing and sorting algorithm. The spike statistics were estimated using the perievent time histogram (PETH) on the network output <xref ref-type="bibr" rid="pcbi.1003511-Luczak1">[6]</xref>. The network output rates were computed for time bins of 1 ms and then filtered with a Gaussian filter function (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e352" xlink:type="simple"/></inline-formula>) to give the smoothed single-trial estimated rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e353" xlink:type="simple"/></inline-formula>. These spike histograms were averaged over 100 trial runs to give the time estimated rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e354" xlink:type="simple"/></inline-formula> for each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e355" xlink:type="simple"/></inline-formula>. For neuron sorting we evaluated the point in time with the highest activity<disp-formula id="pcbi.1003511.e356"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e356" xlink:type="simple"/><label>(40)</label></disp-formula>This was used as criterion to determine the rank index of the output neurons for sorting. The PETHs for all sequences of a learning problem were concatenated before evaluating the maximum firing time (40) to ensure a visual separation between neurons that fired preferentially during one specific sequence. This neuron order was also used to sort the rows and columns of the synaptic weight matrix shown in <xref ref-type="fig" rid="pcbi-1003511-g002">Fig. 2</xref> (neurons that fired on average less than one spike per sequence were excluded from this plot).</p>
<p>To quantify the similarity between spontaneous and evoked network activity we used Spearman's rank correlation <xref ref-type="bibr" rid="pcbi.1003511-Luczak1">[6]</xref>. The correlations were computed by evaluating the rank correlation between the PETH computed on a single spontaneous sequence and the evoked activity averaged over 100 trial runs. For the evaluation only the neurons that produced at least one spike during the spontaneous run were used. The firing rates in <xref ref-type="fig" rid="pcbi-1003511-g004">Fig. 4A,B</xref> were estimated over 100 input sequences. Only the time window during which pattern <italic>a</italic> was present on the input was analyzed. Neurons that fired with average rates less than 10 Hz during these time windows were excluded from the analysis. Neurons with rates above 10 Hz for patterns appearing in one sequence, but not the other, were classified as context specific. Those that fired rates above 10 Hz during both sequences were classified as context unspecific.</p>
</sec><sec id="s4b2">
<title>Details to <italic>Mixed selectivity emerges in multiple interconnected WTA circuits</italic></title>
<p>Here a linear classifier was used to identify separating planes in the network activity. We trained a soft-margin support vector machine with linear kernels <xref ref-type="bibr" rid="pcbi.1003511-Bishop1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Cortes1">[72]</xref>, <xref ref-type="bibr" rid="pcbi.1003511-Schlkopf1">[73]</xref> to classify the network activity during the delay phase of sequence <italic>AB-delay-ab</italic> against that of <italic>BA-delay-ba</italic>. The resulting linear models were used to classify 50 test samples from each of the two sequences. Sequences that were at any point in time on the wrong side of the separating plane were reported as wrongly classified. The mean classification rates over these test samples were reported.</p>
<p>To illustrate the network state during the holding phase we used the dynamic PCA (jPCA) method in experiment 2. This method was recently introduced as an extension to normal PCA, with better applicability to dynamical data <xref ref-type="bibr" rid="pcbi.1003511-Churchland1">[39]</xref>. We applied this method on the smoothed network activities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e357" xlink:type="simple"/></inline-formula> of all network neurons. The jPCA identifies the plane that is aligned with the fastest rotation in the data set. Briefly, the jPCA first uses a preprocessing step in which normal PCA is performed on the data to reduce the dimensionality. We used the first 6 PCA components as suggested in <xref ref-type="bibr" rid="pcbi.1003511-Churchland1">[39]</xref>. Subsequently a projection from the neural state to its slope is found. A skew-symmetric matrix is constructed that projects the PCA components into its first order derivatives. The solution to this constraint optimization problem is a matrix defining the best-fitting rotational linear dynamical system which can describe the data set (see the supplementary derivation of <xref ref-type="bibr" rid="pcbi.1003511-Churchland1">[39]</xref> for details). The orthogonal basis of the jPCA is then given by the real plane associated with the eigenvectors with largest imaginary eigenvalues of this projection matrix. This plane is aligned with the fastest rotation in the data set.</p>
</sec><sec id="s4b3">
<title>Details to <italic>Trajectories in network assemblies emerge for stationary input patterns</italic></title>
<p>In this experiment we employed homeostatic mechanism to control the excitabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e358" xlink:type="simple"/></inline-formula>. A detailed derivation of this intrinsic plasticity was presented in <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss2">[44]</xref>. Following this approach we slowly regulated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e359" xlink:type="simple"/></inline-formula> over time to maximizes the entropy of the network output by demanding that the overall output rate of each neuron, measured over a long time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e360" xlink:type="simple"/></inline-formula>, converges to the target rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e361" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e362" xlink:type="simple"/></inline-formula> for each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e363" xlink:type="simple"/></inline-formula>. A stochastic approximation to that can be achieved by updating the excitabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e364" xlink:type="simple"/></inline-formula> in (1)<disp-formula id="pcbi.1003511.e365"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003511.e365" xlink:type="simple"/><label>(41)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e366" xlink:type="simple"/></inline-formula> is an update rate we have chosen to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e367" xlink:type="simple"/></inline-formula> in this experiment. This mechanism assures that all network neurons participate on average equally in the representation of the hidden state. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e368" xlink:type="simple"/></inline-formula> is chosen small enough this method assures that all network neurons participate equally in the representation of the hidden state <xref ref-type="bibr" rid="pcbi.1003511-Habenschuss2">[44]</xref>.</p>
</sec><sec id="s4b4">
<title>Details to <italic>Learning the temporal structure of an artificial grammar model</italic></title>
<p>Here we used two data sets from <xref ref-type="bibr" rid="pcbi.1003511-Conway1">[27]</xref> - the 12 sequences reported there in appendix A for training and the 20 sequences from table 1 for testing. In each training iteration we randomly drew one example input sequence from the train set. For testing we created 100 legal and illegal sequences randomly drawn from the test set. The sequences were encoded using sparse input patterns, encoded by 10 input neurons, two of which fired with a rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e369" xlink:type="simple"/></inline-formula> for 50 ms for each of the five input pattern, while the others remained silent. All spike patterns were not kept fixed but generated newly at each occurrence of the pattern and also during replay for rejection sampling. In the AGL experiments, the initial network state was reset to zero <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e370" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e371" xlink:type="simple"/></inline-formula> before a new input sequence was presented. To classify grammatical against non-grammatical sequences the one-sample approximation of the log-likelihood (32) was computed for all test sequences. A threshold was computed by taking the mean of these log-likelihoods. Sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e372" xlink:type="simple"/></inline-formula> for which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e373" xlink:type="simple"/></inline-formula> lied above this threshold were classified as grammatical, all others as non-grammatical.</p>
</sec><sec id="s4b5">
<title>Details to <italic>Comparison of the convergence speed and performance of the approximate algorithms</italic></title>
<p>In this experiment, random teacher HMMs were generated by drawing initial state, observation and transition probability tables from a Beta distribution with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e374" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e375" xlink:type="simple"/></inline-formula> and then normalizing the tables to proper conditional probabilities. The models had <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e376" xlink:type="simple"/></inline-formula> states and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e377" xlink:type="simple"/></inline-formula> discrete observations. These models were then used to generate observation sequences. We drew a training set of 200 and a validation set of 2000 sequences of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e378" xlink:type="simple"/></inline-formula>. The complete training data set was repeatedly present to the network. We refer to the presentation of the whole batch of training sequences as an epoch. The weight updates for the WTA circuit were applied at the end of each training sequence. In each epoch all sequences were presented in random order. For the Baum-Welch algorithm (which is not an online algorithm) the updates were computed over all sequences in each epoch (batch learning). We generated 50 trials using 50 different teacher HMMs. The performance of rejection sampling was assessed for the two algorithms to evaluate the normalizing constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003511.e379" xlink:type="simple"/></inline-formula> – the exact version (37) and the linear tracking algorithm.</p>
</sec></sec></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003511-Berger1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berger</surname><given-names>TK</given-names></name>, <name name-style="western"><surname>Perin</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Silberberg</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Frequency-dependent disynaptic inhibition in the pyramidal network - a ubiquitous pathway in the developing rat neocortex</article-title>. <source>The Journal of Neurophysiology</source> <volume>587</volume>: <fpage>5411</fpage>–<lpage>5425</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Okun1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lampl</surname><given-names>I</given-names></name> (<year>2008</year>) <article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title>. <source>Nature Neuroscience</source> <volume>11</volume>: <fpage>535</fpage>–<lpage>537</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Avermann1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Avermann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Tomm</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Mateo</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Petersen</surname><given-names>CC</given-names></name> (<year>2012</year>) <article-title>Microcircuits of excitatory and inhibitory neurons in layer 2/3 of mouse barrel cortex</article-title>. <source>Journal of neurophysiology</source> <volume>107</volume>: <fpage>3116</fpage>–<lpage>3134</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Douglas1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name> (<year>2004</year>) <article-title>Neuronal circuits of the neocortex</article-title>. <source>Annu Rev Neurosci</source> <volume>27</volume>: <fpage>419</fpage>–<lpage>451</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Han1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Han</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Caporale</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2008</year>) <article-title>Reverberation of recent visual experience in spontaneous cortical waves</article-title>. <source>Neuron</source> <volume>60</volume>: <fpage>321</fpage>–<lpage>327</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Luczak1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Barthó</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2009</year>) <article-title>Spontaneous events outline the realm of possible sensory responses in neocortical populations</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>413</fpage>–<lpage>425</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Luczak2"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Barthó</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2007</year>) <article-title>Sequential structure of neocortical spontaneous activity in vivo</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>104</volume>: <fpage>347</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Ji1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ji</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title>. <source>Nature Neuroscience</source> <volume>10</volume>: <fpage>100</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Fujisawa1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fujisawa</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Amarasingham</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex</article-title>. <source>Nature Neuroscience</source> <volume>11</volume>: <fpage>823</fpage>–<lpage>833</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Harvey1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harvey</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Coen</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name> (<year>2012</year>) <article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title>. <source>Nature</source> <volume>484</volume>: <fpage>62</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Caporale1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caporale</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2008</year>) <article-title>Spike timing-dependent plasticity: a Hebbian learning rule</article-title>. <source>Annu Rev Neuroscience</source> <volume>31</volume>: <fpage>25</fpage>–<lpage>46</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Markram1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name> (<year>2011</year>) <article-title>A history of spike-timing-dependent plasticity</article-title>. <source>Frontiers in Synaptic Neuroscience</source> <volume>3</volume>: <fpage>1</fpage>–<lpage>4</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Nessler1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2013</year>) <article-title>Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>: <fpage>e1003037</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Rezende1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rezende</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Wierstra</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Variational learning for recurrent spiking networks</article-title>. <source>Proceedings of NIPS, Advances in Neural Information Processing Systems</source> <volume>24</volume>: <fpage>136</fpage>–<lpage>144</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Brea1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brea</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name> (<year>2011</year>) <article-title>Sequence learning with hidden units in spiking neural networks</article-title>. <source>Proceedings of NIPS, Advances in Neural Information Processing Systems</source> <fpage>1422</fpage>–<lpage>1430</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Shi1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shi</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name> (<year>2009</year>) <article-title>Neural implementation of hierarchical Bayesian inference by importance sampling</article-title>. <source>Proceedings of NIPS, Advances in Neural Information Processing Systems</source> <volume>22</volume>: <fpage>1669</fpage>–<lpage>1677</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Rabiner1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabiner</surname><given-names>LR</given-names></name> (<year>1989</year>) <article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title>. <source>Proceedings of the IEEE</source> <volume>77</volume>: <fpage>257</fpage>–<lpage>286</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Murty1"><label>18</label>
<mixed-citation publication-type="book" xlink:type="simple">Murty MN, Devi VS (2011) Hidden Markov Models. Springer.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Bishop1"><label>19</label>
<mixed-citation publication-type="book" xlink:type="simple">Bishop CM (2006) Pattern Recognition and Machine Learning. New York: Springer.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Koller1"><label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">Koller D, Friedman N (2009) Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning). MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Celeux1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Celeux</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Diebolt</surname><given-names>J</given-names></name> (<year>1985</year>) <article-title>The SEM algorithm: a probabilistic teacher algorithm derived from the EM algorithm for the mixture problem</article-title>. <source>Computational Statistics Quarterly</source> <volume>2</volume>: <fpage>73</fpage>–<lpage>82</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Neal1"><label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">Neal RM, Hinton GE (1998) A view of the EM algorithm that justifies incremental sparse, and other variants. In: Learning in Graphical Models, Kluwer Academic Press. pp. 355–368.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Berdyyeva1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berdyyeva</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Olson</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Monkey supplementary eye field neurons signal the ordinal position of both actions and objects</article-title>. <source>The Journal of Neuroscience</source> <volume>29</volume>: <fpage>591</fpage>–<lpage>599</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Warden1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Warden</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name> (<year>2010</year>) <article-title>Task-dependent changes in short-term memory in the prefrontal cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>15801</fpage>–<lpage>15810</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Rigotti1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigotti</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature (advance online publication)</source> <fpage>1476</fpage>–<lpage>4687</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Xu1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Takagaki</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Compression and reection of visually evoked cortical waves</article-title>. <source>Neuron</source> <volume>55</volume>: <fpage>119</fpage>–<lpage>129</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Conway1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Conway</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Christiansen</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Modality-constrained statistical learning of tactile, visual, and auditory sequences</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>31</volume>: <fpage>24</fpage>–<lpage>39</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Gerstner1"><label>28</label>
<mixed-citation publication-type="book" xlink:type="simple">Gerstner W, Kistler WM (2002) Spiking Neuron Models. Cambridge: Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Jolivet1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jolivet</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rauch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lüscher</surname><given-names>H</given-names></name> (<year>2006</year>) <collab xlink:type="simple">Gerstner W </collab> (<year>2006</year>) <article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models</article-title>. <source>Journal of Computational Neuroscience</source> <volume>21</volume>: <fpage>35</fpage>–<lpage>49</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Dempster1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dempster</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>DB</given-names></name> (<year>1977</year>) <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source> <volume>39</volume>: <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Baum1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baum</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Petrie</surname><given-names>T</given-names></name> (<year>1966</year>) <article-title>Statistical inference for probabilistic functions of finite state Markov chains</article-title>. <source>The Annals of Mathematical Statistics</source> <volume>37</volume>: <fpage>1554</fpage>–<lpage>1563</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Nessler2"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>STDP enables spiking neurons to detect hidden causes of their inputs</article-title>. <source>Proceedings of NIPS, Advances in Neural Information Processing Systems</source> <volume>22</volume>: <fpage>1357</fpage>–<lpage>1365</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Habenschuss1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Habenschuss</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Puhr</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2013</year>) <article-title>Emergence of optimal decoding of population codes through STDP</article-title>. <source>Neural Computation</source> <volume>25</volume>: <fpage>1</fpage>–<lpage>37</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Shima1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shima</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name> (<year>2000</year>) <article-title>Neuronal activity in the supplementary and presupplementary motor areas for temporal organization of multiple movements</article-title>. <source>Journal of Neurophysiology</source> <volume>84</volume>: <fpage>2148</fpage>–<lpage>2160</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Isoda1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isoda</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>Contrasting neuronal activity in the supplementary and frontal eye fields during temporal organization of multiple saccades</article-title>. <source>Journal of Neurophysiology</source> <volume>90</volume>: <fpage>3054</fpage>–<lpage>3065</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Barone1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barone</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Joseph</surname><given-names>J</given-names></name> (<year>1989</year>) <article-title>Prefrontal cortex and spatial sequencing in macaque monkey</article-title>. <source>Experimental Brain Research</source> <volume>78</volume>: <fpage>447</fpage>–<lpage>464</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Shima2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shima</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Isoda</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mushiake</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name> (<year>2006</year>) <article-title>Categorization of behavioural sequences in the prefrontal cortex</article-title>. <source>Nature</source> <volume>445</volume>: <fpage>315</fpage>–<lpage>318</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Brand1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Brand M (1997) Coupled hidden Markov models for modeling interacting processes. Technical report, MIT Media Lab.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Churchland1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Churchland</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Cunningham</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kaufman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Foster</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source> <volume>487</volume>: <fpage>51</fpage>–<lpage>56</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Jin1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jin</surname><given-names>DZ</given-names></name>, <name name-style="western"><surname>Fujii</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Graybiel</surname><given-names>AM</given-names></name> (<year>2009</year>) <article-title>Neural representation of time in cortico-basal ganglia circuits</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>19156</fpage>–<lpage>19161</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Fiete1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>CZ</given-names></name>, <name name-style="western"><surname>Hahnloser</surname><given-names>RH</given-names></name> (<year>2010</year>) <article-title>Spike-time-dependent plasticity and heterosynaptic competition organize networks to produce long scale-free sequences of neural activity</article-title>. <source>Neuron</source> <volume>65</volume>: <fpage>563</fpage>–<lpage>576</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Kozhevnikov1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name> (<year>2007</year>) <article-title>Singing-related activity of identified HVC neurons in the zebra finch</article-title>. <source>Journal of neurophysiology</source> <volume>97</volume>: <fpage>4271</fpage>–<lpage>4283</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Hahnloser1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hahnloser</surname><given-names>RH</given-names></name>, <name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name> (<year>2002</year>) <article-title>An ultra-sparse code underliesthe generation of neural sequences in a songbird</article-title>. <source>Nature</source> <volume>419</volume>: <fpage>65</fpage>–<lpage>70</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Habenschuss2"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Habenschuss</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Bill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name> (<year>2012</year>) <article-title>Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</article-title>. <source>Proceedings of NIPS, Advances in Neural Information Processing Systems</source> <volume>25</volume>: <fpage>782</fpage>–<lpage>790</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Reber1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reber</surname><given-names>AS</given-names></name> (<year>1967</year>) <article-title>Implicit learning of artificial grammars</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source> <volume>6</volume>: <fpage>855</fpage>–<lpage>863</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Neal2"><label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Neal RM (1993) Probabilistic inference using Markov chain monte carlo methods. Technical report, University of Toronto Department of Computer Science.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Izhikevich1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name> (<year>2007</year>) <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling</article-title>. <source>Cerebral Cortex</source> <volume>17</volume>: <fpage>2443</fpage>–<lpage>2452</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Keck1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keck</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Savin</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Lücke</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Feedforward inhibition and synaptic scaling – two sides of the same coin?</article-title> <source>PLoS Computational Biology</source> <volume>8</volume>: <fpage>e1002432</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Haider1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haider</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Duque</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hasenstaub</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>McCormick</surname><given-names>DA</given-names></name> (<year>2006</year>) <article-title>Neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition</article-title>. <source>The Journal of neuroscience</source> <volume>26</volume>: <fpage>4535</fpage>–<lpage>4545</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Klampfl1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klampfl</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2013</year>) <article-title>Emergence of dynamic memory traces in cortical microcircuit models through STDP</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>: <fpage>11515</fpage>–<lpage>11529</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Elman1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elman</surname><given-names>JL</given-names></name> (<year>1990</year>) <article-title>Finding structure in time</article-title>. <source>Cognitive Science</source> <volume>14</volume>: <fpage>179</fpage>–<lpage>211</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Jordan1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name> (<year>1997</year>) <article-title>Serial order: A parallel distributed processing approach</article-title>. <source>Advances in Psychology</source> <volume>121</volume>: <fpage>471</fpage>–<lpage>495</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Cleeremans1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cleeremans</surname><given-names>A</given-names></name>, <name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name> (<year>1991</year>) <article-title>Learning the structure of event sequences</article-title>. <source>Memories, Thoughts, and Emotions: Essays in Honor of George Mandler</source> <volume>120</volume>: <fpage>235</fpage>–<lpage>253</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Boucher1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boucher</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2003</year>) <article-title>Two ways of learning associations</article-title>. <source>Cognitive Science</source> <volume>27</volume>: <fpage>807</fpage>–<lpage>842</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Pothos1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pothos</surname><given-names>EM</given-names></name> (<year>2007</year>) <article-title>Theories of artificial grammar learning</article-title>. <source>Psychological Bulletin</source> <volume>133</volume>: <fpage>227</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Rao1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name> (<year>2004</year>) <article-title>Bayesian computation in recurrent neural circuits</article-title>. <source>Neural Computation</source> <volume>16</volume>: <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Bobrowski1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bobrowski</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Meir</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Eldar</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Bayesian filtering in spiking neural networks: Noise, adaptation, and multisensory integration</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>1277</fpage>–<lpage>1320</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Denve1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Denève</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Bayesian spiking neurons I: inference</article-title>. <source>Neural Computation</source> <volume>20</volume>: <fpage>91</fpage>–<lpage>117</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Boerlin1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boerlin</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Denève</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Spike-based population coding and working memory</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1001080</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Abbott1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Blum</surname><given-names>K</given-names></name> (<year>1996</year>) <article-title>Functional significance of long-term potentiation for sequence learning and prediction</article-title>. <source>Cerebral Cortex</source> <volume>6</volume>: <fpage>406</fpage>–<lpage>416</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Rao2"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name> (<year>2001</year>) <article-title>Spike-timing-dependent Hebbian plasticity as temporal difference learning</article-title>. <source>Neural Computation</source> <volume>13</volume>: <fpage>2221</fpage>–<lpage>2237</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Denve2"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Denève</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Bayesian spiking neurons II: learning</article-title>. <source>Neural Computation</source> <volume>20</volume>: <fpage>118</fpage>–<lpage>145</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Stiller1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stiller</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Radons</surname><given-names>G</given-names></name> (<year>1999</year>) <article-title>Online estimation of hidden Markov models</article-title>. <source>Signal Processing Letters, IEEE</source> <volume>6</volume>: <fpage>213</fpage>–<lpage>215</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Mongillo1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mongillo</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Denève</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Online learning with hidden Markov models</article-title>. <source>Neural Computation</source> <volume>20</volume>: <fpage>1706</fpage>–<lpage>1716</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Lochmann1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lochmann</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Denève</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Neural processing as causal inference</article-title>. <source>Current Opinion in Neurobiology</source> <volume>21</volume>: <fpage>774</fpage>–<lpage>781</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Buhry1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buhry</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Azizi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Cheng</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Reactivation, replay, and preplay: How it might all fit together</article-title>. <source>Neural Plasticity</source> <volume>2011</volume>: <fpage>203462</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Hoffman1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoffman</surname><given-names>K</given-names></name>, <name name-style="western"><surname>McNaughton</surname><given-names>B</given-names></name> (<year>2002</year>) <article-title>Coordinated reactivation of distributed memory traces in primate neocortex</article-title>. <source>Science</source> <volume>297</volume>: <fpage>2070</fpage>–<lpage>2073</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Peyrache1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peyrache</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Khamassi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Benchenane</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wiener</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Battaglia</surname><given-names>F</given-names></name> (<year>2009</year>) <article-title>Replay of rule-learning related neural patterns in the prefrontal cortex during sleep</article-title>. <source>Nature Neuroscience</source> <volume>12</volume>: <fpage>919</fpage>–<lpage>926</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Ribeiro1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ribeiro</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Gervasoni</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Soares</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Long-lasting novelty-induced neuronal reverberation during slow-wave sleep in multiple forebrain areas</article-title>. <source>PLoS Biology</source> <volume>2</volume>: <fpage>e24</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Cheng1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cheng</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>LM</given-names></name> (<year>2008</year>) <article-title>New experiences enhance coordinated neural activity in the hippocampus</article-title>. <source>Neuron</source> <volume>57</volume>: <fpage>303</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Xu2"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2012</year>) <article-title>Activity recall in a visual cortical ensemble</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>449</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Cortes1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cortes</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name> (<year>1995</year>) <article-title>Support-vector networks</article-title>. <source>Machine Learning</source> <volume>20</volume>: <fpage>273</fpage>–<lpage>297</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Schlkopf1"><label>73</label>
<mixed-citation publication-type="book" xlink:type="simple">Schölkopf B, Burges CJ, Smola AJ (1999) Advances in kernel methods: support vector learning. The MIT press.</mixed-citation>
</ref>
<ref id="pcbi.1003511-Gomez1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gomez</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gerken</surname><given-names>L</given-names></name> (<year>1999</year>) <article-title>Artificial grammar learning by 1-year-olds leads to specific and abstract knowledge</article-title>. <source>Cognition</source> <volume>70</volume>: <fpage>109</fpage>–<lpage>135</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>