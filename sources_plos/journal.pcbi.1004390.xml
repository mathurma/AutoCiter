<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01081</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004390</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex</article-title>
<alt-title alt-title-type="running-head">Invariance and Domain Specificity</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Leibo</surname> <given-names>Joel Z.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Liao</surname> <given-names>Qianli</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Anselmi</surname> <given-names>Fabio</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Poggio</surname> <given-names>Tomaso</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Center for Brains, Minds, and Machines, MIT, Cambridge, Massachusetts, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>McGovern Institute for Brain Research, MIT, Cambridge, Massachusetts, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Istituto Italiano di Tecnologia, Genova, Italy</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Kriegeskorte</surname> <given-names>Nikolaus</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Medical Research Council, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: JZL QL FA TP. Performed the experiments: JZL QL. Analyzed the data: JZL QL. Contributed reagents/materials/analysis tools: JZL QL FA TP. Wrote the paper: JZL QL FA TP.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">jzleibo@mit.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>23</day>
<month>10</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>10</issue>
<elocation-id>e1004390</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>6</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>5</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Leibo et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004390" xlink:type="simple"/>
<abstract>
<p>Is visual cortex made up of general-purpose information processing machinery, or does it consist of a collection of specialized modules? If prior knowledge, acquired from learning a set of objects is only transferable to new objects that share properties with the old, then the recognition system’s optimal organization must be one containing specialized modules for different object classes. Our analysis starts from a premise we call the invariance hypothesis: that the computational goal of the ventral stream is to compute an invariant-to-transformations and discriminative signature for recognition. The key condition enabling approximate transfer of invariance without sacrificing discriminability turns out to be that the learned and novel objects transform similarly. This implies that the optimal recognition system must contain subsystems trained only with data from similarly-transforming objects and suggests a novel interpretation of domain-specific regions like the fusiform face area (FFA). Furthermore, we can define an index of transformation-compatibility, computable from videos, that can be combined with information about the statistics of natural vision to yield predictions for which object categories ought to have domain-specific regions in agreement with the available data. The result is a unifying account linking the large literature on view-based recognition with the wealth of experimental evidence concerning domain-specific regions.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Domain-specific regions, like the fusiform face area, are a prominent feature of ventral visual cortex organization. Despite decades of interest from a large number of investigators employing diverse methods, there has been surprisingly little theoretical work on “why” the ventral stream may adopt this modular organization. In this study we propose a computational account of the role played by domain-specific regions in ventral stream function. It follows from a new theoretical analysis of the recognition problem which highlights the importance of building representations that are robust to class-specific transformations. These results provide a unifying account linking neuroimaging and neuropsychology-based ideas of domain-specific regions to the psychophysics and electrophysiology-oriented literature on view-based object recognition and invariance.</p>
</abstract>
<funding-group>
<funding-statement>This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF-1231216. URL: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://cbmm.mit.edu/">http://cbmm.mit.edu/</ext-link> (TP). This research was also sponsored by grants from the National Science Foundation (NSF-0640097, NSF-0827427) URL: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.nsf.gov/">http://www.nsf.gov/</ext-link> (TP), and the Air Force Office of Scientific Research AFOSR-THRL (FA8650-05-C-7262) URL: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.afosr.af.mil">www.afosr.af.mil</ext-link> (TP). Additional support was provided by the Eugene McDermott Foundation (TP). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="1"/>
<page-count count="29"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>Image data was generated automatically using the methods described in the paper. Additionally, the images used for experiment 3 and 4 are available for download from The Center For Brains, Minds, and Machines: cbmm.mit.edu.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The discovery of category-selective patches in the ventral stream—e.g., the fusiform face area (FFA)—is one of the most robust experimental findings in visual neuroscience [<xref ref-type="bibr" rid="pcbi.1004390.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref006">6</xref>]. It has also generated significant controversy. From a computational perspective, much of the debate hinges on the question of whether the algorithm implemented by the ventral stream requires subsystems or modules dedicated to the processing of a single class of stimuli [<xref ref-type="bibr" rid="pcbi.1004390.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref008">8</xref>]. The alternative account holds that visual representations are distributed over many regions [<xref ref-type="bibr" rid="pcbi.1004390.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref010">10</xref>], and the clustering of category selectivity is not, in itself, functional. Instead, it arises from the interaction of biological constraints like anatomically fixed inter-region connectivity and competitive plasticity mechanisms [<xref ref-type="bibr" rid="pcbi.1004390.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref012">12</xref>] or the center-periphery organization of visual cortex [<xref ref-type="bibr" rid="pcbi.1004390.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref017">17</xref>].</p>
<p>The interaction of three factors is thought to give rise to properties of the ventral visual pathway: (1) The computational task; (2) constraints of anatomy and physiology; and (3) the statistics of the visual environment [<xref ref-type="bibr" rid="pcbi.1004390.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref022">22</xref>]. Differing presuppositions concerning their relative weighting lead to quite different models of the origin of category-selective regions. If the main driver is thought to be the visual environment (factor 3), then perceptual expertise-based accounts of category selective regions are attractive [<xref ref-type="bibr" rid="pcbi.1004390.ref023">23</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref025">25</xref>]. Alternatively, mechanistic models show how constraints of the neural “hardware” (factor 2) could explain category selectivity [<xref ref-type="bibr" rid="pcbi.1004390.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref027">27</xref>]. Contrasting with both of these, the perspective of the present paper is one in which computational factors are the main reason for the clustering of category-selective neurons.</p>
<p>The lion’s share of computational modeling in this area has been based on factors 2 and 3. These models seek to explain category selective regions as the inevitable outcome of the interaction between functional processes; typically competitive plasticity, wiring constraints, e.g., local connectivity, and assumptions about the system’s inputs [<xref ref-type="bibr" rid="pcbi.1004390.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref028">28</xref>]. Mechanistic models of category selectivity may even be able to account for the neuropsychology [<xref ref-type="bibr" rid="pcbi.1004390.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref030">30</xref>] and behavioral [<xref ref-type="bibr" rid="pcbi.1004390.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref032">32</xref>] results long believed to support modularity.</p>
<p>Another line of evidence seems to explain away the category selective regions. The large-scale topography of object representation is reproducible across subjects [<xref ref-type="bibr" rid="pcbi.1004390.ref033">33</xref>]. For instance, the scene-selective parahippocampal place area (PPA) is consistently medial to the FFA. To explain this remarkable reproducibility, it has been proposed that the center-periphery organization of early visual areas extends to the later object-selective regions of the ventral stream [<xref ref-type="bibr" rid="pcbi.1004390.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref017">17</xref>]. In particular, the FFA and other face-selective region are associated with an extension of the central representation, and PPA with the peripheral representation. Consistent with these findings, it has also been argued that real-world size is the organizing principle [<xref ref-type="bibr" rid="pcbi.1004390.ref016">16</xref>]. Larger objects, e.g., furniture, evoke more medial activation while smaller objects, e.g., a coffee mug, elicit more lateral activity.</p>
<p>Could category selective regions be explained as a consequence of the topography of visual cortex? Both the eccentricity [<xref ref-type="bibr" rid="pcbi.1004390.ref015">15</xref>] and real-world size [<xref ref-type="bibr" rid="pcbi.1004390.ref016">16</xref>] hypotheses correctly predict that houses and faces will be represented at opposite ends of the medial-lateral organizing axis. Since eccentricity of presentation is linked with acuity demands, the differing eccentricity profiles across object categories may be able to explain the clustering. However, such accounts offer no way of interpreting macaque results indicating multi-stage processing hierarchies [<xref ref-type="bibr" rid="pcbi.1004390.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>]. If clustering was a secondary effect driven by acuity demands, then it would be difficult to explain why, for instance, the macaque face-processing system consists of a hierarchy of patches that are preferentially connected with one another [<xref ref-type="bibr" rid="pcbi.1004390.ref035">35</xref>].</p>
<p>In macaques, there are 6 discrete face-selective regions in the ventral visual pathway, one posterior lateral face patch (PL), two middle face patches (lateral- ML and fundus- MF), and three anterior face patches, the anterior fundus (AF), anterior lateral (AL), and anterior medial (AM) patches [<xref ref-type="bibr" rid="pcbi.1004390.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref036">36</xref>]. At least some of these patches are organized into a feedforward hierarchy. Visual stimulation evokes a change in the local field potential ∼ 20 ms earlier in ML/MF than in patch AM [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>]. Consistent with a hierarchical organization involving information passing from ML/MF to AM via AL, electrical stimulation of ML elicits a response in AL and stimulation in AL elicits a response in AM [<xref ref-type="bibr" rid="pcbi.1004390.ref035">35</xref>]. In addition, spatial position invariance increases from ML/MF to AL, and increases further to AM [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>] as expected for a feedforward processing hierarchy. The firing rates of neurons in ML/MF are most strongly modulated by face viewpoint. Further along the hierarchy, in patch AM, cells are highly selective for individual faces and collectively provide a representation of face identity that tolerates substantial changes in viewpoint [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>].</p>
<p>Freiwald and Tsao argued that the network of face patches is <italic>functional</italic>. Response patterns of face patch neurons are consequences of the role they play in the algorithm implemented by the ventral stream. Their results suggest that the face network computes a representation of faces that is—as much as possible—invariant to 3D rotation-in-depth (viewpoint), and that this representation may underlie face identification behavior [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>].</p>
<p>We carry out our investigation within the framework provided by a recent theory of invariant object recognition in hierarchical feedforward architectures [<xref ref-type="bibr" rid="pcbi.1004390.ref037">37</xref>]. It is broadly in accord with other recent perspectives on the ventral stream and the problem of object recognition [<xref ref-type="bibr" rid="pcbi.1004390.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref038">38</xref>]. The full theory has implications for many outstanding questions that are not directly related to the question of domain specificity we consider here. In other work, it has been shown to yield predictions concerning the cortical magnification factor and visual crowding [<xref ref-type="bibr" rid="pcbi.1004390.ref039">39</xref>]. It has also been used to motivate novel algorithms in computer vision and speech recognition that perform competitively with the state-of-the-art on difficult benchmark tasks [<xref ref-type="bibr" rid="pcbi.1004390.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref044">44</xref>]. The same theory, with the additional assumption of a particular Hebbian learning rule, can be used to derive qualitative receptive field properties. The predictions include Gabor-like tuning in early stages of the visual hierarchy [<xref ref-type="bibr" rid="pcbi.1004390.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref046">46</xref>] and mirror-symmetric orientation tuning curves in the penultimate stage of a face-specific hierarchy computing a view-tolerant representation (as in [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>]) [<xref ref-type="bibr" rid="pcbi.1004390.ref046">46</xref>]. A full account of the new theory is outside the scope of the present work; we refer the interested reader to the references—especially [<xref ref-type="bibr" rid="pcbi.1004390.ref037">37</xref>] for details.</p>
<p>Note that the theory only applies to the first feedforward pass of information, from the onset of the image to the arrival of its representation in IT cortex approximately 100 ms later. For a recent review of evidence that the feedforward pass computes invariant representations, see [<xref ref-type="bibr" rid="pcbi.1004390.ref022">22</xref>]. For an alternative perspective, see [<xref ref-type="bibr" rid="pcbi.1004390.ref011">11</xref>]. Though note also, contrary to a claim in that review, position dependence is fully compatible with the class of models we consider here (including HMAX). [<xref ref-type="bibr" rid="pcbi.1004390.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref047">47</xref>] explicitly model eccentricity dependence in this framework.</p>
<p>Our account of domain specificity is motivated by the following questions: How can past visual experience be leveraged to improve future recognition of novel individuals? Is any past experience useful for improving at-a-glance recognition of any new object? Or perhaps past experience only transfers to similar objects? Could it even be possible that past experience with certain objects actually impedes the recognition of others?</p>
<p>The invariance hypothesis holds that the computational goal of the ventral stream is to compute a representation that is unique to each object and invariant to identity-preserving transformations. If we accept this premise, the key question becomes: Can transformations learned on one set of objects be reliably transferred to another set of objects? For many visual tasks, the variability due to transformations in a single individual’s appearance is considerably larger than the variability between individuals. These tasks have been called “subordinate level identification” tasks, to distinguish them from between-category (basic-level) tasks. Without prior knowledge of transformations, the subordinate-level task of recognizing a novel individual from a single example image is hopelessly under-constrained.</p>
<p>The main thrust of our argument—to be developed below—is this: The ventral stream computes object representations that are invariant to transformations. Some transformations are <italic>generic</italic>; the ventral stream could learn to discount these from experience with any objects. Translation and scaling are both generic (all 2D affine transformations are). However, it is also necessary to discount many transformations that do not have this property. Many common transformations are not generic; 3D-rotation-in-depth is the primary example we consider here (see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref> for more examples). It is not possible to achieve a perfectly view-invariant representation from one 2D example. Out-of-plane rotation depends on information that is not available in a single image, e.g. the object’s 3D structure. Despite this, approximate invariance can still be achieved using prior knowledge of how similar objects transform. In this way, approximate invariance learned on some members of a visual category can facilitate the identification of unfamiliar category members. But, this transferability only goes so far.</p>
<p>Under this account, the key factor determining which objects could be productively grouped together in a domain-specific subsystem is their transformation compatibility. We propose an operational definition that can be computed from videos of transforming objects. Then we use it to explore the question of why certain object classes get dedicated brain regions, e.g., faces and bodies, while others (apparently) do not.</p>
<p>We used 3D graphics to generate a library of videos of objects from various categories undergoing rotations in depth. The model of visual development (or evolution) we consider is highly stylized and non-mechanistic. It is just a clustering algorithm based on our operational definition of transformation compatibility. Despite its simplicity, using the library of depth-rotation videos as inputs, the model predicts large clusters consisting entirely of faces and bodies.</p>
<p>The other objects we tested—vehicles, chairs, and animals—ended up in a large number of small clusters, each consisting of just a few objects. This suggests a novel interpretation of the lateral occipital complex (LOC). Rather than being a “generalist” subsystem, responsible for recognizing objects from diverse categories, our results are consistent with LOC actually being a heterogeneous region that consists of a large number of domain-specific regions too small to be detected with fMRI.</p>
<p>These considerations lead to a view of the ventral visual pathway in which category-selective regions implement a modularity of <italic>content</italic> rather than <italic>process</italic> [<xref ref-type="bibr" rid="pcbi.1004390.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref049">49</xref>]. Our argument is consistent with process-based accounts, but does not require us to claim that faces are automatically processed in ways that are inapplicable to objects (e.g., gaze detection or gender detection) as claimed by [<xref ref-type="bibr" rid="pcbi.1004390.ref011">11</xref>]. Nor does it commit us to claiming there is a region that is specialized for the process of subordinate-level identification—an underlying assumption of some expertise-based models [<xref ref-type="bibr" rid="pcbi.1004390.ref050">50</xref>]. Rather, we show here that the invariance hypothesis implies an algorithmic role that could be fulfilled by the mere clustering of selectivity. Consistent with the idea of a canonical cortical microcircuit [<xref ref-type="bibr" rid="pcbi.1004390.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref052">52</xref>], the computations performed in each subsystem may be quite similar to the computations performed in the others. To a first approximation, the only difference between ventral stream modules could be the object category for which they are responsible.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Theory sketch</title>
<p>To make the invariance hypothesis precise, let <italic>g</italic><sub><italic>θ</italic></sub> denote a transformation with parameter <italic>θ</italic>. Two images <italic>I</italic>, <italic>I</italic>′ depict the same object whenever ∃<italic>θ</italic>, such that <italic>I</italic>′ = <italic>g</italic><sub><italic>θ</italic></sub> <italic>I</italic>. For a small positive constant <italic>ε</italic>, the invariance hypothesis is the claim that the computational goal of the ventral stream is to compute a function <italic>μ</italic>, called a <italic>signature</italic>, such that
<disp-formula id="pcbi.1004390.e001"><alternatives><graphic id="pcbi.1004390.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mi>μ</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>≤</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
We say that a signature for which <xref ref-type="disp-formula" rid="pcbi.1004390.e001">Eq (1)</xref> is satisfied (for all <italic>θ</italic>) is <italic>ϵ</italic>-invariant to the family of transformations {<italic>g</italic><sub><italic>θ</italic></sub>}. An <italic>ϵ</italic>-invariant signature that is unique to an object can be used to discriminate images of that object from images of other objects. In the context of a hierarchical model of the ventral stream, the “top level” representation of an image is its signature.</p>
<p>One approach to modeling the ventral stream, first taken by Fukushima’s Neocognitron [<xref ref-type="bibr" rid="pcbi.1004390.ref053">53</xref>], and followed by many other models [<xref ref-type="bibr" rid="pcbi.1004390.ref054">54</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref058">58</xref>], is based on iterating a basic module inspired by Hubel and Wiesel’s proposal for the connectivity of V1 simple (AND-like) and complex (OR-like) cells. In the case of HMAX [<xref ref-type="bibr" rid="pcbi.1004390.ref055">55</xref>], each “HW”-module consists of one C-unit (corresponding to a complex cell) and all its afferent S-units (corresponding to simple cells); see <xref ref-type="fig" rid="pcbi.1004390.g001">Fig 1B</xref>. The response of an S-unit to an image <italic>I</italic> is typically modeled by a dot product with a stored template <italic>t</italic>, indicated here by ⟨<italic>I</italic>, <italic>t</italic>⟩. Since ⟨<italic>I</italic>, <italic>t</italic>⟩ is maximal when <italic>I</italic> = <italic>t</italic> (assuming that <italic>I</italic> and <italic>t</italic> have unit norm), we can think of an S-unit’s response as a measure of <italic>I</italic>’s similarity to <italic>t</italic>. The module corresponding to Hubel and Wiesel’s original proposal had several S-units, each detecting their stored template at a different position. Let <inline-formula id="pcbi.1004390.e002"><alternatives><graphic id="pcbi.1004390.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e002"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>⃗</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> be the translation operator: when applied to an image, <inline-formula id="pcbi.1004390.e003"><alternatives><graphic id="pcbi.1004390.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e003"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>⃗</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> returns its translation by <inline-formula id="pcbi.1004390.e004"><alternatives><graphic id="pcbi.1004390.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e004"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>⃗</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. This lets us write the response of the specific S-unit which signals the presence of template <italic>t</italic> at position <inline-formula id="pcbi.1004390.e005"><alternatives><graphic id="pcbi.1004390.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e005"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>⃗</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> as <inline-formula id="pcbi.1004390.e006"><alternatives><graphic id="pcbi.1004390.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">⟨</mml:mo> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>⃗</mml:mo></mml:mover></mml:msub> <mml:mi>t</mml:mi></mml:mrow> <mml:mo stretchy="true">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Then, introducing a nonlinear <italic>pooling function</italic>, which for HMAX would be the <monospace>max</monospace> function, the response <italic>C</italic>(<italic>I</italic>) of the C-unit (equivalently: the output of the HW-module, one element of the signature) is given by
<disp-formula id="pcbi.1004390.e007"><alternatives><graphic id="pcbi.1004390.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext mathvariant="monospace">max</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mo>⟨</mml:mo> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>⟩</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where the <monospace>max</monospace> is taken over all the S-units in the module. The region of space covered by a module’s S-units is called its <italic>pooling domain</italic> and the C-unit is said to pool the responses of its afferent S-units. HMAX, as well as more recent models based on this approach typically also pool over a range of scales [<xref ref-type="bibr" rid="pcbi.1004390.ref056">56</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref058">58</xref>]. In most cases, the first layer pooling domains are small intervals of translation and scaling. In the highest layers the pooling domains are usually global, i.e. over the entire range of translation and scaling that is visible during a single fixation. Notice also that this formulation is more general than HMAX. It applies to a wide class of hierarchical models of cortical computation, e.g., [<xref ref-type="bibr" rid="pcbi.1004390.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref058">58</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref060">60</xref>]. For instance, <italic>t</italic> need not be directly interpretable as a template depicting an image of a certain object. A convolutional neural network in the sense of [<xref ref-type="bibr" rid="pcbi.1004390.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref062">62</xref>] is obtained by choosing <italic>t</italic> to be a “prototype” obtained as the outcome of a gradient descent-based optimization procedure. In what follows we use the HW-module language since it is convenient for stating the domain-specificity argument.</p>
<fig id="pcbi.1004390.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004390.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Orbits and HW-modules.</title>
<p>(A) Illustration that the orbit with respect to in-plane rotation is invariant and unique. (B) Three HW-modules are shown. In this example, each HW-module pools over a 9 × 3 region of the image. Each S-unit stores a 3 × 3 template and there are three S-units per HW-module.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.g001"/>
</fig>
<p>HW-modules can compute approximately invariant representations for a broad class of transformations [<xref ref-type="bibr" rid="pcbi.1004390.ref037">37</xref>]. However, and this is a key fact: the conditions that must be met are different for different transformations. Following Anselmi et al. [<xref ref-type="bibr" rid="pcbi.1004390.ref037">37</xref>], we can distinguish two “regimes”. The first regime applies to the important special case of transformations with a group structure, e.g., 2D affine transformations. The second regime applies more broadly to any locally-affine transformation.</p>
<p>For a family of transformations {<italic>g</italic><sub><italic>θ</italic></sub>}, define the <italic>orbit</italic> of an image <italic>I</italic> to be the set <italic>O</italic><sub><italic>I</italic></sub> = {<italic>g</italic><sub><italic>θ</italic></sub> <italic>I</italic>, <italic>θ</italic> ∈ ℝ}. Anselmi et al. [<xref ref-type="bibr" rid="pcbi.1004390.ref037">37</xref>] proved that HW-modules can pool over other transformations besides translation and scaling. It is possible to pool over any transformation for which orbits of template objects are available. A biologically-plausible way to learn the pooling connections within an HW-module could be to associate temporally adjacent frames of the video of visual experience (as in e.g., [<xref ref-type="bibr" rid="pcbi.1004390.ref063">63</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref068">68</xref>]). In both regimes, the following condition is required for the invariance obtained from the orbits of a set of template objects to generalize to new objects. For all <italic>g</italic><sub><italic>θ</italic></sub> <italic>I</italic> ∈ <italic>O</italic><sub><italic>I</italic></sub> there is a corresponding <italic>g</italic><sub><italic>θ</italic>′</sub> <italic>t</italic> ∈ <italic>O</italic><sub><italic>t</italic></sub> such that
<disp-formula id="pcbi.1004390.e008"><alternatives><graphic id="pcbi.1004390.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>⟨</mml:mo> <mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>⟩</mml:mo> <mml:mo>=</mml:mo> <mml:mo>⟨</mml:mo> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:msup><mml:mi>θ</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
In the first regime, <xref ref-type="disp-formula" rid="pcbi.1004390.e008">Eq (3)</xref> holds regardless of the level of similarity between the templates and test objects. Almost any templates can be used to recognize any other images invariantly to group transformations (see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref>). Note also that this is consistent with reports in the literature of strong performance achieved using random filters in convolutional neural networks [<xref ref-type="bibr" rid="pcbi.1004390.ref069">69</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref071">71</xref>]. <xref ref-type="fig" rid="pcbi.1004390.g001">Fig 1A</xref> illustrates that the orbit with respect to in-plane rotation is invariant.</p>
<p>In the second regime, corresponding to non-group transformations, it is not possible to achieve a perfect invariance. These transformations often depend on information that is not available in a single image. For example, rotation in depth depends on an object’s 3D structure and illumination changes depend on its material properties (see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref>). Despite this, approximate invariance to smooth non-group transformations can still be achieved using prior knowledge of how similar objects transform. Second-regime transformations are <italic>class-specific</italic>, e.g., the transformation of object appearance caused by a rotation in depth is not the same 2D transformation for two objects with different 3D structures. However, by restricting to a class where all the objects have similar 3D structure, all objects do rotate (approximately) the same way. Moreover, this commonality can be exploited to transfer the invariance learned from experience with (orbits of) template objects to novel objects seen only from a single example view.</p>
</sec>
<sec id="sec004">
<title>Simulations: Core predictions</title>
<p>The theory makes two core predictions:
<list list-type="order"><list-item><p>Learned invariance to group transformations should be transferable from any set of stimuli to any other.</p></list-item> <list-item><p>For non-group transformations, approximate invariance will transfer within certain object classes. In the case of 3D depth-rotation, it will transfer within classes for which all members share a common 3D structure.</p></list-item></list></p>
<p>Both core predictions were addressed with tests of transformation-tolerant recognition based on a single example view. Two image sets were created to test the first core prediction: (A) 100 faces derived from the Max-Planck institute face dataset [<xref ref-type="bibr" rid="pcbi.1004390.ref072">72</xref>]. Each face was oval-cropped to remove external features and normalized so that all images had the same mean and variance over pixels (as in [<xref ref-type="bibr" rid="pcbi.1004390.ref073">73</xref>]). (B) 100 random greyscale noise patterns. 29 images of each face and random noise pattern were created by placing the object over the horizontal interval from 40 pixels to the left of the image’s center up to 40 pixels to the right of the image’s center in increments of 5 pixels. All images were 256 × 256 pixels.</p>
<p>Three image sets were created to test the second core prediction: (A) 40 untextured face models were rendered at each orientation in 5° increments from −95° to 95°. (B) 20 objects sharing a common gross structure (a conical shape) and differing from one another by the exact placement and size of smaller bumps. (C) 20 objects sharing gross structure consisting of a central pyramid on a flat plane and two walls on either side. Individuals differed from one another by the location and slant of several additional bumps. The face models were generated using Facegen [<xref ref-type="bibr" rid="pcbi.1004390.ref074">74</xref>]. Class B and C models were generated with Blender [<xref ref-type="bibr" rid="pcbi.1004390.ref075">75</xref>]. All rendering was also done with Blender and used perspective projection at a resolution of 256 × 256 pixels.</p>
<p>The tests of transformation-tolerant recognition from a single example were performed as follows. In each “block”, the model was shown a reference image and a set of query images. The reference image always depicted an object under the transformation with the median parameter value. That is, for rotation in depth of faces, it was a frontal face (0°) and for translation, the object was located in the center of the visual field. Each query image either depicted the same object as the reference image (target case) or a different object (distractor case). In each block, each query image was shown at each position or angle in the block’s testing interval. All testing intervals were symmetric about 0. Using a sequence of testing intervals ordered by inclusion, it was possible to investigate how tolerance declines with increasingly demanding transformations. The radius of the testing interval is the abscissa of the plots in Figs <xref ref-type="fig" rid="pcbi.1004390.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1004390.g003">3</xref>.</p>
<fig id="pcbi.1004390.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004390.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Translation invariance.</title>
<p>Bottom panel (II): Example images from the two classes. The faces were obtained from the Max-Planck Institute dataset [<xref ref-type="bibr" rid="pcbi.1004390.ref072">72</xref>] and then contrast normalized and translated over a black background. Top panel (I): The left column shows the results of a test of translation invariance for faces and the right column shows the same test for random noise patterns. The view-based model (blue curve) was built using templates from class A in the top row and class B in the bottom row. The abscissa of each plot shows the maximum invariance range (a distance in pixels) over which target and distractor images were presented. The view-based model was never tested on any of the images that were used as templates. Error bars (±1 standard deviation) were computed over 5 repetitions of the experiment using different (always disjoint) sets of template and testing images.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.g002"/>
</fig>
<fig id="pcbi.1004390.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004390.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Class-specific transfer of depth-rotation invariance.</title>
<p>Bottom panel (II): Example images from the three classes. Top panel (I): The left column shows the results of a test of 3D rotation invariance on faces (class A), the middle column shows results for class B and the right column shows the results for class C. The view-based model (blue curve) was built using images from class A in the top row, class B in the middle row, and class C in the bottom row. The abscissa of each plot shows the maximum invariance range (degrees of rotation away from the frontal face) over which target and distractor images were presented. The view-based model was never tested on any of the images that were used as templates. Error bars (±1 standard deviation) were computed over 20 cross validation runs using different choices of template and test images. Only the plots on the diagonal (train A—test A, train B—test B, train C- test C) show an improvement of the view-based model over the pixel representation. That is, only when the test images transform similarly to the templates is there any benefit from pooling.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.g003"/>
</fig>
<p>For each repetition of the translation experiments, 30 objects were randomly sampled from the template class and 30 objects from the testing class. For each repetition of the depth-rotation experiments, 10 objects were sampled from template and testing classes that were always disjoint from one another.</p>
<p>Networks consisting of <italic>K</italic> HW-modules were constructed where <italic>K</italic> was the number of sampled template objects. The construction followed the procedure described in the method section below. Signatures computed by these networks are vectors with <italic>K</italic> elements. In each block, the signature of the reference image was compared to the signature of each query image by its Pearson correlation and ranked accordingly. This ranked representation provides a convenient way to compute the ROC curve since it admits acceptance thresholds in terms of ranks (as opposed to real numbers). Thus, the final measure of transformation tolerance reported on the ordinate of the plots in Figs <xref ref-type="fig" rid="pcbi.1004390.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1004390.g003">3</xref> is the mean area under the ROC curve (AUC) over all choices of reference object and repetitions of the experiment with different training / test set splits. Since AUC is computed by integrating over acceptance thresholds, it is a bias free statistic. In this case it is analogous to <italic>d</italic>′ for the corresponding 2AFC same-different task. When performance is invariant, AUC as a function of testing interval radius will be a flat line.</p>
<p>If there is imperfect invariance (<italic>ϵ</italic>-invariance), then performance will decline as the radius of the testing interval is increased. To assess imperfect invariance, it is necessary to compare with an appropriate baseline at whatever performance level would be achieved by similarity in the input. Since any choice of input encoding induces its own similarity metric, the most straightforward way to obtain interpretable results is to use the raw pixel representation as the baseline (red curves in Figs <xref ref-type="fig" rid="pcbi.1004390.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1004390.g003">3</xref>). Thus, a one layer architecture was used for these simulations: each HW-module directly receives the pixel representation of the input.</p>
<p>The first core prediction was addressed by testing translation-tolerant recognition with models trained using random noise templates to identify faces and vice versa (<xref ref-type="fig" rid="pcbi.1004390.g002">Fig 2</xref>). The results in the plots on the diagonal for the view-based model (blue curve) indicate that face templates can indeed be used to identify other faces invariantly to translation; and random noise templates can be used to identify random noise invariantly to translation. The key prediction of the theory concerns the off-diagonal plots. In those cases, templates from faces were used to recognize noise patterns and noise was used to recognize faces. Performance was invariant in both cases; the blue curves in <xref ref-type="fig" rid="pcbi.1004390.g002">Fig 2</xref> were flat. This result was in accord with the theory’s prediction for the group transformation case: the templates need not resemble the test images.</p>
<p>The second core prediction concerning class-specific transfer of learned <italic>ϵ</italic>-invariance for non-group transformations was addressed by analogous experiments with 3D depth-rotation. Transfer of invariance both within and between classes was assessed using 3 different object classes: faces and two synthetic classes. The level of rotation tolerance achieved on this difficult task was the amount by which performance of the view-based model (blue curve) exceeded the raw pixel representation’s performance for the plots on the diagonal of <xref ref-type="fig" rid="pcbi.1004390.g003">Fig 3</xref>. The off-diagonal plots show the deleterious effect of using templates from the wrong class.</p>
<p>There are many other non-group transformations besides depth-rotation. <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref> describes additional simulations for changes in illumination. These depend on material properties. It also describes simulations of pose (standing, sitting, etc)-invariant body recognition.</p>
</sec>
<sec id="sec005">
<title>Transformation compatibility</title>
<p>How can object experience—i.e., templates—be assigned to subsystems in order to facilitate productive transfer? If each individual object is assigned to a separate group, the negative effects of using templates from the wrong class are avoided; but past experience can never be transferred to new objects. So far we have only said that “3D structure” determines which objects can be productively grouped together. In this section we derive a more concrete criterion: transformation compatibility.</p>
<p>Given a set of objects sampled from a category, what determines when HW-modules encoding templates for a few members of the class can be used to approximately invariantly recognize unfamiliar members of the category from a single example view? Recall that the transfer of invariance depends on the condition given by <xref ref-type="disp-formula" rid="pcbi.1004390.e008">Eq (3)</xref>. For non-group transformations this turns out to require that the objects “transform the same way” (see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref> for the proof; the notion of a “nice class” is also related [<xref ref-type="bibr" rid="pcbi.1004390.ref076">76</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref077">77</xref>]). Given a set of orbits of different objects (only the image sequences are needed), we would like to have an index <inline-formula id="pcbi.1004390.e009"><alternatives><graphic id="pcbi.1004390.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e009"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> that measures how similarly the objects in the class transform. If an object category has too low <inline-formula id="pcbi.1004390.e010"><alternatives><graphic id="pcbi.1004390.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e010"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, then there would be no gain from creating a subsystem for that category. Whenever a category has high <inline-formula id="pcbi.1004390.e011"><alternatives><graphic id="pcbi.1004390.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e011"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, it is a candidate for having a dedicated subsystem.</p>
<p>The transformation compatibility of two objects <italic>A</italic> and <italic>B</italic> is defined as follows. Consider a smooth transformation <italic>T</italic> parameterized by <italic>i</italic>. Since <italic>T</italic> may be class-specific, let <italic>T</italic><sub><italic>A</italic></sub> denote its application to object <italic>A</italic>. One of the requirements that must be satisfied for <italic>ϵ</italic>-invariance to transfer from an object <italic>A</italic> to an object <italic>B</italic> is that <italic>T</italic><sub><italic>A</italic></sub> and <italic>T</italic><sub><italic>B</italic></sub> have equal Jacobians (see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref>). This suggests an operational definition of the transformation compatibility between two objects <italic>ψ</italic>(<italic>A</italic>, <italic>B</italic>).</p>
<p>Let <italic>A</italic><sub><italic>i</italic></sub> be the <italic>i</italic><sub><italic>th</italic></sub> frame of the video of object A transforming and <italic>B</italic><sub><italic>i</italic></sub> be the <italic>i</italic><sub><italic>th</italic></sub> frame of the video of object B transforming. The Jacobian can be approximated by the “video” of difference images: <italic>J</italic><sub><italic>A</italic></sub>(<italic>i</italic>) = ∣<italic>A</italic><sub><italic>i</italic></sub> − <italic>A</italic><sub><italic>i</italic>+1</sub>∣ (∀<italic>i</italic>). Then define the “instantaneous” transformation compatibility <italic>ψ</italic>(<italic>A</italic>, <italic>B</italic>)(<italic>i</italic>): = ⟨<italic>J</italic><sub><italic>A</italic></sub>(<italic>i</italic>), <italic>J</italic><sub><italic>B</italic></sub>(<italic>i</italic>)⟩. Thus for a range of parameters <italic>i</italic> ∈ <italic>R</italic> = [−<italic>r</italic>, <italic>r</italic>], the empirical transformation compatibility between <italic>A</italic> and <italic>B</italic> is
<disp-formula id="pcbi.1004390.e012"><alternatives><graphic id="pcbi.1004390.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>r</mml:mi></mml:mrow> <mml:mi>r</mml:mi></mml:munderover> <mml:mo>⟨</mml:mo> <mml:mrow><mml:msub><mml:mi>J</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>J</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>⟩</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>The index <inline-formula id="pcbi.1004390.e013"><alternatives><graphic id="pcbi.1004390.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e013"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> that we compute for sets of objects is the mean value of <italic>ψ</italic>(<italic>A</italic>, <italic>B</italic>) taken over all pairs <italic>A</italic>, <italic>B</italic> from the set. For very large sets of objects it could be estimated by randomly sampling pairs. In the present case, we were able to use all pairs in the available data.</p>
<p>For the case of rotation in depth, we used 3D modeling / rendering software [<xref ref-type="bibr" rid="pcbi.1004390.ref075">75</xref>] to obtain (dense samples from) orbits. We computed the transformation compatibility index <inline-formula id="pcbi.1004390.e014"><alternatives><graphic id="pcbi.1004390.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e014"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> for several datasets from different sources. Faces had the highest <inline-formula id="pcbi.1004390.e015"><alternatives><graphic id="pcbi.1004390.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> of any naturalistic category we tested—unsurprising since recognizability likely influenced face evolution. A set of chair objects (from [<xref ref-type="bibr" rid="pcbi.1004390.ref078">78</xref>]) had very low <inline-formula id="pcbi.1004390.e016"><alternatives><graphic id="pcbi.1004390.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e016"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> implying no benefit would be obtained from a chair-specific region. More interestingly, we tested a set of synthetic “wire” objects, very similar to those used in many classic experiments on view-based recognition e.g. [<xref ref-type="bibr" rid="pcbi.1004390.ref079">79</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref081">81</xref>]. We found that the wire objects had the lowest <inline-formula id="pcbi.1004390.e017"><alternatives><graphic id="pcbi.1004390.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> of any category we tested; experience with familiar wire objects does not transfer to new wire objects. Therefore it is never productive to group them into a subsystem.</p>
</sec>
<sec id="sec006">
<title>Simulations: The domain specific architecture of visual cortex</title>
<p>The above considerations suggest an unsupervised strategy for sorting object experience into subsystems. An online <italic>ψ</italic>-based clustering algorithm could sort each newly learned object representation into the subsystem (cluster) with which it transforms most compatibly. With some extra assumptions beyond those required for the main theory, such an algorithm could be regarded as a very stylized model of the development (or evolution) of visual cortex. In this context we asked: Is it possible to derive predictions for the specific object classes that will “get their own private piece of real estate in the brain” [<xref ref-type="bibr" rid="pcbi.1004390.ref008">8</xref>] from the invariance hypothesis?</p>
<p>The extra assumptions required at this point are as follows.
<list list-type="order"><list-item><p>Cortical object representations (HW-modules) are sampled from the distribution <inline-formula id="pcbi.1004390.e018"><alternatives><graphic id="pcbi.1004390.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow/><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> of objects and their transformations encountered under natural visual experience.</p></list-item> <list-item><p>Subsystems are localized on cortex.</p></list-item> <list-item><p>The number of HW-modules in a local region and the proportion belonging to different categories determines the predicted BOLD response for contrasts between the categories. For example, a cluster with 90% face HW-modules, 10% car HW-modules, and no other HW-modules would respond strongly in the faces—cars contrast, but not as strongly as it would in a faces—airplanes contrast. We assume that clusters containing very few HW-modules are too small to be imaged with the resolution of fMRI—though they may be visible with other methods that have higher resolution.</p></list-item></list></p>
<p>Any model that can predict which specific categories will have domain-specific regions must depend on contingent facts about the world, in particular, the—difficult to approximate—distribution <inline-formula id="pcbi.1004390.e019"><alternatives><graphic id="pcbi.1004390.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> of objects and their transformations encountered during natural vision. Consider the following: HW-modules may be assigned to cluster near one another on cortex in order to maximize the transformation compatibility <inline-formula id="pcbi.1004390.e020"><alternatives><graphic id="pcbi.1004390.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e020"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> of the set of objects represented in each local neighborhood. Whenever a new object is learned, its HW-module could be placed on cortex in the neighborhood with which it transforms most compatibly. Assume a new object is sampled from <inline-formula id="pcbi.1004390.e021"><alternatives><graphic id="pcbi.1004390.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e021"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> at each iteration. We conjecture that the resulting cortex model obtained after running this for some time would have a small number of very large clusters, probably corresponding to faces, bodies, and orthography in a literate brain’s native language. The rest of the objects would be encoded by HW-modules at random locations. Since neuroimaging methods like fMRI have limited resolution, only the largest clusters would be visible to them. Cortical regions with low <inline-formula id="pcbi.1004390.e022"><alternatives><graphic id="pcbi.1004390.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e022"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> would appear in neuroimaging experiments as generic “object regions” like LOC [<xref ref-type="bibr" rid="pcbi.1004390.ref082">82</xref>].</p>
<p>Since we did not attempt the difficult task of sampling from <inline-formula id="pcbi.1004390.e023"><alternatives><graphic id="pcbi.1004390.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e023"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, we were not able to test the conjecture directly. However, by assuming particular distributions and sampling from a large library of 3D models [<xref ref-type="bibr" rid="pcbi.1004390.ref074">74</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref078">78</xref>], we can study the special case where the only transformation is rotation in depth. Each object was rendered at a range of viewpoints: −90° to 90° in increments of 5 degrees. The objects were drawn from five categories: faces, bodies, animals, chairs, and vehicles. Rather than trying to estimate the frequencies with which these objects occur in natural vision, we instead aimed for predictions that could be shown to be robust over a range of assumptions on <inline-formula id="pcbi.1004390.e024"><alternatives><graphic id="pcbi.1004390.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e024"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Thus we repeated the online clustering experiment three times, each using a different object distribution (see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S2 Table</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1004390.s007">S6</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s008">S7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s009">S8</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s010">S9</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1004390.s011">S10</xref> Figs).</p>
<p>The <italic>ψ</italic>-based clustering algorithm we used can be summarized as follows: Consider a model consisting of a number of subsystems. When an object is learned, add its newly-created HW-module to the subsystem with which its transformations are most compatible. If the new object’s average compatibility with all the existing subsystems is below a threshold, then create a new subsystem for the newly learned object. Repeat this procedure for each object—sampled according to the distribution of objects encountered in natural vision (or whatever approximation is available). See <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref> for the algorithm’s pseudocode. <xref ref-type="fig" rid="pcbi.1004390.g004">Fig 4</xref> shows example clusters obtained by this method.</p>
<fig id="pcbi.1004390.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004390.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Example clustering results.</title>
<p>Three example clusters that developed in a simulation with an object distribution biased against faces (the same simulation as in <xref ref-type="supplementary-material" rid="pcbi.1004390.s009">S8C</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s010">S9C</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1004390.s011">S10C</xref> Figs).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.g004"/>
</fig>
<p>Robust face and body clusters always appeared (<xref ref-type="fig" rid="pcbi.1004390.g005">Fig 5</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s009">S8</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s010">S9</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1004390.s011">S10</xref> Figs). Due to the strong effect of <inline-formula id="pcbi.1004390.e025"><alternatives><graphic id="pcbi.1004390.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e025"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, a face cluster formed even when the distribution of objects was biased <italic>against</italic> faces as in <xref ref-type="fig" rid="pcbi.1004390.g005">Fig 5</xref>. Most of the other objects ended up in very small clusters consisting of just a few objects. For the experiment of Figs <xref ref-type="fig" rid="pcbi.1004390.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1004390.g005">5</xref>, 16% of the bodies, 64% of the animals, 44% of the chairs, and 22% of the vehicles were in clusters consisting of just one object. No faces ended up in single-object clusters.</p>
<fig id="pcbi.1004390.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004390.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Simulation of the development of domain-specific regions.</title>
<p>In this case the distribution of objects was biased against faces (faces were only 16 of the 156 objects in this simulation). Depth-rotation is the only transformation used here. The main assumption is that the distance along cortex between two HW-modules for two different templates is proportional to how similarly the two templates transform. See <xref ref-type="supplementary-material" rid="pcbi.1004390.s009">S8</xref>, <xref ref-type="supplementary-material" rid="pcbi.1004390.s010">S9</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1004390.s011">S10</xref> Figs for results of the analogous simulations using different object distributions <bold>A.</bold> Multidimensional scaling plot based on pairwise transformation compatibility <italic>ψ</italic>. <bold>B.</bold> Results on a test of view-invariant face verification (same-different matching). Each bar corresponds to a different cluster produced by an iterative clustering algorithm based on <inline-formula id="pcbi.1004390.e026"><alternatives><graphic id="pcbi.1004390.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e026"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> which models visual development—see supplementary methods. The labels on the abscissa correspond to the dominant category in the cluster. <bold>C.</bold> Basic-level categorization results: Cars versus airplanes. Error bars were obtained by repeating the experiment 5 times, presenting the objects in a different random order during development and randomly choosing different objects for the test set.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.g005"/>
</fig>
<p>To confirm that <italic>ψ</italic>-based clustering is useful for object recognition with these images, we compared the recognition performance of the subsystems to the complete system that was trained using all available templates irrespective of their cluster assignment. We simulated two recognition tasks: one basic-level categorization task, view-invariant cars vs. airplanes, and one subordinate-level task, view-invariant face recognition. For these tests, each “trial” consisted of a pair of images. In the face recognition task, the goal was to respond ‘same’ if the two images depicted the same individual. In the cars vs. airplanes case, the goal was to respond ‘same’ if both images depicted objects of the same category. In both cases, all the objects in the cluster were used as templates; the test sets were completely disjoint. The classifier was the same as in Figs <xref ref-type="fig" rid="pcbi.1004390.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1004390.g003">3</xref>. In this case, the threshold was optimized on a held out training set.</p>
<p>As expected from the theory, performance on the subordinate-level view-invariant face recognition task was significantly higher when the face cluster was used (<xref ref-type="fig" rid="pcbi.1004390.g005">Fig 5B</xref>). The basic-level categorization task was performed to similar accuracy using any of the clusters (<xref ref-type="fig" rid="pcbi.1004390.g005">Fig 5C</xref>). This confirms that invariance to class-specific transformations is only necessary for subordinate level tasks.</p>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>We explored implications of the hypothesis that achieving transformation invariance is the main goal of the ventral stream. Invariance from a single example could be achieved for group transformations in a generic way. However, for non-group transformations, only approximate invariance is possible; and even for that, it is necessary to have experience with objects that transform similarly. This implies that the optimal organization of the ventral stream is one that facilitates the transfer of invariance within—but not between—object categories. Assuming that a subsystem must reside in a localized cortical neighborhood, this could explain the function of domain-specific regions in the ventral stream’s recognition algorithm: to enable subordinate level identification of novel objects from a single example.</p>
<p>Following on from our analysis implicating transformation compatibility as the key factor determining when invariance can be productively transferred between objects, we simulated the development of visual cortex using a clustering algorithm based on transformation compatibility. This allowed us to address the question of why faces, bodies, and words get their own dedicated regions but other object categories (apparently) do not [<xref ref-type="bibr" rid="pcbi.1004390.ref008">8</xref>]. This question has not previously been the focus of theoretical study.</p>
<p>Despite the simplicity of our model, we showed that it robustly yields face and body clusters across a range of object frequency assumptions. We also used the model to confirm two theoretical predictions: (1) that invariance to non-group transformations is only needed for subordinate level identification; and (2) that clustering by transformation compatibility yields subsystems that improve performance beyond that of the system trained using data from all categories. These results motivate the the next phase of this work: building biologically-plausible models that learn from natural video. Such models automatically incorporate a better estimate of the natural object distribution. Variants of these models may be able to quantitatively reproduce human level performance on simultaneous multi-category subordinate level (i.e., fine-grained) visual recognition tasks and potentially find application in computer vision as well as neuroscience. In [<xref ref-type="bibr" rid="pcbi.1004390.ref042">42</xref>], we report encouraging preliminary results along these lines.</p>
<p>Why are there domain-specific regions in later stages of the ventral stream hierarchy but not in early visual areas [<xref ref-type="bibr" rid="pcbi.1004390.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref003">3</xref>]? The templates used to implement invariance to group transformations need not be changed for different object classes while the templates implementing non-group invariance are class-specific. Thus it is efficient to put the generic circuitry of the first regime in the hierarchy’s early stages, postponing the need to branch to different domain-specific regions tuned to specific object classes until later, i.e., more anterior, stages. In the macaque face-processing system, category selectivity develops in a series of steps; posterior face regions are less face selective than anterior ones [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref083">83</xref>]. Additionally, there is a progression from a view-specific face representation in earlier regions to a view-tolerant representation in the most anterior region [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>]. Both findings could be accounted for in a face-specific hierarchical model that increases in template size and pooling region size with each subsequent layer (e.g., [<xref ref-type="bibr" rid="pcbi.1004390.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref084">84</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref085">85</xref>]). The use of large face-specific templates may be an effective way to gate the entrance to the face-specific subsystem so as to keep out spurious activations from non-faces. The algorithmic effect of large face-specific templates is to confer tolerance to clutter [<xref ref-type="bibr" rid="pcbi.1004390.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref042">42</xref>]. These results are particularly interesting in light of models showing that large face templates are sufficient to explain holistic effects observed in psychophysics experiments [<xref ref-type="bibr" rid="pcbi.1004390.ref073">73</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref086">86</xref>].</p>
<p>As stated in the introduction, properties of the ventral stream are thought to be determined by three factors: (1) computational and algorithmic constraints; (2) biological implementation constraints; and (3) the contingencies of the visual environment [<xref ref-type="bibr" rid="pcbi.1004390.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref022">22</xref>]. Up to now, we have stressed the contribution of factor (1) over the others. In particular, we have almost entirely ignored factor (2). We now discuss the role played by anatomical considerations in this account of ventral stream function.</p>
<p>That the the circuitry comprising a subsystem must be localized on cortex is a key assumption of this work. In principle, any HW-module could be anywhere, as long as the wiring all went to the right place. However, there are several reasons to think that the actual constraints under which the brain operates and its available information processing mechanisms favor a situation in which, at each level of the hierarchy, all the specialized circuitry for one domain is in a localized region of cortex, separate from the circuitry for other domains. Wiring length considerations are likely to play a role here [<xref ref-type="bibr" rid="pcbi.1004390.ref087">87</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref090">90</xref>]. Another possibility is that localization on cortex enables the use of neuromodulatory mechanisms that act on local neighborhoods of cortex to affect all the circuitry for a particular domain at once [<xref ref-type="bibr" rid="pcbi.1004390.ref091">91</xref>].</p>
<p>There are other domain-specific regions in the ventral stream besides faces and bodies; we consider several of them in light of our results here. It is possible that even more regions for less-common (or less transformation-compatible) object classes would appear with higher resolution scans. One example may be the fruit area, discovered in macaques with high-field fMRI [<xref ref-type="bibr" rid="pcbi.1004390.ref003">3</xref>].
<list list-type="order"><list-item><p><bold>Lateral Occipital Complex (LOC) [<xref ref-type="bibr" rid="pcbi.1004390.ref082">82</xref>]</bold></p> <p>These results imply that LOC is not really a dedicated region for general object processing. Rather, it is a heterogeneous area of cortex containing many domain-specific regions too small to be detected with the resolution of fMRI. It may also include clusters that are not dominated by one object category as we sometimes observed appearing in simulations (see <xref ref-type="fig" rid="pcbi.1004390.g004">Fig 4</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref>).</p></list-item> <list-item><p><bold>The Visual Word Form Area (VWFA) [<xref ref-type="bibr" rid="pcbi.1004390.ref004">4</xref>]</bold></p> <p>In addition to the generic transformations that apply to all objects, printed words undergo several non-generic transformations that never occur with other objects. We can read despite the large image changes occurring when a page is viewed from a different angle. Additionally, many properties of printed letters change with typeface, but our ability to read—even in novel fonts—is preserved. Reading hand-written text poses an even more severe version of the same computational problem. Thus, VWFA is well-accounted for by the invariance hypothesis. Words are frequently-viewed stimuli which undergo class-specific transformations. This account appears to be in accord with others in the literature [<xref ref-type="bibr" rid="pcbi.1004390.ref092">92</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref093">93</xref>].</p></list-item> <list-item><p><bold>Parahippocampal Place Area (PPA) [<xref ref-type="bibr" rid="pcbi.1004390.ref094">94</xref>]</bold></p> <p>A recent study by Kornblith et al. describes properties of neurons in two macaque scene-selective regions deemed the lateral and medial place patches (LPP and MPP) [<xref ref-type="bibr" rid="pcbi.1004390.ref095">95</xref>]. While homology has not been definitively established, it seems likely that these regions are homologous to the human PPA [<xref ref-type="bibr" rid="pcbi.1004390.ref096">96</xref>]. Moreover, this scene-processing network may be analogous to the face-processing hierarchy of [<xref ref-type="bibr" rid="pcbi.1004390.ref034">34</xref>]. In particular, MPP showed weaker effects of viewpoint, depth, and objects than LPP. This is suggestive of a scene-processing hierarchy that computes a representation of scene-identity that is (approximately) invariant to those factors. Any of them might be transformations for which this region is compatible in the sense of our theory. One possibility, which we considered in preliminary work, is that invariant perception of scene identity despite changes in monocular depth signals driven by traversing a scene (e.g., linear perspective) could be discounted in the same manner as face viewpoint. It is possible that putative scene-selective categories compute depth-tolerant representations. We confirmed this for the special case of long hallways differing in the placement of objects along the walls: a view-based model that pools over images of template hallways can be used to recognize novel hallways [<xref ref-type="bibr" rid="pcbi.1004390.ref097">97</xref>]. Furthermore, fast same-different judgements of scene identity tolerate substantial changes in perspective depth [<xref ref-type="bibr" rid="pcbi.1004390.ref097">97</xref>]. Of course, this begs the question: of what use would be a depth-invariant scene representation? One possibility could be to provide a landmark representation suitable for anchoring a polar coordinate system [<xref ref-type="bibr" rid="pcbi.1004390.ref098">98</xref>]. Intriguingly, [<xref ref-type="bibr" rid="pcbi.1004390.ref095">95</xref>] found that cells in the macaque scene-selective network were particularly sensitive to the presence of long straight lines—as might be expected in an intermediate stage on the way to computing perspective invariance.</p></list-item></list></p>
<p>Is this proposal at odds with the literature emphasizing the view-dependence of human vision when tested on subordinate level tasks with unfamiliar examples—e.g. [<xref ref-type="bibr" rid="pcbi.1004390.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref079">79</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref099">99</xref>]? We believe it is consistent with most of this literature. We merely emphasize the substantial view-<italic>tolerance</italic> achieved for certain object classes, while they emphasize the lack of complete invariance. Their emphasis was appropriate in the context of earlier debates about view-invariance [<xref ref-type="bibr" rid="pcbi.1004390.ref100">100</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref103">103</xref>], and before differences between the view-tolerance achieved on basic-level and subordinate-level tasks were fully appreciated [<xref ref-type="bibr" rid="pcbi.1004390.ref104">104</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref106">106</xref>].</p>
<p>The view-dependence observed in experiments with novel faces [<xref ref-type="bibr" rid="pcbi.1004390.ref072">72</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref107">107</xref>] is consistent with the predictions of our theory. The 3D structure of faces does not vary wildly within the class, but there is still some significant variation. It is this variability in 3D structure within the class that is the source of the imperfect performance in our simulations. Many psychophysical experiments on viewpoint invariance were performed with synthetic “wire” objects defined entirely by their 3D structure e.g., [<xref ref-type="bibr" rid="pcbi.1004390.ref079">79</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref081">81</xref>]. We found that they were by far, the least transformation-compatible (lowest <inline-formula id="pcbi.1004390.e027"><alternatives><graphic id="pcbi.1004390.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e027"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>) objects we tested (<xref ref-type="table" rid="pcbi.1004390.t001">Table 1</xref>). Thus our proposal predicts particularly weak performance on viewpoint-tolerance tasks with novel examples of these stimuli and that is precisely what is observed [<xref ref-type="bibr" rid="pcbi.1004390.ref080">80</xref>].</p>
<table-wrap id="pcbi.1004390.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004390.t001</object-id>
<label>Table 1</label>
<caption>
<title>Table of transformation compatibilities.</title>
<p>COIL-100 is a library of images of 100 common household items photographed from a range of orientations using a turntable [<xref ref-type="bibr" rid="pcbi.1004390.ref114">114</xref>]. The wire objects resemble those used in psychophysics and physiology experiments: [<xref ref-type="bibr" rid="pcbi.1004390.ref079">79</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref081">81</xref>]. They were generated according to the same protocol as in those studies.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004390.t001g" mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.t001"/>
<table frame="box" rules="all" border="0">
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="1" colspan="1">Object class</th>
<th align="center" rowspan="1" colspan="1">Transformation</th>
<th align="center" rowspan="1" colspan="1">
<inline-formula id="pcbi.1004390.e028">
<alternatives>
<graphic id="pcbi.1004390.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e028"/>
<mml:math id="M28" display="inline" overflow="scroll">
<mml:mrow>
<mml:mover accent="true">
<mml:mi>ψ</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="1" colspan="1">Chairs</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">0.00540</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">
<xref ref-type="fig" rid="pcbi.1004390.g003">Fig 3</xref> faces</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">0.57600</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">
<xref ref-type="fig" rid="pcbi.1004390.g003">Fig 3</xref> class B</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">0.95310</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">
<xref ref-type="fig" rid="pcbi.1004390.g003">Fig 3</xref> class C</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">0.83800</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">
<xref ref-type="fig" rid="pcbi.1004390.g003">Fig 3</xref> all classes</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">0.26520</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">COIL-100 [<xref ref-type="bibr" rid="pcbi.1004390.ref114">114</xref>]</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">0.00630</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Wire objects [<xref ref-type="bibr" rid="pcbi.1004390.ref080">80</xref>]</td>
<td align="center" rowspan="1" colspan="1">Rotation in depth</td>
<td align="char" char="." rowspan="1" colspan="1">-0.00007</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Tarr and Gauthier (1998) found that learned viewpoint-dependent mechanisms could generalize across members of a homogenous object class [<xref ref-type="bibr" rid="pcbi.1004390.ref106">106</xref>]. They tested both homogenous block-like objects, and several other classes of more complex novel shapes. They concluded that this kind of generalization was restricted to visually similar objects. These results seem to be consistent with our proposal. Additionally, our hypothesis predicts better within-class generalization for object classes with higher <inline-formula id="pcbi.1004390.e029"><alternatives><graphic id="pcbi.1004390.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e029"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. That is, transformation compatibility, not visual similarity per se, may be the factor influencing the extent of within-class generalization of learned view-tolerance. Though, in practice, the two are usually correlated and hard to disentangle. In a related experiment, Sinha and Poggio (1996) showed that the perception of an ambiguous transformation’s rigidity could be biased by experience [<xref ref-type="bibr" rid="pcbi.1004390.ref108">108</xref>]. View-based accounts of their results predict that the effect would generalize to novel objects of the same class. Since this effect can be obtained with particularly simple stimuli, it might be possible to design them so as to separate specific notions of visual similarity and transformation compatibility. In accord with our prediction that group transformations ought to be discounted earlier in the recognition process, [<xref ref-type="bibr" rid="pcbi.1004390.ref108">108</xref>] found that their effect was spared by presenting the training and test objects at different scales.</p>
<p>Many authors have argued that seemingly domain-specific regions are actually explained by perceptual expertise [<xref ref-type="bibr" rid="pcbi.1004390.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1004390.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref109">109</xref>]. Our account is compatible with some aspects of this idea. However, it is largely agnostic about whether the sorting of object classes into subsystems takes place over the course of evolution or during an organism’s lifetime. A combination of both is also possible—e.g. as in [<xref ref-type="bibr" rid="pcbi.1004390.ref110">110</xref>]. That said, our proposal does intersect this debate in several ways.
<list list-type="order"><list-item><p>Our theory agrees with most expertise-based accounts that subordinate-level identification is the relevant task.</p></list-item> <list-item><p>The expertise argument has always relied quite heavily on the idea that discriminating individuals from similar distractors is somehow difficult. Our account allows greater precision: the precise component of difficulty that matters is invariance to non-group transformations.</p></list-item> <list-item><p>Our theory predicts a critical factor determining which objects could be productively grouped into a module that is clearly formulated and operationalized: the transformation compatibility <inline-formula id="pcbi.1004390.e030"><alternatives><graphic id="pcbi.1004390.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e030"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>.</p></list-item></list></p>
<p>Under our account, domain-specific regions arise because they are needed in order to facilitate the generalization of learned transformation invariance to novel category-members. Most studies of clustering and perceptual expertise do not use this task. However, Srihasam et al. tested a version of the perceptual expertise hypothesis that could be understood in this way [<xref ref-type="bibr" rid="pcbi.1004390.ref111">111</xref>]. They trained macaques to associate reward amounts with letters and numerals (26 symbols). In each trial, a pair of symbols were displayed and the task was to pick the symbol associated with greater reward. Importantly, the 3-year training process occurred in the animal’s home cage and eye tracking was not used. Thus, the distance and angle with which the monkey subjects viewed the stimuli was not tightly controlled during training. The symbols would have projected onto their retina in many different ways. These are exactly the same transformations that we proposed are the reason for the VWFA. In accord with our prediction, Srihasam et al. found that this training experience caused the formation of category-selective regions in the temporal lobe. Furthermore, the same regions were activated selectively irrespective of stimulus size, position, and font. Interestingly, this result only held for juvenile macaques, implying there may be a critical period for cluster formation [<xref ref-type="bibr" rid="pcbi.1004390.ref111">111</xref>].</p>
<p>Our main prediction is the link between transformation compatibility and domain-specific clustering. Thus one way to test whether this account of expertise-related clustering is correct could be to train monkeys to recognize individual objects of unfamiliar classes invariantly to 3D rotation in depth. The task should involve generalization from a single example view of a novel exemplar. The training procedure should involve exposure to videos of a large number of objects from each category undergoing rotations in depth. Several categories with different transformation compatibilities should be used. The prediction is that after training there will be greater clustering of selectivity for the classes with greater average transformation compatibility (higher <inline-formula id="pcbi.1004390.e031"><alternatives><graphic id="pcbi.1004390.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e031"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ψ</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>). Furthermore, if one could record from neurons in the category-selective clusters, the theory would predict some similar properties to the macaque face-processing hierarchy: several interconnected regions progressing from view-specificity in the earlier regions to view-tolerance in the later regions. However, unless the novel object classes actually transform like faces, the clusters produced by expertise should be parallel to the face clusters but separate from them.</p>
<p>How should these results be understood in light of recent reports of very strong performance of “deep learning” computer vision systems employing apparently generic circuitry for object recognition tasks e.g., [<xref ref-type="bibr" rid="pcbi.1004390.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref112">112</xref>]? We think that exhaustive greedy optimization of parameters (weights) over a large labeled data set may have found a network similar to the architecture we describe since all the basic structural elements (neurons with nonlinearities, pooling, dot products, layers) required by our theory are identical to the elements in deep learning networks. If this were true, our theory would also explain what these networks do and why they work.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec009">
<title>Training HW-architectures</title>
<p>An <italic>HW-architecture</italic> refers to a feedforward hierarchical network of <italic>HW-layers</italic>. An HW-layer consists of <italic>K</italic> <italic>HW-modules</italic> arranged in parallel to one another (see <xref ref-type="fig" rid="pcbi.1004390.g001">Fig 1B</xref>). For an input image <italic>I</italic>, the output of an HW-layer is a vector <italic>μ</italic>(<italic>I</italic>) with <italic>K</italic> elements. If <italic>I</italic> depicts a particular object, then <italic>μ</italic>(<italic>I</italic>) is said to be the <italic>signature</italic> of that object.</p>
<p>The parameters (weights) of the <italic>k</italic>-th HW-module are uniquely determined by its <italic>template book</italic> <disp-formula id="pcbi.1004390.e032"><alternatives><graphic id="pcbi.1004390.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e032"/><mml:math id="M32" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>For all simulations in this paper, the output of the <italic>k</italic>-th HW-module is given by
<disp-formula id="pcbi.1004390.e033"><alternatives><graphic id="pcbi.1004390.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e033"/><mml:math id="M33" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi>I</mml:mi> <mml:mo>∥</mml:mo> <mml:mo>∥</mml:mo> <mml:mi>t</mml:mi> <mml:mo>∥</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>We used a nonparametric method of training HW-modules that models the outcome of temporal continuity-based unsupervised learning [<xref ref-type="bibr" rid="pcbi.1004390.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1004390.ref067">67</xref>]. In each experiment, the training data consisted of <italic>K</italic> videos represented as sequences of frames. Each video depicted the transformation of just one object. Let <italic>G</italic><sub>0</sub> be a family of transformations, e.g., a subset of the group of translations or rotations. The set of frames in the <italic>k</italic>-th video was <italic>O</italic><sub><italic>t</italic><sub><italic>k</italic></sub></sub> = {<italic>gt</italic><sub><italic>k</italic></sub> ∣ <italic>g</italic> ∈ <italic>G</italic><sub>0</sub>}.</p>
<p>In each simulation, an HW-layer consisting of <italic>K</italic> HW-modules was constructed. The template book <inline-formula id="pcbi.1004390.e034"><alternatives><graphic id="pcbi.1004390.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e034"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:math></alternatives></inline-formula><sub><italic>k</italic></sub> of the <italic>k</italic>-th HW-module was chosen to be
<disp-formula id="pcbi.1004390.e035"><alternatives><graphic id="pcbi.1004390.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e035"/><mml:math id="M35" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>g</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mspace width="3.33333pt"/><mml:mo stretchy="false">|</mml:mo> <mml:mspace width="3.33333pt"/><mml:mi>g</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>Note that HW-architectures are usually trained in a layer-wise manner (e.g., [<xref ref-type="bibr" rid="pcbi.1004390.ref057">57</xref>]). That is, layer ℓ templates are encoded as “neural images” using the outputs of layer ℓ − 1. However, in this paper, all the simulations use a single HW-layer.</p>
<p>One-layer HW-architectures are a particularly stylized abstraction of the ventral stream hierarchy. With our training procedure, they have no free parameters at all. This makes them ideal for simulations in which the aim is not to quantitatively reproduce experimental phenomena, but rather to study general principles of cortical computation that constrain all levels of the hierarchy alike.</p>
</sec>
<sec id="sec010">
<title>Experiment 1 and 2: The test of transformation-tolerance from a single example view</title>
<sec id="sec011">
<title>Procedure</title>
<p>The training set consisted of transformation sequences of <italic>K</italic> template objects. At test time, in each trial the reference image was presented at the 0 transformation parameter (either 0°, or the center of the image for experiment 1 and 2 respectively). In each trial, a number of query images were presented, 50% of which were targets. The signature of the reference image was computed and its Pearson correlation compared with each query image. This allowed the plotting of an ROC curve by varying the acceptance threshold. The statistic reported on the ordinate of Figs <xref ref-type="fig" rid="pcbi.1004390.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1004390.g003">3</xref> was the area under the ROC curve averaged over all choices of reference image and all resampled training and testing sets.</p>
</sec>
</sec>
<sec id="sec012">
<title>1. Translation experiments (<xref ref-type="fig" rid="pcbi.1004390.g002">Fig 2</xref>)</title>
<sec id="sec013">
<title>Stimuli</title>
<p>There were 100 faces and 100 random noise patterns in the dataset. For each repetition of the experiment, two disjoint sets of 30 objects were selected at random from the 100. The first was used as the template set and the second was used as the test set. Each experiment was repeated 5 times with different random choices of template and testing sets. The error bars on the ordinate of <xref ref-type="fig" rid="pcbi.1004390.g002">Fig 2</xref> are ±1 standard deviation computed over the 5 repetitions.</p>
</sec>
</sec>
<sec id="sec014">
<title>2. Rotation in depth experiments (<xref ref-type="fig" rid="pcbi.1004390.g003">Fig 3</xref>)</title>
<sec id="sec015">
<title>Stimuli</title>
<p>All objects were rendered with perspective projection. For rotation in depth experiments, the complete set of objects consisted of 40 untextured faces, 20 class B objects, and 20 class C objects. For each of the 20 repetitions of the experiment, 10 template objects and 10 test objects were randomly selected. The template and test sets were chosen independently and were always disjoint. Each face/object was rendered (using Blender [<xref ref-type="bibr" rid="pcbi.1004390.ref075">75</xref>]) at each orientation in 5° increments from −95° to 95°. The untextured face models were generated using Facegen [<xref ref-type="bibr" rid="pcbi.1004390.ref074">74</xref>].</p>
</sec>
</sec>
<sec id="sec016">
<title>Experiment 3: Transformation compatibility, multidimensional scaling and online clustering experiments (Figs <xref ref-type="fig" rid="pcbi.1004390.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1004390.g005">5</xref>)</title>
<sec id="sec017">
<title>Stimuli: Faces, bodies, vehicles, chairs and animals</title>
<p>Blender was used to render images of 3D models from two sources: 1. the Digimation archive (platinum edition), and 2. Facegen. Each object was rendered at a range of viewpoints: −90° to 90° in increments of 5 degrees. This procedure produced a transformation sequence for each object, i.e., a video. The full Digimation set consisted of ∼ 10,000 objects. However, our simulations only used textured objects from the following categories: bodies, vehicles, chairs, and animals. For each experiment, the number of objects used from each class is listed in <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S2 Table</xref>. A set of textured face models generated with FaceGen were added to the Digimation set. See <xref ref-type="supplementary-material" rid="pcbi.1004390.s008">S7 Fig</xref> for examples.</p>
<p>In total, 23,791 images were rendered for this experiment. The complete dataset is available from cbmm.mit.edu.</p>
</sec>
<sec id="sec018">
<title>Procedure</title>
<p>Let <italic>A</italic><sub><italic>i</italic></sub> be the <italic>i</italic><sub><italic>th</italic></sub> frame of the video of object A transforming and <italic>B</italic><sub><italic>i</italic></sub> be the <italic>i</italic><sub><italic>th</italic></sub> frame of the video of object B transforming. Define a compatibility function <italic>ψ</italic>(<italic>A</italic>, <italic>B</italic>) to quantify how similarly objects A and B transform.</p>
<p>First, approximate the Jacobian of a transformation sequence by the “video” of difference images: <italic>J</italic><sub><italic>A</italic></sub>(<italic>i</italic>) = ∣<italic>A</italic><sub><italic>i</italic></sub> − <italic>A</italic><sub><italic>i</italic>+1</sub>∣ (∀<italic>i</italic>).</p>
<p>Then define the pairwise transformation compatibility as:
<disp-formula id="pcbi.1004390.e036"><alternatives><graphic id="pcbi.1004390.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e036"/><mml:math id="M36" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:mrow><mml:msub><mml:mi>J</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>J</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:msub><mml:mi>J</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∥</mml:mo> <mml:mo>∥</mml:mo></mml:mrow> <mml:msub><mml:mi>J</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∥</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<p>Transformation compatibility can be visualized by Multidimensional Scaling (MDS) [<xref ref-type="bibr" rid="pcbi.1004390.ref113">113</xref>]. The input to the MDS algorithm is the pairwise similarity matrix containing the transformation compatibilities between all pairs of objects.</p>
<p>For the <italic>ψ</italic>-based online clustering experiments, consider a model consisting of a number of subsystems (HW-architectures). The clustering procedure was as follows: At each step a new object is learned. Its newly-created HW-module is added to the subsystem with which its transformations are most compatible. If the new object’s average compatibility with all the existing subsystems is below a threshold, then create a new subsystem for the newly learned object. Repeat this procedure for each object.</p>
<p>The objects for this experiment were sampled from three different distributions: “realistic” distribution, uniform distribution, and the biased against faces distribution, see <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S2 Table</xref> for the numbers of objects used from each class.</p>
<p>The algorithm’s pseudocode is in <xref ref-type="supplementary-material" rid="pcbi.1004390.s001">S1 Text</xref> (Section 5.3). <xref ref-type="fig" rid="pcbi.1004390.g004">Fig 4</xref> shows examples of clusters obtained by this method.</p>
</sec>
</sec>
<sec id="sec019">
<title>Experiment 4: Evaluating the clustered models on subordinate-level and basic-level tasks (<xref ref-type="fig" rid="pcbi.1004390.g005">Fig 5</xref>)</title>
<sec id="sec020">
<title>Stimuli</title>
<p>The stimuli were the same as in experiment 3.</p>
</sec>
<sec id="sec021">
<title>Procedure</title>
<p>To confirm that <italic>ψ</italic>-based clustering is useful for object recognition with these images, we compared the recognition performance of the subsystems to the complete system that was trained using all available templates irrespective of their subsystem assignment.</p>
<p>Two recognition tasks were simulated: one basic level categorization task, view-invariant cars vs. airplanes, and one subordinate level task, view-invariant face recognition. For the subordinate face recognition task, a pair of face images were given, the task was to determine whether they depict the same person (positive) or not (negative). For basic level categorization, a pair of car/airplane images were given; the task was to determine whether they depicted the same basic-level category or not. That is, whether two images are both cars (positive), both airplanes (positive) or one airplane and one car (negative). The classifier used for both tasks was the same as the one used for experiments 1 and 2: for each test pair, the Pearson correlation between the two signatures was compared to a threshold. The threshold was optimized on a disjoint training set.</p>
<p>For each cluster, an HW-architecture was trained using only the objects in that cluster. If there were <italic>K</italic> objects in the cluster, then its HW-architecture had <italic>K</italic> HW-modules. Applying <xref ref-type="disp-formula" rid="pcbi.1004390.e035">Eq (7)</xref>, each HW-module’s template book was the set of frames from the transformation video of one of the objects in the cluster. For both tasks, in the test phase, the signature of each test image was computed with <xref ref-type="disp-formula" rid="pcbi.1004390.e033">Eq (6)</xref>.</p>
<p>Since the clustering procedure depends on the order in which the objects were presented, for each of the 3 object distributions, we repeated the basic-level and subordinate level recognition tasks 5 times using different random presentation orders. The error bars in <xref ref-type="fig" rid="pcbi.1004390.g005">Fig 5B and 5C</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1004390.s011">S10 Fig</xref> convey the variability (one standard deviation) arising from presentation order.</p>
<p><bold>Evaluation parameters</bold>:
<list list-type="bullet"><list-item><p>60 new face objects (disjoint from the clustering set)</p></list-item> <list-item><p>Data was evenly split to 5 folds, 12 objects per fold.</p></list-item> <list-item><p>For each fold, 48 objects were used for threshold optimization. For the face recognition case, 12 faces were used for testing. For the basic-level case, 12 objects of each category were used for testing.</p></list-item> <list-item><p>For each fold, 4000 pairs were used to learn the classification threshold <italic>θ</italic> (see below), 4000 pairs for testing.</p></list-item> <list-item><p>Performance was averaged over all folds.</p></list-item></list></p>
</sec>
</sec>
</sec>
<sec id="sec022">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004390.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supplementary Information.</title>
<p>Text S1</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s002" mimetype="image/tiff" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>It is hypothesized that properties of the ventral stream are determined by these three factors.</title>
<p>We are not the only ones to identify them in this way. For example, Simoncelli and Olshausen distinguished the same three factors [<xref ref-type="bibr" rid="pcbi.1004390.ref020">20</xref>]. The crucial difference between their <italic>efficient coding hypothesis</italic> and our <italic>invariance hypothesis</italic> is the particular computational task that we consider. In their case, the task is to provide an efficient representation of the visual world. In our case, the task is to provide an invariant signature supporting object recognition.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s003" mimetype="image/tiff" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Localization condition of the S-unit response for invariance under the transformation <inline-formula id="pcbi.1004390.e037"><alternatives><graphic id="pcbi.1004390.e037g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004390.e037"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>‾</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s004" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s004" mimetype="image/tiff" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>The Jacobians of the orbits of the image around the point <italic>p</italic> and the template must be approximately equal for <xref ref-type="disp-formula" rid="pcbi.1004390.e008">Eq (3)</xref> to hold in the case of smooth transformations.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s005" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s005" mimetype="image/tiff" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Class-specific transfer of illumination invariance.</title>
<p>Bottom panel (II): Example images from the three classes. Top panel (I): The left column shows the results of a test of illumination invariance on statues of heads made from different materials (class A), the middle column shows results for class B and the right column shows the results for class C. The view-based model (blue curve) was built using images from class A in the top row, class B in the middle row, and class C in the bottom row. The abscissa of each plot shows the maximum invariance range (arbitrary units of the light source’s vertical distance from its central position) over which target and distractor images were generated. The view-based model was never tested on any of the images that were used as templates. Error bars (+/- one standard deviation) were computed over 20 cross validation runs using different choices of template and test images.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s006" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s006" mimetype="image/tiff" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<p><bold>A.</bold> Example images for the pose-invariant body-recognition task. The images appearing in the training phase were used as templates. The test measures the model’s performance on a same-different task in which a reference image is compared to a query image. ‘Same’ responses are marked correct when the reference and query image depict the same body (invariantly to pose-variation).</p>
<p><bold>B.</bold> Model performance: area under the ROC curve (AUC) for the same-different task with 10 testing images. The X-axis indicates the number of bodies used to train the model. Performance was averaged over 10 cross-validation splits. The error bars indicate one standard deviation over splits.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s007" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s007" mimetype="image/tiff" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Two factors are conjectured to influence the development of domain-specific regions.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s008" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s008" mimetype="image/tiff" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Example object videos (transformation sequences) used in the <italic>ψ</italic>-based clustering experiments.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s009" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s009" mimetype="image/tiff" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Multidimensional Scaling (MDS) [<xref ref-type="bibr" rid="pcbi.1004390.ref113">113</xref>] visualizations of the object sets under the <italic>ψ</italic>(<italic>A</italic>, <italic>B</italic>)-dissimilarity metric for the three object distributions.</title>
<p>A. “realistic”, B. uniform, and C. biased against faces (see table).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s010" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s010" mimetype="image/tiff" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>The percentage of objects in the first N clusters containing the dominant category object (clusters sorted by number of objects in dominant category).</title>
<p>A, B and C are respectively, the “realistic” distribution, uniform distribution, and the biased against faces distribution (see table)). 100% of the faces go to the first face cluster—only a single face cluster developed in each experiment. Bodies were more “concentrated” in a small number of clusters, while the other objects were all scattered in many clusters—thus their curves rise slowly. These results were averaged over 5 repetitions of each clustering simulation using different randomly chosen objects.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004390.s011" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004390.s011" mimetype="image/tiff" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>The classification performance on face recognition, a subordinate-level task (top row) and car vs. airplane, a basic-level categorization task (bottom row) using templates from each cluster.</title>
<p>5-fold cross-validation, for each fold, the result from the best-performing cluster of each category is reported. A, B and C indicate “realistic”, uniform, and biased distributions respectively (see table). Note that performance on the face recognition task is strongest when using the face cluster while the performance on the basic-level car vs. airplane task is not stronger with the vehicle cluster (mostly cars and airplanes) than the others.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank users Bohemix and SoylentGreen of the Blender Open Material Repository for contributing the materials used to create the images for the illumination simulations (in supplementary information). We also thank Andrei Rusu, Leyla Isik, Chris Summerfield, Winrich Freiwald, Pawan Sinha, and Nancy Kanwisher for their comments on early versions of this manuscript, and Heejung Kim for her help preparing one of the supplementary figures.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004390.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>MM</given-names></name>. <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source>The Journal of Neuroscience</source>. <year>1997</year>;<volume>17</volume>(<issue>11</issue>):<fpage>4302</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.jneurosci.org/content/17/11/4302.short">http://www.jneurosci.org/content/17/11/4302.short</ext-link></comment> <object-id pub-id-type="pmid">9151747</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Knutsen</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Mandeville</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Tootell</surname> <given-names>RBH</given-names></name>. <article-title>Faces and objects in macaque cerebral cortex</article-title>. <source>Nature Neuroscience</source>. <year>2003</year>;<volume>6</volume>(<issue>9</issue>):<fpage>989</fpage>–<lpage>995</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.nature.com/neuro/journal/v6/n9/abs/nn1111.html">http://www.nature.com/neuro/journal/v6/n9/abs/nn1111.html</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1111" xlink:type="simple">10.1038/nn1111</ext-link></comment> <object-id pub-id-type="pmid">12925854</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ku</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Goense</surname> <given-names>J</given-names></name>. <article-title>fMRI of the Face-Processing Network in the Ventral Temporal Lobe of Awake and Anesthetized Macaques</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>70</volume>(<issue>2</issue>):<fpage>352</fpage>–<lpage>362</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0896627311002054">http://linkinghub.elsevier.com/retrieve/pii/S0896627311002054</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.02.048" xlink:type="simple">10.1016/j.neuron.2011.02.048</ext-link></comment> <object-id pub-id-type="pmid">21521619</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cohen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Naccache</surname> <given-names>L</given-names></name>. <article-title>The visual word form area</article-title>. <source>Brain</source>. <year>2000</year>;<volume>123</volume>(<issue>2</issue>):<fpage>291</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://brain.oxfordjournals.org/content/123/2/291.short">http://brain.oxfordjournals.org/content/123/2/291.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/123.2.291" xlink:type="simple">10.1093/brain/123.2.291</ext-link></comment> <object-id pub-id-type="pmid">10648437</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wald</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Kwong</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Benner</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>Visual word processing and experiential origins of functional selectivity in human extrastriate cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>21</issue>):<fpage>9087</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pnas.org/content/104/21/9087.short">http://www.pnas.org/content/104/21/9087.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0703300104" xlink:type="simple">10.1073/pnas.0703300104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Downing</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>Y</given-names></name>. <article-title>A cortical area selective for visual processing of the human body</article-title>. <source>Science</source>. <year>2001</year>;<volume>293</volume>(<issue>5539</issue>):<fpage>2470</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencemag.org/content/293/5539/2470.short">http://www.sciencemag.org/content/293/5539/2470.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1063414" xlink:type="simple">10.1126/science.1063414</ext-link></comment> <object-id pub-id-type="pmid">11577239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Spiridon</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>How distributed is visual category information in human occipito-temporal cortex?</article-title> <source>An fMRI study. Neuron</source>. <year>2002</year>;<volume>35</volume>(<issue>6</issue>):<fpage>1157</fpage>–<lpage>1165</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/S0896-6273(02)00877-2">http://dx.doi.org/10.1016/S0896-6273(02)00877-2</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(02)00877-2" xlink:type="simple">10.1016/S0896-6273(02)00877-2</ext-link></comment> <object-id pub-id-type="pmid">12354404</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>Functional specificity in the human brain: a window into the functional architecture of the mind</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year>;<volume>107</volume>(<issue>25</issue>):<fpage>11163</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pnas.org/content/107/25/11163.short">http://www.pnas.org/content/107/25/11163.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1005062107" xlink:type="simple">10.1073/pnas.1005062107</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ishai</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schouten</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Haxby</surname> <given-names>JV</given-names></name>. <article-title>Distributed representation of objects in the human ventral visual pathway</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1999</year>;<volume>96</volume>(<issue>16</issue>):<fpage>9379</fpage>–<lpage>9384</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1073/pnas.96.16.9379">http://dx.doi.org/10.1073/pnas.96.16.9379</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.96.16.9379" xlink:type="simple">10.1073/pnas.96.16.9379</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Haxby</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>Gobbini</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Furey</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Ishai</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schouten</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Pietrini</surname> <given-names>P</given-names></name>. <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source>. <year>2001</year>;<volume>293</volume>(<issue>5539</issue>):<fpage>2425</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencemag.org/content/293/5539/2425.short">http://www.sciencemag.org/content/293/5539/2425.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1063736" xlink:type="simple">10.1126/science.1063736</ext-link></comment> <object-id pub-id-type="pmid">11577229</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kravitz</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Saleem</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>, <name name-style="western"><surname>Mishkin</surname> <given-names>M</given-names></name>. <article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title>. <source>Trends in cognitive sciences</source>. <year>2013</year>;<volume>17</volume>(<issue>1</issue>):<fpage>26</fpage>–<lpage>49</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.tics.2012.10.011">http://dx.doi.org/10.1016/j.tics.2012.10.011</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2012.10.011" xlink:type="simple">10.1016/j.tics.2012.10.011</ext-link></comment> <object-id pub-id-type="pmid">23265839</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Behrmann</surname> <given-names>M</given-names></name>. <article-title>Complementary neural representations for faces and words: A computational exploration</article-title>. <source>Cognitive neuropsychology</source>. <year>2011</year>;<volume>28</volume>(<issue>3-4</issue>):<fpage>251</fpage>–<lpage>275</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1080/02643294.2011.609812">http://dx.doi.org/10.1080/02643294.2011.609812</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02643294.2011.609812" xlink:type="simple">10.1080/02643294.2011.609812</ext-link></comment> <object-id pub-id-type="pmid">22185237</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Levy</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Avidan</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hendler</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Malach</surname> <given-names>R</given-names></name>. <article-title>Center–periphery organization of human object areas</article-title>. <source>Nature neuroscience</source>. <year>2001</year>;<volume>4</volume>(<issue>5</issue>):<fpage>533</fpage>–<lpage>539</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1038/87490">http://dx.doi.org/10.1038/87490</ext-link></comment> <object-id pub-id-type="pmid">11319563</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Levy</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Behrmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hendler</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Malach</surname> <given-names>R</given-names></name>. <article-title>Eccentricity bias as an organizing principle for human high-order object areas</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>34</volume>(<issue>3</issue>):<fpage>479</fpage>–<lpage>490</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/s0896-6273(02)00662-1">http://dx.doi.org/10.1016/s0896-6273(02)00662-1</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(02)00662-1" xlink:type="simple">10.1016/S0896-6273(02)00662-1</ext-link></comment> <object-id pub-id-type="pmid">11988177</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Malach</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Levy</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>. <article-title>The topography of high-order human object areas</article-title>. <source>Trends in cognitive sciences</source>. <year>2002</year>;<volume>6</volume>(<issue>4</issue>):<fpage>176</fpage>–<lpage>184</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/s1364-6613(02)01870-3">http://dx.doi.org/10.1016/s1364-6613(02)01870-3</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(02)01870-3" xlink:type="simple">10.1016/S1364-6613(02)01870-3</ext-link></comment> <object-id pub-id-type="pmid">11912041</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Konkle</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>A real-world size organization of object responses in occipitotemporal cortex</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>(<issue>6</issue>):<fpage>1114</fpage>–<lpage>1124</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.04.036">http://dx.doi.org/10.1016/j.neuron.2012.04.036</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.04.036" xlink:type="simple">10.1016/j.neuron.2012.04.036</ext-link></comment> <object-id pub-id-type="pmid">22726840</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lafer-Sousa</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Conway</surname> <given-names>BR</given-names></name>. <article-title>Parallel, multi-stage processing of colors, faces and shapes in macaque inferior temporal cortex</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>12</issue>):<fpage>1870</fpage>–<lpage>1878</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1038/nn.3555">http://dx.doi.org/10.1038/nn.3555</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3555" xlink:type="simple">10.1038/nn.3555</ext-link></comment> <object-id pub-id-type="pmid">24141314</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>. <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory communication</source>. <year>1961</year>;p. <fpage>217</fpage>–<lpage>234</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.7551/mitpress/9780262518420.003.0013">http://dx.doi.org/10.7551/mitpress/9780262518420.003.0013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>. <source>Vision: A computational investigation into the human representation and processing of visual information</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Henry Holt and Co., Inc.</publisher-name>; <year>1982</year>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.7551/mitpress/9780262514620.001.0001">http://dx.doi.org/10.7551/mitpress/9780262514620.001.0001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>. <article-title>Natural image statistics and neural representation</article-title>. <source>Annual Review of Neuroscience</source>. <year>2001</year>;<volume>24</volume>(<issue>1</issue>):<fpage>1193</fpage>–<lpage>1216</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1088/0954-898x/7/2/014">http://dx.doi.org/10.1088/0954-898x/7/2/014</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.24.1.1193" xlink:type="simple">10.1146/annurev.neuro.24.1.1193</ext-link></comment> <object-id pub-id-type="pmid">11520932</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="other">Poggio T, Mutch J, Anselmi F, Leibo JZ, Rosasco L, Tacchetti A. The computational magic of the ventral stream: sketch of a theory (and why some deep architectures work). MIT-CSAIL-TR-2012-035. 2012; <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://hdl.handle.net/1721.1/76248">http://hdl.handle.net/1721.1/76248</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>. <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>434</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S089662731200092X">http://www.sciencedirect.com/science/article/pii/S089662731200092X</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment> <object-id pub-id-type="pmid">22325196</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gauthier</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Skudlarski</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gore</surname> <given-names>JC</given-names></name>. <article-title>Activation of the middle fusiform ‘face area’ increases with expertise in recognizing novel objects</article-title>. <source>nature neuroscience</source>. <year>1999</year>;<volume>2</volume>(<issue>6</issue>):<fpage>569</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1038/9224">http://dx.doi.org/10.1038/9224</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/9224" xlink:type="simple">10.1038/9224</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Gauthier</surname> <given-names>I</given-names></name>. <article-title>FFA: a flexible fusiform area for subordinate-level visual processing automatized by expertise</article-title>. <source>Nature Neuroscience</source>. <year>2000</year>;<volume>3</volume>:<fpage>764</fpage>–<lpage>770</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1038/77666">http://dx.doi.org/10.1038/77666</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/77666" xlink:type="simple">10.1038/77666</ext-link></comment> <object-id pub-id-type="pmid">10903568</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Palmeri</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Gauthier</surname> <given-names>I</given-names></name>. <article-title>Computational approaches to the development of perceptual expertise</article-title>. <source>Trends in cognitive sciences</source>. <year>2004</year>;<volume>8</volume>(<issue>8</issue>):<fpage>378</fpage>–<lpage>386</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.tics.2004.06.001">http://dx.doi.org/10.1016/j.tics.2004.06.001</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2004.06.001" xlink:type="simple">10.1016/j.tics.2004.06.001</ext-link></comment> <object-id pub-id-type="pmid">15335465</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dailey</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Cottrell</surname> <given-names>GW</given-names></name>. <article-title>Organization of face and object recognition in modular neural network models</article-title>. <source>Neural Networks</source>. <year>1999</year>;<volume>12</volume>(<issue>7-8</issue>):<fpage>1053</fpage>–<lpage>1074</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/s0893-6080(99)00050-7">http://linkinghub.elsevier.com/retrieve/pii/s0893-6080(99)00050-7</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(99)00050-7" xlink:type="simple">10.1016/S0893-6080(99)00050-7</ext-link></comment> <object-id pub-id-type="pmid">12662645</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wallis</surname> <given-names>G</given-names></name>. <article-title>Toward a unified model of face and object recognition in the human visual system</article-title>. <source>Frontiers in psychology</source>. <year>2013</year>;<volume>4</volume>(<issue>497</issue>). <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.3389/fpsyg.2013.00497">http://dx.doi.org/10.3389/fpsyg.2013.00497</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mahon</surname> <given-names>BZ</given-names></name>, <name name-style="western"><surname>Caramazza</surname> <given-names>A</given-names></name>. <article-title>What drives the organization of object knowledge in the brain?</article-title> <source>Trends in cognitive sciences</source>. <year>2011</year>;<volume>15</volume>(<issue>3</issue>):<fpage>97</fpage>–<lpage>103</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.tics.2011.01.004">http://dx.doi.org/10.1016/j.tics.2011.01.004</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2011.01.004" xlink:type="simple">10.1016/j.tics.2011.01.004</ext-link></comment> <object-id pub-id-type="pmid">21317022</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wada</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamamoto</surname> <given-names>T</given-names></name>. <article-title>Selective impairment of facial recognition due to a haematoma restricted to the right fusiform and lateral occipital region</article-title>. <source>Journal of Neurology, Neurosurgery &amp; Psychiatry</source>. <year>2001</year>;<volume>71</volume>(<issue>2</issue>):<fpage>254</fpage>–<lpage>257</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1136/jnnp.71.2.254">http://dx.doi.org/10.1136/jnnp.71.2.254</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1136/jnnp.71.2.254" xlink:type="simple">10.1136/jnnp.71.2.254</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barton</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Press</surname> <given-names>DZ</given-names></name>, <name name-style="western"><surname>Keenan</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>O?Connor</surname> <given-names>M</given-names></name>. <article-title>Lesions of the fusiform face area impair perception of facial configuration in prosopagnosia</article-title>. <source>Neurology</source>. <year>2002</year>;<volume>58</volume>(<issue>1</issue>):<fpage>71</fpage>–<lpage>78</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1212/wnl.58.1.71">http://dx.doi.org/10.1212/wnl.58.1.71</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1212/WNL.58.1.71" xlink:type="simple">10.1212/WNL.58.1.71</ext-link></comment> <object-id pub-id-type="pmid">11781408</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yin</surname> <given-names>RK</given-names></name>. <article-title>Looking at upside-down faces</article-title>. <source>Journal of experimental psychology</source>. <year>1969</year>;<volume>81</volume>(<issue>1</issue>):<fpage>141</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0027474" xlink:type="simple">10.1037/h0027474</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tanaka</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Farah</surname> <given-names>M</given-names></name>. <article-title>Parts and wholes in face recognition</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>. <year>1993</year>;<volume>46</volume>(<issue>2</issue>):<fpage>225</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/14640749308401045" xlink:type="simple">10.1080/14640749308401045</ext-link></comment> <object-id pub-id-type="pmid">8316637</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Huth</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Nishimoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vu</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>(<issue>6</issue>):<fpage>1210</fpage>–<lpage>1224</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.10.014" xlink:type="simple">10.1016/j.neuron.2012.10.014</ext-link></comment> <object-id pub-id-type="pmid">23259955</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Functional Compartmentalization and Viewpoint Generalization Within the Macaque Face-Processing System</article-title>. <source>Science</source>. <year>2010</year>;<volume>330</volume>(<issue>6005</issue>):<fpage>845</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencemag.org/cgi/content/abstract/330/6005/845">http://www.sciencemag.org/cgi/content/abstract/330/6005/845</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1194908" xlink:type="simple">10.1126/science.1194908</ext-link></comment> <object-id pub-id-type="pmid">21051642</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Moeller</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Patches with links: a unified system for processing faces in the macaque temporal lobe</article-title>. <source>Science</source>. <year>2008</year>;<volume>320</volume>(<issue>5881</issue>):<fpage>1355</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencemag.org/content/320/5881/1355.short">http://www.sciencemag.org/content/320/5881/1355.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1157436" xlink:type="simple">10.1126/science.1157436</ext-link></comment> <object-id pub-id-type="pmid">18535247</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tootell</surname> <given-names>RBH</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>MS</given-names></name>. <article-title>A cortical region consisting entirely of face-selective cells</article-title>. <source>Science</source>. <year>2006</year>;<volume>311</volume>(<issue>5761</issue>):<fpage>670</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencemag.org/content/311/5761/670.short">http://www.sciencemag.org/content/311/5761/670.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1119983" xlink:type="simple">10.1126/science.1119983</ext-link></comment> <object-id pub-id-type="pmid">16456083</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="other">Anselmi F, Leibo JZ, Mutch J, Rosasco L, Tacchetti A, Poggio T. Unsupervised Learning of Invariant Representations in Hierarchical Architectures. arXiv preprint. 2013; <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://arxiv.org/abs/1311.4158v5">http://arxiv.org/abs/1311.4158v5</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="other">Sundaramoorthi G, Petersen P, Varadarajan VS, Soatto S. On the set of images modulo viewpoint and contrast changes. In: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR); 2009. p. 832–839.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="other">Poggio T, Mutch J, Isik L. Computational role of eccentricity dependent cortical magnification. CBMM Memo No 017 arXiv preprint arXiv:14061770. 2014;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="other">Liao Q, Leibo JZ, Poggio T. Learning invariant representations and applications to face verification. In: Advances in Neural Information Processing Systems (NIPS). Lake Tahoe, CA; 2013.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="other">Liao Q, Leibo JZ, Mroueh Y, Poggio T. Can a biologically-plausible hierarchy effectively replace face detection, alignment, and recognition pipelines? CBMM Memo No 3 arXiv preprint arXiv:13114082. 2013;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="other">Liao Q, Leibo JZ, Poggio T. Unsupervised learning of clutter-resistant visual representations from natural videos. CBMM Memo No 023 arXiv preprint arXiv:14093879. 2014;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="other">Evangelopoulos G, Voinea S, Zhang C, Rosasco L, Poggio T. Learning An Invariant Speech Representation. arXiv preprint arXiv:14063884. 2014;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="other">Voinea S, Zhang C, Evangelopoulos G, Rosasco L, Poggio T. Word-level Invariant Representations From Acoustic Waveforms. In: Fifteenth Annual Conference of the International Speech Communication Association; 2014.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="other">Poggio T, Mutch J, Anselmi F, Tacchetti A, Rosasco L, Leibo JZ. Does invariant recognition predict tuning of neurons in sensory cortex? MIT-CSAIL-TR-2013-019, CBCL-313. 2013;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="other">Leibo JZ. The Invariance Hypothesis and the Ventral Stream. Massachusetts Institute of Technology; 2013. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://hdl.handle.net/1721.1/87458">http://hdl.handle.net/1721.1/87458</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="other">Isik L, Leibo JZ, Mutch J, Lee SW, Poggio T. A hierarchical model of peripheral vision. MIT-CSAIL-TR-2011-031, CBCL-300. 2011; <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://hdl.handle.net/1721.1/64621">http://hdl.handle.net/1721.1/64621</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Fodor</surname> <given-names>JA</given-names></name>. <source>The modularity of mind: An essay on faculty psychology</source>. <publisher-name>MIT press</publisher-name>; <year>1983</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Domain specificity in face perception</article-title>. <source>Nature neuroscience</source>. <year>2000</year>;<volume>3</volume>:<fpage>759</fpage>–<lpage>763</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/77664" xlink:type="simple">10.1038/77664</ext-link></comment> <object-id pub-id-type="pmid">10903567</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gauthier</surname> <given-names>I</given-names></name>. <article-title>What constrains the organization of the ventral temporal cortex?</article-title> <source>Trends in cognitive sciences</source>. <year>2000</year>;<volume>4</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>2</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(99)01416-3" xlink:type="simple">10.1016/S1364-6613(99)01416-3</ext-link></comment> <object-id pub-id-type="pmid">10637614</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>. <article-title>Uniformity of monkey striate cortex: a parallel relationship between field size, scatter, and magnification factor</article-title>. <source>The Journal of Comparative Neurology</source>. <year>1974</year>;<volume>158</volume>(<issue>3</issue>):<fpage>295</fpage>–<lpage>305</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://onlinelibrary.wiley.com/doi/10.1002/cne.901580305/abstract">http://onlinelibrary.wiley.com/doi/10.1002/cne.901580305/abstract</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.901580305" xlink:type="simple">10.1002/cne.901580305</ext-link></comment> <object-id pub-id-type="pmid">4436457</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Douglas</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>KAC</given-names></name>, <name name-style="western"><surname>Whitteridge</surname> <given-names>D</given-names></name>. <article-title>A canonical microcircuit for neocortex</article-title>. <source>Neural Computation</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>480</fpage>–<lpage>488</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.4.480">http://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.4.480</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1989.1.4.480" xlink:type="simple">10.1162/neco.1989.1.4.480</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fukushima</surname> <given-names>K</given-names></name>. <article-title>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biological Cybernetics</source>. <year>1980</year> <month>Apr</month>;<volume>36</volume>(<issue>4</issue>):<fpage>193</fpage>–<lpage>202</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.springerlink.com/content/r6g5w3tt54528137">http://www.springerlink.com/content/r6g5w3tt54528137</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00344251" xlink:type="simple">10.1007/BF00344251</ext-link></comment> <object-id pub-id-type="pmid">7370364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mel</surname> <given-names>BW</given-names></name>. <article-title>SEEMORE: Combining Color, Shape, and Texture Histogramming in a Neurally Inspired Approach to Visual Object Recognition</article-title>. <source>Neural Computation</source>. <year>1997</year> <month>May</month>;<volume>9</volume>(<issue>4</issue>):<fpage>777</fpage>–<lpage>804</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1162/neco.1997.9.4.777">http://dx.doi.org/10.1162/neco.1997.9.4.777</ext-link> <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.4.777">http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.4.777</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1997.9.4.777" xlink:type="simple">10.1162/neco.1997.9.4.777</ext-link></comment> <object-id pub-id-type="pmid">9161022</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature Neuroscience</source>. <year>1999</year> <month>Nov</month>;<volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>–<lpage>1025</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.7843&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.7843&amp;rep=rep1&amp;type=pdf</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/14819" xlink:type="simple">10.1038/14819</ext-link></comment> <object-id pub-id-type="pmid">10526343</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mutch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lowe</surname> <given-names>D</given-names></name>. <article-title>Multiclass object recognition with sparse, localized features</article-title>. <source>Computer Vision and Pattern Recognition 2006</source>. <year>2006</year>;<volume>1</volume>:<fpage>11</fpage>–<lpage>18</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1640736">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1640736</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bileschi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Robust Object Recognition with Cortex-Like Mechanisms</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2007</year>;<volume>29</volume>(<issue>3</issue>):<fpage>411</fpage>–<lpage>426</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2007.56" xlink:type="simple">10.1109/TPAMI.2007.56</ext-link></comment> <object-id pub-id-type="pmid">17224612</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Doukhan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>D</given-names></name>. <article-title>A high-throughput screening approach to discovering good forms of biologically inspired visual representation</article-title>. <source>PLoS Computational Biology</source>. <year>2009</year>;<volume>5</volume>(<issue>11</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000579" xlink:type="simple">10.1371/journal.pcbi.1000579</ext-link></comment> <object-id pub-id-type="pmid">19956750</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="other">LeCun Y, Matan O, Boser B, Denker JS, Henderson D, Howard R, et al. Handwritten zip code recognition with multilayer networks. In: Proceedings of the 10th International Conference on Pattern Recognition. vol. 2. IEEE; 1990. p. 35–40.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rolls</surname> <given-names>E</given-names></name>. <article-title>Invariant visual object and face recognition: neural and computational bases, and a model, VisNet</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2012</year>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2012.00035" xlink:type="simple">10.3389/fncom.2012.00035</ext-link></comment> <object-id pub-id-type="pmid">22723777</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="other">LeCun Y, Huang FJ, Bottou L. Learning methods for generic object recognition with invariance to pose and lighting. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR); 2004. p. 90–97.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="other">Krizhevsky A, Sutskever I, Hinton G. ImageNet classification with deep convolutional neural networks. In: Advances in neural information processing systems. vol. 25. Lake Tahoe, CA; 2012. p. 1106–1114.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref063">
<label>63</label>
<mixed-citation xlink:type="simple" publication-type="other">Földiák P. Learning invariance from transformation sequences. Neural Computation. 1991;3(2):194–200. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1991.3.2.194">http://www.mitpressjournals.org/doi/abs/10.1162/neco.1991.3.2.194</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref064">
<label>64</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wiskott</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Slow feature analysis: Unsupervised learning of invariances</article-title>. <source>Neural computation</source>. <year>2002</year>;<volume>14</volume>(<issue>4</issue>):<fpage>715</fpage>–<lpage>770</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/089976602317318938">http://www.mitpressjournals.org/doi/abs/10.1162/089976602317318938</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602317318938" xlink:type="simple">10.1162/089976602317318938</ext-link></comment> <object-id pub-id-type="pmid">11936959</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref065">
<label>65</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stringer</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>. <article-title>Invariant object recognition in the visual system with novel views of 3D objects</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>11</issue>):<fpage>2585</fpage>–<lpage>2596</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/089976602760407982">http://www.mitpressjournals.org/doi/abs/10.1162/089976602760407982</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602760407982" xlink:type="simple">10.1162/089976602760407982</ext-link></comment> <object-id pub-id-type="pmid">12433291</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref066">
<label>66</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Spratling</surname> <given-names>M</given-names></name>. <article-title>Learning viewpoint invariant perceptual representations from cluttered images</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2005</year>;<volume>27</volume>(<issue>5</issue>):<fpage>753</fpage>–<lpage>761</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1407878">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1407878</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2005.105" xlink:type="simple">10.1109/TPAMI.2005.105</ext-link></comment> <object-id pub-id-type="pmid">15875796</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref067">
<label>67</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Isik</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Leibo</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Learning and disrupting invariance in visual recognition with a temporal association rule</article-title>. <source>Front Comput Neurosci</source>. <year>2012</year>;<volume>6</volume>(<issue>37</issue>). <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.frontiersin.org/Computational_Neuroscience/10.3389/fncom.2012.00037/abstract">http://www.frontiersin.org/Computational_Neuroscience/10.3389/fncom.2012.00037/abstract</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2012.00037" xlink:type="simple">10.3389/fncom.2012.00037</ext-link></comment> <object-id pub-id-type="pmid">22754523</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref068">
<label>68</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Webb</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>E</given-names></name>. <article-title>Deformation-specific and deformation-invariant visual object recognition: pose vs. identity recognition of people and deforming objects</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>37</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2014.00037" xlink:type="simple">10.3389/fncom.2014.00037</ext-link></comment> <object-id pub-id-type="pmid">24744725</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref069">
<label>69</label>
<mixed-citation xlink:type="simple" publication-type="other">Jarrett K, Kavukcuoglu K, Ranzato M, LeCun Y. What is the best multi-stage architecture for object recognition? IEEE International Conference on Computer Vision. 2009;p. 2146–2153. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5459469">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5459469</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref070">
<label>70</label>
<mixed-citation xlink:type="simple" publication-type="other">Leibo JZ, Mutch J, Rosasco L, Ullman S, Poggio T. Learning Generic Invariances in Object Recognition: Translation and Scale. MIT-CSAIL-TR-2010-061, CBCL-294. 2010; <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://hdl.handle.net/1721.1/60378">http://hdl.handle.net/1721.1/60378</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref071">
<label>71</label>
<mixed-citation xlink:type="simple" publication-type="other">Saxe A, Koh PW, Chen Z, Bhand M, Suresh B, Ng AY. On random weights and unsupervised feature learning. Proceedings of the International Conference on Machine Learning (ICML). 2011; <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://ai.stanford.edu/~ang/papers/nipsdlufl10-RandomWeights.pdf">http://ai.stanford.edu/~ang/papers/nipsdlufl10-RandomWeights.pdf</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref072">
<label>72</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Troje</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>. <article-title>Face recognition under varying poses: The role of texture and shape</article-title>. <source>Vision Research</source>. <year>1996</year>;<volume>36</volume>(<issue>12</issue>):<fpage>1761</fpage>–<lpage>1771</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/0042698995002308">http://linkinghub.elsevier.com/retrieve/pii/0042698995002308</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0042-6989(95)00230-8" xlink:type="simple">10.1016/0042-6989(95)00230-8</ext-link></comment> <object-id pub-id-type="pmid">8759445</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref073">
<label>73</label>
<mixed-citation xlink:type="simple" publication-type="other">Tan C, Poggio T. Neural tuning size is a key factor underlying holistic face processing. arXiv preprint arXiv:14063793. 2014;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref074">
<label>74</label>
<mixed-citation xlink:type="simple" publication-type="other">Singular Inversions. FaceGen Modeller 3. Toronto, ON Canada: Ver. 3; 2003.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref075">
<label>75</label>
<mixed-citation xlink:type="simple" publication-type="other">BlenderDotOrg. Blender 2.6. Amsterdam, The Netherlands; 2013.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref076">
<label>76</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vetter</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hurlbert</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>View-based models of 3D object recognition: invariance to imaging transformations</article-title>. <source>Cerebral Cortex</source>. <year>1995</year>;<volume>5</volume>(<issue>3</issue>):<fpage>261</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://cercor.oxfordjournals.org/content/5/3/261.abstract">http://cercor.oxfordjournals.org/content/5/3/261.abstract</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/5.3.261" xlink:type="simple">10.1093/cercor/5.3.261</ext-link></comment> <object-id pub-id-type="pmid">7613081</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref077">
<label>77</label>
<mixed-citation xlink:type="simple" publication-type="other">Leibo JZ, Mutch J, Poggio T. Why The Brain Separates Face Recognition From Object Recognition. In: Advances in Neural Information Processing Systems (NIPS). Granada, Spain; 2011.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref078">
<label>78</label>
<mixed-citation xlink:type="simple" publication-type="other">DigimationDotCom. Digimation archive;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref079">
<label>79</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Edelman</surname> <given-names>S</given-names></name>. <article-title>Psychophysical support for a two-dimensional view interpolation theory of object recognition</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year>;<volume>89</volume>(<issue>1</issue>):<fpage>60</fpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pnas.org/content/89/1/60.short">http://www.pnas.org/content/89/1/60.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.89.1.60" xlink:type="simple">10.1073/pnas.89.1.60</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref080">
<label>80</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Logothetis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pauls</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>View-dependent object recognition by monkeys</article-title>. <source>Current Biology</source>. <year>1994</year>;<volume>4</volume>(<issue>5</issue>):<fpage>401</fpage>–<lpage>414</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0960982200000890">http://linkinghub.elsevier.com/retrieve/pii/S0960982200000890</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0960-9822(00)00089-0" xlink:type="simple">10.1016/S0960-9822(00)00089-0</ext-link></comment> <object-id pub-id-type="pmid">7922354</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref081">
<label>81</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Logothetis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pauls</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Shape representation in the inferior temporal cortex of monkeys</article-title>. <source>Current Biology</source>. <year>1995</year>;<volume>5</volume>(<issue>5</issue>):<fpage>552</fpage>–<lpage>563</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0960982295001084">http://linkinghub.elsevier.com/retrieve/pii/S0960982295001084</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0960-9822(95)00108-4" xlink:type="simple">10.1016/S0960-9822(95)00108-4</ext-link></comment> <object-id pub-id-type="pmid">7583105</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref082">
<label>82</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Malach</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Reppas</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Benson</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Kwong</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kennedy</surname> <given-names>WA</given-names></name>, <etal>et al</etal>. <article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1995</year>;<volume>92</volume>(<issue>18</issue>):<fpage>8135</fpage>–<lpage>8139</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.pnas.org/content/92/18/8135.short">http://www.pnas.org/content/92/18/8135.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.92.18.8135" xlink:type="simple">10.1073/pnas.92.18.8135</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref083">
<label>83</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Issa</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Precedence of the Eye Region in Neural Processing of Faces</article-title>. <source>The Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>47</issue>):<fpage>16666</fpage>–<lpage>16682</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.jneurosci.org/content/32/47/16666.short">http://www.jneurosci.org/content/32/47/16666.short</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2391-12.2012" xlink:type="simple">10.1523/JNEUROSCI.2391-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23175821</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref084">
<label>84</label>
<mixed-citation xlink:type="simple" publication-type="other">Heisele B, Serre T, Pontil M, Poggio T. Component-based Face Detection. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). Kauai, Hawaii, USA: IEEE; 2001. p. 659–657.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref085">
<label>85</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Epshtein</surname> <given-names>B</given-names></name>. <chapter-title>Visual classification by a hierarchy of extended fragments</chapter-title>. In: <source>Toward Category-Level Object Recognition</source>. <publisher-name>Springer</publisher-name>; <year>2006</year>. p. <fpage>321</fpage>–<lpage>344</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref086">
<label>86</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Young</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Hellawell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hay</surname> <given-names>DC</given-names></name>. <article-title>Configurational information in face perception</article-title>. <source>Perception</source>. <year>1987</year>;<volume>16</volume>(<issue>6</issue>):<fpage>747</fpage>–<lpage>759</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1068/p160747" xlink:type="simple">10.1068/p160747</ext-link></comment> <object-id pub-id-type="pmid">3454432</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref087">
<label>87</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Ramon y Cajal</surname> <given-names>S</given-names></name>. <chapter-title>Texture of the Nervous System of Man and the Vertebrates: I</chapter-title>. <source>Springer</source>; <year>1999</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref088">
<label>88</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barlow</surname> <given-names>H</given-names></name>. <article-title>Why have multiple cortical areas?</article-title> <source>Vision Research</source>. <year>1986</year>;<volume>26</volume>(<issue>1</issue>):<fpage>81</fpage>–<lpage>90</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/0042-6989(86)90072-6">http://dx.doi.org/10.1016/0042-6989(86)90072-6</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0042-6989(86)90072-6" xlink:type="simple">10.1016/0042-6989(86)90072-6</ext-link></comment> <object-id pub-id-type="pmid">3716216</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref089">
<label>89</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mitchison</surname> <given-names>G</given-names></name>. <article-title>Neuronal branching patterns and the economy of cortical wiring</article-title>. <source>Proceedings of the Royal Society of London Series B: Biological Sciences</source>. <year>1991</year>;<volume>245</volume>(<issue>1313</issue>):<fpage>151</fpage>–<lpage>158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1991.0102" xlink:type="simple">10.1098/rspb.1991.0102</ext-link></comment> <object-id pub-id-type="pmid">1682939</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref090">
<label>90</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Koulakov</surname> <given-names>AA</given-names></name>. <article-title>Maps in the brain: What can we learn from them?</article-title> <source>Annual Review of Neuroscience</source>. <year>2004</year>;<volume>27</volume>:<fpage>369</fpage>–<lpage>392</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.27.070203.144226" xlink:type="simple">10.1146/annurev.neuro.27.070203.144226</ext-link></comment> <object-id pub-id-type="pmid">15217337</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref091">
<label>91</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Neuromodulation of neuronal circuits: back to the future</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.09.010">http://dx.doi.org/10.1016/j.neuron.2012.09.010</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.09.010" xlink:type="simple">10.1016/j.neuron.2012.09.010</ext-link></comment> <object-id pub-id-type="pmid">23040802</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref092">
<label>92</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sigman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vinckier</surname> <given-names>F</given-names></name>. <article-title>The neural code for written words: a proposal</article-title>. <source>Trends in cognitive sciences</source>. <year>2005</year> <month>Jul</month>;<volume>9</volume>(<issue>7</issue>):<fpage>335</fpage>–<lpage>41</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.tics.2005.05.004">http://dx.doi.org/10.1016/j.tics.2005.05.004</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2005.05.004" xlink:type="simple">10.1016/j.tics.2005.05.004</ext-link></comment> <object-id pub-id-type="pmid">15951224</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref093">
<label>93</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Glezer</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>. <article-title>Evidence for highly selective neuronal tuning to whole words in the visual word form area?</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>(<issue>2</issue>):<fpage>199</fpage>–<lpage>204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.03.017" xlink:type="simple">10.1016/j.neuron.2009.03.017</ext-link></comment> <object-id pub-id-type="pmid">19409265</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref094">
<label>94</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Epstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>. <year>1998</year>;<volume>392</volume>(<issue>6676</issue>):<fpage>598</fpage>–<lpage>601</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.nature.com/nature/journal/v392/n6676/abs/392598a0.html">http://www.nature.com/nature/journal/v392/n6676/abs/392598a0.html</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/33402" xlink:type="simple">10.1038/33402</ext-link></comment> <object-id pub-id-type="pmid">9560155</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref095">
<label>95</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kornblith</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Ohayon</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>A Network for Scene Processing in the Macaque Temporal Lobe</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>(<issue>4</issue>):<fpage>766</fpage>–<lpage>781</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.06.015" xlink:type="simple">10.1016/j.neuron.2013.06.015</ext-link></comment> <object-id pub-id-type="pmid">23891401</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref096">
<label>96</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Epstein</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Julian</surname> <given-names>JB</given-names></name>. <article-title>Scene Areas in Humans and Macaques</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>(<issue>4</issue>):<fpage>615</fpage>–<lpage>617</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.08.001">http://dx.doi.org/10.1016/j.neuron.2013.08.001</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.08.001" xlink:type="simple">10.1016/j.neuron.2013.08.001</ext-link></comment> <object-id pub-id-type="pmid">23972591</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref097">
<label>97</label>
<mixed-citation xlink:type="simple" publication-type="other">Ko EY, Leibo JZ, Poggio T. A hierarchical model of perspective-invariant scene identification. In: Society for Neuroscience (486.16/OO26). Washington D.C.; 2011. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://cbcl.mit.edu/publications/ps/sfn_2011_perspect_poster_V1.pdf">http://cbcl.mit.edu/publications/ps/sfn_2011_perspect_poster_V1.pdf</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref098">
<label>98</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Epstein</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Vass</surname> <given-names>LK</given-names></name>. <article-title>Neural systems for landmark-based wayfinding in humans</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2014</year>;<volume>369</volume>(<issue>1635</issue>):<fpage>20120533</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2012.0533" xlink:type="simple">10.1098/rstb.2012.0533</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref099">
<label>99</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>H</given-names></name>. <article-title>Image-based object recognition in man, monkey and machine</article-title>. <source>Cognition</source>. <year>1998</year>;<volume>67</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>20</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.sciencedirect.com/science/article/pii/S0010027798000262">http://www.sciencedirect.com/science/article/pii/S0010027798000262</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0010-0277(98)00026-2" xlink:type="simple">10.1016/S0010-0277(98)00026-2</ext-link></comment> <object-id pub-id-type="pmid">9735534</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref100">
<label>100</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nishihara</surname> <given-names>HK</given-names></name>. <article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title>. <source>Proceedings of the Royal Society of London Series B Biological Sciences</source>. <year>1978</year>;<volume>200</volume>(<issue>1140</issue>):<fpage>269</fpage>–<lpage>294</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1978.0020" xlink:type="simple">10.1098/rspb.1978.0020</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref101">
<label>101</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Recognition-by-components: a theory of human image understanding</article-title>. <source>Psychological review</source>. <year>1987</year>;<volume>94</volume>(<issue>2</issue>):<fpage>115</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.94.2.115" xlink:type="simple">10.1037/0033-295X.94.2.115</ext-link></comment> <object-id pub-id-type="pmid">3575582</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref102">
<label>102</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ullman</surname> <given-names>S</given-names></name>. <article-title>Aligning pictorial descriptions: An approach to object recognition?</article-title> 1. <source>Cognition</source>. <year>1989</year>;<volume>32</volume>(<issue>3</issue>):<fpage>193</fpage>–<lpage>254</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/001002778990036X">http://linkinghub.elsevier.com/retrieve/pii/001002778990036X</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0010-0277(89)90036-X" xlink:type="simple">10.1016/0010-0277(89)90036-X</ext-link></comment> <object-id pub-id-type="pmid">2752709</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref103">
<label>103</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Edelman</surname> <given-names>S</given-names></name>. <article-title>A network that learns to recognize three-dimensional objects</article-title>. <source>Nature</source>. <year>1990</year>;<volume>343</volume>(<issue>6255</issue>):<fpage>263</fpage>–<lpage>266</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://cbcl.mit.edu/people/poggio-new/journals/poggio-edelman-nature-1990.pdf">http://cbcl.mit.edu/people/poggio-new/journals/poggio-edelman-nature-1990.pdf</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/343263a0" xlink:type="simple">10.1038/343263a0</ext-link></comment> <object-id pub-id-type="pmid">2300170</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref104">
<label>104</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>. <article-title>Is human object recognition better described by geon structural descriptions or by multiple views?</article-title> <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>1995</year>;<volume>21</volume>(<issue>6</issue>):<fpage>1494</fpage>–<lpage>1505</lpage>. <object-id pub-id-type="pmid">7490590</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref105">
<label>105</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>. <article-title>Diagnostic recognition: task constraints, object information, and their interactions</article-title>. <source>Cognition</source>. <year>1998</year>;<volume>67</volume>(<issue>1</issue>):<fpage>147</fpage>–<lpage>179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0010-0277(98)00016-X" xlink:type="simple">10.1016/S0010-0277(98)00016-X</ext-link></comment> <object-id pub-id-type="pmid">9735539</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref106">
<label>106</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Gauthier</surname> <given-names>I</given-names></name>. <article-title>Do viewpoint-dependent mechanisms generalize across members of a class?</article-title> <source>Cognition</source>. <year>1998</year>;<volume>67</volume>(<issue>1</issue>):<fpage>73</fpage>–<lpage>110</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0010-0277(98)00023-7" xlink:type="simple">10.1016/S0010-0277(98)00023-7</ext-link></comment> <object-id pub-id-type="pmid">9735537</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref107">
<label>107</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hill</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Akamatsu</surname> <given-names>S</given-names></name>. <article-title>Information and viewpoint dependence in face recognition</article-title>. <source>Cognition</source>. <year>1997</year>;<volume>62</volume>(<issue>2</issue>):<fpage>201</fpage>–<lpage>222</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0010-0277(96)00785-8" xlink:type="simple">10.1016/S0010-0277(96)00785-8</ext-link></comment> <object-id pub-id-type="pmid">9141907</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref108">
<label>108</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sinha</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Role of learning in three-dimensional form perception</article-title>. <source>Nature</source>. <year>1996</year>;<volume>384</volume>(<issue>6608</issue>):<fpage>460</fpage>–<lpage>463</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/384460a0" xlink:type="simple">10.1038/384460a0</ext-link></comment> <object-id pub-id-type="pmid">8945472</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref109">
<label>109</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gauthier</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Tarr</surname> <given-names>MJ</given-names></name>. <article-title>Becoming a “greeble” expert: Exploring mechanisms for face recognition</article-title>. <source>Vision Research</source>. <year>1997</year>;<volume>37</volume>(<issue>12</issue>):<fpage>1673</fpage>–<lpage>1682</lpage>. <comment>Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1513&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1513&amp;rep=rep1&amp;type=pdf</ext-link></comment> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(96)00286-6" xlink:type="simple">10.1016/S0042-6989(96)00286-6</ext-link></comment> <object-id pub-id-type="pmid">9231232</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref110">
<label>110</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>L</given-names></name>. <article-title>Cultural recycling of cortical maps</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>56</volume>(<issue>2</issue>):<fpage>384</fpage>–<lpage>398</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.10.004" xlink:type="simple">10.1016/j.neuron.2007.10.004</ext-link></comment> <object-id pub-id-type="pmid">17964253</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref111">
<label>111</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Srihasam</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Mandeville</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Morocz</surname> <given-names>IA</given-names></name>, <name name-style="western"><surname>Sullivan</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>MS</given-names></name>. <article-title>Behavioral and anatomical consequences of early versus late symbol training in macaques</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>3</issue>):<fpage>608</fpage>–<lpage>619</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.12.022" xlink:type="simple">10.1016/j.neuron.2011.12.022</ext-link></comment> <object-id pub-id-type="pmid">22325210</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004390.ref112">
<label>112</label>
<mixed-citation xlink:type="simple" publication-type="other">Zeiler MD, Fergus R. Visualizing and Understanding Convolutional Neural Networks. arXiv preprint arXiv:13112901. 2013;.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref113">
<label>113</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Torgerson</surname> <given-names>WS</given-names></name>. <source>Theory and methods of scaling</source>. <publisher-name>Wiley</publisher-name>; <year>1958</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004390.ref114">
<label>114</label>
<mixed-citation xlink:type="simple" publication-type="other">Nene S, Nayar S, Murase H. Columbia Object Image Library (COIL-100). Columbia University Tech Report No CUCS-006-96. 1996;.</mixed-citation>
</ref>
</ref-list>
</back>
</article>