<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01803</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004792</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroanatomy</subject><subj-group><subject>Neural pathways</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Single neuron function</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework</article-title>
<alt-title alt-title-type="running-head">Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Song</surname> <given-names>H. Francis</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Yang</surname> <given-names>Guangyu R.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Wang</surname> <given-names>Xiao-Jing</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Center for Neural Science, New York University, New York, New York, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>NYU-ECNU Institute of Brain and Cognitive Science, NYU Shanghai, Shanghai, China</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Sporns</surname> <given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Indiana University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: HFS GRY XJW. Performed the experiments: HFS GRY. Analyzed the data: HFS GRY. Contributed reagents/materials/analysis tools: HFS GRY. Wrote the paper: HFS GRY XJW.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">xjwang@nyu.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>2</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>29</day>
<month>2</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>2</issue>
<elocation-id>e1004792</elocation-id>
<history>
<date date-type="received">
<day>23</day>
<month>10</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>4</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Song et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004792"/>
<abstract>
<p>The ability to simultaneously record from large numbers of neurons in behaving animals has ushered in a new era for the study of the neural circuit mechanisms underlying cognitive functions. One promising approach to uncovering the dynamical and computational principles governing population responses is to analyze model recurrent neural networks (RNNs) that have been optimized to perform the same tasks as behaving animals. Because the optimization of network parameters specifies the desired output but not the manner in which to achieve this output, “trained” networks serve as a source of mechanistic hypotheses and a testing ground for data analyses that link neural computation to behavior. Complete access to the activity and connectivity of the circuit, and the ability to manipulate them arbitrarily, make trained networks a convenient proxy for biological circuits and a valuable platform for theoretical investigation. However, existing RNNs lack basic biological features such as the distinction between excitatory and inhibitory units (Dale’s principle), which are essential if RNNs are to provide insights into the operation of biological circuits. Moreover, trained networks can achieve the same behavioral performance but differ substantially in their structure and dynamics, highlighting the need for a simple and flexible framework for the exploratory training of RNNs. Here, we describe a framework for gradient descent-based training of excitatory-inhibitory RNNs that can incorporate a variety of biological knowledge. We provide an implementation based on the machine learning library Theano, whose automatic differentiation capabilities facilitate modifications and extensions. We validate this framework by applying it to well-known experimental paradigms such as perceptual decision-making, context-dependent integration, multisensory integration, parametric working memory, and motor sequence generation. Our results demonstrate the wide range of neural activity patterns and behavior that can be modeled, and suggest a unified setting in which diverse cognitive computations and mechanisms can be studied.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Cognitive functions arise from the coordinated activity of many interconnected neurons. As neuroscientists increasingly use large datasets of simultaneously recorded neurons to study the brain, one approach that has emerged as a promising tool for interpreting population responses is to analyze model recurrent neural networks (RNNs) that have been optimized to perform the same tasks as recorded animals. Complete access to the activity and connectivity of the circuit, and the ability to manipulate them in arbitrary ways, make trained networks a convenient proxy for biological circuits and a valuable platform for theoretical investigation. However, existing RNNs lack basic biological features that are essential if RNNs are to provide insights into the circuit-level operation of the brain. Moreover, trained networks can achieve the same behavioral performance but differ substantially in their structure and dynamics, highlighting the need for a simple and flexible framework for the exploratory training of RNNs. Here we describe and provide an implementation for such a framework, which we apply to several well-known experimental paradigms that illustrate the diversity of detail that can be modeled. Our work provides a foundation for neuroscientists to harness trained RNNs in their own investigations of the neural basis of cognition.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by the Swartz Foundation, Office of Naval Research Grant N00014-13-1-0297, and a Google Computational Neuroscience Grant. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="2"/>
<page-count count="30"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/xjwanglab/pycog" xlink:type="simple">https://github.com/xjwanglab/pycog</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<disp-quote>
<p>This is a <italic>PLOS Computational Biology</italic> Methods paper.</p>
</disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Computations in the brain are carried out by populations of interconnected neurons. While single-neuron responses can reveal a great deal about the neural mechanisms underlying various sensory, motor, and cognitive functions, neural mechanisms often involve the coordinated activity of many neurons whose complex individual dynamics are not easily explained by tuning to experimental parameters [<xref ref-type="bibr" rid="pcbi.1004792.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref004">4</xref>]. A growing recognition of the importance of studying population-level responses is reflected in the increasing number of studies that use large datasets of simultaneously or sequentially recorded neurons to infer neural circuit mechanisms [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref009">9</xref>]. At the same time, the novel challenges posed by high-dimensional neural data have led to the development of new methods for analyzing and modeling such data [<xref ref-type="bibr" rid="pcbi.1004792.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref012">12</xref>].</p>
<p>One approach that has emerged as a promising tool for identifying the dynamical and computational mechanisms embedded in large neural populations is to study model recurrent neural networks (RNNs) whose connection weights have been optimized to perform the same tasks as recorded animals [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref007">7</xref>]. In [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>], for example, the “trained” network was analyzed to reveal a previously unknown selection mechanism for context-dependent integration of sensory stimuli that was consistent with data obtained from behaving monkeys. RNNs of rate units, which describe biological circuits as a set of firing rates (nonlinearities) interacting through synapses (connection weights) (<xref ref-type="fig" rid="pcbi.1004792.g001">Fig 1</xref>), interpolate between biophysically detailed spiking-neuron models and the wider class of continuous-time dynamical systems: the units of an RNN can be interpreted as the temporal or ensemble average of one or more co-tuned spiking neurons [<xref ref-type="bibr" rid="pcbi.1004792.ref013">13</xref>], while any nonlinear dynamical system can be approximated by an RNN with a sufficient number of units [<xref ref-type="bibr" rid="pcbi.1004792.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref015">15</xref>]. The optimization of network parameters typically specifies the desired output but not the manner in which to achieve this output, i.e., the <italic>what</italic> but not the <italic>how</italic>. Trained RNNs therefore serve as a source of candidate hypotheses about circuit mechanisms and a testing ground for data analyses that link neural computation to behavior. Complete access to the activity and connectivity of the circuit, and the ability to manipulate them in arbitrary ways, make trained networks a convenient proxy for biological circuits and a valuable platform for theoretical investigation [<xref ref-type="bibr" rid="pcbi.1004792.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref017">17</xref>].</p>
<fig id="pcbi.1004792.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Recurrent neural network (RNN).</title>
<p>A trained RNN of excitatory and inhibitory rate units <bold>r</bold>(<italic>t</italic>) receives time-varying inputs <bold>u</bold>(<italic>t</italic>) and produces the desired time-varying outputs <bold>z</bold>(<italic>t</italic>). Inputs encode task-relevant sensory information or internal rules, while outputs indicate a decision in the form of an abstract decision variable, probability distribution, or direct motor output. Only the recurrent units have their own dynamics: inputs are considered to be given and the outputs are read out from the recurrent units. Each unit of an RNN can be interpreted as the temporally smoothed firing rate of a single neuron or the spatial average of a group of similarly tuned neurons.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g001" xlink:type="simple"/>
</fig>
<p>For many tasks of interest, however, training can result in multiple networks that achieve the same behavioral performance but differ substantially in their connectivity and dynamics. As highlighted in recent work [<xref ref-type="bibr" rid="pcbi.1004792.ref008">8</xref>], the particular solution that is discovered by the training algorithm depends strongly on the set of constraints and “regularizations” used in the optimization process, so that training RNNs to perform a task is not entirely unbiased with respect to the <italic>how</italic>. Indeed, for the purposes of modeling animal tasks in systems neuroscience the question is no longer whether an RNN can be trained to perform the task—the answer appears to be yes in a wide range of settings—but what architectures and regularizations lead to network activity that is most similar to neural recordings obtained from behaving animals.</p>
<p>Answering this question is essential if RNNs are to provide insights into the operation of the brain at the level of neural circuits [<xref ref-type="bibr" rid="pcbi.1004792.ref018">18</xref>], and extends the classical connectionist approach [<xref ref-type="bibr" rid="pcbi.1004792.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref020">20</xref>]. Doing so requires a simple and flexible framework for the exploratory training of RNNs to investigate the effects of different constraints on network properties, particularly those constraints that render the RNNs more biologically plausible. For instance, many RNNs studied to date have “firing rates” that are both positive and negative. More fundamentally, existing networks do not satisfy Dale’s principle [<xref ref-type="bibr" rid="pcbi.1004792.ref021">21</xref>], the basic and ubiquitous observation that neurons in the mammalian cortex have purely excitatory or inhibitory effects on other neurons. The analogous constraint that all connection weights from a given unit must have the same sign can have a profound effect on the types of dynamics, such as non-normality [<xref ref-type="bibr" rid="pcbi.1004792.ref022">22</xref>], that operate in the circuit. Moreover, connections from excitatory and inhibitory neurons exhibit different levels of sparseness and specificity, with non-random features in the distribution of connection patterns among neurons both within local circuits [<xref ref-type="bibr" rid="pcbi.1004792.ref023">23</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref027">27</xref>] and among cortical areas [<xref ref-type="bibr" rid="pcbi.1004792.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref030">30</xref>]. Notably, long-range projections between areas are primarily excitatory. Such details must be included in a satisfactory model of local and large-scale cortical computation.</p>
<p>We address this challenge by describing flexible, gradient descent-based training of excitatory-inhibitory RNNs that can incorporate a variety of biological knowledge, particularly of local and large-scale connectivity in the brain. Several different methods have previously been used to train RNNs for cognitive tasks in neuroscience, including first-order reduced and controlled error (FORCE) [<xref ref-type="bibr" rid="pcbi.1004792.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref032">32</xref>] and Hessian-free (HF) [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref034">34</xref>]. Here we use minibatch stochastic gradient descent (SGD) with the modifications described in [<xref ref-type="bibr" rid="pcbi.1004792.ref035">35</xref>], which remove the major difficulties associated with pure gradient descent training of RNNs. SGD is conceptually simple without sacrificing performance [<xref ref-type="bibr" rid="pcbi.1004792.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref037">37</xref>] and is particularly advantageous in the present context for the following reasons: Unlike FORCE and like HF, SGD allows us to more easily formulate the problem of training an RNN as one of minimizing an objective function that can be modified to induce different types of solutions [<xref ref-type="bibr" rid="pcbi.1004792.ref008">8</xref>]. Meanwhile, like FORCE and unlike HF, for many tasks SGD can update parameters on a trial-by-trial basis, i.e., in an “online” fashion. This opens up the possibility of exploring across-trial effects that cannot be studied when large numbers of trials are required for each iteration of learning, as in the HF algorithm. Although none of the learning methods discussed here can at present be considered biological, recent work also suggests that spike-timing dependent plasticity (STDP) [<xref ref-type="bibr" rid="pcbi.1004792.ref038">38</xref>], which is believed to be a basic rule governing synaptic weight changes in the brain, may correspond to a form of SGD [<xref ref-type="bibr" rid="pcbi.1004792.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref040">40</xref>]. However, the focus of our approach will be on the results, not the mechanism, of learning.</p>
<p>We provide an implementation of this framework based on the Python machine learning library Theano [<xref ref-type="bibr" rid="pcbi.1004792.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref042">42</xref>], whose automatic differentiation capabilities facilitate modifications and extensions. Theano also simplifies the use of Graphics Processing Units (GPUs) when available to speed up computations. The implementation was designed to minimize the overhead for each new task by only requiring a specification of the network structure and correct input-output relationship to be learned. It also streamlines the testing and analysis of the resulting networks by using the same (customizable) specification for both training and testing (<xref ref-type="supplementary-material" rid="pcbi.1004792.s001">S1 Code</xref>). We demonstrate the application of this framework to well-known experimental paradigms that illustrate the diversity of tasks and details that can be modeled: perceptual decision-making, context-dependent integration, multisensory integration, parametric working memory, and eye-movement sequence generation. Using the resulting networks we perform both single-neuron and population-level analyses associated with the corresponding experimental paradigm. Our results show that trained RNNs provide a unified setting in which diverse computations and mechanisms can be studied, laying the foundation for more neuroscientists to harness trained RNNs in their own investigations of the neural basis of cognition.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>In this section we first define the RNNs used in this work, show how constraints can be introduced, then describe training the networks using a modified form of stochastic gradient descent (SGD).</p>
<sec id="sec003">
<title>Recurrent neural networks</title>
<p>RNNs receive a set of <italic>N</italic><sub>in</sub> time-varying inputs <bold>u</bold>(<italic>t</italic>) and produce <italic>N</italic><sub>out</sub> outputs <bold>z</bold>(<italic>t</italic>), where inputs encode task-relevant sensory information and outputs typically represent a decision variable or probability distribution (<xref ref-type="fig" rid="pcbi.1004792.g001">Fig 1</xref>). Outputs can also relate to the direct motor effector, such as eye position, by which an animal indicates its decision in the behavioral paradigm. We consider RNNs whose <italic>N</italic> firing rates <bold>r</bold>(<italic>t</italic>) are related to their corresponding currents <bold>x</bold>(<italic>t</italic>) by the threshold (rectified) linear “<italic>f-I</italic> curve” [<italic>x</italic>]<sub>+</sub> = max(<italic>x</italic>, 0), which maps arbitrary input currents to positive firing rates: <italic>x</italic> if <italic>x</italic> &gt; 0 and 0 otherwise. The RNNs are described by the equations
<disp-formula id="pcbi.1004792.e001"><alternatives><graphic id="pcbi.1004792.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:mi>τ</mml:mi> <mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>in</mml:mtext></mml:msup> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>τ</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mspace width="4pt"/><mml:mi mathvariant="bold">ξ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula> <disp-formula id="pcbi.1004792.e002"><alternatives><graphic id="pcbi.1004792.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula> <disp-formula id="pcbi.1004792.e003"><alternatives><graphic id="pcbi.1004792.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>out</mml:mtext></mml:msup> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
or, more explicitly,
<disp-formula id="pcbi.1004792.e004"><alternatives><graphic id="pcbi.1004792.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mi>τ</mml:mi> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>in</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mtext>in</mml:mtext></mml:msubsup> <mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>τ</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>rec</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mspace width="4pt"/><mml:msub><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1004792.e005"><alternatives><graphic id="pcbi.1004792.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula> <disp-formula id="pcbi.1004792.e006"><alternatives><graphic id="pcbi.1004792.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>ℓ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mtext>out</mml:mtext></mml:msubsup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
for <italic>i</italic> = 1, …, <italic>N</italic> and ℓ = 1, …, <italic>N</italic><sub>out</sub>. In these equations <italic>τ</italic> is the time constant of the network units, <italic>W</italic><sup>in</sup> is an <italic>N</italic> × <italic>N</italic><sub>in</sub> matrix of connection weights from the inputs to network units, <italic>W</italic><sup>rec</sup> is an <italic>N</italic> × <italic>N</italic> matrix of recurrent connection weights between network units, <italic>W</italic><sup>out</sup> is an <italic>N</italic><sub>out</sub> × <italic>N</italic> matrix of connection weights from the network units to the outputs, and <italic><bold>ξ</bold></italic> are <italic>N</italic> independent Gaussian white noise processes with zero mean and unit variance that represent noise intrinsic to the network. It is worth noting that if for some ℓ = 1, …, <italic>N</italic>′, <italic>N</italic>′ ≤ <italic>N</italic>, the output weights <inline-formula id="pcbi.1004792.e007"><alternatives><graphic id="pcbi.1004792.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mtext>out</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> where <italic>δ</italic><sub><italic>ij</italic></sub> = 1 if <italic>i</italic> = <italic>j</italic> and 0 otherwise, then the readout is the same as a subset of the network firing rates. This is useful in situations where the aim is to fix a subset of the units to experimentally recorded firing rates.</p>
<p>Without the rectification nonlinearity [<italic>x</italic>]<sub>+</sub> (in which case <bold>r</bold> = <bold>x</bold>), Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1004792.e003">3</xref> would describe a linear system whose dynamics is completely determined by <italic>W</italic><sup>rec</sup>. Thus, one way to understand the effect of rectification is to consider a linear dynamical system whose coupling matrix <italic>W</italic><sup>rec</sup> at any given time includes only those columns that correspond to “active” units with positive summed current <italic>x</italic><sub><italic>i</italic></sub> (and hence positive firing rate <italic>r</italic><sub><italic>i</italic></sub>) [<xref ref-type="bibr" rid="pcbi.1004792.ref043">43</xref>]. This toggles the network between different linear maps, thereby endowing the network with the capacity for more complex computations than would be possible with a single linear network [<xref ref-type="bibr" rid="pcbi.1004792.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref045">45</xref>]. As a convenient baseline, the recurrent noise in <xref ref-type="disp-formula" rid="pcbi.1004792.e001">Eq 1</xref> has been scaled so that in the corresponding linear network without rectification each unit is an Ornstein-Uhlenbeck process with variance <inline-formula id="pcbi.1004792.e008"><alternatives><graphic id="pcbi.1004792.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>rec</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> when <italic>W</italic><sup>rec</sup> = <italic>W</italic><sup>in</sup> = 0.</p>
<p>In practice, the continuous-time dynamics in Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1004792.e003">3</xref> are discretized to Euler form (which we indicate by writing time as a subscript, <italic>X</italic><sub><italic>t</italic></sub> = <italic>X</italic>(<italic>t</italic> ⋅ Δ<italic>t</italic>) for a time-dependent variable <italic>X</italic>) in time steps of size Δ<italic>t</italic> as [<xref ref-type="bibr" rid="pcbi.1004792.ref046">46</xref>]
<disp-formula id="pcbi.1004792.e009"><alternatives><graphic id="pcbi.1004792.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>in</mml:mtext></mml:msup> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mspace width="4pt"/><mml:mi mathvariant="bold">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula> <disp-formula id="pcbi.1004792.e010"><alternatives><graphic id="pcbi.1004792.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula> <disp-formula id="pcbi.1004792.e011"><alternatives><graphic id="pcbi.1004792.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>out</mml:mtext></mml:msup> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>α</italic> = Δ<italic>t</italic>/<italic>τ</italic> and <bold>N</bold>(0, 1) are normally distributed random numbers with zero mean and unit variance, sampled independently at every time step. In this formulation, the usual discrete-time RNNs used in machine learning applications correspond to <italic>α</italic> = 1 or Δ<italic>t</italic> = <italic>τ</italic>. To minimize computational effort we train the network with a value of Δ<italic>t</italic> that is as large as possible such that the same network behavior is recovered in the continuous limit of Δ<italic>t</italic> → 0.</p>
<p>Although the details of the inputs to the network are specific to each task, it is convenient to represent all inputs as a rectified sum of baseline <bold>u</bold><sup>0</sup>, task-dependent signal <bold>u</bold><sup>task</sup>(<italic>t</italic>), and Gaussian white noise <italic>ξ</italic>:
<disp-formula id="pcbi.1004792.e012"><alternatives><graphic id="pcbi.1004792.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mrow><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mn>0</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mtext>task</mml:mtext></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>τ</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>in</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mspace width="4pt"/><mml:mi mathvariant="bold">ξ</mml:mi></mml:mfenced> <mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
in the continuous description, and
<disp-formula id="pcbi.1004792.e013"><alternatives><graphic id="pcbi.1004792.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mn>0</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>task</mml:mtext></mml:msubsup> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>α</mml:mi></mml:mfrac> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>in</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mspace width="4pt"/><mml:mi mathvariant="bold">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
in the discrete-time description. Motivated by the interpretation that the network under study is only one part of a larger circuit, the baseline and noise terms in the inputs can together be considered the spontaneous firing rate of “upstream” units that project to the network.</p>
<p>We note that in <xref ref-type="disp-formula" rid="pcbi.1004792.e001">Eq 1</xref> the external “sensory” noise ultimately combines with the intrinsic noise, with the difference that input noise can be shared between many units in the network while the recurrent noise is private to each unit. There are many cases where the external and internal noise trade off in their effect on the network, for instance on its psychometric performance in a perceptual decision-making task. However, the two sources of noise can be biologically and conceptually quite different [<xref ref-type="bibr" rid="pcbi.1004792.ref047">47</xref>], and for this reason it is helpful to separate the two types of noise in our formulation.</p>
<p>Finally, in many cases (the exception being networks that are run continuously without reset) it is convenient to optimize the initial condition <bold>x</bold><sub>0</sub> = <bold>x</bold>(0) at time <italic>t</italic> = 0 along with the network weights. This merely selects a suitable starting point for each run, reducing the time it takes for the network to relax to its spontaneous state in the absence of inputs. It has little effect on the robustness of the network due to the recurrent noise used both during and after training; in particular, the network state at the time of stimulus onset is highly variable across trials.</p>
</sec>
<sec id="sec004">
<title>RNNs with separate excitatory and inhibitory populations</title>
<p>A basic and ubiquitous observation in the mammalian cortex, known in the more general case as Dale’s principle [<xref ref-type="bibr" rid="pcbi.1004792.ref021">21</xref>], is that cortical neurons have either purely excitatory or inhibitory effects on postsynaptic neurons. Moreover, excitatory neurons outnumber inhibitory neurons by a ratio of roughly 4 to 1. In a rate model with positive firing rates such as the one given by Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1004792.e003">3</xref>, a connection from unit <italic>j</italic> to unit <italic>i</italic> is “excitatory” if <inline-formula id="pcbi.1004792.e014"><alternatives><graphic id="pcbi.1004792.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and “inhibitory” if <inline-formula id="pcbi.1004792.e015"><alternatives><graphic id="pcbi.1004792.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. A <italic>unit</italic> <italic>j</italic> is excitatory if all of its projections on other units are zero or excitatory, i.e., if <inline-formula id="pcbi.1004792.e016"><alternatives><graphic id="pcbi.1004792.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for all <italic>i</italic>; similarly, unit <italic>j</italic> is inhibitory if <inline-formula id="pcbi.1004792.e017"><alternatives><graphic id="pcbi.1004792.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:mo>≤</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for all <italic>i</italic>. In the case where the outputs are considered to be units in a downstream network, consistency requires that for all ℓ the readout weights satisfy <inline-formula id="pcbi.1004792.e018"><alternatives><graphic id="pcbi.1004792.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>out</mml:mtext></mml:msubsup> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004792.e019"><alternatives><graphic id="pcbi.1004792.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>out</mml:mtext></mml:msubsup> <mml:mo>≤</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for excitatory and inhibitory units <italic>j</italic>, respectively. Since long-range projections in the mammalian cortex are exclusively excitatory, for most networks we limit readout to the excitatory units. It is also natural in most cases to assume that inputs to the network are long-range inputs from an upstream circuit, and we assume all elements of the input weight matrix <italic>W</italic><sup>in</sup> are non-negative. For consistency with the following, we indicate this as <italic>W</italic><sup>in</sup> = <italic>W</italic><sup>in,+</sup>. Once again, this is only meaningful if the inputs themselves are always non-negative, motivating the rectification of inputs in <xref ref-type="disp-formula" rid="pcbi.1004792.e012">Eq 10</xref>.</p>
<p>In order to train RNNs that satisfy the above constraints, we parametrize the recurrent weight matrix <italic>W</italic><sup>rec</sup> as the product of a non-negative matrix <italic>W</italic><sup>rec,+</sup> and a diagonal matrix <italic>D</italic> of 1’s and −1’s, <italic>W</italic><sup>rec</sup> = <italic>W</italic><sup>rec,+</sup> <italic>D</italic>. For example, consider a network containing 4 excitatory units and 1 inhibitory unit; the excitatory/inhibitory signature of the network is then <italic>D</italic> = <italic>diag</italic>(1, 1, 1, 1, −1) (a matrix with the specified entries on the diagonal and zeros everywhere else), and the full recurrent weight matrix has the form
<disp-formula id="pcbi.1004792.e020"><alternatives><graphic id="pcbi.1004792.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>-</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>-</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>-</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>-</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup></mml:munder> <mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec,+</mml:mtext></mml:msup></mml:munder> <mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd/><mml:mtd/><mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd/><mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd/><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd/><mml:mtd/><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:mi>D</mml:mi></mml:munder> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where absent matrix elements indicate zeros. Although an individual unit in an RNN does not necessarily represent a single neuron, we typically fix the self-connections represented by the diagonal elements of <italic>W</italic><sup>rec</sup> to be zero, see below. Similarly, if the readout from the network is considered to be long-range projections to a downstream network, then the output weights are parametrized as <italic>W</italic><sup>out</sup> = <italic>W</italic><sup>out,+</sup> <italic>D</italic>.</p>
<p>During training, the positivity of <italic>W</italic><sup>in,+</sup>, <italic>W</italic><sup>rec,+</sup>, and <italic>W</italic><sup>out,+</sup> can be enforced in several ways, including rectification [<italic>W</italic>]<sub>+</sub> and the absolute value function |<italic>W</italic>|. Here we use rectification.</p>
</sec>
<sec id="sec005">
<title>Specifying the pattern of connectivity</title>
<p>In addition to dividing units into separate excitatory and inhibitory populations, we can also constrain their pattern of connectivity. This can range from simple constraints such as the absence of self-connections to more complex structures derived from biology. Local cortical circuits have distance [<xref ref-type="bibr" rid="pcbi.1004792.ref048">48</xref>], layer [<xref ref-type="bibr" rid="pcbi.1004792.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref050">50</xref>], and cell-type [<xref ref-type="bibr" rid="pcbi.1004792.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref051">51</xref>] dependent patterns of connectivity and different overall levels of sparseness for excitatory to excitatory, inhibitory to excitatory, excitatory to inhibitory, and inhibitory to inhibitory connections [<xref ref-type="bibr" rid="pcbi.1004792.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref053">53</xref>]. Although the density of connections in a trained network can be either fixed (hard constraint) or induced through regularization (soft constraint) (see <xref ref-type="disp-formula" rid="pcbi.1004792.e060">Eq 27</xref>), here we focus on the former to address the more general problem of imposing known biological structure on trained networks. For instance, in models of large-scale, distributed computation in the brain we can consider multiple cortical “areas” characterized by local inhibition within areas and long-range excitation between areas. These long-range connections can be distributed according to a highly complex topology [<xref ref-type="bibr" rid="pcbi.1004792.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref030">30</xref>]. It is also desirable when testing specific hypotheses about circuit structure to fix a subset of the connection weights to predefined values while leaving others as “plastic,” modifiable by training.</p>
<p>A simple way to impose hard constraints on the connectivity is to parametrize the weight matrices using masks. As an example, suppose we would like to train a subset of the excitatory weights and also fix two of the inhibitory weights to <italic>w</italic><sub>1</sub> and <italic>w</italic><sub>2</sub> so that they are not modified during training. We can implement this by writing
<disp-formula id="pcbi.1004792.e021"><alternatives><graphic id="pcbi.1004792.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mrow><mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec,+</mml:mtext></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>M</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup></mml:munder> <mml:mo>⊙</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mo>·</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>·</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>+</mml:mo></mml:mtd> <mml:mtd><mml:mo>·</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec,plastic,+</mml:mtext></mml:msup></mml:munder> <mml:mo>+</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>︸</mml:mo></mml:munder> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec,fixed,+</mml:mtext></mml:msup></mml:munder> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
where ⊙ denotes the element-wise multiplication of two matrices (not standard matrix multiplication). Here <italic>W</italic><sup>rec,plastic,+</sup> is obtained by rectifying the (unconstrained) trained weights <italic>W</italic><sup>rec,plastic</sup>, so that <italic>W</italic><sup>rec,plastic,+</sup> = [<italic>W</italic><sup>rec,plastic</sup>]<sub>+</sub>, while <italic>W</italic><sup>rec,fixed,+</sup> is a matrix of fixed weights. The elements that are marked with a dot are irrelevant and play no role in the network’s dynamics. <xref ref-type="disp-formula" rid="pcbi.1004792.e021">Eq 13</xref> has the effect of optimizing only those elements which are nonzero in the multiplying mask <italic>M</italic><sup>rec</sup>, which ensures that the weights corresponding to zeros do not contribute. Some elements, for instance the inhibitory weights <italic>w</italic><sub>1</sub> and <italic>w</italic><sub>2</sub> in <xref ref-type="disp-formula" rid="pcbi.1004792.e021">Eq 13</xref>, remain fixed at their specified values throughout training. Explicitly, the full weight matrix of the RNN is related to the underlying trained weight matrix <italic>W</italic><sup>rec,plastic</sup> by (cf. <xref ref-type="disp-formula" rid="pcbi.1004792.e020">Eq 12</xref>)
<disp-formula id="pcbi.1004792.e022"><alternatives><graphic id="pcbi.1004792.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup> <mml:mo>⊙</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec,plastic</mml:mtext></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec,fixed,+</mml:mtext></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
and similarly for the input and output weights.</p>
</sec>
<sec id="sec006">
<title>Initialization</title>
<p>In networks that do not contain separate excitatory and inhibitory populations, it is convenient to initialize the recurrent weight matrix as <inline-formula id="pcbi.1004792.e023"><alternatives><graphic id="pcbi.1004792.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup> <mml:mo>=</mml:mo> <mml:mi>ρ</mml:mi> <mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004792.e024"><alternatives><graphic id="pcbi.1004792.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> is formed by setting a fraction <italic>p</italic>, 0 &lt; <italic>p</italic> ≤ 1, of elements to nonzero values drawn from a Gaussian distribution with mean 0 and variance (<italic>pN</italic>)<sup>−1</sup>, and the remaining fraction 1 − <italic>p</italic> to zero [<xref ref-type="bibr" rid="pcbi.1004792.ref031">31</xref>]. This can be understood as first generating a random matrix <inline-formula id="pcbi.1004792.e025"><alternatives><graphic id="pcbi.1004792.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>, then multiplying by <italic>ρ</italic>/<italic>ρ</italic><sub>0</sub> where <italic>ρ</italic><sub>0</sub> = 1 is the spectral radius of <inline-formula id="pcbi.1004792.e026"><alternatives><graphic id="pcbi.1004792.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> and <italic>ρ</italic> is the desired spectral radius of the initial weight matrix. Here the spectral radius is the largest absolute value of the eigenvalues.</p>
<p>To initialize an excitatory-inhibitory network with an arbitrary pattern of connections, we similarly first generate a matrix <inline-formula id="pcbi.1004792.e027"><alternatives><graphic id="pcbi.1004792.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> and let <inline-formula id="pcbi.1004792.e028"><alternatives><graphic id="pcbi.1004792.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>ρ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> where <italic>ρ</italic><sub>0</sub> is the spectral radius of <inline-formula id="pcbi.1004792.e029"><alternatives><graphic id="pcbi.1004792.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>. Unlike in the case of random Gaussian matrices, the (asymptotically) exact spectral radius is usually unknown and must be computed numerically. Moreover, since the signs of the matrix elements are determined by the excitatory or inhibitory nature of the units, it is more natural to use a distribution over positive numbers to first generate <inline-formula id="pcbi.1004792.e030"><alternatives><graphic id="pcbi.1004792.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec,+</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1004792.e020">Eq 12</xref>). Many distributions, including the uniform and log-normal distributions, can be used; inspired by previous work [<xref ref-type="bibr" rid="pcbi.1004792.ref054">54</xref>], here we use the gamma distribution to initialize the recurrent weight matrix <inline-formula id="pcbi.1004792.e031"><alternatives><graphic id="pcbi.1004792.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>rec,+</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>. The means <italic>μ</italic><sub><italic>E</italic></sub> (excitatory) and <italic>μ</italic><sub><italic>I</italic></sub> (inhibitory) of the gamma distributions are chosen to balance the excitatory and inhibitory inputs to each unit [<xref ref-type="bibr" rid="pcbi.1004792.ref055">55</xref>], i.e., ∑<sub><italic>j</italic> ∈ exc</sub> |<italic>μ</italic><sub><italic>j</italic></sub>| = ∑<sub><italic>j</italic> ∈ inh</sub> |<italic>μ</italic><sub><italic>j</italic></sub>|, with the overall mean set by the imposed spectral radius <italic>ρ</italic>. We did not use the “initialization trick” of [<xref ref-type="bibr" rid="pcbi.1004792.ref056">56</xref>], as this requires the existence of self-connections.</p>
<p>For the input weight matrix <inline-formula id="pcbi.1004792.e032"><alternatives><graphic id="pcbi.1004792.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>in,+</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> and output weight matrix <inline-formula id="pcbi.1004792.e033"><alternatives><graphic id="pcbi.1004792.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msubsup><mml:mi>W</mml:mi> <mml:mn>0</mml:mn> <mml:mtext>out,+</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>, we initialize with small positive numbers drawn from a uniform distribution.</p>
</sec>
<sec id="sec007">
<title>Training RNNs with gradient descent</title>
<p>To train an RNN, we assume that at each time step (or subset of time steps) there is a correct set of target outputs <inline-formula id="pcbi.1004792.e034"><alternatives><graphic id="pcbi.1004792.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>target</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> that depend on the current and previous history of inputs <bold>u</bold><sub><italic>t</italic>′</sub> for <italic>t</italic>′ ≤ <italic>t</italic>, i.e., we only consider tasks that can be translated into a “supervised” form. The goal is then to find network parameters, which we collectively denote as <italic><bold>θ</bold></italic>, that minimize the difference between the correct output and the actual output of the network. More generally, we minimize an objective function <inline-formula id="pcbi.1004792.e035"><alternatives><graphic id="pcbi.1004792.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that includes not only this error but other terms such as an <italic>L</italic><sub>1</sub>-regularization term (for encouraging sparse weights or activation patterns) that influence the types of solutions found by the training algorithm. We begin with the case where the objective function depends only on the error; one possibility for the <italic>loss</italic> <inline-formula id="pcbi.1004792.e036"><alternatives><graphic id="pcbi.1004792.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that measures the difference between the correct and actual outputs is the squared sum of differences averaged over <italic>N</italic><sub>trials</sub> trials, <italic>N</italic><sub>out</sub> outputs, and <italic>N</italic><sub>time</sub> time points:
<disp-formula id="pcbi.1004792.e037"><alternatives><graphic id="pcbi.1004792.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:munderover> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula> <disp-formula id="pcbi.1004792.e038"><alternatives><graphic id="pcbi.1004792.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mtext>out</mml:mtext></mml:msub> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>time</mml:mtext></mml:msub></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>time</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>M</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>ℓ</mml:mi></mml:mrow> <mml:mtext>error</mml:mtext></mml:msubsup> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">z</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">z</mml:mi> <mml:mi>t</mml:mi> <mml:mtext>target</mml:mtext></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msub></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
For each trial <italic>n</italic> in <xref ref-type="disp-formula" rid="pcbi.1004792.e038">Eq 16</xref>, (<bold>z</bold><sub><italic>t</italic></sub>)<sub>ℓ</sub> is the ℓ-th output, at time <italic>t</italic>, of the discretized network in <xref ref-type="disp-formula" rid="pcbi.1004792.e011">Eq 9</xref>. The error mask <italic>M</italic><sup>error</sup> is a matrix of ones and zeros that determines whether the error in output ℓ at time <italic>t</italic> should be taken into account. In many decision-making tasks, for example, this allows us to train networks by specifying only the final, but not the intermediate, time course for the outputs.</p>
<p>In gradient descent training the parameters of the network are updated iteratively according to (for more sophisticated forms of gradient descent see, e.g., [<xref ref-type="bibr" rid="pcbi.1004792.ref057">57</xref>])
<disp-formula id="pcbi.1004792.e039"><alternatives><graphic id="pcbi.1004792.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">δ</mml:mi> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
where <italic>i</italic> denotes the iteration. The parameter change, <italic><bold>δθ</bold></italic>, is taken to be proportional to the negative gradient of the objective function with respect to the network parameters as
<disp-formula id="pcbi.1004792.e040"><alternatives><graphic id="pcbi.1004792.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mrow><mml:mi mathvariant="bold-italic">δ</mml:mi> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>η</mml:mi> <mml:mo>∇</mml:mo> <mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
where <italic>η</italic> is the <italic>learning rate</italic> and <inline-formula id="pcbi.1004792.e041"><alternatives><graphic id="pcbi.1004792.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mo>∇</mml:mo> <mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mo>∇</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the value of the gradient evaluated on the parameters from iteration <italic>i</italic> − 1. Importantly, the required gradient can be computed efficiently by backpropagation through time (BPTT) [<xref ref-type="bibr" rid="pcbi.1004792.ref058">58</xref>] and <italic>automatically</italic> by the Python machine library Theano [<xref ref-type="bibr" rid="pcbi.1004792.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref042">42</xref>]. In component form the parameter update at iteration <italic>i</italic> is given by
<disp-formula id="pcbi.1004792.e042"><alternatives><graphic id="pcbi.1004792.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="script">E</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
where <italic>k</italic> runs over all the parameters of the network that are being optimized. Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e039">17</xref> and <xref ref-type="disp-formula" rid="pcbi.1004792.e040">18</xref> are motivated by the observation that, for a small change <italic><bold>δθ</bold></italic> in the value of the parameters, the corresponding change in the value of the objective function is given by
<disp-formula id="pcbi.1004792.e043"><alternatives><graphic id="pcbi.1004792.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">δ</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>≃</mml:mo> <mml:mo>∇</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">δ</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>=</mml:mo> <mml:mo>|</mml:mo> <mml:mo>∇</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">δ</mml:mi> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>|</mml:mo> <mml:mo form="prefix">cos</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
where |⋅| denotes the norm of a vector and <italic>ϕ</italic> is the angle between <inline-formula id="pcbi.1004792.e044"><alternatives><graphic id="pcbi.1004792.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mo>∇</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <italic><bold>δθ</bold></italic>. This change is most negative when <italic>ϕ</italic> = 180°, i.e., when the change in parameters is in the opposite direction of the gradient. “Minibatch stochastic” refers to the fact that the gradient of the objective function <inline-formula id="pcbi.1004792.e045"><alternatives><graphic id="pcbi.1004792.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is only <italic>approximated</italic> by evaluating <inline-formula id="pcbi.1004792.e046"><alternatives><graphic id="pcbi.1004792.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> over a relatively small number of trials (in particular, smaller than or comparable to the number of trial conditions) rather than using many trials to obtain the “true” gradient. Intuitively, this improves convergence to a satisfactory solution when the objective function is a highly complicated function of the parameters by stochastically sampling the gradient and thereby escaping saddle points [<xref ref-type="bibr" rid="pcbi.1004792.ref059">59</xref>] or poor local minima, while still performing an averaged form of gradient descent over many stochastic updates.</p>
<p>Even so, SGD with the objective function given in Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e037">15</xref> and <xref ref-type="disp-formula" rid="pcbi.1004792.e038">16</xref> often fails to converge to a solution when the network must learn dependencies between distant time points [<xref ref-type="bibr" rid="pcbi.1004792.ref060">60</xref>]. To remedy this problem, which is due to some gradient components being too large (<italic>exploding</italic> gradients) and some gradient components being too small (<italic>vanishing</italic> gradients), we follow [<xref ref-type="bibr" rid="pcbi.1004792.ref035">35</xref>] in making two modifications. First, the exploding gradient problem is addressed by simply “clipping” the gradient when its norm exceeds a maximum <italic>G</italic>: instead of <xref ref-type="disp-formula" rid="pcbi.1004792.e040">Eq 18</xref> for the direction and size of the update, we use
<disp-formula id="pcbi.1004792.e047"><alternatives><graphic id="pcbi.1004792.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mi mathvariant="bold-italic">δ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo>∇</mml:mo><mml:msup><mml:mi>ℰ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mfrac><mml:mi>G</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mo>∇</mml:mo><mml:msup><mml:mi>ℰ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="1pt"/><mml:mo>|</mml:mo><mml:mo>∇</mml:mo><mml:msup><mml:mi>ℰ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>i-1</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>&gt;</mml:mo><mml:mtext>G</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo>∇</mml:mo><mml:msup><mml:mi>ℰ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
Second, the vanishing gradient problem is addressed by modifying the objective function with the addition of a regularization term:
<disp-formula id="pcbi.1004792.e048"><alternatives><graphic id="pcbi.1004792.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mo>Ω</mml:mo></mml:msub> <mml:msub><mml:mo>Ω</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
<disp-formula id="pcbi.1004792.e049"><alternatives><graphic id="pcbi.1004792.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e049" xlink:type="simple"/><mml:math display="block" id="M49"><mml:mrow><mml:msub><mml:mo>Ω</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>time</mml:mtext></mml:msub></mml:munderover> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mo>|</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:msup><mml:mo>|</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msup><mml:mo>|</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
In <xref ref-type="disp-formula" rid="pcbi.1004792.e048">Eq 22</xref> the multiplier <italic>λ</italic><sub><italic>Ω</italic></sub> determines the effect of the regularization term <italic>Ω</italic><sub><italic>n</italic></sub>, with no effect for <italic>λ</italic><sub><italic>Ω</italic></sub> = 0. In <xref ref-type="disp-formula" rid="pcbi.1004792.e049">Eq 23</xref>, the first term in parentheses is the ratio between the squared norms of two vectors, which we would like to be close to 1. The somewhat opaque (row) vector expression in the numerator can be unpacked as (cf. <xref ref-type="disp-formula" rid="pcbi.1004792.e009">Eq 7</xref>)
<disp-formula id="pcbi.1004792.e050"><alternatives><graphic id="pcbi.1004792.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mrow><mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>k</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula> <disp-formula id="pcbi.1004792.e051"><alternatives><graphic id="pcbi.1004792.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msup><mml:mi>W</mml:mi> <mml:mtext>rec</mml:mtext></mml:msup></mml:mfenced> <mml:mo>⊙</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mi>j</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
Here each component <italic>r</italic>′(<italic>x</italic><sub><italic>t</italic></sub>) of <bold>r</bold>′(<bold>x</bold><sub><italic>t</italic></sub>) is the derivative of the <italic>f-I</italic> curve, i.e., 1 if <italic>x</italic> &gt; 0 and 0 otherwise in the case of rectification, and ⊙ denotes element-wise multiplication of two vectors. For consistency in notation we treat <inline-formula id="pcbi.1004792.e052"><alternatives><graphic id="pcbi.1004792.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mo>′</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> here as a row vector. One subtlety in the implementation of this term is that, for computational efficiency, only the “immediate” derivative of <italic>Ω</italic><sub><italic>n</italic></sub> with respect to the network parameters is used, i.e., with <bold>x</bold><sub><italic>t</italic></sub> and <inline-formula id="pcbi.1004792.e053"><alternatives><graphic id="pcbi.1004792.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> treated as constant [<xref ref-type="bibr" rid="pcbi.1004792.ref035">35</xref>]. The relevant network parameters in this case are the elements of the trained weight matrix <italic>W</italic><sup>rec,plastic</sup>, which is related to <italic>W</italic><sup>rec</sup> through <xref ref-type="disp-formula" rid="pcbi.1004792.e022">Eq 14</xref>.</p>
<p>The role of the regularization term <italic>Ω</italic><sub><italic>n</italic></sub> is to preserve the size of the gradients as errors are backpropagated through time. This is accomplished by preserving the norm of ∂<bold>x</bold><sub><italic>t</italic></sub>/∂<bold>x</bold><sub><italic>t</italic>−1</sub>, which propagates errors in time [<xref ref-type="bibr" rid="pcbi.1004792.ref035">35</xref>], along <inline-formula id="pcbi.1004792.e054"><alternatives><graphic id="pcbi.1004792.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, which is the direction in which the change in the objective function is greatest with respect to <bold>x</bold><sub><italic>t</italic></sub>. More intuitively, the impact of the regularization term on network dynamics can be understood by noting that if ∂<bold>x</bold><sub><italic>t</italic></sub>/∂<bold>x</bold><sub><italic>t</italic>′</sub> is small for some <italic>t</italic>′ &lt; <italic>t</italic> then, by definition, <bold>x</bold><sub><italic>t</italic></sub> does not depend on small changes in <bold>x</bold><sub><italic>t</italic>′</sub>, which may occur when <bold>x</bold> is close to an attractor. Preserving the norm of ∂<bold>x</bold><sub><italic>t</italic></sub>/∂<bold>x</bold><sub><italic>t</italic>−1</sub> through time therefore encourages the network to remain at the boundaries between basins of attraction and thus encourages longer computation times. For instance, this results in perceptual decision networks that can integrate their inputs for a long period of time, before converging to one of the choice attractors. We note that, although the numerator and denominator in <xref ref-type="disp-formula" rid="pcbi.1004792.e049">Eq 23</xref> appear, by the chain rule, to preserve the ratio of <inline-formula id="pcbi.1004792.e055"><alternatives><graphic id="pcbi.1004792.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> to <inline-formula id="pcbi.1004792.e056"><alternatives><graphic id="pcbi.1004792.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, this is only approximately true. Specifically,
<disp-formula id="pcbi.1004792.e057"><alternatives><graphic id="pcbi.1004792.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
because <inline-formula id="pcbi.1004792.e058"><alternatives><graphic id="pcbi.1004792.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, the component of <inline-formula id="pcbi.1004792.e059"><alternatives><graphic id="pcbi.1004792.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:math></alternatives></inline-formula> from time <italic>t</italic> − 1, depends on <bold>x</bold><sub><italic>t</italic>−1</sub> but not on <bold>x</bold><sub><italic>t</italic></sub>.</p>
<p>Finally, additional regularization terms may be included to change either the dynamics or the connectivity. For instance, there are two ways of obtaining sparse recurrent connectivity. First, we can impose a hard constraint that fixes a chosen subset of weights to be nonzero and modifiable by the optimization algorithm as described above. Second, we may apply a soft constraint by adding the sum of the <italic>L</italic><sub>1</sub>-norms of the weights to the objective function:
<disp-formula id="pcbi.1004792.e060"><alternatives><graphic id="pcbi.1004792.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mo>Ω</mml:mo></mml:msub> <mml:msub><mml:mo>Ω</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:msup><mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:mo>|</mml:mo> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
In addition, we may choose to encourage solutions with small firing rates through regularization of the <italic>L</italic><sub>2</sub>-norms of the firing rates [<xref ref-type="bibr" rid="pcbi.1004792.ref008">8</xref>]:
<disp-formula id="pcbi.1004792.e061"><alternatives><graphic id="pcbi.1004792.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e061" xlink:type="simple"/><mml:math display="block" id="M61"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>trials</mml:mtext></mml:msub></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>λ</mml:mi> <mml:mo>Ω</mml:mo></mml:msub> <mml:msub><mml:mo>Ω</mml:mo> <mml:mi>n</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn> <mml:mtext>fr</mml:mtext></mml:msubsup> <mml:msubsup><mml:mi>R</mml:mi> <mml:mi>n</mml:mi> <mml:mtext>fr</mml:mtext></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi> <mml:mn>1</mml:mn> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:msup><mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:mo>|</mml:mo> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mtext>rec</mml:mtext></mml:msubsup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula> <disp-formula id="pcbi.1004792.e062"><alternatives><graphic id="pcbi.1004792.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e062" xlink:type="simple"/><mml:math display="block" id="M62"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi> <mml:mi>n</mml:mi> <mml:mtext>fr</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>N</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>time</mml:mtext></mml:msub></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mtext>time</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula>
where (<bold>r</bold><sub><italic>t</italic></sub>)<sub><italic>j</italic></sub> is the firing rate of the <italic>j</italic>-th unit at time <italic>t</italic> on each trial. Again, we gain flexibility in defining more complex regularization terms because Theano computes the necessary gradients using BPTT. Although BPTT is simply a specialized chain rule for neural networks, automatic differentiation frees us from implementing new gradients each time the objective function is changed. This greatly facilitates the exploration of soft constraints such as those considered in [<xref ref-type="bibr" rid="pcbi.1004792.ref008">8</xref>].</p>
</sec>
<sec id="sec008">
<title>Training protocol</title>
<p>To demonstrate the robustness of the training method, we used many of the same parameters to train all tasks (<xref ref-type="table" rid="pcbi.1004792.t001">Table 1</xref>). In particular, the learning rate <italic>η</italic>, maximum gradient norm <italic>G</italic>, and the strength <italic>λ</italic><sub><italic>Ω</italic></sub> of the vanishing-gradient regularization term were kept constant for all networks. We also successfully trained networks with values for <italic>G</italic> and <italic>λ</italic><sub><italic>Ω</italic></sub> that were larger than the default values given in <xref ref-type="table" rid="pcbi.1004792.t001">Table 1</xref>. When one or two parameters were modified to illustrate a particular training procedure, they are noted in the task descriptions. For instance, the number of trials used for each parameter update (gradient batch size) was the same in all networks except for the context-dependent integration task (to account for the large number of conditions) and sequence execution task (because of online training, where the number of trials is one). As a simple safeguard against extreme fine-tuning, we removed all weights below a threshold, <italic>w</italic><sub>min</sub>, after training. We also note that, unlike in previous work (e.g., [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>]), we used the same level of stimulus and noise for both training and testing.</p>
<table-wrap id="pcbi.1004792.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.t001</object-id>
<label>Table 1</label>
<caption>
<title>Parameters for stochastic gradient descent (SGD) training of recurrent neural networks (RNNs).</title>
<p>Unless noted otherwise in the task description, networks were trained and run with the parameters listed here.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004792.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Parameter</th>
<th align="center">Symbol</th>
<th align="center">Default value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Learning rate</td>
<td align="center"><italic>η</italic></td>
<td align="center">0.01</td>
</tr>
<tr>
<td align="left">Maximum gradient norm</td>
<td align="center"><italic>G</italic></td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">Multiplier for vanishing-gradient regularization <italic>Ω</italic></td>
<td align="center"><italic>λ</italic><sub><italic>Ω</italic></sub></td>
<td align="center">2</td>
</tr>
<tr>
<td align="left">Unit time constant</td>
<td align="center"><italic>τ</italic></td>
<td align="center">100 ms</td>
</tr>
<tr>
<td align="left">Time step (training)</td>
<td align="center">Δ<italic>t</italic></td>
<td align="center"><italic>τ</italic>/5</td>
</tr>
<tr>
<td align="left">Time step (testing)</td>
<td align="center">Δ<italic>t</italic></td>
<td align="center">0.5 ms</td>
</tr>
<tr>
<td align="left">Initial spectral radius of recurrent weight matrix</td>
<td align="center"><italic>ρ</italic></td>
<td align="center">1.5</td>
</tr>
<tr>
<td align="left">Gradient minibatch size</td>
<td align="center"><italic>N</italic><sub>trials</sub></td>
<td align="center">20</td>
</tr>
<tr>
<td align="left">Baseline input</td>
<td align="center"><italic>u</italic><sup>0</sup></td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="left">Standard deviation for input noise</td>
<td align="center"><italic>σ</italic><sub>in</sub></td>
<td align="center">0.01</td>
</tr>
<tr>
<td align="left">Standard deviation for recurrent noise</td>
<td align="center"><italic>σ</italic><sub>rec</sub></td>
<td align="center">0.15</td>
</tr>
<tr>
<td align="left">Minimum weight threshold after training</td>
<td align="center"><italic>w</italic><sub>min</sub></td>
<td align="center">10<sup>−4</sup></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Code for generating the figures in this work are available from <ext-link ext-link-type="uri" xlink:href="https://github.com/xjwanglab/pycog" xlink:type="simple">https://github.com/xjwanglab/pycog</ext-link>. The distribution includes code for training the networks, running trials, performing analyses, and creating the figures.</p>
</sec>
</sec>
<sec id="sec009" sec-type="results">
<title>Results</title>
<p>In this section we present the results of applying the training framework to well-known experimental paradigms in systems neuroscience: perceptual decision-making [<xref ref-type="bibr" rid="pcbi.1004792.ref061">61</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref063">63</xref>], context-dependent integration [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>], multisensory integration [<xref ref-type="bibr" rid="pcbi.1004792.ref064">64</xref>], parametric working memory [<xref ref-type="bibr" rid="pcbi.1004792.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref065">65</xref>], and eye-movement sequence generation [<xref ref-type="bibr" rid="pcbi.1004792.ref066">66</xref>]. In addition to establishing the relative ease of obtaining networks that perform the selected tasks, we show several single-neuron and population analyses associated with each paradigm. These analyses demonstrate that trained networks exhibit many, though not yet all, features observed in recorded neurons, and the study of these networks therefore has the potential to yield insights into biological neural circuits. A summary of the tasks can be found in <xref ref-type="table" rid="pcbi.1004792.t002">Table 2</xref>.</p>
<table-wrap id="pcbi.1004792.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.t002</object-id>
<label>Table 2</label>
<caption>
<title>Summary of tasks.</title>
<p>In the multisensory integration and parametric working memory tasks, networks receive both positively (pos.; increasing function) and negatively (neg.; decreasing function) tuned versions of the same input.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004792.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Fig</th>
<th align="left">Task</th>
<th align="left">Network inputs</th>
<th align="left">Outputs</th>
<th align="left">Features</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2</xref></td>
<td align="left">Perceptual decision making (variable stimulus duration, VS; reaction-time, RT) [<xref ref-type="bibr" rid="pcbi.1004792.ref061">61</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref063">63</xref>]</td>
<td align="left">Motion 1<break/>Motion 2<break/>Start of stimulus</td>
<td align="left">Choice 1<break/>Choice 2</td>
<td align="left">Psychometric curves (VS, RT)<break/>Percent correct as a function of stimulus duration (VS)<break/>Reaction-time as a function of coherence, distribution (RT)<break/>Coherence-dependent firing rates (VS)<break/>Convergence of firing rates aligned to decision time (RT)</td>
</tr>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3</xref></td>
<td align="left">Perceptual decision making (fixed stimulus duration)</td>
<td align="left">Motion 1<break/>Motion 2</td>
<td align="left">Choice 1<break/>Choice 2</td>
<td align="left">Psychometric curves<break/>No Dale’s principle vs. Dale’s principle<break/>Dense vs. constrained initial connectivity</td>
</tr>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g004">Fig 4</xref></td>
<td align="left">Context-dependent integration [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>]</td>
<td align="left">Motion 1<break/>Motion 2<break/>Color 1<break/>Color 2<break/>Motion context<break/>Color context</td>
<td align="left">Choice 1<break/>Choice 2</td>
<td align="left">Psychometric curves, gating<break/>State-space analysis<break/>Mixed selectivity of single-unit responses<break/>Distribution of regression coefficients</td>
</tr>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g005">Fig 5</xref></td>
<td align="left">Context-dependent integration</td>
<td align="left">Same</td>
<td align="left">Same</td>
<td align="left">Two areas</td>
</tr>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g006">Fig 6</xref></td>
<td align="left">Multisensory integration [<xref ref-type="bibr" rid="pcbi.1004792.ref064">64</xref>]</td>
<td align="left">Pos. tuned visual<break/>Neg. tuned visual<break/>Pos. tuned auditory<break/>Neg. tuned auditory<break/>Start of stimulus</td>
<td align="left">Choice high<break/>Choice low</td>
<td align="left">Psychometric curves with multisensory enhancement<break/>Heterogeneous selectivity in single-unit responses</td>
</tr>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7</xref></td>
<td align="left">Parametric working memory [<xref ref-type="bibr" rid="pcbi.1004792.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref065">65</xref>]</td>
<td align="left">Pos. tuned frequency<break/>Neg. tuned frequency</td>
<td align="left">Choice <italic>f</italic><sub>1</sub> &gt; <italic>f</italic><sub>2</sub><break/>Choice <italic>f</italic><sub>1</sub> &lt; <italic>f</italic><sub>2</sub></td>
<td align="left">Heterogeneous tuning<break/>Correlation of tuning across population<break/>Change of tuning during trial</td>
</tr>
<tr>
<td align="center"><xref ref-type="fig" rid="pcbi.1004792.g008">Fig 8</xref></td>
<td align="left">Sequence execution [<xref ref-type="bibr" rid="pcbi.1004792.ref066">66</xref>]</td>
<td align="left">Targets (9)<break/>Sequence (8)</td>
<td align="left">Eye position (<italic>x</italic>, <italic>y</italic>)</td>
<td align="left">Continuous trials<break/>Online learning<break/>State-space analysis: hierarchical decision making</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The tasks presented in this section represent only a small sample of the diversity of tasks used in neuroscience. In addition, we have chosen—in most cases arbitrarily—a simple set of constraints that do not necessarily reflect the full biological reality. Nevertheless, our work provides the foundation for further exploration of the constraints, regularizations, and network architectures required to achieve the greatest correspondence between trained RNNs and biological neural networks.</p>
<sec id="sec010">
<title>Perceptual decision-making task</title>
<p>Many experimental paradigms in neuroscience require subjects to integrate noisy sensory stimuli in order to choose between two actions (<xref ref-type="fig" rid="pcbi.1004792.g001">Fig 1</xref>). Here we present networks trained to perform two variants of perceptual decision-making inspired by the two common variants of the random dot motion discrimination task [<xref ref-type="bibr" rid="pcbi.1004792.ref061">61</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref063">63</xref>]. For both versions, the network has 100 units (80 excitatory and 20 inhibitory) and receives two noisy inputs, one indicating evidence for choice 1 and the other for choice 2, and must decide which is larger. Importantly, the network is not explicitly told to integrate—it is instead only required to “make a decision” following the onset of stimulus by holding a high value in the output corresponding to the higher input, and a low value in the other.</p>
<p>In the variable stimulus-duration version of the task (<xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2A</xref>), stimulus durations are drawn randomly from a truncated exponential distribution (we note that this is often called the “fixed-duration” version because the experimentalist sets the reaction time, in contrast to the “reaction-time” version in which the subject chooses when to respond). This minimizes the network’s ability to anticipate the end of the stimulus and therefore encourages the network to continue integrating information as long as the stimulus is present [<xref ref-type="bibr" rid="pcbi.1004792.ref063">63</xref>]. In the reaction-time version (<xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2B</xref>), the network must respond soon after the onset of an ongoing stimulus. To control the speed-accuracy tradeoff, the target outputs during training did not require the network to commit to a decision immediately but instead after a short delay [<xref ref-type="bibr" rid="pcbi.1004792.ref062">62</xref>]; the delay determines the cost incurred for answering early but incorrectly versus correctly but at a later time.</p>
<fig id="pcbi.1004792.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Perceptual decision-making task.</title>
<p>(A) Inputs (upper) and target outputs (lower) for a perceptual decision-making task with variable stimulus duration, which we refer to as VS here. The choice 1 output must hold low during fixation (fix.), then high during the decision (dec.) period if the choice 1 input is larger than choice 2 input, low otherwise, and similarly for the choice 2 output. There are no constraints on output during the stimulus period. (B) Inputs and target outputs for the reaction-time version of the integration task, which we refer to as RT. Here the outputs are encouraged to respond after a short delay following the onset of stimulus. The reaction time is defined as the time it takes for the outputs to reach a threshold. (C) Psychometric function for the VS version, showing the percentage of trials on which the network chose choice 1 as a function of the signed coherence. Coherence is a measure of the difference between evidence for choice 1 and evidence for choice 2, and positive coherence indicates evidence for choice 1 and negative for choice 2. Solid line is a fit to a cumulative Gaussian distribution. (D) Psychometric function for the RT version. (E) Percentage of correct responses as a function of stimulus duration in the VS version, for each nonzero coherence level. (F) Reaction time for correct trials in the RT version as a function of coherence. Inset: Distribution of reaction times on correct trials. (G) Example activity of a single unit in the VS version across all correct trials, averaged within conditions after aligning to the onset of the stimulus. Solid (dashed) lines denote positive (negative) coherence. (H) Example activity of a single unit in the RT version, averaged within conditions and across all correct trials aligned to the reaction time.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g002" xlink:type="simple"/>
</fig>
<p>All trials begin with a “fixation” period during which both outputs must maintain a low value, requiring the network to react only to the stimulus. The fixation can be enforced during training in several ways, including a variable fixation period whose duration is drawn from another truncated exponential distribution, or by introducing “catch trials” when no stimuli are presented. For simplicity, here we used a small proportion of catch trials mixed into the training, together with an additional, unambiguous start cue that signals the onset of stimulus.</p>
<p>Networks trained for both versions of the task show comparable performance in their psychometric functions (<xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2C and 2D</xref>), which are the percentage of trials on which the network chose choice 1 as a function of the signed coherence. Coherence is a measure of the difference between evidence for choice 1 and evidence for choice 2, and positive coherence indicates evidence for choice 1 and negative for choice 2. In experiments with monkeys the signs correspond to inside and outside, respectively, the receptive field of the recorded neuron; although we do not show it here, this can be explicitly modeled by combining the present task with the model of “eye position” used in the sequence execution task (below). We emphasize that, unlike in the usual machine learning setting, our objective is not to achieve “perfect” performance. Instead, the networks were trained to an overall performance level of approximately 85% across all nonzero coherences to match the smooth psychometric profiles observed in behaving monkeys. We note that this implies that some networks exhibit a slight bias toward choice 1 or choice 2, as is the case with animal subjects unless care is taken to eliminate the bias through adjustment of the stimuli. Together with the input noise, the recurrent noise enables the network to smoothly interpolate between low-coherence choice 1 and low-coherence choice 2 trials, so that the network chooses choice 1 on approximately half the zero-coherence trials when there is no mean difference between the two inputs. Recurrent noise also forces the network to learn more robust solutions than would be the case without.</p>
<p>For the variable stimulus duration version of the decision-making task, we computed the percentage of correct responses as a function of the stimulus duration for different coherences (<xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2E</xref>), showing that for easy, high-coherence trials the duration of the stimulus period only weakly affects performance [<xref ref-type="bibr" rid="pcbi.1004792.ref063">63</xref>]. In contrast, for difficult, low-coherence trials the network can improve its performance by integrating for a longer period of time. <xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2G</xref> shows the activity of an example unit (selective for choice 1) across all correct trials, averaged within conditions after aligning to the onset of the stimulus. The activity shows a clear tuning of the unit to different signed coherences.</p>
<p>For the reaction-time version of the task, we defined a threshold for the output (here arbitrarily taken to be 1, slightly less than the target of 1.2 during training) that constituted a “decision.” The time it takes to reach this threshold is called the <italic>reaction time</italic>, and <xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2F</xref> shows this reaction time as a function of coherence for correct trials, while the inset shows the distribution of reaction times on correct trials. In the case of the reaction-time version of the task, it is interesting to consider the activity of single units aligned to the decision time in each trial, which shows that the firing rate of the unit converges to a similar value for all positive coherences (<xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2H</xref>) [<xref ref-type="bibr" rid="pcbi.1004792.ref062">62</xref>]. This is a nontrivial observation in both experiment [<xref ref-type="bibr" rid="pcbi.1004792.ref062">62</xref>] and model, as the decision threshold is only imposed on the outputs and not on the recurrent units themselves.</p>
<p>To illustrate the effect of constraints on connectivity structure—but not on performance—we also trained three networks for the fixed stimulus-duration version of the task shown in <xref ref-type="fig" rid="pcbi.1004792.g002">Fig 2A</xref>. For these networks we did not use a start cue. In the first network, no constraints were imposed on the connection weights except for the absence of self-connections (<xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3A</xref>). The second network was required to satisfy Dale’s principle, with a 4-to-1 ratio of the number of excitatory to inhibitory units, and purely excitatory inputs and outputs (<xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3B</xref>). The third network was similar, but with the additional constraint that the inputs that signal evidence for choice 1 and choice 2 project to distinct groups of recurrent units and decisions are read out from the same group of excitatory units (<xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3C</xref>). The two groups of excitatory units send zero excitatory projections to each other, communicating instead only through the inhibitory units and excitatory units that receive no inputs.</p>
<fig id="pcbi.1004792.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Perceptual decision-making networks with different constraints.</title>
<p>(A) Psychometric function (percent choice 1 as a function of signed coherence) and connection weights (input, upper-right; recurrent, upper-left; and output, lower) for a network in which all weights may be positive or negative, trained for a perceptual decision-making task. Connections go from columns (“pre-synaptic”) to rows (“post-synaptic”), with blue representing positive weights and red negative weights. Different color scales (arbitrary units) were used for the input, recurrent, and output matrices but are consistent across the three networks shown. In the psychometric function, solid lines are fits to a cumulative Gaussian distribution. In this and the networks in <italic>B</italic> and <italic>C</italic>, self-connections were not allowed. In each case 100 units were trained, but only the 25 units with the largest absolute selectivity index (<xref ref-type="disp-formula" rid="pcbi.1004792.e063">Eq 30</xref>) are shown, ordered from most selective for choice 1 (large positive) to most selective for choice 2 (large negative). (B) A network trained for the same task as in <italic>A</italic> but with the constraint that excitatory units may only project positive weights and inhibitory units may only project negative weights. All input weights were constrained to be excitatory, and the readout weights, considered to be “long-range,” were nonzero only for excitatory units. All connections except self-connections were allowed, but training resulted in a strongly clustered pattern of connectivity. Units are again sorted by selectivity but separately for excitatory and inhibitory units (20 excitatory, 5 inhibitory). (C) Same as <italic>B</italic> but with the additional constraint that excitatory recurrent units receiving input for choice 1 and excitatory recurrent units receiving input for choice 2 do not project to one another, and each group sends output to the corresponding choice.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g003" xlink:type="simple"/>
</fig>
<p>In all three cases, a clear structure could be discerned in the connectivity of the trained network by sorting the units by their selectivity index
<disp-formula id="pcbi.1004792.e063"><alternatives><graphic id="pcbi.1004792.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mrow><mml:msup><mml:mi>d</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow> <mml:msqrt><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula>
where <inline-formula id="pcbi.1004792.e064"><alternatives><graphic id="pcbi.1004792.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are the mean and variance of the unit’s activity, during the stimulus period, on trials in which the network chose choice 1, and similarly for <inline-formula id="pcbi.1004792.e065"><alternatives><graphic id="pcbi.1004792.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004792.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> for choice 2. For the network without separate excitatory and inhibitory units (<xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3A</xref>), clustering manifests in the form of strong excitation among units with similar <italic>d</italic>′ and strong inhibition between units with different <italic>d</italic>′. The learned input weights also excite one population and inhibit the other. In the case of the network with separate excitatory and inhibitory populations (<xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3B</xref>), units with different <italic>d</italic>′ interact primarily through inhibitory units [<xref ref-type="bibr" rid="pcbi.1004792.ref067">67</xref>]. Importantly, despite the fact that the recurrent weight matrix was initialized with dense, all-to-all connectivity, the two populations send fewer excitatory projections to each other after training. Similarly, despite the fact that the input weights initially send evidence for both choices to the two populations, after training the two groups receive evidence for different choices. Output weights also became segregated after training. In the third network this structure was imposed from the start, confirming that such a network could learn to perform the task (<xref ref-type="fig" rid="pcbi.1004792.g003">Fig 3C</xref>).</p>
</sec>
<sec id="sec011">
<title>Context-dependent integration task</title>
<p>In this section and the next we show networks trained for experimental paradigms in which making a correct decision requires integrating two separate sources of information. We first present a task inspired by the context-dependent integration task of [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>], in which a “context” cue indicates that one type of stimulus (the motion or color of the presented dots) should be integrated and the other completely ignored to make the optimal decision.</p>
<p>A network trained for the context-dependent integration task is able to integrate the relevant input while ignoring the irrelevant input. This is reflected in the psychometric functions, the percentage of trials on which the network chose choice 1 as a function of the signed motion and color coherences (<xref ref-type="fig" rid="pcbi.1004792.g004">Fig 4A</xref>). The network contains a total of 150 units, 120 of which are excitatory and 30 inhibitory. The training protocol was very similar to the (fixed-duration) single-stimulus decision-making task except for the presence of two independent stimuli and a set of context inputs that indicate the relevant stimulus. Because of the large number of conditions, we increased the number of trials for each gradient update to 50.</p>
<fig id="pcbi.1004792.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Context-dependent integration task.</title>
<p>(A) Psychometric function, showing the percentage of trials on which the network chose choice 1 as a function of the signed motion (upper) and signed color (lower) coherence in motion-context (black) and color-context (blue) trials. (B) Average population responses in state space during the stimulus period, projected to the 3-dimensional subspace capturing variance due to choice, motion, and color as in [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>]. Only correct trials were included. The task-related axes were obtained through a linear regression analysis. Note that “choice” here has a unit-specific meaning that depends on the preferred choice of the unit as determined by the selectivity index (<xref ref-type="disp-formula" rid="pcbi.1004792.e063">Eq 30</xref>). For both motion (black) and color (blue), coherences increase from light to dark. Upper plots show trials during the motion context, and lower plots show trials during the color context. (C) Normalized responses of four recurrent units during the stimulus period show mixed representation of task variables. Solid lines indicate the preferred choice and dashed lines the nonpreferred choice of each unit. (D) Denoised regression coefficients from the linear regression analysis. By definition, the coefficients for choice are almost exclusively positive.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g004" xlink:type="simple"/>
</fig>
<p>Previously, population responses in the monkey prefrontal cortex were studied by representing them as trajectories in neural state space [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>]. This was done by using linear regression to define the four orthogonal, task-related axes of choice, motion, color, and context. The projection of the population responses onto these axes reveals how the different task variables are reflected in the neural activity. <xref ref-type="fig" rid="pcbi.1004792.g004">Fig 4B</xref> shows the results of repeating this analysis [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>] with the trained network during the stimulus period. The regression coefficients (<xref ref-type="fig" rid="pcbi.1004792.g004">Fig 4D</xref>) reveal additional relationships between the task variables, which in turn reflect the mixed selectivity of individual units to different task parameters as shown by sorting and averaging trials according to different criteria (<xref ref-type="fig" rid="pcbi.1004792.g004">Fig 4C</xref>).</p>
<p>As a proof of principle, we trained an additional network that could perform the same task but consisted of separate “areas,” with one area receiving inputs and the other sending outputs (<xref ref-type="fig" rid="pcbi.1004792.g005">Fig 5B</xref>), which can be compared to the unstructured connectivity of the original network (<xref ref-type="fig" rid="pcbi.1004792.g005">Fig 5A</xref>). Here each area is conceived of as a cortical area containing a group of inhibitory units that only project locally to excitatory and inhibitory units in the same area. Thus there are no interareal connections originating from inhibitory units. The “sensory” area that receives inputs sends dense, “long-range” excitatory feedforward connections to the “motor” area from which outputs are read out, and receives “sparse” (connection probability 0.2) excitatory feedback projections from the motor area. This example illustrates the promise of using RNNs to explore how large-scale function may arise in the brain.</p>
<fig id="pcbi.1004792.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Constraining the connectivity.</title>
<p>Connectivity after training for the context-dependent integration task (<xref ref-type="fig" rid="pcbi.1004792.g004">Fig 4</xref>), when the connection matrix is (A) unstructured and (B) structured. Both networks consist of 150 units (120 excitatory, 30 inhibitory). In <italic>B</italic> the units are divided into two equal-sized “areas,” each with a local population of inhibitory units (<italic>I</italic><sub><italic>S</italic></sub> and <italic>I</italic><sub><italic>M</italic></sub>) that only project to units in the same area. The “sensory” area (green) receives excitatory inputs and sends dense, “long-range” excitatory feedforward connections <italic>E</italic><sub><italic>M</italic></sub> ← <italic>E</italic><sub><italic>S</italic></sub> to the “motor” area (orange) from which the outputs are read out. The sensory area receives sparse excitatory feedback projections <italic>E</italic><sub><italic>S</italic></sub> ← <italic>E</italic><sub><italic>M</italic></sub> from the motor area.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec012">
<title>Multisensory integration task</title>
<p>The multisensory integration task of [<xref ref-type="bibr" rid="pcbi.1004792.ref064">64</xref>] also presents the animal—rats, in this case—with two sources of information. In contrast to the previous task, however, in the multisensory integration task it is advantageous for the animal to integrate both sources of information when they are available. Specifically, visual flashes and auditory clicks were presented at rates between 9 events/sec and 16 events/sec, and the animal was required to determine whether the inputs were below or above the threshold of 12.5 events/sec. When both visual and auditory inputs were present, they were congruent (presented at the same rate). A network trained for this task is also given one or more congruent inputs, and can improve its performance by combining both inputs when they are available (<xref ref-type="fig" rid="pcbi.1004792.g006">Fig 6A and 6B</xref>). The network contains 150 units, 120 of which are excitatory and 30 inhibitory. A third of the units in the network (both excitatory and inhibitory) received only visual input, another third only auditory input, and the remaining third did not receive any input. Outputs were read out from the entire excitatory population.</p>
<fig id="pcbi.1004792.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Multisensory integration task.</title>
<p>(A) Example inputs for visual only (left), auditory only (middle), and multisensory (both visual and auditory, right) trials. Network units receive both positively tuned (increasing function of event rate) and negatively tuned (decreasing function of event rate) inputs; panels here show positively tuned input corresponding to a rate of 13 events/sec, just above the discrimination boundary. As in the single-stimulus perceptual decision-making task, the outputs of the network were required to hold low during “fixation” (before stimulus onset), then the output corresponding to a high rate was required to hold high if the input was above the decision boundary and low otherwise, and vice versa for the output corresponding to a low rate. (B) Psychometric functions (percentage of choice high as a function of the event rate) for visual, auditory, and multisensory trials show multisensory enhancement. (C) Sorted activity on visual only and auditory only trials for three units selective for choice (high vs. low, left), modality (visual vs. auditory, middle), and both (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g006" xlink:type="simple"/>
</fig>
<p>The training was again mostly similar to the (fixed-duration) single-stimulus perceptual decision-making task, except for the presence of two congruent inputs on multisensory trials. However, in the present task the network must determine whether the given input is larger or smaller than an arbitrary strength, in contrast to the previous integration tasks where two inputs are compared to each other. As a result, giving the network both positively tuned (increasing function of event rate) and negatively tuned (decreasing function of event rate) inputs [<xref ref-type="bibr" rid="pcbi.1004792.ref068">68</xref>] greatly improved training. Although gradient-descent training can find a solution when the inputs are purely positively tuned, this results in much longer training times and more idiosyncratic unit activities. This illustrates that, while RNN training methods are powerful, they must be supplemented with knowledge gained from experiments and previous modeling studies. As in experimentally recorded neurons, the units of the network exhibit heterogeneous responses, with some units showing selectivity to choice, others to modality, and still others showing mixed selectivity (<xref ref-type="fig" rid="pcbi.1004792.g006">Fig 6C</xref>).</p>
<p>The context-dependent and multisensory integration tasks represent the two end-cases of when two separate sources of information are available for making a decision. It is of great interest for future inquiry how the <italic>same</italic> network or set of networks may switch from completely ignoring one input to using both inputs to make the optimal decision depending on the task.</p>
</sec>
<sec id="sec013">
<title>Parametric working memory task</title>
<p>One of the most important—and therefore one of the most widely studied—cognitive functions is working memory, the ability to maintain and manipulate information for several seconds during the planning and execution of a task [<xref ref-type="bibr" rid="pcbi.1004792.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref070">70</xref>]. Working memory has notably been studied in the context of both oculomotor parametric working memory [<xref ref-type="bibr" rid="pcbi.1004792.ref071">71</xref>] and vibrotactile frequency discrimination [<xref ref-type="bibr" rid="pcbi.1004792.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref065">65</xref>], and here we trained a network to perform a task based on the frequency discrimination task. In this task, two temporally separated stimuli, represented by constant inputs whose magnitudes are proportional to the frequency (<xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7A</xref>), are presented and the network must determine which of the two is of higher frequency. This requires the network to remember the frequency of the first input <italic>f</italic><sub>1</sub> throughout the 3-second delay period in order to compare to the second input <italic>f</italic><sub>2</sub> at the end of the delay period. The network contains a total of 500 units (400 excitatory, 100 inhibitory), with a connection probability of 0.1 from excitatory units to all other units and 0.5 from inhibitory units to all other units; these connection probabilities are consistent with what is known for local microcircuits in cortex [<xref ref-type="bibr" rid="pcbi.1004792.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref053">53</xref>]. During training only, the delay was varied by uniformly sampling from the range 2.5–3.5 seconds. As in the multisensory integration task, because the network must compare a single input against itself (rather than comparing two simultaneous inputs to each other), it is helpful for the network to receive both positively tuned and negatively tuned inputs.</p>
<fig id="pcbi.1004792.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Parametric working memory task.</title>
<p>(A) Sample positively tuned inputs, showing the case where <italic>f</italic><sub>1</sub> &gt; <italic>f</italic><sub>2</sub> (upper) and <italic>f</italic><sub>1</sub> &lt; <italic>f</italic><sub>2</sub> (lower). Recurrent units also receive corresponding negatively tuned inputs. (B) Percentage of correct responses for different combinations of <italic>f</italic><sub>1</sub> and <italic>f</italic><sub>2</sub>. This plot also defines the colors used for each condition, labeled by <italic>f</italic><sub>1</sub>, in the remainder of the figure. Due to the overlap in the values of <italic>f</italic><sub>1</sub>, there are 7 distinct colors representing 10 trial conditions. (C) Lower: Correlation of the tuning <italic>a</italic><sub>1</sub> (see text) at different time points to the tuning in the middle of the first stimulus period (blue) and middle of the delay period (green). Upper: The tuning at the end of delay vs. middle of the first stimulus (left) and the end of delay vs. middle of the delay (right). (D) Single-unit activity for a unit that is positively tuned for <italic>f</italic><sub>1</sub> during both stimulus periods (left), and for a unit that is positively tuned during the first stimulus period but negatively tuned during the second stimulus period (right). (E) Proportion of significantly tuned units based on a simple linear regression of the firing rates as a function of <italic>f</italic><sub>1</sub> at each time point.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g007" xlink:type="simple"/>
</fig>
<p>The network’s performance on each condition is shown in <xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7B</xref>. Based on the experimental results, we trained the network until the lowest percentage of correct responses in any condition exceeded 85%; for most conditions the performance is much higher [<xref ref-type="bibr" rid="pcbi.1004792.ref034">34</xref>]. Several different types of behavior are observed in the unit activities. For instance, some units are positively tuned for the frequency <italic>f</italic><sub>1</sub> during both stimulus periods (<xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7D</xref>, left). Other units are positively tuned for <italic>f</italic><sub>1</sub> during the first stimulus period but negatively tuned during the second (<xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7D</xref>, right); the switch can occur at various times during the delay. Following [<xref ref-type="bibr" rid="pcbi.1004792.ref034">34</xref>], we performed a simple linear analysis of the tuning properties of units at different times by fitting the firing rate at each time point to the form <italic>r</italic>(<italic>t</italic>) = <italic>a</italic><sub>0</sub>(<italic>t</italic>) + <italic>a</italic><sub>1</sub>(<italic>t</italic>)<italic>f</italic><sub>1</sub>. The results are presented in <xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7C</xref>, which shows the correlation of <italic>a</italic><sub>1</sub> between different time points across the population, and <xref ref-type="fig" rid="pcbi.1004792.g007">Fig 7E</xref>, which shows the percentage of significantly tuned (two-sided <italic>p</italic>-value &lt;0.05) units at different times. The latter shows trends similar to those observed in monkeys.</p>
</sec>
<sec id="sec014">
<title>Eye-movement sequence execution task</title>
<p>An experimental paradigm that is qualitatively very different from the previous examples involves the memorized execution of a sequence of motor movements, and is inspired by the task of [<xref ref-type="bibr" rid="pcbi.1004792.ref066">66</xref>]. An important difference from a modeling point of view in this case is that, unlike in previous tasks where we interpreted the outputs as representing a decision variable between two choices, here we interpret the network’s two outputs to be the <italic>x</italic> and <italic>y</italic>-coordinates corresponding to the monkey’s eye position on the screen. After maintaining fixation on the central dot for 1 second, the task is to execute a sequence of three eye movements and hold for 500 ms each (<xref ref-type="fig" rid="pcbi.1004792.g008">Fig 8A</xref>). For each movement, two targets are presented as inputs to indicate the possible moves in addition to the current dot; although the targets could be presented in a more realistic manner—in a tuning curve-representation, for instance—here we use the simple encoding in which each input corresponds to a potential target location. Throughout the trial, an additional input is given that indicates which sequence, out of a total of 8, is being executed (<xref ref-type="fig" rid="pcbi.1004792.g008">Fig 8B</xref>).</p>
<fig id="pcbi.1004792.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Eye-movement sequence execution task.</title>
<p>(A) Task structure (for Sequence 5) and (B) sample inputs to the network. During the intertrial interval (ITI) the network receives only the input indicating the current sequence to be executed. Fixation is indicated by the presence of a fixation input, which is (the central) one of 9 possible dot positions on the screen. During each movement, the current dot plus two possible target dots appear. (C) State-space trajectories during the three movements M1, M2, and M3 for each sequence, projected on the first two principal components (PCs) (71% variance explained, note the different axis scales). The network was run with zero noise to obtain the plotted trajectories. The hierarchical organization of the sequence of movements is reflected in the splitting off of state-space trajectories. Note that all sequences start at fixation, or dot 5 (black), and are clustered here into two groups depending on the first move in the sequence. (D) Example run in which the network continuously executes each of the 8 sequences once in a particular order; the network can execute the sequences in any order. Each sequence is separated by a 1-second ITI during which the eye position returns from the final dot in the previous trial to the central fixation dot. Upper: Eye position in “screen” coordinates. Lower: <italic>x</italic> and <italic>y</italic>-positions of the network’s outputs indicating a point on the screen. Note the continuity of dynamics across trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g008" xlink:type="simple"/>
</fig>
<p>For this task we trained a 200-unit (160 excitatory, 40 inhibitory) network on a trial-by-trial basis, i.e., the network parameters were updated after each trial. This corresponds to setting the gradient minibatch size to 1. Moreover, the network was run “continuously,” without resetting the initial conditions for each trial (<xref ref-type="fig" rid="pcbi.1004792.g008">Fig 8D</xref>). During the intertrial interval (ITI), the network returns its eye position to the central fixation point from its location at the end of the third movement, so that the eye position is in the correct position for the start of the next fixation period. This occurs even though the target outputs given to the network during training did not specify the behavior of the outputs during the ITI, which is interesting for future investigation of such networks’ ability to learn tasks with minimal supervision.</p>
<p>During training, each sequence appeared once in a block of 8 randomly permuted trials. Here we used a time constant of <italic>τ</italic> = 50 ms to allow faster transitions between dots. For this task only, we used a smaller recurrent noise of <italic>σ</italic><sub>rec</sub> = 0.01 because the output values were required to be more precise than in previous tasks, and did not limit readout to excitatory units to allow for negative coordinates. We note that, in the original task of [<xref ref-type="bibr" rid="pcbi.1004792.ref066">66</xref>] the monkey was also required to infer the sequence it had to execute in a block of trials, but we did not implement this aspect of the task. Instead, the sequence was explicitly indicated by a separate set of inputs.</p>
<p>Because the sequence of movements are organized hierarchically—for instance, the first movement must decide between going left and going right, the next movement must decide between going up and going down, and so forth—we expect a hierarchical trajectory in state space. This is confirmed by performing a principal components analysis and projecting the network’s dynamics onto the first two principal components (PCs) computed across all conditions (<xref ref-type="fig" rid="pcbi.1004792.g008">Fig 8C</xref>).</p>
</sec>
</sec>
<sec id="sec015" sec-type="conclusions">
<title>Discussion</title>
<p>In this work we have described a framework for gradient descent-based training of excitatory-inhibitory RNNs, and demonstrated the application of this framework to tasks inspired by well-known experimental paradigms in systems neuroscience.</p>
<p>Unlike in machine learning applications, our aim in training RNNs is not simply to maximize the network’s performance, but to train networks so that their performance matches that of behaving animals while both network activity and architecture are as close to biology as possible. We have therefore placed great emphasis on the ability to easily explore different sets of constraints and regularizations, focusing in particular on “hard” constraints informed by biology. The incorporation of separate excitatory and inhibitory populations and the ability to constrain their connectivity is an important step in this direction, and is the main contribution of this work.</p>
<p>The framework described in this work for training RNNs differs from previous studies [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref008">8</xref>] in several other ways. In this work we use threshold (rectified) linear units for the activation function of the units. Biological neurons rarely operate in the saturated firing-rate regime, and the use of an unbounded nonlinearity obviates the need for regularization terms that prevent units from saturating [<xref ref-type="bibr" rid="pcbi.1004792.ref008">8</xref>]. Despite the absence of an upper bound, all firing rates nevertheless remained within a reasonable range. We also favor first-order SGD optimization over second-order HF methods. This is partly because of SGD’s widely acknowledged effectiveness in current approaches to machine learning, but also because gradient descent, unlike HF, allows for trial-by-trial learning and may ultimately be more easily related to synaptic learning rules in the brain [<xref ref-type="bibr" rid="pcbi.1004792.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref040">40</xref>].</p>
<p>Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1004792.e003">3</xref> are a special case of the more general set of equations for RNNs given in <xref ref-type="supplementary-material" rid="pcbi.1004792.s002">S1 Text</xref>, which in turn represent only one of many possible RNN architectures. For instance, machine learning applications typically employ a type of RNN known as Long Short-Term Memory (LSTM), which uses multiplicative gates to facilitate learning of long-term dependencies and currently represents one of the most powerful methods for solving sequence-related problems [<xref ref-type="bibr" rid="pcbi.1004792.ref072">72</xref>]. For reasons of biological interpretation, in our implementation we only consider generalizations that retain the “traditional” RNN architecture given by Eqs <xref ref-type="disp-formula" rid="pcbi.1004792.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1004792.e003">3</xref>. These generalizations include additive bias terms in recurrent and output units (corresponding to variable thresholds), different time constants for each unit (e.g., faster inhibitory units), correlated noise [<xref ref-type="bibr" rid="pcbi.1004792.ref073">73</xref>], and other types of nonlinearities besides simple rectification (e.g., supralinear [<xref ref-type="bibr" rid="pcbi.1004792.ref074">74</xref>] or saturating <italic>f-I</italic> curves) for either recurrent units or outputs. We found that biases, though not used for the networks in this work, can improve training in some situations by endowing the network with greater flexibility. The choice of output nonlinearity can be particularly relevant when considering the precise meaning of the outputs, such as whether the outputs are considered a decision variable, probability distribution, or eye position. Probability output models are useful, for instance, when the animal’s confidence about its decision is of interest in addition to its actual decision.</p>
<p>Several works [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref034">34</xref>] have now demonstrated the value of trained RNNs in revealing circuit mechanisms embedded in large neural populations. In addition to the pioneering work on uncovering a previously unknown selection mechanism for context-dependent integration of sensory inputs in [<xref ref-type="bibr" rid="pcbi.1004792.ref005">5</xref>], work reported in [<xref ref-type="bibr" rid="pcbi.1004792.ref007">7</xref>] used trained RNNs to reveal possible dynamical implementations of response criterion modulation in a perceptual detection task under temporal uncertainty. Yet, more recent methods for training networks have not been widely available or easily accessible to the neuroscience community. We have endeavored to change this by providing an easy-to-use but flexible implementation of our framework that facilitates further modifications and extensions. For the tasks featured in this work, the amount of time needed for training was relatively short and largely consistent across different initializations (<xref ref-type="fig" rid="pcbi.1004792.g009">Fig 9</xref>), and could be made even shorter for exploratory training by reducing the network size and noise level. Although further improvements can be made, our results already demonstrate that exploratory network training can be a practical and useful tool for neuroscientists. Moreover, while the present learning rule is not biologically plausible, it is of interest whether the behavioral trajectory of learning can be made similar to that of animals learning the same tasks. In particular, the question of how many trials are needed to learn a given task in model RNNs and animals merits further investigation.</p>
<fig id="pcbi.1004792.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004792.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Estimated performance during training for networks in the Results.</title>
<p>(A)-(I) Percentage of correct responses. (J) Error in eye position. For each network the relevant figure in the main text and a brief description are given. Black lines are for the networks shown in the main text, while gray lines show the performance for 5 additional networks trained for the same tasks but using different initial weights. Red lines indicate the target performance; training terminated when the mean performance on several (usually 5) evaluations of the validation dataset exceeded the target performance. In <italic>I</italic> the target performance indicates the minimum, rather than mean, percentage of correct responses across conditions. The number of recurrent units (green) is indicated for each network. The number of minutes (in “real-time”) needed for training (blue) are estimates for a MacBook Pro running OS X Yosemite 10.10.4, with a 2.8 GHz Intel Core i7 CPU and 16 GB 1600 MHz DDR3 memory. GPUs were not used in the training of these networks.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.g009" xlink:type="simple"/>
</fig>
<p>Many interesting and challenging questions remain. Although RNNs of rate units often provide a valuable starting point for investigating both the dynamical and neural computational mechanisms underlying cognitive functions, they will not always be the most appropriate level of description for biological neural circuits. In this work we have not addressed the question of how the firing rate description given by RNN training can be properly mapped to the more realistic case of spiking neurons, and indeed it is not completely clear, at present, how spiking neurons may be directly trained for general tasks using this type of approach. In this work we have only addressed tasks that could be formulated in the language of supervised learning, i.e., the correct outputs were explicitly given for each set of inputs. Combining RNN training with reinforcement learning methods [<xref ref-type="bibr" rid="pcbi.1004792.ref075">75</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref077">77</xref>] will be essential to bringing network training closer to the reward-based manner in which animals are trained. Despite limitations, particularly on the range of tasks that can be learned, progress on training spiking neurons with STDP-type rules and reinforcement learning is promising [<xref ref-type="bibr" rid="pcbi.1004792.ref078">78</xref>–<xref ref-type="bibr" rid="pcbi.1004792.ref080">80</xref>], and future work will incorporate such advances. Other physiologically relevant phenomena such as bursting, adaptation, and oscillations are currently not captured by our framework, but can be incorporated in the future; adaptation, for example, can be included in phenomenological form appropriate to a rate model [<xref ref-type="bibr" rid="pcbi.1004792.ref081">81</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref082">82</xref>].</p>
<p>We have also not addressed what computational advantages are conferred, for example, by the existence of separate excitatory and inhibitory populations, instead taking it as a biological fact that must be included in models of animal cognition. Indeed, although our discussion has focused on the distinction between excitatory and inhibitory neurons, the functional role of inhibitory units may only become apparent when the full diversity of excitatory and inhibitory neuronal morphology and physiology, their layer and type-specific distribution and connectivity [<xref ref-type="bibr" rid="pcbi.1004792.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref050">50</xref>], and domain-specific (e.g., dendritic versus somatic) targeting of excitatory pyramidal cells by interneurons [<xref ref-type="bibr" rid="pcbi.1004792.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004792.ref051">51</xref>] in the brain are taken into account. Some of these phenomena can already be implemented in the framework by fixing the pattern of connectivity between groups of units (corresponding, for example, to different layers in a cortical column), and future work will explore the implications of such structure on network dynamics.</p>
<p>Finally, although we have performed a few basic analyses of the resulting networks, we have not addressed the detailed mechanisms by which networks accomplish their tasks. In this regard, although state-space analyses of fixed and “slow” points [<xref ref-type="bibr" rid="pcbi.1004792.ref083">83</xref>] are illuminating they do not yet explain how the network’s connectivity, combined with the nonlinear activation functions, lead to the observed neural trajectories. Discovering general methods for the systematic analysis of trained networks remains one of the most important areas of inquiry if RNNs are to provide useful insights into the operation of biological neural circuits. As a platform for theoretical investigation, trained RNNs offer a unified setting in which diverse cognitive computations and mechanisms can be studied. Our results provide a valuable foundation for tackling this challenge by facilitating the generation of candidate networks to study, and represent a fruitful interaction between modern machine learning and neuroscience.</p>
</sec>
<sec id="sec016">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004792.s001" mimetype="text/x-python" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.s001" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>Example task specification: perceptual decision-making task.</title>
<p>(PY)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004792.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004792.s002" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>General equations for a recurrent neural network.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank J. D. Murray, W. Zaremba, and R. Pascanu for valuable discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004792.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>. <article-title>Neuronal population coding of parametric working memory</article-title>. <year>2010</year> <source>J Neurosci</source>;<volume>30</volume>:<fpage>9424</fpage>–<lpage>9430</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1875-10.2010" xlink:type="simple">10.1523/JNEUROSCI.1875-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20631171</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DBD</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Internal representation of task rules by recurrent dynamics: the importance of the diversity of neural responses</article-title>. <year>2010</year> <source>Front Comput Neurosci</source>;<volume>4</volume>:<fpage>24</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2010.00024" xlink:type="simple">10.3389/fncom.2010.00024</ext-link></comment> <object-id pub-id-type="pmid">21048899</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <etal>et al</etal>. <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <year>2013</year> <source>Nature</source>;<volume>497</volume>:<fpage>585</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12160" xlink:type="simple">10.1038/nature12160</ext-link></comment> <object-id pub-id-type="pmid">23685452</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>From the neuron doctrine to neural networks</article-title>. <year>2015</year> <source>Nat Rev Neurosci</source>;<volume>16</volume>:<fpage>487</fpage>–<lpage>497</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3962" xlink:type="simple">10.1038/nrn3962</ext-link></comment> <object-id pub-id-type="pmid">26152865</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <year>2013</year> <source>Nature</source>;<volume>503</volume>:<fpage>78</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12742" xlink:type="simple">10.1038/nature12742</ext-link></comment> <object-id pub-id-type="pmid">24201281</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <etal>et al</etal>. <article-title>Neural population dynamics during reaching</article-title>. <year>2012</year> <source>Nature</source>;<volume>487</volume>:<fpage>51</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11129" xlink:type="simple">10.1038/nature11129</ext-link></comment> <object-id pub-id-type="pmid">22722855</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carnevale</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>de Lafuente</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Parga</surname> <given-names>N</given-names></name>. <article-title>Dynamic control of response criterion in premotor cortex during perceptual detection under temporal uncertainty</article-title>. <year>2015</year> <source>Neuron</source>;<volume>86</volume>:<fpage>1067</fpage>–<lpage>1077</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.04.014" xlink:type="simple">10.1016/j.neuron.2015.04.014</ext-link></comment> <object-id pub-id-type="pmid">25959731</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>. <article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title>. <year>2015</year> <source>Nat Neurosci</source>;<volume>18</volume>:<fpage>1025</fpage>–<lpage>1033</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4042" xlink:type="simple">10.1038/nn.4042</ext-link></comment> <object-id pub-id-type="pmid">26075643</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Siegel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Buschman</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>. <article-title>Cortical information flow during flexible sensorimotor decisions</article-title>. <year>2015</year> <source>Science</source>;<volume>348</volume>:<fpage>1352</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.aab0551" xlink:type="simple">10.1126/science.aab0551</ext-link></comment> <object-id pub-id-type="pmid">26089513</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stevenson</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>. <article-title>How advances in neural recording affect data analysis</article-title>. <year>2011</year> <source>Nat Neurosci</source>;<volume>14</volume>:<fpage>139</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2731" xlink:type="simple">10.1038/nn.2731</ext-link></comment> <object-id pub-id-type="pmid">21270781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>. <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <year>2014</year> <source>Nat Neurosci</source>;<volume>17</volume>:<fpage>1500</fpage>–<lpage>1509</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3776" xlink:type="simple">10.1038/nn.3776</ext-link></comment> <object-id pub-id-type="pmid">25151264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gao</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>. <article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title>. <year>2015</year> <source>Curr Opin Neurobiol</source>;<volume>32</volume>:<fpage>148</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2015.04.003" xlink:type="simple">10.1016/j.conb.2015.04.003</ext-link></comment> <object-id pub-id-type="pmid">25932978</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref013">
<label>13</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kistler</surname> <given-names>WM</given-names></name>. <source>Spiking Neuron Models: Single Neurons, Populations, Plasticity</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Funahashi</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nakamura</surname> <given-names>Y</given-names></name>. <article-title>Approximation of dynamical systems by continuous time recurrent neural networks</article-title>. <year>1993</year> <source>Neural Networks</source>;<volume>6</volume>:<fpage>801</fpage>–<lpage>806</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(05)80125-X" xlink:type="simple">10.1016/S0893-6080(05)80125-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Siegelmann</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>Sontag</surname> <given-names>ED</given-names></name>. <article-title>On the Computational Power of Neural Nets</article-title>. <year>1995</year> <source>J Comput Syst Sci</source>;<volume>50</volume>:<fpage>132</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/jcss.1995.1013" xlink:type="simple">10.1006/jcss.1995.1013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>Ea</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <year>2014</year> <source>Proc Natl Acad Sci U S A</source>;<volume>111</volume>:<fpage>8619</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment> <object-id pub-id-type="pmid">24812127</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>. <article-title>Neural circuits as computational dynamical systems</article-title>. <year>2014</year> <source>Curr Opin Neurobiol</source>;<volume>25</volume>:<fpage>156</fpage>–<lpage>163</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2014.01.008" xlink:type="simple">10.1016/j.conb.2014.01.008</ext-link></comment> <object-id pub-id-type="pmid">24509098</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title>. <year>1988</year> <source>Nature</source>;<volume>331</volume>:<fpage>679</fpage>–<lpage>684</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/331679a0" xlink:type="simple">10.1038/331679a0</ext-link></comment> <object-id pub-id-type="pmid">3344044</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref019">
<label>19</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>. <source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1986</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Dunbar</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>. <article-title>On the control of automatic processes: a parallel distributed processing account of the Stroop effect</article-title>. <year>1990</year> <source>Psychol Rev</source>;<volume>97</volume>:<fpage>332</fpage>–<lpage>361</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.97.3.332" xlink:type="simple">10.1037/0033-295X.97.3.332</ext-link></comment> <object-id pub-id-type="pmid">2200075</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eccles</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Fatt</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Koketsu</surname> <given-names>K</given-names></name>. <article-title>Cholinergic and inhibitory synapses in a pathway from motor-axon collaterals to motoneurones</article-title>. <year>1954</year> <source>J Physiol</source>;<volume>126</volume>:<fpage>524</fpage>–<lpage>562</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1954.sp005226" xlink:type="simple">10.1113/jphysiol.1954.sp005226</ext-link></comment> <object-id pub-id-type="pmid">13222354</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Murphy</surname> <given-names>BK</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>Balanced amplification: A new mechanism of selective amplification of neural activity patterns</article-title>. <year>2009</year> <source>Neuron</source>;<volume>61</volume>:<fpage>635</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.02.005" xlink:type="simple">10.1016/j.neuron.2009.02.005</ext-link></comment> <object-id pub-id-type="pmid">19249282</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Toledo-Rodriguez</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Silberberg</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>C</given-names></name>. <article-title>Interneurons of the neocortical inhibitory system</article-title>. <year>2004</year> <source>Nat Rev Neurosci</source>;<volume>5</volume>:<fpage>793</fpage>–<lpage>807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1519" xlink:type="simple">10.1038/nrn1519</ext-link></comment> <object-id pub-id-type="pmid">15378039</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <year>2005</year> <source>PLoS Biol</source>;<volume>3</volume>:<fpage>e68</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment> <object-id pub-id-type="pmid">15737062</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfeffer</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Xue</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Scanziani</surname> <given-names>M</given-names></name>. <article-title>Inhibition of inhibition in visual cortex: the logic of connections between molecularly distinct interneurons</article-title>. <year>2013</year> <source>Nat Neurosci</source>;<volume>16</volume>:<fpage>1068</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3446" xlink:type="simple">10.1038/nn.3446</ext-link></comment> <object-id pub-id-type="pmid">23817549</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Potjans</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>. <article-title>The cell-type specific cortical microcircuit: relating structure and activity in a full-scale spiking network model</article-title>. <year>2014</year> <source>Cereb Cortex</source>;<volume>24</volume>:<fpage>785</fpage>–<lpage>806</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhs358" xlink:type="simple">10.1093/cercor/bhs358</ext-link></comment> <object-id pub-id-type="pmid">23203991</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jiang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cadwell</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sinz</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>Principles of connectivity among morphologically defined cell types in adult neocortex</article-title>. <year>2015</year> <source>Science</source>;<volume>350</volume>:<fpage>aac9462</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.aac9462" xlink:type="simple">10.1126/science.aac9462</ext-link></comment> <object-id pub-id-type="pmid">26612957</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ercsey-Ravasz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Markov</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Lamy</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Knoblauch</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Toroczkai</surname> <given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>A predictive network model of cerebral cortical connectivity based on a distance rule</article-title>. <year>2013</year> <source>Neuron</source>;<volume>80</volume>:<fpage>184</fpage>–<lpage>197</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.07.036" xlink:type="simple">10.1016/j.neuron.2013.07.036</ext-link></comment> <object-id pub-id-type="pmid">24094111</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Markov</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Ercsey-Ravasz</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Ribeiro Gomes</surname> <given-names>aR</given-names></name>, <name name-style="western"><surname>Lamy</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Magrou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vezoli</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>A weighted and directed interareal connectivity matrix for macaque cerebral cortex</article-title>. <year>2014</year> <source>Cereb Cortex</source>;<volume>24</volume>:<fpage>17</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhs270" xlink:type="simple">10.1093/cercor/bhs270</ext-link></comment> <object-id pub-id-type="pmid">23010748</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Kennedy</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Spatial embedding of structural similarity in the cerebral cortex</article-title>. <year>2014</year> <source>Proc Natl Acad Sci</source>;<volume>111</volume>:<fpage>16580</fpage>–<lpage>16585</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1414153111" xlink:type="simple">10.1073/pnas.1414153111</ext-link></comment> <object-id pub-id-type="pmid">25368200</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <year>2009</year> <source>Neuron</source>;<volume>63</volume>:<fpage>544</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.07.018" xlink:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laje</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>. <article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title>. <year>2013</year> <source>Nat Neurosci</source>;<volume>16</volume>:<fpage>925</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3405" xlink:type="simple">10.1038/nn.3405</ext-link></comment> <object-id pub-id-type="pmid">23708144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref033">
<label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Martens J, Sutskever I. Learning recurrent neural networks with Hessian-free optimization. 2011 Proc 28th Int Conf Mach Learn;Available from: <ext-link ext-link-type="uri" xlink:href="http://www.icml-2011.org/papers/532_icmlpaper.pdf" xlink:type="simple">http://www.icml-2011.org/papers/532_icmlpaper.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. F<article-title>rom fixed points to chaos: Three models of delayed discrimination</article-title>. <year>2013</year> <source>Prog Neurobiol</source>;<volume>103</volume>:<fpage>214</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.pneurobio.2013.02.002" xlink:type="simple">10.1016/j.pneurobio.2013.02.002</ext-link></comment> <object-id pub-id-type="pmid">23438479</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Pascanu R, Mikolov T, Bengio Y. On the difficulty of training recurrent neural networks. 2013 Proc 30th Int Conf Mach Learn;Available from: <ext-link ext-link-type="uri" xlink:href="http://jmlr.org/proceedings/papers/v28/pascanu13.pdf" xlink:type="simple">http://jmlr.org/proceedings/papers/v28/pascanu13.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Bengio Y, Boulanger-Lewandowski N, Pascanu R. Advances in optimizing recurrent networks. In: Proc. Int. Conf. Acoust. Speech, Signal Process.; 2013. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/pdf/1212.0901v2.pdf" xlink:type="simple">http://arxiv.org/pdf/1212.0901v2.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref037">
<label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Hardt M, Recht B, Singer Y. 2015 Train faster, generalize better: Stability of stochastic gradient descent;Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1509.01240" xlink:type="simple">http://arxiv.org/abs/1509.01240</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>Mm</given-names></name>. <article-title>Spike timing-dependent plasticity: From synapse to perception</article-title>. <year>2006</year> <source>Physiol Rev</source>;p. <fpage>1033</fpage>–<lpage>1048</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/physrev.00030.2005" xlink:type="simple">10.1152/physrev.00030.2005</ext-link></comment> <object-id pub-id-type="pmid">16816145</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref039">
<label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Bengio Y. 2015 Towards biologically plausible deep learning;Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.0415" xlink:type="simple">http://arxiv.org/abs/1502.0415</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Bengio Y, Mesnard T, Fischer A, Zhang S, Wu Y. 2015 An objective function for STDP;Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1509.05936" xlink:type="simple">http://arxiv.org/abs/1509.05936</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Bergstra J, Breuleux O, Bastien F, Lamblin P, Pascanu R, Desjardins G, et al. Theano: A CPU and GPU math compiler in Python. In: Proc. 9th Python Sci. Conf. 2010; 2010. Available from: <ext-link ext-link-type="uri" xlink:href="http://conference.scipy.org/proceedings/scipy2010/pdfs/bergstra.pdf" xlink:type="simple">http://conference.scipy.org/proceedings/scipy2010/pdfs/bergstra.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Bastien F, Lamblin P, Pascanu R, Bergstra J, Goodfellow I, Bergeron A, et al. 2012 Theano: new features and speed improvements;Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1211.5590" xlink:type="simple">http://arxiv.org/abs/1211.5590</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hahnloser</surname> <given-names>RLT</given-names></name>. <article-title>On the piecewise analysis of networks of linear threshold neurons</article-title>. <year>1998</year> <source>Neural Networks</source>;<volume>11</volume>:<fpage>691</fpage>–<lpage>697</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(98)00012-4" xlink:type="simple">10.1016/S0893-6080(98)00012-4</ext-link></comment> <object-id pub-id-type="pmid">12662807</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Pascanu R, Montúfar G, Bengio Y. On the number of response regions of deep feedforward networks with piecewise linear activations. 2014 Int Conf Learn Represent;Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/pdf/1312.6098v5.pdf" xlink:type="simple">http://arxiv.org/pdf/1312.6098v5.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref045">
<label>45</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Montúfar</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Pascanu</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Cho</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <source>On the number of linear regions of deep neural networks</source>; <year>2014</year>. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1402.1869v1" xlink:type="simple">http://arxiv.org/abs/1402.1869v1</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gillespie</surname> <given-names>DT</given-names></name>. <article-title>The mathematics of Brownian motion and Johnson noise</article-title>. <year>1996</year> <source>Am J Phys</source>;<volume>64</volume>:<fpage>225</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1119/1.18210" xlink:type="simple">10.1119/1.18210</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brunton</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>. <article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title>. <year>2013</year> <source>Science</source>;<volume>340</volume>:<fpage>95</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1233912" xlink:type="simple">10.1126/science.1233912</ext-link></comment> <object-id pub-id-type="pmid">23559254</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Levy</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Reyes</surname> <given-names>AD</given-names></name>. <article-title>Spatial profile of excitatory and inhibitory synaptic connectivity in mouse primary auditory cortex</article-title>. <year>2012</year> <source>J Neurosci</source>;<volume>32</volume>:<fpage>5609</fpage>–<lpage>5619</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5158-11.2012" xlink:type="simple">10.1523/JNEUROSCI.5158-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22514322</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thomson</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>West</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bannister</surname> <given-names>AP</given-names></name>. <article-title>Synaptic connections and small circuits involving excitatory and inhibitory neurons in layers 2–5 of adult rat and cat neocortex: triple intracellular recordings and biocytin labelling in vitro</article-title>. <year>2002</year> <source>Cereb Cortex</source>;<volume>12</volume>:<fpage>936</fpage>–<lpage>953</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/12.9.936" xlink:type="simple">10.1093/cercor/12.9.936</ext-link></comment> <object-id pub-id-type="pmid">12183393</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Binzegger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>KAC</given-names></name>. <article-title>A quantitative map of the circuit of cat primary visual cortex</article-title>. <year>2004</year> <source>J Neurosci</source>;<volume>24</volume>:<fpage>8441</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1400-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1400-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15456817</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Tegnér</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Constantinidis</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Division of labor among distinct subtypes of inhibitory neurons in a cortical microcircuit of working memory</article-title>. <year>2004</year> <source>Proc Natl Acad Sci U S A</source>;<volume>101</volume>:<fpage>1368</fpage>–<lpage>1373</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0305337101" xlink:type="simple">10.1073/pnas.0305337101</ext-link></comment> <object-id pub-id-type="pmid">14742867</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fino</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Packer</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>The logic of inhibitory connectivity in the neocortex</article-title>. <year>2012</year> <source>Neurosci</source>;<volume>19</volume>:<fpage>228</fpage>–<lpage>237</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1073858412456743" xlink:type="simple">http://dx.doi.org/10.1177/1073858412456743</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Karnani</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Agetsuma</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>A blanket of inhibition: Functional inferences from dense inhibitory connectivity</article-title>. <year>2014</year> <source>Curr Opin Neurobiol</source>;<volume>26</volume>:<fpage>96</fpage>–<lpage>102</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2013.12.015" xlink:type="simple">10.1016/j.conb.2013.12.015</ext-link></comment> <object-id pub-id-type="pmid">24440415</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Festa</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hennequin</surname> <given-names>G</given-names></name>. <article-title>Analog memories in a balanced rate-based network of E-I neurons</article-title>. In: <source>Adv. Neural Inf. Process. Syst.</source>; <year>2014</year>. Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5336-analog-memories-in-a-balanced-rate-based-network-of-e-i-neurons.pdf" xlink:type="simple">http://papers.nips.cc/paper/5336-analog-memories-in-a-balanced-rate-based-network-of-e-i-neurons.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Eigenvalue Spectra of Random Matrices for Neural Networks</article-title>. <year>2006</year> <source>Phys Rev Lett</source>;<volume>97</volume>:<fpage>188104</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.97.188104" xlink:type="simple">10.1103/PhysRevLett.97.188104</ext-link></comment> <object-id pub-id-type="pmid">17155583</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref056">
<label>56</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Le</surname> <given-names>QV</given-names></name>, <name name-style="western"><surname>Jaitly</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <year>2015</year> <source>A simple way to initialize recurrent networks of rectified linear units</source>;p. <fpage>1</fpage>–<lpage>9</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1504.00941" xlink:type="simple">http://arxiv.org/abs/1504.00941</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref057">
<label>57</label>
<mixed-citation publication-type="other" xlink:type="simple">Sutskever I, Martens J, Dahl G, Hinton G. On the importance of initialization and momentum in deep learning. In: Proc. 30th Int. Conf. Mach. Learn. IEEE; 2013. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.cs.utoronto.ca/~ilya/pubs/2013/1051_2.pdf" xlink:type="simple">http://www.cs.utoronto.ca/~ilya/pubs/2013/1051_2.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref058">
<label>58</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name>. <chapter-title>Learning internal representations by error propagation</chapter-title>. In: <name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>, editors. <source>Parallel Distrib. Process</source>. <volume>Vol. 1</volume>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1986</year>. p. <fpage>318</fpage>–<lpage>362</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref059">
<label>59</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dauphin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Pascanu</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gulcehre</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Cho</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <year>2014</year> <source>Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</source>;Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1406.2572" xlink:type="simple">http://arxiv.org/abs/1406.2572</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Simard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Frasconi</surname> <given-names>P</given-names></name>. <article-title>Learning long-term dependencies with gradient descent is difficult</article-title>. <year>1994</year> <source>IEEE Trans Neural Networks</source>;<volume>5</volume>:<fpage>157</fpage>–<lpage>166</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/72.279181" xlink:type="simple">10.1109/72.279181</ext-link></comment> <object-id pub-id-type="pmid">18267787</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Britten</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>Ja</given-names></name>. <article-title>Neuronal correlates of a perceptual decision</article-title>. <year>1989</year> <source>Nature</source>;<volume>341</volume>:<fpage>52</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/341052a0" xlink:type="simple">10.1038/341052a0</ext-link></comment> <object-id pub-id-type="pmid">2770878</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roitman</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title>. <year>2002</year> <source>J Neurosci</source>;<volume>22</volume>:<fpage>9475</fpage>–<lpage>89</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/22/21/9475.long" xlink:type="simple">http://www.jneurosci.org/content/22/21/9475.long</ext-link>. <object-id pub-id-type="pmid">12417672</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hanks</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment</article-title>. <year>2008</year> <source>J Neurosci</source>;<volume>28</volume>:<fpage>3017</fpage>–<lpage>3029</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4761-07.2008" xlink:type="simple">10.1523/JNEUROSCI.4761-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18354005</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Raposo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Churchland</surname> <given-names>AK</given-names></name>. <article-title>A category-free neural population supports evolving demands during decision-making</article-title>. <year>2014</year> <source>Nat Neurosci</source>;<volume>17</volume>:<fpage>1784</fpage>–<lpage>1792</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3865" xlink:type="simple">10.1038/nn.3865</ext-link></comment> <object-id pub-id-type="pmid">25383902</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lemus</surname> <given-names>L</given-names></name>. <article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title>. <year>1999</year> <source>Nature</source>;<volume>399</volume>:<fpage>470</fpage>–<lpage>473</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/20939" xlink:type="simple">10.1038/20939</ext-link></comment> <object-id pub-id-type="pmid">10365959</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>. <article-title>Prefrontal neural correlates of memory for sequences</article-title>. <year>2007</year> <source>J Neurosci</source>;<volume>27</volume>:<fpage>2204</fpage>–<lpage>2211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4483-06.2007" xlink:type="simple">10.1523/JNEUROSCI.4483-06.2007</ext-link></comment> <object-id pub-id-type="pmid">17329417</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wong</surname> <given-names>KF</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title>. <year>2006</year> <source>J Neurosci</source>;<volume>26</volume>:<fpage>1314</fpage>–<lpage>1328</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3733-05.2006" xlink:type="simple">10.1523/JNEUROSCI.3733-05.2006</ext-link></comment> <object-id pub-id-type="pmid">16436619</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>A recurrent network model of somatosensory parametric working memory in the prefrontal cortex</article-title>. <year>2003</year> <source>Cereb Cortex</source>;<volume>13</volume>:<fpage>1208</fpage>–<lpage>1218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhg101" xlink:type="simple">10.1093/cercor/bhg101</ext-link></comment> <object-id pub-id-type="pmid">14576212</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Synaptic basis of cortical persistent activity: The importance of NMDA receptors to working memory</article-title>. <year>1999</year> <source>J Neurosci</source>;<volume>19</volume>:<fpage>9587</fpage>–<lpage>9603</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/content/19/21/9587.long" xlink:type="simple">http://www.jneurosci.org/content/19/21/9587.long</ext-link>. <object-id pub-id-type="pmid">10531461</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>. <article-title>Working models of working memory</article-title>. <year>2014</year> <source>Curr Opin Neurobiol</source>;<volume>25</volume>:<fpage>20</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2013.10.008" xlink:type="simple">10.1016/j.conb.2013.10.008</ext-link></comment> <object-id pub-id-type="pmid">24709596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Funahashi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Mnemonic coding of visual space in the monkey x2019;s dorsolateral prefrontal cortex</article-title>. <year>1989</year> <source>J Neurophysiol</source>;<volume>61</volume>:<fpage>331</fpage>–<lpage>349</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://jn.physiology.org/content/61/2/331.long" xlink:type="simple">http://jn.physiology.org/content/61/2/331.long</ext-link>. <object-id pub-id-type="pmid">2918358</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Long Short-Term Memory</article-title>. <year>1997</year> <source>Neural Comput</source>;<volume>9</volume>:<fpage>1735</fpage>–<lpage>1780</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1997.9.8.1735" xlink:type="simple">10.1162/neco.1997.9.8.1735</ext-link></comment> <object-id pub-id-type="pmid">9377276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Renart</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>de la Rocha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bartho</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hollender</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Parga</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Reyes</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The asynchronous state in cortical circuits</article-title>. <year>2010</year> <source>Science</source>;<volume>327</volume>:<fpage>587</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1179850" xlink:type="simple">10.1126/science.1179850</ext-link></comment> <object-id pub-id-type="pmid">20110507</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Hooser</surname> <given-names>SDV</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>The stabilized supralinear network: A unifying circuit motif underlying multi-input integration in sensory cortex</article-title>. <year>2013</year> <source>Neuron</source>;<volume>85</volume>:<fpage>1</fpage>–<lpage>51</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.12.026" xlink:type="simple">http://dx.doi.org/10.1016/j.neuron.2014.12.026</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref075">
<label>75</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Reinforcement Learning: An Introduction</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bakker</surname> <given-names>B</given-names></name>. <article-title>Reinforcement learning with Long Short-Term Memory</article-title>. <year>2002</year> <source>Adv Neural Inf Process Syst</source>;Available from: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.pdf" xlink:type="simple">http://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004792.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name>. <article-title>Attention-gated reinforcement learning of internal representations for classification</article-title>. <year>2005</year> <source>Neural Comput</source>;<volume>17</volume>:<fpage>2176</fpage>–<lpage>2214</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766054615699" xlink:type="simple">10.1162/0899766054615699</ext-link></comment> <object-id pub-id-type="pmid">16105222</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref078">
<label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Izhikevich</surname> <given-names>EM</given-names></name>. <article-title>Solving the Distal Reward Problem through Linkage of STDP and Dopamine Signaling</article-title>. <year>2007</year> <source>Cereb Cortex</source>;<volume>17</volume>:<fpage>2443</fpage>–<lpage>2452</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhl152" xlink:type="simple">10.1093/cercor/bhl152</ext-link></comment> <object-id pub-id-type="pmid">17220510</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref079">
<label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Potjans</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>. <article-title>A Spiking Neural Network Model of an Actor-Critic Agent</article-title>. <year>2009</year> <source>Neural Comput</source>;<volume>21</volume>:<fpage>301</fpage>–<lpage>339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.08-07-593" xlink:type="simple">10.1162/neco.2008.08-07-593</ext-link></comment> <object-id pub-id-type="pmid">19196231</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref080">
<label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Neymotin</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Chadderdon</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Kerr</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Francis</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Lytton</surname> <given-names>WW</given-names></name>. <article-title>Reinforcement learning of two-joint arm reaching in a computer model of sensorimotor cortex</article-title>. <year>2013</year> <source>Neural Comput</source>;<volume>25</volume>:<fpage>3263</fpage>–<lpage>3293</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO" xlink:type="simple">http://dx.doi.org/10.1162/NECO</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00521" xlink:type="simple">10.1162/NECO_a_00521</ext-link></comment> <object-id pub-id-type="pmid">24047323</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref081">
<label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Benda</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Herz</surname> <given-names>AVM</given-names></name>. <article-title>A universal model for spike-frequency adaptation</article-title>. <year>2003</year> <source>Neural Comput</source>;<volume>15</volume>:<fpage>2523</fpage>–<lpage>2564</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976603322385063" xlink:type="simple">10.1162/089976603322385063</ext-link></comment> <object-id pub-id-type="pmid">14577853</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref082">
<label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Engel</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Same or different? A neural circuit mechanism of similarity-based pattern match decision making</article-title>. <year>2011</year> <source>J Neurosci</source>;<volume>31</volume>:<fpage>6982</fpage>–<lpage>6996</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6150-10.2011" xlink:type="simple">10.1523/JNEUROSCI.6150-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21562260</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004792.ref083">
<label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>. <article-title>Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks</article-title>. <year>2013</year> <source>Neural Comput</source>;<volume>25</volume>:<fpage>626</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00409" xlink:type="simple">10.1162/NECO_a_00409</ext-link></comment> <object-id pub-id-type="pmid">23272922</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>