<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00584</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004485</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Network Plasticity as Bayesian Inference</article-title>
<alt-title alt-title-type="running-head">Network Plasticity as Bayesian Inference</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Kappel</surname> <given-names>David</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Habenschuss</surname> <given-names>Stefan</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Legenstein</surname> <given-names>Robert</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Maass</surname> <given-names>Wolfgang</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Institute for Theoretical Computer Science, Graz University of Technology, A-8010 Graz, Austria</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Beck</surname> <given-names>Jeff</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Duke University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: DK RL WM SH. Performed the experiments: DK. Analyzed the data: DK. Wrote the paper: WM RL DK SH. Developed the theory: SH RL DK.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">david@igi.tugraz.at</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>11</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>6</day>
<month>11</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>11</issue>
<elocation-id>e1004485</elocation-id>
<history>
<date date-type="received">
<day>8</day>
<month>4</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>8</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Kappel et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004485" xlink:type="simple"/>
<abstract>
<p>General results from statistical learning theory suggest to understand not only brain computations, but also brain plasticity as probabilistic inference. But a model for that has been missing. We propose that inherently stochastic features of synaptic plasticity and spine motility enable cortical networks of neurons to carry out probabilistic inference by sampling from a posterior distribution of network configurations. This model provides a viable alternative to existing models that propose convergence of parameters to maximum likelihood values. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience, how cortical networks can generalize learned information so well to novel experiences, and how they can compensate continuously for unforeseen disturbances of the network. The resulting new theory of network plasticity explains from a functional perspective a number of experimental data on stochastic aspects of synaptic plasticity that previously appeared to be quite puzzling.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Synaptic connectivity between neurons in the brain and the efficacies (“weights”) of these synaptic connections are thought to encode the long-term memory of an organism. But a closer look at their molecular implementation, as well as imaging experiments over longer periods of time, have shown that synaptic connections are subject to numerous stochastic processes. We propose that this seeming unreliability of synaptic connections is not a bug, but an important feature. It endows networks of neurons with an important experimentally observed but theoretically not understood capability: Automatic compensation for internal and external changes. This perspective of network plasticity requires a new conceptual and mathematical framework, which is provided by this article. Stochasticity of synapses is seen here not as noise of an inherently deterministic system, but as an inherent property, similarly as Brownian motion of particles in a physical system cannot be abstracted away if one wants to understand certain properties of a physical system. In fact, we find that this underlying stochasticity of synaptic connections enables a network of neurons to continuously try out new network configurations while maintaining its functionality.</p>
</abstract>
<funding-group>
<funding-statement>Written under partial support of the European Union project #604102 The Human Brain Project (HBP) and CHIST-ERA ERA-Net (Project FWF #I753-N23, PNEUMA). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="31"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>We reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in networks of neurons in the brain. We will focus on synaptic plasticity and network rewiring (spine motility) in this article, but our framework is also applicable to other network plasticity processes. One commonly assumes, that plasticity moves network parameters <bold><italic>θ</italic></bold> (such as synaptic connections between neurons and synaptic weights) to values <bold><italic>θ</italic></bold>* that are optimal for the current computational function of the network. In learning theory, this view is made precise for example as maximum likelihood learning, where model parameters <bold><italic>θ</italic></bold> are moved to values <bold><italic>θ</italic></bold>* that maximize the fit of the resulting internal model to the inputs <bold>x</bold> that impinge on the network from its environment (by maximizing the likelihood of these inputs <bold>x</bold>). The convergence to <bold><italic>θ</italic></bold>* is often assumed to be facilitated by some external regulation of learning rates, that reduces the learning rate when the network approaches an optimal solution.</p>
<p>This view of network plasticity has been challenged on several grounds. From the theoretical perspective it is problematic because in the absence of an intelligent external controller it is likely to lead to overfitting of the internal model to the inputs <bold>x</bold> it has received, thereby reducing its capability to generalize learned knowledge to new inputs. Furthermore, networks of neurons in the brain are apparently exposed to a multitude of internal and external changes and perturbations, to which they have to respond quickly in order to maintain stable functionality.</p>
<p>Other experimental data point to surprising ongoing fluctuations in dendritic spines and spine volumes, to some extent even in the adult brain [<xref ref-type="bibr" rid="pcbi.1004485.ref001">1</xref>] and in the absence of synaptic activity [<xref ref-type="bibr" rid="pcbi.1004485.ref002">2</xref>]. Also a significant portion of axonal side branches and axonal boutons were found to appear and disapper within a week in adult visual cortex, even in the absence of imposed learning and lesions [<xref ref-type="bibr" rid="pcbi.1004485.ref003">3</xref>]. Furthermore surprising random drifts of tuning curves of neurons in motor cortex were observed [<xref ref-type="bibr" rid="pcbi.1004485.ref004">4</xref>]. Apart from such continuously ongoing changes in synaptic connections and tuning curves of neurons, massive changes in synaptic connectivity were found to accompany functional reorganization of primary visual cortex after lesions, see e.g. [<xref ref-type="bibr" rid="pcbi.1004485.ref005">5</xref>].</p>
<p>We therefore propose to view network plasticity as a process that continuously moves high-dimensional network parameters <bold><italic>θ</italic></bold> within some low-dimensional manifold that represents a compromise between overriding structural rules and different ways of fitting the internal model to external inputs <bold>x</bold>. We propose that ongoing stochastic fluctuations (not unlike Brownian motion) continuously drive network parameters <bold><italic>θ</italic></bold> within such low-dimensional manifold. The primary conceptual innovation is the departure from the traditional view of learning as moving parameters to values <bold><italic>θ</italic></bold>* that represent optimal (or locally optimal) fits to network inputs <bold>x</bold>. We show that our alternative view can be turned into a precise learning model within the framework of probability theory. This new model satisfies theoretical requirements for handling priors such as structural constraints and rules in a principled manner, that have previously already been formulated and explored in the context of artificial neural networks [<xref ref-type="bibr" rid="pcbi.1004485.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>], as well as more recent challenges that arise from probabilistic brain models [<xref ref-type="bibr" rid="pcbi.1004485.ref008">8</xref>]. The low-dimensional manifold of parameters <bold><italic>θ</italic></bold> that becomes the new learning goal in our model can be characterized mathematically as the high probability regions of the posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>) of network parameters <bold><italic>θ</italic></bold>. This posterior arises as product of a general prior <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>) for network parameters (that enforces structural rules) with a term that describes the quality of the current internal model (e.g. in a predictive coding or generative modeling framework: the likelihood <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>∣<bold><italic>θ</italic></bold>) of inputs <bold>x</bold> for the current parameter values <bold><italic>θ</italic></bold> of the network 𝒩). More precisely, we propose that brain plasticity mechanisms are designed to enable brain networks to sample from this posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>) through inherent stochastic features of their molecular implementation. In this way synaptic and other plasticity processes are able to carry out probabilistic (or Bayesian) inference through sampling from a posterior distribution that takes into account both structural rules and fitting to external inputs. Hence this model provides a solution to the challenge of [<xref ref-type="bibr" rid="pcbi.1004485.ref008">8</xref>] to understand how posterior distributions of weights can be represented and learned by networks of neurons in the brain.</p>
<p>This new model proposes to reexamine rules for synaptic plasticity. Rather than viewing trial-to-trial variability and ongoing fluctuations of synaptic parameters as the result of a suboptimal implementation of an inherently deterministic plasticity process, it proposes to model experimental data on synaptic plasticity by rules that consist of three terms: the standard (typically deterministic) activity-dependent (e.g., Hebbian or STDP) term that fits the model to external inputs, a second term that enforces structural rules (priors), and a third term that provides the stochastic driving force. This stochastic force enables network parameters to sample from the posterior, i.e., to fluctuate between different possible solutions of the learning task. The stochastic third term can be modeled by a standard formalism (stochastic Wiener process) that had been developed to model Brownian motion. The first two terms can be modeled as drift terms in a stochastic process. A key insight is that one can easily relate details of the resulting more complex rules for the dynamics of network parameters <bold><italic>θ</italic></bold>, which now become stochastic differential equations, to specific features of the resulting posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>) of parameter vectors <bold><italic>θ</italic></bold> from which the network samples. Thereby, this theory provides a new framework for relating experimentally observed details of local plasticity mechanisms (including their typically stochastic implementation on the molecular scale) to functional consequences of network learning. For example, one gets a theoretically founded framework for relating experimental data on spine motility to experimentally observed network properties, such as sparse connectivity, specific distributions of synaptic weights, and the capability to compensate against perturbations [<xref ref-type="bibr" rid="pcbi.1004485.ref009">9</xref>].</p>
<p>We demonstrate the resulting new style of modeling network plasticity in three examples. These examples demonstrate how previously mentioned functional demands on network plasticity, such as incorporation of structural rules, automatic avoidance of overfitting, and inherent and immediate compensation for network perturbances, can be accomplished through stochastic local plasticity processes. We focus here on common models for unsupervised learning in networks of neurons: generative models. We first develop the general learning theory for this class of models, and then describe applications to common non-spiking and spiking generative network models. Both structural plasticity (see [<xref ref-type="bibr" rid="pcbi.1004485.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref011">11</xref>] for reviews) and synaptic plasticity (STDP) are integrated into the resulting theory of network plasticity.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We present a new theoretical framework for analyzing and understanding local plasticity mechanisms of networks of neurons in the brain as stochastic processes, that generate specific distributions <italic>p</italic>(<bold><italic>θ</italic></bold>) of network parameters <bold><italic>θ</italic></bold> over which these parameters fluctuate. This framework can be used to analyze and model many types of learning processes. We illustrate it here for the case of unsupervised learning, i.e., learning without a teacher or rewards. Obviously many learning processes in biological organisms are of this nature, especially learning processes in early sensory areas, and in other brain areas that have to provide and maintain on their own an adequate level of functionality, even in the face of internal or external perturbations.</p>
<p>A common framework for modeling unsupervised learning in networks of neurons are generative models, which date back to the 19th century, when Helmholtz proposed that perception could be understood as unconscious inference [<xref ref-type="bibr" rid="pcbi.1004485.ref012">12</xref>]. Since then the hypothesis of the “generative brain” has been receiving considerable attention, fueling interest in various aspects of the relation between Bayesian inference and the brain [<xref ref-type="bibr" rid="pcbi.1004485.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref014">14</xref>]. The basic assumption of the “Bayesian brain” theory is that the activity <bold><italic>z</italic></bold> of neuronal networks in the brain can be viewed as an internal model for hidden variables in the outside world that give rise to sensory experiences <bold><italic>x</italic></bold> (such as the response <bold><italic>x</italic></bold> of auditory sensory neurons to spoken words that are guessed by an internal model <bold><italic>z</italic></bold>). The internal model <bold><italic>z</italic></bold> is usually assumed to be represented by the activity of neurons in the network, e.g., in terms of the firing rates of neurons, or in terms of spatio-temporal spike patterns. A network 𝒩 of stochastically firing neuron is modeled in this framework by a probability distribution <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>,<bold>z</bold>∣<bold><italic>θ</italic></bold>) that describes the probabilistic relationships between <italic>N</italic> input patterns <bold>x</bold> = <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup> and corresponding network responses <bold>z</bold> = <bold><italic>z</italic></bold><sup>1</sup>, …, <bold><italic>z</italic></bold><sup><italic>N</italic></sup>, where <bold><italic>θ</italic></bold> denotes the vector of network parameters that shape this distribution, e.g., via synaptic efficacies and network connectivity. The marginal probability <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>∣<bold><italic>θ</italic></bold>) = ∑<sub><bold>z</bold></sub> <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>,<bold>z</bold>∣<bold><italic>θ</italic></bold>) of the actually occurring inputs <bold>x</bold> = <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup> under the resulting internal model of the neural network 𝒩 with parameters <bold><italic>θ</italic></bold> can then be viewed as a measure for the agreement between this internal model (which carries out “predictive coding” [<xref ref-type="bibr" rid="pcbi.1004485.ref015">15</xref>]) and its environment (which generates the inputs <bold>x</bold>).</p>
<p>The goal of network learning is usually described in this probabilistic generative framework as finding parameter values <bold><italic>θ</italic></bold>* that maximize this agreement, or equivalently the likelihood of the inputs <bold>x</bold> (maximum likelihood learning):
<disp-formula id="pcbi.1004485.e001"><alternatives><graphic id="pcbi.1004485.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mo form="prefix">arg</mml:mo> <mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:munder> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
Locally optimal parameter solutions are usually determined by gradient ascent on the data likelihood <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>∣<bold><italic>θ</italic></bold>).</p>
<sec id="sec003">
<title>Learning a posterior distribution through stochastic synaptic plasticity</title>
<p>In contrast, we assume here that not only a neural network 𝒩, but also a prior <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>) for its parameters are given. This prior <italic>p</italic><sub>𝒮</sub> can encode both structural constraints (such as sparse connectivity) and structural rules (e.g., a heavy-tailed distribution of synaptic weights). Then the goal of network learning becomes:
<disp-formula id="pcbi.1004485.e002"><alternatives><graphic id="pcbi.1004485.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>learn the posterior distribution</mml:mtext> <mml:mspace width="1pt"/><mml:msup><mml:mi>p</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="1pt"/><mml:mtext>defined (up to normalization) by</mml:mtext></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
The patterns <bold>x</bold> = <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup> are assumed here to be regularly reoccurring network inputs.</p>
<p>A key insight (see <xref ref-type="fig" rid="pcbi.1004485.g001">Fig 1</xref> for an illustration) is that stochastic local plasticity rules for the parameters <italic>θ</italic><sub><italic>i</italic></sub> enable a network to achieve the learning goal <xref ref-type="disp-formula" rid="pcbi.1004485.e002">Eq (2)</xref>: The distribution of network parameters <bold><italic>θ</italic></bold> will converge after a while to the posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>) = <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>)—and produce samples from it—if each network parameter <italic>θ</italic><sub><italic>i</italic></sub> obeys the dynamics
<disp-formula id="pcbi.1004485.e003"><alternatives><graphic id="pcbi.1004485.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>b</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>b</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where the learning rate <italic>b</italic> &gt; 0 controls the speed of the parameter dynamics. <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> is a stochastic differential equation (see [<xref ref-type="bibr" rid="pcbi.1004485.ref016">16</xref>]), which differs from commonly considered differential equations through the stochastic term <italic>d</italic>𝒲<sub><italic>i</italic></sub> that describes infinitesimal stochastic increments and decrements of a Wiener process 𝒲<sub><italic>i</italic></sub>. A Wiener process is a standard model for Brownian motion in one dimension (more precisely: the limit of a random walk with infinitesimal step size and normally distributed increments <inline-formula id="pcbi.1004485.e004"><alternatives><graphic id="pcbi.1004485.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e004"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>−</mml:mo> <mml:msubsup><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi mathvariant="normal">N</mml:mi> <mml:mi mathvariant="normal">o</mml:mi> <mml:mi mathvariant="normal">r</mml:mi> <mml:mi mathvariant="normal">m</mml:mi> <mml:mi mathvariant="normal">a</mml:mi> <mml:mi mathvariant="normal">l</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> between times <italic>t</italic> and <italic>s</italic>). Thus in an approximation of <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> for discrete time steps Δ<italic>t</italic> the term <italic>d</italic>𝒲<sub><italic>i</italic></sub> can be replaced by Gaussian noise with variance Δ<italic>t</italic> (see <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref>). Note that <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> does not have a single solution <italic>θ</italic><sub><italic>i</italic></sub>(<italic>t</italic>), but a continuum of stochastic sample paths (see <xref ref-type="fig" rid="pcbi.1004485.g001">Fig 1F</xref> for an example) that each describe one possible time course of the parameter <italic>θ</italic><sub><italic>i</italic></sub>.</p>
<fig id="pcbi.1004485.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004485.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Maximum likelihood (ML) learning vs. synaptic sampling.</title>
<p><bold>A, B, C</bold>: Illustration of ML learning for two parameters <bold><italic>θ</italic></bold> = (<italic>θ</italic><sub>1</sub>,<italic>θ</italic><sub>2</sub>) of a neural network 𝒩. <bold>A</bold>: 3D plot of an example likelihood function. For a fixed set of inputs <bold>x</bold> it assigns a probability density (amplitude on z-axis) to each parameter setting <bold><italic>θ</italic></bold>. <bold>B</bold>: This likelihood function is defined by some underlying neural network 𝒩. <bold>C</bold>: Multiple trajectories along the gradient of the likelihood function in (A). The parameters are initialized at random initial values (black dots) and then follow the gradient to a local maximum (red triangles). <bold>D</bold>: Example for a prior that prefers small values for <bold><italic>θ</italic></bold>. <bold>E</bold>: The posterior that results as product of the prior (D) and the likelihood (A). <bold>F</bold>: A single trajectory of synaptic sampling from the posterior (E), starting at the black dot. The parameter vector <bold><italic>θ</italic></bold> fluctuates between different solutions, the visited values cluster near local optima (red triangles). <bold>G</bold>: Cartoon illustrating the dynamic forces (plasticity rule <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref>) that enable the network to sample from the posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>) in (E). Magnification of one synaptic sampling step <italic>d<bold>θ</bold></italic> of the trajectory in (F) (green). The three forces acting on <bold><italic>θ</italic></bold>: the deterministic drift term (red) is directed to the next local maximum (red triangle), it consists of the first two terms in <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref>; the stochastic diffusion term <italic>d</italic>𝒲 (black) has a random direction. See <xref ref-type="supplementary-material" rid="pcbi.1004485.s002">S2 Text</xref> for figure details.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.g001"/>
</fig>
<p>Rigorous mathematical results based on Fokker-Planck equations (see <xref ref-type="sec" rid="sec011">Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004485.s001">S1 Text</xref> for details) allow us to infer from the stochastic local dynamics of the parameters <italic>θ</italic><sub><italic>i</italic></sub> given by a stochastic differential equation of the form <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> the probability that the parameter vector <bold><italic>θ</italic></bold> can be found after a while in a particular region of the high-dimensional space in which it moves. The key result is that for the case of the stochastic dynamics according to <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> this probability is equal to the posterior <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>) given by <xref ref-type="disp-formula" rid="pcbi.1004485.e002">Eq (2)</xref>. Hence the stochastic dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> of network parameters <italic>θ</italic><sub><italic>i</italic></sub> enables a network to achieve the learning goal <xref ref-type="disp-formula" rid="pcbi.1004485.e002">Eq (2)</xref>: to learn the posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>). This posterior distribution is not represented in the network through any explicit neural code, but through its stochastic dynamics, as the unique stationary distribution of a Markov process from which it samples continuously. In particular, if most of the mass of this posterior distribution is concentrated on some low-dimensional manifold, the network parameters <bold><italic>θ</italic></bold> will move most of the time within this low-dimensional manifold. Since this realization of the posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>∣<bold>x</bold>) is achieved by sampling from it, we refer to this model defined by <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> (in the case where the parameters <italic>θ</italic><sub><italic>i</italic></sub> represent synaptic parameters) as <italic>synaptic sampling</italic>.</p>
<p>The stochastic term <italic>d</italic>𝒲<sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> provides a simple integrative model for a multitude of biological and biochemical stochastic processes that effect the efficacy of a synaptic connection. The mammalian postsynaptic density comprises over 1000 different types of proteins [<xref ref-type="bibr" rid="pcbi.1004485.ref017">17</xref>]. Many of those proteins that effect the amplitude of postsynaptic potentials and synaptic plasticity, for example NMDA receptors, occur in small numbers, and are subject to Brownian motion within the membrane [<xref ref-type="bibr" rid="pcbi.1004485.ref018">18</xref>]. In addition, the turnover of important scaffolding proteins in the postsynaptic density such as PSD-95, which clusters glutamate receptors and is thought to have a substantial impact on synaptic efficacy, is relatively fast, on the time-scale of hours to days, depending on developmental state and environmental condition [<xref ref-type="bibr" rid="pcbi.1004485.ref019">19</xref>]. Also the volume of spines at dendrites, which is assumed to be directly related to synaptic efficacy [<xref ref-type="bibr" rid="pcbi.1004485.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref021">21</xref>] is reported to fluctuate continuously, even in the absence of synaptic activity [<xref ref-type="bibr" rid="pcbi.1004485.ref002">2</xref>]. Furthermore the stochastically varying internal states of multiple interacting biochemical signaling pathways in the postsynaptic neuron are likely to effect synaptic transmission and plasticity [<xref ref-type="bibr" rid="pcbi.1004485.ref022">22</xref>].</p>
<p>The contribution of the stochastic term <italic>d</italic>𝒲<sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> can be scaled by a temperature parameter <inline-formula id="pcbi.1004485.e005"><alternatives><graphic id="pcbi.1004485.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e005"/><mml:math id="M5" display="inline" overflow="scroll"><mml:msqrt><mml:mi>T</mml:mi></mml:msqrt></mml:math></alternatives></inline-formula>, where <italic>T</italic> can be any positive number. The resulting stationary distribution of <bold><italic>θ</italic></bold> is proportional to <inline-formula id="pcbi.1004485.e006"><alternatives><graphic id="pcbi.1004485.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>p</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, so that the dynamics of the stochastic process can be described by the energy landscape <inline-formula id="pcbi.1004485.e007"><alternatives><graphic id="pcbi.1004485.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e007"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mfrac><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:msup><mml:mi>p</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:math></alternatives></inline-formula>. For high values of <italic>T</italic> this energy landscape is flattened, i.e., the main modes of <italic>p</italic>*(<bold><italic>θ</italic></bold>) become less pronounced. For <italic>T</italic> → 0 the dynamics of <bold><italic>θ</italic></bold> approaches a deterministic process and converges to the next local maximum of <italic>p</italic>*(<bold><italic>θ</italic></bold>). Thus the learning process approximates for low values of <italic>T</italic> maximum a posteriori (MAP) inference [<xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>]. We propose that this temperature parameter <italic>T</italic> is regulated in biological networks of neurons dependent on the developmental state, environment, and behavior of an organism. One can also accommodate a modulation of the dynamics of each individual parameter <italic>θ</italic><sub><italic>i</italic></sub> by a learning rate <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>) that depends on its current value (see <xref ref-type="sec" rid="sec011">Methods</xref>).</p>
</sec>
<sec id="sec004">
<title>Online synaptic sampling</title>
<p>For online learning one assumes that the likelihood <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>∣<bold><italic>θ</italic></bold>) = <italic>p</italic><sub>𝒩</sub>(<bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup>∣<bold><italic>θ</italic></bold>) of the network inputs can be factorized:
<disp-formula id="pcbi.1004485.e008"><alternatives><graphic id="pcbi.1004485.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mn>1</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>N</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
i.e., each network input <bold><italic>x</italic></bold><sup><italic>n</italic></sup> can be explained as being drawn individually from <italic>p</italic><sub>𝒩</sub>(<bold><italic>x</italic></bold><sup><italic>n</italic></sup>∣<bold><italic>θ</italic></bold>), independently from other inputs.</p>
<p>The weight update rule <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> depends on all inputs <bold>x</bold> = <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup>, hence synapses have to keep track of the whole set of all network inputs for the exact dynamics (batch learning). In an online scenario, we assume that only the current network input <bold><italic>x</italic></bold><sup><italic>n</italic></sup> is available for synaptic sampling. One then arrives at the following online-approximation to <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref>
<disp-formula id="pcbi.1004485.e009"><alternatives><graphic id="pcbi.1004485.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>b</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="1em"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>b</mml:mi> <mml:mspace width="0.166667em"/></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
Note the additional factor <italic>N</italic> in the rule. It compensates for the <italic>N</italic>-fold summation of the first and last term in <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> when one moves through all <italic>N</italic> inputs <bold><italic>x</italic></bold><sup><italic>n</italic></sup>. Although convergence to the correct posterior distribution cannot be guaranteed theoretically for this online rule, we show in <xref ref-type="sec" rid="sec011">Methods</xref> that the rule is a reasonable approximation to the batch-rule <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref>. Furthermore, all subsequent simulations are based on this online rule, which demonstrates the viability of this approximation.</p>
</sec>
<sec id="sec005">
<title>Relationship to maximum likelihood learning</title>
<p>Typically, synaptic plasticity in generative network models is modeled as maximum likelihood learning. Time is often discretized into small discrete time steps Δ<italic>t</italic>. For gradient-based approaches the parameter change <inline-formula id="pcbi.1004485.e010"><alternatives><graphic id="pcbi.1004485.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e010"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is then given by the gradient of the log likelihood multiplied with some learning rate <italic>η</italic>:
<disp-formula id="pcbi.1004485.e011"><alternatives><graphic id="pcbi.1004485.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e011"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msubsup><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>L</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
To compare this maximum likelihood update with synaptic sampling, we consider a version of the parameter dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> for discrete time (see <xref ref-type="sec" rid="sec011">Methods</xref> for a derivation):
<disp-formula id="pcbi.1004485.e012"><alternatives><graphic id="pcbi.1004485.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>η</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
where the learning rate <italic>η</italic> is given by <italic>η</italic> = <italic>b</italic> Δ<italic>t</italic> and <inline-formula id="pcbi.1004485.e013"><alternatives><graphic id="pcbi.1004485.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e013"/><mml:math id="M13" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> denotes Gaussian noise with zero mean and variance 1, drawn independently for each parameter <italic>θ</italic><sub><italic>i</italic></sub> and each update time <italic>t</italic>. We see that the maximum likelihood update <xref ref-type="disp-formula" rid="pcbi.1004485.e011">Eq (6)</xref> becomes one term in this online version of synaptic sampling. <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref> is a special case of the online Langevin sampler that was introduced in [<xref ref-type="bibr" rid="pcbi.1004485.ref023">23</xref>].</p>
<p>The first term <inline-formula id="pcbi.1004485.e014"><alternatives><graphic id="pcbi.1004485.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e014"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo mathvariant="bold-italic">θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref> arises from the prior <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>), and has apparently not been considered in previous rules for synaptic plasticity. An additional novel component is the Gaussian noise term <inline-formula id="pcbi.1004485.e015"><alternatives><graphic id="pcbi.1004485.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> (see also <xref ref-type="fig" rid="pcbi.1004485.g001">Fig 1G</xref>). It arises because the accumulated impact of the Wiener process 𝒲<sub><italic>i</italic></sub> over a time interval of length Δ<italic>t</italic> is distributed according to a normal distribution with variance Δ<italic>t</italic>. In contrast to traditional maximum likelihood optimization based on additive noise for escaping local optima, this noise term is not scaled down when learning approaches a local optimum. This ongoing noise is essential for enabling the network to sample from the posterior distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>) via continuously ongoing synaptic plasticity (see <xref ref-type="fig" rid="pcbi.1004485.g001">Fig 1F</xref>).</p>
</sec>
<sec id="sec006">
<title>Synaptic sampling improves the generalization capability of a neural network</title>
<p>The previously described theory for learning a posterior distribution over parameters <bold><italic>θ</italic></bold> can be applied to all neural network models 𝒩 where the derivative <inline-formula id="pcbi.1004485.e016"><alternatives><graphic id="pcbi.1004485.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e016"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> can be efficiently estimated. Since this term also has to be estimated for maximum likelihood learning <xref ref-type="disp-formula" rid="pcbi.1004485.e011">Eq (6)</xref>, synaptic sampling can basically be applied to all neuron and network models that are amenable to maximum likelihood learning. We illustrate salient new features that result from synaptic sampling (i.e., plasticity rules Eqs (<xref ref-type="disp-formula" rid="pcbi.1004485.e009">5</xref>) or (<xref ref-type="disp-formula" rid="pcbi.1004485.e012">7</xref>)) for some of these models. We begin with the Boltzmann machine [<xref ref-type="bibr" rid="pcbi.1004485.ref024">24</xref>], one of the oldest generative neural network models. It is currently still extensively investigated in the context of deep learning [<xref ref-type="bibr" rid="pcbi.1004485.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref026">26</xref>]. We demonstrate in <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2D and 2F</xref> the improved generalization capability of this model for the learning approach <xref ref-type="disp-formula" rid="pcbi.1004485.e002">Eq (2)</xref> (learning of the posterior), compared with maximum likelihood learning (approach <xref ref-type="disp-formula" rid="pcbi.1004485.e001">Eq (1)</xref>), which had been theoretically predicted by [<xref ref-type="bibr" rid="pcbi.1004485.ref006">6</xref>] and [<xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>]. But this model for learning the posterior (approach <xref ref-type="disp-formula" rid="pcbi.1004485.e002">Eq (2)</xref>) in Boltzmann machines is now based on local plasticity rules. Note that the Boltzmann machine with synaptic sampling samples simultaneously on two different time scales: In addition to sampling for given parameters <bold><italic>θ</italic></bold> from likely network states in the usual manner, it now samples simultaneously on a slower time scale according to <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref> from the posterior of network parameters <bold><italic>θ</italic></bold>.</p>
<fig id="pcbi.1004485.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004485.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Priors for synaptic weights improve generalization capability.</title>
<p><bold>A</bold>: The training set, consisting of five samples of a handwritten <italic>1</italic>. Below a cartoon illustrating the network architecture of the restricted Boltzmann machine (RBM), composed of a layer of 784 visible neurons <bold>x</bold> and a layer of 9 hidden neurons <bold>z</bold>. <bold>B</bold>: Examples from the test set. It contains many different styles of writing that are not part of the training set. <bold>C</bold>: Evolution of 50 randomly selected synaptic weights throughout learning (on the training set). The weight histogram (right) shows the distribution of synaptic weights at the end of learning. 80 histogram bins were equally spaced between -4 and 4. <bold>D</bold>: Performance of the network in terms of log likelihood on the training set (blue) and on the test set (red) throughout learning. Mean values over 100 trial runs are shown, shaded area indicates std. The performance on the test set initially increases but degrades for prolonged learning. <bold>E</bold>: Evolution of 50 weights for the same network but with a bimodal prior. The prior <italic>p</italic><sub>𝒮</sub>(<italic>w</italic>) is indicated by the blue curve. Most synaptic weights settle in the mode around 0, but a few larger weights also emerge and stabilize in the larger mode. Weight histogram (green) as in (C). <bold>F</bold>: The log likelihood on the test set maintains a constant high value throughout the whole learning session, compare to (D).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.g002"/>
</fig>
<p>A Boltzmann machine employs extremely simple non-spiking neuron models with binary outputs. Neuron <italic>y</italic><sub><italic>i</italic></sub> outputs 1 with probability <italic>σ</italic>(∑<sub><italic>j</italic></sub> <italic>w</italic><sub><italic>ij</italic></sub> <italic>y</italic><sub><italic>j</italic></sub> + <italic>b</italic><sub><italic>i</italic></sub>), else 0, where <italic>σ</italic> is the logistic sigmoid <inline-formula id="pcbi.1004485.e017"><alternatives><graphic id="pcbi.1004485.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, with synaptic weights <italic>w</italic><sub><italic>ij</italic></sub> and bias parameters <italic>b</italic><sub><italic>i</italic></sub>. Synaptic connections in a Boltzmann machine are bidirectional, with symmetric weights (<italic>w</italic><sub><italic>ij</italic></sub> = <italic>w</italic><sub><italic>ji</italic></sub>). The parameters <bold><italic>θ</italic></bold> for the Boltzmann machine consist of all weights <italic>w</italic><sub><italic>ij</italic></sub> and biases <italic>b</italic><sub><italic>i</italic></sub> in the network. For the special case of a restricted Boltzmann machine (RBM), maximum likelihood learning of these parameters can be done efficiently [<xref ref-type="bibr" rid="pcbi.1004485.ref027">27</xref>], and therefore RBM’s are typically used for deep learning. An RBM has a layered structure with one layer of visible neurons <bold>x</bold> and a second layer of hidden neurons <bold>z</bold>. Synaptic connections are formed only between neurons on different layers (<xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2A</xref>). The maximum likelihood gradients <inline-formula id="pcbi.1004485.e018"><alternatives><graphic id="pcbi.1004485.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>L</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004485.e019"><alternatives><graphic id="pcbi.1004485.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msubsup><mml:mi>b</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>L</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>b</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> can be efficiently approximated for this model, for example
<disp-formula id="pcbi.1004485.e020"><alternatives><graphic id="pcbi.1004485.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where <inline-formula id="pcbi.1004485.e021"><alternatives><graphic id="pcbi.1004485.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e021"/><mml:math id="M21" display="inline" overflow="scroll"><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the output of input neuron <italic>j</italic> while input <bold><italic>x</italic></bold><sup><italic>n</italic></sup> is presented, and <inline-formula id="pcbi.1004485.e022"><alternatives><graphic id="pcbi.1004485.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e022"/><mml:math id="M22" display="inline" overflow="scroll"><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> its output during a subsequent phase of spontaneous activity (“reconstruction phase”); analogously for the hidden neuron <italic>z</italic><sub><italic>j</italic></sub> (see <xref ref-type="sec" rid="sec011">Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004485.s003">S3 Text</xref>).</p>
<p>We integrated this maximum likelihood estimate <xref ref-type="disp-formula" rid="pcbi.1004485.e020">Eq (8)</xref> into the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref> in order to test whether a suitable prior <italic>p</italic><sub>𝒮</sub>(<bold>w</bold>) for the weights improves the generalization capability of the network. The network received as input just five samples <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup>5</sup> of a handwritten Arabic number <italic>1</italic> from the MNIST dataset (the training set, shown in <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2A</xref>) that were repeatedly presented. Each pixel of the digit images was represented by one neuron in the visible layer (which consisted of 784 neurons). We selected a second set of 100 samples of the handwritten digit <italic>1</italic> from the MNIST dataset as test set (<xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2B</xref>). These samples include completely different styles of writing that were not present in the training set. After allowing the network to learn the five input samples from <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2A</xref> for various numbers of update steps (horizontal axis of <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2D and 2F</xref>), we evaluated the learned internal model of this network 𝒩 for the digit <italic>1</italic> by measuring the average log-likelihood log <italic>p</italic><sub>𝒩</sub>(<bold>x</bold>∣<bold><italic>θ</italic></bold>) for the test data. The result is indicated in <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2D and 2F</xref> for the training samples by the blue curves, and for the new test examples, that were never shown while synaptic plasticity was active, by the red curves.</p>
<p>First, a uniform prior over the synaptic weights was used (<xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2C</xref>), which corresponds to the common maximum likelihood learning paradigm <xref ref-type="disp-formula" rid="pcbi.1004485.e020">Eq (8)</xref>. The performance on the test set (shown on vertical axis) initially increases but degrades for prolonged exposure to the training set (length of that prior exposure shown on horizontal axis). This effect is known as overfitting [<xref ref-type="bibr" rid="pcbi.1004485.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>]. It can be reduced by choosing a suitable prior <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>) in the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref>. The choice for the prior distribution is best if it matches the statistics of the training samples [<xref ref-type="bibr" rid="pcbi.1004485.ref006">6</xref>], which has in this case two modes (resulting from black and white pixels). The presence of this prior in the learning rule maintains good generalization capability for test samples even after prolonged exposure to the training set (red curve in <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2F</xref>).</p>
<p>The improved generalization capability of the network is a result of the prior distribution. It is well known that the prior in Bayesian inference allows to effectively prevent overfitting by making solutions that use fewer or smaller parameters more likely. Similar results would therefore emerge in any other implementation of Bayesian learning in neural networks. A thorough discussion on this topic which is known as <italic>Bayesian regularization</italic> can be found in [<xref ref-type="bibr" rid="pcbi.1004485.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>].</p>
<p>As a consequence, the choice of the prior distribution can have a significant impact on the learning result. In <xref ref-type="supplementary-material" rid="pcbi.1004485.s003">S3 Text</xref> we compared a set of different priors and demonstrate this effect more systematically. There it can also be seen that if the choice of the prior is bad, the learning performance can even get worse than in the case without a prior.</p>
</sec>
<sec id="sec007">
<title>Spine motility as synaptic sampling</title>
<p>In the following sections we apply our synaptic sampling framework to networks of spiking neurons and biological models for network plasticity. The number and volume of spines for a synaptic connection is thought to be directly related to its synaptic weight [<xref ref-type="bibr" rid="pcbi.1004485.ref028">28</xref>]. Experimental studies have provided a wealth of information about the stochastic dynamics of dendritic spines (see e.g. [<xref ref-type="bibr" rid="pcbi.1004485.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004485.ref032">32</xref>]). They demonstrate that the volume of a substantial fraction of dendritic spines varies continuously over time, and that all the time new spines and synaptic connections are formed and existing ones are eliminated. We show that these experimental data on spine motility can be understood as special cases of synaptic sampling. The synaptic sampling framework is however very general, and many different models for spine motility can be derived from it as special cases. We demonstrate this here for one simple model, induced by the following assumptions:</p>
<list list-type="order">
<list-item>
<p>We restrict ourselves to plasticity of excitatory synapses, although the framework is general enough to apply to inhibitory synapses as well.</p>
</list-item>
<list-item>
<p>In accordance with experimental studies [<xref ref-type="bibr" rid="pcbi.1004485.ref028">28</xref>], we require that spine sizes have a multiplicative dynamics, i.e., that the amount of change within some given time window is proportional to the current size of the spine.</p>
</list-item>
<list-item>
<p>We assume here for simplicity that a synaptic connection between two neurons is realized by a single spine and that there is a single parameter <italic>θ</italic><sub><italic>i</italic></sub> for each potential synaptic connection <italic>i</italic>.</p>
</list-item>
</list>
<p specific-use="continuation">The last requirement can be met by encoding the state of the synapse in an abstract form, that represents synaptic connectivity and synaptic plasticity in a single parameter <italic>θ</italic><sub><italic>i</italic></sub>. We define that negative values of <italic>θ</italic><sub><italic>i</italic></sub> represent a current disconnection and positive values represent a functional synaptic connection. The distance of the current value of <italic>θ</italic><sub><italic>i</italic></sub> from zero indicates how likely it is that the synapse will soon reconnect (for negative values) or withdraw (for positive values), see <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3A</xref>. In addition the synaptic parameter <italic>θ</italic><sub><italic>i</italic></sub> encodes for positive values the synaptic efficacy <italic>w</italic><sub><italic>i</italic></sub>, i.e., the resulting EPSP amplitudes, by a simple mapping <italic>w</italic><sub><italic>i</italic></sub> = <italic>f</italic>(<italic>θ</italic><sub><italic>i</italic></sub>).</p>
<fig id="pcbi.1004485.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004485.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Integration of spine motility into the synaptic sampling model.</title>
<p><bold>A</bold>: Illustration of the parametrization of spine motility. Values <italic>θ</italic> &gt; 0 indicate a functional synaptic connection. <bold>B</bold>: A Gaussian prior <italic>p</italic><sub>𝒮</sub>(<italic>θ</italic>), and a few stochastic sample trajectories of <italic>θ</italic> according to the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref>. Negative values of <italic>θ</italic> (gray area) are interpreted as non-functional connections. Some stable synaptic connections emerge (traces in the upper half), whereas other synaptic connections come and go (traces in lower half). All traces, as well as survival statistics shown in (E, F), are taken from the network simulation described in detail in the next section and <xref ref-type="supplementary-material" rid="pcbi.1004485.s005">S5 Text</xref>. <bold>C</bold>: The exponential function maps synapse parameters <italic>θ</italic> to synaptic efficacies <italic>w</italic>. Negative values of <italic>θ</italic>, corresponding to (retracted) spines are mapped to a tiny region close to zero in the <italic>w</italic>-space. <bold>D</bold>: The Gaussian prior in the <italic>θ</italic>-space translates to a log-normal distribution in the <italic>w</italic>-space. The traces from (B) are shown in the right panel transformed into the <italic>w</italic>-space. Only persistent synaptic connections contribute substantial synaptic efficacies. <bold>E, F</bold>: The emergent survival statistics of newly formed synaptic connections, (i.e., formed during the preceding 12 hours) evaluated at three different start times throughout learning (blue traces, axes are aligned with start times of the analyses). The survival statistics exhibit in our synaptic sampling model a power-law behavior (red curves, see <xref ref-type="supplementary-material" rid="pcbi.1004485.s005">S5 Text</xref>). The time-scale (and exponent of the power-law) depends on the learning rate <italic>b</italic> in <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref>, and can assume any value in our quite general model (shown is <italic>b</italic> = 10<sup>−4</sup> in (E) and <italic>b</italic> = 10<sup>−6</sup> in (F)).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.g003"/>
</fig>
<p>A large class of mapping functions <italic>f</italic> is supported by our theory (see <xref ref-type="supplementary-material" rid="pcbi.1004485.s004">S4 Text</xref> for details). The second assumption which requires multiplicative synaptic dynamics supports an exponential function <italic>f</italic> in our model, in accordance with previous models of spine motility [<xref ref-type="bibr" rid="pcbi.1004485.ref028">28</xref>]. Thus, we assume in the following that the efficacy <italic>w</italic><sub><italic>i</italic></sub> of synapse <italic>i</italic> is given by
<disp-formula id="pcbi.1004485.e023"><alternatives><graphic id="pcbi.1004485.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e023"/><mml:math id="M23" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
see <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3C</xref>. Note that for a large enough offset <italic>θ</italic><sub>0</sub>, negative parameter values <italic>θ</italic><sub><italic>i</italic></sub> (which model a non-functional synaptic connection) are automatically mapped onto a tiny region close to zero in the <italic>w</italic>-space, so that retracted spines have essentially zero synaptic efficacy. The general rule for online synaptic sampling <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> for the exponential mapping <xref ref-type="disp-formula" rid="pcbi.1004485.e023">Eq (9)</xref> can be written as (see <xref ref-type="supplementary-material" rid="pcbi.1004485.s004">S4 Text</xref>)
<disp-formula id="pcbi.1004485.e024"><alternatives><graphic id="pcbi.1004485.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>b</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="2pt"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="0.166667em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>b</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
In <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref> the multiplicative synaptic dynamics becomes explicit. The gradient <inline-formula id="pcbi.1004485.e025"><alternatives><graphic id="pcbi.1004485.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e025"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, i.e., the activity-dependent contribution to synaptic plasticity, is weighted by <italic>w</italic><sub><italic>i</italic></sub>. Hence, for negative values of <italic>θ</italic><sub><italic>i</italic></sub> (non-functional synaptic connection), the activities of the pre- and post-synaptic neurons have negligible impact on the dynamics of the synapse. Assuming a large enough <italic>θ</italic><sub>0</sub>, retracted synapses therefore evolve solely according to the prior <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>) and the random fluctuations <italic>d</italic>𝒲<sub><italic>i</italic></sub>. For large values of <italic>θ</italic><sub><italic>i</italic></sub> the opposite is the case. The influence of the prior <inline-formula id="pcbi.1004485.e026"><alternatives><graphic id="pcbi.1004485.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e026"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>p</mml:mo> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo mathvariant="bold-italic">θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and the Wiener process <italic>d</italic>𝒲<sub><italic>i</italic></sub> become negligible, and the dynamics is dominated by the activity-dependent likelihood term. Large synapses can therefore become quite stable if the presynaptic activity is strong and reliable (see <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3B</xref>). Through the use of parameters <bold><italic>θ</italic></bold> which determine both synaptic connectivity and synaptic efficacies, the synaptic sampling framework provides a unified model for structural and synaptic plasticity. The prior distribution can have significant impact on the spine motility, encouraging for example sparser or denser synaptic connectivity. If the activity-dependent second term in <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref>, that tries to maximize the likelihood, is small (e.g., because <italic>θ</italic><sub><italic>i</italic></sub> is small or parameters are near a mode of the likelihood) then <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref> implements an Ornstein Uhlenbeck process. This prediction of our model is consistent with a previous analysis which showed that an Ornstein Uhlenbeck process is a viable model for synaptic spine motility [<xref ref-type="bibr" rid="pcbi.1004485.ref028">28</xref>].</p>
<p>The weight dynamics that emerges through the stochastic process <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref> is illustrated in the right panel of <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3D</xref>. A Gaussian parameter prior <italic>p</italic><sub>𝒮</sub>(<italic>θ</italic><sub><italic>i</italic></sub>) results in a log-normal prior <italic>p</italic><sub>𝒮</sub>(<italic>w</italic><sub><italic>i</italic></sub>) in a corresponding stochastic differential equation for synaptic efficacies <italic>w</italic><sub><italic>i</italic></sub> (see <xref ref-type="supplementary-material" rid="pcbi.1004485.s004">S4 Text</xref> for details).</p>
<p>The last term (noise term) in our synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref> predicts that eliminated connections spontaneously regrow at irregular intervals. In this way they can test whether they can contribute to explaining the input. If they cannot contribute, they disappear again. The resulting power-law behavior of the survival of newly formed synaptic connections (<xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3E and 3F</xref>) matches corresponding new experimental data [<xref ref-type="bibr" rid="pcbi.1004485.ref032">32</xref>] and is qualitatively similar to earlier experimental results which revealed a quick decay of transient dendritic spines [<xref ref-type="bibr" rid="pcbi.1004485.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>]. Functional consequences of this structural plasticity are explored in the following sections.</p>
</sec>
<sec id="sec008">
<title>Fast adaptation of synaptic connections and weights to a changing input statistics</title>
<p>We will explore in this and the next section implications of the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref> for network plasticity in simple generative spike-based neural network models.</p>
<p>The main types of spike-based generative neural network models that have been proposed are [<xref ref-type="bibr" rid="pcbi.1004485.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1004485.ref037">37</xref>]. We focus here on the type of models introduced by [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>–<xref ref-type="bibr" rid="pcbi.1004485.ref038">38</xref>], since these models allow an easy estimation of the likelihood gradient (the second term in <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref>) and can relate this likelihood term to STDP. Since these spike-based neural network models have non-symmetric synaptic connections (that model chemical synapses between pyramidal cells in the cortex), they do not allow to regenerate inputs <bold><italic>x</italic></bold> from internal responses <bold><italic>z</italic></bold> by running the network backwards (like in a Boltzmann machine). Rather they are <italic>implicit</italic> generative models, where synaptic weights from inputs to hidden neurons are interpreted as implicit models for presynaptic activity, given that the postsynaptic neuron fires.</p>
<p>We focus in this section on a simple model for an ubiquitous cortical microcircuit motif: an ensemble of pyramidal cells with lateral inhibition, often referred to as Winner-Take-All (WTA) circuit. It has been proposed that this microcircuit motif provides for computational analysis an important bridge between single neurons and larger brain systems [<xref ref-type="bibr" rid="pcbi.1004485.ref039">39</xref>]. We employ a simple form of divisive normalization (as proposed by [<xref ref-type="bibr" rid="pcbi.1004485.ref039">39</xref>]; see <xref ref-type="sec" rid="sec011">Methods</xref>) to model lateral inhibition, thereby arriving at a theoretically tractable version of this microcircuit motif that allows us to compute the maximum likelihood term (second term in <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref>) in the synaptic sampling rule. We assumed Gaussian prior distributions <italic>p</italic><sub>𝒮</sub>(<italic>θ</italic><sub><italic>i</italic></sub>), with mean <italic>μ</italic> and variance <italic>σ</italic><sup>2</sup> over the synaptic parameters <italic>θ</italic><sub><italic>i</italic></sub> (as in <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3B</xref>). Then the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref> yields for this model
<disp-formula id="pcbi.1004485.e027"><alternatives><graphic id="pcbi.1004485.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>b</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>N</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>S</mml:mi> <mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mspace width="2pt"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="0.166667em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>b</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>S</italic>(<italic>t</italic>) denotes the spike train of the postsynaptic neuron and <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) denotes the weight-normalized value of the sum of EPSPs from presynaptic neuron <italic>i</italic> at time <italic>t</italic> (i.e., the summed EPSPs that would arise for weight <italic>w</italic><sub><italic>i</italic></sub> = 1; see <xref ref-type="sec" rid="sec011">Methods</xref> for details). <italic>α</italic> is a parameter that scales the impact of synaptic plasticity depending on the current synaptic efficacy. The resulting activity-dependent component <italic>S</italic>(<italic>t</italic>)(<italic>x<sub>i</sub></italic>(<italic>t</italic>) − <italic>α</italic> <italic>e</italic><sup><italic>w<sub>i</sub></italic></sup>) of the likelihood term is a simplified version of the standard STDP learning rule (<xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4B and 4C</xref>), like in [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref040">40</xref>]. Synaptic plasticity (STDP) for connections from input neurons to pyramidal cells in the WTA circuit can be understood from the generative aspect as fitting a mixture of Poisson (or other exponential family) distributions to high-dimensional spike inputs [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref037">37</xref>]. The factor <italic>w</italic><sub><italic>i</italic></sub> = exp(<italic>θ</italic><sub><italic>i</italic></sub> − <italic>θ</italic><sub>0</sub>) had been discussed in [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>], because it is compatible with the underlying generative model, but provides in addition a better fit to the experimental data of [<xref ref-type="bibr" rid="pcbi.1004485.ref041">41</xref>]. We examine in this section emergent properties of network plasticity in this simple spike-based neural network under the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref>.</p>
<fig id="pcbi.1004485.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004485.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Adaptation of synaptic connections to changing input statistics through synaptic sampling.</title>
<p><bold>A</bold>: Illustration of the network architecture. A WTA circuit consisting of ten neurons <bold>z</bold> receives afferent stimuli from input neurons <bold>x</bold> (few connections shown for a single neuron in <bold>z</bold>). <bold>B</bold>: The STDP learning curve that arises from the likelihood term in <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref>. <bold>C</bold>: Measured STDP curve that results from a related STDP rule for a moderate pairing frequency of 20 Hz, as in [<xref ref-type="bibr" rid="pcbi.1004485.ref041">41</xref>]. (Figure adapted from [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>]). <bold>D, E</bold>: Each sensory experience was modeled by 200 ms long spiking activity of 1000 input neurons, that covered some 3D data space with Gaussian tuning curves (the results do not depend on the finite dimension of the data space, we chose 3 dimension for easier visualization). Insets show the firing activity of randomly chosen 50 of the 1000 input neurons for the sample data points marked by green circles. Objects in the environment were represented by Gaussian clusters (ellipses) in this finite dimensional data space. <bold>F</bold>: During learning phase 1 (3 hours) only samples from SE were presented to the network, in phase 2 (which lasted 1 hour) samples from EE. Shortly after the transition from SE to EE the number of newly formed synaptic connections significantly increases (compare to Fig. 1h in [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>]). <bold>G</bold>: Comparison of the survival of synapses for a network with persistent exposure to EE (EE-EE condition) and a network that was returned to SE (EE-SE condition). Newly formed synaptic connections are transient and quickly decay after formation. A significantly larger fraction of synapses persists if the network continuously receives EE inputs (compare to Fig. 2c in [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>]). The dots show means of measurements taken every 30 minutes, the lines represent two-term exponential fits (<italic>r</italic><sup>2</sup> = 1). The results in (F, G) show means over 5 trial runs. Error bars indicate STD.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.g004"/>
</fig>
<p>It is well documented that cortical dendritic spines are transient and that spine turnover is enhanced by novel experience and training [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref043">43</xref>]. For example, enhanced spine formation as a consequence of sensory enrichment was found in mouse somatosensory cortex [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>]. In this study the animals were exposed to a new sensory environment by adding additional objects to their home cage. This sensory enrichment resulted in a rapid increase in the formation of new spines. If the exposure to the enriched environment was only brief, the newly formed spines quickly decayed.</p>
<p>We wondered whether these experimentally observed effects also emerge in our synaptic sampling model. As in [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>] we exposed the network to different sensory environments to study these effects. Sensory experiences typically involve several processing steps and interactions between multiple brain systems, and precise knowledge about their cortical representation is still missing. Therefore we used here a simple symbolic representation of the sensory environment. We represented each sensory experience by a point in some finite dimensional space which is covered by the tuning curves of a large number of input neurons. Their spike output was then communicated to the WTA circuit in the form of 200 ms-long spike patterns of the 1000 input neurons (see <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4D and 4E</xref> and <xref ref-type="sec" rid="sec011">Methods</xref> for details). Independently drawn sensory experiences were presented sequentially and synaptic sampling according to <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref> was applied continuously to all synapses from the 1000 input neurons to the ten neurons in the WTA circuit.</p>
<p>Each environment was represented as a mixture of Gaussians (clusters) of points in the finite-dimensional sensory space. Each cluster could represent for example different sensory experiences with some object in the environment. Consequently we modelled an enriched environment (EE) simply by adding a few new clusters to the standard environment (SE). In phase 1 the network was exposed to an environment with 3 clusters (standard environment (SE), see <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4D</xref>). After 3 hours the network input was enriched by adding 4 additional clusters (enriched environment (EE), see <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4E</xref>). We found that exposure to EE significantly increased the rate of new synapse formation as in the experimental result of [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>] (<xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4F</xref>).</p>
<p>Most of the newly formed synapses decayed within a few hours after return to the standard environment (EE-SE situation, see <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4G</xref>). In this case only about about 8% become stable. A fraction of about 30% becomes stable when the enriched environment was maintained (EE-EE situation). These results qualitatively reproduce the findings from mouse barrel cortex (compare Figures 1h and 2c in [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>]). Note that we used here relatively large update rates <italic>b</italic> to keep simulation times in a feasible range, which results in spine dynamics on the time scale of hours instead of days as in biological synapses [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>].</p>
</sec>
<sec id="sec009">
<title>Inherent network compensation capability through synaptic sampling</title>
<p>Numerous experimental data show that the same function of a neural circuit is achieved in different individuals with drastically different parameters, and also that a single organism can compensate for disturbances by moving to a new parameter vector [<xref ref-type="bibr" rid="pcbi.1004485.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1004485.ref047">47</xref>]. These results suggest that there exists some low-dimensional submanifold of values for the high-dimensional parameter vector <italic>θ</italic> of a biological neural network that all provide stable network function (degeneracy). We propose that the previously discussed posterior distribution of network parameters <italic>θ</italic> provides a mathematical model for such low-dimensional submanifold. Furthermore we propose that the underlying continuous stochastic fluctuation <italic>d</italic>𝒲 provides a driving force that automatically moves network parameters (with high probability) to a functionally more attractive regime when the current solution performs worse because of perturbations, such as lesions of neurons or network connections. This compensation capability is not an add-on to the synaptic sampling model, but an inherent feature of its organization.</p>
<p>We demonstrate this inherent compensation capability in <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref> for a generative spiking neural network with synaptic parameters <bold><italic>θ</italic></bold> that regulate simultaneously structural plasticity and synaptic plasticity (dynamics of weights) as in Figs <xref ref-type="fig" rid="pcbi.1004485.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1004485.g004">4</xref>. The prior <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>) for these parameters is here the same as in the preceding section (see <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4G</xref> on the left). But in contrast to the previous section we consider here a network that allows us to study the self-organization of connections <italic>between</italic> hidden neurons. The network consists of eight WTA-circuits, but in contrast to <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4</xref> we allow here arbitrary excitatory synaptic connections between neurons within the same or different ones of these WTA circuits. This network models multi-modal sensory integration and association in a simplified manner. Two populations of “auditory” and “visual” input neurons <bold>x</bold><sub><italic>A</italic></sub> and <bold>x</bold><sub><italic>V</italic></sub> project onto corresponding populations <bold>z</bold><sub><italic>A</italic></sub> and <bold>z</bold><sub><italic>V</italic></sub> of hidden neurons (each consisting of one half of the WTA circuits, see lower panel of <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5A</xref>). Only a fraction of the potential synaptic connections became functional (see Fig. S2A in <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref>) through the synaptic sampling rule <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref> that integrates structural and synaptic plasticity. Synaptic weights and connections were not forced to be symmetric or bidirectional.</p>
<fig id="pcbi.1004485.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004485.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Inherent compensation for network perturbations.</title>
<p><bold>A</bold>: A spike-based generative neural network (illustrated at the bottom) received simultaneously spoken and handwritten representations of the same digit (and for tests only spoken digits, see (B)). Stimulus examples for spoken and written digit <italic>2</italic> are shown at the top. These inputs are presented to the network through corresponding firing rates of “auditory” (<bold>x</bold><sub><italic>A</italic></sub>) and “visual” (<bold>x</bold><sub><italic>V</italic></sub>) input neurons. Two populations <bold>z</bold><sub><italic>A</italic></sub> and <bold>z</bold><sub><italic>V</italic></sub> of 40 neurons, each consisting of four WTA circuits like in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4</xref>, receive exclusively auditory or visual inputs. In addition, arbitrary lateral excitatory connections between these “hidden” neurons are allowed. <bold>B</bold>: Assemblies of hidden neurons emerge that encode the presented digit (<italic>1</italic> or <italic>2</italic>). Top panel shows PETH of all neurons from <bold>z</bold><sub><italic>V</italic></sub> for stimulus <italic>1</italic> (left) and <italic>2</italic> (right) after learning, when only an auditory stimulus is presented. Neurons are sorted by the time of their highest average firing. Although only auditory stimuli are presented, it is possible to reconstruct an internally generated “guessed” visual stimulus that represents the same digit (bottom). <bold>C</bold>: First three PCA components of the temporal evolution of a subset <bold><italic>θ</italic></bold>′ of network parameters <bold><italic>θ</italic></bold> (see text). Two major lesions were applied to the network. In the first lesion (transition to red) all neurons that significantly encode stimulus <italic>2</italic> were removed from the population <bold>z</bold><sub><italic>V</italic></sub>. In the second lesion (transition to green) all currently existing synaptic connections between neuron in <bold>z</bold><sub><italic>A</italic></sub> and <bold>z</bold><sub><italic>V</italic></sub> were removed, and not allowed to regrow. After each lesion the network parameters <bold><italic>θ</italic></bold>′ migrate to a new manifold. <bold>D</bold>: The generative reconstruction performance of the “visual” neurons <bold>z</bold><sub><italic>V</italic></sub> for the test case when only an auditory stimulus is presented was tracked throughout the whole learning session, including lesions <italic>1</italic> and <italic>2</italic> (bottom panel). After each lesion the performance strongly degrades, but reliably recovers. Insets show at the top the synaptic weights of neurons in <bold>z</bold><sub><italic>V</italic></sub> at 4 time points <italic>t</italic><sub>1</sub>, …, <italic>t</italic><sub>4</sub>, projected back into the input space like in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4E</xref>. Network diagrams in the middle show ongoing network rewiring for synaptic connections between the hidden neurons <bold>z</bold><sub><italic>A</italic></sub> and <bold>z</bold><sub><italic>V</italic></sub>. Each arrow indicates a functional connection between two neurons. To keep the figure uncluttered only subsets of synapses are shown (1% randomly drawn from the total set of possible lateral connections). Connections at time <italic>t</italic><sub>2</sub> that were already functional at time <italic>t</italic><sub>1</sub> are plotted in gray. The neuron whose parameter vector <bold><italic>θ</italic></bold>′ is tracked in (C) is highlighted in red. The text under the network diagrams shows the total number of functional connections between hidden neurons at the time point.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.g005"/>
</fig>
<p>As in the previous demonstrations we do not use external rewards or teacher-inputs for guiding network plasticity. Rather, we allow the model to discover on its own regularities in its network inputs. The “auditory” hidden neurons <bold>z</bold><sub><italic>A</italic></sub> on the left in <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5A</xref> received temporal spike patterns from the auditory input neurons <bold>x</bold><sub><italic>A</italic></sub> that were generated from spoken utterings of the digit <italic>1</italic> and <italic>2</italic> (which lasted between 320 ms and 520 ms). Simultaneously we presented to the “visual” hidden neurons <bold>z</bold><sub><italic>V</italic></sub> on the right for the same time period a (symbolic) visual representation of the same digit (randomly drawn from the MNIST database like in <xref ref-type="fig" rid="pcbi.1004485.g002">Fig. 2</xref>).</p>
<p>The emergent associations between the two populations <bold>z</bold><sub><italic>A</italic></sub> and <bold>z</bold><sub><italic>V</italic></sub> of hidden neurons were tested by presenting auditory input only and observing the activity of the “visual” hidden neurons <bold>z</bold><sub><italic>V</italic></sub>. <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5B</xref> shows the emergent activity of the neurons <bold>z</bold><sub><italic>V</italic></sub> when only the auditory stimulus was presented (visual input neurons <bold>x</bold><sub><italic>V</italic></sub> remained silent). The generative aspect of the network can be demonstrated by reconstructing for this case the visual stimulus from the activity of the “visual” hidden neurons <bold>z</bold><sub><italic>V</italic></sub>. <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5B</xref> shows reconstructed visual stimuli from a single run where only the auditory stimuli <bold>x</bold><sub><italic>A</italic></sub> for digits <italic>1</italic> (left) and <italic>2</italic> (right) were presented to the network. Digit images were reconstructed by multiplying the synaptic efficacies of synapses from neurons in <bold>x</bold><sub><italic>V</italic></sub> to neurons in <bold>z</bold><sub><italic>V</italic></sub> (which did not receive any input from <bold>x</bold><sub><italic>V</italic></sub> in this experiment) with the instantaneous firing rates of the corresponding <bold>z</bold><sub><italic>V</italic></sub>-neurons.</p>
<p>Interestingly we found that synaptic sampling significantly outperforms the pure deterministic STDP updates introduced in [<xref ref-type="bibr" rid="pcbi.1004485.ref038">38</xref>], which do not impose a prior distribution over synaptic parameters. The structural prior that favors solutions with only a small number of large synaptic weights seems to be beneficial for this task as it allows to learn few but pronounced associations between the neurons (see <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref>).</p>
<p>In order to investigate the inherent compensation capability of synaptic sampling, we applied two lesions to the network within a learning session of 8 hours. In the first lesion all neurons (16 out of 40) that became tuned for digit <italic>2</italic> in the preceding learning (see <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5D</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref>) were removed. The lesion significantly impaired the performance of the network in stimulus reconstruction, but it was able to recover from the lesion after about one hour of continuing network plasticity according to <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref> (<xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5D</xref>). The reconstruction performance of the network was measured here continuously through the capability of a linear readout neuron from the visual ensemble to classify the current auditory stimulus (<italic>1</italic> or <italic>2</italic>).</p>
<p>In the second lesion all synaptic connections between hidden neurons that were present after recovery from the first lesion were removed and not allowed to regrow (2936 synapses in total). After about two hours of continuing synaptic sampling 294 new synaptic connections between hidden neurons emerged. These made it again possible to infer the auditory stimulus from the activity of the remaining 24 hidden neurons in the population <bold>z</bold><sub><italic>V</italic></sub> (in the absence of any input from the population <bold>x</bold><sub><italic>V</italic></sub>), at about 75% of the performance level before the second lesion (see bottom panel of <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5D</xref>).</p>
<p>In order to illustrate the ongoing network reconfiguration we track in <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5C</xref> the temporal evolution of a subset <bold><italic>θ</italic></bold>′ of network parameters (35 parameters <italic>θ</italic><sub><italic>i</italic></sub> associated with the potential synaptic connections of the neuron marked in red in the middle of <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5D</xref> from or to other hidden neurons, excluding those that were removed at lesion <italic>2</italic> and not allowed to regrow). The first three PCA components of this 35-dimensional parameter vector are shown. The vector <bold><italic>θ</italic></bold>′ fluctuates first within one region of the parameter space while probing different solutions to the learning problem, e.g., high probability regions of the posterior distribution (blue trace). Each lesions induced a fast switch to a different region (red, green), accompanied by a recovery of the visual stimulus reconstruction performance (see <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5D</xref>).</p>
<p>The random fluctuations were found to be an integral part of the fast recovery form lesions. In <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref> we analyzed the impact of the diffusion term in <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref> on the learning speed. We found that it acts as a temperature parameter that allows to scale the speed of exploration in the parameter space (see also the <xref ref-type="sec" rid="sec011">Methods</xref> for a detailed derivation).</p>
<p>Altogether this experiment showed that continuously ongoing synaptic sampling maintains stable network function also in a more complex network architecture. Another consequence of synaptic sampling was that the neural codes (assembly sequences) that emerged for the two digit classes within the hidden neurons <bold>z</bold><sub><italic>A</italic></sub> and <bold>z</bold><sub><italic>V</italic></sub> (see Fig. S2B in <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref>) drifted over larger periods of time (also in the absence of lesions), similarly as observed for place cells in [<xref ref-type="bibr" rid="pcbi.1004485.ref048">48</xref>] and for tuning curves of motor cortex neurons in [<xref ref-type="bibr" rid="pcbi.1004485.ref004">4</xref>].</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>We have shown that stochasticity may provide an important function for network plasticity. It enables networks to sample parameters from some low-dimensional manifold in a high-dimensional parameter space that represents attractive combinations of structural constraints and rules (such as sparse connectivity and heavy-tailed distributions of synaptic weights) and a good fit to empirical evidence (e.g., sensory inputs). We have developed a normative model for this new learning perspective, where the traditional gold standard of maximum likelihood optimization is replaced by theoretically optimal sampling from a posterior distribution of parameter settings, where regions of high probability provide a theoretically optimal model for the low-dimensional manifold from which parameter settings should be sampled. The postulate that networks should learn such posterior distributions of parameters, rather than maximum likelihood values, had been proposed already for quite some while for artificial neural networks [<xref ref-type="bibr" rid="pcbi.1004485.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>], since such organization of learning promises better generalization capability to new examples. The open problem how such posterior distributions could be learned by networks of neurons in the brain, in a way that is consistent with experimental data, has been highlighted in [<xref ref-type="bibr" rid="pcbi.1004485.ref008">8</xref>] as a key challenge for computational neuroscience. We have presented here such a model, whose primary innovation is to view experimentally found trial-to-trial variability and ongoing fluctuations of parameters such as spine volumes no longer as a nuisance, but as a functionally important component of the organization of network learning, since it enables sampling from a distribution of network configurations. The mathematical framework that we have presented provides a normative model for evaluating such empirically found stochastic dynamics of network parameters, and for relating specific properties of this “noise” to functional aspects of network learning.</p>
<p>Reports of trial-to-trial variability and ongoing fluctuations of parameters related to synaptic weights are ubiquitous in experimental studies of synaptic plasticity and its molecular implementation, from fluctuations of proteins such as PSD-95 [<xref ref-type="bibr" rid="pcbi.1004485.ref019">19</xref>] in the postsynaptic density that are thought to be related to synaptic strength, over intrinsic fluctuations in spine volumes and synaptic connections [<xref ref-type="bibr" rid="pcbi.1004485.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004485.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref032">32</xref>], to surprising shifts of neural codes on a larger time scale [<xref ref-type="bibr" rid="pcbi.1004485.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref048">48</xref>]. These fluctuations may have numerous causes, from noise in the external environment over noise and fluctuations of internal states in sensory neurons and brain networks, to noise in the pre- and postsynaptic molecular machinery that implements changes in synaptic efficacies on various time scales [<xref ref-type="bibr" rid="pcbi.1004485.ref018">18</xref>]. One might even hypothesize, that it would be very hard for this molecular machinery to implement synaptic weights that remain constant in the absence of learning, and deterministic rules for synaptic plasticity, because the half-life of many key proteins that are involved is relatively short, and receptors and other membrane-bound proteins are subject to Brownian motion. In this context the finding that neural codes shift over time [<xref ref-type="bibr" rid="pcbi.1004485.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref048">48</xref>] appears to be less surprising. In fact, our model predicts (see <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref>) that also stereotypical assembly sequences that emerge in our model through learning, similarly as in the experimental data of [<xref ref-type="bibr" rid="pcbi.1004485.ref049">49</xref>], are subject to such shifts on a larger time scale. However it should be pointed out that our model is agnostic with regard to the time scale on which these changes occur, since this time scale can be defined arbitrarily through the parameter <italic>b</italic> (learning rate) in <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref>.</p>
<p>The model that we have presented makes no assumptions about the actual sources of noise. It only assumes that salient network parameters are subject to stochastic processes, that are qualitatively similar to those which have been studied and modeled in the context of Brownian motion of particles as random walk on the microscale. One can scale the influence of these stochastic forces in the model by a parameter <italic>T</italic> that regulates the “temperature” of the stochastic dynamics of network parameters <bold><italic>θ</italic></bold>. This parameter <italic>T</italic> regulates the tradeoff between trying out different regions (or modes) of the posterior distribution of <bold><italic>θ</italic></bold> (exploration), and staying for longer time periods in a high probability region of the posterior (exploitation). We conjecture that this parameter <italic>T</italic> varies in the brain between different brain regions, and possibly also between different types of synaptic connections within a cortical column. For example, spine turnover is increased for large values of <italic>T</italic>, and network parameters <bold><italic>θ</italic></bold> can move faster to a new peak in the posterior distribution, thereby supporting faster learning (and faster forgetting). Since spine turnover is reported to be higher in the hippocampus than in the cortex [<xref ref-type="bibr" rid="pcbi.1004485.ref050">50</xref>], such higher value of <italic>T</italic> could for example be more adequate for modeling network plasticity in the hippocampus. This model would then also support the hypothesis of [<xref ref-type="bibr" rid="pcbi.1004485.ref050">50</xref>] that memories are more transient in the hippocampus. In addition <italic>T</italic> is likely to be regulated on a larger time scale by developmental processes, and on a shorter time scale by neuromodulators and attentional control. The view that synaptic plasticity is stochastic had already been explored through simulation studies in [<xref ref-type="bibr" rid="pcbi.1004485.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref051">51</xref>]. Artificial neural networks were trained in [<xref ref-type="bibr" rid="pcbi.1004485.ref051">51</xref>] through supervised learning with high learning rates and high amounts of noise both on neuron outputs and synaptic weight changes. The authors explored the influence of various combinations of noise levels and learning rates on the success of learning, which can be understood as varying the temperature parameters <italic>T</italic> in the synaptic sampling framework. In order to measure this parameter <italic>T</italic> experimentally in a direct manner, one would have to apply repeatedly the same plasticity induction protocol to the same synapse, with a complete reset of the internal state of the synapse between trials, and measure the resulting trial-to-trial variability of changes of its synaptic efficacy. Since such complete reset of a synaptic state appears to be impossible at present, one can only try to approximate it by the variability that can be measured between different instances of the same type of synaptic connection.</p>
<p>We have shown that the Fokker-Planck equation, a standard tool in physics for analyzing the temporal evolution of the spatial probability density function for particles under Brownian motion, can be used to create bridges between details of local stochastic plasticity processes on the microscale and the probability distribution of the vector <bold><italic>θ</italic></bold> of all parameters on the network level. This theoretical result provides the basis for the new theory of network plasticity that we are proposing. In particular, this link allows us to derive rules for synaptic plasticity which enable the network to learn, and represent in a stochastic manner, a desirable posterior distribution of network parameters; in other words: to approximate Bayesian inference.</p>
<p>We find that resulting rules for synaptic plasticity contain the familiar term for maximum likelihood learning. But another new term, apart from the Brownian-motion-like stochastic term, is the term <inline-formula id="pcbi.1004485.e028"><alternatives><graphic id="pcbi.1004485.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e028"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> that results from a prior distributions <italic>p</italic><sub>𝒮</sub>(<italic>θ</italic><sub><italic>i</italic></sub>), which could actually be different for each biological parameter <italic>θ</italic><sub><italic>i</italic></sub> and enforce structural requirements and preferences of networks of neurons in the brain. Some systematic dependencies of changes in synaptic weights (for the same pairing of pre- and postsynaptic activity) on their current values had already been reported in [<xref ref-type="bibr" rid="pcbi.1004485.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref052">52</xref>–<xref ref-type="bibr" rid="pcbi.1004485.ref054">54</xref>]. These can be modeled as impact of priors. Other potential functional benefits of priors (on emergent selectivity of neurons) have recently been demonstrated in [<xref ref-type="bibr" rid="pcbi.1004485.ref055">55</xref>] for a restricted Boltzmann machine. An interesting open question is whether the non-local learning rules of their model can be approximated through biologically more realistic local plasticity rules, e.g. through synaptic sampling. We have also demonstrated in Figs <xref ref-type="fig" rid="pcbi.1004485.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1004485.g004">4</xref> that suitable priors can model experimental data from [<xref ref-type="bibr" rid="pcbi.1004485.ref032">32</xref>] and [<xref ref-type="bibr" rid="pcbi.1004485.ref033">33</xref>] on the survival statistics of dendritic spines. The transient behavior of synaptic turnover in our model fits a two-term exponential function, the long-term (stationary) behavior is well described by a power-law. Both findings are in accordance with experimental data.</p>
<p>The results reported in [<xref ref-type="bibr" rid="pcbi.1004485.ref056">56</xref>] suggest that learned neural representations integrate experience with a priori beliefs about the sensory environment. The model presented here could be used to further investigate this hypothesis. Also the Fokker-Planck formalism was previously applied to describe the dynamics of dendritic spines in hippocampus [<xref ref-type="bibr" rid="pcbi.1004485.ref057">57</xref>]. The methods described there to integrate experimental data into computational models could be combined with the synaptic sampling framework to further improve the fit to biology.</p>
<p>Finally, we have demonstrated in Figs <xref ref-type="fig" rid="pcbi.1004485.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1004485.g005">5</xref> that suitable priors for network parameters <italic>θ</italic><sub><italic>i</italic></sub> that model spine volumes endow a neural network with the capability to respond to changes in the input distribution and network perturbations with a network rewiring that maintains or restores the network function, while simultaneously observing structural constraints such as sparse connectivity.</p>
<p>Our model underlines the importance of further experimental investigation of priors for network parameters. How are they implemented on a molecular level? What role does gene regulation have in their implementation? How does the history of a synapse affect its prior? In particular, can consolidation of a synaptic weight <italic>θ</italic><sub><italic>i</italic></sub> be modeled in an adequate manner as a modification of its prior? This would be attractive from a functional perspective, because according to our model it both allows long-term storage of learned information and flexible network responses to subsequent perturbations.</p>
<p>Besides the use of parameter priors, dropout [<xref ref-type="bibr" rid="pcbi.1004485.ref058">58</xref>] and dropconnect [<xref ref-type="bibr" rid="pcbi.1004485.ref059">59</xref>] can be used to avoid overfitting in artificial neural networks. In particular, dropconnect, which drops randomly chosen synaptic connections during training, is reminiscent of stochastic synaptic release in biological neuronal networks. In synaptic sampling, synaptic parameters are assumed to be stochastic, however, this stochastic dynamics evolves on a much slower time scale than stochastic release, which was not modeled in our simulations. An interesting open question is whether synaptic sampling combined with stochastic synaptic release would further improve generalization capabilities of spiking neural networks in a similar manner as dropconnect for artificial neural networks.</p>
<p>We have focused in the examples for our model on the plasticity of synaptic weights and synaptic connections. But the synaptic sampling framework can also be used for studying the plasticity of other synaptic parameters, e.g., parameters that control the short term dynamics of synapses, i.e., their individual mixture of short term facilitation and depression. The corresponding parameters <italic>U</italic>, <italic>D</italic>, <italic>F</italic> of the models from [<xref ref-type="bibr" rid="pcbi.1004485.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref061">61</xref>] are known to depend in a systematic manner on the type of pre- and postsynaptic neuron [<xref ref-type="bibr" rid="pcbi.1004485.ref062">62</xref>], indicative of a corresponding prior. However also a substantial variability within the same type of synaptic connections, had been found [<xref ref-type="bibr" rid="pcbi.1004485.ref062">62</xref>]. Hence it would be interesting to investigate functional properties and experimentally testable consequences of stochastic plasticity rules of type <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> for <italic>U</italic>, <italic>D</italic>, <italic>F</italic>, and to compare the results with those of previously considered deterministic plasticity rules for <italic>U</italic>, <italic>D</italic>, <italic>F</italic> (see e.g., [<xref ref-type="bibr" rid="pcbi.1004485.ref063">63</xref>]).</p>
<p>Early theoretical work on activity-dependent formation and elimination of synapses has been used to model ocular dominance in the visual cortex [<xref ref-type="bibr" rid="pcbi.1004485.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref065">65</xref>]. Theoretical models for structural plasticity have also shown that simple plasticity models combined with mechanisms for rewiring are able to model cortical reorganization after lesions [<xref ref-type="bibr" rid="pcbi.1004485.ref066">66</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref067">67</xref>]. In [<xref ref-type="bibr" rid="pcbi.1004485.ref068">68</xref>] a model was presented that combines structural plasticity and STDP. This model was able to reproduce the existence of transient and persistent spines in the cortex. A recently introduced probabilistic model of structural plasticity was also able to reproduced the statistics of the number of synaptic connections between pairs of neurons in the cortex [<xref ref-type="bibr" rid="pcbi.1004485.ref069">69</xref>]. Furthermore a simple model of structural synaptic plasticity has been introduced that was able to explain cognitive phenomena such as graded amnesia and catastrophic forgetting [<xref ref-type="bibr" rid="pcbi.1004485.ref070">70</xref>]. In contrast to these previous studies, the goal of the current work was to establish a model of structural plasticity that follows from a first functional principle, that is, sampling from the posterior distribution over parameters.</p>
<p>We have demonstrated that this framework provides a new and principled way of modeling structural plasticity [<xref ref-type="bibr" rid="pcbi.1004485.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref011">11</xref>]. The challenge to find a biologically plausible way of modeling structural plasticity as Bayesian inference has been highlighted by [<xref ref-type="bibr" rid="pcbi.1004485.ref008">8</xref>]. In addition, the proposed framework does not treat rewiring and synaptic plasticity separately, but provides a unified theory for both phenomena, that can be directly related to functional aspects of the network via the resulting posterior distribution. We have shown in Figs <xref ref-type="fig" rid="pcbi.1004485.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1004485.g004">4</xref> that this rule produces a population of persistent synapses that remain stable over long periods of time, and another population of transient synaptic connections which disappear and reappear randomly, thereby supporting automatic adaptation of the network structure to changes in the distribution of external inputs (<xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4</xref>) and network perturbation (<xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref>).</p>
<p>On a more general level we propose that a framework for network plasticity where network parameters are sampled continuously from a posterior distribution will automatically be less brittle than previously considered maximum likelihood learning frameworks. The latter require some intelligent supervisor who recognizes that the solution given by the current parameter vector is no longer useful, and induces the network to resume plasticity. In contrast, plasticity processes remain active all the time in our sampling-based framework. Hence network compensation for external or internal perturbations is automatic and inherent in the organization of network plasticity.</p>
<p>The need to rethink observed parameter values and plasticity processes in biological networks of neurons in a way which takes into account their astounding variability and compensation capabilities has been emphasized by Eve Marder (see e.g. [<xref ref-type="bibr" rid="pcbi.1004485.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref071">71</xref>]) and others. This article has introduced a new conceptual and mathematical framework for network plasticity that promises to provide a foundation for such rethinking of network plasticity.</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec012">
<title>Details to <italic>Learning a posterior distribution through stochastic synaptic plasticity</italic></title>
<p>Here we prove that <italic>p</italic>*(<bold><italic>θ</italic></bold>) = <italic>p</italic>(<bold><italic>θ</italic></bold>∣ <bold>x</bold>) is the unique stationary distribution of the parameter dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> that operate on the network parameters <bold><italic>θ</italic></bold> = (<italic>θ</italic><sub>1</sub>, …,<italic>θ</italic><sub><italic>M</italic></sub>). Convergence to this stationary distribution then follows for strictly positive <italic>p</italic>*(<bold><italic>θ</italic></bold>). In fact, we prove here a more general result for parameter dynamics given by
<disp-formula id="pcbi.1004485.e029"><alternatives><graphic id="pcbi.1004485.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo mathvariant="bold-italic">θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
for <italic>i</italic> = 1, …, <italic>M</italic> and <inline-formula id="pcbi.1004485.e030"><alternatives><graphic id="pcbi.1004485.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e030"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. This dynamics includes a temperature parameter <italic>T</italic> and a sampling-speed factor <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>) that can in general depend on the current value of the parameter <italic>θ</italic><sub><italic>i</italic></sub>. The temperature parameter <italic>T</italic> can be used to scale the diffusion term (i.e., the noise). The sampling-speed factor controls the speed of sampling, i.e., how fast the parameter space is explored. It can be made dependent on the individual parameter value without changing the stationary distribution. For example, the sampling speed of a synaptic weight can be slowed down if it reaches very high or very low values. Note that the dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref> is a special case of the dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e029">Eq (12)</xref> with unit temperature <italic>T</italic> = 1 and constant sampling speed <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>) ≡ <italic>b</italic>. We show that the stochastic dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e029">Eq (12)</xref> leaves the distribution
<disp-formula id="pcbi.1004485.e031"><alternatives><graphic id="pcbi.1004485.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e031"/><mml:math id="M31" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>p</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mo>𝒵</mml:mo></mml:mfrac> <mml:msup><mml:mi>q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
invariant, where 𝒵 is a normalizing constant 𝒵 = ∫<italic>q</italic>*(<bold><italic>θ</italic></bold>) <bold><italic>dθ</italic></bold> and
<disp-formula id="pcbi.1004485.e032"><alternatives><graphic id="pcbi.1004485.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e032"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>p</mml:mi> <mml:mspace width="1pt"/><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac></mml:msup> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
Note that the stationary distribution <italic>p</italic>*(<bold><italic>θ</italic></bold>) is shaped by the temperature parameter <italic>T</italic>, in the sense that <italic>p</italic>*(<bold><italic>θ</italic></bold>) is a flattened version of the posterior for high temperature. The result is formalized in the following theorem, which is proven in detail in <xref ref-type="supplementary-material" rid="pcbi.1004485.s001">S1 Text</xref>:</p>
<p><bold>Theorem 1.</bold> <italic>Let p(</italic><bold>x</bold>,<italic><bold>θ</bold>) be a strictly positive, continuous probability distribution over continuous or discrete states</italic> <bold>x</bold> = <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup> <italic>and continuous parameters <bold>θ</bold> = (θ<sub>1</sub>, …,θ<sub>M</sub>), twice continuously differentiable with respect to <bold>θ</bold>. Let b(θ) be a strictly positive, twice continuously differentiable function. Then the set of stochastic differential</italic> <xref ref-type="disp-formula" rid="pcbi.1004485.e029">Eq (12)</xref> <italic>leaves the distribution p*(<bold>θ</bold>) invariant. Furthermore, p*(<bold>θ</bold>) is the unique stationary distribution of the sampling dynamics.</italic></p>
<sec id="sec013">
<title>Online approximation</title>
<p>We show here that the rule <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> is a reasonable approximation to the batch-rule <xref ref-type="disp-formula" rid="pcbi.1004485.e003">Eq (3)</xref>. According to the dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e029">Eq (12)</xref>, synaptic plasticity rules that implement synaptic sampling have to compute the log likelihood derivative <inline-formula id="pcbi.1004485.e033"><alternatives><graphic id="pcbi.1004485.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e033"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. We assume that every <italic>τ</italic><sub><italic>x</italic></sub> time units a different input <bold><italic>x</italic></bold><sup><italic>n</italic></sup> is presented to the network. For simplicity, assume that <bold><italic>x</italic></bold><sup>1</sup>, …, <bold><italic>x</italic></bold><sup><italic>N</italic></sup> are visited in a fixed regular order. Under the assumption that input patterns are drawn independently, the likelihood of the generative model factorizes
<disp-formula id="pcbi.1004485.e034"><alternatives><graphic id="pcbi.1004485.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e034"/><mml:math id="M34" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
The derivative of the log likelihood is then given by
<disp-formula id="pcbi.1004485.e035"><alternatives><graphic id="pcbi.1004485.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e035"/><mml:math id="M35" display="block" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
Using <xref ref-type="disp-formula" rid="pcbi.1004485.e035">Eq (16)</xref> in the dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e029">Eq (12)</xref>, one obtains
<disp-formula id="pcbi.1004485.e036"><alternatives><graphic id="pcbi.1004485.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e036"/><mml:math id="M36" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
Hence, the parameter dynamics depends at any time on all network inputs and network responses.</p>
<p>This “batch” dynamics does not map readily onto a network implementation because the weight update requires at any time knowledge of all inputs <bold><italic>x</italic></bold><sup><italic>n</italic></sup>. We provide here an online approximation for small sampling speeds. To obtain an online learning rule, we consider the parameter dynamics
<disp-formula id="pcbi.1004485.e037"><alternatives><graphic id="pcbi.1004485.e037g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e037"/><mml:math id="M37" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
As in the batch learning setting, we assume that each input <bold><italic>x</italic></bold><sup><italic>n</italic></sup> is presented for a time interval of <italic>τ</italic><sub><italic>x</italic></sub>. Integrating the parameter changes <xref ref-type="disp-formula" rid="pcbi.1004485.e037">Eq (18)</xref> over one full presentation of the data <bold>x</bold>, i.e., starting from <italic>t</italic> = 0 with some initial parameter values <bold><italic>θ</italic></bold>(0) up to time <italic>t</italic> = <italic>Nτ</italic><sub><italic>x</italic></sub>, we obtain for slow sampling speeds (<italic>Nτ</italic><sub><italic>x</italic></sub> <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>) ≪ 1)
<disp-formula id="pcbi.1004485.e038"><alternatives><graphic id="pcbi.1004485.e038g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e038"/><mml:math id="M38" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd><mml:mi>N</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mspace width="1pt"/><mml:mo stretchy="true">(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo mathvariant="bold-italic">θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mo mathvariant="bold-italic">θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi> <mml:mn>0</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula>
This is also what one obtains when integrating <xref ref-type="disp-formula" rid="pcbi.1004485.e036">Eq (17)</xref> for <italic>Nτ</italic><sub><italic>x</italic></sub> time units (for slow <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>)). Hence, for slow enough <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>), <xref ref-type="disp-formula" rid="pcbi.1004485.e037">Eq (18)</xref> is a good approximation of optimal weight sampling. The update rule <xref ref-type="disp-formula" rid="pcbi.1004485.e009">Eq (5)</xref> follows from <xref ref-type="disp-formula" rid="pcbi.1004485.e037">Eq (18)</xref> for <italic>T</italic> = 1 and <italic>b</italic>(<italic>θ</italic><sub><italic>i</italic></sub>) ≡ <italic>b</italic>.</p>
</sec>
<sec id="sec014">
<title>Discrete time approximation</title>
<p>Here we provide the derivation for the approximate discrete time learning rule <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref>. For a discrete time parameter update at time <italic>t</italic> with discrete time step Δ<italic>t</italic> during which <bold><italic>x</italic></bold><sup><italic>n</italic></sup> is presented, a corresponding rule can be obtained by short integration of the continuous time rule <xref ref-type="disp-formula" rid="pcbi.1004485.e037">Eq (18)</xref> over the time interval from <italic>t</italic> to <italic>t</italic> + Δ<italic>t</italic>:
<disp-formula id="pcbi.1004485.e039"><alternatives><graphic id="pcbi.1004485.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e039"/><mml:math id="M39" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>Δ</mml:mi> <mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>Δ</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>Δ</mml:mi> <mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>Δ</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
where <inline-formula id="pcbi.1004485.e040"><alternatives><graphic id="pcbi.1004485.e040g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e040"/><mml:math id="M40" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> denotes Gaussian noise <inline-formula id="pcbi.1004485.e041"><alternatives><graphic id="pcbi.1004485.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e041"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mtext>NORMAL</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The update rule <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref> is obtained by choosing a constant <italic>b</italic>(<italic>θ</italic>) ≡ <italic>b</italic>, <italic>T</italic> = 1, and defining <italic>η</italic> = Δ<italic>t</italic> <italic>b</italic>.</p>
</sec>
<sec id="sec015">
<title>Synaptic sampling with hidden states</title>
<p>When there is a direct relationship between network parameters <bold><italic>θ</italic></bold> and the distribution over input patterns <bold><italic>x</italic></bold><sup><italic>n</italic></sup>, the parameter dynamics can directly be derived from the derivative of the data log likelihood and the derivative of the parameter prior. Typically however, generative models for brain computation assume that the network response <bold><italic>z</italic></bold><sup><italic>n</italic></sup> to input pattern <bold><italic>x</italic></bold><sup><italic>n</italic></sup> represents in some manner the value of hidden variables that explain the current input pattern. In the presence of hidden variables, maximum likelihood learning cannot be applied directly, since the state of the hidden variables is not known from the observed data. The expectation maximization algorithm [<xref ref-type="bibr" rid="pcbi.1004485.ref007">7</xref>] can be used to overcome this problem. We adopt this approach here. In the online setting, when pattern <bold><italic>x</italic></bold><sup><italic>n</italic></sup> is applied to the network, it responds with network state <bold><italic>z</italic></bold><sup><italic>n</italic></sup> according to <italic>p</italic><sub>𝒩</sub>(<bold><italic>z</italic></bold>|<italic><bold>x</bold><sup>n</sup></italic>, <bold><italic>θ</italic></bold>), where the current network parameters are used in this inference process. The parameters are updated in parallel according to the dynamics
<disp-formula id="pcbi.1004485.e042"><alternatives><graphic id="pcbi.1004485.e042g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e042"/><mml:math id="M42" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>T</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>b</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>T</mml:mi> <mml:mi>b</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
Note that in comparison with the dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e037">Eq (18)</xref>, the likelihood term now also contains the current network response <bold><italic>z</italic></bold><sup><italic>n</italic></sup>. It can be shown that this dynamics leaves the stationary distribution
<disp-formula id="pcbi.1004485.e043"><alternatives><graphic id="pcbi.1004485.e043g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e043"/><mml:math id="M43" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>p</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mo>𝒵</mml:mo></mml:mfrac> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
invariant, where 𝒵 is again a normalizing constant (the dynamics <xref ref-type="disp-formula" rid="pcbi.1004485.e042">Eq (20)</xref> is again the online-approximation). Hence, in this setup, the network samples concurrently from circuit states (given <bold><italic>θ</italic></bold>) and network parameters (given the network state <bold><italic>z</italic></bold><sup><italic>n</italic></sup>), which can be seen as a sampling-based version of online expectation maximization.</p>
</sec>
</sec>
<sec id="sec016">
<title>Details to <italic>Improving the generalization capability of a neural network through synaptic sampling</italic></title>
<p>For learning the distribution over different writings of digit <italic>1</italic> with different priors in <xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2</xref>, a restricted Boltzmann machine (RBM) with 748 visible and 9 hidden neurons was used. A detailed definition of the RBM model and additional details to the simulations are given in <xref ref-type="supplementary-material" rid="pcbi.1004485.s003">S3 Text</xref>.</p>
<sec id="sec017">
<title>Network inputs</title>
<p>Handwritten digit images were taken from the MNIST dataset [<xref ref-type="bibr" rid="pcbi.1004485.ref072">72</xref>]. In MNIST, each instance of a handwritten digit is represented by a 784-dimensional vector <bold><italic>x</italic></bold><sup><italic>n</italic></sup>. Each entry is given by the gray-scale value of a pixel in the 28 × 28 pixel image of the handwritten digit. The pixel values were scaled to the interval [0, 1]. In the RBM, each pixel was represented by a single visible neuron. When an input was presented to the network, the output of a visible neuron was set to 1 with probability as given by the scaled gray-scale value of the corresponding pixel.</p>
</sec>
<sec id="sec018">
<title>Learning procedure</title>
<p>In each parameter update step the contrastive divergence algorithm of [<xref ref-type="bibr" rid="pcbi.1004485.ref027">27</xref>] was used to estimate the likelihood gradients. Therefore, each update step consisted of a “wake” phase, a “reconstruction” phase, and the update of the parameters. The “wake” samples were generated by setting the outputs of the visible neurons to the values of a randomly chosen digit <bold><italic>x</italic></bold><sup><italic>n</italic></sup> from the training set and drawing the outputs <inline-formula id="pcbi.1004485.e044"><alternatives><graphic id="pcbi.1004485.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e044"/><mml:math id="M44" display="inline" overflow="scroll"><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> of all hidden layer neurons for the given visible output. The “reconstruction” activities <inline-formula id="pcbi.1004485.e045"><alternatives><graphic id="pcbi.1004485.e045g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e045"/><mml:math id="M45" display="inline" overflow="scroll"><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004485.e046"><alternatives><graphic id="pcbi.1004485.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e046"/><mml:math id="M46" display="inline" overflow="scroll"><mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> were generated by starting from this state of the hidden neurons and then drawing outputs of all visible neurons. After that, the hidden neurons were again updated and so on. In this way we performed five cycles of alternating visible and hidden neuron updates. The outputs of the network neurons after the fifth cycle were taken as the resulting “reconstruction” samples <inline-formula id="pcbi.1004485.e047"><alternatives><graphic id="pcbi.1004485.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e047"/><mml:math id="M47" display="inline" overflow="scroll"><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004485.e048"><alternatives><graphic id="pcbi.1004485.e048g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e048"/><mml:math id="M48" display="inline" overflow="scroll"><mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and used for the parameter updates Eqs (<xref ref-type="disp-formula" rid="pcbi.1004485.e052">22</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1004485.e055">24</xref>) given below. This update of parameters concluded one update step.</p>
<p>Log likelihood derivatives for the biases <inline-formula id="pcbi.1004485.e049"><alternatives><graphic id="pcbi.1004485.e049g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e049"/><mml:math id="M49" display="inline" overflow="scroll"><mml:msubsup><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mtext>hid</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> of hidden neurons are approximated in the contrastive divergence algorithm [<xref ref-type="bibr" rid="pcbi.1004485.ref027">27</xref>] as <inline-formula id="pcbi.1004485.e050"><alternatives><graphic id="pcbi.1004485.e050g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e050"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mtext>hid</mml:mtext></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup><mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (the derivatives for visible biases <inline-formula id="pcbi.1004485.e051"><alternatives><graphic id="pcbi.1004485.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e051"/><mml:math id="M51" display="inline" overflow="scroll"><mml:msubsup><mml:mi>b</mml:mi> <mml:mi>j</mml:mi> <mml:mtext>vis</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula> are analogous). Using <xref ref-type="disp-formula" rid="pcbi.1004485.e012">Eq (7)</xref>, the synaptic sampling update rules for the biases are thus given by
<disp-formula id="pcbi.1004485.e052"><alternatives><graphic id="pcbi.1004485.e052g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e052"/><mml:math id="M52" display="block" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mtext>hid</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="0.166667em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>η</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
<disp-formula id="pcbi.1004485.e053"><alternatives><graphic id="pcbi.1004485.e053g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e053"/><mml:math id="M53" display="block" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>j</mml:mi> <mml:mtext>vis</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="0.166667em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>η</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>ν</mml:mi> <mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
Note that the parameter prior does not show up in these equations since no priors were used for the biases in our experiments. Contrastive divergence approximates the log likelihood derivatives for the weights <italic>w</italic><sub><italic>ij</italic></sub> as <inline-formula id="pcbi.1004485.e054"><alternatives><graphic id="pcbi.1004485.e054g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e054"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. This leads to the synaptic sampling rule
<disp-formula id="pcbi.1004485.e055"><alternatives><graphic id="pcbi.1004485.e055g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e055"/><mml:math id="M55" display="block" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>η</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>N</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="0.166667em"/><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>η</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:msubsup><mml:mi>ν</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>t</mml:mi></mml:msubsup> <mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
In the simulations, we used this rule with <italic>η</italic> = 10<sup>−4</sup> and <italic>N</italic> = 100. Learning started from random initial parameters drawn from a Gaussian distribution with standard deviation 0.25 and means at 0 and -1 for weights <italic>w</italic><sub><italic>ij</italic></sub> and biases (<inline-formula id="pcbi.1004485.e056"><alternatives><graphic id="pcbi.1004485.e056g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e056"/><mml:math id="M56" display="inline" overflow="scroll"><mml:msubsup><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mtext>hid</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004485.e057"><alternatives><graphic id="pcbi.1004485.e057g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e057"/><mml:math id="M57" display="inline" overflow="scroll"><mml:msubsup><mml:mi>b</mml:mi> <mml:mi>j</mml:mi> <mml:mtext>vis</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>), respectively.</p>
<p>To compare learning with and without parameter priors, we performed simulations with an uninformative (i.e., uniform) prior on weights (<xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2C and 2D</xref>), which was implemented by setting <inline-formula id="pcbi.1004485.e058"><alternatives><graphic id="pcbi.1004485.e058g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e058"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> to zero. In simulations with a parameter prior (<xref ref-type="fig" rid="pcbi.1004485.g002">Fig 2E and 2F</xref>), we used a local prior for each weight in order to obtain local plasticity rules. In other words, the prior <italic>p</italic><sub>𝒮</sub>(<bold>w</bold>) was assumed to factorize into priors for individual weights <italic>p</italic><sub>𝒮</sub>(<bold>w</bold>) = ∏<sub><italic>i</italic>, <italic>j</italic></sub> <italic>p</italic><sub>𝒮</sub>(<italic>w</italic><sub><italic>ij</italic></sub>). For each individual weight prior, we used a bimodal distribution implemented by a mixture of two Gaussians
<disp-formula id="pcbi.1004485.e059"><alternatives><graphic id="pcbi.1004485.e059g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e059"/><mml:math id="M59" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:mspace width="0.277778em"/><mml:mtext>NORMAL</mml:mtext> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mspace width="0.277778em"/><mml:mo>|</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:mspace width="0.166667em"/><mml:mtext>NORMAL</mml:mtext> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mspace width="0.277778em"/><mml:mo>|</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
with means <italic>μ</italic><sub>1</sub> = 1.0, <italic>μ</italic><sub>2</sub> = 0.0, and standard deviations <italic>σ</italic><sub>1</sub> = <italic>σ</italic><sub>2</sub> = 0.15.</p>
</sec>
</sec>
<sec id="sec019">
<title>Details to <italic>Fast adaptation to changing input statistics</italic></title>
<sec id="sec020">
<title>Spike-based Winner-Take-All network model</title>
<p>Network neurons were modeled as stochastic spike response neurons with a firing rate that depends exponentially on the membrane voltage [<xref ref-type="bibr" rid="pcbi.1004485.ref073">73</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref074">74</xref>]. The membrane potential <italic>u</italic><sub><italic>k</italic></sub>(<italic>t</italic>) of neuron <italic>k</italic> is given by
<disp-formula id="pcbi.1004485.e060"><alternatives><graphic id="pcbi.1004485.e060g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e060"/><mml:math id="M60" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:msub><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) denotes the (unweighted) input from input neuron <italic>i</italic>, <italic>w</italic><sub><italic>ki</italic></sub> denotes the efficacy of the synapse from input neuron <italic>i</italic>, and <italic>β</italic><sub><italic>k</italic></sub>(<italic>t</italic>) denotes a homeostatic adaptation current (see below). The input <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) models the influence of additive excitatory postsynaptic potentials (EPSPs) on the membrane potential of the neuron. Let <inline-formula id="pcbi.1004485.e061"><alternatives><graphic id="pcbi.1004485.e061g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e061"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denote the spike times of input neuron <italic>i</italic>. Then, <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is given by
<disp-formula id="pcbi.1004485.e062"><alternatives><graphic id="pcbi.1004485.e062g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e062"/><mml:math id="M62" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>f</mml:mi></mml:munder> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
where <italic>ϵ</italic> is the response kernel for synaptic input, i.e., the shape of the EPSP, that had a double-exponential form in our simulations:
<disp-formula id="pcbi.1004485.e063"><alternatives><graphic id="pcbi.1004485.e063g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e063"/><mml:math id="M63" display="block" overflow="scroll"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>f</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>s</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:msup> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula>
with the rise-time constant <italic>τ</italic><sub><italic>r</italic></sub> = 2 ms, the fall-time constant <italic>τ</italic><sub><italic>f</italic></sub> = 20 ms. Θ(⋅) denotes the Heaviside step function. The instantaneous firing rate <italic>ρ</italic><sub><italic>k</italic></sub>(<italic>t</italic>) of network neuron <italic>k</italic> depends exponentially on the membrane potential and is subject to divisive lateral inhibition <italic>I</italic><sub>lat</sub>(<italic>t</italic>) (described below):
<disp-formula id="pcbi.1004485.e064"><alternatives><graphic id="pcbi.1004485.e064g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e064"/><mml:math id="M64" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>ρ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula>
where <italic>ρ</italic><sub>net</sub> = 100 Hz scales the firing rate of neurons. Such exponential relationship between the membrane potential and the firing rate has been proposed as a good approximation to the firing properties of cortical pyramidal neurons [<xref ref-type="bibr" rid="pcbi.1004485.ref073">73</xref>]. Spike trains were then drawn from independent Poisson processes with instantaneous rate <italic>ρ</italic><sub><italic>k</italic></sub>(<italic>t</italic>) for each neuron. We denote the resulting spike train of the <italic>k</italic><sup>th</sup> neuron by <italic>S</italic><sub><italic>k</italic></sub>(<italic>t</italic>).</p>
</sec>
<sec id="sec021">
<title>Homeostatic adaptation current</title>
<p>Each output spike caused a slow depressing current, giving rise to the adaptation current <italic>β</italic><sub><italic>k</italic></sub>(<italic>t</italic>). This implements a slow homeostatic mechanism that regulates the output rate of individual neurons (see [<xref ref-type="bibr" rid="pcbi.1004485.ref075">75</xref>] for details). It was implemented as
<disp-formula id="pcbi.1004485.e065"><alternatives><graphic id="pcbi.1004485.e065g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e065"/><mml:math id="M65" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>=</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>γ</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>f</mml:mi></mml:munder> <mml:mi>κ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula>
where <inline-formula id="pcbi.1004485.e066"><alternatives><graphic id="pcbi.1004485.e066g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e066"/><mml:math id="M66" display="inline" overflow="scroll"><mml:msubsup><mml:mi>t</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> denotes the <italic>f</italic>-th spike of neuron <italic>k</italic> and <italic>κ</italic> is an adaptation kernel that was modeled as a double exponential (<xref ref-type="disp-formula" rid="pcbi.1004485.e063">Eq (28)</xref>) with time constants <italic>τ</italic><sub><italic>r</italic></sub> = 12 s and <italic>τ</italic><sub><italic>f</italic></sub> = 30 s. The scaling parameter <italic>γ</italic> was set to <italic>γ</italic> = -8.</p>
</sec>
<sec id="sec022">
<title>Lateral inhibition</title>
<p>Divisive inhibition [<xref ref-type="bibr" rid="pcbi.1004485.ref039">39</xref>] between the <italic>K</italic> neurons in the WTA network was implemented in an idealized form [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>]
<disp-formula id="pcbi.1004485.e067"><alternatives><graphic id="pcbi.1004485.e067g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e067"/><mml:math id="M67" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>lat</mml:mtext></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(31)</label></disp-formula>
This form of lateral inhibition, that assumes an idealized access to neuronal membrane potentials, has been shown to implement a well-defined generative network model [<xref ref-type="bibr" rid="pcbi.1004485.ref036">36</xref>], see below.</p>
</sec>
<sec id="sec023">
<title>Synaptic sampling in spike-based Winner-Take-All networks as stochastic STDP</title>
<p>It has been shown in [<xref ref-type="bibr" rid="pcbi.1004485.ref037">37</xref>] that the WTA-network defined above implicitly defines a generative model that is a mixture of Poissonian distributions. In this generative model, inputs <bold><italic>x</italic></bold><sup><italic>n</italic></sup> are assumed to be generated in dependence on the value of a hidden multinomial random variable <italic>h</italic><sup><italic>n</italic></sup> that can take on <italic>K</italic> possible values 1, …, <italic>K</italic>. Each neuron <italic>k</italic> in the WTA circuit corresponds to one value <italic>k</italic> of this hidden variable. In the generative model, for a given value of <italic>h</italic><sup><italic>n</italic></sup> = <italic>k</italic>, the value of an input <inline-formula id="pcbi.1004485.e068"><alternatives><graphic id="pcbi.1004485.e068g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e068"/><mml:math id="M68" display="inline" overflow="scroll"><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is then distributed according to a Poisson distribution with a mean that is determined by the synaptic weight <italic>w</italic><sub><italic>ki</italic></sub> from input neuron <italic>i</italic> to network neuron <italic>k</italic>:
<disp-formula id="pcbi.1004485.e069"><alternatives><graphic id="pcbi.1004485.e069g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e069"/><mml:math id="M69" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>h</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mspace width="1pt"/><mml:mtext>Poisson</mml:mtext> <mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:mi>α</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(32)</label></disp-formula>
with a scaling parameter <italic>α</italic> &gt; 0. In other words, the synaptic weight <italic>w</italic><sub><italic>ki</italic></sub> encodes (in log-space) the firing rate of input neuron <italic>i</italic>, given that the hidden cause is <italic>k</italic>. For a given hidden cause, inputs are assumed to be independent, hence one obtains the probability of an input vector for a given hidden cause as
<disp-formula id="pcbi.1004485.e070"><alternatives><graphic id="pcbi.1004485.e070g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e070"/><mml:math id="M70" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>h</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mtext>Poisson</mml:mtext> <mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:mi>α</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(33)</label></disp-formula>
The network implements inference in this generative model, i.e., for a given input <bold><italic>x</italic></bold><sup><italic>n</italic></sup>, the firing rate of network neuron <italic>z</italic><sub><italic>k</italic></sub> is proportional to the posterior probability <italic>p</italic>(<italic>h</italic><sup><italic>n</italic></sup> = <italic>k</italic>∣<bold><italic>x</italic></bold><sup><italic>n</italic></sup>, <bold><italic>w</italic></bold>) of the corresponding hidden cause. An online maximum likelihood learning rule for this generative model was derived in [<xref ref-type="bibr" rid="pcbi.1004485.ref037">37</xref>]. It changes synaptic weights according to
<disp-formula id="pcbi.1004485.e071"><alternatives><graphic id="pcbi.1004485.e071g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e071"/><mml:math id="M71" display="block" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒩</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow> <mml:mi>n</mml:mi></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mspace width="0.166667em"/><mml:mi mathvariant="bold-italic">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(34)</label></disp-formula>
where <italic>S</italic><sub><italic>k</italic></sub>(<italic>t</italic>) denotes the spike train of the postsynaptic neuron and <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) denotes the weight-normalized value of the sum of EPSPs from presynaptic neuron <italic>i</italic> at time <italic>t</italic> (i.e., the summed EPSPs that would arise for weight <italic>w</italic><sub><italic>ki</italic></sub> = 1). To define the synaptic sampling learning rule completely, we also need to define the parameter prior. In our experiments, we used a simple Gaussian prior on each parameter <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>) = ∏<sub><italic>k,i</italic></sub> N<sc>ormal</sc> (<italic>θ</italic><sub><italic>ki</italic></sub>|<italic>μ</italic>, σ<sup>2</sup>) with <italic>μ</italic> = 0.5 and <italic>σ</italic> = 1. The derivative of the log-prior is given by
<disp-formula id="pcbi.1004485.e072"><alternatives><graphic id="pcbi.1004485.e072g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e072"/><mml:math id="M72" display="block" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mo>𝒮</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(35)</label></disp-formula>
Inserting Eqs (<xref ref-type="disp-formula" rid="pcbi.1004485.e071">34</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004485.e072">35</xref>) into the general form <xref ref-type="disp-formula" rid="pcbi.1004485.e024">Eq (10)</xref>, we find that the synaptic sampling rule is given by
<disp-formula id="pcbi.1004485.e073"><alternatives><graphic id="pcbi.1004485.e073g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e073"/><mml:math id="M73" display="block" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>=</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>b</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:mspace width="0.277778em"/><mml:mi>N</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>S</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mspace width="1pt"/><mml:mi>d</mml:mi> <mml:mi>t</mml:mi> <mml:mspace width="0.277778em"/><mml:mo>+</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>b</mml:mi></mml:mrow></mml:msqrt> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:msub><mml:mi mathvariant="script">W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(36)</label></disp-formula>
which corresponds to rule <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref> with double indices <italic>ki</italic> replaced by single parameter indexing <italic>i</italic> to simplify notation.</p>
</sec>
<sec id="sec024">
<title>Simulation details for spiking network simulations</title>
<p>Computer simulations of spiking neural networks (Figs <xref ref-type="fig" rid="pcbi.1004485.g003">3</xref>, <xref ref-type="fig" rid="pcbi.1004485.g004">4</xref> and <xref ref-type="fig" rid="pcbi.1004485.g005">5</xref>) were based on adapted event-based simulation software from [<xref ref-type="bibr" rid="pcbi.1004485.ref038">38</xref>]. In all spiking neural network simulations, synaptic weights were updated according to the rule <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref> with parameters <italic>N</italic> = 100, <italic>α</italic> = <italic>e</italic><sup>−2</sup>, and <italic>b</italic> = 10<sup>−4</sup>, except for panel <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3F</xref> where <italic>b</italic> = 10<sup>−6</sup> was used as a control. In the simulations, we directly implemented the time-continuous evolution of the network parameters in an event-based update scheme. Before learning, initial synaptic parameters were independently drawn from the prior distribution <italic>p</italic><sub>𝒮</sub>(<bold><italic>θ</italic></bold>).</p>
<p>For the mapping <xref ref-type="disp-formula" rid="pcbi.1004485.e023">Eq (9)</xref> from synaptic parameters <italic>θ</italic><sub><italic>ki</italic></sub> to synaptic efficacies <italic>w</italic><sub><italic>ki</italic></sub>, we used as offset <italic>θ</italic><sub>0</sub> = 3. This results in synaptic weights that shrink to small values (&lt; 0.05) when synaptic parameters are below zero. In the simulation, we clipped the synaptic weights to zero for negative synaptic parameters <italic>θ</italic> to account for retracted synapses. More precisely, the actual weights <inline-formula id="pcbi.1004485.e074"><alternatives><graphic id="pcbi.1004485.e074g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e074"/><mml:math id="M74" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> used for the computation of the membrane potential <xref ref-type="disp-formula" rid="pcbi.1004485.e060">Eq (26)</xref> were given by <inline-formula id="pcbi.1004485.e075"><alternatives><graphic id="pcbi.1004485.e075g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004485.e075"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>=</mml:mo> <mml:mspace width="0.166667em"/><mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mo>{</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.166667em"/><mml:mo>-</mml:mo> <mml:mspace width="0.166667em"/><mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo> <mml:mspace width="0.277778em"/></mml:mrow></mml:math></alternatives></inline-formula>. To avoid numerical problems, we clipped the synaptic parameters at -5 and the maximum amplitude of instantaneous parameter changes to 5<italic>b</italic>.</p>
</sec>
<sec id="sec025">
<title>Network inputs</title>
<p>The spatiotemporal spike patterns in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4</xref> are realizations of Poisson spike trains, each representing a certain point in the 3-dimensional sensory environment (a unit cube). Each of the 1000 input neurons was assigned to a Gaussian tuning curve with <italic>σ</italic> = 0.3. Tuning curve centers were independently and equally scattered over the unit cube. For each sensory experience the firing rate of an individual input neuron was given by the support of sensory experience under the neuron’s tuning curve (normalized between 0 Hz and 80 Hz). In addition an offset of 5 Hz background noise was added. The patterns had a duration of 200 ms. During that time the firing rates of input neurons were kept fixed and independent Poisson spike trains were drawn.</p>
<p>The two environments (SE and EE) in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4</xref> were realized by Gaussian mixture models. The means of the Gaussians were randomly placed around the center of the unit cube (each component was independently drawn from N<sc>ormal</sc>(0.5, 0.2)). The covariance matrices of the Gaussian cluster centers were randomly given by 0.04𝕀 + 0.01<bold><italic>ξ</italic></bold>, where 𝕀 is the 3-dimensional identity matrix and <italic>ξ</italic> is a matrix of randomly drawn values from N<sc>ormal</sc>(0, 1). Sensory experiences were generated by randomly selecting one Gaussian cluster (with equal probability) and then drawing a sample position from the corresponding multivariate Gaussian.</p>
</sec>
<sec id="sec026">
<title>Learning schedule and data analysis</title>
<p>The network was first exposed to samples from the standard environment (SE, <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4D</xref>) for 3 hours (54000 input sample presentations). In the second learning phase input samples from the enriched environment (EE, <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4E</xref>) were given for 1 hour (18000 samples). In the third phase samples from either SE (EE-SE condition) or EE (EE-EE condition) were presented for additional 5 hours (90000 samples, the two cases are compared in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4G</xref>).</p>
<p>Formation rates of synaptic connections shown in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4F</xref> represent the number of spines that were formed during a time window of Δ<italic>t</italic> = 30 minutes, i.e. the number of synaptic connections that were not present (<italic>θ</italic><sub><italic>i</italic></sub> ≤ 0) at time <italic>t</italic> − Δ<italic>t</italic> but at time <italic>t</italic>. The SE condition in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4F</xref> was evaluated at the end of learning phase 1, the EE condition was evaluated at the beginning of EE exposure.</p>
<p>For the survival plots in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4G</xref> the newly formed synaptic connections at the end of the EE condition were taken into account (see above). Networks from the EE-EE or EE-SE condition were compared. The presence of synaptic connections (<italic>θ</italic><sub><italic>i</italic></sub> &gt; 0) was evaluated in intervals of 30 minutes. The plot in <xref ref-type="fig" rid="pcbi.1004485.g004">Fig 4E and 4F</xref> show mean values and standard deviations over 5 individual trial runs.</p>
</sec>
</sec>
<sec id="sec027">
<title>Details to <italic>Inherent compensation capabilities of networks with synaptic sampling</italic></title>
<p>Here we provide details to the network model and spiking inputs for the recurrent WTA circuits (<xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref>). Additional details to the data analysis and performance evaluation are provided in <xref ref-type="supplementary-material" rid="pcbi.1004485.s006">S6 Text</xref>.</p>
<sec id="sec028">
<title>Network model</title>
<p>In <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref> two recurrently connected ensembles, each consisting of four WTA circuits, were used. The parameters of neuron and synapse dynamics were as described in the previous section. All synapses, lateral and feedforward, were subject to the same learning rule <xref ref-type="disp-formula" rid="pcbi.1004485.e027">Eq (11)</xref>. Lateral connections within and between the WTA Circuit neurons were unconstrained (allowing potentially all-to-all connectivity). Connections from input neurons were constraint as shown in <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref>. The lateral synapses were treated in the same way as synapses from input neurons but had a synaptic delay of 5 ms.</p>
</sec>
<sec id="sec029">
<title>Network inputs</title>
<p>Handwritten digit images for <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref> were taken from the MNIST dataset [<xref ref-type="bibr" rid="pcbi.1004485.ref072">72</xref>]. Each pixel was represented by a single afferent neuron. Gray scale values where scaled to 0–50 Hz Poisson input rate and 1 Hz input noise was added on top. These Poisson rates were kept fixed for each example input digit for the duration of the input presentation.</p>
<p>The spoken digit presentations in <xref ref-type="fig" rid="pcbi.1004485.g005">Fig 5</xref> were given by reconstructed cochleagrams of speech samples of isolated spoken digits from the TI 46 dataset (also used in [<xref ref-type="bibr" rid="pcbi.1004485.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1004485.ref076">76</xref>]). Each of the 77 channels of the cochleagrams was represented by 10 afferent neurons, giving a total of 770. Cochleagrams were normalized between 0 Hz and 80 Hz and used to draw individual Poisson spike trains for each afferent neuron. In addition 1 Hz Poisson noise was added on top. We used 10 different utterances of digits <italic>1</italic> and <italic>2</italic> of a single speaker. We selected 7 utterances for training and 3 for testing. For training, one randomly selected utterance from the training set was presented together with a randomly chosen instance of the corresponding handwritten digit. The spike patterns for the written digits (see above) had the same duration as the spoken digits. Each digit presentation was padded with 25 ms, 1 Hz Poisson noise before and after the digit pattern.</p>
<p>For test trials in which only the auditory stimulus was presented, the activity of the visual input neurons was set to 1 Hz throughout the whole pattern presentation. The learning rate <italic>b</italic> was set to zero during these trials. The PETH plots were computed over 100 trial responses of the network to the same stimulus class (e.g. presentation of digit <italic>1</italic>). Spike patterns for input stimuli were randomly drawn in each trial for the given rates. Spike trains were then filtered with a Gaussian filter with <italic>σ</italic> = 50 ms and summed in a time-discrete matrix with 10 ms bin length. Maximum firing times were assigned to the time bin with the highest PETH amplitude for each neuron.</p>
</sec>
</sec>
</sec>
<sec id="sec030">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004485.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Proof of Theorem 1.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004485.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s002" mimetype="application/pdf" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Supporting information to <xref ref-type="fig" rid="pcbi.1004485.g001">Fig 1</xref>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004485.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s003" mimetype="application/pdf" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Supporting information to <italic>Synaptic sampling improves the generalization capability of a neural network</italic>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004485.s004" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s004" mimetype="application/pdf" xlink:type="simple">
<label>S4 Text</label>
<caption>
<title>Supporting information to <italic>Spine motility as synaptic sampling</italic>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004485.s005" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s005" mimetype="application/pdf" xlink:type="simple">
<label>S5 Text</label>
<caption>
<title>Supporting information to <xref ref-type="fig" rid="pcbi.1004485.g003">Fig 3</xref>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004485.s006" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s006" mimetype="application/pdf" xlink:type="simple">
<label>S6 Text</label>
<caption>
<title>Supporting information to <italic>Inherent network compensation capability through synaptic sampling</italic>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004485.s007" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004485.s007.tar" mimetype="application/x-gzip" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>Source Code.</title>
<p>(TAR.GZ)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Seth Grant, Christopher Harvey, Jason MacLean and Simon Rumpel for helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004485.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Holtmaat</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>Experience-dependent structural synaptic plasticity in the mammalian brain</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2009</year>;<volume>10</volume>(<issue>9</issue>):<fpage>647</fpage>–<lpage>658</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2699" xlink:type="simple">10.1038/nrn2699</ext-link></comment> <object-id pub-id-type="pmid">19693029</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yasumatsu</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Matsuzaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Miyazaki</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Noguchi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kasai</surname> <given-names>H</given-names></name>. <article-title>Principles of long-term dynamics of dendritic spines</article-title>. <source>The Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>50</issue>):<fpage>13592</fpage>–<lpage>13608</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0603-08.2008" xlink:type="simple">10.1523/JNEUROSCI.0603-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19074033</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Stettler</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Yamahachi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>. <article-title>Axons and synaptic boutons are highly dynamic in adult visual cortex</article-title>. <source>Neuron</source>. <year>2006</year>;<volume>49</volume>:<fpage>877</fpage>–<lpage>887</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.02.018" xlink:type="simple">10.1016/j.neuron.2006.02.018</ext-link></comment> <object-id pub-id-type="pmid">16543135</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rokni</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Richardson</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Bizzi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Motor learning with unstable neural representations</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>54</volume>(<issue>4</issue>):<fpage>653</fpage>–<lpage>666</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.04.030" xlink:type="simple">10.1016/j.neuron.2007.04.030</ext-link></comment> <object-id pub-id-type="pmid">17521576</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yamahachi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Marik</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>McManus</surname> <given-names>JNJ</given-names></name>, <name name-style="western"><surname>Denk</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>CD</given-names></name>. <article-title>Rapid axonal sprouting and pruning accompany functional reorganization in primary visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>64</volume>(<issue>5</issue>):<fpage>719</fpage>–<lpage>729</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.11.026" xlink:type="simple">10.1016/j.neuron.2009.11.026</ext-link></comment> <object-id pub-id-type="pmid">20005827</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>MacKay</surname> <given-names>DJ</given-names></name>. <article-title>Bayesian interpolation</article-title>. <source>Neural Computation</source>. <year>1992</year>;<volume>4</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>447</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1992.4.3.415" xlink:type="simple">10.1162/neco.1992.4.3.415</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>9</issue>):<fpage>1170</fpage>–<lpage>1178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3495" xlink:type="simple">10.1038/nn.3495</ext-link></comment> <object-id pub-id-type="pmid">23955561</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Variability, compensation and modulation in neurons and circuits</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>3</issue>):<fpage>15542</fpage>–<lpage>15548</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1010674108" xlink:type="simple">10.1073/pnas.1010674108</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>May</surname> <given-names>A</given-names></name>. <article-title>Experience-dependent structural plasticity in the adult human brain</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2011</year>;<volume>15</volume>(<issue>10</issue>):<fpage>475</fpage>–<lpage>482</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2011.08.002" xlink:type="simple">10.1016/j.tics.2011.08.002</ext-link></comment> <object-id pub-id-type="pmid">21906988</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Caroni</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Donato</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>D</given-names></name>. <article-title>Structural plasticity upon learning: regulation and functions</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2012</year>;<volume>13</volume>(<issue>7</issue>):<fpage>478</fpage>–<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3258" xlink:type="simple">10.1038/nrn3258</ext-link></comment> <object-id pub-id-type="pmid">22714019</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Hatfield</surname> <given-names>G</given-names></name>. <chapter-title>Perception as Unconscious Inference</chapter-title>. In: <source>Perception and the Physical World: Psychological and Philosophical Issues in Perception</source>. <publisher-name>John Wiley &amp; Sons, Ltd</publisher-name>; <year>2002</year>. p. <fpage>115</fpage>–<lpage>143</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <source>Probabilistic Models of the Brain</source>. <publisher-name>MIT Press</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>. <source>Bayesian Brain: Probabilistic Approaches to Neural Coding</source>. <publisher-name>MIT-Press</publisher-name>; <year>2007</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Winkler</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Denham</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mill</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Böhm</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Bendixen</surname> <given-names>A</given-names></name>. <article-title>Multistability in auditory stream segregation: a predictive coding view</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>2012</year>;<volume>367</volume>(<issue>1591</issue>):<fpage>1001</fpage>–<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2011.0359" xlink:type="simple">10.1098/rstb.2011.0359</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Gardiner</surname> <given-names>CW</given-names></name>. <source>Handbook of Stochastic Methods</source>. <edition>3rd ed</edition>. <publisher-name>Springer</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Coba</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Pocklington</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Kopanitsa</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Uren</surname> <given-names>RT</given-names></name>, <name name-style="western"><surname>Swamy</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Neurotransmitters Drive Combinatorial Multistate Postsynaptic Density Networks</article-title>. <source>Science Signaling</source>. <year>2009</year>;<volume>2</volume>(<issue>68</issue>):<fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/scisignal.2000102" xlink:type="simple">10.1126/scisignal.2000102</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ribrault</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sekimoto</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Triller</surname> <given-names>A</given-names></name>. <article-title>From the stochasticity of molecular processes to the variability of synaptic transmission</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2011</year>;<volume>12</volume>(<issue>7</issue>):<fpage>375</fpage>–<lpage>387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3025" xlink:type="simple">10.1038/nrn3025</ext-link></comment> <object-id pub-id-type="pmid">21685931</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gray</surname> <given-names>NW</given-names></name>, <name name-style="western"><surname>Weimer</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Bureau</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>Rapid redistribution of synaptic PSD-95 in the neocortex in vivo</article-title>. <source>PLoS Biology</source>. <year>2006</year>;<volume>4</volume>(<issue>11</issue>):<fpage>e370</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0040370" xlink:type="simple">10.1371/journal.pbio.0040370</ext-link></comment> <object-id pub-id-type="pmid">17090216</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Engert</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bonhoeffer</surname> <given-names>T</given-names></name>. <article-title>Dendritic spine changes associated with hippocampal long-term synaptic plasticity</article-title>. <source>Nature</source>. <year>1999</year>;<volume>399</volume>(<issue>6731</issue>):<fpage>66</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/19978" xlink:type="simple">10.1038/19978</ext-link></comment> <object-id pub-id-type="pmid">10331391</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ho</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>KC</given-names></name>. <article-title>The cell biology of synaptic plasticity</article-title>. <source>Science</source>. <year>2011</year>;<volume>334</volume>(<issue>6056</issue>):<fpage>623</fpage>–<lpage>628</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1209236" xlink:type="simple">10.1126/science.1209236</ext-link></comment> <object-id pub-id-type="pmid">22053042</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bhalla</surname> <given-names>US</given-names></name>, <name name-style="western"><surname>Iyengar</surname> <given-names>R</given-names></name>. <article-title>Emergent properties of networks of biological signaling pathways</article-title>. <source>Science</source>. <year>1999</year>;<volume>283</volume>(<issue>5400</issue>):<fpage>381</fpage>–<lpage>387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.283.5400.381" xlink:type="simple">10.1126/science.283.5400.381</ext-link></comment> <object-id pub-id-type="pmid">9888852</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="other">Welling M, Teh YW. Bayesian learning via stochastic gradient Langevin dynamics. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11); 2011. p. 681–688.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ackley</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>A Learning Algorithm for Boltzmann Machines</article-title>. <source>Cognitive Science</source>. <year>1985</year>;<volume>9</volume>:<fpage>147</fpage>–<lpage>169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1207/s15516709cog0901_7" xlink:type="simple">10.1207/s15516709cog0901_7</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Osindero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name>. <article-title>A fast learning algorithm for deep belief nets</article-title>. <source>Neural Computation</source>. <year>2006</year>;<volume>18</volume>(<issue>7</issue>):<fpage>1527</fpage>–<lpage>1554</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.7.1527" xlink:type="simple">10.1162/neco.2006.18.7.1527</ext-link></comment> <object-id pub-id-type="pmid">16764513</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Salakhutdinov</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>An Efficient Learning Procedure for Deep Boltzmann Machines</article-title>. <source>Neural Computation</source>. <year>2012</year>;<volume>24</volume>:<fpage>1967</fpage>–<lpage>2006</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00311" xlink:type="simple">10.1162/NECO_a_00311</ext-link></comment> <object-id pub-id-type="pmid">22509963</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>Training products of experts by minimizing contrastive divergence</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>8</issue>):<fpage>1771</fpage>–<lpage>1800</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602760128018" xlink:type="simple">10.1162/089976602760128018</ext-link></comment> <object-id pub-id-type="pmid">12180402</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Löwenstein</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kuras</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rumpel</surname> <given-names>S</given-names></name>. <article-title>Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo</article-title>. <source>The Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>26</issue>):<fpage>9481</fpage>–<lpage>9488</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6130-10.2011" xlink:type="simple">10.1523/JNEUROSCI.6130-10.2011</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Trachtenberg</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Knott</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Feng</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sanes</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Welker</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Long-term in vivo imaging of experience-dependent synaptic plasticity in adult cortex</article-title>. <source>Nature</source>. <year>2002</year>;<volume>420</volume>(<issue>6917</issue>):<fpage>788</fpage>–<lpage>794</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature01273" xlink:type="simple">10.1038/nature01273</ext-link></comment> <object-id pub-id-type="pmid">12490942</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Zuo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gan</surname> <given-names>WB</given-names></name>. <article-title>Development of long-term dendritic spine stability in diverse regions of cerebral cortex</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.04.001" xlink:type="simple">10.1016/j.neuron.2005.04.001</ext-link></comment> <object-id pub-id-type="pmid">15848798</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Holtmaat</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Trachtenberg</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Wilbrecht</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Shepherd</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Knott</surname> <given-names>GW</given-names></name>, <etal>et al</etal>. <article-title>Transient and Persistent Dendritic Spines in the Neocortex In Vivo</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>45</volume>:<fpage>279</fpage>–<lpage>291</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.01.003" xlink:type="simple">10.1016/j.neuron.2005.01.003</ext-link></comment> <object-id pub-id-type="pmid">15664179</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Löwenstein</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yanover</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Rumpel</surname> <given-names>S</given-names></name>. <article-title>Predicting the dynamics of network connectivity in the neocortex</article-title>. <source>The Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>36</issue>):<fpage>12535</fpage>–<lpage>1</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2917-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2917-14.2015</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Yang</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Pan</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gan</surname> <given-names>WB</given-names></name>. <article-title>Stably maintained dendritic spines are associated with lifelong memories</article-title>. <source>Nature</source>. <year>2009</year>;<volume>462</volume>(<issue>7275</issue>):<fpage>920</fpage>–<lpage>924</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature08577" xlink:type="simple">10.1038/nature08577</ext-link></comment> <object-id pub-id-type="pmid">19946265</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brea</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>. <article-title>Matching Recall and Storage in Sequence Learning with Spiking Neural Networks</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>23</issue>):<fpage>9565</fpage>–<lpage>9575</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4098-12.2013" xlink:type="simple">10.1523/JNEUROSCI.4098-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23739954</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Boerlin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Denéve</surname> <given-names>S</given-names></name>. <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year>;<volume>9</volume>(<issue>11</issue>):<fpage>e1003258</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003258" xlink:type="simple">10.1371/journal.pcbi.1003258</ext-link></comment> <object-id pub-id-type="pmid">24244113</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year>;<volume>9</volume>(<issue>4</issue>):<fpage>e1003037</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003037" xlink:type="simple">10.1371/journal.pcbi.1003037</ext-link></comment> <object-id pub-id-type="pmid">23633941</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Habenschuss</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Puhr</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Emergence of optimal decoding of population codes through STDP</article-title>. <source>Neural Computation</source>. <year>2013</year>;<volume>25</volume>:<fpage>1</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00446" xlink:type="simple">10.1162/NECO_a_00446</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kappel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>STDP Installs in Winner-Take-All Circuits an Online Approximation to Hidden Markov Model Learning</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year>;<volume>10</volume>(<issue>3</issue>):<fpage>e1003511</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003511" xlink:type="simple">10.1371/journal.pcbi.1003511</ext-link></comment> <object-id pub-id-type="pmid">24675787</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>. <article-title>From circuits to behavior: a bridge too far?</article-title> <source>Nature Neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>4</issue>):<fpage>507</fpage>–<lpage>509</lpage>. <object-id pub-id-type="pmid">22449960</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Klampfl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Emergence of dynamic memory traces in cortical microcircuit models through STDP</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>28</issue>):<fpage>11515</fpage>–<lpage>11529</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5044-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5044-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23843522</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SB</given-names></name>. <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>32</volume>(<issue>6</issue>):<fpage>1149</fpage>–<lpage>1164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(01)00542-6" xlink:type="simple">10.1016/S0896-6273(01)00542-6</ext-link></comment> <object-id pub-id-type="pmid">11754844</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hofer</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Bonhoeffer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hübener</surname> <given-names>M</given-names></name>. <article-title>Experience leaves a lasting structural trace in cortical circuits</article-title>. <source>Nature</source>. <year>2009</year>;<volume>457</volume>(<issue>7227</issue>):<fpage>313</fpage>–<lpage>317</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07487" xlink:type="simple">10.1038/nature07487</ext-link></comment> <object-id pub-id-type="pmid">19005470</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kuhlman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>O’Connor</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Fox</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>Structural plasticity within the barrel cortex during initial phases of whisker-dependent learning</article-title>. <source>The Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>17</issue>):<fpage>6078</fpage>–<lpage>6083</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4919-12.2014" xlink:type="simple">10.1523/JNEUROSCI.4919-12.2014</ext-link></comment> <object-id pub-id-type="pmid">24760867</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tang</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Goeritz</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Caplan</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Fisek</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Precise temperature compensation of phase in a rhythmic motor pattern</article-title>. <source>PLoS Biology</source>. <year>2010</year>;<volume>8</volume>(<issue>8</issue>):<fpage>e1000469</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1000469" xlink:type="simple">10.1371/journal.pbio.1000469</ext-link></comment> <object-id pub-id-type="pmid">20824168</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grashow</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brookings</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Compensation for variable intrinsic neuronal excitability by circuit-synaptic interactions</article-title>. <source>The Journal of Neuroscience</source>. <year>2010</year>;<volume>20</volume>(<issue>27</issue>):<fpage>9145</fpage>–<lpage>9156</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0980-10.2010" xlink:type="simple">10.1523/JNEUROSCI.0980-10.2010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>AL</given-names></name>. <article-title>Multiple models to capture the variability in biological neurons and networks</article-title>. <source>Nature Neuroscience</source>. <year>2011</year>;<volume>14</volume>(<issue>2</issue>):<fpage>133</fpage>–<lpage>138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2735" xlink:type="simple">10.1038/nn.2735</ext-link></comment> <object-id pub-id-type="pmid">21270780</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Prinz</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Bucher</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>. <article-title>Similar network activity from disparate circuit parameters</article-title>. <source>Nature Neuroscience</source>. <year>2004</year>;<volume>7</volume>(<issue>12</issue>):<fpage>1345</fpage>–<lpage>1352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1352" xlink:type="simple">10.1038/nn1352</ext-link></comment> <object-id pub-id-type="pmid">15558066</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ziv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Cocker</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Hamel</surname> <given-names>EO</given-names></name>, <name name-style="western"><surname>Ghosh</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Kitch</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Long-term dynamics of CA1 hippocampal place codes</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>3</issue>):<fpage>264</fpage>–<lpage>266</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3329" xlink:type="simple">10.1038/nn.3329</ext-link></comment> <object-id pub-id-type="pmid">23396101</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Harvey</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Coen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Choice-specific sequencis in parietal cortex during a virtual-navigation decision task</article-title>. <source>Nature</source>. <year>2012</year>;<volume>484</volume>:<fpage>62</fpage>–<lpage>68</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature10918" xlink:type="simple">10.1038/nature10918</ext-link></comment> <object-id pub-id-type="pmid">22419153</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Attardo</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fitzgerald</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Schnitzer</surname> <given-names>MJ</given-names></name>. <article-title>Impermanence of dendritic spines in live adult CA1 hippocampus</article-title>. <source>Nature</source>. <year>2015</year>;<volume>523</volume>:<fpage>592</fpage>–<lpage>596</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature14467" xlink:type="simple">10.1038/nature14467</ext-link></comment> <object-id pub-id-type="pmid">26098371</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ajemian</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dâ??Ausilio</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bizzi</surname> <given-names>E</given-names></name>. <article-title>A theory for how sensorimotor skills are learned and retained in noisy and nonstationary neural circuits</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2013</year>;<volume>110</volume>(<issue>52</issue>):<fpage>5078</fpage>–<lpage>E5087</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1320116110" xlink:type="simple">10.1073/pnas.1320116110</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Liao</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Malinow</surname> <given-names>R</given-names></name>. <article-title>Direct measurement of quantal changes underlying long-term potentiation in CA1 hippocampus</article-title>. <source>Neuron</source>. <year>1992</year>;<volume>9</volume>(<issue>6</issue>):<fpage>1089</fpage>–<lpage>1097</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0896-6273(92)90068-O" xlink:type="simple">10.1016/0896-6273(92)90068-O</ext-link></comment> <object-id pub-id-type="pmid">1334418</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bi</surname> <given-names>GQ</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>MM</given-names></name>. <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>J Neuroscience</source>. <year>1998</year>;<volume>18</volume>(<issue>24</issue>):<fpage>10464</fpage>–<lpage>10472</lpage>. <object-id pub-id-type="pmid">9852584</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Montgomery</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Pavlidis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Madison</surname> <given-names>DV</given-names></name>. <article-title>Pair recordings reveal all-silent synaptic connections and the postsynaptic expression of long-term potentiation</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>29</volume>(<issue>3</issue>):<fpage>691</fpage>–<lpage>701</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(01)00244-6" xlink:type="simple">10.1016/S0896-6273(01)00244-6</ext-link></comment> <object-id pub-id-type="pmid">11301028</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Xiong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Szedmak</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rodríguez-Sanchez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Piater</surname> <given-names>J</given-names></name>. <chapter-title>Towards sparsity and selectivity: Bayesian learning of restricted Boltzmann machine for early visual features</chapter-title>. In: <source>Artificial Neural Networks and Machine Learning ICANN 2014</source>, <publisher-name>Springer International Publishing</publisher-name>. <year>2014</year>;p. <fpage>419</fpage>–<lpage>426</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>. <article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2010</year>;<volume>14</volume>(<issue>3</issue>):<fpage>119</fpage>–<lpage>130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.01.003" xlink:type="simple">10.1016/j.tics.2010.01.003</ext-link></comment> <object-id pub-id-type="pmid">20153683</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Donnell</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Nolan</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>van Rossum</surname> <given-names>MC</given-names></name>. <article-title>Dendritic spine dynamics regulate the long-term stability of synaptic plasticity</article-title>. <source>The Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>45</issue>):<fpage>16142</fpage>–<lpage>16156</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2520-11.2011" xlink:type="simple">10.1523/JNEUROSCI.2520-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22072667</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="other">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov RR. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint. 2012;arXiv:12070580.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="other">Wan L, Zeiler M, Zhang S, Cun YL, Fergus R. Regularization of neural networks using dropconnect. In: Proceedings of the 30th International Conference on Machine Learning (ICML-13); 2013. p. 1058–1066.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Varela</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Sen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gibson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fost</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SB</given-names></name>. <article-title>A quantitative description of short-term plasticity at excitatory synapses in layer 2/3 of rat primary visual cortex</article-title>. <source>The Journal of Neuroscience</source>. <year>1997</year>;<volume>17</volume>:<fpage>220</fpage>–<lpage>224</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>. <article-title>Differential signaling via the same axon of neocortical pyramidal neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1998</year>;<volume>95</volume>:<fpage>5323</fpage>–<lpage>5328</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.95.9.5323" xlink:type="simple">10.1073/pnas.95.9.5323</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Toledo-Rodriguez</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gupta</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Silberberg</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>C</given-names></name>. <article-title>Interneurons of the neocortical inhibitory system</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2004</year>;<volume>5</volume>(<issue>10</issue>):<fpage>793</fpage>–<lpage>807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1519" xlink:type="simple">10.1038/nrn1519</ext-link></comment> <object-id pub-id-type="pmid">15378039</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref063">
<label>63</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Natschlaeger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Zador</surname> <given-names>A</given-names></name>. <article-title>Efficient Temporal Processing with Biologically Realistic Dynamic Synapses</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2001</year>;<volume>12</volume>:<fpage>75</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/net.12.1.75.87" xlink:type="simple">10.1080/net.12.1.75.87</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref064">
<label>64</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Elliott</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Shadbolt</surname> <given-names>NR</given-names></name>. <article-title>Competition for neurotrophic factors: ocular dominance columns</article-title>. <source>The Journal of Neuroscience</source>. <year>1998</year>;<volume>18</volume>(<issue>15</issue>):<fpage>5850</fpage>–<lpage>5858</lpage>. <object-id pub-id-type="pmid">9671672</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref065">
<label>65</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Elliott</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Shadbolt</surname> <given-names>N</given-names></name>. <article-title>Competition for neurotrophic factors: mathematical analysis</article-title>. <source>Neural Computation</source>. <year>1998</year>;<volume>10</volume>(<issue>8</issue>):<fpage>1939</fpage>–<lpage>1981</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976698300016927" xlink:type="simple">10.1162/089976698300016927</ext-link></comment> <object-id pub-id-type="pmid">9804667</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref066">
<label>66</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Butz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name>. <article-title>A simple rule for dendritic spine and axonal bouton formation can account for cortical reorganization after focal retinal lesions</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year>;<volume>9</volume>:<fpage>e1003259</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003259" xlink:type="simple">10.1371/journal.pcbi.1003259</ext-link></comment> <object-id pub-id-type="pmid">24130472</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref067">
<label>67</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Butz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Steenbuck</surname> <given-names>ID</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name>. <article-title>Homeostatic structural plasticity can account for topology changes following deafferentation and focal stroke</article-title>. <source>Frontiers in Neuroanatomy</source>. <year>2014</year>;<volume>8</volume>:<fpage>115</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnana.2014.00115" xlink:type="simple">10.3389/fnana.2014.00115</ext-link></comment> <object-id pub-id-type="pmid">25360087</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref068">
<label>68</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Deger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Helias</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rotter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>. <article-title>Spike-timing dependence of structural plasticity explains cooperative synapse formation in the neocortex</article-title>. <source>PLoS Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>9</issue>):<fpage>e1002689</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002689" xlink:type="simple">10.1371/journal.pcbi.1002689</ext-link></comment> <object-id pub-id-type="pmid">23028287</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref069">
<label>69</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fauth</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wörgötter</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Tetzlaff</surname> <given-names>C</given-names></name>. <article-title>The formation of multi-synaptic connections by the interaction of synaptic and structural plasticity and their functional consequences</article-title>. <source>PLoS Computational Biology</source>. <year>2015</year>;<volume>11</volume>(<issue>1</issue>):<fpage>e1004031</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004031" xlink:type="simple">10.1371/journal.pcbi.1004031</ext-link></comment> <object-id pub-id-type="pmid">25590330</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref070">
<label>70</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Körner</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Körner</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>Structural Synaptic Plasticity Has High Memory Capacity and Can Explain Graded Amnesia, Catastrophic Forgetting, and the Spacing Effect</article-title>. <source>PLoS One</source>. <year>2014</year>;<volume>9</volume>(<issue>5</issue>):<fpage>e96485</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0096485" xlink:type="simple">10.1371/journal.pone.0096485</ext-link></comment> <object-id pub-id-type="pmid">24858841</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref071">
<label>71</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Goaillard</surname> <given-names>JM</given-names></name>. <article-title>Variability, compensation and homeostasis in neuron and network function</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2006</year>;<volume>7</volume>(<issue>7</issue>):<fpage>563</fpage>–<lpage>574</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1949" xlink:type="simple">10.1038/nrn1949</ext-link></comment> <object-id pub-id-type="pmid">16791145</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref072">
<label>72</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Haffner</surname> <given-names>P</given-names></name>. <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source>. <year>1998</year>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>2324</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/5.726791" xlink:type="simple">10.1109/5.726791</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref073">
<label>73</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jolivet</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rauch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lüscher</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2006</year>;<volume>21</volume>(<issue>1</issue>):<fpage>35</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-006-7074-5" xlink:type="simple">10.1007/s10827-006-7074-5</ext-link></comment> <object-id pub-id-type="pmid">16633938</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004485.ref074">
<label>74</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Mensi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Naud</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <chapter-title>From stochastic nonlinear integrate-and-fire to generalized linear models</chapter-title>. In: <source>Advances in Neural Information Processing Systems</source>. <volume>vol. 24</volume>; <year>2011</year>. p. <fpage>1377</fpage>–<lpage>1385</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref075">
<label>75</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Habenschuss</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bill</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>. <chapter-title>Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</chapter-title>. In: <source>Advances in Neural Information Processing Systems</source>. <volume>vol. 25</volume>; <year>2012</year>. p. <fpage>782</fpage>–<lpage>790</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004485.ref076">
<label>76</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>. <article-title>What is a moment? Transient synchrony as a collective mechanism for spatiotemporal integration</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2001</year>;<volume>98</volume>(<issue>3</issue>):<fpage>1282</fpage>–<lpage>1287</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.98.3.1282" xlink:type="simple">10.1073/pnas.98.3.1282</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>