<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">09-PLCB-RA-1286R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000757</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience</subject><subject>Neuroscience/Neural Homeostasis</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Independent Component Analysis in Spiking Neurons</article-title><alt-title alt-title-type="running-head">ICA with Spiking Neurons</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Savin</surname><given-names>Cristina</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Joshi</surname><given-names>Prashant</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Triesch</surname><given-names>Jochen</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group><aff id="aff1">          <addr-line>Frankfurt Institute for Advanced Studies, Frankfurt am Main, Germany</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Morrison</surname><given-names>Abigail</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">RIKEN Brain Science Institute, Japan</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">savin@fias.uni-frankfurt.de</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: CS JT. Performed the experiments: CS. Analyzed the data: CS. Wrote the paper: CS JT. Developed the mathematical model used for implementing intrinsic plasticity in the paper: PJ.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>4</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>22</day><month>4</month><year>2010</year></pub-date><volume>6</volume><issue>4</issue><elocation-id>e1000757</elocation-id><history>
<date date-type="received"><day>22</day><month>10</month><year>2009</year></date>
<date date-type="accepted"><day>23</day><month>3</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Savin et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Although models based on independent component analysis (ICA) have been successful in explaining various properties of sensory coding in the cortex, it remains unclear how networks of spiking neurons using realistic plasticity rules can realize such computation. Here, we propose a biologically plausible mechanism for ICA-like learning with spiking neurons. Our model combines spike-timing dependent plasticity and synaptic scaling with an intrinsic plasticity rule that regulates neuronal excitability to maximize information transmission. We show that a stochastically spiking neuron learns one independent component for inputs encoded either as rates or using spike-spike correlations. Furthermore, different independent components can be recovered, when the activity of different neurons is decorrelated by adaptive lateral inhibition.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>How the brain learns to encode and represent sensory information has been a longstanding question in neuroscience. Computational theories predict that sensory neurons should reduce redundancies between their responses to a given stimulus set in order to maximize the amount of information they can encode. Specifically, a powerful set of learning algorithms called Independent Component Analysis (ICA) and related models, such as sparse coding, have emerged as a standard for learning efficient codes for sensory information. These algorithms have been able to successfully explain several aspects of sensory representations in the brain, such as the shape of receptive fields of neurons in primary visual cortex. Unfortunately, it remains unclear how networks of spiking neurons can implement this function and, even more difficult, how they can learn to do so using known forms of neuronal plasticity. This paper solves this problem by presenting a model of a network of spiking neurons that performs ICA-like learning in a biologically plausible fashion, by combining three different forms of neuronal plasticity. We demonstrate the model's effectiveness on several standard sensory learning problems. Our results highlight the importance of studying the interaction of different forms of neuronal plasticity for understanding learning processes in the brain.</p>
</abstract><funding-group><funding-statement>The authors are supported by EC MEXT project PLICON and the German Federal Ministry of Education and Research (BMBF) within the Bernstein Focus: Neurotechnology through research grant 01GQ0840. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Independent component analysis is a well-known signal processing technique for extracting statistically independent components from high-dimensional data. For the brain, ICA-like processing could play an essential role in building efficient representations of sensory data <xref ref-type="bibr" rid="pcbi.1000757-Barlow1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1000757-Olshausen1">[4]</xref>. However, although many algorithms have been proposed for solving the ICA problem <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen1">[5]</xref>, only few consider spiking neurons. Moreover, the existing spike-based models <xref ref-type="bibr" rid="pcbi.1000757-Klampfl1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Parra1">[7]</xref> do not answer the question how this type of learning can be realized in networks of spiking neurons using local, biologically plausible plasticity mechanisms (but see <xref ref-type="bibr" rid="pcbi.1000757-Clopath1">[8]</xref>).</p>
<p>Classic ICA algorithms often exploit the non-Gaussianity principle, which allows the ICA model to be estimated by maximizing some non-Gaussianity measure, such as kurtosis or negentropy <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen1">[5]</xref>. A related representational principle is sparse coding, which has been used to explain various properties of V1 receptive fields <xref ref-type="bibr" rid="pcbi.1000757-Olshausen2">[9]</xref>. Sparse coding states that only a small number of neurons are activated at the same time, or alternatively, that each individual unit is activated only rarely <xref ref-type="bibr" rid="pcbi.1000757-Olshausen3">[10]</xref>. In the context of neural circuits, it offers a different interpretation of the goal of the ICA transform, from the perspective of metabolic efficiency. As spikes are energetically expensive, neurons have to operate under tight metabolic constraints <xref ref-type="bibr" rid="pcbi.1000757-Lennie1">[11]</xref>, which affect the way information is encoded. Moreover, experimental evidence supports the idea that the activity of neurons in V1 is sparse. Close to exponential distributions of firing rates have been reported in various visual areas in response to natural scenes <xref ref-type="bibr" rid="pcbi.1000757-Baddeley1">[12]</xref>.</p>
<p>Interestingly, certain homeostatic mechanisms are thought to regulate the distribution of firing rates of a neuron <xref ref-type="bibr" rid="pcbi.1000757-Stemmler1">[13]</xref>. These intrinsic plasticity (IP) mechanisms adjust ionic channel properties, inducing persistent changes in neuronal excitability <xref ref-type="bibr" rid="pcbi.1000757-Zhang1">[14]</xref>. They have been reported for a variety of systems, in brain slices and neuronal cultures <xref ref-type="bibr" rid="pcbi.1000757-Zhang1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Cudmore1">[15]</xref> and they are generally thought to play a role in maintaining system homeostasis. Moreover, IP has been found to occur in behaving animals, in response to learning (see <xref ref-type="bibr" rid="pcbi.1000757-Zhang1">[14]</xref> for review).</p>
<p>From a computational perspective, it is believed that IP may maximize information transmission of a neuron, under certain metabolic constraints <xref ref-type="bibr" rid="pcbi.1000757-Stemmler1">[13]</xref>. Additionally, we have previously shown for a rate neuron model that, when interacting with Hebbian synaptic plasticity, IP allows the discovery of heavy-tailed directions in the input <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>. Here, we extend these results for a network of spiking neurons. Specifically, we combine spike-timing dependent plasticity (STDP) <xref ref-type="bibr" rid="pcbi.1000757-Gerstner1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1000757-Bi1">[19]</xref>, synaptic scaling <xref ref-type="bibr" rid="pcbi.1000757-Turrigiano1">[20]</xref> and an IP rule similar to <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>, which tries to make the distribution of instantaneous neuronal firing rates close to exponential.</p>
<p>We show that IP and synaptic scaling complement STDP learning, allowing single spiking neurons to learn useful representations of their inputs for several ICA problems. First, we show that output sparsification by IP together with synaptic learning is sufficient for demixing two zero mean supergaussian sources, a classic formulation of ICA. When using biologically plausible inputs and STDP, complex tasks, such as Foldiák's bars problem <xref ref-type="bibr" rid="pcbi.1000757-Foldik1">[21]</xref>, and learning oriented receptive fields for natural visual stimuli, can be tackled. Moreover, a population of neurons learns to extract several independent components if the activity of different neurons are decorrelated by adaptive lateral inhibition. When investigating the mechanisms how learning occurs in our model, we show that IP is necessary for learning, as it enforces a sparse output, guiding learning towards heavy-tailed directions in the input. Lastly, for specific STDP implementations, we show that IP shifts the threshold between potentiation and depression, similar to a sliding threshold for Bienenstock-Cooper-Munro (BCM) learning <xref ref-type="bibr" rid="pcbi.1000757-Bienenstock1">[22]</xref>.</p>
<p>The underlying assumption behind our approach, implicit in all standard models of V1 receptive field development, is that both input and output information are encoded in rates. In this light, one may think of our current work as a translation of the model in <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref> to a spike-based version. However, the principles behind our model are more general than suggested by our work with rate neurons. We show that the same rule can be applied when inputs are encoded as spike-spike correlation patterns, where a rate-based model would fail.</p>
</sec><sec id="s2">
<title>Results</title>
<p>A schematic view of the learning rules is shown in <xref ref-type="fig" rid="pcbi-1000757-g001">Fig. 1</xref>. The stochastically spiking neuron <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref> generates spikes as an inhomogeneous Poisson process, with mean expressed as a function of the total incoming current to the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e001" xlink:type="simple"/></inline-formula>, parametrized by variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e002" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e003" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e004" xlink:type="simple"/></inline-formula>. This transfer function is optimized by adapting the three parameters to make the distribution of instantaneous firing rates of the neuron approximatively exponential (for a complete mathematical formulation, see the <xref ref-type="sec" rid="s4">Methods</xref> section). Additionally, Hebbian synaptic plasticity, implemented by nearest-neighbor STDP <xref ref-type="bibr" rid="pcbi.1000757-Izhikevich1">[24]</xref>, changes incoming weights and a synaptic scaling mechanism keeps the sum of all incoming weights constant over time.</p>
<fig id="pcbi-1000757-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g001</object-id><label>Figure 1</label><caption>
<title>Overview of plasticity rules used for ICA-like learning.</title>
<p>Synapse weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e005" xlink:type="simple"/></inline-formula> are modified by nearest-neighbor STDP and synaptic scaling. Additionally, intrinsic plasticity changes the neuron's transfer function by adjusting three parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e006" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e007" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e008" xlink:type="simple"/></inline-formula>. Different transfer functions show the effects of changing each of the three parameters individually relative to the default case depicted in blue. Namely, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e009" xlink:type="simple"/></inline-formula> gives the slope of the curve, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e010" xlink:type="simple"/></inline-formula> shifts the entire curve left or right, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e011" xlink:type="simple"/></inline-formula> can be used for rescaling the membrane potential axis. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e012" xlink:type="simple"/></inline-formula> is increased by a factor of 1.5, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e013" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e014" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e015" xlink:type="simple"/></inline-formula> by a factor of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e016" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g001" xlink:type="simple"/></fig><sec id="s2a">
<title>A simple demixing problem</title>
<p>To illustrate the basic mechanism behind our approach, we first ask if enforcing a sparse prior by IP and Hebbian learning can yield a valid ICA implementation for the classic problem of demixing two supergaussian independent sources. In the standard form of this problem, zero mean, unit variance inputs ensure that the covariance matrix is the identity, such that simple Hebbian learning with a linear unit (equivalent to principal component analysis) would not be able to exploit the input statistics and would just perform a random walk in the input space. This is, however, a purely mathematical formulation, and does not make much sense in the context of biological neurons. Inputs to real neurons are bounded and —in a rate-based encoding— all positive. Nonetheless, we chose this standard formulation to illustrate the basic underlying principles behind our model. Below, we will consider different spike-based encodings of the input and learning with STDP.</p>
<p>As a special case of a demixing problem, we use two independent Laplacian distributed inputs, with unit variance: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e017" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e018" xlink:type="simple"/></inline-formula>. For the linear superposition, we use a rotation matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e019" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e020" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e021" xlink:type="simple"/></inline-formula> is the angle of rotation, resulting in a set of inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e022" xlink:type="simple"/></inline-formula>. Samples are drawn at each time step from the input distribution and are mapped into a total input to the neuron as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e023" xlink:type="simple"/></inline-formula>, with the weight vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e024" xlink:type="simple"/></inline-formula> normalized). The neuron's transfer functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e025" xlink:type="simple"/></inline-formula>, the same as for our spiking model (<xref ref-type="fig" rid="pcbi-1000757-g001">Fig. 1</xref>), is adapted based on our IP rule, to make the distribution of firing rates exponential. For simplicity, here weights change by classic Hebbian learning: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e026" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e027" xlink:type="simple"/></inline-formula> being the synaptic learning rate (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Similar results can be obtained when synaptic changes follow the BCM rule.</p>
<p>In <xref ref-type="fig" rid="pcbi-1000757-g002">Fig. 2A</xref> we show the evolution of synaptic weights for different starting conditions. As our IP rule adapts the neuron parameters to make the output distribution sparse (<xref ref-type="fig" rid="pcbi-1000757-g002">Fig. 2B,C</xref>), the weight vector aligns itself along the direction of one of the sources. With this simple model, we are able to demix a linear combination of two independent sources for different mixing matrices and different weights constraints (<xref ref-type="fig" rid="pcbi-1000757-g002">Fig. 2D</xref>), as any other single-unit implementation of ICA.</p>
<fig id="pcbi-1000757-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g002</object-id><label>Figure 2</label><caption>
<title>A demixing problem: two rotated Laplace directions.</title>
<p>(A) Evolution of the weights (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e028" xlink:type="simple"/></inline-formula> in blue, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e029" xlink:type="simple"/></inline-formula> in red) for different initial conditions, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e030" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e031" xlink:type="simple"/></inline-formula> weight normalization. (B) Evolution of the instantaneous firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e032" xlink:type="simple"/></inline-formula>, sampled each 1000 ms, for the initial weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e033" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e034" xlink:type="simple"/></inline-formula>. (C) Corresponding changes in transfer function parameters, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e035" xlink:type="simple"/></inline-formula> in Hz and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e036" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e037" xlink:type="simple"/></inline-formula> in mV. (D) Final weight vector for different rotation angles <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e038" xlink:type="simple"/></inline-formula> (in red). In the first example, normalization was done by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e039" xlink:type="simple"/></inline-formula> (the estimated rotation angle is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e040" xlink:type="simple"/></inline-formula>, instead of the actual value 0.5236); for the others <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e041" xlink:type="simple"/></inline-formula> was used. In all cases the final weight vector was scaled by a factor of 5, to improve visibility.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g002" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>One neuron learns an independent component</title>
<p>After showing that combining IP and synaptic learning can solve a classical formulation of ICA, we focus on spike-based, biologically plausible inputs. In the following, STDP is used for implementing synaptic learning, while the IP and the synaptic scaling implementations remain the same.</p>
<sec id="s2b1">
<title>Demixing with spikes</title>
<p>The demixing problem above can be solved also in a spike-based setting, after a few changes. First, the positive and negative inputs have to be separated into on- and off- channels (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e042" xlink:type="simple"/></inline-formula>) and converted into Poisson spike trains of a certain duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e043" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref>), with the corresponding rates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e044" xlink:type="simple"/></inline-formula> (note that the inputs are no longer white in this four-dimensional space). Secondly, to avoid the unbiological situation of having few very strong inputs, such that single presynaptic spikes always elicit a spike in the postsynaptic neuron, each channel consists of several (here, 25) synapses, with independent inputs having the same firing rate. Synapses are all positive and adapt by STDP, under the normalization <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e045" xlink:type="simple"/></inline-formula>. A more detailed description of the parameters can be found in the <xref ref-type="sec" rid="s4">Methods</xref> section.</p>
<p>The evolution of the weights is shown in <xref ref-type="fig" rid="pcbi-1000757-g003">Fig. 3A</xref>. The corresponding receptive field of the neuron can be obtained by projecting the weight vector back onto the original two-dimensional space (details of this procedure are described in the <xref ref-type="sec" rid="s4">Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1000757.s001">Text S1</xref>). As in the original formulation of the problem, the neuron receptive field slowly aligns itself along one of the independent components (<xref ref-type="fig" rid="pcbi-1000757-g003">Fig. 3B</xref>).</p>
<fig id="pcbi-1000757-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g003</object-id><label>Figure 3</label><caption>
<title>The demixing problem with inputs encoded as spike trains.</title>
<p>(A) Evolution of the weights for a rotation angle <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e046" xlink:type="simple"/></inline-formula>. (B) Final corresponding weight vector in the original two-dimensional space. The final weight vector is scaled by a factor of 100, to improve visibility.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g003" xlink:type="simple"/></fig></sec><sec id="s2b2">
<title>Foldiák's bars: input encoded as firing rates</title>
<p>As a second test case for our model, we consider Foldiák's well-known bars problem <xref ref-type="bibr" rid="pcbi.1000757-Foldik1">[21]</xref>. This is a classic non-linear ICA problem and is interesting from a biological perspective, as it mimics relevant nonlinearities, e.g. occlusions. In the classic formulation, for a two-dimensional input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e047" xlink:type="simple"/></inline-formula>, of size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e048" xlink:type="simple"/></inline-formula>, a single bar must be learned after observing samples consisting of the nonlinear superposition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e049" xlink:type="simple"/></inline-formula> possible individual bars (see <xref ref-type="fig" rid="pcbi-1000757-g004">Fig. 4A</xref>), each appearing independently, with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e050" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e051" xlink:type="simple"/></inline-formula>). The superposition is non-linear, as the intersection of two bars has the same intensity as the other pixels in the bars (a binary OR).</p>
<fig id="pcbi-1000757-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g004</object-id><label>Figure 4</label><caption>
<title>Learning a single independent component for the bars problem.</title>
<p>(A) A set of randomly generated samples from the input distribution, (B) Evolution of the neuron's receptive field as the IP rule converges and instantaneous firing rate of the neuron. Each dot corresponds to the instantaneous firing rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e052" xlink:type="simple"/></inline-formula>) sampled each 500 ms.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g004" xlink:type="simple"/></fig>
<p>In our implementation, the input vector is normalized and the value of each pixel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e053" xlink:type="simple"/></inline-formula> is converted into a corresponding Poisson spike train of duration on the order of a fixation duration <xref ref-type="bibr" rid="pcbi.1000757-MartinezConde1">[25]</xref>. The details of the experimental setup are described in the <xref ref-type="sec" rid="s4">Methods</xref> section. As the IP mechanism begins to take effect, making neuronal activity sparse, the receptive field of the neuron slowly adapts to one of the bars (<xref ref-type="fig" rid="pcbi-1000757-g004">Fig. 4B</xref>). This effect is robust for a wide range of parameters (see <xref ref-type="supplementary-material" rid="pcbi.1000757.s002">Text S2</xref>) and, as suggested by our previous results for rate neurons <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>, does not critically depend on the particular implementation of the synaptic learning. We obtain similar results with an additive <xref ref-type="bibr" rid="pcbi.1000757-Song1">[26]</xref> and a simple triplet <xref ref-type="bibr" rid="pcbi.1000757-Pfister1">[27]</xref> model of STDP.</p>
<p>The input normalization makes a bar in an input sample containing a single IC stronger than a bar in a sample containing multiple ICs. This may suggest that a single component emerges by preferentially learning ‘easy’ examples, i.e. examples with a single bar. However, this is not the case and one bar is learned even when single bars never appear in the input, as in <xref ref-type="bibr" rid="pcbi.1000757-Lcke1">[28]</xref>. Specifically, we use a variant of the bars problem, in which the input always consists of 4 distinct bars, selected at random. In this case, the neuron correctly learns a single component. Moreover, similar results can be obtained for 2–5 bars (see <xref ref-type="supplementary-material" rid="pcbi.1000757.s002">Text S2</xref>), using the same set of parameters.</p>
</sec><sec id="s2b3">
<title>Foldiák's bars: input encoded by spike-spike correlations</title>
<p>In the previous experiments, input information was encoded as firing rates. In the following, we show that this stimulus encoding is not critical and the presence of spike-spike correlations is sufficient to learn an independent component, even in the absence of any presynaptic rate modulation. To demonstrate this, we consider a slight modification of the above bars problem, with input samples consisting of always two bars, each two pixel wide, with N distinct bars. This is a slightly more difficult task, as wide bars emphasize non-linearities due to overlap, but can be solved by our model with a rate encoding (see <xref ref-type="supplementary-material" rid="pcbi.1000757.s002">Text S2</xref>).</p>
<p>In this case, all inputs have the same mean firing rate and the information about whether a pixel belongs to a bar or not is encoded in correlation patterns between different inputs (see <xref ref-type="sec" rid="s4">Methods</xref>). Specifically, background inputs are uncorrelated, while inputs belonging to bars are all pairwise correlated, with a correlation coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e054" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1000757-g005">Fig. 5B</xref>).</p>
<fig id="pcbi-1000757-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g005</object-id><label>Figure 5</label><caption>
<title>Bars in a correlation-based encoding.</title>
<p>(A) Example of 20 spike trains with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e055" xlink:type="simple"/></inline-formula>. (B) A sample containing two 2-pixel wide bars and the corresponding covariance matrix used for its encoding. (C) Evolution of the weights during learning. (D) Final receptive field and corresponding weights histogram.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g005" xlink:type="simple"/></fig>
<p>As for the rate-based encoding, the neuron is able to reliably learn a bar (<xref ref-type="fig" rid="pcbi-1000757-g005">Fig. 5C, D</xref>). Similar results were obtained for the version with always two bars, each one pixel wide, but with slower convergence. The fact that our approach also works for correlated inputs rests on the properties of STDP and IP. In the original rate formulation, strong inputs lead to higher firing in the postsynaptic neuron, causing the potentiation of the corresponding synapses. Similarly, if presynaptic inputs fire all with the same rate, correlated inputs are more successful in driving the neuron and hence their weights are preferentially strengthened <xref ref-type="bibr" rid="pcbi.1000757-Song1">[26]</xref>. Moreover, as before, IP enforces sparse post-synaptic responses, guiding learning towards a heavy-tailed direction in the input (a more formal analysis of the interaction between IP and STDP is presented below).</p>
<p>Due to its stochastic nature, our neuron model is not particularly sensitive to input correlations, hence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e056" xlink:type="simple"/></inline-formula> cannot be too small. Stable receptive fields with a single 2 pixel wide bar are still obtained for lower correlation coefficients (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e057" xlink:type="simple"/></inline-formula>), but with slower convergence. We expect better results with a deterministic neuron model, such as the leaky integrate-and-fire. An approximation of IP, based on moment matching could be used in this case. Additionally, STDP-induced competition (due to the relatively small number of inputs, in some cases one weight grows big enough to elicit a spike in the postsynaptic neuron) <xref ref-type="bibr" rid="pcbi.1000757-Song1">[26]</xref> enforces some constraints on the model parameters to ensure the stability of a solution with multiple non-zero weights. This could be done by increasing the size of the input, restricting the overall mean firing of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e058" xlink:type="simple"/></inline-formula> or by slightly changing the STDP parameters (see <xref ref-type="sec" rid="s4">Methods</xref>). These parameter changes do not affect learning with the rate-based encoding, however.</p>
</sec><sec id="s2b4">
<title>Natural scenes input</title>
<p>The third classical ICA problem we consider is the development of V1-like receptive fields for natural scenes <xref ref-type="bibr" rid="pcbi.1000757-Simoncelli2">[3]</xref>. Several computational studies have emphasized that simple cell receptive fields in V1 may be learned from the statistics of natural images by ICA or other similar component extraction algorithms <xref ref-type="bibr" rid="pcbi.1000757-Olshausen2">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Bell1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen2">[30]</xref>. We hypothesized that the same type of computation could be achieved in our spiking neuron model, by combining different forms of plasticity. Only a rate encoding was used for this problem, partly for computational reasons and partly because it is not immediately obvious how a correlation-based encoding would look like in this case.</p>
<p>We use a set of images from the van Hateren database <xref ref-type="bibr" rid="pcbi.1000757-vanHateren1">[31]</xref>, with standard preprocessing (see <xref ref-type="sec" rid="s4">Methods</xref>). The rectified values of the resulting image patches are linearly mapped into a firing frequency for an on- and off-input population, as done for the bars. The STDP, IP and other simulation parameters are the same as before.</p>
<p>As shown in <xref ref-type="fig" rid="pcbi-1000757-g006">Fig. 6A</xref>, the receptive field of the neuron computed as the difference between the weights of the on- and the off- input populations (depicted in <xref ref-type="fig" rid="pcbi-1000757-g006">Fig. 6B</xref>) evolves to an oriented filter, similar to those obtained by other ICA learning procedures <xref ref-type="bibr" rid="pcbi.1000757-Olshausen2">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Bell1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Falconbridge1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Weber1">[33]</xref>. A similar receptive field can be obtained by reverse correlation from white noise stimuli. The least-mean-square-error fit to a Gabor wavelet <xref ref-type="bibr" rid="pcbi.1000757-Lcke2">[34]</xref> is shown in <xref ref-type="fig" rid="pcbi-1000757-g006">Fig. 6C</xref>. As vertical edges are usually over-represented in the input, the neuron will typically learn a vertical edge filter, with a phase shift depending on the initial conditions. The receptive field has low spatial frequency, but more localized solutions result for a neural population (see below).</p>
<fig id="pcbi-1000757-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g006</object-id><label>Figure 6</label><caption>
<title>Learning a Gabor-like receptive field.</title>
<p>(A) Evolution of the neuronal activity during learning, (B) Learned weights corresponding to the inputs from the on and off populations, (C) The receptive field learned by the neuron, and its l.m.s. Gabor fit.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g006" xlink:type="simple"/></fig></sec></sec><sec id="s2c">
<title>ICA in a neuron population</title>
<p>So far, learning has been restricted to a single neuron. For learning multiple independent components, we implement a neuron population in which the activities of different neurons are decorrelated by adaptive lateral inhibition. This approach is standardly used for feature extraction methods based on single-unit contrast functions <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen2">[30]</xref>. Here, we consider a simple scheme for parallel (symmetrical) decorrelation. The all-to-all inhibitory weights (<xref ref-type="fig" rid="pcbi-1000757-g007">Fig. 7A</xref>) change by STDP and are subject to synaptic scaling, as done for the input synapses. We only use a rate-based encoding for this case, due to computational overhead, which also limits the size of networks we can simulate.</p>
<fig id="pcbi-1000757-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g007</object-id><label>Figure 7</label><caption>
<title>Learning multiple ICs.</title>
<p>(A) Overview of the network structure: all neurons receive signals from the input layer and are recurrently connected by all-to-all inhibitory synapses, (B) A set of receptive fields learned for the bars problem, (C) Evolution of the mean correlation coefficient and mutual information in time, both computed by dividing the neuron output in bins of width 1000 s and estimating <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e059" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e060" xlink:type="simple"/></inline-formula> for each bin, (D) Learned inhibitory lateral connections, (E) A set of receptive fields learned for natural image patches.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g007" xlink:type="simple"/></fig>
<p>We consider a population of 10 neurons. In order to have a full basis set for the bars problem, we use 2 pixel wide bars. For this case, our learning procedure is able to recover the original basis (<xref ref-type="fig" rid="pcbi-1000757-g007">Fig. 7B</xref>). As lateral inhibition begins to take effect, the average correlation coefficient between the responses of different neurons in the population decreases (<xref ref-type="fig" rid="pcbi-1000757-g007">Fig. 7C</xref>), making the final inhibitory weights unspecific (<xref ref-type="fig" rid="pcbi-1000757-g007">Fig. 7D</xref>). As decorrelation is not a sufficient condition for independence, we show that, simultaneously, the normalized mutual information decreases (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Using the same network for the image patches, we obtain oriented, localized receptive fields (<xref ref-type="fig" rid="pcbi-1000757-g007">Fig. 7E</xref>).</p>
<p>Due to the adaptive nature of IP, the balance between excitation and inhibition does not need to be tightly controlled, allowing for robustness to changes in parameters. However, the inhibition strength influences the time required for convergence (the stronger the inhibition, the longer it takes for the system to reach a stable state). A more important constraint is that the adaptation of inhibitory connections needs to be faster than that of feedforward connections to allow for efficient decorrelation (see <xref ref-type="sec" rid="s4">Methods</xref> for parameters).</p>
</sec><sec id="s2d">
<title>Is IP necessary for learning?</title>
<p>We wondered what the role of IP is in this learning procedure. Does IP simply find an optimal nonlinearity for the neuron's transfer function, given the input, something that could be computed offline (as for InfoMax <xref ref-type="bibr" rid="pcbi.1000757-Bell1">[29]</xref>), or is the interaction between IP and STDP critical for learning? To answer this question, we go back to Foldiák's bars. We repeat our first bars experiment (<xref ref-type="fig" rid="pcbi-1000757-g004">Fig. 4</xref>) for a fixed gain function, given by the parameters obtained after learning (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e061" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e062" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e063" xlink:type="simple"/></inline-formula>). In this case, the receptive field does not evolve to an IC (<xref ref-type="fig" rid="pcbi-1000757-g008">Fig. 8</xref>). This suggests that ICA-like computation relies on the interplay between weight changes and the corresponding readjustment of neuronal excitability, which forces the output to be sparse. Note that this result holds for simulation times significantly larger than in the experiment before, where a bar emerged after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e064" xlink:type="simple"/></inline-formula>, suggesting that, even if the neuron would eventually learn a bar, it would take significantly longer to do so.</p>
<fig id="pcbi-1000757-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g008</object-id><label>Figure 8</label><caption>
<title>IP is critical for learning.</title>
<p>Evolution of the receptive field for a neuron with a fixed gain function, given by the final parameters obtained after learning in the previous bars experiment. A bar cannot be learned in this case.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g008" xlink:type="simple"/></fig>
<p>We could assume that the neuron failed to learn a bar for the fixed transfer function just because the postsynaptic firing was too low, slowing down learning. Hence, it may be that a simpler rule, regulating just the mean firing rate of the neuron, would suffice to learn an IC. To test this hypothesis, we construct an alternative IP rule, which adjusts just <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e065" xlink:type="simple"/></inline-formula> to preserve the average firing rate of the neuron (see <xref ref-type="sec" rid="s4">Methods</xref>). With the same setup as before and the new IP rule, no bar is learned and the output distribution is Gaussian, with a small standard deviation around the target value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e066" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1000757-g009">Fig. 9A</xref>). However, after additional parameter tuning, a bar can sometimes be learned, as shown in <xref ref-type="fig" rid="pcbi-1000757-g009">Fig. 9B</xref>. In this case, the final output distribution is highly kurtotic, due to the receptive field. The outcome depends on the variance of the total input, which has to be large enough to start the learning process (variance was regulated by the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e067" xlink:type="simple"/></inline-formula>, see <xref ref-type="sec" rid="s4">Methods</xref>). Most importantly, this dependence on model parameters shows that regulating the mean of the output distribution is not sufficient for reliably learning a bar and higher order moments need to be considered as well.</p>
<fig id="pcbi-1000757-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000757.g009</object-id><label>Figure 9</label><caption>
<title>Mean firing constraint is not sufficient for reliable learning.</title>
<p>(A) Evolution of neuron activation for a neuron with a gain function regulated by a simplified IP rule, which adjusts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e068" xlink:type="simple"/></inline-formula> to maintain the same mean average firing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e069" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e070" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e071" xlink:type="simple"/></inline-formula>, in the first and second row, respectively. Inset illustrates final receptive field for each case. (B) Corresponding evolution of weights and (C) their final distribution.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.g009" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>Interaction between IP and STDP</title>
<p>A good starting point for elucidating the mechanism by which the interaction between STDP and IP facilitates the discovery of an independent component is our initial problem of a single unit receiving a two dimensional input. We have previously shown in simulations that for a bounded, whitened, two dimensional input the weight vector tends to rotate towards the heavy-tailed direction in the input <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>. Here, we extend these results both analytically and in simulations. Our analysis focuses on the theoretical formulation of zero mean, unit variance inputs used for the demixing problem before and is restricted to expected changes in weights given the input and output firing rates, ignoring the time of individual spikes.</p>
<p>We report here only the main results of these experiments, while a detailed description is provided as supplemental information (see <xref ref-type="supplementary-material" rid="pcbi.1000757.s003">Text S3</xref>). Firstly, for conveniently selected pairs of input distributions, it is possible to show analytically that the weight vector rotates towards the heavy-tailed direction in the input, under the assumption that IP adaptation is faster than synaptic learning (previously demonstrated numerically in <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>). Secondly, due to the IP rule, weight changes mostly occur on the tail of the output distribution and are significantly larger for the heavy-tailed input. Namely, IP focuses learning to the heavy tailed direction in the input. When several inputs are supergaussian, the learning procedure results in the maximization of the output kurtosis, independent of the shape of the input distributions. Most importantly, we show that, for simple problems when a solution can be obtained by nonlinear PCA, our IP rule significantly speeds up learning of an independent component.</p>
<p>One way to understand these results could be in terms of nonlinear PCA theory. Given that for a random initial weight vector, the total input distribution is close to Gaussian, in order to enforce a sparse output, the IP has to change the transfer function in a way that ‘hides’ most of the input distribution (for example by shifting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e072" xlink:type="simple"/></inline-formula> somewhere above the mean of the Gaussian). As a result, the nonlinear part of the transfer function will cover the ‘visible’ part of the input distribution, facilitating the discovery of sparse inputs by a mechanism similar to nonlinear PCA. In this light, IP provides the means to adapt the transfer function in a way that makes the nonlinear PCA particularly efficient.</p>
<p>Lastly, from an information-theoretic perspective, our approach can be linked to previous work on maximizing information transmission between neuronal input and output by optimizing synaptic learning <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref>. This synaptic optimization procedure was shown to yield a generalization of the classic BCM rule <xref ref-type="bibr" rid="pcbi.1000757-Bienenstock1">[22]</xref>. We can show that, for a specific family of STDP implementations, which have a quadratic dependence on postsynaptic firing, IP effectively acts as a sliding threshold for BCM learning (see <xref ref-type="supplementary-material" rid="pcbi.1000757.s004">Text S4</xref>).</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>Although ICA and related sparse coding models have been very successful in describing sensory coding in the cortex, it has been unclear how such computations can be realized in networks of spiking neurons in a biologically plausible fashion. We have presented a network of stochastically spiking neurons that performs ICA-like learning by combining different forms of plasticity. Although this is not the only attempt at computing ICA with spiking neurons, in previous models synaptic changes were not local, depending on the activity of neighboring neurons within a population <xref ref-type="bibr" rid="pcbi.1000757-Klampfl1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Parra1">[7]</xref>. In this light, our model is, to our knowledge, the first to offer a mechanistic explanation of how ICA-like computation could arise by biologically plausible learning mechanisms.</p>
<p>In our model, IP, STDP and synaptic scaling interact to give rise to robust receptive field development. This effect does not depend on a particular implementation of STDP, but it does require an IP mechanism which enforces a sparse output distribution. Although there are very good theoretical arguments why this should be the case <xref ref-type="bibr" rid="pcbi.1000757-Lennie1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Stemmler1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>, the experimental evidence supporting this assumption is limited <xref ref-type="bibr" rid="pcbi.1000757-Baddeley1">[12]</xref>. A likely explanation for this situation is the fact that it is difficult to map the experimentally observable output spikes into a probability of firing. Spike count estimates cannot be used directly, as they critically depend on the bin size. Additionally, the inter-spike interval (ISI) of an inhomogeneous Poisson process with exponentially distributed mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e073" xlink:type="simple"/></inline-formula> is indistinguishable from the ISI of a homogeneous Poisson distribution with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e074" xlink:type="simple"/></inline-formula>. Hence, more complex statistical analyses are required for disentangling the two (see <xref ref-type="bibr" rid="pcbi.1000757-Cox1">[35]</xref>).</p>
<p>From a computational perspective, our approach is reminiscent of several by-now classic ICA algorithms. As mentioned before, IP enforces the output distribution to be heavy-tailed, like in sparse coding <xref ref-type="bibr" rid="pcbi.1000757-Olshausen2">[9]</xref>. Our model also shares conceptual similarities to InfoMax <xref ref-type="bibr" rid="pcbi.1000757-Bell1">[29]</xref>, which attempts to maximize output entropy (however, at the population level) by regulating the weights and a neuron threshold parameter. Maximizing information transmission between pre- and post-synaptic spike trains under the constraint of a fixed mean postsynaptic firing rate links our method to previous work on synaptic plasticity. A spike-based synaptic rule optimizing the above criterion <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref> yields a generalization of the BCM rule <xref ref-type="bibr" rid="pcbi.1000757-Bienenstock1">[22]</xref>, a powerful form of learning, which is able to discover heavy-tailed directions in the input <xref ref-type="bibr" rid="pcbi.1000757-Intrator1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Intrator2">[37]</xref> and to learn Gabor receptive fields <xref ref-type="bibr" rid="pcbi.1000757-Blais1">[38]</xref> in linear neurons. We have shown that, sliding threshold BCM can be viewed as a particular case of IP learning, for a specific family of STDP models.</p>
<p>It is interesting to think of the mechanism presented here in relation to projection pursuit <xref ref-type="bibr" rid="pcbi.1000757-Huber1">[39]</xref>, which tries to find good representations of high-dimensional spaces by projecting data on a lower dimensional space. The algorithm searches for interesting projection directions, a typical measure of interest being the non-Gaussianity of the distribution of data in the lower dimensional space. The difference here is that, although we do not explicitly define a contrast function maximizing kurtosis or other similar measure, our IP rule implicitly yields highly kurtotic output distributions. By sparsifying the neuron output, IP guides the synaptic learning towards the interesting (i.e. heavy-tailed) directions in the input.</p>
<p>From a different perspective, we can relate our method to nonlinear PCA. It is known that, for zero mean whitened data, nonlinear Hebbian learning in a rate neuron can successfully capture higher order correlations in the input <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen3">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Oja1">[41]</xref>. Moreover, it has been suggested that the precise shape of the Hebbian nonlinearity can be used for optimization purposes, for example for incorporating prior knowledge about the sources' distribution <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen3">[40]</xref>. IP goes one step further in this direction, by adapting the transfer function online, during learning. From a biological perspective, there are some advantages in adapting the neuron's excitability during computation. Firstly, IP speeds up the nonlinear decorrelation of inputs. Secondly, the system gains great robustness to changes in parameters (as demonstrated in <xref ref-type="supplementary-material" rid="pcbi.1000757.s002">Text S2</xref>). Additionally, IP regulation plays a homeostatic role, making constraints on the input mean or second order statistics unnecessary. In the end, all the methods we have mentioned are closely related and, though conceptually similar, our approach is another distinct solution.</p>
<p>Our previous work was restricted to the a rate model neuron <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>. Beyond translating our results to a spiking neuron model, we have shown here that similar principles can be applied when information is encoded as spike-spike correlations, where a model relying just on firing rates would fail. It is a interesting challenge for future work to further investigate the exact mechanisms of receptive field development for different types of input encoding.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Neuron model</title>
<p>We consider a stochastically spiking neuron with refractoriness <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref>. The model defines the neuron's instantaneous probability of firing as a function of the current membrane potential and the refractory state of the neuron,which depends on the time since its last spike. More specifically, the membrane potential is computed as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e075" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e076" xlink:type="simple"/></inline-formula> is the resting potential, while the second term represents the total incoming drive to the neuron, computed as the linear summation of post-synaptic potentials evoked by incoming spikes. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e077" xlink:type="simple"/></inline-formula> gives the strength of synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e078" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e079" xlink:type="simple"/></inline-formula> is the time of a presynaptic spike, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e080" xlink:type="simple"/></inline-formula> the corresponding evoked post-synaptic potential, modeled as a decaying exponential, with time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e081" xlink:type="simple"/></inline-formula> (for GABA-ergic synapses, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e082" xlink:type="simple"/></inline-formula>) and amplitude <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e083" xlink:type="simple"/></inline-formula>.</p>
<p>The refractory state of the neuron, with values in the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e084" xlink:type="simple"/></inline-formula>, is defined as a function of the time of the last spike <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e085" xlink:type="simple"/></inline-formula>, namely:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e086" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e087" xlink:type="simple"/></inline-formula> gives the absolute refractory period, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e088" xlink:type="simple"/></inline-formula> is the relative refractory period and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e089" xlink:type="simple"/></inline-formula> is the Heaviside function.</p>
<p>The probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e090" xlink:type="simple"/></inline-formula> of the stochastic neuron firing at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e091" xlink:type="simple"/></inline-formula> is given as a function of its membrane potential and refractory state <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e092" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e093" xlink:type="simple"/></inline-formula> is the time step of integration, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e094" xlink:type="simple"/></inline-formula> is a gain function, defined as: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e095" xlink:type="simple"/></inline-formula> Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e096" xlink:type="simple"/></inline-formula> , <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e097" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e098" xlink:type="simple"/></inline-formula> are model parameters, whose values are adjusted by intrinsic plasticity, as described below.</p>
</sec><sec id="s4b">
<title>Intrinsic plasticity</title>
<p>Our intrinsic plasticity model attempts to maximize the mutual information between input and output, for a fixed energy budget <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Joshi1">[42]</xref>. More specifically, it induces changes in neuronal excitability that lead to an exponential distribution of the instantaneous firing rate of the neuron <xref ref-type="bibr" rid="pcbi.1000757-Stemmler1">[13]</xref>. The specific shape of the output distribution is justified from an information theoretic perspective, as the exponential distribution has maximum entropy for a fixed mean. This is true for distributions defined on the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e099" xlink:type="simple"/></inline-formula>, but, under certain assumptions, can be a good approximation for the case where the interval is bounded, as it happens in our model due to the neuron's refractory period (see below). Optimizing information transmission under the constraint of a fixed mean is equivalent to minimizing the Kullback-Leibler divergence between the neuron's firing rate distribution and that of an exponential with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e100" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e101" xlink:type="simple"/></disp-formula>with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e102" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e103" xlink:type="simple"/></inline-formula> denoting the entropy and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e104" xlink:type="simple"/></inline-formula> the expected value. Note that the above expression assumes that the instantaneous firing rate of the neuron is proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e105" xlink:type="simple"/></inline-formula>, that is that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e106" xlink:type="simple"/></inline-formula>. When taking into account the refractory period of the neuron, which imposes an upper-bound <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e107" xlink:type="simple"/></inline-formula> on the output firing rate, the maximum entropy distribution for a specific mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e108" xlink:type="simple"/></inline-formula> is a truncated exponential <xref ref-type="bibr" rid="pcbi.1000757-Kapur1">[43]</xref>. The deviation between the optimal exponential for the infinite and the bounded case depends on the values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e109" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e110" xlink:type="simple"/></inline-formula>, but it is small in cases in which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e111" xlink:type="simple"/></inline-formula>. Hence, our approximation is valid as long as the instantaneous firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e112" xlink:type="simple"/></inline-formula> is significantly lower than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e113" xlink:type="simple"/></inline-formula>, that is when the mean firing rate of the neuron is small. In our case, we restrict <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e114" xlink:type="simple"/></inline-formula>. If not otherwise stated, all simulations have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e115" xlink:type="simple"/></inline-formula>. Note also that the values considered here are in the range of firing rates reported for V1 neurons <xref ref-type="bibr" rid="pcbi.1000757-Olshausen4">[44]</xref>.</p>
<p>Computing the gradient of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e116" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e117" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e118" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e119" xlink:type="simple"/></inline-formula>, and using stochastic gradient descent, the optimization process translates into the following update rules <xref ref-type="bibr" rid="pcbi.1000757-Joshi1">[42]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e120" xlink:type="simple"/></disp-formula>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e121" xlink:type="simple"/></inline-formula> is a small learning rate. Here, the instantaneous firing rate is assumed to be directly accessible for learning. Alternatively, it could be estimated based on the recent spike history.</p>
<p>Additionally, as a control, we have considered a simplified rule, which adjusts a single transfer function parameter in order to maintain the mean firing rate of the neuron to a constant value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e122" xlink:type="simple"/></inline-formula>. More specifically, a low-pass-filtered version of the neuron firing rate is used to estimate the current mean firing rate of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e123" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e124" xlink:type="simple"/></inline-formula> is the Dirac function and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e125" xlink:type="simple"/></inline-formula> is the time of firing of the post-synaptic neuron and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e126" xlink:type="simple"/></inline-formula>. Based on this estimate, the value of the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e127" xlink:type="simple"/></inline-formula> is adjusted as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e128" xlink:type="simple"/></inline-formula> Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e129" xlink:type="simple"/></inline-formula> is the goal mean firing rate, as before and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e130" xlink:type="simple"/></inline-formula> is a learning rate, set such that, for a fixed Gaussian input distribution, convergence is reached as fast as for our IP rule described before (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e131" xlink:type="simple"/></inline-formula>).</p>
</sec><sec id="s4c">
<title>Synaptic learning</title>
<p>The STDP rule implemented here considers only nearest-neighbor interactions between spikes <xref ref-type="bibr" rid="pcbi.1000757-Izhikevich1">[24]</xref>. The change in weights is determined by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e132" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e133" xlink:type="simple"/></inline-formula> is the amplitude of the STDP change for potentiation and depression, respectively (default values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e134" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e135" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e136" xlink:type="simple"/></inline-formula> are the time scales for potentiation and depression (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e137" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e138" xlink:type="simple"/></inline-formula>; for learning spike-spike correlations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e139" xlink:type="simple"/></inline-formula>)<xref ref-type="bibr" rid="pcbi.1000757-Bi1">[19]</xref>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e140" xlink:type="simple"/></inline-formula> is the time difference between the firing of the pre- and post-synaptic neuron. For the lateral inhibitory connections, the STDP learning is faster, namely <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e141" xlink:type="simple"/></inline-formula>. In all cases, weights are always positive and clipped to zero if they become negative.</p>
<p>This STDP implementation is particularly interesting as it can be shown that, under the assumption of uncorrelated or weakly correlated pre- and post-synaptic Poisson spike trains, it induces weight changes similar to a BCM rule, namely <xref ref-type="bibr" rid="pcbi.1000757-Izhikevich1">[24]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e142" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e143" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e144" xlink:type="simple"/></inline-formula> are the firing rates of the pre- and post-synaptic neuron, respectively.</p>
<p>For the above expression, the fixed BCM threshold can be computed as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e145" xlink:type="simple"/></disp-formula>which is positive when potentiation dominates depression on the short time scale, while, overall, synaptic weakening is larger than potentiation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e146" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e147" xlink:type="simple"/></disp-formula></p>
<p>In some experiments we also consider the classical case of additive all-to-all STDP <xref ref-type="bibr" rid="pcbi.1000757-Song1">[26]</xref>, which acts as simple Hebbian learning, the induced change in weight being proportional to the product of the pre- and post-synaptic firing rates (see <xref ref-type="bibr" rid="pcbi.1000757-Izhikevich1">[24]</xref> for comparison of different STDP implementations). The parameters used in this case are: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e148" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e149" xlink:type="simple"/></inline-formula> and the same time constants as for the nearest neighbor case. Additionally, the simple triplets STDP model used as an alternative BCM-like STDP implementation is described in <xref ref-type="supplementary-material" rid="pcbi.1000757.s004">Text S4</xref>.</p>
</sec><sec id="s4d">
<title>Synaptic scaling</title>
<p>As in approaches which directly maximize kurtosis or similar measures <xref ref-type="bibr" rid="pcbi.1000757-Triesch1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1000757-Hyvrinen3">[40]</xref>, the weight vector is normalized: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e150" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e151" xlink:type="simple"/></inline-formula>, with weights being always positive. This value is arbitrary, as it represents a scaling factor of the total current, which can be compensated for by IP. It was selected in order to keep the final parameters close to those in <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref>. Additionally, for the natural image patches the normalization was done independently for the on- and off- populations, using the same value for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e152" xlink:type="simple"/></inline-formula> in each case.</p>
<p>In a neural population, the same normalization is applied for the lateral inhibitory connections. As before, weights do not change sign and are constrained by the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e153" xlink:type="simple"/></inline-formula> norm: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e154" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e155" xlink:type="simple"/></inline-formula>.</p>
<p>Currently, the normalization is achieved by dividing each weight by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e156" xlink:type="simple"/></inline-formula>, after the presentation of each sample. Biologically, this operation would be implemented by a synaptic scaling mechanism, which multiplicatively scales the synaptic weights to preserve the average input drive received by the neuron <xref ref-type="bibr" rid="pcbi.1000757-Turrigiano1">[20]</xref>.</p>
</sec><sec id="s4e">
<title>Setup for experiments</title>
<p>In all experiments, excitatory weights were initialized at random from the uniform distribution and normalized as described before. The transfer function was initialized to the parameters in <xref ref-type="bibr" rid="pcbi.1000757-Toyoizumi1">[23]</xref> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e157" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e158" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e159" xlink:type="simple"/></inline-formula>). Unless otherwise specified, all model parameters had the default values defined in the corresponding sections above. For all spike-based experiments, each sample was presented for a time interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e160" xlink:type="simple"/></inline-formula>, followed by the weight normalization.</p>
<p>For the experiments involving the rate-based model and a two-dimensional input, each sample was presented for one time step and the learning rates for IP and Hebbian learning were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e161" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e162" xlink:type="simple"/></inline-formula>, respectively. In this case, the weight normalization procedure can influence the final solution. Namely, positive weights with constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e163" xlink:type="simple"/></inline-formula> norm always yield a weight vector in the first quadrant, but this limitation can be removed by a different normalization, which keeps the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e164" xlink:type="simple"/></inline-formula> norm of the vector constant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e165" xlink:type="simple"/></inline-formula>).</p>
<p>For the demixing problem, the input was generated as described for the rate-based scenario above. After the rectification, the firing rates of the input on the on- and off- channels were scaled by a factor of 20, to speed up learning. After convergence, the total weight of each channel was estimated as the sum of individual weights corresponding to that input. The resulting four-dimensional weight vector was projected back to the original two-dimensional input space using: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e166" xlink:type="simple"/></inline-formula>, with a sign given by that of the channel with maximum weight (positive for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e167" xlink:type="simple"/></inline-formula>, negative otherwise). This procedure results by a minimum error projection of the weight vector onto the subspace defined by the constraint <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e168" xlink:type="simple"/></inline-formula>, see <xref ref-type="supplementary-material" rid="pcbi.1000757.s001">Text S1</xref> for details.</p>
<p>For all variants of the bars problem, the input vector was normalized to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e169" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e170" xlink:type="simple"/></inline-formula> defining the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e171" xlink:type="simple"/></inline-formula> norm, as in <xref ref-type="bibr" rid="pcbi.1000757-Butko1">[45]</xref>. Inputs were encoded using firing rates with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e172" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e173" xlink:type="simple"/></inline-formula> is the frequency of a background pixel and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e174" xlink:type="simple"/></inline-formula> gives the maximum input frequency, corresponding to a sample containing a single bar in the original bars problem.</p>
<p>When using the correlation-based encoding, all inputs had the same mean firing rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e175" xlink:type="simple"/></inline-formula>). Inputs corresponding to pixels in the background were uncorrelated, while inputs belonging to bars were all pairwise correlated, with a correlation coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e176" xlink:type="simple"/></inline-formula>. Poisson processes with such correlation structure can be generated in a computationally efficient fashion by using dichotomous Gaussian distributions <xref ref-type="bibr" rid="pcbi.1000757-Macke1">[46]</xref>.</p>
<p>When learning Gabor receptive fields, images from the van Hateren database <xref ref-type="bibr" rid="pcbi.1000757-vanHateren1">[31]</xref> were convolved with a difference-of-gaussians filter with center and surround widths of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e177" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e178" xlink:type="simple"/></inline-formula> pixels, respectively. Random patches of size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e179" xlink:type="simple"/></inline-formula> were selected from various positions in the images. Patches having very low contrast were discarded. The individual input patches were normalized to zero mean and unit variance, similar to the processing in <xref ref-type="bibr" rid="pcbi.1000757-Butko1">[45]</xref>. The rectified values of the resulting image were mapped into a firing frequency for an on- and off-input population (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e180" xlink:type="simple"/></inline-formula>) and, as before, samples were presented for a duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e181" xlink:type="simple"/></inline-formula>.</p>
<p>For a neuronal population, input-related parameters were as for the single component, but with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e182" xlink:type="simple"/></inline-formula>, to speed up learning. The initial parameters of the neuron transfer function were uniformly distributed around the default values mentioned above, with variance 0.1, 5, and 0.2 for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e183" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e184" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e185" xlink:type="simple"/></inline-formula>, respectively. Additionally, the inhibitory weights were initialized at random, with no self-connections, and normalized as described before. The mutual information (MI), estimated within a window of 1000 s, was computed as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e186" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000757.e187" xlink:type="simple"/></inline-formula> denoting the entropy (see <xref ref-type="bibr" rid="pcbi.1000757-Butko1">[45]</xref>), applied for the average firing rate of the neurons for each input sample.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000757.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>Receptive field estimation for the spike-based demixing problem</p>
<p>(0.04 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000757.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.s002" xlink:type="simple"><label>Text S2</label><caption>
<p>Parameter dependence for learning one IC</p>
<p>(0.07 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000757.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.s003" xlink:type="simple"><label>Text S3</label><caption>
<p>Learning with a two-dimensional input</p>
<p>(0.53 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000757.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000757.s004" xlink:type="simple"><label>Text S4</label><caption>
<p>A link to BCM</p>
<p>(0.15 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Jörg Lücke and Claudia Clopath and for fruitful discussions. Additionally, we thank Jörg Lücke for providing the code for the Gabor l.m.s. fit and to Felipe Gerhard for his help with the natural images preprocessing.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000757-Barlow1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name>
</person-group>             <year>2001</year>             <article-title>Redundancy reduction revisited.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>241</fpage>             <lpage>253</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Simoncelli1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
</person-group>             <year>2001</year>             <article-title>Natural image statistics and neural representations.</article-title>             <source>Annu Rev Neuroscience</source>             <volume>24</volume>             <fpage>1193</fpage>             <lpage>1216</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Simoncelli2"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name>
</person-group>             <year>2003</year>             <article-title>Vision and the statistics of the visual environment.</article-title>             <source>Curr Op Neurobiol</source>             <volume>13</volume>             <fpage>144</fpage>             <lpage>149</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Olshausen1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>D</given-names></name>
</person-group>             <year>2004</year>             <article-title>Sparse coding of sensory inputs.</article-title>             <source>Curr Op Neurobiol</source>             <volume>14</volume>             <fpage>481</fpage>             <lpage>487</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Hyvrinen1"><label>5</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Karhunen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Oja</surname><given-names>E</given-names></name>
</person-group>             <year>2001</year>             <article-title>Independent Component Analysis.</article-title>             <comment>Wiley Series on Adaptive and Learning Systems for Signal Processing, Communication and Control</comment>          </element-citation></ref>
<ref id="pcbi.1000757-Klampfl1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Klampfl</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2009</year>             <article-title>Spiking neurons can learn to solve information bottleneck problems and extract independent components.</article-title>             <source>Neural Computation</source>             <volume>21</volume>             <fpage>911</fpage>             <lpage>959</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Parra1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Parra</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Beck</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bell</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>On the maximization of information flow between spiking neurons.</article-title>             <source>Neural Comp</source>             <volume>21</volume>             <fpage>1</fpage>             <lpage>19</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Clopath1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Büsing</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2010</year>             <article-title>Connectivity reflects coding: a model of voltage-based stdp with homeostasis.</article-title>             <source>Nature Neuroscience</source>          </element-citation></ref>
<ref id="pcbi.1000757-Olshausen2"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>D</given-names></name>
</person-group>             <year>1997</year>             <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title>             <source>Vision Research</source>             <volume>37</volume>             <fpage>3311</fpage>             <lpage>3325</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Olshausen3"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Field</surname><given-names>D</given-names></name>
</person-group>             <year>1996</year>             <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>607</fpage>             <lpage>609</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Lennie1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lennie</surname><given-names>P</given-names></name>
</person-group>             <year>2003</year>             <article-title>The cost of neural computation.</article-title>             <source>Current Biology</source>             <volume>13</volume>             <fpage>493</fpage>             <lpage>497</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Baddeley1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baddeley</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Booth</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sengpiel</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Freeman</surname><given-names>T</given-names></name>
<etal/></person-group>             <year>1997</year>             <article-title>Responses of neurons in primary and inferior temporal visual cortices to natural scenes.</article-title>             <source>Proceedings Biological Sciences</source>             <volume>264</volume>             <fpage>1775</fpage>             <lpage>1783</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Stemmler1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stemmler</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>
</person-group>             <year>1999</year>             <article-title>How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate.</article-title>             <source>Nature Neuroscience</source>             <volume>2</volume>             <fpage>521</fpage>             <lpage>527</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Zhang1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zhang</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Linden</surname><given-names>D</given-names></name>
</person-group>             <year>2003</year>             <article-title>The other side of the engram: experience-dependent changes in neuronal intrinsic excitability.</article-title>             <source>Nature Reviews Neuroscience</source>             <volume>4</volume>             <fpage>885</fpage>             <lpage>900</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Cudmore1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cudmore</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Turrigiano</surname><given-names>G</given-names></name>
</person-group>             <year>2004</year>             <article-title>Long-term potentiation of intrinsic excitability in LV visual cortical neurons.</article-title>             <source>J Neurophysiol</source>             <volume>92</volume>             <fpage>341</fpage>             <lpage>348</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Triesch1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <article-title>Synergies between intrinsic and synaptic plasticity mechanisms.</article-title>             <source>Neural Computation</source>             <volume>19</volume>             <fpage>885</fpage>             <lpage>909</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Gerstner1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name>
<name name-style="western"><surname>van Hemmen</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Wagner</surname><given-names>H</given-names></name>
</person-group>             <year>1996</year>             <article-title>A neuronal learning rule for sub-millisecond temporal coding.</article-title>             <source>Nature</source>             <volume>383</volume>             <fpage>76</fpage>             <lpage>78</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Markram1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Markram</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Lübke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Sackmann</surname><given-names>B</given-names></name>
</person-group>             <year>1997</year>             <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APS and EPSPS.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>213</fpage>             <lpage>215</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Bi1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bi</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Poo</surname><given-names>M</given-names></name>
</person-group>             <year>1998</year>             <article-title>Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type.</article-title>             <source>J Neurosci</source>             <volume>18</volume>             <fpage>10464</fpage>             <lpage>10472</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Turrigiano1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Turrigiano</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name>
</person-group>             <year>2004</year>             <article-title>Homeostatic plasticity in the developing nervous system.</article-title>             <source>Nature Reviews Neuroscience</source>             <volume>5</volume>             <fpage>97</fpage>             <lpage>107</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Foldik1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foldiák</surname><given-names>P</given-names></name>
</person-group>             <year>1990</year>             <article-title>Forming sparse representations by local anti-Hebbian learning.</article-title>             <source>Biological Cybernetics</source>             <volume>64</volume>             <fpage>165</fpage>             <lpage>170</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Bienenstock1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bienenstock</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Cooper</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Munro</surname><given-names>P</given-names></name>
</person-group>             <year>1982</year>             <article-title>Theory for the development of neuron selectivity: Orientation specificity and binocular interactions in visual cortex.</article-title>             <source>Journal of Neuroscience</source>             <volume>2</volume>             <fpage>32</fpage>             <lpage>48</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Toyoizumi1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Pfister</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Aihara</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2005</year>             <article-title>Generalized Bienenstock-Cooper-Munro rule for spiking neurons that maximizes information transmission.</article-title>             <source>PNAS</source>             <volume>102</volume>             <fpage>5239</fpage>             <lpage>5244</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Izhikevich1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Izhikevich</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Desai</surname><given-names>N</given-names></name>
</person-group>             <year>2003</year>             <article-title>Relating STDP to BCM.</article-title>             <source>Neural Computation</source>             <volume>15</volume>             <fpage>1511</fpage>             <lpage>1523</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-MartinezConde1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Martinez-Conde</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Macknik</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name>
</person-group>             <year>2004</year>             <article-title>The role of fixational eye movements in visual perception.</article-title>             <source>Nature Reviews Neuroscience</source>             <volume>5</volume>             <fpage>229</fpage>             <lpage>240</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Song1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Song</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Miller</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name>
</person-group>             <year>2000</year>             <article-title>Competitive Hebbian learning through spike-timing-dependent synaptic plasticity.</article-title>             <source>Nature Neurosci</source>             <volume>3</volume>             <fpage>919</fpage>             <lpage>926</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Pfister1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pfister</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2006</year>             <article-title>Triplets of spikes in a model of spike timing-dependent plasticity.</article-title>             <source>Journal of Neuroscience</source>             <volume>26</volume>             <fpage>9673</fpage>             <lpage>9682</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Lcke1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lücke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Maximal causes for non-linear component extraction.</article-title>             <source>Journal of Machine Learning Research</source>             <volume>9</volume>             <fpage>1227</fpage>             <lpage>1267</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Bell1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bell</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>1997</year>             <article-title>The independent components of natural scenes are edge filters.</article-title>             <source>Vision Research</source>             <volume>37</volume>             <fpage>3327</fpage>             <lpage>3338</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Hyvrinen2"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
</person-group>             <year>1999</year>             <article-title>Survey on Independent Component Analysis.</article-title>             <source>Neural Computing Surveys</source>             <volume>2</volume>             <fpage>94</fpage>             <lpage>128</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-vanHateren1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>van Hateren</surname><given-names>J</given-names></name>
</person-group>             <year>1998</year>             <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex.</article-title>             <source>Proceedings of the Royal Society B: Biological Sciences</source>             <volume>265</volume>             <fpage>359</fpage>             <lpage>366</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Falconbridge1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Falconbridge</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Stamps</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Badcock</surname><given-names>D</given-names></name>
</person-group>             <year>2006</year>             <article-title>A simple Hebbian/anti-Hebbian network learns the sparse, independent components of natural images.</article-title>             <source>Neural Computation</source>             <volume>18</volume>             <fpage>415</fpage>             <lpage>429</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Weber1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Weber</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name>
</person-group>             <year>2008</year>             <article-title>A sparse generative model of V1 simple cells with intrinsic plasticity.</article-title>             <source>Neural Computation</source>             <volume>20</volume>             <fpage>1261</fpage>             <lpage>1284</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Lcke2"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lücke</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Receptive field self-organization in a model of the fine-structure in V1 cortical columns.</article-title>             <source>Neural Computation</source>             <volume>21</volume>             <fpage>2805</fpage>             <lpage>2845</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Cox1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cox</surname><given-names>D</given-names></name>
</person-group>             <year>1955</year>             <article-title>Some statistical methods connected with series of events.</article-title>             <source>Journal of the Royal Statistical Society, Series B</source>             <volume>17</volume>             <fpage>129</fpage>             <lpage>164</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Intrator1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Intrator</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Cooper</surname><given-names>L</given-names></name>
</person-group>             <year>1992</year>             <article-title>Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions.</article-title>             <source>Neural Networks</source>             <volume>5</volume>             <fpage>3</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Intrator2"><label>37</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Intrator</surname><given-names>N</given-names></name>
</person-group>             <collab xlink:type="simple">Springer-Verlag, editor</collab>             <year>1998</year>             <article-title>Neuronal goals: efficient coding and coincidence detection.</article-title>             <source>ICONIP Hong Kong: Progress in Neural Information Processing</source>             <fpage>29</fpage>             <lpage>34</lpage>             <comment>volume 1</comment>          </element-citation></ref>
<ref id="pcbi.1000757-Blais1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Blais</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Intrator</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Shouval</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Cooper</surname><given-names>L</given-names></name>
</person-group>             <year>1998</year>             <article-title>Receptive field formation in natural scene environments: comparison of single cell learning rules.</article-title>             <source>Neural Computation</source>             <volume>10</volume>             <fpage>1797</fpage>             <lpage>1813</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Huber1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Huber</surname><given-names>P</given-names></name>
</person-group>             <year>1985</year>             <article-title>Projection pursuit.</article-title>             <source>The Annals of Statistics</source>             <volume>13</volume>             <fpage>435</fpage>             <lpage>475</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Hyvrinen3"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>
</person-group>             <year>1997</year>             <article-title>One-unit contrast functions for independent component analysis: a statistical analysis.</article-title>             <source>Neural Networks for Signal Processing</source>             <fpage>388</fpage>             <lpage>397</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Oja1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Oja</surname><given-names>E</given-names></name>
</person-group>             <year>1997</year>             <article-title>The nonlinear PCA learning rule in independent component analysis.</article-title>             <source>Neurocomputing</source>             <volume>17</volume>             <fpage>25</fpage>             <lpage>46</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Joshi1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joshi</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name>
</person-group>             <year>2009</year>             <article-title>Rules for information-maximization in spiking neurons using intrinsic plasticity.</article-title>             <source>Proc. IJCNN</source>             <fpage>1456</fpage>             <lpage>1461</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Kapur1"><label>43</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kapur</surname><given-names>N</given-names></name>
</person-group>             <year>1990</year>             <source>Maximum Entropy Models in Science and Engineering</source>             <publisher-name>Wiley-Interscience</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000757-Olshausen4"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name>
</person-group>             <year>2001</year>             <article-title>Natural image statistics and neural representation.</article-title>             <source>Annu Rev Neuroscience</source>             <volume>24</volume>             <fpage>1193</fpage>             <lpage>1216</lpage>          </element-citation></ref>
<ref id="pcbi.1000757-Butko1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Butko</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <article-title>Learning sensory representations with intrinsic plasticity.</article-title>             <source>Neurocomputing</source>             <volume>70</volume>          </element-citation></ref>
<ref id="pcbi.1000757-Macke1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Macke</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Berens</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Ecker</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Tolias</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>Generating spike trains with specified correlation coefficients.</article-title>             <source>Neural Comp</source>             <volume>21</volume>             <fpage>397</fpage>             <lpage>423</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>