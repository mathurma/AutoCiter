<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01709</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002594</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Auditory system</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Sparse Codes for Speech Predict Spectrotemporal Receptive Fields in the Inferior Colliculus</article-title><alt-title alt-title-type="running-head">Sparse Coding Predicts IC Receptive Fields</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Carlson</surname>
            <given-names>Nicole L.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Ming</surname>
            <given-names>Vivienne L.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>DeWeese</surname>
            <given-names>Michael Robert</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Redwood Center for Theoretical Neuroscience, University of California, Berkeley, California, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Physics, University of California, Berkeley, California, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Helen Wills Neuroscience Institute, University of California, Berkeley, California, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Behrens</surname>
            <given-names>Tim</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University of Oxford, United Kingdom</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">deweese@berkeley.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: NLC MRD. Performed the experiments: NLC. Analyzed the data: NLC VLM MRD. Wrote the paper: NLC MRD.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>7</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>12</day>
        <month>7</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>7</issue><elocation-id>e1002594</elocation-id><history>
        <date date-type="received">
          <day>16</day>
          <month>11</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>18</day>
          <month>5</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Carlson et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>We have developed a sparse mathematical representation of speech that minimizes the number of active model neurons needed to represent typical speech sounds. The model learns several well-known acoustic features of speech such as harmonic stacks, formants, onsets and terminations, but we also find more exotic structures in the spectrogram representation of sound such as localized checkerboard patterns and frequency-modulated excitatory subregions flanked by suppressive sidebands. Moreover, several of these novel features resemble neuronal receptive fields reported in the Inferior Colliculus (IC), as well as auditory thalamus and cortex, and our model neurons exhibit the same tradeoff in spectrotemporal resolution as has been observed in IC. To our knowledge, this is the first demonstration that receptive fields of neurons in the ascending mammalian auditory pathway beyond the auditory nerve can be predicted based on coding principles and the statistical properties of recorded sounds.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>The receptive field of a neuron can be thought of as the stimulus that most strongly causes it to be active. Scientists have long been interested in discovering the underlying principles that determine the structure of receptive fields of cells in the auditory pathway to better understand how our brains process sound. One possible way of predicting these receptive fields is by using a theoretical model such as a sparse coding model. In such a model, each sound is represented by the smallest possible number of active model neurons chosen from a much larger group. A primary question addressed in this study is whether the receptive fields of model neurons optimized for natural sounds will predict receptive fields of actual neurons. Here, we use a sparse coding model on speech data. We find that our model neurons do predict receptive fields of auditory neurons, specifically in the Inferior Colliculus (midbrain) as well as the thalamus and cortex. To our knowledge, this is the first time any theoretical model has been able to predict so many of the diverse receptive fields of the various cell-types in those areas.</p>
      </abstract><funding-group><funding-statement>NLC was supported by a National Science Foundation graduate fellowship; MRD gratefully acknowledges support from the National Science Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link>), the McKnight Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.mcknight.org" xlink:type="simple">www.mcknight.org</ext-link>), the McDonnell Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.jsmf.org" xlink:type="simple">www.jsmf.org</ext-link>), and the Hellman Family Faculty Fund. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="15"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Our remarkable ability to interpret the highly structured sounds in our everyday environment suggests that auditory processing in the brain is somehow specialized for natural sounds. Many authors have postulated that the brain tries to transmit and encode information efficiently, so as to minimize the energy expended <xref ref-type="bibr" rid="pcbi.1002594-Laughlin1">[1]</xref>, reduce redundancy <xref ref-type="bibr" rid="pcbi.1002594-Attneave1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1002594-Atick1">[4]</xref>, maximize information flow <xref ref-type="bibr" rid="pcbi.1002594-Laughlin2">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1002594-Zhao1">[8]</xref>, or facilitate computations at later stages of processing <xref ref-type="bibr" rid="pcbi.1002594-Fldik1">[9]</xref>, among other possible objectives. One way to create an efficient code is to enforce population sparseness, having only a few active neurons at a time. Sparse coding schemes pick out the statistically important features of a signal — those features that occur much more often than chance — which can then be used to efficiently represent a complex signal with few active neurons.</p>
      <p>The principle of sparse coding has led to important insights into the neural encoding of visual scenes within the primary visual cortex (V1). Sparse coding of natural images revealed local, oriented edge-detectors that qualitatively match the receptive fields of simple cells in V1 <xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[10]</xref>. More recently, overcomplete sparse coding schemes have uncovered a greater diversity of features that more closely matches the full range of simple cell receptive field shapes found in V1 <xref ref-type="bibr" rid="pcbi.1002594-Rehn1">[11]</xref>. An encoding is called overcomplete if the number of neurons available to represent the stimulus is larger than the dimensionality of the input. This is a biologically realistic property for a model of sensory processing because information is encoded by increasing numbers of neurons as it travels from the optic nerve to higher stages in the visual pathway, just as auditory sensory information is encoded by increasing numbers of neurons as it travels from the auditory nerve to higher processing stages <xref ref-type="bibr" rid="pcbi.1002594-DeWeese2">[12]</xref>.</p>
      <p>Despite experimental evidence for sparse coding in the auditory system <xref ref-type="bibr" rid="pcbi.1002594-DeWeese3">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Hromdka1">[14]</xref>, there have been fewer theoretical sparse coding studies in audition than in vision. However, there has been progress, particularly for the earliest stages of auditory processing. Sparse coding of raw sound pressure level waveforms of natural sounds produced a “dictionary” of acoustic filters closely resembling the impulse response functions of auditory nerve fibers <xref ref-type="bibr" rid="pcbi.1002594-Lewicki1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Smith1">[16]</xref>. Acoustic features learned by this model were best fit to the neural data for a particular combination of animal vocalizations and two subclasses of environmental sounds. Intriguingly, they found that training on speech alone produced features that were just as well-fit to the neural data as the optimal combination of natural sounds, suggesting that speech provides the right mixture of acoustic features for probing and predicting the properties of the mammalian auditory system.</p>
      <p>Another pioneering sparse coding study <xref ref-type="bibr" rid="pcbi.1002594-Klein1">[17]</xref> took as its starting point speech that was first preprocessed using a model of the cochlea — one of several so-called cochleogram representations of sound. This group found relatively simple acoustic features that were fairly localized in time and frequency as well as some temporally localized harmonic stacks. These results were roughly consistent with some properties of receptive fields in primary auditory cortex (A1), but modeled responses did not capture the majority of the specific shapes of neuronal spectrotemporal receptive fields (STRFs; <xref ref-type="bibr" rid="pcbi.1002594-Aertsen1">[18]</xref>) reported in the literature. That study only considered undercomplete dictionaries, and it focused solely on a “soft” sparse coding model that minimized the mean activity of the model's neurons, as opposed to “hard” sparse models that minimize the number of active neurons.</p>
      <p>The same group also considered undercomplete, soft sparse coding of spectrograms of speech <xref ref-type="bibr" rid="pcbi.1002594-Krding1">[19]</xref>, which did yield some STRFs showing multiple subfields and temporally modulated harmonic stacks, but the range of STRF shapes they reported was still modest compared with what has been seen experimentally in auditory midbrain, thalamus, or cortex. Another recent study considered sparse coding of music <xref ref-type="bibr" rid="pcbi.1002594-Henaff1">[20]</xref> in order to develop automated genre classifiers.</p>
      <p>To our knowledge, there are no published studies of complete or overcomplete, sparse coding of either spectrograms or cochleograms of speech or natural sounds. We note that one preliminary sparse coding study utilizing a complete dictionary trained on spectrograms did find STRFs resembling formants, onset-sensitive neurons, and harmonic stacks (J. Wang, B.A. Olshausen, and V.L. Ming, COSYNE 2008) but they did not obtain novel acoustic features, nor any that closely resembled STRFs from the auditory system.</p>
      <p>Our goal is two-fold. First, we test whether an overcomplete, hard sparse coding model trained on spectrograms of speech can more fully reveal the structure of natural sounds than previous models. Second, we ask whether our model can accurately predict receptive fields in the ascending auditory pathway beyond the auditory nerve. We have found that, when trained on spectrograms of human speech, an overcomplete, hard sparse coding model does learn features resembling those of STRF shapes previously reported in the inferior colliculus (IC), as well as auditory thalamus and cortex. Moreover, our model exhibits a similar tradeoff in spectrotemporal resolution as previously reported in IC. Finally, our model has identified novel acoustic features for probing the response properties of neurons in the auditory pathway that have thus far resisted classification and meaningful analysis.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Sparse Coding Model of Speech</title>
        <p>In order to uncover important acoustic features that can inform us about how the nervous system processes natural sounds, we have developed a sparse coding model of human speech (see <xref ref-type="sec" rid="s4">Methods</xref> for details). As illustrated in <xref ref-type="fig" rid="pcbi-1002594-g001"><bold>Fig. 1</bold></xref>, raw sound pressure level waveforms of recorded speech were first preprocessed by one of two simple models of the peripheral auditory system. The first of these preprocessing models was the spectrogram, which can be thought of as the power spectrum of short segments of the original waveform at each moment in time. We also explored an alternative preprocessing step that was meant to more accurately model the cochlea <xref ref-type="bibr" rid="pcbi.1002594-Lyon1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Slaney1">[22]</xref>; the original waveform was sent through a filter bank with center frequencies based on the properties of cochlear nerve fibers. Both models produced representations of the waveform as power at different frequencies over time. The spectrograms (cochleograms) were then separated into segments of length 216 ms (250 ms). Because of the high dimensionality of these training examples, we performed principal components analysis (PCA) and retained only the first two hundred components to reduce the dimensionality (from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e001" xlink:type="simple"/></inline-formula> values down to 200), as was done previously in some visual <xref ref-type="bibr" rid="pcbi.1002594-vanHateren1">[23]</xref> and auditory <xref ref-type="bibr" rid="pcbi.1002594-Klein1">[17]</xref> sparse coding studies; the latter group also performed the control of repeating their analysis without the PCA step and they found that their results did not change.</p>
        <fig id="pcbi-1002594-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Schematic illustration of our sparse coding model.</title>
            <p>(<bold>a</bold>) Stimuli used to train the model consisted of examples of recorded speech. The blue curve represents the raw sound pressure waveform of a woman saying, “The north wind and the sun were disputing which was the stronger, when a traveler came along wrapped in a warm cloak.” (<bold>b</bold>) The raw waveforms were first put through one of two preprocessing steps meant to model the earliest stages of auditory processing to produce either a spectrogram or a “cochleogram” (not shown; see <xref ref-type="sec" rid="s4">Methods</xref> for details). In either case, the power spectrum across acoustic frequencies is displayed as a function of time, with warmer colors indicating high power content and cooler colors indicating low power. (<bold>c</bold>) The spectrograms were then divided into overlapping 216 ms segments. (<bold>d</bold>) Subsequently, principal components analysis (PCA) was used to project each segment onto the space of the first two hundred principal components (first ten shown), in order to reduce the dimensionality of the data to make it tractable for further analysis while retaining its basic structure <xref ref-type="bibr" rid="pcbi.1002594-Klein1">[17]</xref>. (<bold>e</bold>) These projections were then input to a sparse coding network in order to learn a “dictionary” of basis elements analogous to neuronal receptive fields, which can then be used to form a representation of any given stimulus (<italic>i.e.</italic>, to perform inference). We explored networks capable of learning either “hard” (L0) sparse dictionaries or “soft” (L1) sparse dictionaries (described in the text and <xref ref-type="sec" rid="s4">Methods</xref>) that were undercomplete (fewer dictionary elements than PCA components), complete (equal number of dictionary elements), or over-complete (greater number of dictionary elements).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g001" xlink:type="simple"/>
        </fig>
        <p>We then trained a “dictionary” of model neurons that could encode this data using the Locally Competitive Algorithm (LCA), a recently developed sparse encoding algorithm <xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[24]</xref>. This flexible algorithm allowed us to approximately enforce either the so-called “hard” sparseness (L0 sparseness; minimizing the number of simultaneously active model neurons) or “soft” sparseness (L1 sparseness; minimizing the sum of all simultaneous activity across all model neurons) during encoding by our choice of thresholding function. Additionally, we explored the effect of dictionary overcompleteness (with respect to the number of principal components) by training dictionaries that were half-complete, complete, or overcomplete (two or four times). Following training, the various resulting dictionaries were analyzed for cell-types and compared to experimental receptive fields reported in the literature.</p>
      </sec>
      <sec id="s2b">
        <title>Cochleogram-Trained Models</title>
        <p>In general, training our network on cochleogram representations of speech resulted in smooth and simple shapes for the learned receptive fields of model neurons. Klein and colleagues <xref ref-type="bibr" rid="pcbi.1002594-Klein1">[17]</xref> used a sparse coding algorithm that imposed an L1-like sparseness constraint to learn a half-complete dictionary of cochleograms. Their dictionary elements consisted of harmonic stacks at the lower frequencies and localized elements at the higher frequencies. To make contact with these results, we trained a half-complete L0-sparse dictionary on cochleograms and compared the response properties of our model neurons with those of the previous study. The resulting dictionary (<xref ref-type="fig" rid="pcbi-1002594-g002"><bold>Fig. 2</bold></xref>) consists of similar shapes to this previous work with the exception of one “onset element” in the upper left (this is the least used of all of the elements from this dictionary). Subsequent simulations revealed that the form of the dictionary is strongly dependent on the degree of overcompleteness. Even a complete dictionary exhibits a greater diversity of shapes than this half-complete dictionary (<bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref></bold>). This was true for L1-sparse dictionaries trained with LCA <xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[24]</xref> or with Sparsenet <xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[10]</xref> (<bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s015">Figs. S15</xref></bold> and <bold>S19</bold>).</p>
        <fig id="pcbi-1002594-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>A half-complete sparse coding dictionary trained on cochleogram representations of speech.</title>
            <p>This dictionary exhibits a limited range of shapes. The full set of 100 elements from a half-complete, L0-sparse dictionary trained on cochleograms of human speech resemble those found in a previous study <xref ref-type="bibr" rid="pcbi.1002594-Klein1">[17]</xref>. Nearly all elements are extremely smooth, with most consisting of a single frequency subfield or an unmodulated harmonic stack. Each rectangle can be thought of as representing the spectro-temporal receptive field (STRF) of a single element in the dictionary (see <xref ref-type="sec" rid="s4">Methods</xref> for details); time is plotted along the horizontal axis (from 0 to 250 ms), and log frequency is plotted along the vertical axis, with frequencies ranging from 73 Hz to 7630 Hz. Color indicates the amount of power present at each frequency at each moment in time, with warm colors representing high power and cool colors representing low power. Each element has been normalized to have unit Euclidean length. Elements are arranged in order of their usage during inference (<italic>i.e.</italic>, when used to represent individual sounds drawn from the training set) with usage increasing from left to right along each row, and all elements of lower rows used more than those of higher rows.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g002" xlink:type="simple"/>
        </fig>
        <p>The inability of the half-complete dictionary to produce the more complex receptive field shapes of the complete dictionary, such as those resembling STRFs measured in IC, or those in auditory thalamus or cortex, suggests that overcompleteness in those regions is crucial to the flexibility of their auditory codes.</p>
      </sec>
      <sec id="s2c">
        <title>Spectrogram-Trained Models</title>
        <p>The spectrogram-trained dictionaries provide a much richer and more diverse set of dictionary elements than those trained on cochleograms. We display representative elements of the different categories of shapes found in a half-complete L0-sparse spectrogram dictionary (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3a–f</bold></xref>) along with a histogram of the usage of the elements (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3g</bold></xref>) when used to represent individual sounds drawn from the training set (<italic>i.e.</italic>, during inference). Interestingly, we find that the different qualitative types of neurons separate according to their usage into a series of rises and plateaus. The least used elements are the harmonic stacks (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3a</bold></xref>), which is perhaps unsurprising since, in principle, only one of them needs to be active at many points in time for a typical epoch of a recording from a single human speaker. We note that, while such harmonic stack receptive fields are apparently rare in the colliculus, thalamus, and cortex, they are well represented in the dorsal cochlear nucleus (DCN) (e.g., see <xref ref-type="fig" rid="pcbi-1002594-g005">Fig. 5b</xref> in <xref ref-type="bibr" rid="pcbi.1002594-Backoff1">[25]</xref>). The neighboring flat region consists of onset elements (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3b</bold></xref>), which contain broad frequency subfields that change abruptly at one moment in time. These neurons were all used approximately equally often across the training set since it is equally probable that a stimulus transient will occur any time during the 216 ms time window.</p>
        <fig id="pcbi-1002594-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>A half-complete, L0-sparse dictionary trained on spectrograms of speech.</title>
            <p>This dictionary exhibits a variety of distinct shapes that capture several classes of acoustic features present in speech and other natural sounds. (<bold>a–f</bold>) Selected elements from the dictionary that are representative of different types of receptive fields: (<bold>a</bold>) a harmonic stack; (<bold>b</bold>) an onset element; (<bold>c</bold>) a harmonic stack with flanking suppression; (<bold>d</bold>) a more localized onset/termination element; (<bold>e</bold>) a formant; (<bold>f</bold>) a tight checkerboard pattern (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref></bold> for the full dictionary). Each rectangle represents the spectro-temporal receptive field (STRF) of a single element in the dictionary; time is plotted along the horizontal axis (from 0 to 216 msec) and log frequency is plotted along the vertical axis, with frequencies ranging from 100 Hz to 4000 Hz. (<bold>g</bold>) A graph of the usage of the dictionary elements showing that the different types of receptive field shapes separate based on usage into a series of rises and plateaus; red symbols indicate where each of the examples from panels <bold>a–f</bold> fall on the graph. The vertical axis represents the number of stimuli that required a given dictionary element in order to be represented accurately during inference.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g003" xlink:type="simple"/>
        </fig>
        <p>The third region consists of more complex harmonic stacks that contain low-power subfields on the sides (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3c</bold></xref>), a feature sometimes referred to as “temporal inhibition” or “band-passed inhibition” when observed in neural receptive fields; we will refer to this as “suppression” rather than inhibition to indicate that the model is agnostic as to whether these suppressed regions reflect direct synaptic inhibition to the neuron, rather than a decrease in excitatory synaptic input. The next flat region represents stimulus onsets, or ON-type cells, that tend to be more localized in frequency (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3d</bold></xref>). The fifth group of elements is reminiscent of formants (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3e</bold></xref>), which are resonances of the vocal tract that appear as characteristic frequency modulations common in speech. Formants are modulations “on top of” the underlying harmonic stack, often consisting of pairs of subfields that diverge or converge over time in a manner that is not consistent with a pair of harmonics rising or falling together due to fluctuations in the fundamental frequency of the speaker's voice. The final region consists of the most active neurons, which are highly localized in time and frequency and exhibit tight checkerboard-like patterns of excitatory and suppressive subfields (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3f</bold></xref>). These features are exciting because they are similar to experimentally measured receptive field shapes that to our knowledge have not previously been theoretically predicted, as discussed below.</p>
      </sec>
      <sec id="s2d">
        <title>Overcompleteness Affects Learned Features</title>
        <p>Analogous to sparse coding studies in vision <xref ref-type="bibr" rid="pcbi.1002594-Rehn1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Olshausen2">[26]</xref>, we find that the degree of overcompleteness strongly influences the range and complexity of model STRF shapes.</p>
        <p><xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4</bold></xref> presents representative examples of essentially all distinct cell types found in a four-times overcomplete L0-sparse dictionary trained on spectrograms. Features in the half-complete dictionary do appear as subsets of the larger dictionaries (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4a, c, e, g, l</bold></xref>), but with increasing overcompleteness more complex features emerge, exhibiting richer patterns of excitatory and suppressive subfields. In general, optimized overcomplete representations can better capture structured data with fewer active elements, since the greater number of elements allows for important stimulus features to be explicitly represented by dedicated elements. In the limit of an infinite dictionary, for example, each element could be used as a so-called “grandmother cell” that perfectly represents a single, specific stimulus while all other elements are inactive.</p>
        <fig id="pcbi-1002594-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>A four-times overcomplete, L0-sparse dictionary trained on speech spectrograms.</title>
            <p>This dictionary shows a greater diversity of shapes than the undercomplete dictionaries. (<bold>a–l</bold>) Representative elements <bold>a</bold>, <bold>c</bold>, <bold>e</bold>, <bold>g</bold>, <bold>j</bold>, and <bold>l</bold> resemble those of the half-complete dictionary (see <xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3</bold></xref>). Other neurons display more complex shapes than those found in less overcomplete dictionaries: (<bold>b</bold>) a harmonic stack with flanking suppressive subregions; (<bold>d</bold>) a neuron sensitive to lower frequencies; (<bold>f</bold>) a short harmonic stack; (<bold>h</bold>) a localized but complex pattern of excitation with flanking suppression; (<bold>i</bold>) a localized checkerboard with larger excitatory and suppressive subregions than those in panel <bold>l</bold>; (<bold>k</bold>) a checkerboard pattern that extends for many cycles in time. Several of these patterns resemble neural spectro-temporal receptive fields (STRFs) reported in various stages of the auditory pathway that have not been predicted by previous theoretical models (see text and <xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Figs. 6</bold></xref><bold>–</bold><xref ref-type="fig" rid="pcbi-1002594-g008"><bold>8</bold></xref>). (<bold>m</bold>) A graph of usage of the dictionary elements during inference. The different classes of dictionary elements still separate according to usage (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s004">Fig. S4</xref></bold> for the full dictionary) although the notable rises and plateaus as seen in <xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3g</bold></xref> are less apparent in this larger dictionary.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g004" xlink:type="simple"/>
        </fig>
        <p>Novel features that were not observed in smaller dictionaries include: an excitatory harmonic stack flanked by a suppressive harmonic stack (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4b</bold></xref>); a neuron excited by low frequencies (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4d</bold></xref>); a neuron sensitive to two middle frequencies (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4f</bold></xref>); a localized but complex excitatory subregion followed by a suppressive subregion that is strongest for high frequencies (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4h</bold></xref>); a checkerboard pattern with roughly eight distinct subregions (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4i</bold></xref>); a highly temporally localized OFF-type neuron (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4j</bold></xref>); and a broadband checkerboard pattern that extends for many cycles in time (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4k</bold></xref>). Several of these features resemble STRFs reported in IC and further up the auditory pathway (see the “Predicting acoustic features that drive neurons in IC and later stages in the ascending auditory pathway” section below). One interesting property of the checkerboard units is that they are largely separable in space and time <xref ref-type="bibr" rid="pcbi.1002594-Depireux1">[27]</xref>, which has been studied for these and other types of neurons in ferret IC <xref ref-type="bibr" rid="pcbi.1002594-Shechtere1">[28]</xref>. This is in contrast to some of the other model STRF shapes we have found, such as the example shown in <xref ref-type="fig" rid="pcbi-1002594-g004">Fig. 4e</xref>, which contains a strong diagonal subfield that is not well described by a product of independent functions of time and frequency.</p>
        <p>As in the case of the half-complete dictionary (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3</bold></xref>), the different classes of receptive field shapes segregate as a function of usage even as more intermediary shapes appear (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s004">Fig. S4</xref></bold> for the entire four-times overcomplete dictionary). However, the plateaus and rises evident in the usage plot for the half-complete dictionary (<xref ref-type="fig" rid="pcbi-1002594-g003"><bold>Fig. 3g</bold></xref>) are far less distinct for the overcomplete representation (<xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4m</bold></xref>).</p>
        <p>These same trends are present in the cochleogram-trained dictionaries. More types of STRFs appear when the degree of overcompleteness is increased (<bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Figs. S11</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002594.s012">S12</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002594.s013">S13</xref></bold>). For example, with more overcomplete dictionaries, some neurons have subfields spanning all frequencies or the full time-window within the cochleogram inputs. Additionally, we find neurons that exhibit both excitation and suppression in complex patterns, though the detailed shapes differ from what we find for the dictionaries trained on spectrograms.</p>
        <p>We wondered to what extent the specific form of sparseness we imposed on the representation was affecting the particular features learned by our network. To study this, we used the LCA algorithm <xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[24]</xref> to find the soft sparse solution (<italic>i.e.</italic>, one that minimizes the L1 norm), and obtained similar results to what we found for the hard sparse cases: increasing overcompleteness resulted in greater diversity and complexity of learned features (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s005">Figs. S5</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002594.s006">S6</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002594.s007">S7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002594.s008">S8</xref></bold>). We also trained some networks using a different algorithm, called Sparsenet <xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[10]</xref>, for producing soft sparse dictionaries, and we again obtained similar results as for our hard sparse dictionaries (<bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s009">Figs. S9</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002594.s010">S10</xref></bold>). It has been proven mathematically <xref ref-type="bibr" rid="pcbi.1002594-Donoho1">[29]</xref> that signals that are actually L0-sparse can be uncovered effectively by L1-sparse coding algorithms, which suggests that speech is an L0-sparse signal given that we find similar features using algorithms designed to achieve either L1 or L0 sparseness. Thus, preprocessing with spectrograms rather than a more nuanced cochlear model, and the degree of overcompleteness, greatly influenced the learned dictionaries, unlike the different sparseness penalties we employed.</p>
        <p>The specific form of the sparseness penalty did, however, affect the performance of the various dictionaries. In particular, the level of sparseness achieved across the population of model neurons exhibited different relationships with the fidelity of their representations, suggesting that some model choices resulted in population codes that were more efficient at using small numbers of neurons to represent stimuli efficiently, while others were more effective at increasing their representational power when incorporating more active neurons (<bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s020">Fig. S20</xref></bold>).</p>
      </sec>
      <sec id="s2e">
        <title>Modulation Power Spectra</title>
        <p>Our four-times overcomplete, spectrogram-trained dictionary exhibits a clear tradeoff in spectrotemporal resolution (red points, <xref ref-type="fig" rid="pcbi-1002594-g005"><bold>Fig. 5</bold></xref>), similar to what has been found experimentally in IC <xref ref-type="bibr" rid="pcbi.1002594-Rodrguez1">[30]</xref>. IC is the lowest stage in the ascending auditory pathway to exhibit such a tradeoff, but it has yet to be determined for higher stages of processing, such as A1. This trend is not present in the half-complete cochleogram-trained dictionary (blue open circles, <xref ref-type="fig" rid="pcbi-1002594-g005"><bold>Fig. 5</bold></xref>). Rather, these elements display a limited range of temporal modulations, but they span nearly the full range of possible spectral modulations. Thus, by this measure the spectrogram-trained dictionary is a better model of IC than the cochleogram-trained model. In the next section, we compare the shapes of the various classes of model STRFs with individual neuronal STRFs from IC, and again find good agreement between our overcomplete spectrogram-trained model and the neural data.</p>
        <fig id="pcbi-1002594-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Our overcomplete, spectrogram-trained model exhibits similar spectrotemporal tradeoff as Inferior Coliculus.</title>
            <p>Modulation spectra of half-complete cochleogram-trained dictionary and four-times overcomplete spectroram-trained dictionary are shown. The four-times overcomplete spectrogram-trained dictionary elements (red dots; same dictionary as in <xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4</bold></xref>) display a clear tradeoff between spectral and temporal modulations, similar to what has been reported for Inferior Colliculus (IC) <xref ref-type="bibr" rid="pcbi.1002594-Rodrguez1">[30]</xref>. By contrast, the half-complete cochleogram-trained dictionary (blue circles; same dictionary as in <xref ref-type="fig" rid="pcbi-1002594-g002"><bold>Fig. 2</bold></xref>) exhibits a much more limited range of temporal modulations, with no such tradeoff in spectrotemporal resolution. Each data point represents the centroid of the modulation spectrum of the corresponding element. The elements shown in <xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4</bold></xref> are indicated on the graph with the same symbols as before.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g005" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2f">
        <title>Predicting Acoustic Features that Drive Neurons in IC and Later Stages in the Ascending Auditory Pathway</title>
        <p>Our model learns features that resemble STRFs reported in IC <xref ref-type="bibr" rid="pcbi.1002594-Rodrguez1">[30]</xref>–<xref ref-type="bibr" rid="pcbi.1002594-Qiu1">[33]</xref>, as well as in the ventral side of the medial geninculate body (MGBv) <xref ref-type="bibr" rid="pcbi.1002594-Escab1">[34]</xref> and A1 <xref ref-type="bibr" rid="pcbi.1002594-Escab1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1002594-Fritz1">[36]</xref>. We are unaware of any previous theoretical work that has provided accurate predictions for receptive fields in these areas.</p>
        <p><xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Figs. 6</bold></xref>, <xref ref-type="fig" rid="pcbi-1002594-g007"><bold>7</bold></xref>, and <xref ref-type="fig" rid="pcbi-1002594-g008"><bold>8</bold></xref> present several examples of previously reported experimental receptive fields that qualitatively match some of our model's dictionary elements. We believe the most important class of STRFs we have found are localized checkerboard patterns of excitation and suppression, which qualitatively match receptive fields of neurons in IC and MGBv (<xref ref-type="fig" rid="pcbi-1002594-g007"><bold>Fig. 7</bold></xref>).</p>
        <fig id="pcbi-1002594-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Model comparisons to receptive fields from auditory midbrain.</title>
            <p>Complete and overcomplete sparse coding models trained on spectrograms of speech predict Inferior Colliculus (IC) spectro-temporal receptive field (STRF) shapes with excitatory and suppressive subfields that are localized in frequency but separated in time. (<bold>a</bold>) Two examples of Gerbil IC neural STRFs <xref ref-type="bibr" rid="pcbi.1002594-Lesica1">[31]</xref> exhibiting ON-type response patterns with excitation following suppression; data courtesy of N.A. Lesica. (<bold>b</bold>) Representative model dictionary elements from each of three dictionaries that match this pattern of excitation and suppression. The three dictionaries were all trained on spectrogram representations of speech, using a hard sparseness (L0) penalty; the representations were complete (left column; <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s002">Fig. S2</xref></bold>), two-times overcomplete (middle column; <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s003">Fig. S3</xref></bold>), and four-times overcomplete (right column; <xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4</bold></xref> and <bold><xref ref-type="supplementary-material" rid="pcbi.1002594.s004">Fig. S4</xref></bold>). (<bold>c</bold>) Two example neuronal STRFs from cat IC <xref ref-type="bibr" rid="pcbi.1002594-Rodrguez1">[30]</xref> exhibiting OFF-type patterns with excitation preceding suppression; data courtesy of M.A. Escabí. (<bold>d</bold>) Other model neurons from the same set of three dictionaries as in panel <bold>b</bold> also exhibit this OFF-type pattern.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g006" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002594-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Model comparisons to receptive fields from auditory midbrain and thalamus.</title>
            <p>An overcomplete sparse coding model trained on spectrograms of speech predicts Inferior Colliculus (IC) and auditory thalamus (ventral division of the medial geniculate body; MGBv) spectro-temporal receptive fields (STRFs) consisting of localized checkerboard patterns containing roughly four to nine distinct subfields. (<bold>a</bold>) Example STRFs of localized checkerboard patterns from two Gerbil IC neurons <xref ref-type="bibr" rid="pcbi.1002594-Lesica1">[31]</xref>, one cat IC neuron <xref ref-type="bibr" rid="pcbi.1002594-Qiu1">[33]</xref>, and one cat MGBv neuron <xref ref-type="bibr" rid="pcbi.1002594-Escab1">[34]</xref> (top to bottom). Data courtesy of N.A. Lesica (top two cells) and M.A. Escabí (bottom two cells). (<bold>b</bold>) Elements from the four-times overcomplete, L0-sparse, spectrogram-trained dictionary with similar checkerboard patterns as the neurons in panel <bold>a</bold>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g007" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002594-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002594.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Model comparisons to receptive fields from auditory midbrain and cortex.</title>
            <p>on spectrograms of speech predicts several classes of broadband spectro-temporal receptive field (STRF) shapes found in Inferior Colliculus (IC) and primary auditory cortex (A1). (<bold>a,b</bold>) An example broadband OFF-type STRF from cat IC <xref ref-type="bibr" rid="pcbi.1002594-Escab1">[34]</xref> (top; data courtesy of M.A. Escabí) and an example broadband ON-type subthreshold STRF from rat A1 <xref ref-type="bibr" rid="pcbi.1002594-Machens1">[35]</xref> (bottom; data courtesy of M. Wehr) shown in panel <bold>a</bold> resemble example elements from a four-times overcomplete, L0-sparse, spectrogram-trained dictionary shown in panel <bold>b</bold>. (<bold>c</bold>) STRFs from a bat IC neuron <xref ref-type="bibr" rid="pcbi.1002594-Andoni1">[32]</xref> (top; data courtesy of S. Andoni) and a cat A1 neuron <xref ref-type="bibr" rid="pcbi.1002594-Escab1">[34]</xref> (bottom; data courtesy of M.A. Escabí) each consist of a primary excitatory subfield that is modulated in frequency over time, flanked by similarly angled suppressive subfields. (<bold>d</bold>) Example STRFs from four elements taken from the same dictionary as in panels <bold>b</bold> exhibit similar patterns as the neuronal STRFs in panel <bold>c</bold>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.g008" xlink:type="simple"/>
        </fig>
        <p>IC neurons often exhibit highly localized excitation and suppression patterns (<xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Fig. 6</bold></xref>), sometimes referred to as “ON” or “OFF” responses, depending on the temporal order of excitation and suppression. We show multiple examples drawn from the complete, two-times overcomplete, and four-times overcomplete dictionaries, trained on spectrograms, that exhibit these patterns. The receptive fields of two neurons recorded in gerbil IC exhibit suppression at a particular frequency followed by excitation at the same frequency (<xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Fig. 6a</bold></xref>). Such neurons are found in our model dictionaries (<xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Fig. 6b</bold></xref>). The reverse pattern is also found in which suppression follows excitation as shown in two cat IC STRFs (<xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Fig. 6c</bold></xref>) with matching examples from our model dictionaries (<xref ref-type="fig" rid="pcbi-1002594-g006"><bold>Fig. 6d</bold></xref>). Note that the experimental receptive fields extend to higher frequencies because the studies were done in cats and gerbils, which are sensitive to higher frequencies than we were probing with our human speech training set. The difference in time-scales between our spectrogram representation and the experimental STRFs could reflect the different timescales of speech and behaviorally relevant sounds for cats and rodents.</p>
        <p>A common feature of thalamic and midbrain neural receptive fields is a localized checkerboard pattern of excitation and suppression (<xref ref-type="fig" rid="pcbi-1002594-g007"><bold>Fig. 7</bold></xref>), typically containing between four to nine distinct subfields. We present experimental gerbil IC, cat IC and cat MGBv STRFs of this type in <xref ref-type="fig" rid="pcbi-1002594-g007"><bold>Fig. 7a</bold></xref> beside similar examples from our model (<xref ref-type="fig" rid="pcbi-1002594-g007"><bold>Fig. 7b</bold></xref>). This pattern is displayed by many elements in our sparse coding dictionaries, but to our knowledge it has not been predicted by previous theories.</p>
        <p>We also find some less localized receptive fields that strongly resemble experimental data. Some model neurons (<xref ref-type="fig" rid="pcbi-1002594-g008"><bold>Fig. 8b</bold></xref>) consist of a suppression/excitation pattern that extends across most frequencies, reminiscent of broadband OFF and ON responses as reported in cat IC and rat A1 (<xref ref-type="fig" rid="pcbi-1002594-g008"><bold>Fig. 8a</bold></xref>).</p>
        <p>Another shape seen in experimental STRFs of bat IC (top), and cat A1 (bottom; <xref ref-type="fig" rid="pcbi-1002594-g008"><bold>Fig. 8c</bold></xref>) is a diagonal pattern of excitation flanked by suppression at the higher frequencies. This pattern of excitation flanked by suppression is present in our dictionaries (<xref ref-type="fig" rid="pcbi-1002594-g008"><bold>Fig. 8d</bold></xref>), including at the highest frequencies probed. This type of STRF pattern is reminiscent of the two-dimentional Gabor-like patches seen in V1, which have been well captured by sparse coding models of natural scenes <xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Rehn1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Olshausen2">[26]</xref>.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have applied the principle of sparse coding to spectrogram and cochleogram representations of human speech recordings in order to uncover some important features of natural sounds. Of the various models we considered, we have found that the specific form of preprocessing (<italic>i.e.</italic>, cochleograms vs. spectrograms) and the degree of overcompleteness are the most significant factors in determining the complexity and diversity of receptive field shapes. Importantly, we have also found that features learned by our sparse coding model resemble a diverse set of receptive field shapes in IC, as well as MGBv and A1. Even though a spectrogram may not provide as accurate a representation of the output of the cochlea as a more explicit cochleogram model, such as the one we explored here, we have found that sparse coding of spectrograms yields closer agreement to experimentally measured receptive fields, demonstrating that we can infer important aspects of sensory processing in the brain by identifying the statistically important features of natural sounds without having to impose many constraints from biology into our models from the outset.</p>
      <p>Indeed, it is worth emphasizing that the agreement we have found did not result from fitting the neural physiology, <italic>per se</italic>; it emerged naturally from the statistics of the speech data we used to train our model. Specifically, the model parameters we explored — undercomplete vs. overcomplete representation, L0 vs. L1 sparseness penalty, and cochleogram vs. spectrogram preprocessing — represent a low-dimensional space of essentially eight different choices compared with the rich, high-dimensional space of potential STRF shapes we could have obtained.</p>
      <p>Intriguingly, while we have emphasized the agreement between our model and IC, the receptive fields we have found resemble experimental data from multiple levels of the mammalian ascending auditory pathway. This may reflect the possibility that the auditory pathway is not strictly hierarchical, so that neurons in different anatomical locations may perform similar roles, and thus are represented by neurons from the same sparse coding dictionary. This view is consistent with the well-known observation that there is a great deal of feedback from higher to lower stages of processing in the sub-cortical auditory pathway <xref ref-type="bibr" rid="pcbi.1002594-Read1">[37]</xref>, as compared with the visual pathway, for example. Some of our shapes have even been reported at lower levels. Harmonic stacks, including some with band-passed inhibition, have been reported in the dorsal cochlear nucleus <xref ref-type="bibr" rid="pcbi.1002594-Backoff1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Clopton1">[38]</xref> and they have been observed in presynaptic responses in IC (M.A. Escabí, C. Chen, and H. Read, Society for Neuroscience Abstracts 2011), but these shapes have not yet been reported in IC spiking responses or further up the ascending auditory pathway. The tradeoff in spectrotemporal resolution we have found in our model resembles that of IC, which is the lowest stage of the ascending auditory pathway to exhibit a tradeoff that cannot be accounted for by the uncertainty principle, as is the case for auditory nerve fibers <xref ref-type="bibr" rid="pcbi.1002594-Rodrguez1">[30]</xref>, but it remains to be seen if such a tradeoff also exists in MGBv or A1.</p>
      <p>A related issue is that an individual neuron might play different roles depending on the stimulus ensemble being presented to the nervous system. In fact, changing the contrast level of the acoustic stimuli used to probe individual IC neurons can affect the number of prominent subfields in the measured STRF of the neuron <xref ref-type="bibr" rid="pcbi.1002594-Lesica1">[31]</xref>. Our model does not specify which neuron should represent any given feature, it just predicts the STRFs that should be represented in the neural population in order to achieve a sparse encoding of the stimulus.</p>
      <p>Moreover, for even moderate levels of overcompleteness, our sparse coding dictionaries include categories of features that have not been reported in the experimental literature. For example, the STRF shown in <xref ref-type="fig" rid="pcbi-1002594-g004"><bold>Fig. 4k</bold></xref> represents a well-defined class of elements in our sparse dictionaries, but we are unaware of reports of this type of STRF in the auditory pathway. Thus, our theoretical receptive fields could be used to develop acoustic stimuli that might drive auditory neurons that do not respond to traditional probe stimuli. In particular, our dictionaries contain many broadband STRFs with complex structures. These broadband neurons may not have been found experimentally since by necessity researchers often probe neurons extensively with stimuli that are concentrated around the neuron's best frequency.</p>
      <p>It is important to recognize that STRFs do not fully capture the response properties of neurons in IC, just as most of the explainable variance is not captured by linear receptive fields of V1 simple cells <xref ref-type="bibr" rid="pcbi.1002594-David1">[39]</xref>. We note, however, that while our sparse coding framework involves a linear generative model, the encoding is non-linear. Thus, one of the questions addressed by this study is the degree to which the competitive nonlinearity of a highly over-complete model can account for the rich assortment of STRFs in IC. We have found that this is a crucial factor in learning a sparse representation that captures the rich variety of STRF shapes observed in IC, as well as in thalamus and cortex.</p>
      <p>We have presented several classes of STRFs from our model that qualitatively match the shapes of neural receptive fields, but in many cases the neurons are sensitive to higher frequencies than the model neurons. This is likely due to the fact that we trained our network on human speech, which has its greatest power in the low kHz range, whereas the example neural data available in the literature come from animals with hearing that extends to much higher acoustic frequencies, and with much higher-pitched vocalizations, than humans.</p>
      <p>Our primary motivation for using speech came from the success of previous studies that yielded good qualitative <xref ref-type="bibr" rid="pcbi.1002594-Lewicki1">[15]</xref> and quantitative <xref ref-type="bibr" rid="pcbi.1002594-Smith1">[16]</xref> predictions of auditory nerve (AN) response properties based on sparse coding of speech. In fact, in order to obtain comparable results using environmental sounds and animal vocalizations, the relative proportion of training examples from each of three classes of natural sounds had to be adjusted to empirically match the results found using speech alone. Thus, speech provides a parameter-free stimulus set for matching AN properties, just as we have found for our model of IC. Moreover, good agreement between the model and AN physiology required selecting high SNR epochs within typically noisy recordings from the field; good results also required the selection of epochs containing isolated animal vocalizations rather than simultaneous calls from many individuals. By contrast, the speech databases used in those studies and the present study consist of clean, high SNR recordings of individual speakers. The issue of SNR is especially important for our study given that the dimensionality of our training examples is much higher (6,400 values for our spectrogram patches; 200 values after PCA) compared with typical vision studies (<italic>e.g.</italic>, 64 pixel values <xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[10]</xref>).</p>
      <p>Beyond the practical benefits of training on speech, the basic question of whether IC is best thought of as specialized for conspecific vocalizations or suited for more general auditory processing remains unanswered, but it seems reasonable to assume that it plays both roles. Questions such as this have inspired an important debate about the use of artificial and ecologically relevant stimuli <xref ref-type="bibr" rid="pcbi.1002594-Rust1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Felsen1">[41]</xref> and what naturalistic stimuli can tell us about sensory coding <xref ref-type="bibr" rid="pcbi.1002594-Hsu1">[42]</xref>–<xref ref-type="bibr" rid="pcbi.1002594-Vinje1">[44]</xref>. The fact that several of the different STRFs we find have been observed in a variety of species, including rats, cats, and ferrets, suggests that there exist sufficiently universal features shared by the specific acoustic environments of these creatures to allow some understanding of IC function without having to narrowly tailor the stimulus set to each species.</p>
      <p>Even if sparse coding is, indeed, a central organizing principle throughout the nervous system, it could still be that the sparse representations we predict with our model correspond best to the subthreshold, postsynaptic responses of the membrane potentials of neurons, rather than their spiking outputs. In fact, we show an example of a subthreshold STRF (<xref ref-type="fig" rid="pcbi-1002594-g008"><bold>Fig. 8a</bold></xref> bottom) that agrees well with one class of broadband model STRFs (<xref ref-type="fig" rid="pcbi-1002594-g008"><bold>Fig. 8b</bold></xref>). The tuning properties of postsynaptic responses are typically broader than spiking responses, as one would expect, which could offer a clue as to which is more naturally associated with model dictionary elements. If our model elements are to be interpreted as subthreshold responses, then the profoundly unresponsive regions surrounding the active subfields of the neuronal STRFs could be more accurately fit by our model STRFs after they are post-processed by being passed through a model of a spiking neuron with a finite spike threshold.</p>
      <p>It is encouraging that sparse encoding of speech can identify acoustic features that resemble neuronal STRFs from auditory midbrain, as well as those in thalamus and cortex, and it is notable that the majority of these features bear little resemblance to the Gabor-like shapes and elongated edge detectors that have been predicted by sparse coding representations of natural images. Clearly, our results are not an unavoidable consequence of the sparse coding procedure itself, but instead reflect the structure of the speech spectrograms and cochleograms we have used to train our model. Previous work to categorize receptive fields in A1 has often focused on oriented features that are localized in time and frequency <xref ref-type="bibr" rid="pcbi.1002594-Depireux1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Shamma1">[45]</xref>, and some authors have suggested that such Gabor-like features are the primary cell types in A1 <xref ref-type="bibr" rid="pcbi.1002594-deCharms1">[46]</xref>, but the emerging picture of the panoply of STRF shapes in IC, MGBv, and A1 is much more complex, with several distinct classes of features, just as we have found with our model. An important next step will be to develop parameterized functional forms for the various classes of STRFs we have found, which can assume the role that Gabor wavelets have played in visual studies. We hope that this approach will continue to yield insights into sensory processing in the ascending auditory pathway.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Sparse Coding</title>
        <p>In sparse coding, the input (spectrograms or cochleograms) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e002" xlink:type="simple"/></inline-formula> is encoded as a matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e003" xlink:type="simple"/></inline-formula> multiplied by a vector of weighting coefficients <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e004" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e005" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e006" xlink:type="simple"/></inline-formula> is the error. Each column of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e007" xlink:type="simple"/></inline-formula> represents one dictionary element or receptive field, the stimulus that most strongly drives the neuron. If there are more columns in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e008" xlink:type="simple"/></inline-formula> than elements in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e009" xlink:type="simple"/></inline-formula>, this will be an overcomplete representation. We defined the degree of overcompleteness relative to the number of principle components. We learned the dictionary and inferred the coefficients by descending an energy function that minimizes the mean squared error of reconstruction under a sparsity constraint.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e010" xlink:type="simple"/><label>(1)</label></disp-formula></p>
        <p>Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e011" xlink:type="simple"/></inline-formula> controls the relative weighting of the two terms and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e012" xlink:type="simple"/></inline-formula> represents the sparsity constraint.</p>
        <p>The sparsity constraint requires the column vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e013" xlink:type="simple"/></inline-formula> to be sparse by some definition. In this paper, we focus on the L0-norm, minimizing the number of non-zero coefficients in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e014" xlink:type="simple"/></inline-formula> (or equivalently the number of active neurons in a network). Another norm we have investigated is the L1-norm, minimizing the absolute activity of all of the neurons.</p>
      </sec>
      <sec id="s4b">
        <title>Locally Competitive Algorithm</title>
        <p>We performed inference of the coefficients with a recently developed algorithm, a Locally Competitive Algorithm <xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[24]</xref>, which minimizes close approximations of either the L0- or L1-norms. Each basis function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e015" xlink:type="simple"/></inline-formula> is correlated with a computing unit defined by an internal variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e016" xlink:type="simple"/></inline-formula> as well as the output coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e017" xlink:type="simple"/></inline-formula>. All of the neurons begin with the coefficients set to zero. These values change over time depending on the input. A neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e018" xlink:type="simple"/></inline-formula> increases by an amount <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e019" xlink:type="simple"/></inline-formula> if the input overlaps with the receptive field of the neuron: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e020" xlink:type="simple"/></inline-formula>. The neurons evolve as a group following dynamics in which the neurons compete with one another to represent the input. The neurons inhibit each other with the strength of the inhibition increasing as the overlap of their receptive fields and the output coefficient values increase. This internal variable is then put through a thresholding function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e021" xlink:type="simple"/></inline-formula> to produce the output value: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e022" xlink:type="simple"/></inline-formula>.</p>
        <p>In vector notation, the full dynamic equation of inference is:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e023" xlink:type="simple"/><label>(2)</label></disp-formula>The variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e024" xlink:type="simple"/></inline-formula> sets the time-scale of the dynamics.</p>
        <p>The thresholding function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e025" xlink:type="simple"/></inline-formula> is determined by the sparsity constraint <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e026" xlink:type="simple"/></inline-formula>. It is specified via:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e027" xlink:type="simple"/><label>(3)</label></disp-formula></p>
      </sec>
      <sec id="s4c">
        <title>Learning</title>
        <p>Learning is done via gradient descent on the energy function:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e028" xlink:type="simple"/><label>(4)</label></disp-formula></p>
        <p>The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002594.e029" xlink:type="simple"/></inline-formula> term is a device for increasing orthogonality between basis functions <xref ref-type="bibr" rid="pcbi.1002594-Lee1">[47]</xref>. This is equivalent to adding in a prior that the basis functions are unique.</p>
      </sec>
      <sec id="s4d">
        <title>Stimuli</title>
        <p>We used two corpora of speech recordings from the handbook of the International Phonetic Association (<ext-link ext-link-type="uri" xlink:href="http://web.uvic.ca/ling/resources/ipa/handbook_downloads.htm" xlink:type="simple">http://web.uvic.ca/ling/resources/ipa/handbook_downloads.htm</ext-link>) and TIMIT <xref ref-type="bibr" rid="pcbi.1002594-Garofolo1">[48]</xref>. These consist of people telling narratives in approximately 30 different languages. We resampled all waveforms to 16000 Hz, and then converted them into spectrograms by taking the squared Fourier Transform of the raw waveforms. We sampled at 256 frequencies logarithmically spaced between 100 and 4000 Hz. We monotonically transformed the output with the logarithm function, resulting in the log-power of the sound at specified frequencies over time.</p>
        <p>The data was then divided into segments covering all frequencies and 25 overlapping time points (16 ms each) representing 216 ms total. Subsequently, we performed principal components analysis on the samples to whiten the data as well as reduce the dimensionality. We retained the first 200 principal components as this captured over 93% of the variance in the spectrograms and lowered the simulation time. During analysis, the dictionaries were dewhitened back into spectrogram space.</p>
        <p>We also trained with another type of input, cochleograms <xref ref-type="bibr" rid="pcbi.1002594-Lyon1">[21]</xref>, . These are similar to spectrograms, but the frequency filters mimic known properties of the cochlea via a cochlear model <xref ref-type="bibr" rid="pcbi.1002594-Lyon1">[21]</xref>. The cochlear model sampled at 86 frequencies between 73 and 7630 Hz. For this input, the total time for each sample was 250 ms (still 25 time points), and the first 200 principle components captured over 98% of the variance.</p>
      </sec>
      <sec id="s4e">
        <title>Presentation of Dictionaries</title>
        <p>All dictionary neurons were scaled to be between −1 and 1 when displayed. The coefficients in the encoding can take on positive or negative values during encoding. To reflect this, we looked at the skewness of each dictionary element. If the skewness was negative, the colors of the dictionary element were inverted when being displayed to reflect the way that element was actually being used.</p>
      </sec>
      <sec id="s4f">
        <title>Modulation Power Spectra</title>
        <p>To calculate the modulation power spectra, we took a 2D Fourier Transform of each basis function. For each element, we plotted the peak of the temporal and spectral modulation transfer functions (<xref ref-type="fig" rid="pcbi-1002594-g005"><bold>Fig. 5</bold></xref>). For the cochleogram-trained basis functions, we approximated the cochleogram frequency spacing as being log-spaced to allow comparison with the spectrogram-trained dictionaries.</p>
      </sec>
      <sec id="s4g">
        <title>Presentation of Experimental Data</title>
        <p>Data from <xref ref-type="bibr" rid="pcbi.1002594-Lesica1">[31]</xref> was given to us in raw STRF format. Each was interpolated by a factor of three, but no noise was removed. Data from <xref ref-type="bibr" rid="pcbi.1002594-Rodrguez1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002594-Andoni1">[32]</xref>–<xref ref-type="bibr" rid="pcbi.1002594-Machens1">[35]</xref> were given to us in the same format as they were originally published.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002594.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p><bold>The full set of elements from a half-complete, L0-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Each rectangle represents the spectrotemporal receptive field of a single element in the dictionary; time is plotted along the horizontal axis (from 0 to 216 msec), and log frequency is plotted along the vertical axis, with frequencies ranging from 100 Hz to 4000 Hz. Color indicates the amount of power present at each frequency at each moment in time, with warm colors representing high power and cool colors representing low power. Each element has been normalized to have unit Euclidean length. Elements are arranged in order of their usage during inference with usage increasing from left to right along each row, and all elements of lower rows used more than those of higher rows.</p>
          <p>(TIFF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p><bold>The full set of elements from a complete, L0-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p><bold>The full set of elements from a two times overcomplete, L0-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p><bold>The full set of elements from a four times overcomplete, L0-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s005" xlink:type="simple">
        <label>Figure S5</label>
        <caption>
          <p><bold>The full set of elements from a half-complete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s006" xlink:type="simple">
        <label>Figures S6</label>
        <caption>
          <p><bold>The full set of elements from a complete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s007" xlink:type="simple">
        <label>Figure S7</label>
        <caption>
          <p><bold>The full set of elements from a two times overcomplete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s008" xlink:type="simple">
        <label>Figure S8</label>
        <caption>
          <p><bold>The full set of elements from a four times overcomplete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s009" xlink:type="simple">
        <label>Figure S9</label>
        <caption>
          <p><bold>The full set of elements from a half-complete, L1-sparse dictionary trained with Sparsenet </bold><xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[<bold>10</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s010" xlink:type="simple">
        <label>Figure S10</label>
        <caption>
          <p><bold>The full set of elements from a complete, L1-sparse dictionary trained with Sparsenet </bold><xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[<bold>10</bold>]</xref><bold> on spectrograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s001">Fig. S1</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s011" xlink:type="simple">
        <label>Figure S11</label>
        <caption>
          <p><bold>The full set of elements from a complete, L0-sparse dictionary trained using LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Each rectangle represents the spectrotemporal receptive field of a single element in the dictionary; time is plotted along the horizontal axis (from 0 to 250 ms), and log frequency is plotted along the vertical axis, with frequencies ranging from 73 Hz to 7630 Hz. Color indicates the amount of power present at each frequency at each moment in time, with warm colors representing high power and cool colors representing low power. Each element has been normalized to have unit Euclidean length. Elements are arranged in order of their usage during inference with usage increasing from left to right along each row, and all elements of lower rows used more than those of higher rows.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s012" xlink:type="simple">
        <label>Figure S12</label>
        <caption>
          <p><bold>The full set of elements from a two times overcomplete, L0-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s013" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s013" xlink:type="simple">
        <label>Figure S13</label>
        <caption>
          <p><bold>The full set of elements from a four times overcomplete, L0-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s014" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s014" xlink:type="simple">
        <label>Figure S14</label>
        <caption>
          <p><bold>The full set of elements from a half-complete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s015" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s015" xlink:type="simple">
        <label>Figure S15</label>
        <caption>
          <p><bold>The full set of elements from a complete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s016" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s016" xlink:type="simple">
        <label>Figure S16</label>
        <caption>
          <p><bold>The full set of elements from a two times overcomplete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s017" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s017" xlink:type="simple">
        <label>Figure S17</label>
        <caption>
          <p><bold>The full set of elements from a four times overcomplete, L1-sparse dictionary trained with LCA </bold><xref ref-type="bibr" rid="pcbi.1002594-Rozell1">[<bold>24</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s018" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s018" xlink:type="simple">
        <label>Figure S18</label>
        <caption>
          <p><bold>The full set of elements from a half-complete, L1-sparse dictionary trained with Sparsenet </bold><xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[<bold>10</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s019" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s019" xlink:type="simple">
        <label>Figure S19</label>
        <caption>
          <p><bold>The full set of elements from a complete, L1-sparse dictionary trained with Sparsenet </bold><xref ref-type="bibr" rid="pcbi.1002594-Olshausen1">[<bold>10</bold>]</xref><bold> on cochleograms of speech.</bold> Same conventions as <xref ref-type="supplementary-material" rid="pcbi.1002594.s011">Fig. S11</xref>.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002594.s020" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002594.s020" xlink:type="simple">
        <label>Figure S20</label>
        <caption>
          <p><bold>The signal to noise ratio (SNR) of sparse coding dictionaries increases with overcompleteness and with increasing numbers of active elements.</bold> Blue lines with triangles represent L0-sparse dictionaries, whereas green lines represent L1-sparse dictionaries. As expected, representations are more accurate with increasing numbers of active neurons and also when the level of overcompleteness is increased. Interestingly, the L0-sparse dictionaries typically have higher SNRs than the L1-sparse dictionaries. A few other general trends are evident as well. Most notably, the L0-sparse dictionaries have higher SNRs than the L1-sparse dictionaries for similar levels of sparseness. Also, the more overcomplete dictionaries have higher SNRs than half-complete ones, even with the same absolute number of active neurons. The half-complete and complete dictionaries do not show much improvement in performance even as the number of active neurons increases. Interestingly, we find that the performance of the L0-sparse dictionaries tend to saturate as the fraction of active neurons approaches unity whereas the corresponding curves for the L1-sparse dictionaries tend to curve upwards. Note that we did not optimize the dictionaries at each data point, but instead used the same parameters used when training the network.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank J. Wang and E. Bumbacher for providing computer code and for helpful discussions. We are grateful to the following authors for sharing their neural data: F.A. Rodríguez, H.L. Read, M.A. Escabí, S. Andoni, N. Li, G.D. Pollak, N.A. Lesica, B. Grohe, A. Qiu, C.E. Schreiner, C. Machens, M. Wehr, H. Asari, and A.M. Zador. We thank M.A. Escabí, H.L. Read, A.M. Zador, and all members of the Redwood Center for useful discussions, and we thank K. Körding for helpful discussions and comments on the manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002594-Laughlin1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name></person-group>             <year>2001</year>             <article-title>Energy as a constraint on the coding and processing of sensory information.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>11</volume>             <fpage>475</fpage>             <lpage>480</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Attneave1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Attneave</surname><given-names>F</given-names></name></person-group>             <year>1954</year>             <article-title>Some informational aspects of visual perception.</article-title>             <source>Psychol Rev</source>             <volume>61</volume>             <fpage>183</fpage>             <lpage>193</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Barlow1">
        <label>3</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>             <year>1961</year>             <article-title>Possible principles underlying the transformations of sensory messages.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Rosenblith</surname><given-names>W</given-names></name></person-group>             <source>Sensory Communication</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>217</fpage>             <lpage>234</lpage>             <comment>editor.</comment>             <comment>pp.</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Atick1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Redlich</surname><given-names>AN</given-names></name></person-group>             <year>1992</year>             <article-title>What does the retina know about natural scenes?</article-title>             <source>Neural Comput</source>             <volume>4</volume>             <fpage>196</fpage>             <lpage>210</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Laughlin2">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name></person-group>             <year>1981</year>             <article-title>A simple coding procedure enhances a neuron's information capacity.</article-title>             <source>Z Naturforsch</source>             <volume>36c</volume>             <fpage>910</fpage>             <lpage>912</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Rieke1">
        <label>6</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1999</year>             <source>Spikes: Exploring the neural code</source>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-DeWeese1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>DeWeese</surname><given-names>MR</given-names></name></person-group>             <year>1996</year>             <article-title>Optimization principles for the neural code.</article-title>             <source>Network</source>             <volume>7</volume>             <fpage>325</fpage>             <lpage>331</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Zhao1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>L</given-names></name><name name-style="western"><surname>Zhaoping</surname><given-names>L</given-names></name></person-group>             <year>2011</year>             <article-title>Understanding auditory spectro-temporal receptive fields and their changes with input statistics by efficient coding principles.</article-title>             <source>PLoS Comp Bio</source>             <volume>7</volume>             <fpage>e1002123</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Fldik1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name></person-group>             <year>1990</year>             <article-title>Forming sparse representations by local anti-hebbian learning.</article-title>             <source>Biol Cybern</source>             <volume>64</volume>             <fpage>165</fpage>             <lpage>170</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Olshausen1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>             <year>1996</year>             <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>607</fpage>             <lpage>609</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Rehn1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rehn</surname><given-names>M</given-names></name><name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name></person-group>             <year>2007</year>             <article-title>A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields.</article-title>             <source>J Comput Neurosci</source>             <volume>22</volume>             <fpage>135</fpage>             <lpage>146</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-DeWeese2">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>DeWeese</surname><given-names>MR</given-names></name><name name-style="western"><surname>Hromdka</surname><given-names>T</given-names></name><name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name></person-group>             <year>2005</year>             <article-title>Reliability and representational bandwidth in the auditory cortex.</article-title>             <source>Neuron</source>             <volume>48</volume>             <fpage>479</fpage>             <lpage>488</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-DeWeese3">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>DeWeese</surname><given-names>MR</given-names></name><name name-style="western"><surname>Wehr</surname><given-names>M</given-names></name><name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name></person-group>             <year>2003</year>             <article-title>Binary spiking in auditory cortex.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>7940</fpage>             <lpage>7949</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Hromdka1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hromdka</surname><given-names>T</given-names></name><name name-style="western"><surname>DeWeese</surname><given-names>MR</given-names></name><name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name></person-group>             <year>2008</year>             <article-title>Sparse representation of sounds in the unanesthetized auditory cortex.</article-title>             <source>PLoS Biol</source>             <volume>6</volume>             <fpage>e16</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Lewicki1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <article-title>Efficient coding of natural sounds.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <fpage>356</fpage>             <lpage>1111</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Smith1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>EC</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2006</year>             <article-title>Efficient auditory coding.</article-title>             <source>Nature</source>             <volume>439</volume>             <fpage>978</fpage>             <lpage>982</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Klein1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Klein</surname><given-names>D</given-names></name><name name-style="western"><surname>König</surname><given-names>P</given-names></name><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name></person-group>             <year>2003</year>             <article-title>Sparse spectrotemporal coding of sounds.</article-title>             <source>J Appl Signal Proc</source>             <volume>7</volume>             <fpage>659</fpage>             <lpage>667</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Aertsen1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Aertsen</surname><given-names>AMHJ</given-names></name><name name-style="western"><surname>Johannesma</surname><given-names>PIM</given-names></name></person-group>             <year>1981</year>             <article-title>A comparison of the spectro-temporal sensitivity of auditory neurons to tonal ad natural stimuli.</article-title>             <source>Biol Cybern</source>             <volume>42</volume>             <fpage>142</fpage>             <lpage>156</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Krding1">
        <label>19</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name><name name-style="western"><surname>König</surname><given-names>P</given-names></name><name name-style="western"><surname>Klein</surname><given-names>D</given-names></name></person-group>             <year>2002</year>             <article-title>Learning of sparse auditory receptive fields.</article-title>             <comment>In: Proceedings of the International Joint Conference on Neural Networks 2002; 12–17 May 2002; Honolulu, Hawaii, United States. IJCNN '02</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Henaff1">
        <label>20</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Henaff</surname><given-names>M</given-names></name><name name-style="western"><surname>Jarrett</surname><given-names>K</given-names></name><name name-style="western"><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name></person-group>             <year>2011</year>             <article-title>Unsupervised learning of sparse features for scalable audio classification.</article-title>             <comment>In: Proceedings of the 12<sup>th</sup> International Symposium on Music Information Retrieval; 24–28 October 2011; Miami, Florida, United States. ISMIR 2011</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Lyon1">
        <label>21</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lyon</surname><given-names>RF</given-names></name></person-group>             <year>1982</year>             <article-title>A computational model of filtering, detection, and compression in the cochlea.</article-title>             <fpage>1282</fpage>             <lpage>1285</lpage>             <comment>In: Proc. IEEE Int. Conf. Acoust., Speech Signal Processing; Paris, France. pp</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Slaney1">
        <label>22</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Slaney</surname><given-names>M</given-names></name></person-group>             <year>1998</year>             <article-title>Auditory toolbox version 2. Interval Research Corporation.</article-title>             <comment>Technical Report 1998-010</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-vanHateren1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name><name name-style="western"><surname>van der Schaaf</surname><given-names>A</given-names></name></person-group>             <year>1998</year>             <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex.</article-title>             <source>Proc R Soc Lond B</source>             <volume>265</volume>             <fpage>359</fpage>             <lpage>366</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Rozell1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rozell</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>DH</given-names></name><name name-style="western"><surname>Baraniuk</surname><given-names>RG</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name></person-group>             <year>2008</year>             <article-title>Sparse coding via thresholding and local competition in neural circuits.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>2526</fpage>             <lpage>2563</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Backoff1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Backoff</surname><given-names>PM</given-names></name><name name-style="western"><surname>Clopton</surname><given-names>BM</given-names></name></person-group>             <year>1991</year>             <article-title>A spectrotemporal analysis of dcn of single unit responses to wideband noise in guinea pig.</article-title>             <source>Hearing Res</source>             <volume>53</volume>             <fpage>28</fpage>             <lpage>40</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Olshausen2">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Cadieu</surname><given-names>CF</given-names></name><name name-style="western"><surname>Warland</surname><given-names>DK</given-names></name></person-group>             <year>2009</year>             <article-title>Learning real and complex overcomplete representations from the statistics of natural images.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Goyal</surname><given-names>VK</given-names></name><name name-style="western"><surname>Papadakis</surname><given-names>M</given-names></name><name name-style="western"><surname>van de Ville</surname><given-names>D</given-names></name></person-group>             <comment>editors. Proc. SPIE, volume 7446. San Diego, California</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Depireux1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Depireux</surname><given-names>DA</given-names></name><name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name><name name-style="western"><surname>Klein</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name></person-group>             <year>2001</year>             <article-title>Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex.</article-title>             <source>J Neurophysiol</source>             <volume>85</volume>             <fpage>1220</fpage>             <lpage>1111</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Shechtere1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shechtere</surname><given-names>B</given-names></name><name name-style="western"><surname>Marvit</surname><given-names>P</given-names></name><name name-style="western"><surname>Depireux</surname><given-names>DA</given-names></name></person-group>             <year>2010</year>             <article-title>Lagged cells in the inferior colliculus of the awake ferret.</article-title>             <source>Eur J Neurosci</source>             <volume>31</volume>             <fpage>42</fpage>             <lpage>48</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Donoho1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Donoho</surname><given-names>DL</given-names></name></person-group>             <year>2004</year>             <article-title>Compressed sensing.</article-title>             <source>IEEE Trans Inform Theory</source>             <volume>52</volume>             <fpage>1289</fpage>             <lpage>1396</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Rodrguez1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rodríguez</surname><given-names>FA</given-names></name><name name-style="western"><surname>Read</surname><given-names>HL</given-names></name><name name-style="western"><surname>Escabí</surname><given-names>MA</given-names></name></person-group>             <year>2010</year>             <article-title>Spectral and temporal modulation tradeoff in the inferior colliculus.</article-title>             <source>J Neurophysiol</source>             <volume>103</volume>             <fpage>887</fpage>             <lpage>903</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Lesica1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lesica</surname><given-names>NA</given-names></name><name name-style="western"><surname>Grothe</surname><given-names>B</given-names></name></person-group>             <year>2008</year>             <article-title>Dynamic spectrotemporal feature selectivity in the auditory midbrain.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>5412</fpage>             <lpage>5421</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Andoni1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Andoni</surname><given-names>S</given-names></name><name name-style="western"><surname>Li</surname><given-names>N</given-names></name><name name-style="western"><surname>Pollak</surname><given-names>GD</given-names></name></person-group>             <year>2007</year>             <article-title>Spectrotemporal receptive fields in the inferior colliculus revealing selectivity for spectral motion in conspecific vocalizations.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>4882</fpage>             <lpage>4893</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Qiu1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Qiu</surname><given-names>A</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name><name name-style="western"><surname>Escabí</surname><given-names>MA</given-names></name></person-group>             <year>2003</year>             <article-title>Gabor analysis of auditory midbrain receptive fields: spectro-temporal and binaural composition.</article-title>             <source>J Neurophysiol</source>             <volume>90</volume>             <fpage>456</fpage>             <lpage>476</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Escab1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Escabí</surname><given-names>MA</given-names></name><name name-style="western"><surname>Read</surname><given-names>HL</given-names></name></person-group>             <year>2005</year>             <article-title>Neural mechanisms for spectral analysis in the auditory midbrain, thalamus, and cortex.</article-title>             <source>Int Rev Neurobiol</source>             <volume>70</volume>             <fpage>207</fpage>             <lpage>252</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Machens1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name><name name-style="western"><surname>Wehr</surname><given-names>MS</given-names></name><name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name></person-group>             <year>2004</year>             <article-title>Linearity of cortical receptive fields measured with natural sounds.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>1089</fpage>             <lpage>1100</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Fritz1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fritz</surname><given-names>J</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name><name name-style="western"><surname>Klein</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>Rapid task-related plasticity of spectrotemporal receptive íelds in primary auditory cortex.</article-title>             <source>Nat Neurosci</source>             <volume>6</volume>             <fpage>1216</fpage>             <lpage>1223</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Read1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Read</surname><given-names>HL</given-names></name><name name-style="western"><surname>Winer</surname><given-names>JA</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name></person-group>             <year>2002</year>             <article-title>Functional architecture of auditory cortex.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>12</volume>             <fpage>433</fpage>             <lpage>440</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Clopton1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Clopton</surname><given-names>BM</given-names></name><name name-style="western"><surname>Backoff</surname><given-names>PM</given-names></name></person-group>             <year>1991</year>             <article-title>Spectrotemporal receptive fields of neurons in cochlear nucleus of guinea pig.</article-title>             <source>Hearing Res</source>             <volume>52</volume>             <fpage>329</fpage>             <lpage>44</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-David1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>SV</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name></person-group>             <year>2005</year>             <article-title>Predicting neuronal responses during natural vision.</article-title>             <source>Network</source>             <volume>16</volume>             <fpage>239</fpage>             <lpage>60</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Rust1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name></person-group>             <year>2005</year>             <article-title>In praise of artifice.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>1647</fpage>             <lpage>50</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Felsen1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Felsen</surname><given-names>G</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2005</year>             <article-title>A natural approach to studying vision.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>1643</fpage>             <lpage>6</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Hsu1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hsu</surname><given-names>A</given-names></name><name name-style="western"><surname>Woolley</surname><given-names>SM</given-names></name><name name-style="western"><surname>Fremouw</surname><given-names>TE</given-names></name><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name></person-group>             <year>2004</year>             <article-title>Modulation power and phase spectrum of natural sounds enhance neural encoding performed by single auditory neurons.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>9201</fpage>             <lpage>11</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Rieke2">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>Bodnar</surname><given-names>DA</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1995</year>             <article-title>Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents.</article-title>             <source>Proc Biol Sci</source>             <volume>262</volume>             <fpage>259</fpage>             <lpage>65</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Vinje1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vinje</surname><given-names>WE</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name></person-group>             <year>2000</year>             <article-title>Sparse coding and decorrelation in primary visual cortex during natural vision.</article-title>             <source>Science</source>             <volume>287</volume>             <fpage>1273</fpage>             <lpage>6</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Shamma1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name></person-group>             <year>2001</year>             <article-title>On the role of space and time in auditory processing.</article-title>             <source>TRENDS Cogn Sci</source>             <volume>5</volume>             <fpage>340</fpage>             <lpage>348</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-deCharms1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>deCharms</surname><given-names>RC</given-names></name><name name-style="western"><surname>Blake</surname><given-names>DT</given-names></name><name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name></person-group>             <year>1998</year>             <article-title>Optimizing sound features for cortical neurons.</article-title>             <source>Science</source>             <volume>280</volume>             <fpage>1439</fpage>             <lpage>1111</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Lee1">
        <label>47</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H</given-names></name><name name-style="western"><surname>Pham</surname><given-names>P</given-names></name><name name-style="western"><surname>Largman</surname><given-names>Y</given-names></name><name name-style="western"><surname>Ng</surname><given-names>A</given-names></name></person-group>             <year>2009</year>             <article-title>Unsupervised feature learning for audio classification using convolutional deep belief networks.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>Schuurmans</surname><given-names>D</given-names></name><name name-style="western"><surname>Lafferty</surname><given-names>J</given-names></name><name name-style="western"><surname>Williams</surname><given-names>CKI</given-names></name><name name-style="western"><surname>Culotta</surname><given-names>A</given-names></name></person-group>             <source>Advances in Neural Information Processing Systems 22</source>             <fpage>1096</fpage>             <lpage>1104</lpage>             <comment>editors.</comment>             <comment>pp</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002594-Garofolo1">
        <label>48</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Garofolo</surname><given-names>JS</given-names></name><etal/></person-group>             <year>1993</year>             <source>Timit acoustic-phonetic continuous speech corpus</source>             <publisher-loc>Philadelphia</publisher-loc>             <publisher-name>Linguistic Data Consortium</publisher-name>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>