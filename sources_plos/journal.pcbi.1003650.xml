<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00966</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003650</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Biochemical simulations</subject></subj-group></subj-group><subj-group><subject>Biophysics</subject><subj-group><subject>Biophysical simulations</subject></subj-group></subj-group><subj-group><subject>Computational biology</subject></subj-group><subj-group><subject>Systems biology</subject></subj-group><subj-group><subject>Theoretical biology</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Signaling networks</subject></subj-group></subj-group><subj-group><subject>Systems science</subject><subj-group><subject>Complex systems</subject><subject>Control theory</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject></subj-group></subj-group><subj-group><subject>Applied mathematics</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Model Selection in Systems Biology Depends on Experimental Design</article-title>
<alt-title alt-title-type="running-head">Model Selection and Experimental Design</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Silk</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kirk</surname><given-names>Paul D. W.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Barnes</surname><given-names>Chris P.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Toni</surname><given-names>Tina</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Stumpf</surname><given-names>Michael P. H.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Centre for Integrative Systems Biology at Imperial College London, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Rost</surname><given-names>Burkhard</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Tum, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">m.stumpf@imperial.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: DS MPHS. Performed the experiments: DS. Analyzed the data: DS CPB. Contributed reagents/materials/analysis tools: DS CPB PDWK TT MPHS. Wrote the paper: DS CPB MPHS.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>6</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>12</day><month>6</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>6</issue>
<elocation-id>e1003650</elocation-id>
<history>
<date date-type="received"><day>30</day><month>5</month><year>2013</year></date>
<date date-type="accepted"><day>10</day><month>4</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Silk et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Experimental design attempts to maximise the information available for modelling tasks. An optimal experiment allows the inferred models or parameters to be chosen with the highest expected degree of confidence. If the true system is faithfully reproduced by one of the models, the merit of this approach is clear - we simply wish to identify it and the true parameters with the most certainty. However, in the more realistic situation where all models are incorrect or incomplete, the interpretation of model selection outcomes and the role of experimental design needs to be examined more carefully. Using a novel experimental design and model selection framework for stochastic state-space models, we perform high-throughput <italic>in-silico</italic> analyses on families of gene regulatory cascade models, to show that the selected model can depend on the experiment performed. We observe that experimental design thus makes confidence a criterion for model choice, but that this does not necessarily correlate with a model's predictive power or correctness. Finally, in the special case of linear ordinary differential equation (ODE) models, we explore how wrong a model has to be before it influences the conclusions of a model selection analysis.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Different models of the same process represent distinct hypotheses about reality. These can be decided between within the framework of model selection, where the evidence for each is given by their ability to reproduce a set of experimental data. Even if one of the models is correct, the chances of identifying it can be hindered by the quality of the data, both in terms of its signal to measurement error ratio and the intrinsic discriminatory potential of the experiment undertaken. This potential can be predicted in various ways, and maximising it is one aim of experimental design. In this work we present a computationally efficient method of experimental design for model selection. We exploit the efficiency to consider the implications of the realistic case where all models are more or less incorrect, showing that experiments can be chosen that, considered individually, lead to unequivocal support for opposed hypotheses.</p>
</abstract>
<funding-group><funding-statement>This work was funded by the Biotechnology and Biological Science Research Council (<ext-link ext-link-type="uri" xlink:href="http://www.bbsrc.ac.uk" xlink:type="simple">www.bbsrc.ac.uk</ext-link>) grant BB/K003909/1 (to DS and MPHS, the Human Frontiers Science Programme (ww.hfsp.org) grant RG0061/2011 (to PDWK and MPHS), and a Wellcome Trust-MIT fellowship (<ext-link ext-link-type="uri" xlink:href="http://www.wellcome.ac.uk" xlink:type="simple">www.wellcome.ac.uk</ext-link>) to TT. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Mathematical models provide a rich framework for biological investigation. Depending upon the questions posed, the relevant existing knowledge and alternative hypotheses may be combined and conveniently encoded, ready for analysis via a wealth of computational techniques. The consequences of each hypothesis can be understood through the model behaviour, and predictions made for experimental validation. Values may be inferred for unknown physical parameters and the actions of unobserved components can be predicted via model simulations. Furthermore, a well-designed modelling study allows conclusions to be probed for their sensitivity to uncertainties in any assumptions made, which themselves are necessarily made explicit.</p>
<p>While the added value of a working model is clear, how to create one is decidedly not. Choosing an appropriate formulation (e.g. mechanistic, phenomenological or empirical), identifying the important components to include (and those that may be safely ignored), and defining the laws of interaction between them remains highly challenging, and requires a combination of experimentation, domain knowledge and, at times, a measure of luck. Even the most sophisticated models will still be subject to an unknown level of inaccuracy â€“ how this affects the modelling process, and in particular experimental design for Bayesian inference, will be the focus of this study.</p>
<p>Both the time and financial cost of generating data, and a growing understanding of the data dependancy of model and parameter identifiability <xref ref-type="bibr" rid="pcbi.1003650-Erguler1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003650-Liepe1">[2]</xref>, has driven research into experimental design. In essence, experimental design seeks experiments that maximise the expected information content of the data with respect to some modelling task. Recent developments include the work of Liepe et. al <xref ref-type="bibr" rid="pcbi.1003650-Liepe1">[2]</xref> that builds upon existing methods <xref ref-type="bibr" rid="pcbi.1003650-Lindley1">[3]</xref>â€“<xref ref-type="bibr" rid="pcbi.1003650-Apgar1">[8]</xref>, by utilising a sequential approximate Bayesian computation framework to choose the experiment that maximises the expected mutual information between prior and posterior parameter distributions. In so doing, they are able to optimally narrow the resulting posterior parameter or predictive distributions, incorporate preliminary experimental data and provide sensitivity and robustness analyses. In a markedly different approach, Apgar et. al <xref ref-type="bibr" rid="pcbi.1003650-Apgar1">[8]</xref> use control theoretic principles to distinguish between competing models; here the favoured model is that which is best able to inform a controller to drive the experimental system through a target trajectory.</p>
<p>In order to explore the effects of model inaccuracies we work with a computationally efficient experimental design framework. We build on the methods of Flassig and Sundmacher <xref ref-type="bibr" rid="pcbi.1003650-Flassig1">[9]</xref> where expected likelihoods are predicted using efficient Sigma-point approximations and leveraged for optimal experimental design, and Busetto et al. <xref ref-type="bibr" rid="pcbi.1003650-Busetto1">[10]</xref> where choosing the optimal measurement readouts and time points is undertaken in an iterative fashion, using Sigma-point approximations to update the posterior distributions. Here we show how mixtures distributions may be exploited to cope with non-Gaussian parameter and predictive distributions and further, derive an extension to the case of stochastic state space models. The intuition behind the approach (described fully in Materials and Methods) is shown in <xref ref-type="fig" rid="pcbi-1003650-g001">Figure 1</xref>, where for identical inputs, two ODE models (illustrated in blue and red respectively) are simulated for a range of parameter values, with times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e001" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e002" xlink:type="simple"/></inline-formula> representing two possible choices of times at which the true system can be measured and data gathered. Time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e003" xlink:type="simple"/></inline-formula> represents an uninformative experimental choice since the behaviour of the two models is very similar, while data obtained at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e004" xlink:type="simple"/></inline-formula> is more likely to favour one model over another, since the distributions of simulated trajectories completely separate. More formally, the key steps in the method are as follow: Firstly we define the limited range of experimental options to be explored and encode them as parameterised extensions of the competing models. Secondly, the so called unscented transform (UT) <xref ref-type="bibr" rid="pcbi.1003650-Julier1">[11]</xref> is used to approximate the prior predictive distribution as a mixture of Gaussians, for each model and a given experiment. Finally, optimisation is performed over the experiment parameters in order to best 'separate' the prior predictive distributions of the competing models. Parameters obtained by this optimisation represent an experiment whose generated data is predicted to maximise the differences in the subsequent marginal likelihood values of the models.</p>
<fig id="pcbi-1003650-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g001</object-id><label>Figure 1</label><caption>
<title>Outline of the proposed experimental design framework.</title>
<p>a) We will be concerned with state-space formulations, which model a true state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e005" xlink:type="simple"/></inline-formula>, as it evolves under the parametric function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e006" xlink:type="simple"/></inline-formula> subject to a process noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e007" xlink:type="simple"/></inline-formula>, and observations made of this process, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e008" xlink:type="simple"/></inline-formula>, via the 'observation' function, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e009" xlink:type="simple"/></inline-formula>, with measurement noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e010" xlink:type="simple"/></inline-formula>. b) Plots of simulations from two different models (blue and red) for various parameter values, under the same experimental conditions. At time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e011" xlink:type="simple"/></inline-formula>, the behaviour of the two models is very similar, while at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e012" xlink:type="simple"/></inline-formula>, the trajectories separate. c) Gaussian approximations of the model simulations at times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e013" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e014" xlink:type="simple"/></inline-formula> (in general these will be mixtures of Gaussians) obtained via the unscented transform. Time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e015" xlink:type="simple"/></inline-formula> is likely to be more informative than time point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e016" xlink:type="simple"/></inline-formula> for model selection purposes. Experiments can be scored by how separated these distributions are, which we quantify using the Hellinger distance.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g001" position="float" xlink:type="simple"/></fig>
<p>The contributions of this article are threefold; firstly, we extend a promising and computationally efficient experimental design framework for model selection to the stochastic setting, with non-Gaussian prior distributions; secondly, we utilise this efficiency to explore the robustness of model selection outcomes to experimental choices; and finally, we observe that experimental design can give rise to levels of confidence in selected models that may be misleading as a guide to their predictive power or correctness. The latter two points are undertaken via high-throughput <italic>in-silico</italic> analyses (at a scale completely beyond the Monte Carlo based approaches mentioned above) on families of gene regulatory cascade models and various existing models of the JAK STAT pathway.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Identifying crosstalk connections between signalling pathways</title>
<p>We first illustrate the experimental design and model selection framework in the context of crosstalk identification. After observing how the choice of experiment can be crucial for a positive model selection outcomes, the example will be used to illustrate and explore the inconsistency of selection between misspecified models.</p>
<p>We consider pairs of regulatory cascades, each consisting of four transcription factors, modelled by ordinary differential equations of the form, <disp-formula id="pcbi.1003650.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e017" xlink:type="simple"/></disp-formula>for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e018" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e019" xlink:type="simple"/></inline-formula> is the rate at which protein <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e020" xlink:type="simple"/></inline-formula> degrades, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e021" xlink:type="simple"/></inline-formula> represents the maximal rate of production of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e022" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e023" xlink:type="simple"/></inline-formula> is the amount of the transcription factor, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e024" xlink:type="simple"/></inline-formula>, needed for half the maximal response, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e025" xlink:type="simple"/></inline-formula> is called the Hill-coefficient, and determines the steepness of the response. A range of crosstalk models are formed (<xref ref-type="fig" rid="pcbi-1003650-g002">Figure 2</xref>) by inserting additional regulatory links between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e026" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e027" xlink:type="simple"/></inline-formula> with the same kinetics as above. A single model is chosen as the 'true' biological system to which we perform experiments, and six others with equal prior probabilities are proposed as models of the true system â€“ our task will be to identify the most suitable one.</p>
<fig id="pcbi-1003650-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g002</object-id><label>Figure 2</label><caption>
<title>Crosstalk between regulatory cascades.</title>
<p>Our task is to identify an unknown crosstalk connection between pathways 1 and 2. A limited range of experiments are considered, involving external stimulation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e028" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e029" xlink:type="simple"/></inline-formula>, and observation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e030" xlink:type="simple"/></inline-formula>, and a set of models (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e031" xlink:type="simple"/></inline-formula>) corresponding to different crosstalk options are selected between. The times and strengths of the stimuli, and the time of measurement of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e032" xlink:type="simple"/></inline-formula> are optimised to best distinguish between the competing crosstalk models.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g002" position="float" xlink:type="simple"/></fig>
<p>An experiment is defined by the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e033" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e034" xlink:type="simple"/></inline-formula> denotes the strength of an external stimulus to the production of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e035" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e036" xlink:type="simple"/></inline-formula> which is modelled as a term, <disp-formula id="pcbi.1003650.e037"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e037" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pcbi.1003650.e038"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e038" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula id="pcbi.1003650.e039"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e039" xlink:type="simple"/><label>(3)</label></disp-formula>added to the relevant ODE equations. The time delay between the two stimulus applications is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e040" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e041" xlink:type="simple"/></inline-formula> is the time at which a single measurement of the system (of species <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e042" xlink:type="simple"/></inline-formula> only) is taken. Prior distributions for the model parameters are set as Gaussian with means of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e043" xlink:type="simple"/></inline-formula> and covariances of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e044" xlink:type="simple"/></inline-formula> for both the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e045" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e046" xlink:type="simple"/></inline-formula> respectively, with the Hill coefficient fixed at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e047" xlink:type="simple"/></inline-formula>.</p>
<p>The results of this round of experimental design are shown in the top left of <xref ref-type="fig" rid="pcbi-1003650-g003">Figure 3</xref>, where a good choice of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e048" xlink:type="simple"/></inline-formula> is found to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e049" xlink:type="simple"/></inline-formula>, with a corresponding score of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e050" xlink:type="simple"/></inline-formula>. From the figure, it can be seen that this experiment is predicted to distinguish some pairs of models better than others. In particular, the distribution of scores suggests that while the marginal likelihoods of most pairs of models are separated as desired, there is no power to discriminate between models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e052" xlink:type="simple"/></inline-formula>, or models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e053" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e054" xlink:type="simple"/></inline-formula>. Indeed, data obtained by performing the experiment upon our 'true' system, leads to posterior probabilities for each model with the same pattern.</p>
<fig id="pcbi-1003650-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g003</object-id><label>Figure 3</label><caption>
<title>Flow diagram showing two rounds of experimental design and model selection.</title>
<p>The heat maps on the left show the Hellinger distances between the prior predictive distributions of model pairs, for the chosen experiments. Bar plots on the right give the posterior probabilities of each model with respect to data produced by the chosen experiment. After the first experiment, models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e055" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e056" xlink:type="simple"/></inline-formula> have the most support, but evidence to choose between them is negligible. However a second experiment designed for only these two models (with priors set according to the posterior probability proportions after the first round of model selection) strongly favours model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e057" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g003" position="float" xlink:type="simple"/></fig>
<p>As a sanity check, we first choose the true model from amongst the set of competing models (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e058" xlink:type="simple"/></inline-formula>), and as expected find that it is recovered by model selection with probability 1. However if the true model is not represented by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e059" xlink:type="simple"/></inline-formula> (a far more realistic case) but instead the crosstalk model with a single connection from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e060" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e061" xlink:type="simple"/></inline-formula>, then models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e063" xlink:type="simple"/></inline-formula> are found to have similar posterior probabilities of approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e064" xlink:type="simple"/></inline-formula>. Likewise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e066" xlink:type="simple"/></inline-formula> share a posterior probability of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e067" xlink:type="simple"/></inline-formula>, while a clear difference exists between any other pair of models. To distinguish further between the pair of highest scoring models, a further round of experimental design was performed, with the resulting experiment and data providing strong evidence in favour of model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e068" xlink:type="simple"/></inline-formula>.</p>
<p>In an attempt to evaluate the added value of choosing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e069" xlink:type="simple"/></inline-formula> rationally for this example, we calculate scores for a uniform sample of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e070" xlink:type="simple"/></inline-formula> values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e071" xlink:type="simple"/></inline-formula> from the same range as explored above. The resulting score distribution shown in <xref ref-type="fig" rid="pcbi-1003650-g004">Figure 4a</xref>, peaks in the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e072" xlink:type="simple"/></inline-formula> which corresponds to an average Hellinger distance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e073" xlink:type="simple"/></inline-formula> between the maximally separated marginal likelihoods of each pair of models. This is in contrast to the experiment found by our approach which lives in the tail of the distribution, with an average Hellinger distance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e074" xlink:type="simple"/></inline-formula>, and highlights how unlikely it is to find suitable experiments by chance alone. Experiments with even higher information content are found, which suggests that more care could be taken with the optimisation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e075" xlink:type="simple"/></inline-formula>, by for example, increasing the population size, or number of generations of the genetic algorithm used.</p>
<fig id="pcbi-1003650-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g004</object-id><label>Figure 4</label><caption>
<title>Robustness of model selection.</title>
<p>a) Frequency distribution of scores for 1000 uniformly sampled values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e076" xlink:type="simple"/></inline-formula>. Scores concentrate around the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e077" xlink:type="simple"/></inline-formula>, corresponding to very little information content. The dotted line indicates the score of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e078" xlink:type="simple"/></inline-formula> chosen in the first round of experimental design. b) Using the 16 crosstalk models consisting of a single connection from pathway 1 to 2, a true model is fixed and 1000 uniformly sampled experiments are performed upon it. The frequencies at which the remaining 15 crosstalk models are selected, with each data set considered independently are shown. (blue) At a low level of measurement noise (with variance 0.01) model 5 is chosen most frequently, but is still outperformed for over half the experiments. (grey) When the measurement noise is increased to a variance of 0.1, the choice of model becomes even less robust. c, d) Each heatmap shows the posterior probabilities of model 1 (versus model 2), calculated independently for 9025 experiments, with data sets of different sizes (1 and 8 respectively). Each coordinate represents a different experiment, with variations to both the time delay between stimuli, and the measurement times.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g004" position="float" xlink:type="simple"/></fig>
<p>Perhaps unnervingly, the evidence in the first experiment is found to contradict (though not significantly in this case) the decision in favour of model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e079" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e080" xlink:type="simple"/></inline-formula>, which is based on additional data from the second experiment. This suggests the possibility that the choice of experiment influences not only the amount of information available to select a particular model, but also the outcome of the model selection itself. Indeed the distribution of independently selected models from data generated by random experiments is surprisingly flat (<xref ref-type="fig" rid="pcbi-1003650-g004">Figure 4b</xref>). Even at very low levels of assumed noise, the most frequently selected model is chosen for less than half the experiments undertaken. This has been, to our knowledge, completely overlooked by the experimental design literature, but has important implications that we will explore further below.</p>
</sec><sec id="s2b">
<title>The robustness of model selection to choice of experiment</title>
<p>To examine this last observation in more detail, we work with three of the crosstalk models described above, with connections between, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e081" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e083" xlink:type="simple"/></inline-formula> respectively. The last of these is designated as the true model, and the others are considered as competing hypotheses about the location of the crosstalk connection. We perform 36100 experiments to collect data sets of size 1, 2, 4 and 8 equally spaced time points, each consisting of simulating the true model with different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e084" xlink:type="simple"/></inline-formula> that correspond to changes in the delay between stimulus applications, and variation of the time at which the state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e085" xlink:type="simple"/></inline-formula> is first measured. An independent round of model selection is performed for each data set, and the posterior probabilities for each model are calculated.</p>
<p>The results for data sets of size 1 and 8 are illustrated in <xref ref-type="fig" rid="pcbi-1003650-g004">Figure 4c and 4d</xref> as heatmaps of posterior probabilities of the first model, and show that the vast majority of the space of experiments is split into distinct regions of high, low and equal probability for each model. In the case of a single time point, most of the explored experiment subspace is found to be uninformative, with the data providing equal support for each model. Three other distinct regions are identified, of which two show decisive support (on the Jeffreys scale) for the first model, and one for which the second model is chosen decisively. In other words, by varying the experimental conditions an unequivocal choice (in isolation) for either model can be obtained. As more data points are considered, the uninformative region grows smaller, but regions of decisive support for each model remain. Interestingly, these regions are located in distinctly different places for single or multiple time points, although they remain similar for 2 or more time points. This reflects the added value of time series experiments â€“ the marginal likelihoods now balance the ability of the models to reproduce each time point, with their ability to capture the autocorrelation of the time series.</p>
<p>In order to establish whether the observed inconsistencies are an artefact of the UT approximations, we perform a similar but necessarily course grained study using MultiNest <xref ref-type="bibr" rid="pcbi.1003650-Feroz1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003650-Kirk1">[13]</xref>, an implementation of nested sampling (a Monte Carlo based technique with convergence rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e086" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003650-Aitken1">[14]</xref>). Results obtained using MultiNest (shown in the upper right of <xref ref-type="fig" rid="pcbi-1003650-g005">figure 5</xref>) are almost identical to those of <xref ref-type="fig" rid="pcbi-1003650-g004">figure 4c</xref>, displaying the same regions of decisive support for each model. Given how difficult it is to estimate marginal likelihoods in general, the excellent performance of the UT (with only one Gaussian component) may seem rather surprising, until one notes that for the models and experiments considered, the prior predictive distributions are approximately Gaussian themselves (<xref ref-type="fig" rid="pcbi-1003650-g005">Figure 5</xref>). We discuss how the framework can deal with non-Gaussian effects, such as those found in the next examples, in the appendix.</p>
<fig id="pcbi-1003650-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g005</object-id><label>Figure 5</label><caption>
<title>Monte Carlo validation.</title>
<p>The top right plot shows posterior model probabilities obtained using MultiNest. The necessarily course grained results match those obtained by the UT in <xref ref-type="fig" rid="pcbi-1003650-g004">figure 4c</xref>. Each of the other plots compare UT approximations to the prior predictive distributions with Monte Carlo approximations using samples of size 10000, for different experimental conditions indicated by arrows. The red and blue lines correspond to UT approximations (using a single Gaussian component) for model 1 and 2 respectively. The dotted line indicates the data simulated from the true model.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s2c">
<title>JAK-STAT signalling</title>
<p>In this section we undertake an analysis of three mass action models of varying degrees of resolution of the JAK-STAT signalling pathway <xref ref-type="bibr" rid="pcbi.1003650-Quaiser1">[15]</xref>. Each model describes the initial pathway activity after receptor activation (<xref ref-type="fig" rid="pcbi-1003650-g006">Figure 6</xref>), but before any feedback occurs. In brief, the signalling process consists of a receptor binding to JAK to form a complex that can dimerise in the presence of interferon-<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e087" xlink:type="simple"/></inline-formula> (IFN). This dimer is activated by phosphorylation by JAK, and in turn deactivated after being bound by tyrosine phosphatase (SHP_2). In its active state, the receptor complex phosphorylates cytoplasmic STAT1, which is then able to dimerise and act as a transcription factor <xref ref-type="bibr" rid="pcbi.1003650-Yamada1">[16]</xref>.</p>
<fig id="pcbi-1003650-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g006</object-id><label>Figure 6</label><caption>
<title>JAK STAT pathway models (adapted from Quaiser et al. <xref ref-type="bibr" rid="pcbi.1003650-Quaiser1">[15]</xref>).</title>
<p>Arrows indicate association or dissociation reactions between the protein species. Grey reactions only occur in the true model, (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e088" xlink:type="simple"/></inline-formula>). Model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e089" xlink:type="simple"/></inline-formula> consists of the purple, orange and green components. Model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e090" xlink:type="simple"/></inline-formula> is obtained by removing the green components, and replacing the orange reactions by the reaction in the bottom right oval.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g006" position="float" xlink:type="simple"/></fig>
<p>We take the most detailed model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e091" xlink:type="simple"/></inline-formula>, with 17 state variables and 25 parameters (published by Yamada et al. <xref ref-type="bibr" rid="pcbi.1003650-Yamada1">[16]</xref>), as our true system to which <italic>in-silico</italic> experiments can be performed, and select between two of the other models proposed by Quaiser et al. The first of these competing models, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e092" xlink:type="simple"/></inline-formula>, simplifies the true system, by neglecting a reaction â€“ the re-association of phosphorylated STAT1 to the activated receptor â€“ and thereby reducing the system to 16 states and 23 parameters. A series of five other 'biologically inspired' simplifications leads to our second model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e093" xlink:type="simple"/></inline-formula>, which has 9 states and 10 parameters (these steps are summarised in <xref ref-type="fig" rid="pcbi-1003650-g006">Figure 6</xref>).</p>
<p>We set the parameter priors as a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e094" xlink:type="simple"/></inline-formula> component mixture of Gaussians fit to a uniform sample from the hypercube <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e095" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e096" xlink:type="simple"/></inline-formula> is the parameter dimension, such that all the parameter values inferred for each model by Quaiser et al. are supported. We define and undertake two classes of experiment upon the true model (with parameters fixed to the published values); in the first, the IFN stimulus strength and the initial time point of a time series of 8 equally spaced measurements of the amount of JAK bound to the receptor are varied, and in the second, the species to be measured and the time at which this first measurement takes place are adjusted.</p>
<p>Model selection outcomes for each experiment (shown in <xref ref-type="fig" rid="pcbi-1003650-g007">Figure 7</xref>) show similar features to those for the crosstalk models, with distinct region of high posterior probability for each model. For the first class of experiments, selection between models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e097" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e098" xlink:type="simple"/></inline-formula> reveals strong support for the simpler model when data is gathered at earlier time points. The more complex model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e099" xlink:type="simple"/></inline-formula>, is generally favoured for later time series, and also for a very limited range of IFN stimuli strengths at early time series. For the second class of experiments, the model selection outcome is found to depend strongly upon which species is measured. The simpler model is chosen decisively and almost independently of the measurement times considered when cytoplasmic phosphorylated STAT1, in monomeric or dimeric form, or two forms of the receptor complex (IFN_R_JAKPhos_2 and IFN_R_JAK) are measured. The same is true of the complex model for measurements of two other forms of the receptor complex (IFN_R_JAK2 and IFN_R_JAKPhos_2_SHP_2). Otherwise the model selection outcome is time dependant or the choice of species is found to be uninformative.</p>
<fig id="pcbi-1003650-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g007</object-id><label>Figure 7</label><caption>
<title>JAK STAT model selection sensitivity.</title>
<p>a) IFN stimulus strength and the initial measurement time are varied. b) The species to be measured and the time at which this initial measurement takes place are adjusted. In both figures, distinct regions of high probability for each model can be seen. Comparison of UT and Monte Carlo approximations to the prior predictive distributions when c) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e100" xlink:type="simple"/></inline-formula> is chosen and d) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e101" xlink:type="simple"/></inline-formula> chosen for IFN stimulus strengths 1 and 0.6 at times 60 and 1 minute respectively. The 10 components used in the mixture distribution allow non-Gaussian effects to be captured. The error in the UT approximations is significantly smaller than the differences between models.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g007" position="float" xlink:type="simple"/></fig>
<p>Both these case studies make it clear that under the realistic assumption that all models are more or less incorrect, model selection outcomes can be sensitive to the choice of experiment. This observation has particular importance for studies that treat models as competing hypotheses that are decided between using experimental data; it is quite possible that if different experiments are undertaken, the conclusions drawn will also be different. In particular, the confidence calculated for such a conclusion (using the Jeffreys scale or another measure) can be misleading as a guide to how correct or predictive a model is (<xref ref-type="fig" rid="pcbi-1003650-g008">Figure 8a</xref>); in both the examples studied here, conditions exist such that any of the competing models can score a 'decisive' selection. The model selection outcome and associated confidence must therefore be strictly interpreted, as only increasing the odds of one model (with respect to others) for the data gathered under the specific experimental conditions.</p>
<fig id="pcbi-1003650-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g008</object-id><label>Figure 8</label><caption>
<title>Model predictive power v.s. predicted confidence.</title>
<p>Model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e102" xlink:type="simple"/></inline-formula> explains data produced from experiments in the blue region better than model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e103" xlink:type="simple"/></inline-formula>. The opposite is true for the larger orange region. In this example, the most informative experiment generates data that favours model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e104" xlink:type="simple"/></inline-formula>. Performing model selection using such data will lead to the highest possible confidence we can generate for either model, and yet the chosen model will be the least predictive i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e105" xlink:type="simple"/></inline-formula> reflects reality better for the majority of considered experimental conditions. In this particular case, we have a greater chance of choosing the most predictive model by performing a random experiment.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g008" position="float" xlink:type="simple"/></fig>
<p>In light of this observation, the role of experimental design may need to be examined further. Since different models can be selected depending on the experiment undertaken, the use of experimental design will necessarily lead to choosing the model which, for some 'optimal' experiment, has the highest possible predicted level of confidence i.e. experimental design implicitly makes confidence a selection criterion. Is it misleading to claim high confidence in a model selection result when the models have been set up (by extensions to mimic the optimal experiment) for this purpose? Is a bias introduced into the inference via experiment design? In the context of experiment design for parameter estimation, MacKay suggests this is not a problem <xref ref-type="bibr" rid="pcbi.1003650-MacKay1">[17]</xref>, stating that Bayesian inference depends only on the data collected, and not on other data that could have been gathered but was not. Our situation here is different since we consider changes not only to the data collection procedure, but also the data generation process and in turn the competing models themselves. It seems plausible that some models will gain or lose more flexibility than others with regards to fitting data for a particular choice of experiment. Even if the actual model selection is not biased, the confidence we associate with it will scale with the optimality of the experiment. After performing the optimal experiment, should there be any surprise that the selected model seems to have high support from the data? We feel these questions need further investigation.</p>
</sec><sec id="s2d">
<title>Measuring sensitivity to model inaccuracies</title>
<p>In practical terms, the important question seems to be: how wrong does the model structure (or parameter values) have to be before the less predictive model (or that which captures less about the true system) is chosen? Clearly the answer is sensitive to the system and models under study, and moreover, the issue of how to compare the size of different structural inaccuracies is non trivial. Here, as a first attempt, we limit ourselves to considering the simple case of parameter inaccuracies in linear ODE models.</p>
<p>We define a 'base' model as the linear ode system defined by its Jacobian matrix with entries, <disp-formula id="pcbi.1003650.e106"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e106" xlink:type="simple"/></disp-formula>and 'extensions' to this model as an extra row and column, <disp-formula id="pcbi.1003650.e107"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e107" xlink:type="simple"/></disp-formula></p>
<p>Biologically such an extension may represent the inclusion of an extra molecular species into the model, along with rules for how it interacts with components of the original system. Defining true base and extension models by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e108" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e109" xlink:type="simple"/></inline-formula>, we consider two models, <disp-formula id="pcbi.1003650.e110"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e110" xlink:type="simple"/></disp-formula>and <disp-formula id="pcbi.1003650.e111"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e111" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e112" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e113" xlink:type="simple"/></inline-formula>, are competing (true and false) hypotheses about the structure of the model extension, with a zero <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e114" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e115" xlink:type="simple"/></inline-formula> indicating a belief that species <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e116" xlink:type="simple"/></inline-formula> does not directly affect the rate of increase of species <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e117" xlink:type="simple"/></inline-formula>. Parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e118" xlink:type="simple"/></inline-formula>, are the unknown strengths of these interactions, over which we place a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e119" xlink:type="simple"/></inline-formula> component mixture of Gaussians prior, fit to a uniform distribution over the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e120" xlink:type="simple"/></inline-formula> for each parameter. We represent inaccuracies in modelling the base as additive perturbations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e121" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e122" xlink:type="simple"/></inline-formula>. Data was generated by simulating the state of the first variable of the true model at times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e123" xlink:type="simple"/></inline-formula>, for initial condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e124" xlink:type="simple"/></inline-formula>.</p>
<p>Model selection outcomes for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e125" xlink:type="simple"/></inline-formula> different pairs of values for the perturbations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e126" xlink:type="simple"/></inline-formula>, are shown in <xref ref-type="fig" rid="pcbi-1003650-g009">Figure 9</xref>. Distinct regions for each possible outcome are found and colour coded in the figure, with red indicating that the true extension has been identified successfully, yellow representing a decision in favour of the false extension, orange that evidence for either model is not substantial on the Jeffreys scale, and finally blue indicating that the marginal likelihood for both models is found to be less than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e127" xlink:type="simple"/></inline-formula>, for which any conclusion would be subject to numerical error. Increasing this threshold has the effect of replacing red areas with blue.</p>
<fig id="pcbi-1003650-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003650.g009</object-id><label>Figure 9</label><caption>
<title>Model selection outcomes for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e128" xlink:type="simple"/></inline-formula>, different pairs of linear ode models.</title>
<p>Each model represents one of two competing hypotheses (the model extension), but with a different base model generated by perturbing Jacobian matrix entries <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e129" xlink:type="simple"/></inline-formula> (x-axis) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e130" xlink:type="simple"/></inline-formula> (y-axis). Regions where the different hypotheses receive support are given by the red (true extension), yellow (false extension), orange (no significant support for either extension), and blue (marginal likelihood values for both models are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e131" xlink:type="simple"/></inline-formula>) coloured regions. Increasing the threshold for the blue region to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e132" xlink:type="simple"/></inline-formula> results in reduction of the red region, but not of the yellow. Using the true base model (represented by the cross at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e133" xlink:type="simple"/></inline-formula>), the true extension is also identified.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003650.g009" position="float" xlink:type="simple"/></fig>
<p>In the majority of cases tested, the true extension is correctly identified despite inaccuracies in the base model. However, a set of perturbations are seen to confound the selection, and allow the false extension to obtain substantial support. Furthermore, the selection outcome is found to be more sensitive in some directions than others, with relatively small perturbations to base model entry <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e134" xlink:type="simple"/></inline-formula> causing a change in outcome and creating decision boundaries near the lines <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e135" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e136" xlink:type="simple"/></inline-formula>. Prior to our analysis, it would be hard to predict these observations even when the true model is known and as simple as that explored here.</p>
<p>In real applications, where the true model is unknown and more complex, it may not be possible to tell whether a conclusion is an artefact of model inaccuracies, even when the truth of the conclusion itself can be tested by direct experimental measurement. However, the type of analysis undertaken here at least gives a measure of robustness for the conclusion to a range of model inaccuracies. Unfortunately, this remains difficult to implement in a more general setting â€“ for example, in climatology, where the accepted method of coping with structural uncertainty is through the use of large ensembles of similar models produced by various research groups <xref ref-type="bibr" rid="pcbi.1003650-Team1">[18]</xref>, a luxury that cannot be afforded on the scale of the most ambitious systems biology projects. While the practical challenges of dealing with large numbers of models is somewhat overcome by the model selection algorithm described above, a harder conceptual problem exists of how to define perturbations to more complicated classes of model, and to compare their strengths.</p>
<p>Finally, the example also highlights the difficulty of testing a hypothesis that represents only part of a model. The study shows that the implicit assumption that the base model is accurate, is not necessarily benign, and can affect any conclusions drawn â€“ a result that is borne out by the logical principle that from a false statement, anything is provable.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>The scale of the analyses detailed above, comprising thousands of marginal likelihood computations, requires extreme computational efficiency. Indeed it is completely beyond Monte Carlo based methods such as that recently developed by Liepe et al. <xref ref-type="bibr" rid="pcbi.1003650-Liepe1">[2]</xref>, which are limited to exploring small sets of models and experiments. Here, the efficiency was obtained by using the unscented transform for propagating Gaussian mixture distributions through non-linear functions. Further computational savings can be made by exploiting the highly parallelizable nature of Flassig and Sundmacher's method <xref ref-type="bibr" rid="pcbi.1003650-Flassig1">[9]</xref>, which we have extended for use with mixture distributed priors and stochastic state space models.</p>
<p>This efficiency has allowed us to explore model selection problems involving relatively large numbers of models and experiments, and investigate the robustness of model selection results to both changes in experimental conditions and inaccuracies in the models. Results from the latter two studies illustrate some common, but often ignored, pitfalls associated with modelling and inference. Firstly, we show that the conclusions of a model selection analysis can change depending on the experiment undertaken. Related to this, we observe that confidence in such a conclusion is not a good estimator of the predictive power of a model, or the correctness of the model structure. Further we note that the use of experimental design in this context maximises the expected discriminatory information available, and implicitly makes confidence in the outcome a criterion for model selection. In the future we intend to investigate the desirability of this property and how it affects the interpretation of the confidence associated with model selection outcomes.</p>
<p>At the heart of these issues is a lack of understanding of the implications of model (or parameter) inaccuracies. Often improved fits to data or better model predictions are interpreted as evidence that more about the true system is being captured. This assumption underlines a guiding paradigm of systems biology <xref ref-type="bibr" rid="pcbi.1003650-Kitano1">[19]</xref>, where a modelling project is ideally meant to be a cycle of model prediction, experimental testing and subsequent data inspired model/parameter improvement. However, it is possible that improved data fitting and predictive power (although desirable in their own right) can be achieved by including more inaccuracies in the model. In the context of parameter estimation, this concept of local optima is widely known, and their avoidance is a challenge when performing any non-trivial inference. One simple method to do so is to include random perturbations in the inference, in order to 'kick' the search out of a local optimum. Perhaps a similar strategy might be included in the modelling paradigm; by performing random experiments, or adding or removing interactions in a model structure, data might be gathered or hypotheses generated that allows a leap to be made to a more optimal solution.</p>
<p>While we have been concerned solely with the statistical setting, it is reasonable to expect similar results can be found for alternative model discrimination approaches e.g the use of Semidefinite programming to establish lower bounds on the discrepancy between candidate models and data <xref ref-type="bibr" rid="pcbi.1003650-Anderson1">[20]</xref>. Here the particular subset of models that are invalidated will be dependent upon the experiment undertaken. However, emphasis on invalidating wrong models instead of evaluating the relative support for each at least reduces the temptation for extrapolated and, perhaps, false conclusions.</p>
<p>George E. P. Box famously stated that 'Essentially, all models are wrong, but some are useful'. Here we would add that if nothing else, models provide a natural setting for mathematicians, engineers and physicists to explore biological problems, exercise their own intuitions, apply theoretical techniques, and ultimately generate novel hypotheses. Whether the hypotheses are correct or not, the necessary experimental checking will reveal more about the biology.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>The unscented transform</title>
<p>The UT is a method that describes how the moments of a random variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e137" xlink:type="simple"/></inline-formula>, are transformed by a non-linear function, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e138" xlink:type="simple"/></inline-formula>. The algorithm begins by calculating a set of weighted particles (called sigma-points) with the same sample moments up to a desired order as the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e139" xlink:type="simple"/></inline-formula>. For the results shown here, we use a scaled sigma-point set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e140" xlink:type="simple"/></inline-formula> that captures both means and covariances <xref ref-type="bibr" rid="pcbi.1003650-Julier2">[21]</xref>, <disp-formula id="pcbi.1003650.e141"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e141" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e142" xlink:type="simple"/></inline-formula> is the dimension of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e143" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e144" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e145" xlink:type="simple"/></inline-formula> are the mean and covariance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e146" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e147" xlink:type="simple"/></inline-formula> represents the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e148" xlink:type="simple"/></inline-formula>th column of a matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e149" xlink:type="simple"/></inline-formula>, and <disp-formula id="pcbi.1003650.e150"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e150" xlink:type="simple"/></disp-formula></p>
<p>The sigma-point weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e151" xlink:type="simple"/></inline-formula> are given by, <disp-formula id="pcbi.1003650.e152"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e152" xlink:type="simple"/></disp-formula>and finally, the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e153" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e154" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e155" xlink:type="simple"/></inline-formula> may be chosen to control the positive definiteness of covariance matrices, spread of the sigma-points, and error in the kurtosis respectively. For the results in this article we take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e156" xlink:type="simple"/></inline-formula> as is standard in the literature <xref ref-type="bibr" rid="pcbi.1003650-Wan1">[22]</xref>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e157" xlink:type="simple"/></inline-formula> which is optimal for Gaussian input distributions, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e158" xlink:type="simple"/></inline-formula>, controlling the spread of sigma-points is taken small as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e159" xlink:type="simple"/></inline-formula> to avoid straddling non-local non-linear effects with a single Gaussian component <xref ref-type="bibr" rid="pcbi.1003650-Julier2">[21]</xref>.</p>
<p>The mean and covariance of the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e160" xlink:type="simple"/></inline-formula>, can be estimated as the weighted mean and covariance of the propagated sigma-points, <disp-formula id="pcbi.1003650.e161"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e161" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula id="pcbi.1003650.e162"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e162" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>We denote the resulting approximate probability density function for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e163" xlink:type="simple"/></inline-formula>, by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e164" xlink:type="simple"/></inline-formula>.</p>
<p>By matching terms in the Taylor expansions of the estimated and true values of these moments, it can be shown that the UT is accurate to second order in the expansion. More generally, if the sigma-point set approximates the moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e165" xlink:type="simple"/></inline-formula> up to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e166" xlink:type="simple"/></inline-formula> order then the estimates of the mean and covariance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e167" xlink:type="simple"/></inline-formula> will be accurate up to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e168" xlink:type="simple"/></inline-formula> term <xref ref-type="bibr" rid="pcbi.1003650-Julier1">[11]</xref>. Crucially, the number of points required (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e169" xlink:type="simple"/></inline-formula> for this scheme) is much smaller than the number required to reach convergence with Monte-Carlo methods.</p>
</sec><sec id="s4b">
<title>Unscented model selection</title>
<p>We will consider discrete time state space models, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e170" xlink:type="simple"/></inline-formula>, with stateâ€“transition (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e171" xlink:type="simple"/></inline-formula>) and observation (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e172" xlink:type="simple"/></inline-formula>) functions both parametrized by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e173" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003650.e174"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e174" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula id="pcbi.1003650.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e175" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e176" xlink:type="simple"/></inline-formula>, is the time series of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e177" xlink:type="simple"/></inline-formula> dimensional measurements that we are trying to model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e178" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e179" xlink:type="simple"/></inline-formula> dimensional true state of the system at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e180" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e181" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e182" xlink:type="simple"/></inline-formula> are independent, but not necessarily additive, Gaussian white-noise process and measurement terms. Bayesian model selection compares competing models, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e183" xlink:type="simple"/></inline-formula>, by combining the <italic>a priori</italic> belief in each model, encoded by the model prior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e184" xlink:type="simple"/></inline-formula>, with the evidence for each model in the data <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e185" xlink:type="simple"/></inline-formula>, as quantified by the marginal likelihood,<disp-formula id="pcbi.1003650.e186"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e186" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e187" xlink:type="simple"/></inline-formula> is the parameter prior for model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e188" xlink:type="simple"/></inline-formula>. In the Bayesian setting, the relative suitabilities of a pair of models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e189" xlink:type="simple"/></inline-formula> are often compared using the ratio of posterior probabilities, known as the Bayes factor,<disp-formula id="pcbi.1003650.e190"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e190" xlink:type="simple"/></disp-formula>with a Bayes factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e191" xlink:type="simple"/></inline-formula> seen as substantial <xref ref-type="bibr" rid="pcbi.1003650-Jeffreys1">[23]</xref>. However, for complex or stochastic models, the marginal likelihood can be intractable, and so approximate likelihood free methods, such as Approximate Bayesian Computation are becoming increasingly important and popular within the biosciences <xref ref-type="bibr" rid="pcbi.1003650-Sunnker1">[24]</xref>. A big drawback of such Monte-Carlo based algorithms is the large number of simulations â€“ and associated computational cost â€“ required to estimate the posterior distributions or Bayes factors. Even with GPU implementation <xref ref-type="bibr" rid="pcbi.1003650-Zhou1">[25]</xref>, applications are currently still limited to comparing pairs or handfuls of models.</p>
<p>In order to address the issues raised above, a higher-throughput model selection algorithm is needed. Our approach will be to fit mixture of Gaussian models to the prior parameter distribution for each model, <disp-formula id="pcbi.1003650.e192"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e192" xlink:type="simple"/></disp-formula>so that we can exploit the UT within the state-space framework to drastically reduce the number of simulations necessary to estimate the distribution of the output of the model. Gaussian mixture measurement and process noise can also be considered, as in the work on Gaussian sum filters <xref ref-type="bibr" rid="pcbi.1003650-Alspach1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003650-Faubel1">[27]</xref>, although the number of mixture components required to model the output at each time point then increases exponentially, and in the case of long time series, component reduction schemes need to be implemented.</p>
<p>With this approximation, the marginal likelihood may be expressed as the sum, <disp-formula id="pcbi.1003650.e193"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e193" xlink:type="simple"/><label>(8)</label></disp-formula><disp-formula id="pcbi.1003650.e194"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e194" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula id="pcbi.1003650.e195"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e195" xlink:type="simple"/><label>(10)</label></disp-formula>where the components, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e196" xlink:type="simple"/></inline-formula>, can be determined using the UT as described below. Note that the accuracy of the approximation can be controlled by the number of components used. However, in the presence of nonlinearities, choosing the number and position of components solely to fit the prior distribution may not be adequate. This is because we need to have enough flexibility to also fit a complex and possibly multi-modal output. Indeed, except at the asymptotic limit of dense coverage by the mixture components, it is possible to construct badly behaved mappings that will lead to loss of performance. For the applications visited in this article, the models proved well behaved enough such that a single component and 10 components respectively for the crosstalk and JAK-STAT systems sufficed for sufficient agreement with the nested sampling and Monte Carlo results. An improvement to the method described here would be to update the number of components automatically with respect to the model behaviour in a manner similar to how Gaussian mixtures can be adaptively chosen in particle based simulation of Liouville-type equations <xref ref-type="bibr" rid="pcbi.1003650-Horenko1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003650-Weie1">[29]</xref>.</p>
<p>For the deterministic case including the examples considered in this article, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e197" xlink:type="simple"/></inline-formula>, and the stateâ€“space model simplifies to, <disp-formula id="pcbi.1003650.e198"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e198" xlink:type="simple"/></disp-formula>where might represent the simulation of certain variables of a system of ODEs, parameterised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e199" xlink:type="simple"/></inline-formula>, with additive measurement error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e200" xlink:type="simple"/></inline-formula>. In this case the marginal likelihood can then be expressed as, <disp-formula id="pcbi.1003650.e201"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e201" xlink:type="simple"/></disp-formula>where each component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e202" xlink:type="simple"/></inline-formula> is obtained simply through application of the UT with input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e203" xlink:type="simple"/></inline-formula>, and liklihood that is Gaussian with mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e204" xlink:type="simple"/></inline-formula>, and variance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e205" xlink:type="simple"/></inline-formula>.</p>
<p>To estimate the marginal likelihood in the stochastic case (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e206" xlink:type="simple"/></inline-formula>), we assume the observation function takes the form of a linear transformation of the true state and measurement noise at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e207" xlink:type="simple"/></inline-formula> with additive noise, <disp-formula id="pcbi.1003650.e208"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e208" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e209" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e210" xlink:type="simple"/></inline-formula> matrix. In practice this might correspond to the common situation where observations are scaled measurements of the abundance of various homo- or heterogeneous groups of molecules.</p>
<p>We may then write the mean of the observation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e211" xlink:type="simple"/></inline-formula>, in terms of the statistics of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e212" xlink:type="simple"/></inline-formula>, <disp-formula id="pcbi.1003650.e213"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e213" xlink:type="simple"/><label>(12)</label></disp-formula>for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e214" xlink:type="simple"/></inline-formula>, and from the bilinearity of the covariance function, the covariance between any pair of observations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e215" xlink:type="simple"/></inline-formula>, as, <disp-formula id="pcbi.1003650.e216"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e216" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p><disp-formula id="pcbi.1003650.e217"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e217" xlink:type="simple"/><label>(14)</label></disp-formula>since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e218" xlink:type="simple"/></inline-formula> is independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e219" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e220" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e221" xlink:type="simple"/></inline-formula>. We now need to find expressions for the process state covariance terms in equation 14. To do so we apply the UT iteratively for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e222" xlink:type="simple"/></inline-formula> to transform the state-variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e223" xlink:type="simple"/></inline-formula> through the state-transition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e224" xlink:type="simple"/></inline-formula>, with input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e225" xlink:type="simple"/></inline-formula> given by, <disp-formula id="pcbi.1003650.e226"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e226" xlink:type="simple"/></disp-formula></p>
<p>The result is a Gaussian approximation to the joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e227" xlink:type="simple"/></inline-formula> for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e228" xlink:type="simple"/></inline-formula>, and hence also to the conditional distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e229" xlink:type="simple"/></inline-formula>. Given that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e230" xlink:type="simple"/></inline-formula> is a Markov process and that the product of Gaussian functions is Gaussian, we also have a Gaussian expression for the joint distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e231" xlink:type="simple"/></inline-formula>, <disp-formula id="pcbi.1003650.e232"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e232" xlink:type="simple"/></disp-formula></p>
<p>The covariance between any pair of observations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e233" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e234" xlink:type="simple"/></inline-formula>, may then be found by substituting relevant entries from the covariance matrix of the density of Equation into Equation 14. The subsequent Gaussian approximation to the joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e235" xlink:type="simple"/></inline-formula>, given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e236" xlink:type="simple"/></inline-formula>, constitutes one component in the mixture approximation of the marginal likelihood given in <xref ref-type="disp-formula" rid="pcbi.1003650.e195">Equation 10</xref>.</p>
</sec><sec id="s4c">
<title>Experimental design</title>
<p>We first introduce a vector of experiment parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e237" xlink:type="simple"/></inline-formula>, that describes how the dataset is created, specifying, for example, the times at which the system is stimulated, the strengths and targets of the stimuli, knockouts or knockdowns, along with the choice of observable to be measured at each time point. We can then model the system and experiments jointly, extending the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e238" xlink:type="simple"/></inline-formula> to include terms describing the possible experimental perturbations, and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e239" xlink:type="simple"/></inline-formula> to capture the measurement options, <disp-formula id="pcbi.1003650.e240"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e240" xlink:type="simple"/><label>(15)</label></disp-formula><disp-formula id="pcbi.1003650.e241"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e241" xlink:type="simple"/><label>(16)</label></disp-formula></p>
<p>We assume that there is overlap between the system observables appearing in each model so that experiments that allow model comparison can be designed.</p>
<p>To illustrate how this might be done in practice, we consider a typical set of ordinary differential equations used to describe a gene regulatory mechanism, <disp-formula id="pcbi.1003650.e242"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e242" xlink:type="simple"/><label>(17)</label></disp-formula><disp-formula id="pcbi.1003650.e243"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e243" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e244" xlink:type="simple"/></inline-formula> are the parameters controlling the rates of production and degradation of an mRNA, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e245" xlink:type="simple"/></inline-formula>, and a protein, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e246" xlink:type="simple"/></inline-formula>, subject to the concentration of a repressor protein, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e247" xlink:type="simple"/></inline-formula>. We define the state transition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e248" xlink:type="simple"/></inline-formula> as their solution evaluated at the next measurement time-point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e249" xlink:type="simple"/></inline-formula> which is now dependant on the choice of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e250" xlink:type="simple"/></inline-formula>, given the state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e251" xlink:type="simple"/></inline-formula>, and subject to some additive noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e252" xlink:type="simple"/></inline-formula>. These equations have be extended as, <disp-formula id="pcbi.1003650.e253"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e253" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pcbi.1003650.e254"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e254" xlink:type="simple"/><label>(20)</label></disp-formula>to model a range of possible experimental perturbations, e.g. setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e255" xlink:type="simple"/></inline-formula> mimics a knockout of the gene producing mRNA <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e256" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e257" xlink:type="simple"/></inline-formula> an input stimulus to species <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e258" xlink:type="simple"/></inline-formula>. The observation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e259" xlink:type="simple"/></inline-formula>, as before can be some linear function of the states, however, the selection of variables and coefficients is now an experimental choice specified by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e260" xlink:type="simple"/></inline-formula>, <disp-formula id="pcbi.1003650.e261"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e261" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s4d">
<title>Experimental design as an optimisation problem</title>
<p>Given a particular set of experimental options, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e262" xlink:type="simple"/></inline-formula>, the marginal likelihood of model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e263" xlink:type="simple"/></inline-formula> <italic>for any possible</italic> data set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e264" xlink:type="simple"/></inline-formula> (the prior predictive distribution) can be estimated efficiently from <xref ref-type="disp-formula" rid="pcbi.1003650.e195">equation 10</xref>, <disp-formula id="pcbi.1003650.e265"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e265" xlink:type="simple"/></disp-formula>with the components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e266" xlink:type="simple"/></inline-formula> calculated with respect to the extended system and experiment model. Comparisons between such prior predictive distributions for competing models provides a means to predict the discriminatory value of a proposed experiment. Intuitively, values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e267" xlink:type="simple"/></inline-formula>, for which the prior predictive distributions of two models are separated, correspond to experimental conditions under which the models make distinct predictions of the system behaviour. Data gathered under these conditions are thus more likely to yield a significant model selection outcome. More formally, we can quantify the value of an experiment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e268" xlink:type="simple"/></inline-formula>, using the Hellinger distance between the prior predictive distributions, <disp-formula id="pcbi.1003650.e269"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e269" xlink:type="simple"/></disp-formula>which takes the following closed form for multivariate Gaussian distributions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e270" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e271" xlink:type="simple"/></inline-formula>, <disp-formula id="pcbi.1003650.e272"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e272" xlink:type="simple"/></disp-formula>where, <disp-formula id="pcbi.1003650.e273"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e273" xlink:type="simple"/></disp-formula>or for Gaussian mixtures, it can be evaluated using the method suggested in <xref ref-type="bibr" rid="pcbi.1003650-Kristan1">[30]</xref>.</p>
<p>The experimental design problem may then be posed as an optimisation problem (the results in this article used a genetic algorithm <xref ref-type="bibr" rid="pcbi.1003650-Perone1">[31]</xref> of population size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e274" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e275" xlink:type="simple"/></inline-formula> generations) over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e276" xlink:type="simple"/></inline-formula> - we search for the set of experimental parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e277" xlink:type="simple"/></inline-formula>, for which the Hellinger distance between the competing models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e278" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e279" xlink:type="simple"/></inline-formula>, is maximal. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e280" xlink:type="simple"/></inline-formula> will then specify the experiment that gives the greatest chance of distinguishing between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e281" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003650.e282" xlink:type="simple"/></inline-formula>. In the case where more than two models are considered, the cost function is taken as <disp-formula id="pcbi.1003650.e283"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003650.e283" xlink:type="simple"/></disp-formula>where the sum of exponentials is introduced to encourage selection of experiments with a high chance of distinguishing between a subset of the model pairs, over experiments with less decisive information for any pair of models, but perhaps a larger average Hellinger distance over all model pairs.</p>
</sec></sec></body>
<back>
<ack>
<p>We would like to thank the members of the <italic>Theoretical Systems Biology Group</italic> at Imperial College London for fruitful discussions. We dedicate this paper to Joe Silk.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003650-Erguler1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erguler</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Stumpf</surname><given-names>MPH</given-names></name> (<year>2011</year>) <article-title>Practical limits for reverse engineering of dynamical systems: a statistical analysis of sensitivity and parameter inferability in systems biology models</article-title>. <source>Molecular bioSystems</source> <volume>7</volume>: <fpage>1593</fpage>â€“<lpage>1602</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Liepe1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liepe</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Filippi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Komorowski</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stumpf</surname><given-names>MPH</given-names></name> (<year>2013</year>) <article-title>Maximizing the information content of experiments in systems biology</article-title>. <source>PLoS computational biology</source> <volume>9</volume>: <fpage>e1002888</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Lindley1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindley</surname><given-names>DV</given-names></name> (<year>1956</year>) <article-title>On a measure of the information provided by an experiment</article-title>. <source>The Annals of Mathematical Statistics</source> <volume>27</volume>: <fpage>986</fpage>â€“<lpage>1005</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Vanlier1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanlier</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tiemann</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Hilbers</surname><given-names>PAJ</given-names></name>, <name name-style="western"><surname>van Riel</surname><given-names>NAW</given-names></name> (<year>2012</year>) <article-title>A Bayesian approach to targeted experiment design</article-title>. <source>Bioinformatics (Oxford, England)</source> <volume>28</volume>: <fpage>1136</fpage>â€“<lpage>1142</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Huan1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huan</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Marzouk</surname><given-names>YM</given-names></name> (<year>2012</year>) <article-title>Simulation-based optimal Bayesian experimental design for nonlinear systems</article-title>. <source>Journal of Computational Physics</source> <volume>232</volume>: <fpage>288</fpage>â€“<lpage>317</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Kutalik1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kutalik</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Cho</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Wolkenhauer</surname><given-names>O</given-names></name> (<year>2004</year>) <article-title>Optimal sampling time selection for parameter estimation in dynamic pathway modeling</article-title>. <source>Biosystems</source> <volume>75</volume>: <fpage>43</fpage>â€“<lpage>55</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Chu1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hahn</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>Integrating parameter selection with experimental design under uncertainty for nonlinear dynamic systems</article-title>. <source>AIChE Journal</source> <volume>54</volume>: <fpage>2310</fpage>â€“<lpage>2320</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Apgar1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Apgar</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Witmer</surname><given-names>DK</given-names></name>, <name name-style="western"><surname>White</surname><given-names>FM</given-names></name>, <name name-style="western"><surname>Tidor</surname><given-names>B</given-names></name> (<year>2010</year>) <article-title>Sloppy models, parameter uncertainty, and the role of experimental design</article-title>. <source>Molecular bioSystems</source> <volume>6</volume>: <fpage>1890</fpage>â€“<lpage>1900</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Flassig1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flassig</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Sundmacher</surname><given-names>K</given-names></name> (<year>2012</year>) <article-title>Optimal design of stimulus experiments for robust discrimination of biochemical reaction networks</article-title>. <source>Bioinformatics (Oxford, England)</source> <volume>28</volume>: <fpage>3089</fpage>â€“<lpage>3096</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Busetto1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Busetto</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Hauser</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Krummenacher</surname><given-names>G</given-names></name>, <name name-style="western"><surname>SunnÃ¥ker</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dimopoulos</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Near-optimal experimental design for model selection in systems biology</article-title>. <source>Bioinformatics</source> <volume>29</volume>: <fpage>2625</fpage>â€“<lpage>2632</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Julier1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Julier</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Uhlmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Durrant-Whyte</surname><given-names>H</given-names></name> (<year>2000</year>) <article-title>A new method for the nonlinear transformation of means and covariances in filters and estimators</article-title>. <source>IEEE Transactions on Automatic Control</source> <volume>45</volume>: <fpage>477</fpage>â€“<lpage>482</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Feroz1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feroz</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Hobson</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Bridges</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>MULTINEST: an efficient and robust Bayesian inference tool for cosmology and particle physics</article-title>. <source>Mon Not Roy Astron Soc</source> <volume>398</volume>: <fpage>1601</fpage>â€“<lpage>1614</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Kirk1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kirk</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Thorne</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Stumpf</surname><given-names>MP</given-names></name> (<year>2013</year>) <article-title>Model selection in systems and synthetic biology</article-title>. <source>Current opinion in biotechnology</source> <volume>24</volume>: <fpage>551</fpage>â€“<lpage>826</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Aitken1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aitken</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Akman</surname><given-names>OE</given-names></name> (<year>2013</year>) <article-title>Nested sampling for parameter inference in systems biology: application to an exemplar circadian model</article-title>. <source>BMC systems biology</source> <volume>7</volume>: <fpage>72</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Quaiser1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quaiser</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Dittrich</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schaper</surname><given-names>F</given-names></name>, <name name-style="western"><surname>MÃ¶nnigmann</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>A simple work flow for biologically inspired model reductionâ€“application to early JAK-STAT signaling</article-title>. <source>BMC systems biology</source> <volume>5</volume>: <fpage>30</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Yamada1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamada</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Shiono</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Joo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Yoshimura</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Control mechanism of JAK/STAT signal transduction pathway</article-title>. <source>Febs Letters</source> <volume>534</volume>: <fpage>190</fpage>â€“<lpage>196</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-MacKay1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>MacKay</surname><given-names>DJ</given-names></name> (<year>1992</year>) <article-title>Information-based objective functions for active data selection</article-title>. <source>Neural computation</source> <volume>4</volume>: <fpage>590</fpage>â€“<lpage>604</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Team1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Team CW (2010) Good practice guidance paper on assessing and combining multi model climate projections. In: IPCC Expert Meeting on Assessing and Combining Multi Model Climate Projections. p. 1.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Kitano1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kitano</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Systems biology: a brief overview</article-title>. <source>Science</source> <volume>295</volume>: <fpage>1662</fpage>â€“<lpage>1664</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Anderson1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Papachristodoulou</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>On validation and invalidation of biological models</article-title>. <source>BMC bioinformatics</source> <volume>10</volume>: <fpage>132</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Julier2"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Julier SJ The scaled unscented transformation. In: Proceedings of the 2002 American Control Conference. American Automatic Control Council. pp. 4555â€“4559.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Wan1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wan</surname><given-names>E</given-names></name>, <name name-style="western"><surname>van der Merwe</surname><given-names>R</given-names></name> (<year>2000</year>) <article-title>The unscented Kalman filter for nonlinear estimation</article-title>. <source>Adaptive Systems for Signal Processing, Communications, and Control Symposium 2000 AS-SPCC The IEEE</source> <volume>2000</volume> <fpage>153</fpage>â€“<lpage>158</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Jeffreys1"><label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Jeffreys H (1961) Theory of Probability. 3rd ed. Oxford: The Clarendon Press.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Sunnker1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>SunnÃ¥ker</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Busetto</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Numminen</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Corander</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Foll</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Approximate Bayesian computation</article-title>. <source>PLoS computational biology</source> <volume>9</volume>: <fpage>e1002803</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002803" xlink:type="simple">10.1371/journal.pcbi.1002803</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003650-Zhou1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Liepe</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Sheng</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Stumpf</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Gpu accelerated biochemical network simulation</article-title>. <source>Bioinformatics</source> <volume>27</volume>: <fpage>874</fpage>â€“<lpage>876</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Alspach1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alspach</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Sorenson</surname><given-names>H</given-names></name> (<year>1972</year>) <article-title>Nonlinear Bayesian estimation using Gaussian sum approximations</article-title>. <source>Automatic Control, IEEE Transactions on</source> <volume>17</volume>: <fpage>439</fpage>â€“<lpage>448</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Faubel1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Faubel F, McDonough J (2009) The split and merge unscented Gaussian mixture filter. Signal Processing Letters.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Horenko1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horenko</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Weiser</surname><given-names>M</given-names></name> (<year>2003</year>) <article-title>Adaptive integration of molecular dynamics</article-title>. <source>Journal of computational chemistry</source> <volume>24</volume>: <fpage>1921</fpage>â€“<lpage>1929</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Weie1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>WeiÎ²e</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Horenko</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Huisinga</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Adaptive approach for modelling variability in pharmacokinetics</article-title>. <source>Computational Life Sciences</source> <volume>II</volume> <fpage>194</fpage>â€“<lpage>204</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Kristan1"><label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Kristan M, Leonardis A (2010) Multivariate online kernel density estimation. In: Computer Vision Winter Workshop. pp. 77â€“86.</mixed-citation>
</ref>
<ref id="pcbi.1003650-Perone1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perone</surname><given-names>CS</given-names></name> (<year>2009</year>) <article-title>Pyevolve: a Python open-source framework for genetic algorithms</article-title>. <source>ACM SIGEVOlution</source> <volume>4</volume>: <fpage>12</fpage>â€“<lpage>20</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>