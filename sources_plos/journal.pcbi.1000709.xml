<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">09-PLCB-RA-1199R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000709</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Computational Biology/Metabolic Networks</subject><subject>Computational Biology/Systems Biology</subject><subject>Computational Biology/Transcriptional Regulation</subject><subject>Mathematics/Statistics</subject><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Comparing Families of Dynamic Causal Models</article-title><alt-title alt-title-type="running-head">Comparing Model Families</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Penny</surname><given-names>Will D.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>Klaas E.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>Jean</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Rosa</surname><given-names>Maria J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schofield</surname><given-names>Thomas M.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Leff</surname><given-names>Alex P.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Wellcome Trust Centre for Neuroimaging, University College, London, United Kingdom</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Branco-Weiss Laboratory for Social and Neural Systems Research, Empirical Research in Economics, University of Zurich, Zurich, Switzerland</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Kording</surname><given-names>Konrad P.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">Northwestern University, United States of America</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">w.penny@fil.ion.ucl.ac.uk</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: TMS APL. Performed the experiments: TMS APL. Analyzed the data: WDP MJR TMS APL. Wrote the paper: WDP KES JD KJF.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>3</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>12</day><month>3</month><year>2010</year></pub-date><volume>6</volume><issue>3</issue><elocation-id>e1000709</elocation-id><history>
<date date-type="received"><day>2</day><month>10</month><year>2009</year></date>
<date date-type="accepted"><day>8</day><month>2</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Penny et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Mathematical models of scientific data can be formally compared using Bayesian model evidence. Previous applications in the biological sciences have mainly focussed on model selection in which one first selects the model with the highest evidence and then makes inferences based on the parameters of that model. This “best model” approach is very useful but can become brittle if there are a large number of models to compare, and if different subjects use different models. To overcome this shortcoming we propose the combination of two further approaches: (i) family level inference and (ii) Bayesian model averaging within families. Family level inference removes uncertainty about aspects of model structure other than the characteristic of interest. For example: What are the inputs to the system? Is processing serial or parallel? Is it linear or nonlinear? Is it mediated by a single, crucial connection? We apply Bayesian model averaging within families to provide inferences about parameters that are independent of further assumptions about model structure. We illustrate the methods using Dynamic Causal Models of brain imaging data.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>Bayesian model comparison provides a formal method for evaluating different computational models in the biological sciences. Emerging application domains include dynamical models of neuronal and biochemical networks based on differential equations. Much previous work in this area has focussed on selecting the single best model. This approach is useful but can become brittle if there are a large number of models to compare and if different subjects use different models. This paper shows that these problems can be overcome with the use of Family Level Inference and Bayesian Model Averaging within model families.</p>
</abstract><funding-group><funding-statement>This work was supported by the Wellcome Trust. JD and KES also acknowledge support from Systems X, the Swiss Systems Biology Initiative and the University Research Priority Program “Foundations of Human Social Behavior” at the University of Zurich. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Mathematical models of scientific data can be formally compared using Bayesian model evidence <xref ref-type="bibr" rid="pcbi.1000709-Gelman1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1000709-Mackay1">[3]</xref>, an approach that is now widely used in statistics <xref ref-type="bibr" rid="pcbi.1000709-Hoeting1">[4]</xref>, signal processing <xref ref-type="bibr" rid="pcbi.1000709-Penny1">[5]</xref>, machine learning <xref ref-type="bibr" rid="pcbi.1000709-Beal1">[6]</xref>, natural language processing <xref ref-type="bibr" rid="pcbi.1000709-Kemp1">[7]</xref>, and neuroimaging <xref ref-type="bibr" rid="pcbi.1000709-Penny2">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1000709-Friston1">[10]</xref>. An emerging area of application is the evaluation of dynamical system models represented using differential equations, both in neuroimaging <xref ref-type="bibr" rid="pcbi.1000709-Penny3">[11]</xref> and systems biology <xref ref-type="bibr" rid="pcbi.1000709-Girolami1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1000709-Toni1">[14]</xref>.</p>
<p>Much previous practice in these areas has focussed on model selection in which one first selects the model with the highest evidence and then makes inferences based on the parameters of that model <xref ref-type="bibr" rid="pcbi.1000709-Acs1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1000709-Summerfield1">[18]</xref>. This ‘best model’ approach is very useful but, as we shall see, can become brittle if there are a large number of models to compare, or if in the analysis of data from a group of subjects, different subjects use different models (as is the case for a random effects analysis <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>). This brittleness, refers to the fact that which is the best model can depend critically on which set of models are being compared. In random effects analysis, augmenting the comparison set with a single extra model can, for example, reverse the ranking of the best and second best models. To address this issue we propose the combination of two further approaches (i) family level inference and (ii) Bayesian model averaging within families.</p>
<p>We envisage that these methods will be useful for the comparison of large numbers of models (eg. tens, hundreds or thousands). In the context of neuroimaging, for example, inferences about changes in brain connectivity can be made using Dynamic Causal Models <xref ref-type="bibr" rid="pcbi.1000709-Friston2">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Friston3">[21]</xref>. These are differential equation models which relate neuronal activity in different brain areas using a dynamical systems approach. One can then ask a number of generic questions. For example: Is processing serial or parallel? Is it linear or nonlinear? Is it mediated by changes in forward or backward connections? A schematic of a DCM used in this paper is shown in <xref ref-type="fig" rid="pcbi-1000709-g001">Figure 1</xref>. The particular questions we will address in this paper are (i) which regions receive driving input? and (ii) which connections are modulated by other experimental factors?</p>
<fig id="pcbi-1000709-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g001</object-id><label>Figure 1</label><caption>
<title>Dynamic Causal Models.</title>
<p>The DCMs in this paper were used to analyse fMRI data from three brain regions: (i) left posterior temporal sulcus (region P), (ii) left anterior superior temporal sulcus (region A) and (iii) pars orbitalis of the inferior frontal gyrus (region F). The DCMs themselves comprised the following variables; experimental inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e001" xlink:type="simple"/></inline-formula> for auditory stimulation and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e002" xlink:type="simple"/></inline-formula> for speech intelligibility, a neuronal activity vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e003" xlink:type="simple"/></inline-formula> with three elements (one for each region P, A, and F), exogenous connections specified by the three-by-three connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e004" xlink:type="simple"/></inline-formula> (dotted arrows in figure), modulatory connections specified by three-by-three modulatory matrics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e005" xlink:type="simple"/></inline-formula> for inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e006" xlink:type="simple"/></inline-formula> (the solid line ending with a filled circle denotes the single non-zero entry for this particular model), and a 3-by-2 direct input connectivity matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e007" xlink:type="simple"/></inline-formula> with non-zero entries shown by solid arrows. The dynamics of this model are govenered by equation 1. All DCMs in this paper used all-to-all endogenous connectivity ie. there were endogenous connections between all three regions. Different models were set up by specifying which regions received direct (auditory) input (non-zero entries in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e008" xlink:type="simple"/></inline-formula>) and which connections could be modulated by the speech intelligibility (non-zero entries in the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e009" xlink:type="simple"/></inline-formula>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g001" xlink:type="simple"/></fig>
<p>This paper proposes that the above questions are best answered by ‘Family level inference’. That is inference at the level of model families, rather than at the level of the individual models themselves. As a simple example, in previous work <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref> we have considered comparison of a number of DCMs, half of which embodied linear hemodynamics and half nonlinear hemodynamics. The model space was thus partitioned into two families; linear and nonlinear. One can compute the relative evidence of the two model families to answer the question: does my imaging data provide evidence in favour of linear versus nonlinear hemodynamics? This effectively removes uncertainty about aspects of model structure other than the characteristic of interest.</p>
<p>We have provided a simple illustration of this approach in previous work <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>. We now provide a formal introduction to family level inference and describe the key issues. These include, importantly, the issue of how to deal with families that do not contain the same number of models. Additionally, this paper shows how Bayesian model averaging can be used to provide a summary measure of likely parameter values for each model family. We provide an example of family-level inference using data from neuroimaging, a DCM study of auditory word processing, but envisage that the methods can be applied throughout the biological sciences. Before proceeding we note that the use of Bayesian model averaging is a standard approach in the field of Bayesian statistics <xref ref-type="bibr" rid="pcbi.1000709-Hoeting1">[4]</xref>, but has yet to be applied extensively in computational biology. The use of model families is also accomodated naturally within the framework of hierarchical Bayesian models <xref ref-type="bibr" rid="pcbi.1000709-Gelman1">[1]</xref> and is proposed to address the well known issue of model dilution <xref ref-type="bibr" rid="pcbi.1000709-Hoeting1">[4]</xref>.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>This section first briefly reviews DCM and methods for computing the model evidence. We then review the fixed and random effects methods for group level model inference, which differ as to whether or not subjects are thought to use the same or a different model. This includes the description of a novel Gibbs sampling method for random effects model inference that is useful when there are many models to compare. We then show that, for random effects inference, the selection of the single best model can be critically dependent on the set of models that are to be compared. This then motivates the subsequent subsection on family level inference, in which inferences about model characteristics are invariant to the comparison set. We describe family level inference in both a fixed and random effects context. The final subsection then describes a sample-based algorithm for implementing Bayesian model averaging using the notion of model families.</p>
<sec id="s2a">
<title>Dynamic Causal Models</title>
<p>Dynamic Causal Modelling is a framework for fitting differential equation models of neuronal activity to brain imaging data using Bayesian inference. The DCM approach can be applied to functional Magnetic Resonance Imaging (fMRI), Electroencephalographic (EEG), Magnetoencephalographic (MEG), and Local Field Potential (LFP) data <xref ref-type="bibr" rid="pcbi.1000709-Daunizeau1">[22]</xref>. The empirical work in this paper uses DCM for fMRI. DCMs for fMRI comprise a bilinear model for the neurodynamics and an extended Balloon model <xref ref-type="bibr" rid="pcbi.1000709-Friston4">[23]</xref> for the hemodynamics. The neurodynamics are described by the following multivariate differential equation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e010" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e011" xlink:type="simple"/></inline-formula> indexes continuous time and the dot notation denotes a time derivative. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e012" xlink:type="simple"/></inline-formula>th entry in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e013" xlink:type="simple"/></inline-formula> corresponds to neuronal activity in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e014" xlink:type="simple"/></inline-formula>th region, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e015" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e016" xlink:type="simple"/></inline-formula>th experimental input.</p>
<p>A DCM is characterised by a set of ‘exogenous connections’, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e017" xlink:type="simple"/></inline-formula>, that specify which regions are connected and whether these connections are unidirectional or bidirectional. We also define a set of input connections, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e018" xlink:type="simple"/></inline-formula>, that specify which inputs are connected to which regions, and a set of modulatory connections, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e019" xlink:type="simple"/></inline-formula>, that specify which intrinsic connections can be changed by which inputs. The overall specification of input, intrinsic and modulatory connectivity comprise our assumptions about model structure. This in turn represents a scientific hypothesis about the structure of the large-scale neuronal network mediating the underlying cognitive function. A schematic of a DCM is shown in <xref ref-type="fig" rid="pcbi-1000709-g001">Figure 1</xref>.</p>
<p>In DCM, neuronal activity gives rise to fMRI activity by a dynamic process described by an extended Balloon model <xref ref-type="bibr" rid="pcbi.1000709-Buxton1">[24]</xref> for each region. This specifies how changes in neuronal activity give rise to changes in blood oxygenation that are measured with fMRI. It involves a set of hemodynamic state variables, state equations and hemodynamic parameters, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e020" xlink:type="simple"/></inline-formula>. In brief, for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e021" xlink:type="simple"/></inline-formula>th region, neuronal activity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e022" xlink:type="simple"/></inline-formula> causes an increase in vasodilatory signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e023" xlink:type="simple"/></inline-formula> that is subject to autoregulatory feedback. Inflow <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e024" xlink:type="simple"/></inline-formula> responds in proportion to this signal with concomitant changes in blood volume <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e025" xlink:type="simple"/></inline-formula> and deoxyhemoglobin content <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e026" xlink:type="simple"/></inline-formula>.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e027" xlink:type="simple"/><label>(2)</label></disp-formula>Outflow is related to volume <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e028" xlink:type="simple"/></inline-formula> through Grubb's exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e029" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000709-Friston2">[20]</xref>. The oxygen extraction is a function of flow <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e030" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e031" xlink:type="simple"/></inline-formula> is resting oxygen extraction fraction. The Blood Oxygenation Level Dependent (BOLD) signal is then taken to be a static nonlinear function of volume and deoxyhemoglobin that comprises a volume-weighted sum of extra- and intra-vascular signals <xref ref-type="bibr" rid="pcbi.1000709-Friston2">[20]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e032" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e033" xlink:type="simple"/></inline-formula> is resting blood volume fraction. The hemodynamic parameters comprise <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e034" xlink:type="simple"/></inline-formula> and are specific to each brain region. Together these equations describe a nonlinear hemodynamic process that converts neuronal activity in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e035" xlink:type="simple"/></inline-formula> th region <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e036" xlink:type="simple"/></inline-formula> to the fMRI signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e037" xlink:type="simple"/></inline-formula> (which is additionally corrupted by additive Gaussian noise). Full details are given in <xref ref-type="bibr" rid="pcbi.1000709-Friston2">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Friston4">[23]</xref>.</p>
<p>In DCM, model parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e038" xlink:type="simple"/></inline-formula> are estimated using Bayesian methods. Usually, the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e039" xlink:type="simple"/></inline-formula> parameters are of greatest interest as these describe how connections between brain regions are dependent on experimental manipulations. For a given DCM indexed by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e040" xlink:type="simple"/></inline-formula>, a prior distribution, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e041" xlink:type="simple"/></inline-formula> is specified using biophysical and dynamic constraints <xref ref-type="bibr" rid="pcbi.1000709-Friston2">[20]</xref>. The likelihood, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e042" xlink:type="simple"/></inline-formula> can be computed by numerically integrating the neurodynamic (equation 1) and hemodynamic processes (equation 2). The posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e043" xlink:type="simple"/></inline-formula> is then estimated using a nonlinear variational approach described in <xref ref-type="bibr" rid="pcbi.1000709-Friston4">[23]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Friston5">[25]</xref>. Other Bayesian estimation algorithms can, of course, be used to approximate the posterior density. Reassuringly, posterior confidence regions found using the nonlinear variational approach have been found to be very similar to those obtained using a computationally more expensive sample-based algorithm <xref ref-type="bibr" rid="pcbi.1000709-Chumbley1">[26]</xref>.</p>
</sec><sec id="s2b">
<title>Model Evidence</title>
<p>This section reviews methods for computing the evidence for a model, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e044" xlink:type="simple"/></inline-formula>, fitted to a single data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e045" xlink:type="simple"/></inline-formula>. Bayesian estimation provides estimates of two quantities. The first is the posterior distribution over model parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e046" xlink:type="simple"/></inline-formula> which can be used to make inferences about model parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e047" xlink:type="simple"/></inline-formula>. The second is the probability of the data given the model, otherwise known as the model evidence. In general, the model evidence is not straightforward to compute, since this computation involves integrating out the dependence on model parameters<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e048" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>A common technique for approximating the above integral is the Variational Bayes (VB) approach <xref ref-type="bibr" rid="pcbi.1000709-Penny4">[27]</xref>. This is an analytic method that can be formulated by analogy with statistical physics as a gradient ascent on the ‘negative variational Free Energy’ (or Free Energy for short), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e049" xlink:type="simple"/></inline-formula>, of the system. This quantity is related to the model evidence by the relation <xref ref-type="bibr" rid="pcbi.1000709-Penny4">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Beal2">[28]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e050" xlink:type="simple"/><label>(5)</label></disp-formula>where the last term in Eq.(5) is the Kullback-Leibler (KL) divergence between an ‘approximate’ posterior density, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e051" xlink:type="simple"/></inline-formula>, and the true posterior, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e052" xlink:type="simple"/></inline-formula>. This quantity is always positive, or zero when the densities are identical, and therefore <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e053" xlink:type="simple"/></inline-formula> is bounded below by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e054" xlink:type="simple"/></inline-formula>. Because the evidence is fixed (but unknown), maximising <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e055" xlink:type="simple"/></inline-formula> implicitly minimises the KL divergence. The Free Energy then becomes an increasingly tighter lower bound on the desired log-model evidence. Under the assumption that this bound is tight, model comparison can then proceed using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e056" xlink:type="simple"/></inline-formula> as a surrogate for the log-model evidence.</p>
<p>The Free Energy is but one approximation to the model evidence, albeit one that is widely used in neuroimaging <xref ref-type="bibr" rid="pcbi.1000709-Woolrich1">[29]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Sato1">[30]</xref>. A simpler approximation, the Bayesian Information Criterion (BIC) <xref ref-type="bibr" rid="pcbi.1000709-Penny3">[11]</xref>, uses a fixed complexity penalty for each parameter. This is to be compared with the free energy approach in which the complexity penalty is given by the KL-divergence between the prior and approximate posterior <xref ref-type="bibr" rid="pcbi.1000709-Penny3">[11]</xref>. This allows parameters to be differentially penalised. If, for example, a parameter is unchanged from its prior, there will be no penalty. This adaptability makes the Free Energy a better approximation to the model evidence, as has been shown empirically <xref ref-type="bibr" rid="pcbi.1000709-Beal1">[6]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Roberts1">[31]</xref>.</p>
<p>There are also a number of sample-based approximations to the model evidence. For models with small numbers of parameters the Posterior Harmomic Mean provides a good approximation. This has been used in neuroscience applications, for example, to infer based on spike data whether neurons are responsive to particular features, and if so what form the dependence takes <xref ref-type="bibr" rid="pcbi.1000709-Cronin1">[32]</xref>. For models with a larger number of parameters the evidence can be well approximated using Annealed Importance Sampling (AIS) <xref ref-type="bibr" rid="pcbi.1000709-Neal1">[33]</xref>. In a comparison of sample-based methods using synthetic data from biochemical networks, AIS provided the best balance between accuracy and computation time <xref ref-type="bibr" rid="pcbi.1000709-Vyshemirsky1">[13]</xref>. In other comparisons, based on simulation of graphical model structures <xref ref-type="bibr" rid="pcbi.1000709-Beal1">[6]</xref> the Free Energy method approached the performance of AIS and clearly outperformed BIC. In this paper model evidence is approximated using the Free Energy.</p>
</sec><sec id="s2c">
<title>Fixed Effects Analysis</title>
<p>Neuroimaging data sets usually comprise data from multiple subjects as the perhaps subtle cognitive effects one is interested in are often only manifest at the group level. In this and following sections we therefore consider group model inference where we fit models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e057" xlink:type="simple"/></inline-formula> to data from subjects <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e058" xlink:type="simple"/></inline-formula>. Every model is fitted to every subjects data. In Fixed Effects (FFX) Analysis it is assumed that every subject uses the same model, whereas Random Effects (RFX) Analysis allows for the possibility that different subjects use different models. This section focusses on FFX.</p>
<p>Given that our overall data set, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e059" xlink:type="simple"/></inline-formula>, which comprises data for each subject, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e060" xlink:type="simple"/></inline-formula>, is independent over subjects, we can write the overall model evidence as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e061" xlink:type="simple"/><label>(6)</label></disp-formula>Bayesian inference at the model level can then be implemented using Bayes rule<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e062" xlink:type="simple"/><label>(7)</label></disp-formula>Under uniform model priors, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e063" xlink:type="simple"/></inline-formula>, the comparison of a pair of models, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e064" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e065" xlink:type="simple"/></inline-formula>, can be implemented using the Bayes Factor which is defined as the ratio of model evidences<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e066" xlink:type="simple"/><label>(8)</label></disp-formula>Given only two models and uniform priors, the posterior model probability is greater than 0.95 if the BF is greater than twenty. Bayes Factors have also been stratified into different ranges deemed to correspond to different strengths of evidence. ‘Strong’ evidence, for example, corresponds to a BF of over twenty <xref ref-type="bibr" rid="pcbi.1000709-Raftery1">[34]</xref>. Under non-uniform priors, pairs of models can be compared using Odds Ratios. The prior and posterior Odds Ratios are defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e067" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e068" xlink:type="simple"/></disp-formula>resepectively, and are related by the Bayes Factor<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e069" xlink:type="simple"/><label>(10)</label></disp-formula>When comparing two models across a group of subjects, one can multiply the individual Bayes factors (or exponentiate the sum of log evidence differences); this is referred to as the Group Bayes Factor (GBF) <xref ref-type="bibr" rid="pcbi.1000709-Stephan1">[16]</xref>. As is made clear in <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref> the GBF approach implicitly assumes that every subject uses the same model. It is therefore a Fixed Effects analysis. If one believes that the optimal model structure is identical across subjects, then an FFX approach is entirely valid. This assumption is warranted when studying a basic physiological mechanism that is unlikely to vary across subjects, such as the role of forward and backward connections in visual processing <xref ref-type="bibr" rid="pcbi.1000709-Chen1">[35]</xref>.</p>
</sec><sec id="s2d">
<title>Random Effects Analysis</title>
<p>An alternative procedure for group level model inference allows for the possibility that different subjects use different models. This may be the case in neuroimaging when investigating pathophysiological mechanisms in a spectrum disease or when dealing with cognitive tasks that can be performed with different strategies. RFX inference is based on the characteristics of the population from which the subjects are drawn. Given a candidate set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e070" xlink:type="simple"/></inline-formula> models, we denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e071" xlink:type="simple"/></inline-formula> as the frequency with which model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e072" xlink:type="simple"/></inline-formula> is used in the population. We also refer to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e073" xlink:type="simple"/></inline-formula> as the model probability.</p>
<p>We define a prior distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e074" xlink:type="simple"/></inline-formula> which in this paper, and in previous work <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>, is taken to be a Dirichlet density (but see later)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e075" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e076" xlink:type="simple"/></inline-formula> is a normalisation term and the parameters, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e077" xlink:type="simple"/></inline-formula>, are strictly positively valued and can be interpreted as the number of times model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e078" xlink:type="simple"/></inline-formula> has been observed or selected. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e079" xlink:type="simple"/></inline-formula> the density is convex in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e080" xlink:type="simple"/></inline-formula>-space, whereas for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e081" xlink:type="simple"/></inline-formula> it is concave.</p>
<p>Given that we have drawn <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e082" xlink:type="simple"/></inline-formula> subjects from the population of interest we then define the indicator variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e083" xlink:type="simple"/></inline-formula> as equal to unity if model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e084" xlink:type="simple"/></inline-formula> has been assigned to subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e085" xlink:type="simple"/></inline-formula>. The probability of the ‘assignation vector’, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e086" xlink:type="simple"/></inline-formula>, is then given by the multinomial density<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e087" xlink:type="simple"/><label>(12)</label></disp-formula>The model evidence, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e088" xlink:type="simple"/></inline-formula>, together with the above densities for model probabilities and model assignations constitutes a generative model for the data, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e089" xlink:type="simple"/></inline-formula> (see figure 1 in <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>). This model, can then be inverted to make inferences about the model probabilites from experimental data. Such an inversion has been described in previous work, which developed an approximate inference procedure based on a variational approximation <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref> (this was in addition to the variational approximation used to compute the Free Energy for each model). The robustness and accuracy of this method was verified via simulations using data from synthetic populations with known frequencies of competing models <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>. This algorithm produces an approximation to the posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e090" xlink:type="simple"/></inline-formula> on which subsequent RFX inferences are based.</p>
<p>As we shall see in the following section, unbiased family level inferences require uniform priors over families. This requires that the prior model counts, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e091" xlink:type="simple"/></inline-formula>, take on very small values (see equation 24). These values become smaller as the number of models in a family increases. It turns out that although the variational algorithm is robust for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e092" xlink:type="simple"/></inline-formula>, it is not accurate for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e093" xlink:type="simple"/></inline-formula>. This is a generic problem with the VB approach and is explained further in the the supporting material (see file <xref ref-type="supplementary-material" rid="pcbi.1000709.s001">Text S1</xref>). For this reason, in this paper we choose to take a Gibbs sampling instead of a VB approach. Additionally, the use of Gibbs sampling allows us to relax the assumption made in VB that the posterior densities over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e094" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e095" xlink:type="simple"/></inline-formula> factorise <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>. Gibbs sampling is the Monte-Carlo method of choice when it is possible to iteratively sample from the conditional posteriors <xref ref-type="bibr" rid="pcbi.1000709-Gelman1">[1]</xref>. Fortunately, this is the case with the RFX models as we can iterate between sampling from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e096" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e097" xlink:type="simple"/></inline-formula>. Such iterated sampling eventually produces samples from the marginal posteriors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e098" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e099" xlink:type="simple"/></inline-formula> by allowing for a sufficient burn-in period after which the Markov-chain will have converged <xref ref-type="bibr" rid="pcbi.1000709-Gelman1">[1]</xref>. The procedure is described in the following section.</p>
<sec id="s2d1">
<title>Gibbs sampling for random effects inference over models</title>
<p>First, model probabilites are drawn from the prior distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e100" xlink:type="simple"/><label>(13)</label></disp-formula>where by default we set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e101" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e102" xlink:type="simple"/></inline-formula> (but see later). For each subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e103" xlink:type="simple"/></inline-formula> and model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e104" xlink:type="simple"/></inline-formula> we use the model evidences from model inversion to compute<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e105" xlink:type="simple"/><label>(14)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e106" xlink:type="simple"/></disp-formula>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e107" xlink:type="simple"/></inline-formula> is our posterior belief that model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e108" xlink:type="simple"/></inline-formula> generated the data from subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e109" xlink:type="simple"/></inline-formula> (these posteriors will be used later for Bayesian model averaging). For each subject, model assignation vectors are then drawn from the multinomial distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e110" xlink:type="simple"/><label>(15)</label></disp-formula>We then compute new model counts<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e111" xlink:type="simple"/><label>(16)</label></disp-formula>and draw new model probabilities<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e112" xlink:type="simple"/><label>(17)</label></disp-formula>Equations 14 to 17 are then iterated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e113" xlink:type="simple"/></inline-formula> times. For the results in this paper we used a total of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e114" xlink:type="simple"/></inline-formula> samples and discarded the first 10,000. These remaining samples then constitute our approximation to the posterior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e115" xlink:type="simple"/></inline-formula>. From this density we can compute usual quantities such as the posterior expectation, denoted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e116" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e117" xlink:type="simple"/></inline-formula>. This completes the description of model level inference.</p>
<p>The above algorithm was derived for Dirichlet priors over model probabilities (see equation 11). The motivation for the Dirichlet form originally derived from the use of a free-form VB approximation <xref ref-type="bibr" rid="pcbi.1000709-Penny4">[27]</xref> in which the optimal form for the approximate posterior density over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e118" xlink:type="simple"/></inline-formula> would be a Dirichlet if the prior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e119" xlink:type="simple"/></inline-formula> was also a Dirichlet. This is not a concern in the context of Gibbs sampling. In principle any prior density over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e120" xlink:type="simple"/></inline-formula> will do, but for continuity with previous work we follow the Dirichlet approach.</p>
<p>We end this section by noting that the Gibbs sampling method is to be preferred over the VB implementation for model level inferences in which the number of models exceeds the number of subjects, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e121" xlink:type="simple"/></inline-formula>. This is because it is important that the total prior count, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e122" xlink:type="simple"/></inline-formula>, does not dominate over the number of subjects, otherwise posterior densities will be dominated by the prior rather than the data. This is satisfied, for example, by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e123" xlink:type="simple"/></inline-formula>. However, as described in the the supporting material (see file <xref ref-type="supplementary-material" rid="pcbi.1000709.s001">Text S1</xref>), the VB implementation does not work well for small <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e124" xlink:type="simple"/></inline-formula>. But if we wish to compare a small number of models then VB is the preferred method because it is faster as well as being accurate, as shown in previous simulations <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>.</p>
</sec></sec><sec id="s2e">
<title>Comparison Set</title>
<p>We have so far described procedures for Bayesian inference over models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e125" xlink:type="simple"/></inline-formula>. These models comprise the comparison set, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e126" xlink:type="simple"/></inline-formula>. This section points out a number of generic features of Bayesian model comparison.</p>
<p>First, for any data set there exists an infinite number of possible models that could explain it. The purpose of model comparison is not to discover a ‘true’ model, but to determine that model, given a set of plausible alternatives, which is most ‘useful’, ie. represents an optimal balance between accuracy and complexity. In other words Bayesian model inference has nothing to say about ‘true’ models. All that it provides is an inference about which is more likely, given the data, among a set of candidate models.</p>
<p>Second, we emphasise that posterior model probabilities depend on the comparison set. For FFX inference this can be clearly seen in equation 7 where the denominator is given by a sum over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e127" xlink:type="simple"/></inline-formula>. Similarly, for RFX inference, the dependence of posterior model probabilities on the comparison set can be seen in equation 14. Other factors being constant, posterior model probabilities are therefore likely to be smaller for larger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e128" xlink:type="simple"/></inline-formula>.</p>
<p>Our third point relates to the ranking of models. For FFX analysis the relative ranking of a pair of models is not dependent on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e129" xlink:type="simple"/></inline-formula>. That is, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e130" xlink:type="simple"/></inline-formula> then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e131" xlink:type="simple"/></inline-formula> for any two comparison sets <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e132" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e133" xlink:type="simple"/></inline-formula> that contain models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e134" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e135" xlink:type="simple"/></inline-formula>. This follows trivially from equation 7 as the comparison set acts only as a normalisation term.</p>
<p>However, for group random effects inference the ranking of models can be critically dependent on the comparison set. That is, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e136" xlink:type="simple"/></inline-formula> then it could be that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e137" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e138" xlink:type="simple"/></inline-formula> is the posterior expected probability of model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e139" xlink:type="simple"/></inline-formula> given comparison set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e140" xlink:type="simple"/></inline-formula>. The same holds for other quantities derived from the posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e141" xlink:type="simple"/></inline-formula>, such as the exceedance probability (see <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref> and later). This means that the decision as to which is the best model depends on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e142" xlink:type="simple"/></inline-formula>. This property arises because different subjects can use different models and we illustrate it with the following example.</p>
<p>Consider that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e143" xlink:type="simple"/></inline-formula> comprises just two models <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e144" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e145" xlink:type="simple"/></inline-formula>. Further assume that we have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e146" xlink:type="simple"/></inline-formula> subjects and model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e147" xlink:type="simple"/></inline-formula> is preferred by 7 of these subjects and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e148" xlink:type="simple"/></inline-formula> by the remaining 10. We assume, for simplicity, that the degrees of preference (ie differences in evidence) are the same for each subject. The quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e149" xlink:type="simple"/></inline-formula> then simply reflects the proportion of subjects that prefer model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e150" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>. So <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e151" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e152" xlink:type="simple"/></inline-formula> and for comparison set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e153" xlink:type="simple"/></inline-formula> model 2 is the highest ranked model. Although the differences in posterior expected values are small the corresponding differences in exceedance probabilities will be much greater. Now consider a new comparison set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e154" xlink:type="simple"/></inline-formula> that contains an addditional model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e155" xlink:type="simple"/></inline-formula>. This model is very <italic>similar</italic> to model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e156" xlink:type="simple"/></inline-formula> such that, of the ten subjects who previously preferred it, six still do but four now prefer model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e157" xlink:type="simple"/></inline-formula>. Again, assuming identical degrees of preference, we now have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e158" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e159" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e160" xlink:type="simple"/></inline-formula>. So, for comparison set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e161" xlink:type="simple"/></inline-formula> model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e162" xlink:type="simple"/></inline-formula> is now the best model. So which is the best model: model one or two?</p>
<p>We suggest that this seeming paradox shows, not that group random effects inference is unreliable, but that it is not always appropriate to ask which is the best model. As is usual in Bayesian inference it is wise to consider the full posterior density rather than just the single maximum posterior value. We can ask what is common to models two and three. Perhaps they share some structural assumption such as the existence of certain connections or other characteristic such as nonlinearity. If one were to group the models based on this characteristic then the inference <italic>about the characteristic</italic> would be robust. This notion of grouping models together is formalised using family-level inference which is described in the following section. One can then ask: of the models that have this characteristic what are the typical parameter values? This can be addressed using Bayesian Model Averaging within families.</p>
</sec><sec id="s2f">
<title>Family Inference</title>
<p>To implement family level inference one must specify which models belong to which families. This amounts to specifying a partition, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e163" xlink:type="simple"/></inline-formula>, which splits S into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e164" xlink:type="simple"/></inline-formula> disjoint subsets. The subset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e165" xlink:type="simple"/></inline-formula> contains all models belonging family <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e166" xlink:type="simple"/></inline-formula> and there are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e167" xlink:type="simple"/></inline-formula> models in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e168" xlink:type="simple"/></inline-formula> th subset.</p>
<p>Different questions can be asked by specifying different partitions. For example, to test model space for the ‘effect of linearity’ one would specify a partition into linear and nonlinear subsets. One could then test the same model space for the ‘effect of seriality’ using a different partition comprising serial and parallel subsets. The subsets must be non-overlapping and their union must be equal to S. For example, when testing for effects of “seriality”, some models may be neither serial or parallel; these models would then define a third subset.</p>
<p>The usefulness of the approach is that many models (perhaps all models) are used to answer (perhaps) all questions. This is similar to factorial experimental designs in psychology <xref ref-type="bibr" rid="pcbi.1000709-Howell1">[36]</xref> where data from all cells are used to assess the strength of main effects and interactions. We now relate the two-levels of inference: family and model.</p>
<sec id="s2f1">
<title>Fixed effects</title>
<p>To avoid any unwanted bias in our inference we wish to have a uniform prior at the family level<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e169" xlink:type="simple"/><label>(18)</label></disp-formula>Given that this is related to the model level as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e170" xlink:type="simple"/><label>(19)</label></disp-formula>the uniform family prior can be implemented by setting<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e171" xlink:type="simple"/><label>(20)</label></disp-formula>The posterior distribution over families is then given by summing up the relevant posterior model probabilities<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e172" xlink:type="simple"/><label>(21)</label></disp-formula>where the posterior over models is given by equation 7. Because posterior probabilities can be very close to unity we will sometimes quote one minus the posterior probability. This is the combined probability of the alternative hypotheses which we refer to as the alternative probability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e173" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2f2">
<title>Random effects</title>
<p>The family probabilities are given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e174" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e175" xlink:type="simple"/></inline-formula> is the frequency of the family of models in the population. We define a prior distribution over this probability using a Dirichlet density<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e176" xlink:type="simple"/><label>(23)</label></disp-formula>A uniform prior over family probabilities can be obtained by setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e177" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e178" xlink:type="simple"/></inline-formula>. From equations 13 and 22 we see that this can be achieved by setting<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e179" xlink:type="simple"/><label>(24)</label></disp-formula>We can then run the Gibbs sampling method described above for drawing samples from the posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e180" xlink:type="simple"/></inline-formula>. Samples from the family probability posterior, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e181" xlink:type="simple"/></inline-formula>, can then be computed using equation 22.</p>
<p>The posterior means, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e182" xlink:type="simple"/></inline-formula>, are readily computed from these samples. Another option is to compute an exceedance probability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e183" xlink:type="simple"/></inline-formula>, which corresponds to the belief that family <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e184" xlink:type="simple"/></inline-formula> is more likely than any other (of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e185" xlink:type="simple"/></inline-formula> families compared), given the data from all subjects:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e186" xlink:type="simple"/><label>(25)</label></disp-formula>Exceedance probabilities are particularly intuitive when comparing just two families as they can be written:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e187" xlink:type="simple"/><label>(26)</label></disp-formula></p>
<p>Family level inference addresses the issue of ‘dilution’ in model selection <xref ref-type="bibr" rid="pcbi.1000709-Hoeting1">[4]</xref>. If one uses uniform model priors and many models are similar, then excessive prior probability is allocated to this set of similar models. One way of avoiding this problem is to use priors which dilute the probability within subsets of similar models (<xref ref-type="bibr" rid="pcbi.1000709-Hoeting1">[4]</xref>). Grouping models into families, and setting model priors according to eg. equation 24, achieves exactly this.</p>
</sec></sec><sec id="s2g">
<title>Bayesian Model Averaging</title>
<p>So far, we have dealt with inference on model-space, using partitions into families. We now consider inference on parameters. Usually, the key inference is on models, while the maximum a posteriori (MAP) estimates of parameters are reported to provide a quantitative interpretation of the best model (or family). Alternatively, people sometimes use subject-specific MAP estimates as summary statistics for classical inference at the group level. These applications require only a point (MAP) estimate. However for completeness, we now describe how to access the full posterior density on parameters, from which MAP estimates can be harvested.</p>
<p>The basic idea here is to use Bayesian model averaging within a family; in other words, summarise family-specific coupling parameters in a way that avoids brittle assumptions about any particular model. For example, the marginal posterior for subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e188" xlink:type="simple"/></inline-formula> and family <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e189" xlink:type="simple"/></inline-formula> is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e190" xlink:type="simple"/><label>(27)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e191" xlink:type="simple"/></inline-formula> is our variational approximation to the subject specific posterior and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e192" xlink:type="simple"/></inline-formula> is the posterior probability that subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e193" xlink:type="simple"/></inline-formula> uses model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e194" xlink:type="simple"/></inline-formula>. We could take this to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e195" xlink:type="simple"/></inline-formula> under the FFX assumption that all subjects use the same model, or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e196" xlink:type="simple"/></inline-formula> under the RFX assumption that each subject uses their own model (see equation 14).</p>
<p>Finally, to provide a single posterior density over subjects one can define the parameters for an average subject<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e197" xlink:type="simple"/><label>(28)</label></disp-formula>and compute the posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e198" xlink:type="simple"/></inline-formula> from the above relation and the individual subject posteriors from equation 27.</p>
<p>Equation 27 arises from a straightforward application of probability theory in which a marginal probability is computed by marginalising over quantities one is uninterested in (see also equation 4 for marginalising over parameters). Use of equation 27 in this context is known as Bayesian Model Averaging (BMA) <xref ref-type="bibr" rid="pcbi.1000709-Hoeting1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000709-Penny5">[37]</xref>. In neuroimaging BMA has previously been used for source reconstruction of MEG and EEG data <xref ref-type="bibr" rid="pcbi.1000709-TrujilloBarreto1">[9]</xref>. We stress that no additional assumptions are required to implement equation 27.</p>
<p>One can make <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e199" xlink:type="simple"/></inline-formula> small or large. If we make <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e200" xlink:type="simple"/></inline-formula>, the entire model-space, the posteriors on the parameters become conventional Bayesian model averages where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e201" xlink:type="simple"/></inline-formula>. Conversely, if we make <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e202" xlink:type="simple"/></inline-formula>, a single model, we get conventional parameter inference of the sort used when selecting the best model; i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e203" xlink:type="simple"/></inline-formula>. This is formally identical to using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e204" xlink:type="simple"/></inline-formula> under the assumption that the posterior model density is a point mass at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e205" xlink:type="simple"/></inline-formula>. More generally, we want to average within families of similar models that have been identified by inference on families.</p>
<p>One can see from equation 27 that models with low probability contribute little to the estimate of the marginal density. This property can be made use of to speed up the implementation of BMA by excluding low probability models from the summation. This can be implemented by including only models for which<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e206" xlink:type="simple"/><label>(29)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e207" xlink:type="simple"/></inline-formula> is the minimal posterior odds ratio. Models satisfying this criterion are said to be in Occam's window <xref ref-type="bibr" rid="pcbi.1000709-Madigan1">[38]</xref>. The number of models in the window, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e208" xlink:type="simple"/></inline-formula>, is a useful indicator as smaller values correspond to peakier posteriors. In this paper we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e209" xlink:type="simple"/></inline-formula>. We emphasise that the use of Occam's window is for computational expedience only.</p>
<p>Although it is fairly simple to compute the MAP estimates of the Bayesian parameter (MAP) averages analytically, the full posteriors per se have a complicated form. This is because they are mixtures of Gaussians (and delta functions for models where some parameters are precluded a priori). This means the posteriors can be multimodal and are most simply evaluated by sampling. The sampling approach can be implemented as follows. This generates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e210" xlink:type="simple"/></inline-formula> samples from the posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e211" xlink:type="simple"/></inline-formula>. For each sample, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e212" xlink:type="simple"/></inline-formula>, and subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e213" xlink:type="simple"/></inline-formula> we first select a model as follows. For RFX we draw from<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e214" xlink:type="simple"/><label>(30)</label></disp-formula>where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e215" xlink:type="simple"/></inline-formula>th element of the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e216" xlink:type="simple"/></inline-formula> is the posterior model probability for subject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e217" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e218" xlink:type="simple"/></inline-formula> (we will use the expected values from equation 14). For FFX the model probabilities are the same for all subjects and we draw from<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e219" xlink:type="simple"/><label>(31)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e220" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e221" xlink:type="simple"/></inline-formula> vector of posterior model probabilities with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e222" xlink:type="simple"/></inline-formula>th element equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e223" xlink:type="simple"/></inline-formula>. For each subject one then draws a single parameter vector, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e224" xlink:type="simple"/></inline-formula> from the subject and model specific posterior<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e225" xlink:type="simple"/><label>(32)</label></disp-formula>These <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e226" xlink:type="simple"/></inline-formula> samples can then be averaged to produce a single sample<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e227" xlink:type="simple"/><label>(33)</label></disp-formula>One then generates another sample by repeating steps 30/31, 32 and 33. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e228" xlink:type="simple"/></inline-formula> samples then provide a sample-based representation of the posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e229" xlink:type="simple"/></inline-formula> from which the usual posterior means and exceedance probabilities can be derived. Model averaging can also be restricted to be within-subject (using equations 30/31 and 32 only). Summary statistics from the resulting within-subject densities can then be entered into standard random effects inference (eg using t-tests) <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>.</p>
<p>For any given parameter, some models assume that the parameter is zero. Other models allow it to be non-zero and its value is estimated. The posterior densities from equation 27 will therefore include a delta function at zero, the height of which corresponds to the posterior probability mass of models which assume that the parameter is zero. For the applications in this paper, the posterior densities from equation 27 will therefore correspond to a mixture of delta functions and Gaussians because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e230" xlink:type="simple"/></inline-formula> for DCMs have a Gaussian form. This is reminiscent of the model selection priors used in <xref ref-type="bibr" rid="pcbi.1000709-Clyde1">[39]</xref> but in our case we have posterior densities.</p>
</sec></sec><sec id="s3">
<title>Results</title>
<p>We illustrate the methods using neuroimaging data from a previously published study on the cortical dynamics of intelligible speech <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref>. This study applied dynamic causal modelling of fMRI responses to investigate activity among three key multimodal regions: the left posterior and anterior superior temporal sulcus (subsequently referred to as regions P and A respectively) and pars orbitalis of the inferior frontal gyrus (region F). The aim of the study was to see how connections among regions depended on whether the auditory input was intelligible speech or time-reversed speech. Full details of the experimental paradigm and imaging parameters are available in <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref>.</p>
<p>An example DCM is shown in <xref ref-type="fig" rid="pcbi-1000709-g001">figure 1</xref>. Other models varied as to which regions received direct input and which connections could be modulated by ‘speech intelligibility’. Given that each intrinsic connection can be either modulated or not, there are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e231" xlink:type="simple"/></inline-formula> possible patterns of modulatory connections. Given that the auditory stimulus is either a direct input to a region or is not there are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e232" xlink:type="simple"/></inline-formula> possible patterns of input connectivity. But we discount models without any input so this leaves 7 input patterns. The 64 modulatory patterns were then crossed with the 7 input patterns producing a total of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e233" xlink:type="simple"/></inline-formula> different models. These models were fitted to data from a total of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e234" xlink:type="simple"/></inline-formula> subjects (see <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref> for details). Overall <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e235" xlink:type="simple"/></inline-formula> DCMs were fitted. The next two sections focus on family level inference. As this is a methodological paper we present results using both an FFX and RFX approach (ordinarily one would use either FFX or RFX alone).</p>
<sec id="s3a">
<title>Input Regions</title>
<p>Our first family level inference concerns the pattern of input connectivity. To this end we assign each of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e236" xlink:type="simple"/></inline-formula> models to one of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e237" xlink:type="simple"/></inline-formula> input pattern families. These are family A (models 1 to 64), F (65 to 128), P (129 to 192), AF (193 to 256), PA (257 to 320), PF (321 to 384) and PAF (285 to 448). Family PA, for example, has auditory inputs to both region P and A.</p>
<p>The first two numerical columns of <xref ref-type="table" rid="pcbi-1000709-t001">Table 1</xref> show the posterior family probabilities from an FFX analysis computed using equation 21. These are overwhelmingly in support of models in which region P alone receives auditory input (alternative probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e238" xlink:type="simple"/></inline-formula>). The last two columns in <xref ref-type="table" rid="pcbi-1000709-t001">Table 1</xref> show the corresponding posterior expectations and exceedance probabilities from an RFX analysis computed using equation 25. The conclusions from RFX analysis are less clear cut. But we can say, with high confidence (total exceedance probability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e239" xlink:type="simple"/></inline-formula>) that either region A alone or region P alone receives auditory input. Out of these two possibilities it is much more likely that region P alone receives auditory input (exceedance probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e240" xlink:type="simple"/></inline-formula>) rather than region A (exceedance probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e241" xlink:type="simple"/></inline-formula>). <xref ref-type="fig" rid="pcbi-1000709-g002">Figure 2</xref> shows the posterior distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e242" xlink:type="simple"/></inline-formula>, from an RFX analysis, for each of the model families.</p>
<fig id="pcbi-1000709-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g002</object-id><label>Figure 2</label><caption>
<title>RFX posterior densities for input families.</title>
<p>The histograms show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e243" xlink:type="simple"/></inline-formula> versus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e244" xlink:type="simple"/></inline-formula> for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e245" xlink:type="simple"/></inline-formula> input families. Input family ‘P’ has the highest posterior expected probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e246" xlink:type="simple"/></inline-formula>. See <xref ref-type="table" rid="pcbi-1000709-t001">Table 1</xref> for other posterior expectations.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g002" xlink:type="simple"/></fig><table-wrap id="pcbi-1000709-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.t001</object-id><label>Table 1</label><caption>
<title>Inference over input families.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000709-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Input</td>
<td align="left" colspan="2" rowspan="1">FFX</td>
<td align="left" colspan="2" rowspan="1">RFX</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e247" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Log Posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e248" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Expected <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e249" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">exceedance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e250" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">A</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−25.33</td>
<td align="left" colspan="1" rowspan="1">0.27</td>
<td align="left" colspan="1" rowspan="1">0.19</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−55.08</td>
<td align="left" colspan="1" rowspan="1">0.16</td>
<td align="left" colspan="1" rowspan="1">0.03</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">P</td>
<td align="left" colspan="1" rowspan="1">1.00</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">0.44</td>
<td align="left" colspan="1" rowspan="1">0.78</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">AF</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−86.97</td>
<td align="left" colspan="1" rowspan="1">0.03</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PA</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−61.70</td>
<td align="left" colspan="1" rowspan="1">0.03</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PF</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−68.59</td>
<td align="left" colspan="1" rowspan="1">0.03</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PAF</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−134.67</td>
<td align="left" colspan="1" rowspan="1">0.03</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>All values are tabulated to two decimal places (dp). For an FFX inference, the alternative probability for input family <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e251" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e252" xlink:type="simple"/></inline-formula>. The expected and exceedance probabilities for RFX were computed from the posterior densities shown in <xref ref-type="fig" rid="pcbi-1000709-g002">Figure 2</xref>. For RFX inference the total exceedance probability that either region A alone or region P alone receives auditory input is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e253" xlink:type="simple"/></inline-formula>.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3b">
<title>Forward versus Backward</title>
<p>Having established that auditory input most likely enters region <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e254" xlink:type="simple"/></inline-formula> we now turn to a family level inference regarding modulatory structure. For this inference we restrict our set of candidate models, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e255" xlink:type="simple"/></inline-formula>, to the 64 models receiving input to region <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e256" xlink:type="simple"/></inline-formula>. We then assign each of these models to one of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e257" xlink:type="simple"/></inline-formula> modulatory families. These were specified by first defining a hierarchy with region P at the bottom, A in the middle and F at the top; in accordance with recent studies that tend to place F above A in the language hierarchy <xref ref-type="bibr" rid="pcbi.1000709-Visser1">[40]</xref>. For each structure we then counted the number of forward, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e258" xlink:type="simple"/></inline-formula>, and backward, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e259" xlink:type="simple"/></inline-formula>, connections and defined the following families: predominantly forward (F, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e260" xlink:type="simple"/></inline-formula>), predominantly backward (B, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e261" xlink:type="simple"/></inline-formula>), balanced (BAL, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e262" xlink:type="simple"/></inline-formula>), or None.</p>
<p>The first two numerical columns of <xref ref-type="table" rid="pcbi-1000709-t002">Table 2</xref> show the posterior family probabilities from an FFX analysis. We can say, with high confidence (total posterior probability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e263" xlink:type="simple"/></inline-formula>) that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e264" xlink:type="simple"/></inline-formula>. The last two columns in <xref ref-type="table" rid="pcbi-1000709-t002">Table 2</xref> show the posterior expectations and exceedance probabilities from an RFX analysis. These were computed from the posterior densities shown in <xref ref-type="fig" rid="pcbi-1000709-g003">Figure 3</xref>. The conclusions we draw, in this case, are identical to those from the FFX analysis. That is, we can say, with high confidence (total exceedance probability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e265" xlink:type="simple"/></inline-formula>) that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e266" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1000709-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g003</object-id><label>Figure 3</label><caption>
<title>RFX Posterior densities for modulatory families.</title>
<p>The histograms show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e267" xlink:type="simple"/></inline-formula> versus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e268" xlink:type="simple"/></inline-formula> for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e269" xlink:type="simple"/></inline-formula> modulatory families. Modulatory family ‘F’ has the highest posterior expected probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e270" xlink:type="simple"/></inline-formula>. See <xref ref-type="table" rid="pcbi-1000709-t002">Table 2</xref> for other posterior expectations.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g003" xlink:type="simple"/></fig><table-wrap id="pcbi-1000709-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.t002</object-id><label>Table 2</label><caption>
<title>Inference over modulatory families.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1000709-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Modulation</td>
<td align="left" colspan="2" rowspan="1">FFX</td>
<td align="left" colspan="2" rowspan="1">RFX</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e271" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Log Posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e272" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">Expected <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e273" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">exceedance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e274" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Forward, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e275" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.64</td>
<td align="left" colspan="1" rowspan="1">−0.44</td>
<td align="left" colspan="1" rowspan="1">0.52</td>
<td align="left" colspan="1" rowspan="1">0.66</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Backard, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e276" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.07</td>
<td align="left" colspan="1" rowspan="1">−2.71</td>
<td align="left" colspan="1" rowspan="1">0.13</td>
<td align="left" colspan="1" rowspan="1">0.06</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Balanced, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e277" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">0.29</td>
<td align="left" colspan="1" rowspan="1">−1.22</td>
<td align="left" colspan="1" rowspan="1">0.28</td>
<td align="left" colspan="1" rowspan="1">0.28</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">None</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
<td align="left" colspan="1" rowspan="1">−38.37</td>
<td align="left" colspan="1" rowspan="1">0.07</td>
<td align="left" colspan="1" rowspan="1">0.00</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt102"><label/><p>All values are tabulated to two decimal places (dp).</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3c">
<title>Relating Family and Model Levels</title>
<p>Family level posteriors are related to model level posteriors via summation over family members according to equation 21 for FFX and equation 22 for RFX. <xref ref-type="fig" rid="pcbi-1000709-g004">Figure 4</xref> shows the how the posterior probabilities over input families break down into posterior probabilities for individual models. <xref ref-type="fig" rid="pcbi-1000709-g005">Figure 5</xref> shows the same for the modulatory families.</p>
<fig id="pcbi-1000709-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g004</object-id><label>Figure 4</label><caption>
<title>Model level inference for input families.</title>
<p>For FFX (top panel) the figure shows that models in the P family have by far the greatest posterior probability mass. For RFX (bottom panel) models in both A and P families have high posterior expected probability, although the probability mass for P dominates.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g004" xlink:type="simple"/></fig><fig id="pcbi-1000709-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g005</object-id><label>Figure 5</label><caption>
<title>Model level inference for modulatory families.</title>
<p>For FFX (top panel) the figure shows that models in the F and BAL families have most probability mass. The expected posteriors from the RFX inference show a similar pattern (bottom panel). The ordering of models in this figure is not the same as the ordering of P models in <xref ref-type="fig" rid="pcbi-1000709-g004">figure 4</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g005" xlink:type="simple"/></fig>
<p>The maximum posterior model for the input family inference is model number 185 having posterior probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e278" xlink:type="simple"/></inline-formula>. Given that all families have the same number of members, the model priors are uniform, so the maximum posterior model is also the one with highest aggregate model evidence. This model has input to region P and modulatory connections as shown in <xref ref-type="fig" rid="pcbi-1000709-g006">Figure 6(a)</xref>.</p>
<fig id="pcbi-1000709-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g006</object-id><label>Figure 6</label><caption>
<title>Likely models.</title>
<p>The figure shows the input (filled square and solid arrow) and modulatory connectivity (solid arrows) stuctures for four models in Occam's window (assessed using FFX). Note that all models also have full endogenous connectivity (not shown). These four models are (a) model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e279" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e280" xlink:type="simple"/></inline-formula>, rank = 1, (b) model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e281" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e282" xlink:type="simple"/></inline-formula>, rank = 2, (c) model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e283" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e284" xlink:type="simple"/></inline-formula>, rank = 15 and (d) model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e285" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e286" xlink:type="simple"/></inline-formula>, rank = 16. All models have auditory input entering region P.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g006" xlink:type="simple"/></fig>
<p>The model evidence for the DCMs fitted in this paper was computed using the free energy approximation. This is to be contrasted with previous work in which (the most conservative of) AIC and BIC was used <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref>. One notable difference arising from this distinction is that the top-ranked models in <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref> contained significantly fewer connections than those in this paper (one sample t-test, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e287" xlink:type="simple"/></inline-formula>). The top 10 models in <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref> contained an average 2.4 modulatory connections whereas those in this paper contained an average of 4.5. This difference reflects the fact that the AIC/BIC approximation to the log evidence penalizes models for each additional connection (parameter) without considering interdependencies or covariances amongst parameters, whereas the free energy approximation takes such dependencies into account.</p>
</sec><sec id="s3d">
<title>Model Averaging</title>
<p>We now follow up the family-level inferences about input connections with Bayesian model averaging. As previously discussed, this is especially useful when the posterior model density is not sharply peaked, as is the case here (see <xref ref-type="fig" rid="pcbi-1000709-g004">Figure 4</xref>. All of the averaging results in this paper are obtained with an Occam's window defined using a minimal posterior odds ratio of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e288" xlink:type="simple"/></inline-formula>.</p>
<p>For FFX inference the input was inferred to enter region P only. We therefore restrict the averaging to those 64 models in family P. This produces 16 models in Occam's window (itself indicating that the posterior is not sharply peaked). The worst one is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e289" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e290" xlink:type="simple"/></inline-formula>. The posterior odds of the best relative to the worst is only <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e291" xlink:type="simple"/></inline-formula> (the largest it could be is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e292" xlink:type="simple"/></inline-formula>), meaning these models are not significantly better than one another. Four of the models in Occam's window are shown in <xref ref-type="fig" rid="pcbi-1000709-g006">Figure 6</xref>. <xref ref-type="fig" rid="pcbi-1000709-g007">Figure 7</xref> shows the posterior densities of average modulatory connections (averaging over models and subjects). The height of the delta functions in these histograms correspond to the total posterior probability mass of models which assume that the connection is zero.</p>
<fig id="pcbi-1000709-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g007</object-id><label>Figure 7</label><caption>
<title>Average Modulatory Connections from FFX for input family P.</title>
<p>The figures show the posterior densities of average network parameters from fixed effects Bayesian model averaging for the modulatory connections. Only forward connections from P to A and from P to F are modulated by speech intelligibility.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g007" xlink:type="simple"/></fig>
<p>For RFX inference the input was inferred to most likely enter region P alone (posterior exceedance probability, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e293" xlink:type="simple"/></inline-formula>). In the RFX model averaging the Occam's windowing procedure was specific to each subject, thus each subject can have a different number of models in Occam's window. For the input model P family there were an average of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e294" xlink:type="simple"/></inline-formula> models in Occam's window and <xref ref-type="fig" rid="pcbi-1000709-g008">Figure 8</xref> shows the posterior densities of the average modulatory connections (averaging over models and subjects). Both the RFX and FFX model averages within family P show that only connections from P to A, and from P to F, are facilitated by speech intelligibility.</p>
<fig id="pcbi-1000709-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000709.g008</object-id><label>Figure 8</label><caption>
<title>Average Modulatory Connections from RFX for input family P.</title>
<p>The figures show the posterior densities of average network parameters from random effects Bayesian model averaging for the modulatory connections. Only forward connections from P to A and from P to F are modulated by speech intelligibility.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.g008" xlink:type="simple"/></fig></sec></sec><sec id="s4">
<title>Discussion</title>
<p>This paper has investigated the formal comparison of models using Bayesian model evidence. Previous application of the method in the biological sciences has focussed on model selection in which one first selects the model with the highest evidence and then makes inferences based on the parameters of that model. We have shown that this ‘best model’ approach, though useful when the number of models is small, can become brittle if there are a large number of models, and if different subjects use different models.</p>
<p>To overcome this shortcoming we have proposed the combination of two further approaches (i) family level inference and (ii) Bayesian model averaging within families. Family level inference removes uncertainty about aspects of model structure other than the characteristic one is interested in. Bayesian model averaging can then be used to provide a summary measure of likely parameter values for each family.</p>
<p>We have applied these approaches to neuroimaging data, specifically a DCM study of auditory word processing using fMRI. Our results indicate that spoken words most likely stimulate a region in posterior STS and that if the word is intelligible connections are strengthened both to anterior STS and an inferior frontal region. These conclusions were drawn based on family level inference and Bayesian model averaging.</p>
<p>The model evidence for the DCMs fitted in this paper was computed using the free energy approximation whereas previous work used (the most conservative of) AIC and BIC <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref>. This resulted in the highly ranked models containing significantly more connections than in the previous study. This is due to a bias in the AIC/BIC criterion which leads to overly simple models being selected. Previous work in graphical models favours the free energy approach over BIC <xref ref-type="bibr" rid="pcbi.1000709-Beal1">[6]</xref> and work on biochemical models finds AIS to be the best of the more computationally expensive sampling methods. The relative merits of the different model selection criteria, as applied to brain imaging models and data, will be addressed in a future publication. The family level inference procedures described in this paper can be applied whatever method is used for estimating the model evidence.</p>
<p>Interestingly, the use of BMA produced an average network structure with speech input to region P, and modulatory connections from P to A and from P to F. This is exactly the winning model from earlier work <xref ref-type="bibr" rid="pcbi.1000709-Leff1">[17]</xref> (based on AIC/BIC approximation of model evidence). It is not, however, the best model as indicated by the free energy. The model with the highest free energy (see <xref ref-type="fig" rid="pcbi-1000709-g006">figure 6(a)</xref>) does not, however, have significantly higher evidence than the second best model, or indeed, any model in Occam's window. This indicates that in the particular example we have studied the use of Bayes factors or posterior odds ratios would be inconclusive, whereas clear conclusions can be drawn from family level inference.</p>
<p>This paper has also introduced a Gibbs sampling method for RFX model level inference when the number of models is large. This sampling method should be preferred to the previously suggested VB method <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref> when the number of models exceeds the number of subjects (ie. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e295" xlink:type="simple"/></inline-formula>). We do emphasise, however, that for RFX model level inferences involving a small number of models (as in previous work <xref ref-type="bibr" rid="pcbi.1000709-Stephan2">[19]</xref>) the VB approach is perfectly valid, and is indeed the preferred approach because it is faster.</p>
<p>The issue of family versus model level inference is orthogonal to the issue of random versus fixed effects analysis. The same critera re. FFX versus RFX apply at the family level as at the model level. For the data in this paper one might use RFX analysis as auditory word processing is part of the high level language system and one expect might expect differences in the neuronal instantiation (eg. lateralisation). If the issue remains unclear one could adopt a more pragmatic approach by first implementing a FFX analysis, and if there appear to be outlying subjects, then one could follow this up with an RFX analysis.</p>
<p>Family level inferences under FFX assumptions are simple to implement. Families with (the same and) different numbers of models are accommodated by setting model priors using equation 20, model posteriors are computed using equation 7, and family level posteriors using equation 21. This is a simple non-iterative procedure. Family level inferences under RFX assumptions are more subtle and have been the main focus of this paper. Families with (equal and) unequal numbers of models are accommodated using the model priors in equation 24, model posteriors are computed using an iterative Gibbs sampling procedure, and family level posteriors are computed using equation 22. We envisage that family level inference under RFX assumptions will be particularly useful in neuroimaging studies of high level cognition or for clinical groups where there is a high degree of intersubject variability. Where subjects can be clearly divided into two or more groups on behavioural or other grounds (e.g. patients and controls), then it would be correct to group the models accordingly, and proceed with a between group analysis on selected parameters of the averaged models.</p>
<p>Finally, we comment on the broader issue of comparison of discrete models (the ‘Discrete’ approach adopted in this work) versus a hierarchical approach embodying Automatic Relevance Determination (ARD) in which irrelevant connections are ‘switched off’ during model fitting <xref ref-type="bibr" rid="pcbi.1000709-MacKay1">[41]</xref> (for the case of DCMs the ARD approach is currently hypothetical as no such algorithm has yet been implemented). The ARD approach provides an estimate of the marginal density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e296" xlink:type="simple"/></inline-formula> directly without recourse to Bayesian model averaging. The Discrete approach allows for quantitative family-level inferences about issues such as whether processing is serial or parallel, linear or nonlinear. Additionally, Bayesian Model Averaging can be used with the Discrete approach to provide estimates of the marginal density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000709.e297" xlink:type="simple"/></inline-formula>. Overall, the ARD approach is probably the preffered method if one is solely interested in the marginal density over parameters, because it will likely be faster. If one is additionally interested in quantitative family-level inference then the Discrete approach would be the method of choice.</p>
<p>We expect that the comparison of model families will prove useful for a range of model comparison applications in biology, from connectivity models of brain imaging data, to behavioural models of learning and decision making, and dynamical models in molecular biology.</p>
</sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000709.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000709.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>Supplementary Information</p>
<p>(0.08 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Uta Noppeney and Dominich Bach for providing examples where the ranking of models from group random effects inference is critically dependent on the comparison set. We thank Nelson Trujillo-Barreto for discussions regarding dilution in model selection.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000709-Gelman1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gelman</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Carlin</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Stern</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Rubin</surname><given-names>D</given-names></name>
</person-group>             <year>1995</year>             <source>Bayesian Data Analysis</source>             <publisher-loc>Boca Raton</publisher-loc>             <publisher-name>Chapman and Hall</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Bernardo1"><label>2</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bernardo</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Smith</surname><given-names>A</given-names></name>
</person-group>             <year>2000</year>             <source>Bayesian Theory</source>             <publisher-loc>Chichester</publisher-loc>             <publisher-name>Wiley</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Mackay1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mackay</surname><given-names>D</given-names></name>
</person-group>             <year>2003</year>             <source>Information Theory, Inference and Learning Algorithms</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Hoeting1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hoeting</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Madigan</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Raftery</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Volinsky</surname><given-names>C</given-names></name>
</person-group>             <year>1999</year>             <article-title>Bayesian Model Averaging: A Tutorial.</article-title>             <source>Statistical Science</source>             <volume>14</volume>             <fpage>382</fpage>             <lpage>417</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Penny1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Roberts</surname><given-names>S</given-names></name>
</person-group>             <year>2002</year>             <article-title>Bayesian multivariate autoregresive models with structured priors.</article-title>             <source>IEE Proceedings on Vision, Image and Signal Processing</source>             <volume>149</volume>             <fpage>33</fpage>             <lpage>41</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Beal1"><label>6</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beal</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name>
</person-group>             <year>2003</year>             <article-title>The Variational Bayesian EM algorithms for incomplete data: with application to scoring graphical model structures.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Bernardo</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bayarri</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Berger</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Dawid</surname><given-names>A</given-names></name>
</person-group>             <publisher-name>Bayesian Statistics 7, Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Kemp1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Perfors</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>
</person-group>             <year>2007</year>             <article-title>Learning overhypotheses with hierarchical Bayesian models.</article-title>             <source>Dev Sci</source>             <volume>10</volume>             <fpage>307</fpage>             <lpage>21</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Penny2"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2003</year>             <article-title>Variational Bayesian Inference for fMRI time series.</article-title>             <source>NeuroImage</source>             <volume>19</volume>             <fpage>727</fpage>             <lpage>741</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-TrujilloBarreto1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Aubert-Vazquez</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Valdes-Sosa</surname><given-names>P</given-names></name>
</person-group>             <year>2004</year>             <article-title>Bayesian model averaging in EEG/MEG imaging.</article-title>             <source>NeuroImage</source>             <volume>21</volume>             <fpage>1300</fpage>             <lpage>1319</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Friston1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Phillips</surname><given-names>C</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Multiple sparse priors for the M/EEG inverse problem.</article-title>             <source>NeuroImage</source>             <volume>39</volume>             <fpage>1104</fpage>             <lpage>1120</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Penny3"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Mechelli</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2004</year>             <article-title>Comparing Dynamic Causal Models.</article-title>             <source>NeuroImage</source>             <volume>22</volume>             <fpage>1157</fpage>             <lpage>1172</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Girolami1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Girolami</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Bayesian inference for differential equations.</article-title>             <source>Theoretical Computer Science</source>             <volume>408</volume>             <fpage>4</fpage>             <lpage>16</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Vyshemirsky1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vyshemirsky</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Girolami</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Bayesian ranking of biochemical system models.</article-title>             <source>Bioinformatics</source>             <volume>24</volume>             <fpage>833</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Toni1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Toni</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Welch</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Strelkowa</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Ipsen</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Stumpf</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems.</article-title>             <source>J R Soc Interface</source>             <volume>6</volume>             <fpage>187</fpage>             <lpage>202</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Acs1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Acs</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Greenlee</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Connectivity modulation of early visual processing areas during covert and overt tracking tasks.</article-title>             <source>Neuroimage</source>             <volume>41</volume>             <fpage>380</fpage>             <lpage>8</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Stephan1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Marshall</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Fink</surname><given-names>G</given-names></name>
</person-group>             <year>2007</year>             <article-title>Interhemispheric integration of visual processing during task-driven lateralization.</article-title>             <source>Journal of Neuroscience</source>             <volume>27</volume>             <fpage>3512</fpage>             <lpage>3522</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Leff1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Leff</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Schofield</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Crinion</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>The cortical dynamics of intelligible speech.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>13209</fpage>             <lpage>15</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Summerfield1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Summerfield</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Koechlin</surname><given-names>E</given-names></name>
</person-group>             <year>2008</year>             <article-title>A neural representation of prior information during perceptual inference.</article-title>             <source>Neuron</source>             <volume>59</volume>             <fpage>336</fpage>             <lpage>47</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Stephan2"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
</person-group>             <year>2009</year>             <article-title>Bayesian model selection for group studies.</article-title>             <source>Neuroimage</source>             <volume>46</volume>             <fpage>1004</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Friston2"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Dynamic Causal Modelling.</article-title>             <source>NeuroImage</source>             <volume>19</volume>             <fpage>1273</fpage>             <lpage>1302</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Friston3"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2009</year>             <article-title>Causal modelling and brain connectivity in functional magnetic resonance imaging.</article-title>             <source>PLoS Biol</source>             <volume>7</volume>             <fpage>e1000033</fpage>          </element-citation></ref>
<ref id="pcbi.1000709-Daunizeau1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
</person-group>             <year>2009</year>             <article-title>Dynamic causal modelling of distributed electromagnetic responses.</article-title>             <source>Neuroimage</source>             <volume>47</volume>             <fpage>590</fpage>             <lpage>601</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Friston4"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2002</year>             <article-title>Bayesian estimation of dynamical systems: An application to fMRI.</article-title>             <source>NeuroImage</source>             <volume>16</volume>             <fpage>513</fpage>             <lpage>530</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Buxton1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Buxton</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Uludag</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Dubowitz</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Liu</surname><given-names>T</given-names></name>
</person-group>             <year>2004</year>             <article-title>Modelling the hemodynamic response to brain activation.</article-title>             <source>Neuroimage</source>             <volume>23</volume>             <fpage>220</fpage>             <lpage>233</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Friston5"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
</person-group>             <year>2007</year>             <article-title>Variational free energy and the Laplace approximation.</article-title>             <source>Neuroimage</source>             <volume>34</volume>             <fpage>220</fpage>             <lpage>234</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Chumbley1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chumbley</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Fearn</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
</person-group>             <year>2007</year>             <article-title>A Metropolis-Hastings algorithm for dynamic causal models.</article-title>             <source>Neuroimage</source>             <volume>38</volume>             <fpage>478</fpage>             <lpage>87</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Penny4"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
</person-group>             <year>2006</year>             <article-title>Variational Bayes.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Nichols</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
</person-group>             <source>Statistical Parametric Mapping: The analysis of functional brain images</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Elsevier</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Beal2"><label>28</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beal</surname><given-names>M</given-names></name>
</person-group>             <year>2003</year>             <article-title>Variational algorithms for approximate Bayesian inference.</article-title>             <comment>Ph.D. thesis, University College London</comment>          </element-citation></ref>
<ref id="pcbi.1000709-Woolrich1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Woolrich</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Behrens</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Smith</surname><given-names>S</given-names></name>
</person-group>             <year>2004</year>             <article-title>Constrained linear basis sets for HRF modelling using Variational Bayes.</article-title>             <source>NeuroImage</source>             <volume>21</volume>             <fpage>1748</fpage>             <lpage>1761</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Sato1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sato</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Yoshioka</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Kajihara</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Toyama</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Goda</surname><given-names>N</given-names></name>
<etal/></person-group>             <year>2004</year>             <article-title>Hierarchical Bayesian estimation for MEG inverse problem.</article-title>             <source>NeuroImage</source>             <volume>23</volume>             <fpage>806</fpage>             <lpage>826</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Roberts1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Roberts</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
</person-group>             <year>2002</year>             <article-title>Variational Bayes for Generalised Autoregressive models.</article-title>             <source>IEEE Transactions on Signal Processing</source>             <volume>50</volume>             <fpage>2245</fpage>             <lpage>2257</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Cronin1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cronin</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Stevenson</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Sur</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Kording</surname><given-names>K</given-names></name>
</person-group>             <year>2010</year>             <article-title>Hierarchical Bayesian modeling and Markov chain Monte Carlo sampling for tuning curve analysis.</article-title>             <source>J Neurophysiol</source>             <volume>103</volume>             <fpage>591</fpage>             <lpage>602</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Neal1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Neal</surname><given-names>RM</given-names></name>
</person-group>             <year>2001</year>             <article-title>Annealed importance sampling.</article-title>             <source>Statistics and Computing</source>             <volume>11</volume>             <fpage>125</fpage>             <lpage>139</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Raftery1"><label>34</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Raftery</surname><given-names>A</given-names></name>
</person-group>             <year>1995</year>             <article-title>Bayesian model selection in social research.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Marsden</surname><given-names>P</given-names></name>
</person-group>             <source>Sociological Methodology</source>             <publisher-loc>Cambridge, Mass</publisher-loc>             <fpage>111</fpage>             <lpage>196</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Chen1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chen</surname><given-names>CC</given-names></name>
<name name-style="western"><surname>Henson</surname><given-names>RN</given-names></name>
<name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>
<name name-style="western"><surname>Kilner</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
</person-group>             <year>2009</year>             <article-title>Forward and backward connections in the brain: a DCM study of functional asymmetries.</article-title>             <source>Neuroimage</source>             <volume>45</volume>             <fpage>453</fpage>             <lpage>62</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Howell1"><label>36</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Howell</surname><given-names>D</given-names></name>
</person-group>             <year>1992</year>             <source>Statistical methods for psychology</source>             <publisher-name>Duxbury Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Penny5"><label>37</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>
</person-group>             <year>2006</year>             <article-title>Bayesian model selection and averaging.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Nichols</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>
</person-group>             <source>Statistical Parametric Mapping: The analysis of functional brain images</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Elsevier</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000709-Madigan1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Madigan</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Raftery</surname><given-names>A</given-names></name>
</person-group>             <year>1994</year>             <article-title>Model selection and accounting for uncertainty in graphical models using Occam's window.</article-title>             <source>Journal of the American Statistical Association</source>             <volume>89</volume>             <fpage>1535</fpage>             <lpage>1546</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Clyde1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Clyde</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Parmigiani</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Vidakovic</surname><given-names>B</given-names></name>
</person-group>             <year>1998</year>             <article-title>Multiple shrinkage and subset selection in wavelets.</article-title>             <source>Biometrika</source>             <volume>85</volume>             <fpage>391</fpage>             <lpage>402</lpage>          </element-citation></ref>
<ref id="pcbi.1000709-Visser1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Visser</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Jefferies</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Ralph</surname><given-names>MAL</given-names></name>
</person-group>             <year>2009</year>             <article-title>Semantic Processing in the Anterior Temporal Lobes: A Meta-analysis of the Functional Neuroimaging Literature.</article-title>             <source>J Cogn Neurosci: Epub ahead of print</source>          </element-citation></ref>
<ref id="pcbi.1000709-MacKay1"><label>41</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>MacKay</surname><given-names>DJC</given-names></name>
</person-group>             <year>1993</year>             <article-title>Bayesian non-linear modeling for the prediction competition.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>GR</surname><given-names>H</given-names></name>
</person-group>             <source>Maximum Entropy and Bayesian method</source>             <publisher-loc>Santa Barbara</publisher-loc>             <publisher-name>Kluwer Academic Publisher</publisher-name>             <fpage>221</fpage>             <lpage>234</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>