<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01503</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003469</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The Sign Rule and Beyond: Boundary Effects, Flexibility, and Noise Correlations in Neural Population Codes</article-title>
<alt-title alt-title-type="running-head">The Sign Rule and Beyond</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hu</surname><given-names>Yu</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Zylberberg</surname><given-names>Joel</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Shea-Brown</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Applied Mathematics, University of Washington, Seattle, Washington, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Program in Neurobiology and Behavior, University of Washington, Seattle, Washington, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Department of Physiology and Biophysics, University of Washington, Seattle, Washington, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>Jonathan W.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>The University of Texas at Austin, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">huyu@uw.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: YH JZ ESB. Performed the experiments: YH. Analyzed the data: YH. Contributed reagents/materials/analysis tools: YH JZ ESB. Wrote the paper: YH JZ ESB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>2</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>27</day><month>2</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>2</issue>
<elocation-id>e1003469</elocation-id>
<history>
<date date-type="received"><day>22</day><month>8</month><year>2013</year></date>
<date date-type="accepted"><day>17</day><month>12</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Hu et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Over repeat presentations of the same stimulus, sensory neurons show variable responses. This “noise” is typically correlated between pairs of cells, and a question with rich history in neuroscience is how these noise correlations impact the population's ability to encode the stimulus. Here, we consider a very general setting for population coding, investigating how information varies as a function of noise correlations, with all other aspects of the problem – neural tuning curves, etc. – held fixed. This work yields unifying insights into the role of noise correlations. These are summarized in the form of theorems, and illustrated with numerical examples involving neurons with diverse tuning curves. Our main contributions are as follows. (1) We generalize previous results to prove a <italic>sign rule</italic> (SR) — if noise correlations between pairs of neurons have opposite signs vs. their signal correlations, then coding performance will improve compared to the independent case. This holds for three different metrics of coding performance, and for arbitrary tuning curves and levels of heterogeneity. This generality is true for our other results as well. (2) As also pointed out in the literature, the SR does not provide a necessary condition for good coding. We show that a diverse set of correlation structures can improve coding. Many of these violate the SR, as do experimentally observed correlations. There is structure to this diversity: we prove that the optimal correlation structures must lie on boundaries of the possible set of noise correlations. (3) We provide a novel set of necessary and sufficient conditions, under which the coding performance (in the presence of noise) will be as good as it would be if there were no noise present at all.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Sensory systems communicate information to the brain — and brain areas communicate between themselves — via the electrical activities of their respective neurons. These activities are “noisy”: repeat presentations of the same stimulus do not yield to identical responses every time. Furthermore, the neurons' responses are not independent: the variability in their responses is typically correlated from cell to cell. How does this change the impact of the noise — for better or for worse? Our goal here is to classify (broadly) the sorts of noise correlations that are either good or bad for enabling populations of neurons to transmit information. This is helpful as there are many possibilities for the noise correlations, and the set of possibilities becomes large for even modestly sized neural populations. We prove mathematically that, for larger populations, there are many highly diverse ways that favorable correlations can occur. These often differ from the noise correlation structures that are typically identified as beneficial for information transmission – those that follow the so-called “sign rule.” Our results help in interpreting some recent data that seems puzzling from the perspective of this rule.</p>
</abstract>
<funding-group><funding-statement>Supported for this work came from NSF CRCNS grant DMS-1208027 and a Career Award at the Scientific Interface from the Burroughs-Wellcome Fund to ESB. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="22"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Neural populations typically show correlated variability over repeat presentation of the same stimulus <xref ref-type="bibr" rid="pcbi.1003469-Mastronarde1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003469-Gawne1">[4]</xref>. These are called <italic>noise correlations</italic>, to differentiate them from correlations that arise when neurons respond to similar features of a stimulus. Such <italic>signal correlations</italic> are measured by observing how pairs of mean (averaged over trials) neural responses co-vary as the stimulus is changed <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>.</p>
<p>How do noise correlations affect the population's ability to encode information? This question is well-studied <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003469-Josi1">[16]</xref>, and prior work indicates that the presence of noise correlations can either improve stimulus coding, diminish it, or have little effect (<xref ref-type="fig" rid="pcbi-1003469-g001">Fig. 1</xref>). Which case occurs depends richly on details of the signal and noise correlations, as well as the specific assumptions made. For example <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Shamir1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-daSilveira1">[14]</xref> show that a classical picture — wherein positive noise correlations prevent information from increasing linearly with population size — does not generalize to heterogeneously tuned populations. Similar results were obtained by <xref ref-type="bibr" rid="pcbi.1003469-Tkacik1">[17]</xref>, and these examples emphasize the need for general insights.</p>
<fig id="pcbi-1003469-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003469.g001</object-id><label>Figure 1</label><caption>
<title>Different structures of correlated trial-to-trial variability lead to different coding accuracies in a neural population.</title>
<p>(Modified and extended from <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>.) We illustrate the underlying issues via a three neuron population, encoding two possible stimulus values (yellow and blue). Neurons' mean responses are different for each stimulus, representing their tuning. Trial-to-trial variability (noise) around these means is represented by the ellips(oid)s, which show 95% confidence levels. This noise has two aspects: for each individual neuron, its trial-to-trial variance; and at the population level, the noise correlations between pairs of neurons. We fix the former (as well as mean stimulus tuning), and ask how noise correlations impact stimulus coding. Different choices (<bold>A–D</bold>) of noise correlations affect the orientation and shape of response distributions in different ways, yielding different levels of overlap between the full (3D) distributions for each stimulus. The smaller the overlap, the more discriminable are the stimuli and the higher the coding performance. We also show the 2D projections of these distributions, to facilitate the comparison with the geometrical intuition of <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>. First, Row <bold>A</bold> is the reference case where neurons' noise is independent: zero noise correlations. Row <bold>B</bold> illustrates how noise correlations can increase overlap and worsen coding performance. Row <bold>C</bold> demonstrates the opposite case, where noise correlations are chosen consistently with the sign rule (SR) and information is enhanced compared to the independent noise case. Intriguingly, Row <bold>D</bold> demonstrates that there are more favorable possibilities for noise correlations: here, these violate the SR, yet improve coding performance vs. the independent case. Detailed parameter values are listed in <xref ref-type="sec" rid="s4">Methods</xref> Section “Details for numerical examples and simulations”.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003469.g001" position="float" xlink:type="simple"/></fig>
<p>Thus, we study a more general mathematical model, and investigate how coding performance changes as the noise correlation are varied. <xref ref-type="fig" rid="pcbi-1003469-g001">Figure 1</xref>, modified and extended from <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>, illustrates this process. In this figure, the only aspect of the population responses that differs from case to case are the noise correlations, resulting in differently shaped distributions. These different noise structures lead to different levels of stimulus discriminability, and hence coding performance. The different cases illustrate our approach: given any set of tuning curves and noise variances, we study how encoded stimulus information varies with respect to the set of all pairwise noise correlations.</p>
<p>Compared to previous work in this area, there are two key differences that makes our analysis novel: we make no particular assumptions on the structure of the tuning curves; and we do not restrict ourselves to any particular correlation structure such as the “limited-range” correlations often used in prior work <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Abbott1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>. Our results still apply to the previously-studied cases, but also hold much more generally. This approach leads us to derive mathematical theorems relating encoded stimulus information to the set of pairwise noise correlations. We prove the same theorems for several common measures of coding performance: the linear Fisher information, the precision of the optimal linear estimator (OLE <xref ref-type="bibr" rid="pcbi.1003469-Salinas1">[18]</xref>), and the mutual information between Gaussian stimuli and responses.</p>
<p>First, we prove that coding performance is always enhanced – relative to the case of independent noise – when the noise and signal correlations have opposite signs for all cell pairs (see <xref ref-type="fig" rid="pcbi-1003469-g001">Fig. 1</xref>). This “sign rule” (SR) generalizes prior work. Importantly, the converse is not true, noise correlations that perfectly violate the SR –and thus have the same signs as the signal correlations – can yield better coding performance than does independent noise. Thus, as previously observed <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Shamir1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-daSilveira1">[14]</xref>, the SR does not provide a necessary condition for correlations to enhance coding performance.</p>
<p>Since experimentally observed noise correlations often have the same signs as the signal correlations <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Zohary1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Kohn1">[19]</xref>, new theoretical insights are needed. To that effect, we develop a new organizing principle: optimal coding will always be obtained on the boundary of the set of allowed correlation coefficients. As we discuss, this boundary can be defined in flexible ways that incorporate constraints from statistics or biological mechanisms.</p>
<p>Finally, we identify conditions under which appropriately chosen noise correlations can yield coding performance as good as would be obtained with deterministic neural responses. For large populations, these conditions are satisfied with high probability, and the set of such correlation matrices is very high-dimensional. Many of them also strongly violate the SR.</p>
</sec><sec id="s2">
<title>Results</title>
<p>The layout of our <xref ref-type="sec" rid="s2">Results</xref> section is as follows. We will begin by describing our setup, and the quantities we will be computing, in Section “Problem setup”.</p>
<p>In Section “The sign rule revisited”, we will then discuss our generalized version of the “sign rule”, Theorem 1, namely that signal and noise correlations between pairs of neurons with opposite signs will always improve encoded information compared with the independent case. Next, in Section “Optimal correlations lie on boundaries”, we use the fact that all of our information quantities are convex functions of the noise correlation coefficients to conclude that the optimal noise correlation structure must lie on the boundary of the allowed set of correlation matrices, Theorem 2.</p>
<p>We will further observe that there will typically be a large set of correlation matrices that all yield optimal (or near-optimal) coding performance, in a numerical example of heterogeneously tuned neural populations in Section “Heterogeneously tuned neural populations”.</p>
<p>We prove that these observations are general in Section “Noise cancellation” by studying the noise canceling correlations (those that yield the same high coding fidelity as would be obtained in the absence of noise). We will provide a set of necessary and sufficient conditions for correlations to be “noise canceling”, Theorem 3, and for a system to allow for these noise canceling correlations, Theorem 4. Finally, we will prove a result that suggests that, in large neural populations with randomly chosen stimulus response characteristics, these conditions are likely to be satisfied, Theorem 5.</p>
<p>A summary of most frequent notations we use is listed in <xref ref-type="table" rid="pcbi-1003469-t001">Table 1</xref>.</p>
<table-wrap id="pcbi-1003469-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003469.t001</object-id><label>Table 1</label><caption>
<title>Notations.</title>
</caption><alternatives><graphic id="pcbi-1003469-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003469.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e001" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">stimulus</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e002" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">response of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e003" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e004" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">mean response of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e005" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e006" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">derivative against <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e007" xlink:type="simple"/></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1003469.e245">Eq. (6)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e008" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">covariance between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e009" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e010" xlink:type="simple"/></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1003469.e272">Eq. (11)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e011" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">noise covariance matrix (averaged or conditional, Section “Summary of the problem set-up”)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e012" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">covariance of the mean response, <xref ref-type="disp-formula" rid="pcbi.1003469.e263">Eq. (10)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e013" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e014" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">(a matrix is) positive definite and positive semidefinite</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e015" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">total covariance,<xref ref-type="disp-formula" rid="pcbi.1003469.e263">Eq. (10)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e016" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">optimal readout vector of OLE, <xref ref-type="disp-formula" rid="pcbi.1003469.e262">Eq. (9)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e017" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">noise correlations, <xref ref-type="disp-formula" rid="pcbi.1003469.e323">Eq. (15)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e018" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">signal correlations, <xref ref-type="disp-formula" rid="pcbi.1003469.e324">Eq. (16)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e019" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">linear Fisher information, <xref ref-type="disp-formula" rid="pcbi.1003469.e243">Eq. (5)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e020" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">OLE information (accuracy of OLE), <xref ref-type="disp-formula" rid="pcbi.1003469.e279">Eq. (12)</xref></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e021" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">mutual information for Gaussian distributions, <xref ref-type="disp-formula" rid="pcbi.1003469.e294">Eq. (13)</xref></td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><sec id="s2a">
<title>Problem setup</title>
<p>We will consider populations of neurons that generate noisy responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e022" xlink:type="simple"/></inline-formula> in response to a stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e023" xlink:type="simple"/></inline-formula>. The responses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e024" xlink:type="simple"/></inline-formula> – wherein each component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e025" xlink:type="simple"/></inline-formula> represents one cell's response – can be considered to be continuous-valued firing rates, discrete spike counts, or binary “words”, wherein each neuron's response is a 1 (“spike”) or 0 (“not spike”). The only exception is that, when we consider <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e026" xlink:type="simple"/></inline-formula> (discussed below), the responses must be continuous-valued. We consider arbitrary tuning for the neurons; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e027" xlink:type="simple"/></inline-formula>. For scalar stimuli, this definition of “tuning” corresponds to the notion of a tuning curve. In the case of more complex stimuli, it is similar to the typical notion of a receptive field. Recall that the signal correlations are determined by the co-variation of the mean responses of pairs of neurons as the stimulus is varied, and thus they are determined by the similarity in the tuning functions.</p>
<p>As for the structure of noise across the population, our analysis allows for the general case in which the noise covariance matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e028" xlink:type="simple"/></inline-formula> (superscript <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e029" xlink:type="simple"/></inline-formula> denotes “noise”) depends on the stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e030" xlink:type="simple"/></inline-formula>. This generality is particularly interesting given the observations of Poisson-like variability <xref ref-type="bibr" rid="pcbi.1003469-Softky1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Britten1">[21]</xref> in neural systems, and that correlations can vary with stimuli <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Josi1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Kohn1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-delaRocha1">[22]</xref>. We will assume that the diagonal entries of the conditional covariance matrix – which describe each cells' variance – will be fixed, and then ask how coding performance changes as we vary the off-diagonal entries, which describe the covariance between the cell's responses (recall that the noise correlations are the pairwise covariances, divided by the geometric mean of the relevant variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e031" xlink:type="simple"/></inline-formula>).</p>
<p>We quantify the coding performance with the following measures, which are defined more precisely in the <xref ref-type="sec" rid="s4">Methods</xref> Section “Defining the information quantities, signal and noise correlations”, below. First, we consider the linear Fisher information (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e032" xlink:type="simple"/></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1003469.e243">Eq. (5)</xref>), which measures how easy it is to separate the response distributions that result from two similar stimuli, with a linear discriminant. This is equivalent to the quantity used by <xref ref-type="bibr" rid="pcbi.1003469-Averbeck2">[11]</xref> and <xref ref-type="bibr" rid="pcbi.1003469-Sompolinsky1">[10]</xref> (where Fisher information reduces to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e033" xlink:type="simple"/></inline-formula>). While Fisher information is a measure of <italic>local</italic> coding performance, we are also interested in global measures.</p>
<p>We will consider two such global measures, the OLE information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e034" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003469.e279">Eq. (12)</xref>) and mutual information for Gaussian stimuli and responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e035" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003469.e294">Eq. (13)</xref>). <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e036" xlink:type="simple"/></inline-formula> quantifies how well the optimal linear estimator (OLE) can recover the stimulus from the neural responses: large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e037" xlink:type="simple"/></inline-formula> corresponds to small mean squared error of OLE and vice versa. For the OLE, there is one set of read-out weights used to estimate the stimulus, and those weights do not change as the stimulus is varied. For contrast, with linear Fisher information, there is generally a different set of weights used for each (small) range of stimuli within which the discrimination is being performed.</p>
<p>Consequently, in the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e039" xlink:type="simple"/></inline-formula>, we will be considering the average noise covariance matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e040" xlink:type="simple"/></inline-formula>, where the expectation is taken over the stimulus distribution. Here we overload the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e041" xlink:type="simple"/></inline-formula> be the covariance matrix that one chooses during the optimization, which will be either local (conditional covariances at a particular stimulus) or global depending on the information measure we consider.</p>
<p>While <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e042" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e043" xlink:type="simple"/></inline-formula> are concerned with the performance of linear decoders, the mutual information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e044" xlink:type="simple"/></inline-formula> between stimuli and responses describes how well the optimal read-out could recover the stimulus from the neural responses, without any assumptions about the form of that decoder. However, we emphasize that our results for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e045" xlink:type="simple"/></inline-formula> only apply to jointly Gaussian stimulus and response distributions, which is a less general setting than the conditionally Gaussian cases studied in many places in the literature. An important exception is that Theorem 2 additionally applies to the case of conditionally Gaussian distributions (see discussion in Section “Convexity of information measures”).</p>
<p>For simplicity, we describe most results for scalar stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e046" xlink:type="simple"/></inline-formula> if not stated otherwise, but the theory holds for multidimensional stimuli (see <xref ref-type="sec" rid="s4">Methods</xref> Section “Defining the information quantities, signal and noise correlations”).</p>
</sec><sec id="s2b">
<title>The sign rule revisited</title>
<p>Arguments about pairs of neurons suggest that coding performance is improved – relative to the case of independent, or trial-shuffled data – when the noise correlations have the opposite sign from the signal correlations <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Abbott1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Sompolinsky1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Romo1">[13]</xref>: we dub this the “sign rule” (SR). This notion has been explored and demonstrated in many places in the experimental and theoretical literature, and formally established for homogenous positive correlations <xref ref-type="bibr" rid="pcbi.1003469-Sompolinsky1">[10]</xref>. However, its applicability in general cases is not yet known.</p>
<p>Here, we formulate this SR property as a theorem without restrictions on homogeneity or population size.</p>
<p><bold>Theorem 1.</bold> <italic>If, for each pair of neurons, the signal and noise correlations have opposite signs, the linear Fisher information is greater than the case of independent noise (trial-shuffled data). In the opposite situation where the signs are the same, the linear Fisher information is decreased compared to the independent case, in a regime of very weak correlations. Similar results hold for </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e047" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e048" xlink:type="simple"/></inline-formula><italic>, with a modified definition of signal correlations given in Section.</italic> “Defining the information quantities, signal and noise correlations”.</p>
<p>In the case of Fisher information, the signal correlation between two neurons is defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e049" xlink:type="simple"/></inline-formula> (Section “Defining the information quantities, signal and noise correlations”). Here, the derivatives are taken with respect to the stimulus. This definition recalls the notion of the alignment in the change in the neurons' mean responses in, e.g., <xref ref-type="bibr" rid="pcbi.1003469-Averbeck2">[11]</xref>. It is important to note that this definition for signal correlation is locally defined near a stimulus value; thus, it differs from some other notions of “signal correlation” in the literature, that quantify how similar the whole tuning curves are for two neurons (see discussion on the alternative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e050" xlink:type="simple"/></inline-formula> in Section “Defining the information quantities, signal and noise correlations”). We choose to define signal correlations for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e051" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e052" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e053" xlink:type="simple"/></inline-formula> as described in Section “Defining the information quantities, signal and noise correlations” to reflect precisely the mechanism behind the examples in <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>, among others.</p>
<p>It is a consequence of Theorem 1 that the SR holds pairwise; different pairs of neurons will have different signs of noise correlations, as long as they are consistent with their (pairwise) signal correlations. The result holds as well for heterogenous populations. The essence of our proof of Theorem 1 is to calculate the gradient of the information function in the space of noise correlations. We compute this gradient at the point representing the case where the noise is independent. The gradient itself is determined by the signal correlations, and will have a positive dot product with any direction of changing noise correlations that obeys the sign rule. Thus, information is increased by following the sign rule, and the gradient points to (locally) the direction for changing noise correlations that maximally improves the information, for a given strength of correlations. A detailed proof is included in <xref ref-type="sec" rid="s4">Methods</xref> Section “Proof of Theorem 1: the generality of the sign rule”; this includes a formula for the gradient direction (Remark 1 in Section “Proof of Theorem 1: the generality of the sign rule”). We have proven the same result for all three of our coding metrics, and for both scalar, and multi-dimensional, stimuli.</p>
<p>Intriguingly, there exists an asymmetry between the result on improving information (above), and the (converse) question of what noise correlations are worst for population coding. As we will show later, the information quantities are convex functions of the noise correlation coefficients (see <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>). As a consequence, performance will keep increasing as one continues to move along a “good” direction, for example indicated by the SR. This is what one expects when climbing a parabolic landscape in which the second derivative is always nonnegative. The same convexity result indicates that the performance will not decrease monotonically along a “bad” direction, such as the anti-SR direction. For example, if, while following the anti-SR direction, the system passed by the minimum of the information quantity, then continued increases in correlation magnitude would yield increases in the information. In fact, it is even possible for anti-SR correlations to yield better coding performance than would be achieved with independent noise. An example of this is shown in <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>, where the arrow points in the direction in correlation space predicted by the SR, but performance that is better than with independent noise can also be obtained by choosing noise correlations in the opposite direction.</p>
<fig id="pcbi-1003469-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003469.g002</object-id><label>Figure 2</label><caption>
<title>The “sign rule” may fail to identify the globally optimal correlations.</title>
<p>The optimal linear estimator (OLE) information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e054" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003469.e279">Eq. (12)</xref>), which is maximized when the OLE produces minimum-variance signal estimates, is shown as a function of all possible choices of noise correlations (enclosed within the dashed line). These values are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e055" xlink:type="simple"/></inline-formula> (x-axis) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e056" xlink:type="simple"/></inline-formula> (y-axis) for a 3-neuron population. The bowl shape exemplifies the general fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e057" xlink:type="simple"/></inline-formula> is a convex function and thus must attain its maximum on the boundary (Theorem 2) of the allowed region of noise correlations. The independent noise case and global optimal noise correlations are labeled by a black dot and triangle respectively. The arrow shows the gradient vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e058" xlink:type="simple"/></inline-formula>, evaluated at zero noise correlations. It points to the quadrant in which noise correlations and signal correlations have opposite signs, as suggested by Theorem 1. Note that this gradient vector, derived from the “sign rule”, does not point towards the global maximum, and actually misses the entire quadrant containing that maximum. This plot is a two-dimensional slice of the cases considered in <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref>, while restricting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e059" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref> Section “Details for numerical examples and simulations” for further parameters).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003469.g002" position="float" xlink:type="simple"/></fig>
<p>Thus, the result that anti-SR noise correlations harm coding is only a “local” result – near the point of zero correlations – and therefore requires the assumption of weak correlations. We emphasize that this asymmetry of the SR is intrinsic to the problem, due to the underlying convexity.</p>
<p>One obvious limitation of Theorem 1 and the “sign rule” results in general is that they only compare information in the presence of correlated noise with the baseline case of independent noise. This approach does not address the issue of finding the optimal noise correlations, nor does it provide much insight into experimental data that do not obey the SR. Does the sign rule rule describe optimal configurations? What are the properties of the global optima? How should we interpret noise correlations that do not follow the SR? We will address these questions in the following sections.</p>
</sec><sec id="s2c">
<title>Optimal correlations lie on boundaries</title>
<p>Let us begin by considering a simple example to see what could happen for the optimization problem we described in Section “Problem setup”, when the baseline of comparison is no longer restricted to the case of independent noise. This example is for a population of 3 neurons. In order to better visualize the results, we further require that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e060" xlink:type="simple"/></inline-formula>. Therefore the configurations of correlations is two dimensional. In <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>, we plot information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e061" xlink:type="simple"/></inline-formula> as a function of the two free correlation coefficients (in this example the variances are all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e062" xlink:type="simple"/></inline-formula>, thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e063" xlink:type="simple"/></inline-formula>).</p>
<p>First, notice that there is a parabola-shaped region of all attainable correlations (in <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>, enclosed by black dashed lines and the upper boundary of the square). The region is determined not only by the entry-wise constraint <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e064" xlink:type="simple"/></inline-formula> (the square), but also by a global constraint that the covariance matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e065" xlink:type="simple"/></inline-formula> must be positive semidefinite. For linear Fisher information and mutual information for Gaussian distributions, we further assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e066" xlink:type="simple"/></inline-formula> (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e067" xlink:type="simple"/></inline-formula> is positive definite) so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e069" xlink:type="simple"/></inline-formula> remain finite (see also Section “Defining the information quantities, signal and noise correlations”). As we will see again below, this important constraint leads to many complex properties of the optimization problem. This constraint can be understood by noting that correlations must be chosen to be “consistent” with each other and cannot be freely and independently chosen. For example, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e070" xlink:type="simple"/></inline-formula> are large and positive, then cells 2 and 3 will be positively correlated – since they both covary positively with cell 1 – and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e071" xlink:type="simple"/></inline-formula> may thus not take negative values. In the extreme of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e072" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e073" xlink:type="simple"/></inline-formula> is fully determined to be 1. Cases like this are reflecting the corner shape in the upper right of the allowed region in <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>.</p>
<p>The case of independent noise is denoted by a black dot in the middle of <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>, and the gradient vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e074" xlink:type="simple"/></inline-formula> points to a quadrant that is guaranteed to increase information vs. the independent case (Theorem 1). The direction of this gradient satisfies the sign rule, as also guaranteed by Theorem 1. However, the gradient direction and the quadrant of the SR both fail to capture the globally optimal correlations, which are at upper right corner of the allowed region, and indicated by the red triangle. This is typically what happens for larger, and less symmetric populations, as we will demonstrate next.</p>
<p>Since the sign rule cannot be relied upon to indicate the global optimum, what other tools do we have at hand? A key observation, which we prove in the <xref ref-type="sec" rid="s4">Methods</xref> Section “Proof of Theorem 2: optima lie on boundaries”, is that information is a convex function of the noise correlations (off-diagonal elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e075" xlink:type="simple"/></inline-formula>). This immediately implies:</p>
<p><bold>Theorem 2.</bold> <italic>The optimal </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e076" xlink:type="simple"/></inline-formula><italic> that maximize information must lie on the boundary of the region of correlations considered in the optimization.</italic></p>
<p>As we saw in <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>, mathematically feasible noise correlations may not be chosen arbitrarily but are constrained by the fact that the noise covariance matrix be positive semidefinite. We denote this condition by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e077" xlink:type="simple"/></inline-formula>, and recall that it is equivalent to all of its eigenvalues being non-negative. According to our problem setup, the diagonal elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e078" xlink:type="simple"/></inline-formula>, which are the individual neurons' response variances, are fixed. It can be shown that this diagonal constraint specifies a linear slice through the cone of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e079" xlink:type="simple"/></inline-formula>, resulting a bounded convex region in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e080" xlink:type="simple"/></inline-formula> called a <italic>spectrahedron</italic>, for a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e081" xlink:type="simple"/></inline-formula> neurons. These spectrahedra are the largest possible regions of noise correlation matrices that are physically realizable, and are the set over which we optimize, unless stated otherwise.</p>
<p>Importantly for biological applications, Theorem 2 will continue to apply, when additional constraints define smaller allowed regions of noise correlations within the spectrahedron. These constraints may come from circuit or neuron-level factors. For example, in the case where correlations are driven by common inputs <xref ref-type="bibr" rid="pcbi.1003469-delaRocha1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Binder1">[23]</xref>, one could imagine a restriction on the maximal value of any individual correlation value. In other settings, one might consider a global constraint by restricting the maximum Euclidean norm (2-norm) of the noise correlations (defined in <xref ref-type="disp-formula" rid="pcbi.1003469.e347">Eq. (18)</xref> in <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>For a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e082" xlink:type="simple"/></inline-formula> neurons, there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e083" xlink:type="simple"/></inline-formula> possible correlations to consider; naturally, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e084" xlink:type="simple"/></inline-formula> increases, the optimal structure of noise correlations can therefore become more complex. Thus we illustrate the Theorem above with an example of 3 neurons encoding a scalar stimulus, in which there are 3 noise correlations to vary. In <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref>, we demonstrate two different cases, each with distinct <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e085" xlink:type="simple"/></inline-formula> matrix and vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e086" xlink:type="simple"/></inline-formula> (values are given in <xref ref-type="sec" rid="s4">Methods</xref> Section “S:numerics”). In the first case, there is a unique optimum (panel <bold>A</bold>, largest information is associated with the lightest color). In the second case, there are 4 disjoint optima (panel <bold>B</bold>), all of which lie on the boundary of the spectrahedron.</p>
<fig id="pcbi-1003469-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003469.g003</object-id><label>Figure 3</label><caption>
<title>Optimal coding is obtained on the boundary of the allowed region of noise correlations.</title>
<p>For fixed neuronal responses variances and tuning curves, we compute coding performance – quantified by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e087" xlink:type="simple"/></inline-formula> information values – for different values of the pair-wise noise correlations. To be physically realizable, the correlation coefficients must form a positive semi-definite matrix. This constraint defines a spectrahedron, or a swelled tetrahedron, for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e088" xlink:type="simple"/></inline-formula> cells used. The colors of the points represent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e089" xlink:type="simple"/></inline-formula> information values. With different parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e090" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e091" xlink:type="simple"/></inline-formula> (see values in <xref ref-type="sec" rid="s4">Methods</xref> Section “Details for numerical examples and simulations”), the optimal configuration can appear at different locations, either unique (<bold>A</bold>) or attained at multiple disjoint places (<bold>B</bold>), but always on the boundary of the spectrahedron. In both panels, plot titles give the maximum value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e092" xlink:type="simple"/></inline-formula> attained over the allowed space of noise correlations, and the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e093" xlink:type="simple"/></inline-formula> that would obtained with the given tuning curves, and perfectly deterministic neural responses. This provides an upper bound on the attainable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e094" xlink:type="simple"/></inline-formula> (see text Section “Noise cancellation”). Interestingly, in panel (<bold>A</bold>), the noisy population achieves this upper bound on performance, but this is not the case in (<bold>B</bold>). Details of parameters used are in <xref ref-type="sec" rid="s4">Methods</xref> Section “Details for numerical examples and simulations”.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003469.g003" position="float" xlink:type="simple"/></fig>
<p>In the next section, we will build from this example to a more complex one including more neurons. This will suggest further principles that govern the role of noise correlations in population coding.</p>
</sec><sec id="s2d">
<title>Heterogeneously tuned neural populations</title>
<p>We next follow <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Shamir1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Wilke1">[15]</xref> and study a numerical example of a larger (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e095" xlink:type="simple"/></inline-formula>) heterogeneously tuned neural population. The stimulus encoded is the direction of motion, which is described by a 2-D vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e096" xlink:type="simple"/></inline-formula>. We used the same parameters and functional form for the shape of tuning curves as in <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, the details of which are provided in <xref ref-type="sec" rid="s4">Methods</xref> Section “Details for numerical examples and simulation”. The tuning curve for each neuron was allowed to have randomly chosen width and magnitude, and the trial-to-trial variability was assumed to be Poisson: the variance is equal to the mean. As shown in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4 <bold>A</bold></xref>, under our choice of parameters the neural tuning curves – and by extension, their responses to the stimuli – are highly heterogenous. Once again, we quantify coding by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e097" xlink:type="simple"/></inline-formula> (see definition in Section “Problem setup” or <xref ref-type="disp-formula" rid="pcbi.1003469.e279">Eq. (12)</xref> in <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<fig id="pcbi-1003469-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003469.g004</object-id><label>Figure 4</label><caption>
<title>Heterogeneous neural population and violations of the sign rule with increasing correlation strength.</title>
<p>We consider signal encoding in a population of 20 neurons, each of which has a different dependence of its mean response on the stimulus (heterogeneous tuning curves shown in <bold>A</bold>). We optimize the coding performance of this population with respect to the noise correlations, under several different constraints on the magnitude of the allowed noise correlations. Panel (<bold>B</bold>) shows the resultant – optimal given the constraint – values of OLE information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e098" xlink:type="simple"/></inline-formula>, with different noise correlation strengths (blue circles). The strength of correlations is quantified by the Euclidean norm (<xref ref-type="disp-formula" rid="pcbi.1003469.e347">Eq. (18)</xref>). For comparison, the red crosses show information obtained for correlations that obey the sign rule (in particular, pointing along the gradient giving greatest information for weak correlations); this information is always less than or equal to the optimum, as it must be. Note that correlations that follow the sign rule fail to exist for large correlation strengths, as the defining vector points outside of the allowed region (spectrahedron) beyond a critical length (labeled (ii)). For correlation strengths beyond this point, distinct optimized noise correlations continue to exist; the information values they obtain eventually saturate at noise-free levels (see text), which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e099" xlink:type="simple"/></inline-formula> for the example shown here. This occurs for a wide range of correlation strengths. Panel (<bold>C</bold>) shows how well these optimized noise correlations are predicted from the corresponding signal correlations (by the sign rule), as quantified by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e100" xlink:type="simple"/></inline-formula> statistic (between 0 and 1, see <xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5</xref>). For small magnitudes of correlations, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e101" xlink:type="simple"/></inline-formula> values are high, but these decline when the noise correlations are larger.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003469.g004" position="float" xlink:type="simple"/></fig>
<p>Our goal with this example is to illustrate two distinct regimes, with different properties of the noise correlations that lead to optimal coding. In the first regime, which occurs closest to the case of independent noise, the SR determines the optimal correlation structure. In the second, moving further away from the independent case, the optimal correlations may disobey the SR. (A related effect was found by <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>; we return to this in the <xref ref-type="sec" rid="s3">Discussion</xref>.) We accomplish this in a very direct way: for gradually increasing the (additional) constraint on the Euclidean norm of correlations (<xref ref-type="disp-formula" rid="pcbi.1003469.e347">Eq. (18)</xref> in <xref ref-type="sec" rid="s4">Methods</xref> Section “Defining the information quantities, signal and noise correlations”), we numerically search for optimal noise correlation matrices and compare them to predictions from the SR.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4 <bold>B</bold></xref> we show the results, comparing the information attained with noise correlations that obey the sign rule with those that are optimized, for a variety of different noise correlation strengths. As they must be, the optimized correlations always produce information values as high as, or higher than, the values obtained with the sign rule.</p>
<p>In the limit where the correlations are constrained to be small, the optimized correlations agree with the sign rule; an example of these “local” optimized correlations is shown in <xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5 <bold>ADG</bold></xref>, corresponding to the point labeled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e102" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4 <bold>BC</bold></xref>. This is predicted by Theorem 1. In this “local” region of near-zero noise correlations, we see a linear alignment of signal and noise correlations (<xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5 <bold>D</bold></xref>). As larger correlation strengths are reached (points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e103" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e104" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4 <bold>BC</bold></xref>), we observe a gradual violation of the sign rule for the optimized noise correlations. This is shown by the gradual loss of the linear relationship between signal and noise correlations in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4 <bold>D</bold> vs. <bold>E</bold> vs. <bold>F</bold></xref>, as quantified by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e105" xlink:type="simple"/></inline-formula> statistic. Interestingly, this can happen even when the correlation coefficients continue have reasonably small values, and are broadly consistent with the ranges of noise correlations seen in physiology experiments <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Hansen1">[24]</xref>.</p>
<fig id="pcbi-1003469-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003469.g005</object-id><label>Figure 5</label><caption>
<title>In our larger neural population, the sign rule governs optimal noise correlations only when these correlations are forced to be very small in magnitude; for stronger correlations, optimized noise correlations have a diverse structure.</title>
<p>Here we investigate the structure of the optimized noise correlations obtained in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4</xref>; we do this for three examples with increasing correlation strength, indicated by the labels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e106" xlink:type="simple"/></inline-formula> in that figure. First (<bold>ABC</bold>) show scatter plots of the noise correlations of the neural pairs, as a function of their signal correlations (defined in <xref ref-type="sec" rid="s4">Methods</xref> Section “Defining the information quantities, signal and noise correlations”). For each example, we also show (<bold>DEF</bold>) a version of the scatter plot where the signal correlations have been rescaled in a manner discussed in Section “Parameters for <xref ref-type="fig" rid="pcbi-1003469-g001">Fig. 1</xref>, <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref> and <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref>”, that highlights the linear relationship (wherever it exists) between signal and noise correlations. In both sets of panels, we see the same key effect: the sign rule is violated as the (Euclidean) strength of noise correlations increases. In (<bold>ABC</bold>), this is seen by noting the quadrants where the dots are located: the sign rule predicts they should only be in the second and fourth quadrants. In (<bold>DEF</bold>), we quantify agreement with the sign rule by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e107" xlink:type="simple"/></inline-formula> statistic. Finally, (<bold>GHI</bold>) display histograms of the noise correlations; these are concentrated around 0, with low average values in each case.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003469.g005" position="float" xlink:type="simple"/></fig>
<p>The two different regimes of optimized noise correlations arise because, at a certain correlation strength, the correlation strength can no longer be increased along the direction that defines the sign rule without leaving the region of positive semidefinite covariance matrices. However, correlation matrices still exist that allow for more informative coding with larger correlation strengths. This reflects the geometrical shape of the spectrahedron, wherein the optima may lie in the “corners”, as shown in <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref>. For these larger-magnitude correlations, the sign rule no longer describes optimized correlations, as shown with an example of optimized correlations in <xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5 <bold>CF</bold></xref>.</p>
<p><xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5</xref> illustrates another interesting feature. There is a diverse set of correlation matrices, with different Euclidean norms beyond the value of (roughly) 1.2, that all achieve the same globally optimal information level. As we see in the next section, this phenomenon is actually typical for large populations, and can be described precisely.</p>
</sec><sec id="s2e">
<title>Noise cancellation</title>
<p>For certain choices of tuning curves and noise variances, including the examples in <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3 <bold>A</bold></xref> and Section “Heterogeneously tuned neural populations”, we can tell precisely the value of the globally optimized information quantities — that is, the information levels obtained with optimal noise correlations. For the OLE, this global optimum is the upper bound on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e108" xlink:type="simple"/></inline-formula>. This is shown formally in Lemma 8, but it simply translates to an intuitive lower bound of the OLE error, similar to the data processing inequality for mutual information. This bound states that the OLE error cannot be smaller than the OLE error when there is no noise in the responses, i.e. when the neurons produce a deterministic response conditioned on the stimulus. This upper bound may — and often will (Theorem 5) — be achievable by populations of noisy neurons.</p>
<p>Let us first consider an extremely simple example. Consider the case of two neurons with identical tuning curves, so that their responses are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e109" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e110" xlink:type="simple"/></inline-formula> is the noise in the response of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e111" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e112" xlink:type="simple"/></inline-formula> is the same mean response under stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e113" xlink:type="simple"/></inline-formula>. In this case, the “noise free” coding is when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e114" xlink:type="simple"/></inline-formula> on all trials, and the inference accuracy is determined by the shape of the tuning curve <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e115" xlink:type="simple"/></inline-formula> (whether or not it is invertible, for example). Now let us consider the case where the noise in the neurons' responses is non-zero but perfectly anti correlated, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e116" xlink:type="simple"/></inline-formula> on all trials. We can then choose the read-out as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e117" xlink:type="simple"/></inline-formula> to cancel the noise and achieve the same coding accuracy as the “noise free” case.</p>
<p>The preceding example shows that, at least in some cases, one can choose noise correlations in such a way that a linear decoder achieves “noise-free” performance. One is naturally left to wonder whether this observation applies more generally.</p>
<p>First, we state the conditions on the noise covariance matrices under which the noise-free coding performance is obtained. We will then identify the conditions on parameters of the problem, i.e. the tuning curves (or receptive fields) and noise variances, under which this condition can be satisfied. Recall that the OLE is based on a fixed (across stimuli) linear readout coefficient vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e118" xlink:type="simple"/></inline-formula> defined in <xref ref-type="disp-formula" rid="pcbi.1003469.e262">Eq. (9)</xref></p>
<p><bold>Theorem 3.</bold> <italic>A covariance matrix </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e119" xlink:type="simple"/></inline-formula><italic> attains the noise-free bound for OLE information (and hence is optimal), if and only if </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e120" xlink:type="simple"/></inline-formula><italic>. Here </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e121" xlink:type="simple"/></inline-formula><italic> is the cross-covariance between the stimuli responses (</italic><xref ref-type="disp-formula" rid="pcbi.1003469.e272"><italic>Eq. (11)</italic></xref>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e122" xlink:type="simple"/></inline-formula><italic> is the covariance of the mean response (</italic><xref ref-type="disp-formula" rid="pcbi.1003469.e263"><italic>Eq. (10)</italic></xref><italic>), and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e123" xlink:type="simple"/></inline-formula><italic> is the linear readout vector for OLE, which is the same as in the noise-free case — that is, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e124" xlink:type="simple"/></inline-formula><italic> — when the condition is satisfied.</italic></p>
<p>We note that when the condition is satisfied, the conditional variance of the OLE is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e125" xlink:type="simple"/></inline-formula>. This indicates that all the error comes from the bias, if we as usual write the mean square error (for scalar <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e126" xlink:type="simple"/></inline-formula>) in two parts, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e127" xlink:type="simple"/></inline-formula>. The condition obtained here can also be interpreted as “signal/readout being orthogonal to the noise.” While this perspective gives useful intuition about the result, we find that other ideas are more useful for constructing proofs of this and other results. We discuss this issue more thoroughly in Section “The geometry of the covariance matrix”.</p>
<p>In general, this condition may not be satisfied by some choices of pairwise correlations. The above theorem implies that, given the tuning curves, the issue of whether or not such “noise free” coding is achievable will be determined only by the relative magnitude, or heterogeneity, of the noise variances for each neuron – the diagonal entries of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e128" xlink:type="simple"/></inline-formula>. The following theorem outlines precisely the conditions under which such “noise-free” coding performance is possible, a condition that can be easily checked for given parameters of a model system, or for experimental data.</p>
<p><bold>Theorem 4.</bold> <italic>For scalar stimulus, let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e129" xlink:type="simple"/></inline-formula><italic>, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e130" xlink:type="simple"/></inline-formula><italic>, where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e131" xlink:type="simple"/></inline-formula><italic> is the readout vector for OLE in the noise-free case. Noise correlations may be chosen so that coding performance matches that which could be achieved in the absence of noise if and only if</italic><disp-formula id="pcbi.1003469.e132"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e132" xlink:type="simple"/><label>(1)</label></disp-formula><italic>When “</italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e133" xlink:type="simple"/></inline-formula><italic>” is satisfied, all optimal correlations attaining the maximum form a </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e134" xlink:type="simple"/></inline-formula><italic> dimensional convex set on the boundary of the spectrahedron. When “</italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e135" xlink:type="simple"/></inline-formula><italic>” is attained, the dimension of that set is </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e136" xlink:type="simple"/></inline-formula><italic>, where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e137" xlink:type="simple"/></inline-formula><italic> is the number of zeros in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e138" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p>We pause to make three observations about this Theorem. First, the set of optimal correlations, when it occurs, is high-dimensional. This bears out the notion that there are many different, highly diverse noise correlation structures that all give the same (optimal) level of the information metrics. Second, and more technically, we note that the (convex) set of optimal correlations is flat (contained in a hyperplane of its dimension), as viewed in the higher dimensional space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e139" xlink:type="simple"/></inline-formula>. A third intriguing implication of the theorem is that when noise-cancellation is possible, all optimal correlations are connected, as the set is convex (any two points are connected by a linear segment that also in the set), and thus the case of disjoint optima as in <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3 <bold>B</bold></xref> will never happen when optimal coding achieves noise-free levels. Indeed, in <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3 <bold>B</bold></xref>, the noise-free bound is not attained.</p>
<p>The high dimension of the convex set of noise-canceling correlations explains the diversity of optimal correlations seen in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4 <bold>B</bold></xref> (i.e., with different Euclidean norms). Such a property is nontrivial from a geometric point of view. One may conclude prematurely that the dimension result is obvious if one considers algebraically the number of free variables and constraints in the condition of Theorem 3. This argument would give the dimension of the resulting linear space. However, as shown in the proof, there is another nontrivial step to show that the linear space has some finite part that also satisfies the positive semidefinite constraint. Otherwise, many dimensions may shrink to zero in size, as happens at the corner of the spectrahedron, resulting in a small dimension.</p>
<p>The optimization problem can be thought of as finding the level set of information function associated with as large as possible value while still intersecting with the spectrahedron. The level sets are collections of all points where the information takes the same value. These form high dimensional surfaces, and contain each other, much as layers of an onion. Here these surfaces are also guaranteed to be convex as the information function itself is. Next, note from <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref> that we have already seen that the spectrahedron has sharp corners. Combining this with our view of the level sets, one might guess that the set of optimal solutions — i.e. the intersection — should be very low dimensional. Such intuition is often used in mathematics and computer science, e.g. with regards to the sparsity promoting tendency of L1 optimization. The high dimensionality shown by our theorem therefore reflects a nontrivial relationship between the shape of the spectrahedron and the level sets of the information quantities.</p>
<p>Although our theorem only characterizes the abundance of the set of <italic>exact</italic> optimal noise correlations, it is not hard to imagine the same, if not more, abundance should also hold for correlations that approximately achieve the maximal information level. This is indeed what we see in numerical examples. For example, note the long, curved level-set curves in <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref> near the boundaries of the allowed region. Along these lines lie many different noise correlation matrices that all achieve the same nearly-optimal values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e140" xlink:type="simple"/></inline-formula>. The same is true of the many dots in <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3 <bold>A</bold></xref> that all share a similar “bright” color corresponding to large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e141" xlink:type="simple"/></inline-formula>.</p>
<p>One may worry that the noise cancellation discussed above is rarely achievable, and thus somewhat spurious. The following theorem suggests that the opposite is true. In particular, it gives one simple condition under which the noise cancellation phenomenon, and resultant high-dimensional sets of optimal noise correlation matrices, will almost surely be possible in large neural populations.</p>
<p><bold>Theorem 5.</bold> <italic>If the </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e142" xlink:type="simple"/></inline-formula><italic> defined in Theorem 4 are independent and identically distributed (i.i.d.) as a random variable </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e143" xlink:type="simple"/></inline-formula><italic> on </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e144" xlink:type="simple"/></inline-formula><italic> with </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e145" xlink:type="simple"/></inline-formula><italic>, then the probability</italic><disp-formula id="pcbi.1003469.e146"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e146" xlink:type="simple"/><label>(2)</label></disp-formula>In actual populations, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e147" xlink:type="simple"/></inline-formula> might not be well described as i.i.d.. However, we believe that the conditions of the inequality of <xref ref-type="disp-formula" rid="pcbi.1003469.e578">Eq.(1)</xref> are still likely to be satisfied, as the contrary seems to require one neuron with highly outlying tuning and noise variance value (a few comparable outliers won't necessary violate the condition, as their magnitudes will enter on the right hand side of the condition, thus the condition only breaks with a single “outlier of outliers”).</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Summary</title>
<p>In this paper, we considered a general mathematical setup in which we investigated how coding performance changes as noise correlations are varied. Our setup made no assumptions about the shapes (or heterogeneity) of the neural tuning curves (or receptive fields), or the variances in the neural responses. Thus, our results – which we summarize below – provide general insights into the problem of population coding. These are as follows:</p>
<list list-type="bullet"><list-item>
<p>We proved that the <italic>sign rule</italic> — if signal and noise correlations have opposite signs, then the presence of noise correlations will improve encoded information vs. the independent case — holds for any neural population. In particular, we showed that this holds for three different metrics of encoded information, and for arbitrary tuning curves and levels of heterogeneity. Furthermore, we showed that, in the limit of weak correlations, the sign rule predicts the optimal structure of noise correlations for improving encoded information.</p>
</list-item><list-item>
<p>However, as also found in the literature (see below), the sign rule is not a necessary condition for good coding performance to be obtained. We observed that there will typically be a diverse family of correlation matrices that yield good coding performance, and these will often violate the sign rule.</p>
</list-item><list-item>
<p>There is significantly more structure to the relationship between noise correlations and encoded information than that given by the sign rule alone. The information metrics we considered are all <italic>convex</italic> functions with respect to the entries in the noise correlation matrix. Thus, we proved that the optimal correlation structures must lie on boundaries of any allowed set. These boundaries could come from mathematical constraints – all covariance matrices must be positive semidefinite – or mechanistic/biophysical ones.</p>
</list-item><list-item>
<p>Moreover, boundaries containing optimal noise correlations have several important properties. First, they typically contain correlation matrices that lead to the same high coding fidelity that one could obtain in the absence of noise. Second, when this occurs there is a high-dimensional set of different correlation matrices that all yield the same high coding fidelity – and many of these matrices strongly violate the sign rule.</p>
</list-item><list-item>
<p>Finally, for reasonably large neural populations, we showed that both the noise-free, and more general SR-violating optimal, correlation structures emerge while the average noise correlations remain quite low — with values comparable to some reports in the experimental literature.</p>
</list-item></list>
</sec><sec id="s3b">
<title>Convexity of information measures</title>
<p>Convexity of information with respect to noise correlations arises conceptually throughout the paper, and specifically in Theorem 2. We have shown that such convexity holds for all three particular measures of information studied above (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e148" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e149" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e150" xlink:type="simple"/></inline-formula>). Here, we show that these observations may reflect a property intrinsic to the concept of information, so that our results could apply more generally.</p>
<p>It is well known that mutual information is convex with respect to conditional distributions. Specifically, consider two random variables (or vectors) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e151" xlink:type="simple"/></inline-formula>, each with conditional distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e152" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e153" xlink:type="simple"/></inline-formula> (with respect the random “stimulus” variable(s) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e154" xlink:type="simple"/></inline-formula>). Suppose another variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e155" xlink:type="simple"/></inline-formula> has a conditional distribution given by a nonnegative linear combination of the two, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e156" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e157" xlink:type="simple"/></inline-formula>. The mutual information must satisfy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e158" xlink:type="simple"/></inline-formula>. Notably, this fact can be proved using only the axiomatic properties of mutual information (the chain rule for conditional information and nonnegativity) <xref ref-type="bibr" rid="pcbi.1003469-Cover1">[25]</xref>.</p>
<p>It is easy to see how this convexity in conditional distributions is related to the convexity in noise correlations we use. To do this, we further assume that the two conditional means are the same, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e159" xlink:type="simple"/></inline-formula>, and let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e160" xlink:type="simple"/></inline-formula> be random vectors. Introduce an auxiliary Bernoulli random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e161" xlink:type="simple"/></inline-formula> that is independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e162" xlink:type="simple"/></inline-formula>, with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e163" xlink:type="simple"/></inline-formula> of being 1. The variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e164" xlink:type="simple"/></inline-formula> can then be explicitly constructed using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e165" xlink:type="simple"/></inline-formula>: for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e166" xlink:type="simple"/></inline-formula>, draw <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e167" xlink:type="simple"/></inline-formula> according to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e168" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e169" xlink:type="simple"/></inline-formula> and according to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e170" xlink:type="simple"/></inline-formula> otherwise. Using the law of total covariance, the covariance (conditioned on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e171" xlink:type="simple"/></inline-formula>) between the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e172" xlink:type="simple"/></inline-formula> elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e173" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1003469.e174"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e174" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e175" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e176"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e176" xlink:type="simple"/></disp-formula>This shows that the noise covariances are expressed accordingly as linear combinations. If the information depends only on covariances (besides the fixed means), as for the three measures we consider, the two notions of convexity become equivalent. A direct corollary of this argument is that the convexity result of Theorem 2 also holds in the case of mutual information for conditionally Gaussian distributions (i.e., such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e177" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e178" xlink:type="simple"/></inline-formula> is Gaussian distributed).</p>
</sec><sec id="s3c">
<title>Sensitivity and robustness of the impact of correlations on encoded information</title>
<p>One obvious concern about our results, especially those related to the “noise-free” coding performance, is that this performance may not be robust to small perturbations in the covariance matrix – and thus, for example, real biological systems might be unable to exploit noise correlations in signal coding. This issue was recently highlighted, in particular, by <xref ref-type="bibr" rid="pcbi.1003469-Beck1">[26]</xref>.</p>
<p>At first, concerns about robustness might appear to be alleviated by our observation that there is typically a large set of possible correlation structures that all yield similar (optimal) coding performance (Theorem 4). However, if the correlation matrix was perturbed along a direction orthogonal to the level set of the information quantity at hand, this could still lead to arbitrary changes in information. To address this matter directly, we explicitly calculated the following upper bound for the sensitivity of information, or <italic>condition number</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e179" xlink:type="simple"/></inline-formula> with respect (sufficiently small) perturbations. The condition number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e180" xlink:type="simple"/></inline-formula> is defined as the ratio of relative change in the function to that in its variables. For example, the condition number corresponding to perturbing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e181" xlink:type="simple"/></inline-formula> is the smallest number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e182" xlink:type="simple"/></inline-formula> that satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e183" xlink:type="simple"/></inline-formula>. Similarly one can define condition number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e184" xlink:type="simple"/></inline-formula> for perturbing the tuning of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e185" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Proposition 6.</bold> <italic>The local condition number of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e186" xlink:type="simple"/></inline-formula><italic> under perturbations of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e187" xlink:type="simple"/></inline-formula><italic> (where magnitude is quantified by 2-norm) is bounded by</italic><disp-formula id="pcbi.1003469.e188"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e188" xlink:type="simple"/><label>(3)</label></disp-formula><italic>where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e189" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e190" xlink:type="simple"/></inline-formula><italic> are the largest and smallest eigenvalue of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e191" xlink:type="simple"/></inline-formula><italic> respectively. Here </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e192" xlink:type="simple"/></inline-formula><italic> is the condition number with respect to the 2-norm, as defined in the above equation.</italic></p>
<p><italic>Similarly, the condition number for perturbing of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e193" xlink:type="simple"/></inline-formula><italic> is bounded by</italic><disp-formula id="pcbi.1003469.e194"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e194" xlink:type="simple"/><label>(4)</label></disp-formula><italic>where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e195" xlink:type="simple"/></inline-formula><italic> is the </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e196" xlink:type="simple"/></inline-formula><italic>-th column of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e197" xlink:type="simple"/></inline-formula><italic> and assume </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e198" xlink:type="simple"/></inline-formula><italic> for all </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e199" xlink:type="simple"/></inline-formula><italic>. Here </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e200" xlink:type="simple"/></inline-formula><italic> is the dimension of the stimulus </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e201" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p>Though stated for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e202" xlink:type="simple"/></inline-formula>, same results also hold for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e203" xlink:type="simple"/></inline-formula> when replacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e204" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e205" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003469.e879">Eq. (3)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003469.e884">(4)</xref>. We believe that a similar property is possible to derive for mutual information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e206" xlink:type="simple"/></inline-formula>, but that the expression could be quite cumbersome; we do not pursue this further here.</p>
<p>To interpret this Proposition, we make the following observations which explain when the sensitivity or condition numbers will (or will not) be themselves reasonable in size, for given noise correlations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e207" xlink:type="simple"/></inline-formula>. In our setup, the diagonal of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e208" xlink:type="simple"/></inline-formula> (or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e209" xlink:type="simple"/></inline-formula> for OLE) is fixed, and therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e210" xlink:type="simple"/></inline-formula> is bounded (Gershgorin circle theorem). As long as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e211" xlink:type="simple"/></inline-formula> (or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e212" xlink:type="simple"/></inline-formula>) is not close to singular, the information should therefore be robust, i.e. with a reasonably bounded condition number. For OLE, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e213" xlink:type="simple"/></inline-formula>, we always have a universal bound of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e214" xlink:type="simple"/></inline-formula> determined only by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e215" xlink:type="simple"/></inline-formula>. For the linear Fisher information, however, nearly singular <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e216" xlink:type="simple"/></inline-formula> can more typically occur near optimal solutions; in these cases, the condition numbers will be very large.</p>
</sec><sec id="s3d">
<title>Relationship to previous work</title>
<p>Much prior work has investigated the relationship between noise correlations and the fidelity of signal coding <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003469-Averbeck2">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Romo1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1003469-Josi1">[16]</xref>. Two aspects of our current work complement and generalize those studies.</p>
<p>The first are our results on the sign rule (Section “The sign rule revisited”). Here, we find that, if each cell pair has noise correlations that have the opposite sign vs. their signal correlations, the encoded information is always improved, and that, at least in the case of weak noise correlations, noise correlations that have the same sign as the signal correlations will diminish encoded information. This effect was observed by <xref ref-type="bibr" rid="pcbi.1003469-Zohary1">[6]</xref> for neural populations with identically tuned cells. Since the tuning was identical in their work, all signal correlations were positive. Thus, their observation that positive noise correlations diminish encoded information is consistent with the SR results described above.</p>
<p>Relaxing the assumption of identical tuning, several studies followed <xref ref-type="bibr" rid="pcbi.1003469-Zohary1">[6]</xref> that used cell populations with tuning that differed from cell to cell, but maintained some homogeneous structure – i.e., identically shaped, and evenly spaced (along the stimulus axis) tuning curves, e.g., <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Abbott1">[7]</xref>. The models that were investigated then assumed that the noise correlation between each cell pair was a decaying function of the displacement between the cells' tuning curve peaks. The amplitude of the correlation function – which determines the maximal correlation over all cell pairs, attained for “nearby” cells – was the independent variable in the numerical experiments. Recall that these nearby (in tuning-curve space) cells, with overlapping tuning curves, will have positive signal correlations. These authors found that positive signs of noise correlations diminished encoded information, while negative noise correlations enhanced it. This is once again broadly consistent with the sign rule, at least for nearby cells which have the strongest correlation. Finally, we note that <xref ref-type="bibr" rid="pcbi.1003469-Averbeck1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Sompolinsky1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Latham1">[12]</xref> give a crisp geometrical interpretation of the sign rule in the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e217" xlink:type="simple"/></inline-formula> cells.</p>
<p>At the same time, experiments typically show noise correlations that are stronger for cell pairs with higher signal correlations <xref ref-type="bibr" rid="pcbi.1003469-Cohen1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Zohary1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Kohn1">[19]</xref>, which is certainly not in keeping with the sign rule. This underscores the need for new theoretical insights. To this effect, we demonstrated that, while noise correlations that obey the sign rule are guaranteed to improve encoded information relative to the independent case, this improvement can also occur for a diverse range of correlation structures that violate it. (Recall the asymmetry of our findings for the sign rule: noise correlations that violate the sign rule are only guaranteed to diminish encoded information if those noise correlations are very weak).</p>
<p>This finding is anticipated by the work of <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Shamir1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-daSilveira1">[14]</xref>, who used elegant analytical and numerical studies to reveal improvements in coding performance in cases where the sign rule was violated. They studied heterogeneous neural populations, with, for example, different maximal firing rates for different neurons. In particular, these authors show how heterogeneity can simultaneously improve the accuracy and capacity of stimulus encoding <xref ref-type="bibr" rid="pcbi.1003469-daSilveira1">[14]</xref>, or can create coding subspaces that are nearly orthogonal to directions of noise covariance <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Shamir1">[9]</xref>. Taken together, these studies show that the same noise correlation structure discussed above – with nearby cells correlated – could lead to improved population coding, so long as the noise correlations are sufficiently strong. <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref> also demonstrated that the magnitude of correlations needed to satisfy the “sufficiently strong” condition decreases as the population size increases, and that in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e218" xlink:type="simple"/></inline-formula> limit, certain coding properties become invariant to the structure of noise correlations. Overall, these findings agree with our observations about a large diversity of SR-violating noise correlation structures that improve encoded information.</p>
<p>One final study requires its own discussion. Whereas the current study (and those discussed above) investigated how coding relates to noise correlations with no concerns for the biophysical origin of those correlations <xref ref-type="bibr" rid="pcbi.1003469-Tkacik1">[17]</xref>, studied a semi-mechanistic model in which noise correlations were generated by inter-neuronal coupling. They observed that coupling that generates anti-SR correlations is beneficial for population coding when the noise level is very high, but that at low noise levels, the optimal population would follow the SR. Understanding why different mechanistic models can display different trends in their noise correlations is important, and we are currently investigating that issue.</p>
</sec><sec id="s3e">
<title>The geometry of the covariance matrix</title>
<p>One geometrical, and intuitively helpful, way to think about problems involving noise correlations is to ask when the noise is “orthogonal to the signal”: in these cases, the noise can be separated from or be orthogonal to the signal, and high coding performance is obtained. This geometrical view is equally valid for the cases we study (e.g., the conditions we derive in Theorem 3), and is implicit in the diagrams in <xref ref-type="fig" rid="pcbi-1003469-g001">Figure 1</xref>. To make the approach explicit, one could perform an eigenvector analysis on the covariance matrices at hand, where quantities like linear Fisher information are rewritten as a sum of projections of the tuning vector to the eigen-basis of the covariance matrix, weighted by the appropriate eigenvalues.</p>
<p>This invites the question of whether a simpler way to obtain the results in our paper wouldn't be to consider how covariance eigenvectors and eigenvalues could be manipulated more directly. For example, if one could simply “rotate” the eigenvectors of the covariance matrix out of the signal direction – or shrink the eigenvalues in that direction – one would necessarily improve coding performance. So why don't we simply do this when exploring spaces of covariance matrices? The reason is that these eigenvalue and eigenvector manipulations are not as easy to enact as they might at first sound (to us, and possibly to the reader). Recall that we asked how noise correlations affect coding subject to the specific constraint that the noise variance of each neuron is fixed, which translates in general to rather complex constraints on the eigenvalues and eigenvectors. For example, the eigenvalues of a fixed-diagonal covariance matrix cannot be <italic>equivalently</italic> described by simply having a fixed sum (which is a necessary condition for the diagonals to be constant, but is not a sufficient one). These facts limit the insights that a direct approach to adjusting eigenvalues and eigenvectors can have for our problem, and emphasize the non-trivial nature of our results.</p>
<p>An exception comes, for example, in special cases when the covariance matrix has a circulant structure, and consequently always has the Fourier basis for eigenvectors. These cases include many situations considered in the literature <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Sompolinsky1">[10]</xref>. For contrast, the covariance matrices we studied were allowed to change freely, as long as the diagonals remained fixed.</p>
</sec><sec id="s3f">
<title>Limitations and extensions</title>
<p>We have developed a rich picture of how correlated noise impacts population coding. For our results on noise cancellation in particular, this was done by allowing noise correlations to be chosen from the largest mathematically possible space (i.e., the entire spectrahedron). This describes the fundamental structure of the problem at hand, but are conclusions derived in this way important for biology? It is not hard to imagine many biological constraints that may further limit the range of possible noise correlations (e.g., limits on the strength of recurrent connections or shared inputs). On the one hand, the likelihood that the underlying phenomena could be found in biological systems seems increased by the fact that many different correlation matrices will suffice for noise free coding and that, as we discuss in Proposition 6, information levels appear to have some robustness under perturbations of the underlying correlation matrices.</p>
<p>However, care must still be taken in interpreting what we mean by “noise free.” As emphasized by, e.g., <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Beck2">[27]</xref>, noise upstream from the neural population in question can never be removed in subsequent processing. Therefore, the “noise free” bound we discuss in Lemma 8 should not allow for a higher information level than that determined by this upstream noise. In some cases, this fact could lead to a consistency requirement on either the set of signal correlations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e219" xlink:type="simple"/></inline-formula>, the set of allowed noise correlations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e220" xlink:type="simple"/></inline-formula>, or both. To specify these constraints and avoid possible over-interpretations of the abstract coding model as we study, one could combine a explicit mechanistic model with the present approach.</p>
<p>On another note, we have asked what noise correlations allow for linear decoders to best recover the stimulus from the set of neural population responses. At the same time, there is reason to be wary of linear decoders <xref ref-type="bibr" rid="pcbi.1003469-Shamir2">[28]</xref> (see also <xref ref-type="bibr" rid="pcbi.1003469-Josi1">[16]</xref>), as they might miss significant information that is only accessible via a non-linear read-out. Furthermore, given the non-linearity inherent in dendritic processing and spike generation <xref ref-type="bibr" rid="pcbi.1003469-Koch1">[29]</xref>, there is added motivation to consider information without assuming linearity.</p>
<p>Furthermore, we have herein restricted ourselves to asking about pairwise noise correlations, while there are many studies that identify higher-order correlations (HOC) in neural data <xref ref-type="bibr" rid="pcbi.1003469-Ganmor1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003469-Ohiorhenuan1">[31]</xref>, and some numerical results <xref ref-type="bibr" rid="pcbi.1003469-Zylberberg1">[32]</xref> that hint at when those HOC are beneficial for coding. In light of this study, it is interesting to ask whether we can derive a similarly general theory for HOC, and to investigate how the optimal pairwise and higher-order correlations interrelate. Note this issue is closely related to the type of decoder that is assumed: the performance of linear decoder (as measured by mean squared error) depends on the pairwise correlations, but not HOC. Therefore the effect of HOC must be studied in the context of nonlinear coding.</p>
<p>Finally, we note that here we used an abstract coding model that evaluates information based on the statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e221" xlink:type="simple"/></inline-formula> and so on. For generality, we made no assumptions on the structure of these statistics, and any links among them. This suggests two questions for future work: whether an arbitrary set of such statistics is realizable in a constructive model of random variables, and whether there are any typical relationships between these statistics when they arise from tuned neural populations. As a preliminary investigation, we partially confirmed the answer to the first question, except for a “zero measure” set of statistics, under generic assumptions (data not shown).</p>
</sec><sec id="s3g">
<title>Experimental implications</title>
<p>Recall that we observed that, in general, for a given set of tuning curves and noise variances, there will be a diverse family of noise correlation matrices that will yield good (optimal, or near-optimal) performance. This effect can be observed in <xref ref-type="fig" rid="pcbi-1003469-g002">Figs. 2</xref>, <xref ref-type="fig" rid="pcbi-1003469-g003">3</xref>, and <xref ref-type="fig" rid="pcbi-1003469-g005">5</xref>, as well as our result about the dimension of the set of correlation matrices that yield (when it is possible) noise-free coding performance (Theorem 4).</p>
<p>At least compared with the alternative of a unique optimal noise correlation structure, our findings imply that it could be relatively “easy” for the biological system to find good correlation matrices. At the same time, since the set of good solutions is so large, we should not be surprised to see heterogeneity in the correlation structures exhibited by biological systems. Similar observations have previously been made in the context of neural oscillators: Prinz and colleagues <xref ref-type="bibr" rid="pcbi.1003469-Prinz1">[33]</xref> observed that neuronal circuits with a variety of different parameter values could produce the types of rhythmic activity patterns displayed by the crab stomatogastric ganglion. Consequently, there is much animal-to-animal variability in this circuit <xref ref-type="bibr" rid="pcbi.1003469-Marder1">[34]</xref>, even though the system's performance is strongly conserved.</p>
<p>At the same time, the potential diversity of solutions could present a serious challenge for analyzing data (cf. <xref ref-type="bibr" rid="pcbi.1003469-Beck1">[26]</xref>). Notice, that, at least for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e222" xlink:type="simple"/></inline-formula> cases of <xref ref-type="fig" rid="pcbi-1003469-g002">Figs. 2</xref> and <xref ref-type="fig" rid="pcbi-1003469-g003">3</xref> for example, how much the performance can vary as one of the correlation coefficients is changed, while keeping the other ones fixed. If this phenomenon is general, it means that, in an experiment where we observe a (possibly small) subset of the correlation coefficients, it may be very hard to know how those correlations actually affect coding: the answer to that question depends strongly on all of the other (unobserved) correlation coefficients. As our recording technologies improve <xref ref-type="bibr" rid="pcbi.1003469-Stevenson1">[35]</xref>, and we make more use of optical methods, these “gaps” in our datasets will get smaller, and this issue may be resolved; further theoretical work to gauge the seriousness of the underlying issue is also needed. In the meanwhile, caution seems wise when analyzing noise correlations in sparsely sampled data.</p>
<p>Finally, recall that the optimal noise correlations will always lie on the boundary of the allowed region of such correlations. Importantly, what we mean by that boundary is flexible. It may be the mathematical requirement of positive semidefinite covariance matrices – the loosest possible requirement – or there may be tighter constraints that restrict the set of correlation coefficients. Since biophysical mechanisms determine noise correlations, we expect that there will be identifiable regions of possible correlation coefficients that are possible in a given circuit/system. Understanding those “allowed” regions will, we anticipate, be important for attempts to relate noise correlations to coding performance, and ultimately to help untangle the relationship between structure and function in sensory systems.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<p>In the <xref ref-type="sec" rid="s4">Methods</xref> below, we will first revisit the problem set-up, and define our metrics of coding quality. We will then prove the theorems from the main text. Finally, we will provide the details of our numerical examples. A summary of our most frequently used notation is listed in <xref ref-type="table" rid="pcbi-1003469-t001">Table 1</xref>.</p>
<sec id="s4a">
<title>Summary of the problem set-up</title>
<p>We consider populations of neurons that encode a stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e223" xlink:type="simple"/></inline-formula> by their noisy responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e224" xlink:type="simple"/></inline-formula>. For simplicity, we will suppress the vector notation in the <xref ref-type="sec" rid="s4">Methods</xref> Unless otherwise stated, most of our results apply equally well to either scalar, or multi-dimensional, stimuli.</p>
<p>The mean activity or “tuning” of the neurons are described by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e225" xlink:type="simple"/></inline-formula>. In the case of scalar stimuli, this corresponds to the notion of a tuning curve. For more complex stimuli, this is more aligned with the idea of a receptive field.</p>
<p>The trial-to-trial noise part in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e226" xlink:type="simple"/></inline-formula>, given a fixed stimulus, can be described by the conditional covariance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e227" xlink:type="simple"/></inline-formula> (superscript <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e228" xlink:type="simple"/></inline-formula> denotes “noise”). In particular <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e229" xlink:type="simple"/></inline-formula> are noise variances of each neuron.</p>
<p>We ask questions of the following type: given fixed tuning curves <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e230" xlink:type="simple"/></inline-formula> and noise variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e231" xlink:type="simple"/></inline-formula>, how does the choice of noise covariance structure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e232" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e233" xlink:type="simple"/></inline-formula> affect linear Fisher information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e234" xlink:type="simple"/></inline-formula> (see Section “Defining the information quantities, signal and noise correlations”)?</p>
<p>Besides the local information measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e235" xlink:type="simple"/></inline-formula> that quantifies coding near a specific stimulus, we also considered global measures that describe overall coding of the entire ensemble of stimuli. These are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e236" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e237" xlink:type="simple"/></inline-formula>, described in Section “Defining the information quantities, signal and noise correlations”. For these quantities, the relevant noise covariance is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e238" xlink:type="simple"/></inline-formula>. We overload the notation with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e239" xlink:type="simple"/></inline-formula> in these global coding contexts. The optimization problem can then be identically stated for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e240" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e241" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b">
<title>Defining the information quantities, signal and noise correlations</title>
<sec id="s4b1">
<title>Linear Fisher information</title>
<p>Linear Fisher information quantifies how accurately the stimulus near a value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e242" xlink:type="simple"/></inline-formula> can be decoded by a local linear unbiased estimator, and is given by<disp-formula id="pcbi.1003469.e243"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e243" xlink:type="simple"/><label>(5)</label></disp-formula>In the case of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e244" xlink:type="simple"/></inline-formula> dimensional stimulus the same definition holds, with<disp-formula id="pcbi.1003469.e245"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e245" xlink:type="simple"/><label>(6)</label></disp-formula>In order for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e246" xlink:type="simple"/></inline-formula> to be defined by <xref ref-type="disp-formula" rid="pcbi.1003469.e243">Eq. (5)</xref>, we assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e247" xlink:type="simple"/></inline-formula> is invertible and hence positive definite: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e248" xlink:type="simple"/></inline-formula>. It can be shown that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e249" xlink:type="simple"/></inline-formula> is the (attainable) lower bound of the covariance matrix of the error of any local linear unbiased estimator. Here the term lower bound is used in the sense of positive semidefiniteness, that is the ordering <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e250" xlink:type="simple"/></inline-formula> if and only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e251" xlink:type="simple"/></inline-formula>. To obtain a scalar information quantity, we consider <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e252" xlink:type="simple"/></inline-formula> and also denote this by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e253" xlink:type="simple"/></inline-formula> if not stated otherwise.</p>
</sec><sec id="s4b2">
<title>Optimal linear estimator</title>
<p>To quantify the <italic>global</italic> ability of the population to encode the stimulus (instead of <italic>locally</italic>, as for discrimination tasks involving small deviations from a particular stimulus value), we follow <xref ref-type="bibr" rid="pcbi.1003469-Salinas1">[18]</xref> and consider a linear estimator of the stimulus, given responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e254" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003469.e255"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e255" xlink:type="simple"/><label>(7)</label></disp-formula>with fixed parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e256" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e257" xlink:type="simple"/></inline-formula> unchanged with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e258" xlink:type="simple"/></inline-formula>. The set of readout coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e259" xlink:type="simple"/></inline-formula> that minimize the mean square error for a scalar random stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e260" xlink:type="simple"/></inline-formula>, i.e.<disp-formula id="pcbi.1003469.e261"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e261" xlink:type="simple"/><label>(8)</label></disp-formula>can be solved analytically as in <xref ref-type="bibr" rid="pcbi.1003469-Salinas1">[18]</xref>, yielding:<disp-formula id="pcbi.1003469.e262"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e262" xlink:type="simple"/><label>(9)</label></disp-formula>where<disp-formula id="pcbi.1003469.e263"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e263" xlink:type="simple"/><label>(10)</label></disp-formula>and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e264" xlink:type="simple"/></inline-formula> is a column vector with entries <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e265" xlink:type="simple"/></inline-formula>. Here the expectation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e266" xlink:type="simple"/></inline-formula> generally means averaging over both noise and stimulus (except in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e267" xlink:type="simple"/></inline-formula>, where averaging is only over the stimulus).</p>
<p>For multidimensional stimuli <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e268" xlink:type="simple"/></inline-formula>, similar to the case for linear Fisher information, the lower bound (in sense of positive semidefiniteness) of the error covariance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e269" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e270" xlink:type="simple"/></inline-formula>. Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e271" xlink:type="simple"/></inline-formula> is extended to form a matrix<disp-formula id="pcbi.1003469.e272"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e272" xlink:type="simple"/><label>(11)</label></disp-formula>Furthermore, a corresponding lower bound for the sum of squared errors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e273" xlink:type="simple"/></inline-formula> is the scalar version <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e274" xlink:type="simple"/></inline-formula>.</p>
<p>When minimizing the OLE error with respect to noise correlations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e275" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e276" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e277" xlink:type="simple"/></inline-formula> are constants with respect to the optimization. Minimizing OLE error is therefore equivalent to maximizing the second term above, given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e278" xlink:type="simple"/></inline-formula>. This motivates us to define what we call “the information for OLE”, which is simply the second term (above) — i.e., the term that is subtracted from the signal variance to yield the OLE error. Specifically,<disp-formula id="pcbi.1003469.e279"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e279" xlink:type="simple"/><label>(12)</label></disp-formula>Thus, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e280" xlink:type="simple"/></inline-formula> is large, the decoding error is small, and vice versa. Comparing with the expression for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e281" xlink:type="simple"/></inline-formula>, we see a similar mathematical structure, which will enable almost identical proofs of our theorems for both of these measures of coding performance.</p>
<p>Similar to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e282" xlink:type="simple"/></inline-formula>, we need <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e283" xlink:type="simple"/></inline-formula> to be invertible in order to calculate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e284" xlink:type="simple"/></inline-formula>. Since the signal covariance matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e285" xlink:type="simple"/></inline-formula> does not change as we vary <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e286" xlink:type="simple"/></inline-formula>, this requirement is easy to satisfy. In particular, we assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e287" xlink:type="simple"/></inline-formula> is invertible (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e288" xlink:type="simple"/></inline-formula>), and thus for all consistent – i.e. positive semidefinite – <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e289" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e290" xlink:type="simple"/></inline-formula>, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e291" xlink:type="simple"/></inline-formula> is invertible.</p>
</sec><sec id="s4b3">
<title>Mutual information for Gaussian distributions</title>
<p>While the OLE and the linear Fisher information assume that a linear read-out of the population responses is used to estimate the stimulus, one may also be interested in how well the stimulus could be recovered by more sophisticated, nonlinear estimators. Mutual information, based on Shannon entropy is a useful quantity of this sort. It has many desirable properties consistent with the intuitive notion of “information”, and it we will use it to quantify how well a non-linear estimator could recover the stimulus.</p>
<p>Assuming that the joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e292" xlink:type="simple"/></inline-formula> is Gaussian (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e293" xlink:type="simple"/></inline-formula> can be multidimensional), the mutual information has a simple expression<disp-formula id="pcbi.1003469.e294"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e294" xlink:type="simple"/><label>(13)</label></disp-formula>The quantities above are the same as in the definitions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e295" xlink:type="simple"/></inline-formula>. Moreover, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e296" xlink:type="simple"/></inline-formula> is taken to base <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e297" xlink:type="simple"/></inline-formula>, and hence the information is in units of nats. To convert to bits, one must simply divide our <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e298" xlink:type="simple"/></inline-formula> values by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e299" xlink:type="simple"/></inline-formula>.</p>
<p>There is a consistency constraint that must be satisfied by any joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e300" xlink:type="simple"/></inline-formula>, namely that<disp-formula id="pcbi.1003469.e301"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e301" xlink:type="simple"/><label>(14)</label></disp-formula>This guarantees that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e302" xlink:type="simple"/></inline-formula> is always defined and real (but could be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e303" xlink:type="simple"/></inline-formula>). To keep <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e304" xlink:type="simple"/></inline-formula> finite, one needs to further assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e305" xlink:type="simple"/></inline-formula>, which is equivalent to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e306" xlink:type="simple"/></inline-formula>. This can be seen by rewriting mutual information while exchanging the position of the two variables (since mutual information is symmetric),<disp-formula id="pcbi.1003469.e307"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e307" xlink:type="simple"/></disp-formula></p>
<p>It is easy to see that the formula contains terms similar to those in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e308" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e309" xlink:type="simple"/></inline-formula>. In the scalar stimulus case, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e310" xlink:type="simple"/></inline-formula> is an increasing function, maximizing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e311" xlink:type="simple"/></inline-formula> is equivalent to maximizing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e312" xlink:type="simple"/></inline-formula>. In fact, the leading term in the Taylor expansion of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e313" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e314" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e315" xlink:type="simple"/></inline-formula>, which is proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e316" xlink:type="simple"/></inline-formula>. In the case of multivariate stimuli <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e317" xlink:type="simple"/></inline-formula>, we note that the operation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e318" xlink:type="simple"/></inline-formula> preserves ordering defined in the positive semidefinite sense, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e319" xlink:type="simple"/></inline-formula>. This close relationship suggests a way of transforming <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e320" xlink:type="simple"/></inline-formula> to a comparable scale of information in nats (or bits) as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e321" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b4">
<title>Signal and noise correlations</title>
<p>Given the noise covariance matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e322" xlink:type="simple"/></inline-formula> one can normalize it as usual by its diagonal elements (variances) to obtain correlation coefficients<disp-formula id="pcbi.1003469.e323"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e323" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>We next discuss signal correlations, which describe how similar the tuning of a pair of neurons is. For linear Fisher information, we define signal correlations as<disp-formula id="pcbi.1003469.e324"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e324" xlink:type="simple"/><label>(16)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e325" xlink:type="simple"/></inline-formula> is the sensitivity vector describing how the mean response of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e326" xlink:type="simple"/></inline-formula> changes with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e327" xlink:type="simple"/></inline-formula>. With the above normalization, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e328" xlink:type="simple"/></inline-formula> takes value between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e329" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e330" xlink:type="simple"/></inline-formula>.</p>
<p>For the other two information measures we use, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e331" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e332" xlink:type="simple"/></inline-formula>, a similar signal correlation can be defined. Here, we first define analogous tuning sensitivity vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e333" xlink:type="simple"/></inline-formula> for each neuron, which will replace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e334" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003469.e324">Eq. (16)</xref>. These vectors are<disp-formula id="pcbi.1003469.e335"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e335" xlink:type="simple"/><label>(17)</label></disp-formula>for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e336" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e337" xlink:type="simple"/></inline-formula> respectively. Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e338" xlink:type="simple"/></inline-formula> is the diagonal matrix of noise variances, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e339" xlink:type="simple"/></inline-formula>.</p>
<p>The definitions of signal correlations above are chosen so that they are tied directly to the concept of the sign rule, as demonstrated in the proof of Theorem 1. As a consequence, for the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e340" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e341" xlink:type="simple"/></inline-formula>, signal correlations are defined through the population readout vector. This has an important implication that we note here. Consider a case where only a subset of the total population is “read out” to decode a stimulus. Then, the population readout vector — and hence the signal correlations defined above — could vary in magnitude and even possibly change signs depending on which neurons are included in the subset.</p>
<p>A different definition of signal correlations for OLE is sometimes used in literature, which we denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e342" xlink:type="simple"/></inline-formula>. Naturally, one should not expect our sign rule results to apply exactly under this definition. However, when we redid our plots of signal vs. noise correlations using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e343" xlink:type="simple"/></inline-formula> for our major numerical example (<xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5 <bold>ABC</bold></xref>), we observed the same qualitative trend (data not shown). This reflects the fact that, at least in this specific example, the signal correlations defined in the two ways are positively correlated. Understanding how general this phenomenon is would require further studies taking into account how the relevant statistics (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e344" xlink:type="simple"/></inline-formula>, L, etc.) are generated from tuning curves or neuron models.</p>
<p>We next define the notion of the magnitude or strength of correlations, as came up throughout the paper. In particular, in Section “Heterogeneously tuned neural populations”, we considered restrictions on the magnitudes of noise correlations when finding their optimal values. We proceed as follows. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e345" xlink:type="simple"/></inline-formula>, the list of all pairwise correlations of the population can be regarded as a single point in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e346" xlink:type="simple"/></inline-formula>. If not stated otherwise, the vector 2-norm in that space (Euclidean norm) is what we call the “strength of correlations:”<disp-formula id="pcbi.1003469.e347"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e347" xlink:type="simple"/><label>(18)</label></disp-formula></p>
</sec></sec><sec id="s4c">
<title>Proof of Theorem 1: The generality of the sign rule</title>
<p>We will now restate and then prove Theorem 1, first for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e348" xlink:type="simple"/></inline-formula> and then for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e349" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e350" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Theorem 1.</bold> <italic>If, for each pair of neurons, the signal and noise correlations have opposite signs, the linear Fisher information is greater than the case of independent noise (trial-shuffled data). In the opposite situation where the signs are the same, the linear Fisher information is decreased compared to the independent case, in a regime of very weak correlations. Similar results hold for I</italic><sub>OLE</sub> <italic>and I</italic><sub>mut</sub>;<sub>G</sub>, <italic>with a modified definition of signal correlations given in Section</italic> “Defining the information quantities, signal and noise correlations”.</p>
<p>The proof proceeds by showing that information increases along the direction indicated by the sign rule, and that the information quantities are convex, so that information is guaranteed to increase monotonically along that direction.</p>
<p><italic>Proof.</italic> Consider linear Fisher information<disp-formula id="pcbi.1003469.e351"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e351" xlink:type="simple"/><label>(19)</label></disp-formula>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e352" xlink:type="simple"/></inline-formula> be the diagonal part of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e353" xlink:type="simple"/></inline-formula>, corresponding to (noise) variance for each neuron. We change the off-diagonal entries of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e354" xlink:type="simple"/></inline-formula> along a certain direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e355" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e356" xlink:type="simple"/></inline-formula> and consider a parameterization of the resultant covariance matrix, with parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e357" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e358" xlink:type="simple"/></inline-formula>. We evaluate the directional derivative (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e359" xlink:type="simple"/></inline-formula>) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e360" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e361" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003469.e362"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e362" xlink:type="simple"/><label>(20)</label></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e363" xlink:type="simple"/></inline-formula>, and we have used the identity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e364" xlink:type="simple"/></inline-formula> and the fact <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e365" xlink:type="simple"/></inline-formula>. Recalling the definition of signal correlations in <xref ref-type="disp-formula" rid="pcbi.1003469.e324">Eq. (16)</xref>, if the sign of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e366" xlink:type="simple"/></inline-formula> is chosen to be opposite to the sign of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e367" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e368" xlink:type="simple"/></inline-formula>, then <xref ref-type="disp-formula" rid="pcbi.1003469.e362">Eq. (20)</xref> ensures that the directional derivative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e369" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e370" xlink:type="simple"/></inline-formula>.</p>
<p>We now derive a global consequence of this local derivative calculation. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e371" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e372" xlink:type="simple"/></inline-formula> has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e373" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e374" xlink:type="simple"/></inline-formula> is smooth, there exists <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e375" xlink:type="simple"/></inline-formula>, such that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e376" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e377" xlink:type="simple"/></inline-formula>. For corresponding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e378" xlink:type="simple"/></inline-formula>, applying the mean value theorem, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e379" xlink:type="simple"/></inline-formula>. Similarly, for the opposite case where all the signs of the noise correlations are the same as the signs of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e380" xlink:type="simple"/></inline-formula>, the information will be smaller than the independent case (at least for weak enough correlations). This proves the local “sign rule”.</p>
<p>Thus, at least for small noise correlations, choosing noise correlations that oppose signal correlations will always be yield higher information values than the case of uncorrelated noise. To prove the “global” version of this theorem — that opponent signal and noise correlations always yield better coding than does independent noise — we will need to establish the convexity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e381" xlink:type="simple"/></inline-formula>. This is done in Theorem 2.</p>
<p>Note that, as we will soon prove, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e382" xlink:type="simple"/></inline-formula> is a convex function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e383" xlink:type="simple"/></inline-formula>, and hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e384" xlink:type="simple"/></inline-formula> is increasing with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e385" xlink:type="simple"/></inline-formula>. This means that the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e386" xlink:type="simple"/></inline-formula> from our prior argument can be made arbitrarily large, and the same result – that performance improves when noise correlations are added, so long as they lie along this direction – will hold, provided that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e387" xlink:type="simple"/></inline-formula> is still physically realizable. Thus, the improvement over the independent case is guaranteed globally for any magnitude of noise correlations.</p>
<p>Note that the arguments above do not guarantee that the globally optimal noise correlation structure will follow the sign rule. Indeed, we have seen concrete examples of this in <xref ref-type="fig" rid="pcbi-1003469-g002">Figs. 2</xref> and <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref>.</p>
<p><italic>Remark</italic> 1. From <xref ref-type="disp-formula" rid="pcbi.1003469.e362">Eq. (20)</xref>, the gradient (steepest uphill direction) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e388" xlink:type="simple"/></inline-formula> evaluated with independent noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e389" xlink:type="simple"/></inline-formula> <italic>is </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e390" xlink:type="simple"/></inline-formula>.</p>
<p><italic>Remark</italic> 2. The same result can be shown for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e391" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e392" xlink:type="simple"/></inline-formula>, replacing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e393" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e394" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e395" xlink:type="simple"/></inline-formula>, respectively, in the definition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e396" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003469.e324">Eq. (16)</xref>. The gradients are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e397" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e398" xlink:type="simple"/></inline-formula>, respectively, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e399" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e400" xlink:type="simple"/></inline-formula>-th row of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e401" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e402" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4d">
<title>Proof of Theorem 2: Optima lie on boundaries</title>
<p>We begin by restating Theorem 2, which we then prove first for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e403" xlink:type="simple"/></inline-formula> and then for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e404" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e405" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Theorem 2.</bold> <italic>The optimal C<sup>n</sup> that maximize information must lie on the boundary of the region of correlations considered in the optimization.</italic></p>
<p>We will show that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e406" xlink:type="simple"/></inline-formula> is a convex function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e407" xlink:type="simple"/></inline-formula> and hence it will either attain its maximum value <italic>only</italic> on the boundary of the allowed region, or it will be uniformly constant. The latter is a trivial case that only happens when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e408" xlink:type="simple"/></inline-formula>, as we see below.</p>
<p><italic>Proof.</italic> To show that a function is convex, it is sufficient to show its second derivative along any linear direction is non-negative. For any constant direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e409" xlink:type="simple"/></inline-formula> of changing (off-diagonal entries of) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e410" xlink:type="simple"/></inline-formula>, we consider a straight-line perturbation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e411" xlink:type="simple"/></inline-formula> parameterized by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e412" xlink:type="simple"/></inline-formula>. Taking the derivative of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e413" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e414" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003469.e415"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e415" xlink:type="simple"/><label>(21)</label></disp-formula>We have used that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e416" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e417" xlink:type="simple"/></inline-formula>. Taking another derivative gives<disp-formula id="pcbi.1003469.e418"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e418" xlink:type="simple"/><label>(22)</label></disp-formula>The inequality is because of Lemma 7 (see below) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e419" xlink:type="simple"/></inline-formula> being positive definite. Also, note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e420" xlink:type="simple"/></inline-formula>.</p>
<p>For the case when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e421" xlink:type="simple"/></inline-formula> is constant over the region, using Proposition 10 (below), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e422" xlink:type="simple"/></inline-formula> for any direction of change <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e423" xlink:type="simple"/></inline-formula>. Letting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e424" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e425" xlink:type="simple"/></inline-formula>, we see that the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e426" xlink:type="simple"/></inline-formula>-th row of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e427" xlink:type="simple"/></inline-formula> must be 0. This leads to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e428" xlink:type="simple"/></inline-formula> and, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e429" xlink:type="simple"/></inline-formula>, to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e430" xlink:type="simple"/></inline-formula>. This was the claim in the beginning. In other words, in the case where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e431" xlink:type="simple"/></inline-formula> is constant with respect to the noise correlations, the optimal read-out is zero, regardless of the neurons' responses. With the exception of this (trivial) case, the optimal coding performance is obtained when the noise correlation matrix lies on a boundary of the allowed region.</p>
<p><bold>Lemma 7.</bold> <italic>(Linear algebra fact) For any positive semidefinite matrix </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e432" xlink:type="simple"/></inline-formula><italic>, and any matrix </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e433" xlink:type="simple"/></inline-formula><italic>, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e434" xlink:type="simple"/></inline-formula><italic> (assuming the dimensions match for matrix multiplications) is positive semidefinite and hence </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e435" xlink:type="simple"/></inline-formula><italic>. If “ = ” is attained, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e436" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Remark</italic> 3. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e437" xlink:type="simple"/></inline-formula> i.e. positive definite, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e438" xlink:type="simple"/></inline-formula> leads to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e439" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e440" xlink:type="simple"/></inline-formula> is invertible.</p>
<p><italic>Proof.</italic> For any vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e441" xlink:type="simple"/></inline-formula> (with the same dimension as the number of columns in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e442" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e443" xlink:type="simple"/></inline-formula> since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e444" xlink:type="simple"/></inline-formula>. Thus, by definition, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e445" xlink:type="simple"/></inline-formula>, and therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e446" xlink:type="simple"/></inline-formula>.</p>
<p>For the second part, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e447" xlink:type="simple"/></inline-formula>, all the eigenvalues of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e448" xlink:type="simple"/></inline-formula> must be 0 (since none of them can be negative as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e449" xlink:type="simple"/></inline-formula>), hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e450" xlink:type="simple"/></inline-formula>. This in fact requires <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e451" xlink:type="simple"/></inline-formula>. To see this, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e452" xlink:type="simple"/></inline-formula> be an orthogonal diagonalization of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e453" xlink:type="simple"/></inline-formula>. For any vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e454" xlink:type="simple"/></inline-formula> as above, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e455" xlink:type="simple"/></inline-formula>. Since the eigenvalues <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e456" xlink:type="simple"/></inline-formula> are non-negative, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e457" xlink:type="simple"/></inline-formula> be the diagonal matrix with the square roots of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e458" xlink:type="simple"/></inline-formula>. We have<disp-formula id="pcbi.1003469.e459"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e459" xlink:type="simple"/><label>(23)</label></disp-formula>Therefore the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e460" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e461" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e462" xlink:type="simple"/></inline-formula> can be any vector, we must have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e463" xlink:type="simple"/></inline-formula>.</p>
<p><italic>Remark</italic> 4. Because of the similarities in the formulae for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e464" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e465" xlink:type="simple"/></inline-formula>, the same property can be shown for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e466" xlink:type="simple"/></inline-formula>. In order for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e467" xlink:type="simple"/></inline-formula> to be invertible, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e468" xlink:type="simple"/></inline-formula> is only defined over the open set of positive definite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e469" xlink:type="simple"/></inline-formula>. We therefore assume the closure of the allowed region is contained within this open set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e470" xlink:type="simple"/></inline-formula> to state the boundary result.</p>
<p>A parallel version of Theorem 2 can also be established for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e471" xlink:type="simple"/></inline-formula>, as we next show.</p>
<p><italic>Proof of Theorem 2</italic> for <italic>I</italic><sub>mut;G</sub>. Again consider the linear parameterization <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e472" xlink:type="simple"/></inline-formula> along a direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e473" xlink:type="simple"/></inline-formula>, as defined above. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e474" xlink:type="simple"/></inline-formula>. The consistency constraint in <xref ref-type="disp-formula" rid="pcbi.1003469.e301">Eq. (14)</xref> assures <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e475" xlink:type="simple"/></inline-formula>. To keep <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e476" xlink:type="simple"/></inline-formula> finite, we further assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e477" xlink:type="simple"/></inline-formula>. Then, the derivative of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e478" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e479" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1003469.e480"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e480" xlink:type="simple"/><label>(24)</label></disp-formula>where we have used the identity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e481" xlink:type="simple"/></inline-formula>. The second derivative is thus<disp-formula id="pcbi.1003469.e482"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e482" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e483"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e483" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e484"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e484" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e485"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e485" xlink:type="simple"/></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e486" xlink:type="simple"/></inline-formula> is the identity matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e487" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e488" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e489" xlink:type="simple"/></inline-formula> as defined below <xref ref-type="disp-formula" rid="pcbi.1003469.e262">Eq. (9)</xref>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e490" xlink:type="simple"/></inline-formula> being positive definite allows us to split it into its square root <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e491" xlink:type="simple"/></inline-formula>. Moreover, the identity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e492" xlink:type="simple"/></inline-formula>, for any matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e493" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e494" xlink:type="simple"/></inline-formula>, is used in deriving the last line in the above equation. For the last inequality, we apply Lemma 7 to the two terms with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e495" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e496" xlink:type="simple"/></inline-formula> being positive semidefinite.</p>
<p>We have thus shown that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e497" xlink:type="simple"/></inline-formula> is convex. For the special case that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e498" xlink:type="simple"/></inline-formula> is constant, Proposition 10 shows <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e499" xlink:type="simple"/></inline-formula>. With the same argument as for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e500" xlink:type="simple"/></inline-formula>, we observe that, in this (trivial) case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e501" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4e">
<title>Proof of Theorem 3: Conditions on the noise covariance matrix, under which noise-free coding is possible</title>
<p>We begin by showing that, for a given set of tuning curves, the maximum possible information – which may or may not be attainable in the presence of noise – is that which would be achieved if there were no noise in the responses. This is the content of Lemma 8. Next, we will introduce Lemma 9, which is a useful linear-algebraic fact that we will use repeatedly in our proofs.</p>
<p>We will then prove Theorem 3, which provides the conditions under which such noise-free performance can be obtained. One direction of the proof of Theorem 3 (sufficiency) is straightforward, while the other direction (necessity) relies on the observation of several conditions that are equivalent to the one in the theorem. We prove these equalities in Proposition 10.</p>
<p>For Theorem 3, we will only consider <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e502" xlink:type="simple"/></inline-formula>, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e503" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e504" xlink:type="simple"/></inline-formula> will typically be infinity in the noise-free case (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e505" xlink:type="simple"/></inline-formula> becomes singular). If one takes all instances of infinite information as “equally optimal,” a version of Theorem 3 can also be obtained; moreover, the condition in Theorem 3 becomes a sufficient but not necessary condition for infinite information.</p>
<p><bold>Lemma 8</bold> (Upper bound by noise-free information).<disp-formula id="pcbi.1003469.e506"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e506" xlink:type="simple"/><label>(25)</label></disp-formula><italic>Here the noise-free information </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e507" xlink:type="simple"/></inline-formula><italic> refers to that which is obtained when plugging in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e508" xlink:type="simple"/></inline-formula><italic> at the place of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e509" xlink:type="simple"/></inline-formula><italic> in </italic><xref ref-type="disp-formula" rid="pcbi.1003469.e279"><italic>Eq. (12)</italic></xref><italic>.</italic></p>
<p><italic>Proof.</italic> This follows essentially from the consistency between the information quantity and the positive semidefinite ordering of covariance matrices. First, we write<disp-formula id="pcbi.1003469.e510"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e510" xlink:type="simple"/><label>(26)</label></disp-formula>Then, we note the fact that for two positive definite matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e511" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e512" xlink:type="simple"/></inline-formula> if and only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e513" xlink:type="simple"/></inline-formula>. From this, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e514" xlink:type="simple"/></inline-formula>. Finally, applying Lemma 7 yields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e515" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Lemma 9 (</bold><bold><italic>Useful linear algebra fact</italic></bold><bold>).</bold> <italic>If, for any </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e516" xlink:type="simple"/></inline-formula><italic>, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e517" xlink:type="simple"/></inline-formula><italic>, and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e518" xlink:type="simple"/></inline-formula><italic>, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e519" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e520" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e521" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Proposition 10.</bold> <italic>(</italic>Equivalent conditions used in proving the noise-free coding Theorem 3<italic>)</italic>.</p>
<p><italic>Along a certain direction </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e522" xlink:type="simple"/></inline-formula><italic>, the following conditions are equivalent.</italic><disp-formula id="pcbi.1003469.e523"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e523" xlink:type="simple"/><label>(27)</label></disp-formula><italic>The same also holds for </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e524" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e525" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof for I</italic><sub>OLE</sub>. “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e526" xlink:type="simple"/></inline-formula>”:</p>
<p>We again consider parametrized deviations from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e527" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e528" xlink:type="simple"/></inline-formula> for some constant matrix B. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e529" xlink:type="simple"/></inline-formula>, and recall (<xref ref-type="disp-formula" rid="pcbi.1003469.e418">Eq. (22)</xref>),<disp-formula id="pcbi.1003469.e530"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e530" xlink:type="simple"/><label>(28)</label></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e531" xlink:type="simple"/></inline-formula> is positive definite, according to the remark after Lemma 7, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e532" xlink:type="simple"/></inline-formula>.</p>
<p>“<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e533" xlink:type="simple"/></inline-formula>)”: If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e534" xlink:type="simple"/></inline-formula>, by Lemma 9, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e535" xlink:type="simple"/></inline-formula>. We have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e536" xlink:type="simple"/></inline-formula>, for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e537" xlink:type="simple"/></inline-formula> in the allowed region, and hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e538" xlink:type="simple"/></inline-formula>.</p>
<p>“<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e539" xlink:type="simple"/></inline-formula>”: immediate.</p>
<p>This concludes the proof for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e540" xlink:type="simple"/></inline-formula>.</p>
<p><italic>Proof for I</italic><sub>F,lin</sub>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e541" xlink:type="simple"/></inline-formula>, we further assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e542" xlink:type="simple"/></inline-formula> to avoid infinite information. Identical arguments will prove the properties above, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e543" xlink:type="simple"/></inline-formula> is replaced by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e544" xlink:type="simple"/></inline-formula>.</p>
<p><italic>Proof for I</italic><sub>mut, G</sub>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e545" xlink:type="simple"/></inline-formula>, we similarly assume <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e546" xlink:type="simple"/></inline-formula> (as defined in the proof of Theorem 2). Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e547" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e548" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003469.e549"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e549" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e550"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e550" xlink:type="simple"/></disp-formula>It is easy to see <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e551" xlink:type="simple"/></inline-formula>. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e552" xlink:type="simple"/></inline-formula> holds, using Lemma 7, each of the two terms must be 0. In particular, as we discussed in the proof of Theorem 2 for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e553" xlink:type="simple"/></inline-formula> (above), each of the terms is non-negative. Thus, if their sum is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e554" xlink:type="simple"/></inline-formula>, then each term must individually be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e555" xlink:type="simple"/></inline-formula>. According to the remark after Lemma 7, the second term being 0 indicates that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e556" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e557" xlink:type="simple"/></inline-formula>, which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e558" xlink:type="simple"/></inline-formula>.</p>
<p>If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e559" xlink:type="simple"/></inline-formula> holds, by Lemma 9, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e560" xlink:type="simple"/></inline-formula>. We have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e561" xlink:type="simple"/></inline-formula>, for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e562" xlink:type="simple"/></inline-formula> in the allowed region, and hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e563" xlink:type="simple"/></inline-formula>. Similarly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e564" xlink:type="simple"/></inline-formula>. This proves the property for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e565" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Theorem 3.</bold> <italic>A covariance matrix C<sup>n</sup> attains the noise-free bound for OLE information (and hence is optimal), if and only if C<sup>n</sup>A = C<sup>n</sup>(C<sup>μ</sup>)</italic><sup>−1</sup><italic>L = 0. Here L is the cross-covariance between the stimuli responses (</italic><xref ref-type="disp-formula" rid="pcbi.1003469.e272"><italic>Eq. (11)</italic></xref><italic>), C<sup>μ</sup> is the covariance of the mean response (</italic><xref ref-type="disp-formula" rid="pcbi.1003469.e263"><italic>Eq. (10)</italic></xref><italic>), and A is the linear readout vector for OLE, which is the same as in the noise-free case — that is, A = (C<sup>n</sup>+C<sup>μ</sup>)</italic><sup>−1</sup><italic>L = (C<sup>μ</sup>)</italic><sup>−1</sup><italic>L — when the condition is satisfied.</italic></p>
<p><italic>Proof.</italic> If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e566" xlink:type="simple"/></inline-formula>, then Lemma 9 implies that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e567" xlink:type="simple"/></inline-formula>, which means that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e568" xlink:type="simple"/></inline-formula>, using the definition in <xref ref-type="disp-formula" rid="pcbi.1003469.e279">Eq. (12)</xref>.</p>
<p>For the other direction of the theorem, consider a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e569" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e570" xlink:type="simple"/></inline-formula>, whose values at the endpoints are equal, according to saturation of the information bound. The mean value theorem assures that there exists a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e571" xlink:type="simple"/></inline-formula> such that<disp-formula id="pcbi.1003469.e572"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e572" xlink:type="simple"/><label>(29)</label></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e573" xlink:type="simple"/></inline-formula> is positive semidefinite, according to Lemma 7, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e574" xlink:type="simple"/></inline-formula>. Now using Lemma 9, we have that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e575" xlink:type="simple"/></inline-formula>, and the readout vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e576" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4f">
<title>Proof of Theorem 4: Conditions on tuning curves and variance, under which noise-free coding performance is possible</title>
<p>Next, we will restate, and then prove, Theorem 4. The proof will require using geometric ideas in Lemma 11, which we will state and prove below.</p>
<p><bold>Theorem 4.</bold> <italic>For scalar stimulus, let q<sub>i</sub> = </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e577" xlink:type="simple"/></inline-formula><italic>, i = 1⋅⋅⋅N, where A = (C<sup>μ</sup>)</italic><sup>−1</sup><italic>L is the readout vector for OLE in the noise-free case. Noise correlations may be chosen so that coding performance matches that which could be achieved in the absence of noise if and only if</italic><disp-formula id="pcbi.1003469.e578"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e578" xlink:type="simple"/><label>(1)</label></disp-formula><italic>When “&lt;” is satisfied, all optimal correlations attaining the maximum form a </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e579" xlink:type="simple"/></inline-formula><italic> dimensional convex set on the boundary of the spectrahedron. When “ = ” is attained, the dimension of that set is </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e580" xlink:type="simple"/></inline-formula><italic>, where N<sub>0</sub> is the number of zeros in {q<sub>i</sub>}.</italic></p>
<p>The proof is based on the condition in Theorem 3. After taking several invertible transforms of the equation, the problem of finding a noise-canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e581" xlink:type="simple"/></inline-formula> is transformed to that of finding a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e582" xlink:type="simple"/></inline-formula> vectors, whose length are specified by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e583" xlink:type="simple"/></inline-formula>, that sum to zero (the vectors form a closed loop when connected consecutively). This allows us to take a geometrical point of view, in which inequality <xref ref-type="disp-formula" rid="pcbi.1003469.e578">Eq. (1)</xref> becomes the triangle inequality. This will prove the “necessary” part of the Theorem. Lemma 0.5 shows the opposite direction, by inductively constructing the set of vectors that sum to zero.</p>
<p>This procedure will yield one “particular” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e584" xlink:type="simple"/></inline-formula> with the noise-canceling property. Very much like finding all general solutions of an ODE, we then add to our particular solution an arbitrary homogeneous solution, which belongs to a vector space of dimension <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e585" xlink:type="simple"/></inline-formula>. In order for our perturbed solution, at least for small enough perturbations, to still be positive semidefinite, the particular <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e586" xlink:type="simple"/></inline-formula> we start with must be generic. In other words, it must satisfy a rank condition, which is guaranteed by the construction in Lemma 11. We can then conclude that the set of all noise canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e587" xlink:type="simple"/></inline-formula> forms a linear segment with the dimension of the space of homogeneous solutions.</p>
<p>Finally, special treatments are given for the cases of “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e588" xlink:type="simple"/></inline-formula>” in <xref ref-type="disp-formula" rid="pcbi.1003469.e578">Eq. (1)</xref>, as well as cases where some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e589" xlink:type="simple"/></inline-formula> are 0.</p>
<p><italic>Proof.</italic> To establish the necessity direction of the Theorem, first let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e590" xlink:type="simple"/></inline-formula> be a diagonal matrix with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e591" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e592" xlink:type="simple"/></inline-formula>, where vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e593" xlink:type="simple"/></inline-formula>. Note that<disp-formula id="pcbi.1003469.e594"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e594" xlink:type="simple"/><label>(30)</label></disp-formula>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e595" xlink:type="simple"/></inline-formula>, a positive semidefinite matrix with diagonal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e596" xlink:type="simple"/></inline-formula>.</p>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e597" xlink:type="simple"/></inline-formula> can be diagonalized by an orthogonal matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e598" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e599" xlink:type="simple"/></inline-formula>. Without loss of generality, further assume that the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e600" xlink:type="simple"/></inline-formula> diagonal elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e601" xlink:type="simple"/></inline-formula> are positive, with the rest being 0, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e602" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e603" xlink:type="simple"/></inline-formula> be the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e604" xlink:type="simple"/></inline-formula> block of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e605" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e606" xlink:type="simple"/></inline-formula> be the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e607" xlink:type="simple"/></inline-formula> rows of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e608" xlink:type="simple"/></inline-formula>. Then we have<disp-formula id="pcbi.1003469.e609"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e609" xlink:type="simple"/><label>(31)</label></disp-formula>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e610" xlink:type="simple"/></inline-formula>, a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e611" xlink:type="simple"/></inline-formula> matrix, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e612" xlink:type="simple"/></inline-formula> be the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e613" xlink:type="simple"/></inline-formula>-th column. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e614" xlink:type="simple"/></inline-formula>, the 2-norm of vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e615" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e616" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e617" xlink:type="simple"/></inline-formula> be the maximum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e618" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003469.e619"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e619" xlink:type="simple"/><label>(32)</label></disp-formula>This concludes the necessary direction of our proof.</p>
<p>To establish sufficiency, we first focus on the case of “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e620" xlink:type="simple"/></inline-formula>” and all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e621" xlink:type="simple"/></inline-formula>. We will construct a generic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e622" xlink:type="simple"/></inline-formula> that has rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e623" xlink:type="simple"/></inline-formula>, satisfying <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e624" xlink:type="simple"/></inline-formula>. We will basically reverse the direction of arguments in <xref ref-type="disp-formula" rid="pcbi.1003469.e594">Eq. (30</xref>–<xref ref-type="disp-formula" rid="pcbi.1003469.e619">32)</xref>. We will later deal with the “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e625" xlink:type="simple"/></inline-formula>” case, and the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e626" xlink:type="simple"/></inline-formula> for some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e627" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Lemma 11</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e628" xlink:type="simple"/></inline-formula><italic>, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e629" xlink:type="simple"/></inline-formula><italic> be an orthonormal basis of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e630" xlink:type="simple"/></inline-formula><italic>. Given a set of positive </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e631" xlink:type="simple"/></inline-formula><italic> satisfying “</italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e632" xlink:type="simple"/></inline-formula><italic>” in </italic><xref ref-type="disp-formula" rid="pcbi.1003469.e578"><italic>Eq. (1)</italic></xref><italic>, there exist </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e633" xlink:type="simple"/></inline-formula><italic> vectors </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e634" xlink:type="simple"/></inline-formula><italic>, such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e635" xlink:type="simple"/></inline-formula><italic>, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e636" xlink:type="simple"/></inline-formula><italic> and the spanned linear subspace </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e637" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> We prove this by induction. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e638" xlink:type="simple"/></inline-formula> has to be at least 3 for the inequality to hold. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e639" xlink:type="simple"/></inline-formula>, this is the case of a triangle. There is a (unique) triangle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e640" xlink:type="simple"/></inline-formula>, for which the length of the three sides <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e641" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e642" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e643" xlink:type="simple"/></inline-formula> are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e644" xlink:type="simple"/></inline-formula> respectively. The altitude from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e645" xlink:type="simple"/></inline-formula> intersects the line of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e646" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e647" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e648" xlink:type="simple"/></inline-formula> be the origin of the coordinate system, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e649" xlink:type="simple"/></inline-formula> being the x-axis and aligned with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e650" xlink:type="simple"/></inline-formula>, and the altitude <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e651" xlink:type="simple"/></inline-formula> being the y-axis aligned with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e652" xlink:type="simple"/></inline-formula>. From such a picture, it is easy to verify the following: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e653" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e654" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e655" xlink:type="simple"/></inline-formula> satisfies the lemma, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e656" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e657" xlink:type="simple"/></inline-formula> lies within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e658" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e659" xlink:type="simple"/></inline-formula> otherwise.</p>
<p>For the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e660" xlink:type="simple"/></inline-formula>, assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e661" xlink:type="simple"/></inline-formula> is the largest of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e662" xlink:type="simple"/></inline-formula>. Because of the inequality, there will always exist some non-negative real number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e663" xlink:type="simple"/></inline-formula> (not necessarily one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e664" xlink:type="simple"/></inline-formula>) such that<disp-formula id="pcbi.1003469.e665"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e665" xlink:type="simple"/><label>(33)</label></disp-formula>We can verify that the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e666" xlink:type="simple"/></inline-formula> satisfies the inequality as well. By the assumption of induction, there exist vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e667" xlink:type="simple"/></inline-formula> that span the space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e668" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e669" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e670" xlink:type="simple"/></inline-formula>.</p>
<p>Note the choice of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e671" xlink:type="simple"/></inline-formula> also guarantees that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e672" xlink:type="simple"/></inline-formula> can be the edge lengths of a triangle. Applying the result at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e673" xlink:type="simple"/></inline-formula>, the three sides <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e674" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e675" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e676" xlink:type="simple"/></inline-formula> correspond to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e677" xlink:type="simple"/></inline-formula> respectively. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e678" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e679" xlink:type="simple"/></inline-formula>. It is easy to verify that these <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e680" xlink:type="simple"/></inline-formula> satisfy the lemma.</p>
<p>Using the lemma, we have a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e681" xlink:type="simple"/></inline-formula>. Stacking them as column vectors gives a matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e682" xlink:type="simple"/></inline-formula>; moreover, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e683" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e684" xlink:type="simple"/></inline-formula>, which is positive semidefinite with diagonals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e685" xlink:type="simple"/></inline-formula>. It is easy to show that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e686" xlink:type="simple"/></inline-formula>, by comparing the null spaces of the matrices. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e687" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e688" xlink:type="simple"/></inline-formula> is defined as above. Then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e689" xlink:type="simple"/></inline-formula>.</p>
<p>Now consider the case where there are zeros in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e690" xlink:type="simple"/></inline-formula>. Assume that the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e691" xlink:type="simple"/></inline-formula> entries contain all of the the non-zero values. We apply the construction above for the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e692" xlink:type="simple"/></inline-formula> dimensions, and get a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e693" xlink:type="simple"/></inline-formula> matrix such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e694" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e695" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e696" xlink:type="simple"/></inline-formula> is part of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e697" xlink:type="simple"/></inline-formula> with the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e698" xlink:type="simple"/></inline-formula> elements. The following block diagonal matrix<disp-formula id="pcbi.1003469.e699"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e699" xlink:type="simple"/><label>(34)</label></disp-formula>satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e700" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e701" xlink:type="simple"/></inline-formula>.</p>
<p>We have shown that for the “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e702" xlink:type="simple"/></inline-formula>” case in the theorem, there is always a noise canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e703" xlink:type="simple"/></inline-formula>. Consider the direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e704" xlink:type="simple"/></inline-formula>, in which off-diagonal elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e705" xlink:type="simple"/></inline-formula> vary, while keeping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e706" xlink:type="simple"/></inline-formula> (temporarily ignoring the positive semidefinite constraint). The set of all such <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e707" xlink:type="simple"/></inline-formula> form a linear subspace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e708" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e709" xlink:type="simple"/></inline-formula>, determined by the linear system <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e710" xlink:type="simple"/></inline-formula>. Since there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e711" xlink:type="simple"/></inline-formula> equations, the dimension of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e712" xlink:type="simple"/></inline-formula> is at least <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e713" xlink:type="simple"/></inline-formula>.</p>
<p>In the “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e714" xlink:type="simple"/></inline-formula>” case, there must be at least 3 non-zero <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e715" xlink:type="simple"/></inline-formula> in order for the triangle inequality to be satisfied in <xref ref-type="disp-formula" rid="pcbi.1003469.e578">Eq. 1</xref>. We will choose these three <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e716" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e717" xlink:type="simple"/></inline-formula>. Consider a block of the coefficient matrix associated with the system <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e718" xlink:type="simple"/></inline-formula> (note that the entries of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e719" xlink:type="simple"/></inline-formula> are considered to be unknown variables), that are columns corresponding to variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e720" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003469.e721"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e721" xlink:type="simple"/><label>(35)</label></disp-formula></p>
<p>Performing Gaussian elimination on the columns of this matrix, we obtain the following matrix, which will have the same rank.<disp-formula id="pcbi.1003469.e722"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e722" xlink:type="simple"/><label>(36)</label></disp-formula>This matrix – which determines the number of constraints that must be satisfied in order for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e723" xlink:type="simple"/></inline-formula> – has rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e724" xlink:type="simple"/></inline-formula>, and hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e725" xlink:type="simple"/></inline-formula> is exactly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e726" xlink:type="simple"/></inline-formula>.</p>
<p>For any direction in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e727" xlink:type="simple"/></inline-formula>, we can always perturb the generic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e728" xlink:type="simple"/></inline-formula> we found above by some finite amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e729" xlink:type="simple"/></inline-formula>, and still have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e730" xlink:type="simple"/></inline-formula> be positive semidefinite. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e731" xlink:type="simple"/></inline-formula> be the smallest non-zero eigenvalue of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e732" xlink:type="simple"/></inline-formula>. Take any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e733" xlink:type="simple"/></inline-formula>. For any vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e734" xlink:type="simple"/></inline-formula>, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e735" xlink:type="simple"/></inline-formula> be an orthogonal decomposition where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e736" xlink:type="simple"/></inline-formula> is the projection along the direction of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e737" xlink:type="simple"/></inline-formula>. Then<disp-formula id="pcbi.1003469.e738"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e738" xlink:type="simple"/><label>(37)</label></disp-formula></p>
<p>This shows that the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e739" xlink:type="simple"/></inline-formula> are positive semidefinite and they form a set of dimension as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e740" xlink:type="simple"/></inline-formula>. We can always take the admissible <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e741" xlink:type="simple"/></inline-formula> values to their extremes, and the resulting matrices are all the possible noise canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e742" xlink:type="simple"/></inline-formula>. For any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e743" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e744" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e745" xlink:type="simple"/></inline-formula> must be in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e746" xlink:type="simple"/></inline-formula>. Note that the sets of positive semidefinite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e747" xlink:type="simple"/></inline-formula> (spectrahedra) are convex. As a consequence, any point along the segment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e748" xlink:type="simple"/></inline-formula> will be positive semidefinite. This shows we must have encompassed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e749" xlink:type="simple"/></inline-formula> when considering the largest possible perturbations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e750" xlink:type="simple"/></inline-formula>, in any direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e751" xlink:type="simple"/></inline-formula>. Moreover, we note that the set of all noise-canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e752" xlink:type="simple"/></inline-formula> is convex: if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e753" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e754" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e755" xlink:type="simple"/></inline-formula> for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e756" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e757" xlink:type="simple"/></inline-formula> is positive semidefinite, with the diagonal matching <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e758" xlink:type="simple"/></inline-formula>.</p>
<p>Thus, we have proved the claim about the dimension and convexity of the set of optimal correlations for the case of “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e759" xlink:type="simple"/></inline-formula>” in <xref ref-type="disp-formula" rid="pcbi.1003469.e578">Eq. (1)</xref>.</p>
<p>Finally, for the special case of “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e760" xlink:type="simple"/></inline-formula>” in <xref ref-type="disp-formula" rid="pcbi.1003469.e578">Eq. (1)</xref>, again first consider the case where all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e761" xlink:type="simple"/></inline-formula>. As before, solving <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e762" xlink:type="simple"/></inline-formula> is equivalent to solving <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e763" xlink:type="simple"/></inline-formula> and there is an one to one correspondence between the two. Revisiting <xref ref-type="disp-formula" rid="pcbi.1003469.e619">Eq. (32)</xref> in the proof above, the equality condition in the triangle inequality implies that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e764" xlink:type="simple"/></inline-formula> all point along the same direction, and that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e765" xlink:type="simple"/></inline-formula> is in the opposite direction, in order to cancel their sum. This fully determines <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e766" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e767" xlink:type="simple"/></inline-formula>, and<disp-formula id="pcbi.1003469.e768"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e768" xlink:type="simple"/><label>(38)</label></disp-formula>It is easy to verify that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e769" xlink:type="simple"/></inline-formula>, and hence there is a unique noise canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e770" xlink:type="simple"/></inline-formula>.</p>
<p>For the case when there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e771" xlink:type="simple"/></inline-formula> 0's among the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e772" xlink:type="simple"/></inline-formula>, assume that the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e773" xlink:type="simple"/></inline-formula> coordinates are non-zero, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e774" xlink:type="simple"/></inline-formula>. Next, we write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e775" xlink:type="simple"/></inline-formula> in block matrix form, with blocks of dimension <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e776" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e777" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003469.e778"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e778" xlink:type="simple"/><label>(39)</label></disp-formula>Applying the previous argument from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e779" xlink:type="simple"/></inline-formula> case, there is a unique <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e780" xlink:type="simple"/></inline-formula>. Moreover, note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e781" xlink:type="simple"/></inline-formula>, following from the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e782" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003469.e768">Eq. (38)</xref> has rank 1. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e783" xlink:type="simple"/></inline-formula> be the orthogonal diagonalization and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e784" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e785" xlink:type="simple"/></inline-formula> be the identity matrix of dimension <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e786" xlink:type="simple"/></inline-formula>. Then we can take an orthogonal transform:<disp-formula id="pcbi.1003469.e787"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e787" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e788"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e788" xlink:type="simple"/></disp-formula>With the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e789" xlink:type="simple"/></inline-formula>, the original problem <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e790" xlink:type="simple"/></inline-formula> is therefore equivalent to finding all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e791" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e792" xlink:type="simple"/></inline-formula> such that,<disp-formula id="pcbi.1003469.e793"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e793" xlink:type="simple"/><label>(40)</label></disp-formula>while keeping the matrix in this equation positive semidefinite.</p>
<p>For any positive semidefinite matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e794" xlink:type="simple"/></inline-formula>, it is easy to show that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e795" xlink:type="simple"/></inline-formula> by considering the principle minor with indices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e796" xlink:type="simple"/></inline-formula>, which must be non-negative. Note that since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e797" xlink:type="simple"/></inline-formula> has only one non-zero diagonal entry, this forces the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e798" xlink:type="simple"/></inline-formula> columns of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e799" xlink:type="simple"/></inline-formula> to be entirely 0. So we can rewrite the block matrix by dimension <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e800" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e801" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003469.e802"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e802" xlink:type="simple"/><label>(41)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e803" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e804" xlink:type="simple"/></inline-formula>-th column of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e805" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e806" xlink:type="simple"/></inline-formula>, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e807" xlink:type="simple"/></inline-formula>. It can be verified that, as long as the block structure of <xref ref-type="disp-formula" rid="pcbi.1003469.e802">Eq. (41)</xref> is satisfied, <xref ref-type="disp-formula" rid="pcbi.1003469.e793">Eq. (40)</xref> is always true. The positive semidefinite constraint becomes the constraint that the lower block be positive semidefinite; in turn, this corresponds to a spectrahedron (and hence a convex set) of dimension <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e808" xlink:type="simple"/></inline-formula>. Note that this dimensionality and convexity will be preserved when we undo the invertible linear transforms performed in prior steps to obtain the noise-canceling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e809" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4g">
<title>Proof of Theorem 5: Probability that noise-free coding is possible</title>
<p>In this subsection, we will restate, and then prove, Theorem 5.</p>
<p><bold>Theorem 5.</bold> If the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e810" xlink:type="simple"/></inline-formula> <italic>defined in Theorem 4 are independent and identically distributed (i.i.d.) as a random variable X on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e811" xlink:type="simple"/></inline-formula> then the probability</italic><disp-formula id="pcbi.1003469.e812"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e812" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p><italic>Proof.</italic> We will use the following fact to establish a lower bound for the probability of the event in the theorem (below, we denote this event as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e813" xlink:type="simple"/></inline-formula>).<disp-formula id="pcbi.1003469.e814"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e814" xlink:type="simple"/><label>(42)</label></disp-formula>We choose the two events <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e815" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e816" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e817" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e818" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e819" xlink:type="simple"/></inline-formula> implies C,<disp-formula id="pcbi.1003469.e820"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e820" xlink:type="simple"/><label>(43)</label></disp-formula>the event in concern. We will then show that, for large populations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e821" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e822" xlink:type="simple"/></inline-formula>, and thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e823" xlink:type="simple"/></inline-formula>.</p>
<p>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e824" xlink:type="simple"/></inline-formula>, by the law of large numbers, the average should converge to the expectation (which is a positive number), hence<disp-formula id="pcbi.1003469.e825"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e825" xlink:type="simple"/><label>(44)</label></disp-formula></p>
<p>We next consider event B. Let the cumulative distribution function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e826" xlink:type="simple"/></inline-formula> be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e827" xlink:type="simple"/></inline-formula>. Then cumulative distribution function for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e828" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e829" xlink:type="simple"/></inline-formula> by the assumption that these variables are drawn i.i.d. It follows that<disp-formula id="pcbi.1003469.e830"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e830" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e831"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e831" xlink:type="simple"/></disp-formula>Here, the first inequality is obtained via the lower bound of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e832" xlink:type="simple"/></inline-formula> over the interval of integration, and the second uses the fact <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e833" xlink:type="simple"/></inline-formula>.</p>
<p>As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e834" xlink:type="simple"/></inline-formula>, the last integral converges to 0 because of the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e835" xlink:type="simple"/></inline-formula>, together with the Lebesgue dominated convergence theorem. Hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e836" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e837" xlink:type="simple"/></inline-formula>.</p>
<p>Combining the limits of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e838" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e839" xlink:type="simple"/></inline-formula> using <xref ref-type="disp-formula" rid="pcbi.1003469.e814">Eq. (42)</xref>, together with the fact <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e840" xlink:type="simple"/></inline-formula>, we conclude that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e841" xlink:type="simple"/></inline-formula> must approach 1 as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e842" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4h">
<title>Proof of Proposition 6: Sensitivity to perturbations</title>
<p>Here, we will prove Proposition 6, which puts bounds on the condition numbers that define the sensitivity of our coding metrics to perturbations in noise correlations or the tuning curves. For our proof, we will require three different lemmas. We state and prove these, before moving on to Proposition 6.</p>
<p>Here, we will first consider the condition number for the case of a scalar stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e843" xlink:type="simple"/></inline-formula>, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e844" xlink:type="simple"/></inline-formula> is a vector. In the proof of the proposition, we show how to extend the results to the case of multivariate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e845" xlink:type="simple"/></inline-formula>. As we mentioned in Section “Sensitivity and robustness of the impact of correlations on encoded information”, the same proof works for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e846" xlink:type="simple"/></inline-formula> as well as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e847" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Lemma 12.</bold> <italic>For any submultiplicative matrix norm </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e848" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e849" xlink:type="simple"/></inline-formula><italic>,</italic><disp-formula id="pcbi.1003469.e850"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e850" xlink:type="simple"/><label>(45)</label></disp-formula></p>
<p><italic>Proof.</italic> Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e851" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e852" xlink:type="simple"/></inline-formula> exists and<disp-formula id="pcbi.1003469.e853"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e853" xlink:type="simple"/><label>(46)</label></disp-formula></p>
<p><bold>Lemma 13.</bold> <italic>For any positive definite matrix </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e854" xlink:type="simple"/></inline-formula><italic>, vectors </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e855" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e856" xlink:type="simple"/></inline-formula><italic> such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e857" xlink:type="simple"/></inline-formula><italic>,</italic><disp-formula id="pcbi.1003469.e858"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e858" xlink:type="simple"/><label>(47)</label></disp-formula></p>
<p><italic>Proof.</italic><disp-formula id="pcbi.1003469.e859"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e859" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e860"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e860" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e861"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e861" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e862"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e862" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e863"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e863" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e864"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e864" xlink:type="simple"/></disp-formula>Here, we have used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e865" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e866" xlink:type="simple"/></inline-formula>, and the assumed condition in the last line.</p>
<p><bold>Lemma 14.</bold> <italic>For any positive definite matrix </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e867" xlink:type="simple"/></inline-formula><italic>, vector </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e868" xlink:type="simple"/></inline-formula><italic> and matrix </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e869" xlink:type="simple"/></inline-formula><italic> where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e870" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003469.e871"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e871" xlink:type="simple"/><label>(48)</label></disp-formula></p>
<p><italic>Proof.</italic><disp-formula id="pcbi.1003469.e872"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e872" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e873"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e873" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e874"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e874" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e875"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e875" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e876"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e876" xlink:type="simple"/></disp-formula>Here we have used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e877" xlink:type="simple"/></inline-formula>. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e878" xlink:type="simple"/></inline-formula>, we apply Lemma 12 is applied to obtain the last line.</p>
<p><bold>Proposition 6.</bold> <italic>The local condition number of I<sub>F;lin</sub> under perturbations of C<sup>n</sup> (where magnitude is quantified by 2-norm) is bounded by</italic><disp-formula id="pcbi.1003469.e879"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e879" xlink:type="simple"/><label>(3)</label></disp-formula><italic>where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e880" xlink:type="simple"/></inline-formula><italic><sub>max</sub> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e881" xlink:type="simple"/></inline-formula><italic><sub>min</sub> are the largest and smallest eigenvalue of C<sup>n</sup> respectively. Here </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e882" xlink:type="simple"/></inline-formula><italic> is the condition number with respect to the 2-norm, as defined in the above equation.</italic></p>
<p><italic>Similarly, the condition number for perturbing of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e883" xlink:type="simple"/></inline-formula><italic> is bounded by</italic><disp-formula id="pcbi.1003469.e884"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e884" xlink:type="simple"/><label>(4)</label></disp-formula><italic>where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e885" xlink:type="simple"/></inline-formula><italic> is the i-th column of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e886" xlink:type="simple"/></inline-formula><italic> and assume </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e887" xlink:type="simple"/></inline-formula><italic> for all i. Here K is the dimension of the stimulus s.</italic></p>
<p><italic>Proof.</italic> Note that<disp-formula id="pcbi.1003469.e888"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e888" xlink:type="simple"/><label>(49)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e889" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e890" xlink:type="simple"/></inline-formula>-th unit vector (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e891" xlink:type="simple"/></inline-formula>). Since the bound in Lemma 14 does not depend on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e892" xlink:type="simple"/></inline-formula>, we apply the Lemma for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e893" xlink:type="simple"/></inline-formula> and each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e894" xlink:type="simple"/></inline-formula> respectively. For any perturbation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e895" xlink:type="simple"/></inline-formula> satisfying <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e896" xlink:type="simple"/></inline-formula>, we have<disp-formula id="pcbi.1003469.e897"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e897" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e898"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e898" xlink:type="simple"/></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e899" xlink:type="simple"/></inline-formula>. We then note that for positive semidefinite matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e900" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e901" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e902" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e903" xlink:type="simple"/></inline-formula> are the smallest and largest eigenvalues of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e904" xlink:type="simple"/></inline-formula>. This proves the bound on the condition number for perturbing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e905" xlink:type="simple"/></inline-formula>.</p>
<p>Similarly, for a perturbation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e906" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e907" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e908" xlink:type="simple"/></inline-formula>. This guarantees that<disp-formula id="pcbi.1003469.e909"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e909" xlink:type="simple"/><label>(50)</label></disp-formula>Applying Lemma 13 for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e910" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e911" xlink:type="simple"/></inline-formula>, we have<disp-formula id="pcbi.1003469.e912"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e912" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e913"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e913" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e914"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e914" xlink:type="simple"/></disp-formula>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e915" xlink:type="simple"/></inline-formula> is the Frobenius norm and we have used the fact for any matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e916" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e917" xlink:type="simple"/></inline-formula>. The last inequality follows from the definition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e918" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4i">
<title>Details for numerical examples and simulation</title>
<p>Here, we describe the parameters of our numerical models, and the numerical methods we used.</p>
<sec id="s4i1">
<title>Parameters for <xref ref-type="fig" rid="pcbi-1003469-g001">Fig. 1</xref>, <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref> and <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref></title>
<p>All parameters we use are dimensionless, unless stated otherwise.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003469-g001">Fig. 1</xref>, the mean response for the three neurons under stimulus 1 (red) and 2 (blue) is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e919" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e920" xlink:type="simple"/></inline-formula> respectively:<disp-formula id="pcbi.1003469.e921"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e921" xlink:type="simple"/><label>(51)</label></disp-formula>For each case of correlation structure (i.e., for each row in <xref ref-type="fig" rid="pcbi-1003469-g001">Figure 1</xref>), the noise covariance matrix is the same for the two stimuli, and all neuron variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e922" xlink:type="simple"/></inline-formula>. In detail:<disp-formula id="pcbi.1003469.e923"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e923" xlink:type="simple"/><label>(52)</label></disp-formula><disp-formula id="pcbi.1003469.e924"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e924" xlink:type="simple"/><label>(53)</label></disp-formula></p>
<p>The confidence circles and spheres are calculated based on a Gaussian assumption for the response distributions.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003469-g002">Fig. 2</xref>, the noise variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e925" xlink:type="simple"/></inline-formula> are all set to 1. Additionally,<disp-formula id="pcbi.1003469.e926"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e926" xlink:type="simple"/><label>(54)</label></disp-formula></p>
<p>In <xref ref-type="fig" rid="pcbi-1003469-g003">Fig. 3</xref>, the noise variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e927" xlink:type="simple"/></inline-formula> are all set to 1. In panel <bold>A</bold><disp-formula id="pcbi.1003469.e928"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e928" xlink:type="simple"/><label>(55)</label></disp-formula>For panel <bold>B</bold><disp-formula id="pcbi.1003469.e929"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e929" xlink:type="simple"/><label>(56)</label></disp-formula></p>
</sec><sec id="s4i2">
<title>Heterogeneous tuning curves</title>
<p>For the results in Section “Heterogeneously tuned neural populations”, we use the same model and parameters as in <xref ref-type="bibr" rid="pcbi.1003469-Ecker1">[8]</xref> to set up a heterogeneous population with tuning curves of random amplitude and width. For completeness, we include the details of this setup as follows:</p>
<p>The shape of each tuning curve (specifying firing rates) is modeled by a von Mises distribution. This an analog of the Gaussian distribution over the unit circle:<disp-formula id="pcbi.1003469.e930"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e930" xlink:type="simple"/><label>(57)</label></disp-formula>The parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e931" xlink:type="simple"/></inline-formula> respectively control the magnitude, width and preferred direction for each neuron. We set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e932" xlink:type="simple"/></inline-formula> to be equally spaced along <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e933" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e934" xlink:type="simple"/></inline-formula>. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e935" xlink:type="simple"/></inline-formula> are independently chosen from a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e936" xlink:type="simple"/></inline-formula>-square distribution with 3 degrees of freedom, scaled to a mean of 19. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e937" xlink:type="simple"/></inline-formula> is similarly drawn from a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e938" xlink:type="simple"/></inline-formula>-normal distribution with parameters giving mean 2 and standard deviation 2 (for the underlying normal distribution).</p>
<p>We assume Poisson firing variability, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e939" xlink:type="simple"/></inline-formula>, and use a spike-count window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e940" xlink:type="simple"/></inline-formula> ms in <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4</xref> and <xref ref-type="fig" rid="pcbi-1003469-g005">5</xref>.</p>
</sec><sec id="s4i3">
<title>Equivalence between penalty functions and constrained optimizations</title>
<p>In this section we note a standard fact about implementing constrained optimization with penalty functions — i.e., the method of Lagrange multipliers.</p>
<p>Consider an optimization problem: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e941" xlink:type="simple"/></inline-formula>. Now add a penalty term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e942" xlink:type="simple"/></inline-formula> with constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e943" xlink:type="simple"/></inline-formula> and consider the new optimization problem: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e944" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e945" xlink:type="simple"/></inline-formula> is one of the solutions to this new optimization problem, then it is also an optimal solution to the constraint optimization problem <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e946" xlink:type="simple"/></inline-formula>.</p>
<p>To show this, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e947" xlink:type="simple"/></inline-formula> be any point that satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e948" xlink:type="simple"/></inline-formula>. Further, note <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e949" xlink:type="simple"/></inline-formula> is also the solution to the problem of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e950" xlink:type="simple"/></inline-formula>, since we simply add a constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e951" xlink:type="simple"/></inline-formula>. Therefore,<disp-formula id="pcbi.1003469.e952"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e952" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003469.e953"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e953" xlink:type="simple"/></disp-formula></p>
<p>As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e954" xlink:type="simple"/></inline-formula> also satisfies the constraint, we conclude that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e955" xlink:type="simple"/></inline-formula> is an optimal solution to the constrained optimization problem.</p>
<p>We use this fact to find the information-maximizing noise correlations, with the restriction that the noise correlations by small in magnitude. For a given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e956" xlink:type="simple"/></inline-formula>, we perform the optimization <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e957" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e958" xlink:type="simple"/></inline-formula> in this case is one of our information measures, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e959" xlink:type="simple"/></inline-formula> refers to the off-diagonal elements of the covariance matrix, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e960" xlink:type="simple"/></inline-formula> is the measure of the correlation strength as in <xref ref-type="disp-formula" rid="pcbi.1003469.e965">Eq. (58)</xref>. Thanks to the above result, we can be assured that the resulting covariance matrix (described by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e961" xlink:type="simple"/></inline-formula>) will be the one that maximizes the information for a particular strength of correlations. By varying <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e962" xlink:type="simple"/></inline-formula> (or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e963" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003469.e965">Eq. (58)</xref>), we can thus parametrically explore how the optimal correlation structures change as one allows either larger, or smaller, correlations in the system.</p>
</sec><sec id="s4i4">
<title>Penalty function</title>
<p>In Section “Heterogeneously tuned neural populations”, our aim is to plot optimized noise correlations at various levels of the correlation strength, as quantified by the Euclidean norm. This constrained optimization problem can be achieved, as shown in the previous section, by adding a term to the information that penalizes the Euclidean norm — that is, a constant times the sum-of-squares of correlations. This is precisely the procedure that we follow, ranging over a number of different values of the constant to produce the plot of <xref ref-type="fig" rid="pcbi-1003469-g004">Fig. 4</xref>.</p>
<p>In more detail, we choose these different values of the constant as follows. To force the correlations towards a fixed strength of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e964" xlink:type="simple"/></inline-formula>, we optimize a modified objective function with an additional term:<disp-formula id="pcbi.1003469.e965"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e965" xlink:type="simple"/><label>(58)</label></disp-formula>As will become clear, the term before the sum is a constant with respect to the terms being optimized; from one optimization to the next, we adjust the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e966" xlink:type="simple"/></inline-formula> in this term. Here the variance terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e967" xlink:type="simple"/></inline-formula> are constants to scale <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e968" xlink:type="simple"/></inline-formula> properly as correlation coefficients. Also, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e969" xlink:type="simple"/></inline-formula> is the gradient vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e970" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e971" xlink:type="simple"/></inline-formula> (the diagonal matrix corresponding independent noise) with respect to off-diagonal entries of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e972" xlink:type="simple"/></inline-formula> (see the remarks after the proof of Theorem 1). <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e973" xlink:type="simple"/></inline-formula> means the entry-wise product of the two vectors (of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e974" xlink:type="simple"/></inline-formula> indexed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e975" xlink:type="simple"/></inline-formula>). Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e976" xlink:type="simple"/></inline-formula> is the ordinary vector 2-norm.</p>
<p>To understand this choice of the constant in (58), note that the new optimal correlations with the penalty can be characterized by setting the gradient of the total objective function to 0. In a small neighborhood of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e977" xlink:type="simple"/></inline-formula>, the gradient of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e978" xlink:type="simple"/></inline-formula> is close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e979" xlink:type="simple"/></inline-formula>. With these substitutions, the equation for the gradient of the total objective function yields approximately:<disp-formula id="pcbi.1003469.e980"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003469.e980" xlink:type="simple"/><label>(59)</label></disp-formula>where we took an entry-wise product with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e981" xlink:type="simple"/></inline-formula> and rearranged terms to obtain the final equality. The final equality implies that the (vector) 2-norm of noise correlations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e982" xlink:type="simple"/></inline-formula> (i.e., the Euclidean norm) is approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e983" xlink:type="simple"/></inline-formula>. This is what we set out to achieve with the additional term in the objective function.</p>
</sec><sec id="s4i5">
<title>Rescaling signal correlation</title>
<p>In <xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5 <bold>DEF</bold></xref>, we make scatter plots comparing noise correlations with the rescaled signal correlations. Here, we explain how and why this rescaling was done.</p>
<p>First, we note that the rescaling is done by multiplying each signal correlation by a positive weight. This will not change its sign, the property associated with the sign rule (<xref ref-type="fig" rid="pcbi-1003469-g005">Fig. 5 <bold>ABC</bold></xref>).</p>
<p>Next, recall that in deriving the sign rule (<xref ref-type="disp-formula" rid="pcbi.1003469.e362">Eq. (20)</xref>), we calculated the gradient of the information with respect to noise correlations. One should expect alignment between this gradient and the optimal correlations when their magnitudes are small. In other words, if we make a scatter plot with dots whose y and x coordinates are entries of the gradient and noise correlation vectors, respectively (so that the number of dots is the length of these vectors), we expect to see that a straight line will pass through all the dots.</p>
<p>We next note that the entries of the gradient vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e984" xlink:type="simple"/></inline-formula> are not exactly the normalized signal correlations (see <xref ref-type="disp-formula" rid="pcbi.1003469.e980">Eq. (59)</xref>). Instead, this vector has additional “weight factors” that differ for each entry (neuron pair), and hence for each dot in the scatter plot. Thus, to reveal a linear relationship between signal and noise correlations in a scatter plot, we must scale each signal correlation with a proper (positive) weight, determined below, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e985" xlink:type="simple"/></inline-formula>. We then redo the scatter plots with these new values on the horizontal axis. As we will see, the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e986" xlink:type="simple"/></inline-formula> (defined below) do not depend on the noise correlations.</p>
<p>We now determine <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e987" xlink:type="simple"/></inline-formula>. Recall that our goal is to define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e988" xlink:type="simple"/></inline-formula> such that, when it is used to rescale signal correlations as above, we will see a linear alignment between signal and noise correlations. In other words, if we choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e989" xlink:type="simple"/></inline-formula> correctly, we will have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e990" xlink:type="simple"/></inline-formula> (for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e991" xlink:type="simple"/></inline-formula>). Comparing the formulae for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e992" xlink:type="simple"/></inline-formula> (from the remarks after the proof of Theorem 1) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e993" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003469.e335">Eq. (17)</xref>), we see that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e994" xlink:type="simple"/></inline-formula> satisfies this (with constant<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e995" xlink:type="simple"/></inline-formula>). Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003469.e996" xlink:type="simple"/></inline-formula>.</p>
</sec></sec></sec></body>
<back>
<ack>
<p>This work was inspired by an ongoing collaboration with Fred Rieke and his colleagues on retinal coding, which suggested the importance of “mapping” the full space of possible signal and noise correlations. We gratefully acknowledge the ideas and insights of these scientists. We further wish to thank Andrea Barreiro, Fred Rieke, Kresimir Josic and Xaq Pitkow for helpful comments on this manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003469-Mastronarde1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mastronarde</surname><given-names>D</given-names></name> (<year>1983</year>) <article-title>Correlated firing of cat retinal ganglion cells. I. Spontaneously active inputs to X- and Y- cells</article-title>. <source>Journal of Neurophysiology</source> <volume>49</volume>: <fpage>303</fpage>–<lpage>324</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Alonso1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alonso</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Usrey</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Reid</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>Precisely correlated firing of cells in the lateral geniculate nucleus</article-title>. <source>Nature</source> <volume>383</volume>: <fpage>815</fpage>–<lpage>819</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Cohen1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Kohn</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Measuring and interpreting neuronal correlations</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>811</fpage>–<lpage>819</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Gawne1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gawne</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Richmond</surname><given-names>B</given-names></name> (<year>1993</year>) <article-title>How independent are the messages carried by adjacent inferior temporal cortical neurons</article-title>? <source>Journal of Neuroscience</source> <volume>13</volume>: <fpage>2758</fpage>–<lpage>2771</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Averbeck1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Averbeck</surname><given-names>BB</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Neural correlations, population coding and computation</article-title>. <source>Nature Reviews Neuroscience</source> <volume>7</volume>: <fpage>358</fpage>–<lpage>366</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Zohary1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zohary</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name> (<year>1994</year>) <article-title>Correlated neuronal discharge rate and its implications for psychophysical performance</article-title>. <source>Nature</source> <volume>370</volume>: <fpage>140</fpage>–<lpage>143</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Abbott1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>1999</year>) <article-title>The effect of correlated variability on the accuracy of a population code</article-title>. <source>Neural Computation</source> <volume>11</volume>: <fpage>91</fpage>–<lpage>101</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Ecker1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ecker</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tolias</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>The Effect of Noise Correlations in Populations of Diversely Tuned Neurons</article-title>. <source>Journal of Neuroscience</source> <volume>31</volume>: <fpage>14272</fpage>–<lpage>14283</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Shamir1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamir</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Implications of neuronal diversity on population coding</article-title>. <source>Neural Comput</source> <volume>18</volume>: <fpage>1951</fpage>–<lpage>1986</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Sompolinsky1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Yoon</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shamir</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>Population coding in neuronal systems with correlated noise</article-title>. <source>Physical Review E</source> <volume>64</volume>: <fpage>051904</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Averbeck2"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Averbeck</surname><given-names>BB</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Effects of noise correlations on information encoding and decoding</article-title>. <source>Journal of Neurophysiology</source> <volume>95</volume>: <fpage>3633</fpage>–<lpage>3644</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Latham1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Latham</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Role of correlations in population coding</article-title>. <source>arXiv preprint arXiv</source> <fpage>11096524</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Romo1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Romo</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hernandez</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zainos</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Salinas</surname><given-names>E</given-names></name> (<year>2003</year>) <article-title>Correlated neuronal discharges that increase coding efficiency during perceptual discrimination</article-title>. <source>Neuron</source> <volume>38</volume>: <fpage>649</fpage>–<lpage>657</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-daSilveira1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>da Silveira</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2013</year>) <article-title>High-Fidelity Coding with Correlated Neurons</article-title>. <source>arXivorg</source></mixed-citation>
</ref>
<ref id="pcbi.1003469-Wilke1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilke</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Eurich</surname><given-names>CW</given-names></name> (<year>2002</year>) <article-title>Representational accuracy of stochastic neural populations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>155</fpage>–<lpage>189</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Josi1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Josić</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Doiron</surname><given-names>B</given-names></name>, <name name-style="western"><surname>de la Rocha</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Stimulus-dependent correlations and population codes</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>2774</fpage>–<lpage>2804</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Tkacik1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkacik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Prentice</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2010</year>) <article-title>Optimal population coding by noisy spiking neurons</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>107</volume>: <fpage>14419</fpage>–<lpage>14424</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Salinas1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salinas</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name> (<year>1994</year>) <article-title>Vector reconstruction from firing rates</article-title>. <source>Journal of Computational Neuroscience</source> <volume>1</volume>: <fpage>89</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Kohn1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohn</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Stimulus Dependence of Neuronal Correlation in Primary Visual Cortex of the Macaque</article-title>. <source>Journal of Neuroscience</source> <volume>25</volume>: <fpage>3661</fpage>–<lpage>3673</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Softky1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Softky</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>1993</year>) <article-title>The highly irregular firing of cortical cells is incosistent with temporal integration of random epsp's</article-title>. <source>Journal of Neuroscience</source> <volume>13</volume>: <fpage>334</fpage>–<lpage>350</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Britten1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Britten</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Movshon</surname><given-names>J</given-names></name> (<year>1993</year>) <article-title>Responses of neurons in macaque MT to stochastic motion signals</article-title>. <source>Visual Neurosci</source> <volume>10</volume>: <fpage>1157</fpage>–<lpage>1169</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-delaRocha1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de la Rocha</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Doiron</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Josić</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Reyes</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Correlation between neural spike trains increases with firing rate</article-title>. <source>Nature</source> <volume>448</volume>: <fpage>802</fpage>–<lpage>806</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Binder1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Binder</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Powers</surname><given-names>R</given-names></name> (<year>2001</year>) <article-title>Relationship between Simulated Common Synaptic Input and Discharge Synchrony in Cat Spinal Motoneurons</article-title>. <source>J Neurophysiol</source> <volume>86</volume>: <fpage>2266</fpage>–<lpage>2275</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Hansen1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Chelaru</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dragoi</surname><given-names>V</given-names></name> (<year>2012</year>) <article-title>Correlated variability in laminar cortical circuits</article-title>. <source>Neuron</source> <volume>76</volume>: <fpage>590</fpage>–<lpage>602</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Cover1"><label>25</label>
<mixed-citation publication-type="book" xlink:type="simple">Cover TM, Thomas JA (2006) Elements of Information Theory. John Wiley &amp; Sons.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Beck1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kanitscheider</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pitkow</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name> (<year>2013</year>) <article-title>The perils of inferring information from correlations</article-title>. <source>Cosyne Abstracts 2013 Salt Late City USA</source></mixed-citation>
</ref>
<ref id="pcbi.1003469-Beck2"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>Pitkow</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>Not noisy, just wrong: the role of suboptimal inference in behavioral variability</article-title>. <source>Neuron</source> <volume>74</volume>: <fpage>30</fpage>–<lpage>39</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Shamir2"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamir</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Nonlinear population codes</article-title>. <source>Neural Computation</source> <volume>16</volume>: <fpage>1105</fpage>–<lpage>1136</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Koch1"><label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">Koch C (1999) Biophysics of Computation. Oxford University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Ganmor1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganmor</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>108</volume>: <fpage>9679</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Ohiorhenuan1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ohiorhenuan</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Mechler</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Purpura</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Schmid</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>Q</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Sparse coding and high-order correlations in fine-scale cortical network</article-title>. <source>Nature</source> <volume>466</volume>: <fpage>617</fpage>–<lpage>621</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Zylberberg1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zylberberg</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname><given-names>E</given-names></name> (<year>2012</year>) <article-title>Input nonlinearities shape beyond-pairwise correlations and improve information transmission by neural populations</article-title>. <source>arXiv preprint arXiv</source> <fpage>12123549</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Prinz1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prinz</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bucher</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Marder</surname><given-names>E</given-names></name> (<year>2004</year>) <article-title>Similar network activity from disparate circuit parameters</article-title>. <source>Nature Neuroscience</source> <volume>7</volume>: <fpage>1354</fpage>–<lpage>1352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Marder1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marder</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>Variability, compensation, and modulation in neurons and circuits</article-title>. <source>Proceedings of the National Academy of Sciences USA</source> <volume>108</volume>: <fpage>15542</fpage>–<lpage>15548</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003469-Stevenson1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevenson</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Kording</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>How advances in neural recording affect data analysis</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>139</fpage>–<lpage>142</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>