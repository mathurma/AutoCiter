<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00293</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006897</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Aspect ratio</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject><subj-group><subject>Old World monkeys</subject><subj-group><subject>Macaque</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title>
<alt-title alt-title-type="running-head">V1 system identification with CNNs</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7508-4443</contrib-id>
<name name-style="western">
<surname>Cadena</surname> <given-names>Santiago A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3699-3828</contrib-id>
<name name-style="western">
<surname>Denfield</surname> <given-names>George H.</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0057-957X</contrib-id>
<name name-style="western">
<surname>Walker</surname> <given-names>Edgar Y.</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gatys</surname> <given-names>Leon A.</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4305-6376</contrib-id>
<name name-style="western">
<surname>Tolias</surname> <given-names>Andreas S.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2392-5105</contrib-id>
<name name-style="western">
<surname>Ecker</surname> <given-names>Alexander S.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of Tübingen, Tübingen, Germany</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Bernstein Center for Computational Neuroscience, Tübingen, Germany</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, Texas, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Department of Neuroscience, Baylor College of Medicine, Houston, Houston, Texas, United States of America</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Electrical and Computer Engineering, Rice University, Houston, Houston, Texas, United States of America</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname> <given-names>Wolfgang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Technische Universitat Chemnitz, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared no competing interests.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">santiago.cadena@uni-tuebingen.de</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>23</day>
<month>4</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>4</issue>
<elocation-id>e1006897</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>2</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Cadena et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006897"/>
<abstract>
<p>Despite great efforts over several decades, our best models of primary visual cortex (V1) still predict spiking activity quite poorly when probed with natural stimuli, highlighting our limited understanding of the nonlinear computations in V1. Recently, two approaches based on deep learning have emerged for modeling these nonlinear computations: transfer learning from artificial neural networks trained on object recognition and data-driven convolutional neural network models trained end-to-end on large populations of neurons. Here, we test the ability of both approaches to predict spiking activity in response to natural images in V1 of awake monkeys. We found that the transfer learning approach performed similarly well to the data-driven approach and both outperformed classical linear-nonlinear and wavelet-based feature representations that build on existing theories of V1. Notably, transfer learning using a pre-trained feature space required substantially less experimental time to achieve the same performance. In conclusion, multi-layer convolutional neural networks (CNNs) set the new state of the art for predicting neural responses to natural images in primate V1 and deep features learned for object recognition are better explanations for V1 computation than all previous filter bank theories. This finding strengthens the necessity of V1 models that are multiple nonlinearities away from the image domain and it supports the idea of explaining early visual cortex based on high-level functional goals.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Predicting the responses of sensory neurons to arbitrary natural stimuli is of major importance for understanding their function. Arguably the most studied cortical area is primary visual cortex (V1), where many models have been developed to explain its function. However, the most successful models built on neurophysiologists’ intuitions still fail to account for spiking responses to natural images. Here, we model spiking activity in primary visual cortex (V1) of monkeys using deep convolutional neural networks (CNNs), which have been successful in computer vision. We both trained CNNs directly to fit the data, and used CNNs trained to solve a high-level task (object categorization). With these approaches, we are able to outperform previous models and improve the state of the art in predicting the responses of early visual neurons to natural images. Our results have two important implications. First, since V1 is the result of several nonlinear stages, it should be modeled as such. Second, functional models of entire visual pathways, of which V1 is an early stage, do not only account for higher areas of such pathways, but also provide useful representations for V1 predictions.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Deutsche Forschungsgemeinschaft (DE)</institution>
</funding-source>
<award-id>479/1-1</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2392-5105</contrib-id>
<name name-style="western">
<surname>Ecker</surname> <given-names>Alexander S.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Bernstein Center for Computational Neuroscience Tübingen (DE)</institution>
</funding-source>
<award-id>FKZ 01GQ1002</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Intelligence Advanced Research Projects Activity (US)</institution>
</funding-source>
<award-id>D16PC00003</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4305-6376</contrib-id>
<name name-style="western">
<surname>Tolias</surname> <given-names>Andreas S.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id>
<institution>National Eye Institute</institution>
</institution-wrap>
</funding-source>
<award-id>T32EY00700140</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3699-3828</contrib-id>
<name name-style="western">
<surname>Denfield</surname> <given-names>George H.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award005">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id>
<institution>National Eye Institute</institution>
</institution-wrap>
</funding-source>
<award-id>F30EY025510</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0057-957X</contrib-id>
<name name-style="western">
<surname>Walker</surname> <given-names>Edgar Y.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award006">
<funding-source>
<institution>National Eye Institute (US)</institution>
</funding-source>
<award-id>R01EY026927</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4305-6376</contrib-id>
<name name-style="western">
<surname>Tolias</surname> <given-names>Andreas S.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>Research reported in this publication was supported by the German Research Foundation (DFG) grant EC 479/1-1 to A.S.E [<ext-link ext-link-type="uri" xlink:href="http://www.dfg.de/" xlink:type="simple">http://www.dfg.de/</ext-link>] and the Collaborative Research Center (SFB 1233, Robust Vision [<ext-link ext-link-type="uri" xlink:href="https://uni-tuebingen.de/en/research/core-research/collaborative-research-centers/sfb-1233.html" xlink:type="simple">https://uni-tuebingen.de/en/research/core-research/collaborative-research-centers/sfb-1233.html</ext-link>]); the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002 [<ext-link ext-link-type="uri" xlink:href="https://www.bccn-tuebingen.de/" xlink:type="simple">https://www.bccn-tuebingen.de/</ext-link>]); the German Excellency Initiative through the Centre for Integrative Neuroscience Tübingen (EXC307 [<ext-link ext-link-type="uri" xlink:href="https://www.cin.uni-tuebingen.de/" xlink:type="simple">https://www.cin.uni-tuebingen.de/</ext-link>]); the National Eye Institute of the National Institutes of Health under Award Numbers R01EY026927 [<ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/" xlink:type="simple">https://nei.nih.gov/</ext-link>](A.S.T.), DP1 EY023176 (A.S.T.), and NIH-Pioneer award DP1-OD008301 [<ext-link ext-link-type="uri" xlink:href="https://commonfund.nih.gov/pioneer/" xlink:type="simple">https://commonfund.nih.gov/pioneer/</ext-link>](A.S.T). This research was also supported by NEI/NIH Core Grant for Vision Research (EY-002520-37 [<ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/" xlink:type="simple">https://nei.nih.gov/</ext-link>]), NEI training grant T32EY00700140 [<ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/" xlink:type="simple">https://nei.nih.gov/</ext-link>](G.H.D) and F30EY025510 (E.Y.W.). This research was also supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003 [<ext-link ext-link-type="uri" xlink:href="http://www.iarpa.gov" xlink:type="simple">www.iarpa.gov</ext-link>]. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="2"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-05-03</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data used for this study is available in a GIN repository <ext-link ext-link-type="uri" xlink:href="https://doid.gin.g-node.org/2e31e304e03d6357c98ac735a1fe5788/" xlink:type="simple">https://doid.gin.g-node.org/2e31e304e03d6357c98ac735a1fe5788/</ext-link> (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.12751/g-node.2e31e3" xlink:type="simple">10.12751/g-node.2e31e3</ext-link>). The code used to train all models in this study is available in the following repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/sacadena/Cadena2019PlosCB" xlink:type="simple">https://github.com/sacadena/Cadena2019PlosCB</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>An essential step towards understanding visual processing in the brain is building models that accurately predict neural responses to arbitrary stimuli [<xref ref-type="bibr" rid="pcbi.1006897.ref001">1</xref>]. Primary visual cortex (V1) has been a strong focus of sensory neuroscience ever since Hubel and Wiesel’s seminal studies demonstrated that neurons in primary visual cortex (V1) respond selectively to distinct image features like local orientation and contrast [<xref ref-type="bibr" rid="pcbi.1006897.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref003">3</xref>]. Our current standard model of V1 is based on linear-nonlinear models (LN) [<xref ref-type="bibr" rid="pcbi.1006897.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref005">5</xref>] and energy models [<xref ref-type="bibr" rid="pcbi.1006897.ref006">6</xref>] to explain simple and complex cells, respectively. While these models work reasonably well to model responses to simple stimuli such as gratings, they fail to account for neural responses to more complex patterns [<xref ref-type="bibr" rid="pcbi.1006897.ref007">7</xref>] and natural images [<xref ref-type="bibr" rid="pcbi.1006897.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref009">9</xref>]. Moreover, the computational advantage of orientation-selective LN neurons over simple center-surround filters found in the retina would be unclear [<xref ref-type="bibr" rid="pcbi.1006897.ref010">10</xref>].</p>
<p>There are a number of hypotheses about nonlinear computations in V1, including normative models like overcomplete sparse coding [<xref ref-type="bibr" rid="pcbi.1006897.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref012">12</xref>] or canonical computations like divisive normalization [<xref ref-type="bibr" rid="pcbi.1006897.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref014">14</xref>]. The latter has been used to explain specific phenomena such as center-surround interactions with carefully designed stimuli [<xref ref-type="bibr" rid="pcbi.1006897.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref018">18</xref>]. However, to date, these ideas have not been turned into predictive models of spiking responses that generalize beyond simple stimuli—especially to natural images.</p>
<p>To go beyond simple LN models for natural stimuli, LN-LN cascade models have been proposed, which either learn (convolutional) subunits [<xref ref-type="bibr" rid="pcbi.1006897.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref021">21</xref>] or use handcrafted wavelet representations [<xref ref-type="bibr" rid="pcbi.1006897.ref022">22</xref>]. These cascade models outperform simple LN models, but they currently do not capture the full range of nonlinearities observed in V1, like gain control mechanisms and potentially other not-yet-understood nonlinear response properties. Because experimental time is limited, LN-LN models have to be designed very carefully to keep the number of parameters tractable, which currently limits their expressiveness, essentially, to energy models for direction-selective and complex cells.</p>
<p>Thus, to make progress in a quantitative sense, recent advances in machine learning and computer vision using deep neural networks (‘deep learning’) have opened a new door by allowing us to learn much more complex nonlinear models of neural responses. There are two main approaches, which we refer to as <italic>goal-driven</italic> and <italic>data-driven</italic>.</p>
<p>The idea behind the goal-driven approach is to train a deep neural network on a high-level task and use the resulting intermediate representations to model neural responses [<xref ref-type="bibr" rid="pcbi.1006897.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref024">24</xref>]. In the machine learning community, this concept is known as transfer learning and has been very successful in deep learning [<xref ref-type="bibr" rid="pcbi.1006897.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref026">26</xref>]. Deep convolutional neural networks (CNNs) have reached human-level performance on visual tasks like object classification by training on over one million images [<xref ref-type="bibr" rid="pcbi.1006897.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref030">30</xref>]. These CNNs have proven extremely useful as nonlinear feature spaces for tasks where less labeled data is available [<xref ref-type="bibr" rid="pcbi.1006897.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref031">31</xref>]. This transfer to a new task can be achieved by (linearly) reading out the network’s internal representations of the input. Yamins, DiCarlo and colleagues showed recently that using deep networks trained on large-scale object recognition as nonlinear feature spaces for neural system identification works remarkably well in <italic>higher areas</italic> of the ventral stream, such as V4 and IT [<xref ref-type="bibr" rid="pcbi.1006897.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref033">33</xref>]. Other groups have used similar approaches for early cortical areas using fMRI [<xref ref-type="bibr" rid="pcbi.1006897.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref036">36</xref>]. However, this approach has not yet been used to model spiking activity of early stages such as V1.</p>
<p>The deep data-driven approach, on the other hand, is based on fitting all model parameters directly to neural data [<xref ref-type="bibr" rid="pcbi.1006897.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref041">41</xref>]. The most critical advance of these models in neural system identification is that they can have many more parameters than the classical LN cascade models discussed above, because they exploit computational similarities between different neurons [<xref ref-type="bibr" rid="pcbi.1006897.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>]. While previous approaches treated each neuron as an individual multivariate regression problem, modern CNN-based approaches learn one model for an entire population of neurons, thereby exploiting two key properties of local neural circuits: (1) they share the same presynaptic circuitry (for V1: retina and LGN) [<xref ref-type="bibr" rid="pcbi.1006897.ref038">38</xref>] and (2) many neurons perform essentially the same computation, but at different locations (topographic organization, implemented by convolutional weight sharing) [<xref ref-type="bibr" rid="pcbi.1006897.ref039">39</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref041">41</xref>].</p>
<p>While both the goal-driven and the data-driven approach have been shown to outperform LN models in some settings, neither approach has been evaluated on spiking activity in monkey V1 (see [<xref ref-type="bibr" rid="pcbi.1006897.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref043">43</xref>] for concurrent work). In this paper, we fill this gap and evaluate both approaches in monkey V1. We found that deep neural networks lead to substantial performance improvements over older models. In our natural image dataset, goal-driven and data-driven models performed similarly well. The goal-driven approach reached this performance with as little as 20% of the dataset and its performance saturated thereafter. In contrast, the data-driven approach required the full dataset for maximum performance, suggesting that it could benefit from a larger dataset and reach even better performance. Our key finding is that the best models required at least four nonlinear processing steps, suggesting that we need to revise our view of V1 as a Gabor filter bank and appreciate the nonlinear nature of its computations. We conclude that deep networks are not just one among many approaches that can be used, but are—despite their limitations—currently the single most accurate model of V1 computation.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We measured the spiking activity of populations of neurons in V1 of two awake, fixating rhesus macaques using a linear 32-channel array spanning all cortical layers. Monkeys were viewing stimuli that consisted of 1450 natural images and four sets of textures synthesized to keep different levels of higher-order correlations present in these images (<xref ref-type="fig" rid="pcbi.1006897.g001">Fig 1</xref>, see <xref ref-type="sec" rid="sec012">Methods</xref>). Each trial consisted of a sequence of images shown for 60 ms each, with no blanks in between. In each session, we centered the stimuli on the population receptive field of the neurons.</p>
<fig id="pcbi.1006897.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Stimulus paradigm.</title>
<p>A: Classes of images shown in the experiment. We used grayscale natural images (labeled ‘original’) from the ImageNet dataset [<xref ref-type="bibr" rid="pcbi.1006897.ref044">44</xref>] along with textures synthesized from these images using the texture synthesis algorithm described by [<xref ref-type="bibr" rid="pcbi.1006897.ref045">45</xref>]. Each row shows four synthesized versions of three example original images using different convolutional layers (see <xref ref-type="sec" rid="sec012">Materials and Methods</xref> for details). Lower convolutional layers capture more local statistics compared to higher ones. B: Stimulus sequence. In each trial, we showed a randomized sequence of images (each displayed for 60 ms covering 2 degrees of visual angle) centered on the receptive fields of the recorded neurons while the monkey sustained fixation on a target. The images were masked with a circular mask with cosine fadeout.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g001" xlink:type="simple"/>
</fig>
<p>We isolated 262 neurons in 17 sessions. The neurons responded well to the fast succession of natural images with a typical response latency of 40ms (<xref ref-type="fig" rid="pcbi.1006897.g002">Fig 2B</xref>). Therefore, we extracted the spike counts in the window 40–100 ms after image onset (<xref ref-type="fig" rid="pcbi.1006897.g002">Fig 2B</xref>). The recorded neurons were diverse in their temporal response properties (e.g. see autocorrelogram <xref ref-type="fig" rid="pcbi.1006897.g002">Fig 2A</xref>), average firing rates in response to stimulus (21.1 ± 20.8 spikes/s, mean ± S.D.), cortical depth (55% of cells in granular, 18% in supragranular, and 27% infragranular layers), and response-triggered average (RTA) structure (<xref ref-type="fig" rid="pcbi.1006897.g002">Fig 2C</xref>), but neurons recorded on the same array generally had their receptive fields at similar locations approximately centered on the stimulus (<xref ref-type="fig" rid="pcbi.1006897.g002">Fig 2C</xref>). Prior to analysis, we selected neurons based on how reliable their responses were from trial to trial and included only neurons for which at least 15% of their total variance could be attributed to the stimulus (see <xref ref-type="sec" rid="sec012">Methods</xref>). This selection resulted in 166 neurons, which form the basis of the models we describe in the following.</p>
<fig id="pcbi.1006897.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g002</object-id>
<label>Fig 2</label>
<caption>
<title>V1 electrophysiological responses.</title>
<p>A: Isolated single-unit activity. We performed acute recordings with a 32-channel, linear array (NeuroNexus V1x32-Edge-10mm-60-177, layout shown in the left) to record in primary visual cortex of two awake, fixating macaques. The channel mean-waveform footprints of the spiking activity of 23 well-isolated neurons in one example session are shown in the central larger panel. The upper panel shows color-matched autocorrelograms. B: Peri-stimulus time histograms (PSTH) of four example neurons from A. Spike counts where binned with t = 1 ms, aligned to the onset of each stimulus image, and averaged over trials. The 60 ms interval where the image was displayed is shown in red. We ignored the temporal profile of the response and extracted spike counts for each image on the 40–100 ms interval after image onset (shown in light gray). C: The Response Triggered Average (RTA) calculated by reverse correlation of the extracted responses.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g002" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Generalized linear model with pre-trained CNN features</title>
<p>We start by investigating the goal-driven approach [<xref ref-type="bibr" rid="pcbi.1006897.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref024">24</xref>]. Here, the idea is to use a high-performing neural network trained on a specific goal—object recognition in this case—as a non-linear feature space and train only a simple linear-nonlinear readout. We chose VGG-19 [<xref ref-type="bibr" rid="pcbi.1006897.ref028">28</xref>] over other neural networks, because it has a simple architecture (described below), a fine increase in receptive field size along its hierarchy and reasonably high classification accuracy.</p>
<p>VGG-19 is a CNN trained on the large image classification task ImageNet (ILSVRC2012) that takes an RGB image as input and infers the class of the dominant object in the image (among 1000 possible classes). The architecture of VGG-19 consists of a hierarchy of linear-nonlinear transformations (layers), where the input is spatially convolved with a set of filters and then passed through a rectifying nonlinearity (<xref ref-type="fig" rid="pcbi.1006897.g003">Fig 3</xref>). The output of this operation is again an image with multiple channels. However, these channels do not represent color—as the three channels in the input image—but learned features. They are therefore also called feature maps. Each feature map can be viewed as a filtered version of its input. The collection of such feature maps serves as input for the next layer. Additionally, the network has five pooling layers, where the feature maps are downsampled by a factor of two by taking the local maximum value of four neighboring pixels. There are 16 convolutional layers that can be grouped into five groups named conv1 to conv5 with 2, 2, 4, 4, 4 convolutional layers and 64, 128, 256, 512, 512 output feature maps, respectively, and a pooling layer after each group.</p>
<fig id="pcbi.1006897.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Our proposed model based on VGG-19 features.</title>
<p>VGG-19 [<xref ref-type="bibr" rid="pcbi.1006897.ref028">28</xref>] (gray background) is a trained CNN that takes an input image and produces a class label. For each of the 16 convolutional layers of VGG-19, we extract the feature representations (feature maps) of the images shown to the monkey. We then train for each recorded neuron and convolutional layer, a Generalized Linear Model (GLM) using the feature maps as input to predict the observed spike counts. The GLM is formed by a linear projection (dot product) of the feature maps, a pointwise nonlinearity, and an assumed noise distribution (Poisson) that determines the optimization loss for training. We additionally imposed strong regularization constraints on the readout weights (see text).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g003" xlink:type="simple"/>
</fig>
<p>We used VGG-19 as a feature space in the following way: We selected the output of a convolutional layer as input features for a Generalized Linear Model (GLM) that predicts the recorded spike counts (<xref ref-type="fig" rid="pcbi.1006897.g003">Fig 3</xref>). Specifically, we fed each image <italic>x</italic> in our stimulus set through VGG-19 to extract the resulting feature maps Φ(<italic>x</italic>) of a certain layer. These feature maps were then linearly weighted with a set of learned readout weights <italic>w</italic>. This procedure resulted in a single scalar value for each image that was then passed through a (static) output nonlinearity to produce a prediction for the firing rate:
<disp-formula id="pcbi.1006897.e001"><alternatives><graphic id="pcbi.1006897.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mo>[</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
We assumed this prediction to be the mean rate of a Poisson process (see <xref ref-type="sec" rid="sec012">Methods</xref> for details). In addition, we applied a number of regularization terms on the readout weights that we explain later.</p>
</sec>
<sec id="sec004">
<title>Intermediate layers of VGG best predict V1 responses</title>
<p>We first asked which convolutional layer of VGG-19 provides the best feature space for V1. To answer this question, we fitted a readout for each layer and compared the performance. We measured performance by computing the fraction of explainable variance explained (<italic>FEV</italic>). This metric, which ranges from zero to one, measures what fraction of the stimulus-driven response is explained by the model, ignoring the unexplainable trial-to-trial variability in the neurons’ responses (for details see <xref ref-type="sec" rid="sec012">Methods</xref>).</p>
<p>We found that the fifth (out of sixteen) layers’ features (called ‘conv3_1’, <xref ref-type="fig" rid="pcbi.1006897.g003">Fig 3</xref>) best predicted neuronal responses to novel images not seen during training (<xref ref-type="fig" rid="pcbi.1006897.g004">Fig 4</xref>, solid line). This model predicted on average 51.6% of the explainable variance. In contrast, performance for the very first layer was poor (31% FEV), but increased monotonically up to conv3_1. Afterwards, the performance again decreased continually up the hierarchy (<xref ref-type="fig" rid="pcbi.1006897.g004">Fig 4</xref>). These results followed our intuition that early to intermediate processing stages in a hierarchical model should match primary visual cortex, given that V1 is the third processing stage in the visual hierarchy after the retina and the lateral geniculate nucleus (LGN) of the thalamus.</p>
<fig id="pcbi.1006897.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Model performance on test set.</title>
<p>Average fraction of explainable variance explained (<italic>FEV</italic>) for models using different VGG layers as nonlinear feature spaces for a GLM. Error bars show 95% confidence intervals of the population means. The model based on layer conv3_1 shows on average the highest predictive performance.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Control for input resolution and receptive field sizes</title>
<p>An important issue to be aware of is that the receptive field sizes of VGG units grow along the hierarchy—just like those of visual neurons in the brain. Incidentally, the receptive fields of units in the best-performing layer conv3_1 subtended approximately 0.68 degrees of visual angle, roughly matching the expected receptive sizes of our V1 neurons given their eccentricities between 1 and 3 degrees. Because receptive fields in VGG are defined in terms of image pixels, their size in degrees of visual angle depends on the resolution at which we present images to VGG, which is a free parameter whose choice will affect the results.</p>
<p>VGG-19 was trained on images of 224 × 224 px. Given the image resolution we used for the analyses presented above, an entire image would subtend ∼6.4 degrees of visual angle (the crops shown to the monkey were 2 degrees; see <xref ref-type="sec" rid="sec012">Methods</xref> for details). Although this choice appears to be reasonable and consistent with earlier work [<xref ref-type="bibr" rid="pcbi.1006897.ref033">33</xref>], it is to some extent arbitrary. If we had presented the images at lower resolution, the receptive fields sizes of all VGG units would have been larger. As a consequence, the receptive fields of units in earlier layers would match those of V1 and these layers may perform better. If this was indeed the case, there would be nothing special about layer conv3_1 with respect to V1.</p>
<p>To ensure that the choice of input resolution did not affect our results, we performed a control experiment, which substantiated our claim that conv3_1 provides the best features for V1. We repeated the model comparison presented above with different input resolutions, rescaling the image crops by a factor of 0.67 and 1.5. These resolutions correspond to 9.55 and 4.25 degrees of full visual field for VGG-19, respectively. While changing the input resolution did shift the optimal layer towards that with matching receptive field sizes (<xref ref-type="fig" rid="pcbi.1006897.g005">Fig 5</xref>, first and third row), the resolution we had picked for our main experiment yielded the best overall performance (<xref ref-type="fig" rid="pcbi.1006897.g005">Fig 5</xref>, second row, third column). Thus, over a range of input resolutions and layers, conv3_1 performed best, although conv2_2 at lower resolution yielded only slightly lower performance.</p>
<fig id="pcbi.1006897.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g005</object-id>
<label>Fig 5</label>
<caption>
<title>VGG-19 based model performance at different input scales.</title>
<p>The performance on test set of cross-validated models that use as feature spaces layers conv2_1 to conv3_3 for different input resolutions. With the original scale used in <xref ref-type="fig" rid="pcbi.1006897.g004">Fig 4</xref>, we assumed that VGG-19 was trained with 6.4 degrees field of view. Scaling this resolution by a factor of 0.67 and 1.5 justifies the original choice of resolution for further analysis. At the bottom, the receptive field sizes in pixels of the different layers are shown.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Careful regularization is necessary</title>
<p>The number of predictors given by the convolutional feature space of a large pre-trained network is much larger than the number of pixels in the image. Most of these predictors will likely be irrelevant for most recorded neurons—for example, network units at spatial positions that are not aligned with the neuron’s receptive field or feature maps that compute nonlinearities unrelated to those of the cells. Naïvely including many unimportant predictors would prevent us from learning a good mapping, because they lead to overfitting. We therefore used a regularization scheme with the following three terms for the readout weights: (1) sparsity, to encourage the selection of a few units; (2) smoothness, for a regular spatial continuity of the predictors’ receptive fields; and (3) group sparsity, to encourage the model to pool from a small number of feature maps (see <xref ref-type="sec" rid="sec012">Methods</xref> for details).</p>
<p>We found that regularization was key to obtaining good performance (<xref ref-type="table" rid="pcbi.1006897.t001">Table 1</xref>). The full model with all three terms had the best performance on the test set and vastly outperformed a model with no regularization. Eliminating one of the three terms while keeping the other two hurt performance only marginally. Among the three regularizers, sparsity appeared to be the most important one quantitatively, whereas smoothness and group sparsity could be dropped without hurting overall performance.</p>
<table-wrap id="pcbi.1006897.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.t001</object-id>
<label>Table 1</label>
<caption>
<title>Ablation experiments for VGG-based model, removing regularization terms (rows 2–5) and using factorized readout weights (row 6, [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>]).</title>
</caption>
<alternatives>
<graphic id="pcbi.1006897.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-bottom:thick">Model</th>
<th align="center" style="border-bottom:thick">FEV</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>Full model</bold></td>
<td align="char" char="."><bold>0.52</bold></td>
</tr>
<tr>
<td align="left">No smoothness</td>
<td align="char" char=".">0.51</td>
</tr>
<tr>
<td align="left">No sparsity</td>
<td align="char" char=".">0.49</td>
</tr>
<tr>
<td align="left">No group sparsity</td>
<td align="char" char=".">0.51</td>
</tr>
<tr>
<td align="left">No regularization</td>
<td align="char" char=".">0.33</td>
</tr>
<tr>
<td align="left">Factorized readout [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>]</td>
<td align="char" char=".">0.45</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>To understand the effect of the different regularizers qualitatively, we visualized the readout weights of each feature map of our conv3_1-based model, ordered by their spatial energy for each cell, for each of the regularization schemes (see <xref ref-type="fig" rid="pcbi.1006897.g006">Fig 6A</xref> for five sample neurons). Without the sparsity constraint, we obtained smooth but spread-out weights that were not well localized. Dropping the smoothness term—despite performing equally in a quantitative sense—produced sparse activations that were less localized and not smooth. Without any regularization, the weights appeared noisy and one could not get any insights about the locality of the neuron. On the other hand, the full model—in addition to having the best performance—also provides localized and smooth filters that provide information about the neurons’ receptive field and the set of useful feature maps for prediction.</p>
<fig id="pcbi.1006897.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Learned readout weights with different regularization modes.</title>
<p><bold>A</bold>. For five example neurons (rows), the five highest-energy spatial readouts out of 256 feature maps of conv3_1 for each regularization scheme we explored. Feature map weights are 10 × 10 for a 40 × 40 input (∼ 1.1°). The full model exhibits the most localized and smooth receptive fields. The scale bar is shared by all features maps of a each model and neuron from their minimum (white) to the maximum (dark). <bold>B</bold>. The highest normalized spatial energy of the learned readouts as a function of ordered feature maps (first 70 out of 256 of conv3_1 shown) averaged for all cells. With regularization, only a few feature maps are used for prediction, quickly asymptoting at 1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g006" xlink:type="simple"/>
</fig>
<p>Finally, we also observed that only a small number of feature maps was used for each neuron: the weights decayed exponentially and only 20 feature maps out of 256 contained on average 82% of the readout energy (<xref ref-type="fig" rid="pcbi.1006897.g006">Fig 6B</xref>).</p>
<p>An alternative form of regularization or inductive bias would be to constrain the readout weights to be factorized in space and features [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>], which reduces the number of parameters substantially. However, the best model with this factorized readout achieved only 45.5% FEV (<xref ref-type="table" rid="pcbi.1006897.t001">Table 1</xref>), presumably because the feature space has not been optimized for such a constrained readout.</p>
</sec>
<sec id="sec007">
<title>Goal-driven and data driven CNNs set the state of the art</title>
<p>Multi-layer feedforward networks have been fitted successfully to neural data on natural image datasets in mouse V1 [<xref ref-type="bibr" rid="pcbi.1006897.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>]. Thus, we inquired how our goal-driven model compares to a model belonging to the same functional class, but directly fitted to the neural data. Following the methods proposed by Klindt et. al [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>], we fitted CNNs with one to five convolutional layers (<xref ref-type="fig" rid="pcbi.1006897.g007">Fig 7A</xref>; see <xref ref-type="sec" rid="sec012">Methods</xref> for details).</p>
<fig id="pcbi.1006897.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Data-driven convolutional network model.</title>
<p>We trained a convolutional neural network to produce a feature space fed to a GLM-like model. In contrast to the VGG-based model, both feature space and readout weights are trained only on the neural data. <bold>A</bold>. Three-layer architecture with a factorized readout [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>] used for comparison with other models. <bold>B</bold>. Performance of the data driven approach as a function of the number of convolutional layers on held-out data. Three convolutional layers provided the best performance on the validation set. See <xref ref-type="sec" rid="sec012">Methods</xref> for details.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g007" xlink:type="simple"/>
</fig>
<p>The data-driven CNNs with three or more convolutional layers yielded the best performance, outperforming their competitors with fewer (one or two) layers (<xref ref-type="fig" rid="pcbi.1006897.g007">Fig 7B</xref>). We therefore decided to use the CNN with three layers for model comparison, as it is the simplest model with highest predictive power on the validation set.</p>
<p>We then asked how the predictive performance of both data-driven and goal driven models compares to previous models of V1. As a baseline, we fitted a regularized version of the classical linear-nonlinear Poisson model (LNP; [<xref ref-type="bibr" rid="pcbi.1006897.ref046">46</xref>]). The LNP is a very popular model used to estimate the receptive field of neurons and offers interpretability and convexity for its optimization. This model gave us a good idea of the nonlinearity of the cells’ responses. Additionally, we fit a model based on a handcrafted nonlinear feature space consisting of a set of Gabor wavelets [<xref ref-type="bibr" rid="pcbi.1006897.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref047">47</xref>–<xref ref-type="bibr" rid="pcbi.1006897.ref049">49</xref>] and energy terms of each quadrature pair [<xref ref-type="bibr" rid="pcbi.1006897.ref006">6</xref>]. We refer to this model as the ‘Gabor filter bank’ (GFB). It builds upon existing knowledge about V1 function and is able to model simple and complex cells as well as linear combinations thereof. Moreover, this model is the current state of the art in the neural prediction challenge for monkey V1 responses to natural images [<xref ref-type="bibr" rid="pcbi.1006897.ref050">50</xref>] and therefore a strong baseline for a quantitative evaluation.</p>
<p>We compared the models for a number of cells selected randomly (<xref ref-type="fig" rid="pcbi.1006897.g008">Fig 8A</xref>). There was a diversity of cells, both in terms of how much variance could be explained in principle (dark gray bars) and how well the individual models performed (colored bars). Overall, the deep learning models consistently outperformed the two simpler models of V1. This trend was consistent across the entire dataset (<xref ref-type="fig" rid="pcbi.1006897.g008">Fig 8B and 8D</xref>). The LNP model achieved 16.3% <italic>FEV</italic>, the GFB model 45.6% <italic>FEV</italic>. The performance of the CNN trained directly on the data was comparable to that of the VGG-based model (<xref ref-type="fig" rid="pcbi.1006897.g008">Fig 8C and 8D</xref>); they predicted 49.8% and 51.6% <italic>FEV</italic>, respectively, on average. The differences in performance between all four models were statistically significant (Wilcoxon signed rank test, <italic>n</italic> = 166; family-wise error rate <italic>α</italic> = 0.05 using Holm-Bonferroni method to account for multiple comparisons). Note that the one-layer CNN (mean 34.5% <italic>FEV</italic>, <xref ref-type="fig" rid="pcbi.1006897.g007">Fig 7</xref>) structurally resembles the convolutional subunit model proposed by Vintch and colleagues [<xref ref-type="bibr" rid="pcbi.1006897.ref021">21</xref>]. Thus, deeper CNNs also outperform learned LN-LN cascade models.</p>
<fig id="pcbi.1006897.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Deep models are the new state of the art.</title>
<p>A: Randomly selected cells. The normalized explainable variance (oracle) per cell is shown in gray. For each cell from left to right, the variance explained of: regularized LNP [<xref ref-type="bibr" rid="pcbi.1006897.ref046">46</xref>], GFB [<xref ref-type="bibr" rid="pcbi.1006897.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref048">48</xref>], three-layer CNN trained on neural responses, and VGG conv3_1 model (ours). B. CNN and VGG conv3_1 models outperform for most cells LNP and GFB. Black line denotes the identity. The performance is given in <italic>FEV</italic>. C: VGG conv3_1 features perform slightly better than the three-layer CNN. D: Performance of the four models in fraction of explainable variance explained (<italic>FEV</italic>) averaged across neurons. Error bars show 95% confidence intervals of the population means. All models perform significantly different from each other (Wilcoxon signed rank test, <italic>n</italic> = 166; family-wise error rate <italic>α</italic> = 0.05 using Holm-Bonferroni method to account for multiple comparisons).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Improvement of model predictions is not linked to neurons’ tuning properties</title>
<p>We next asked whether the improvement in predictive performance afforded by our deep neural network models was related in any way to known tuning properties of V1 neurons such as the shape of their orientation tuning curve or their classification along the simple-complex axis. To investigate this question, we performed an in-silico experiment: we showed Gabor patches of the same size as our image stimulus with various orientations, spatial frequencies and phases (<xref ref-type="fig" rid="pcbi.1006897.g009">Fig 9A</xref>) to our CNN model of each cell. Based on the model output, we computed tuning curves for orientation (<xref ref-type="fig" rid="pcbi.1006897.g009">Fig 9B</xref>) and spatial phase (<xref ref-type="fig" rid="pcbi.1006897.g009">Fig 9D</xref>) by using the set of Gabors with the optimal spatial frequency for each neuron.</p>
<fig id="pcbi.1006897.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Relationship between model performance and neurons’ tuning properties.</title>
<p><bold>A</bold>. A sample subset of the Gabor stimuli with a rich diversity of frequencies, orientations, and phases.<bold>B</bold>. Dots: Orientation tuning curves of 80 sample neurons predicted by our CNN model. Tuning curves computed at the optimal spatial frequency and phase for each neuron. Lines: von Mises fits.<bold>C</bold>. Difference in performance between pairs of the four models as a function of tuning width. Tuning width was defined as the full width at half maximum of the fitted tuning curve.<bold>D</bold>. Dots: Phase tuning curves of the same 80 sample neurons as in B, predicted by our CNN model. Tuning curves computed at the optimal spatial frequency and orientation for each neuron. Lines: Cosine tuning curve with fitted amplitude and offset (see <xref ref-type="sec" rid="sec012">Methods</xref>).<bold>E</bold>. Like C, the difference in performance between pairs of models as a function of the neurons’ linearity index. Linearity index: ratio of amplitude of cosine over offset (0: complex; 1: simple).<bold>F</bold>. Performance comparison between GFB and LNP model. Red: simple cells (top 16% linearity, linearity &gt; 0.3); blue: complex cells (bottom 28% linearity, linearity &lt; 0.04).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g009" xlink:type="simple"/>
</fig>
<p>Based on the phase tuning curves we compute a linearity index (see <xref ref-type="sec" rid="sec012">Methods</xref>), which locates each cell on the axis from simple (linearity index close to one) to complex (index close to zero). We then asked whether there are systematic differences in model performance as a function of this simple-complex characterization. As expected, we found that more complex cells are explained better by the Gabor filter bank model than an LNP model (<xref ref-type="fig" rid="pcbi.1006897.g009">Fig 9C</xref>). The same was true for both the data-driven CNN and the VGG-based model. However, the simple-complex axis did not predict whether and how much the CNN models outperformed the Gabor filter bank model. Thus, whatever aspect of V1 computation was additionally explained by the CNN models, it was shared by both simple and complex cells.</p>
<p>Next, we asked whether there is a relationship between orientation selectivity (tuning width) and the performance of any of our models. We found that for cells with sharper orientation tuning, the performance gain afforded by the Gabor filter bank model (and both CNN-based models) over an LNP was larger than for less sharply tuned cells (<xref ref-type="fig" rid="pcbi.1006897.g009">Fig 9E</xref>). This result is not unexpected given that cells in layer 2/3 tend to have narrower tuning curves and also tend to be more complex [<xref ref-type="bibr" rid="pcbi.1006897.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref052">52</xref>]. However, as for the simple-complex axis, tuning width was not predictive of the performance gain afforded by a CNN-based model over the Gabor filter bank (<xref ref-type="fig" rid="pcbi.1006897.g009">Fig 9E</xref>). Therefore, any additional nonlinearity in V1 computation captured by the CNN models is not specific to sharply or broadly tuned neurons.</p>
</sec>
<sec id="sec009">
<title>Models generalize across stimulus statistics</title>
<p>Our stimulus set contains both natural images as well as four sets of textures generated from those images. These textures differ in how accurately and over what spatial extent they reproduce the local image statistics (see <xref ref-type="fig" rid="pcbi.1006897.g001">Fig 1</xref>). On the one end of the spectrum, samples from the conv1 model reproduce relatively linear statistics over small regions of a few minutes of arc. On the other end of the spectrum, samples from the conv4 model almost perfectly reproduce the statistics of natural images over larger regions of 1–2 degrees of visual angle, covering the entire classical and at least part of the extra-classical receptive field of V1 neurons.</p>
<p>We asked to what extent including these different image statistics helps or hurts building a predictive model. To answer this question, we additionally fit both the data-driven CNN model and the VGG-based model to subsets of the data containing only images from a single image type (originals or one of four texture classes). We then evaluated each of these models on all image types (<xref ref-type="fig" rid="pcbi.1006897.g010">Fig 10</xref>). Perhaps surprisingly, we found that using any of the four texture statistics or the original images for training lead to approximately equal performance, independent of which images were used for testing the model (<xref ref-type="fig" rid="pcbi.1006897.g010">Fig 10</xref>). This result held for both the VGG-based (<xref ref-type="fig" rid="pcbi.1006897.g010">Fig 10A</xref>) and the data-driven CNN model (<xref ref-type="fig" rid="pcbi.1006897.g010">Fig 10B</xref>). Thus, using the very localized conv1 textures worked just as well for predicting the responses to natural images as did training directly on natural images—or any other combination of training and test set. This result is somewhat surprising to us, as the conv1 textures match only very simple and local statistics on spatial scales smaller than individidual neurons’ receptive fields and perceptually are much closer to noise than to natural images.</p>
<fig id="pcbi.1006897.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Training and evaluation on the different stimulus types.</title>
<p>For both conv3_1 features of VGG-19 (left) and CNN-based models, we trained with all and every individual stimulus type (rows) (see <xref ref-type="fig" rid="pcbi.1006897.g001">Fig 1</xref>) and tested on all and every individual type. The VGG model showed good domain transfer in general. The same was true for the data-driven CNN model, although it performed worse overall when trained on only one set of images due to the smaller training sample. There were no substantial differences in performance across image statistics.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.g010" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>VGG-based model needs less training data</title>
<p>An interesting corollary of the analysis above is the difference in absolute performance between the VGG-based and the data-driven CNN model when using only a subset of images for training: while the performance of the VGG-based model remains equally high when using only a fifth of the data for training (<xref ref-type="fig" rid="pcbi.1006897.g010">Fig 10A</xref>), the data-driven CNN takes a substantial hit (<xref ref-type="fig" rid="pcbi.1006897.g010">Fig 10B</xref>, second and following rows). Thus, while the two models perform similarly when using our entire dataset, the VGG-based model works better when less training data is available. This result indicates that, for our current experimental paradigm, training the readout weights is not the bottleneck—despite the readout containing a large number of parameters in the VGG-based model (<xref ref-type="table" rid="pcbi.1006897.t002">Table 2</xref>). Because we know that only a small number of non-zero weights are necessary, the L1 regularizer works very well in this case. In contrast, the data-driven model takes a substantial hit when using only a subset of the data, suggesting that learning the shared feature space is the bottleneck for this model. Thus collecting a larger dataset could help the data-driven model but is unlikely to improve performance of the VGG-based one.</p>
<table-wrap id="pcbi.1006897.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006897.t002</object-id>
<label>Table 2</label>
<caption>
<title>Number of learned parameters for the different models.</title>
<p>‘Core’ refers to the part shared among all neurons. ‘Readout’ refers to the parameters required for each neuron.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006897.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006897.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" style="border-bottom:thick">Model</th>
<th align="center" style="border-bottom:thick">Core</th>
<th align="center" style="border-bottom:thick">Readout/neuron</th>
<th align="center" style="border-bottom:thick">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">LNP</td>
<td align="center">-</td>
<td align="center">1,601</td>
<td align="center">265,766</td>
</tr>
<tr>
<td align="center">GFB</td>
<td align="center">-</td>
<td align="center">5,545</td>
<td align="center">920,470</td>
</tr>
<tr>
<td align="center">CNN</td>
<td align="center">23,936</td>
<td align="center">867</td>
<td align="center">167,858</td>
</tr>
<tr>
<td align="center">VGG</td>
<td align="center">512</td>
<td align="center">25,601</td>
<td align="center">4,250,278</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>Our goal was to find which model among various alternatives is best for one of the most studied systems in modern systems neuroscience: primary visual cortex. We fit two models based on convolutional neural networks to V1 responses to natural stimuli in awake, fixating monkeys: a goal-driven model, which uses the representations learned by a CNN trained on object recognition (VGG-19), and a data-driven model, which learns both the convolutional and readout parameters using stimulus-response pairs with multiple neurons simultaneously. Both approaches yielded comparable performance and substantially outperformed the widely used LNP [<xref ref-type="bibr" rid="pcbi.1006897.ref046">46</xref>] and a rich Gabor filter bank (GFB), which held the previous state of the art in prediction of V1 responses to natural images. This finding is of great importance because it suggests that deep neural networks can be used to model not only higher cortex, but also lower cortical areas. In fact, deep networks are not just one among many approaches that can be used, but the only class of models that has been shown to provide the multiple nonlinearities necessary to accurately describe V1 responses to natural stimuli.</p>
<p>Our work contributes to a growing body of research where goal-driven deep learning models [<xref ref-type="bibr" rid="pcbi.1006897.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref024">24</xref>] have shown unprecedented predictive performance in higher areas of the visual stream [<xref ref-type="bibr" rid="pcbi.1006897.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref033">33</xref>], and a hierarchical correspondence between deep networks and the ventral stream [<xref ref-type="bibr" rid="pcbi.1006897.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref053">53</xref>]. Studies based on fMRI have established a correspondence between early layers of CNNs trained on object recognition and V1 [<xref ref-type="bibr" rid="pcbi.1006897.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref054">54</xref>]. Here, with electrophysiological data and a deeper network (VGG-19), we found that V1 is better explained by feature spaces multiple nonlinearities away from the pixels. We found that it takes five layers (a quarter of the way) into the computational stack of the object categorization network to explain V1 best, which is in contrast to the many models that treat V1 as only one or two nonlinearities away from pixels (i.e. GLMs, energy models). Earlier layers of our CNNs might explain subcortical areas better (i.e. retina and LGN), as they are known to be modeled best with multiple, but fewer, nonlinearities already [<xref ref-type="bibr" rid="pcbi.1006897.ref041">41</xref>].</p>
<p>What are, then, the additional nonlinearities captured by our deep convolutional models beyond those in LNP or GFB? Our first attempts to answer this question via an in-silico analysis revealed that whatever the CNNs capture beyond the Gabor filter bank model is not specific to the cells’ tuning properties, such as width of the orientation tuning curve and their characterization along the simple-complex spectrum. This result suggests that the missing nonlinearity may be relatively generic and applicable to most cells. There are a few clear candidates for such nonlinear computations, including divisive normalization [<xref ref-type="bibr" rid="pcbi.1006897.ref055">55</xref>] and overcomplete sparse coding [<xref ref-type="bibr" rid="pcbi.1006897.ref012">12</xref>]. Unfortunately, quantifying whether these theories provide an equally good account of the data is not straightforward: so far they have not been turned into predictive models for V1 neurons that are applicable to natural images. In the case of divisive normalization, the main challenge is learning the normalization pool. There is evidence for multiple normalization pools, both tuned and untuned and operating in the receptive field center and surround [<xref ref-type="bibr" rid="pcbi.1006897.ref056">56</xref>]. However, previous work investigating these normalization pools has employed simple stimuli such as gratings [<xref ref-type="bibr" rid="pcbi.1006897.ref018">18</xref>] and we are not aware of any work learning the entire normalization function from neural responses to natural stimuli. Similarly, sparse coding has so far been evaluated only qualitatively by showing that the learned basis functions resemble Gabor filters [<xref ref-type="bibr" rid="pcbi.1006897.ref012">12</xref>]. Solving a convolutional sparse coding problem [<xref ref-type="bibr" rid="pcbi.1006897.ref057">57</xref>] and using the resulting representation as a feature space would be a promising direction for future work, but we consider re-implementing and thoroughly evaluating this approach to be beyond the scope of the current paper.</p>
<p>To move forward in understanding such nonlinearities may require developing more interpretable neural networks or methods that provide interpretability of networks, which are an active area of research in the machine learning community. Alternatively, we could build predictive models constrained with specific hard-coded nonlinearities (such as normalization) that express our knowledge about important computations.</p>
<p>It is also possible that the mechanistic level of circuit components remains underconstrained by function and thus allows only for explanations up to some degree of degeneracy, requiring knowledge of the objective function the system optimizes (e.g. sparse coding, predictive coding). Our results show that object categorization—despite being a relatively impoverished visual task—is a very useful learning objective not only for high-level areas in the ventral stream, but also for a more low-level and general-purpose area like V1, despite the fact that V1 clearly serves a large number of tasks beyond object categorization. This finding resonates well with results from computer vision, where object categorization has also been found to be an extremely useful objective to learn features applicable to numerous other visual tasks [<xref ref-type="bibr" rid="pcbi.1006897.ref025">25</xref>].</p>
<p>Our current best models still leave almost half of the explainable variance unexplained, raising the question of how to make further progress. Our finding that the VGG-based model performed equally well with only 20% of the images in the training set suggests that its performance was not limited by the amount of data available to learn the readout weights, which make for the bulk of the parameters in this model (<xref ref-type="table" rid="pcbi.1006897.t002">Table 2</xref>). Instead, the VGG-based model appears to be limited by a remaining mismatch between VGG features and V1 computation. This mismatch could potentially be reduced by using features from neural networks trained simultaneously on multiple ethologically relevant tasks beyond object categorization. The data-driven model reached its full performance only with the full training set, suggesting that learning the nonlinear feature space is the bottleneck. In this case, pooling over a larger number of neurons or recording longer from the same neurons should improve performance because most of the parameters are in the shared feature space (<xref ref-type="table" rid="pcbi.1006897.t002">Table 2</xref>) and this number is independent of the number of neurons being modeled.</p>
<p>We conclude that previous attempts to describe the basic computations that different types of neurons in primary visual cortex perform (e.g. “edge detection”) do not account for the complexity of multi-layer nonlinear computations that are necessary for the performance boost achieved with CNNs. Although these models, which so far best describe these computations, are complex and lack a concise intuitive description, they can be obtained by a simple principle: optimize a network to solve an ecologically relevant task (object categorization) and use the hidden representations of such a network. For future work, combining data- and goal-driven models and incorporating the recurrent lateral and feedback connections of the neocortex promise to provide a framework for incrementally unravelling the nonlinear computations of V1 neurons.</p>
</sec>
<sec id="sec012" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec013">
<title>Ethics statement</title>
<p>All behavioral and electrophysiological data were obtained from two healthy, male rhesus macaque (Macaca mulatta) monkeys aged 12 and 9 years and weighing 12 and 10 kg, respectively, during the time of study. All experimental procedures complied with guidelines of the NIH and were approved by the Baylor College of Medicine Institutional Animal Care and Use Committee (permit number: AN-4367). Animals were housed individually in a large room located adjacent to the training facility, along with around ten other monkeys permitting rich visual, olfactory and auditory interactions, on a 12h light/dark cycle. Regular veterinary care and monitoring, balanced nutrition and environmental enrichment were provided by the Center for Comparative Medicine of Baylor College of Medicine. Surgical procedures on monkeys were conducted under general anesthesia following standard aseptic techniques. To ameliorate pain after surgery, analgesics were given for 7 days. Animals were not sacrificed after the experiments.</p>
</sec>
<sec id="sec014">
<title>Electrophysiological recordings</title>
<p>We performed non-chronic recordings using a 32-channel linear silicon probe (NeuroNexus V1x32-Edge-10mm-60-177). The surgical methods and recording protocol were described previously [<xref ref-type="bibr" rid="pcbi.1006897.ref058">58</xref>]. Briefly, form-specific titanium recording chambers and headposts were implanted under full anesthesia and aseptic conditions. The bone was originally left intact and only prior to recordings, small trephinations (2 mm) were made over medial primary visual cortex at eccentricities ranging from 1.4 to 3.0 degrees of visual angle. Recordings were done within two weeks of each trephination. Probes were lowered using a Narishige Microdrive (MO-97) and a guide tube to penetrate the dura. Care was taken to lower the probe slowly, not to penetrate the cortex with the guide tube and to minimize tissue compression (for a detailed description of the procedure, see [<xref ref-type="bibr" rid="pcbi.1006897.ref058">58</xref>]).</p>
</sec>
<sec id="sec015">
<title>Data acquisition and spike sorting</title>
<p>Electrophysiological data were collected continuously as broadband signal (0.5Hz–16kHz) digitized at 24 bits as described previously [<xref ref-type="bibr" rid="pcbi.1006897.ref059">59</xref>]. Our spike sorting methods are based on [<xref ref-type="bibr" rid="pcbi.1006897.ref060">60</xref>], code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/aecker/moksm" xlink:type="simple">https://github.com/aecker/moksm</ext-link>, but with adaptations to the novel type of silicon probe as described previously [<xref ref-type="bibr" rid="pcbi.1006897.ref058">58</xref>]. Briefly, we split the linear array of 32 channels into 14 groups of 6 adjacent channels (with a stride of two), which we treated as virtual electrodes for spike detection and sorting. Spikes were detected when channel signals crossed a threshold of five times the standard deviation of the noise. After spike alignment, we extracted the first three principal components of each channel, resulting in an 18-dimensional feature space used for spike sorting. We fitted a Kalman filter mixture model [<xref ref-type="bibr" rid="pcbi.1006897.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref062">62</xref>] to track waveform drift typical for non-chronic recordings. The shape of each cluster was modeled with a multivariate <italic>t</italic>-distribution (<italic>df</italic> = 5) with a ridge-regularized covariance matrix. The number of clusters was determined based on a penalized average likelihood with a constant cost per additional cluster [<xref ref-type="bibr" rid="pcbi.1006897.ref060">60</xref>]. Subsequently, we used a custom graphical user interface to manually verify single-unit isolation by assessing the stability of the units (based on drifts and health of the cells throughout the session), identifying a refractory period, and inspecting the scatter plots of the pairs of channel principal components.</p>
</sec>
<sec id="sec016">
<title>Visual stimulation and eye tracking</title>
<p>Visual stimuli were rendered by a dedicated graphics workstation and displayed on a 19” CRT monitor (40 × 30 cm) with a refresh rate of 100 Hz at a resolution of 1600 × 1200 pixels and a viewing distance of 100 cm (resulting in ∼70 px/deg). The monitors were gamma-corrected to have a linear luminance response profile. A camera-based, custom-built eye tracking system verified that monkeys maintained fixation within ∼ 0.42 degrees around the target. Offline analysis showed that monkeys typically fixated much more accurately. The monkeys were trained to fixate on a red target of ∼ 0.15 degrees in the middle of the screen. After they maintained fixation for 300 ms, a visual stimulus appeared. If the monkeys fixated throughout the entire stimulus period, they received a drop of juice at the end of the trial.</p>
</sec>
<sec id="sec017">
<title>Receptive field mapping</title>
<p>At the beginning of each session, we first mapped receptive fields. We used a sparse random dot stimulus for receptive field mapping. A single dot of size 0.12 degrees of visual field was presented on a uniform gray background, changing location and color (black or white) randomly every 30 ms. Each trial lasted for two seconds. We obtained multi-unit receptive field profiles for every channel using reverse correlation. We then estimated the population receptive field location by fitting a 2D Gaussian to the spike-triggered average across channels at the time lag that maximizes the signal-to-noise-ratio. We subsequently placed our natural image stimulus at this location.</p>
</sec>
<sec id="sec018">
<title>Natural image stimulus</title>
<p>We used a set of 1450 grayscale images as well as four texturized versions of each image. We used grayscale images to avoid the complexity of dealing with color and focus on spatial image statistics. The texturized stimuli allowed us to vary the degree of naturalness, ranging from relatively simple, local statistics to very realistic textures capturing image statistics over spatial scales covering both classical and at least parts of the extra-classical receptive field of neurons. The images were taken from ImageNet [<xref ref-type="bibr" rid="pcbi.1006897.ref044">44</xref>], converted to grayscale and rescaled to 256 × 256 pixels. We generated textures with different degrees of naturalness by capturing different levels of higher-order correlations from a local to a global scale by using a parametric model for texture synthesis [<xref ref-type="bibr" rid="pcbi.1006897.ref045">45</xref>]. This texture model uses summary statistics of feature activations in different layers of the VGG-19 network [<xref ref-type="bibr" rid="pcbi.1006897.ref028">28</xref>] as parameters for the texture. The lowest-level model uses only the statistics of layer conv1_1. We refer to it as the “conv1” model. The next one uses statistics of conv1_1 and conv2_1 (referred to as conv2), and so on for conv3 and conv4. Due to the increasing level of nonlinearity of the VGG-19 features and their increasing receptive field sizes with depth, the textures synthesized from these models become increasingly more natural (see <xref ref-type="fig" rid="pcbi.1006897.g001">Fig 1</xref> and [<xref ref-type="bibr" rid="pcbi.1006897.ref045">45</xref>] for more examples)</p>
<p>To synthesize the textures, we start with a random white noise image and iteratively refine pixels via gradient descent such that the resulting image matches the feature statistics of the original image [<xref ref-type="bibr" rid="pcbi.1006897.ref045">45</xref>]. For displaying and further analyses, we cropped the central 140 pixels of each image, which corresponds to 2 degrees of visual angle.</p>
<p>The entire data set contains 1450 × 5 = 7250 images (original plus synthesized). During each trial, 29 images were displayed, each for 60 ms, with no blanks in between (<xref ref-type="fig" rid="pcbi.1006897.g001">Fig 1B</xref>). We chose this fast succession of images to maximize the number of images we can get through in a single experiment, resulting in a large training set for model fitting. The short presentation times also mean that the responses we observe are mainly feedforward, since feedback processes take some time to be engaged. Each image was masked by a circular mask with a diameter of 2 degrees (140 px) and a soft fade-out starting at a diameter of 1 degree:
<disp-formula id="pcbi.1006897.e002"><alternatives><graphic id="pcbi.1006897.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mn>0</mml:mn> <mml:mo>&lt;</mml:mo> <mml:mi>r</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn><mml:mspace width="2pt"/><mml:mtext>cos</mml:mtext><mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mi>π</mml:mi> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>r</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>Images were randomized such that consecutive images were not of the same type or synthesized from the same image. A full pass through the dataset took 250 successful trials, after which it was traversed again in a new random order. Images were repeated between one and four times, depending on how many trials the monkeys completed in each session.</p>
</sec>
<sec id="sec019">
<title>Dataset and inclusion criteria</title>
<p>We recorded a total of 307 neurons in 23 recording sessions. We did not consider six of these sessions, for which we did not obtain enough trials to have at least two repetitions for each image. In the remaining 17 sessions, we quantified the fraction of total variance of each neuron attributable to the stimulus by computing the ratio of explainable and total variance (grey bars in <xref ref-type="fig" rid="pcbi.1006897.g008">Fig 8</xref>):
<disp-formula id="pcbi.1006897.e003"><alternatives><graphic id="pcbi.1006897.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>noise</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:mtext>Var</mml:mtext> <mml:mo>[</mml:mo> <mml:mi>y</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
The explainable variance is the total variance minus the variance of the observation noise. We estimated the variance of the observation noise, <inline-formula id="pcbi.1006897.e004"><alternatives><graphic id="pcbi.1006897.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>noise</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, by averaging (across images) the variance (across repetitions) of responses to the same stimulus:
<disp-formula id="pcbi.1006897.e005"><alternatives><graphic id="pcbi.1006897.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>noise</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="normal">E</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>x</italic><sub><italic>j</italic></sub> is the <italic>j</italic><sup>th</sup> image and <italic>y</italic><sub><italic>i</italic></sub> the response to the <italic>i</italic><sup>th</sup> repetition. We discarded neurons with a ratio of explainable-to-total variance (see <xref ref-type="disp-formula" rid="pcbi.1006897.e003">Eq 3</xref>) smaller than 0.15, yielding 166 isolated neurons (monkey A: 51, monkey B: 115) recorded in 17 sessions with an average explainable variance of 0.285. Monkey A had only sessions with two repetitions while Monkey B had four repetitions per image.</p>
</sec>
<sec id="sec020">
<title>Image preprocessing</title>
<p>All images were contrast-matched before displaying them on the screen. To do so, we rescaled the pixel intensities of all images such that the central, unmasked 1° (70 pixels) of each image had the same mean and standard deviation. We set the mean to 128 (same as the gray background) and the standard deviation to the average standard deviation across images. Any pixels falling outside the range of [0, 255] after this procedure were cropped to this range.</p>
<p>Prior to model fitting, we additionally cropped the central 80 pixels (1.1°) of the 140-pixel (2°) images shown to the monkey. For most of the analyses presented in this paper, we sub-sampled these crops to half their size (40 × 40) and z-scored them. For the input resolution control (<xref ref-type="fig" rid="pcbi.1006897.g005">Fig 5</xref>), we resampled with bicubic interpolation the original 80 × 80 crops to 60 × 60, 40 × 40, and 27 × 27 for scales 1.5, 1, 0.67, respectively.</p>
</sec>
<sec id="sec021">
<title>GLM with pre-trained CNN features</title>
<p>Our proposed model consists of two parts: feature extraction and a generalized linear model (GLM; <xref ref-type="fig" rid="pcbi.1006897.g003">Fig 3</xref>). The features are the output maps Φ(<italic>x</italic>) of convolutional layers of VGG-19 [<xref ref-type="bibr" rid="pcbi.1006897.ref028">28</xref>] to a stimulus image <italic>x</italic>, followed by a batch normalization layer. We perform this normalization to ensure that the activations of each feature map have zero mean and unit variance (before ReLU), which is important because the readout weights are regularized by an <italic>L</italic><sub>1</sub> penalty and having input features with different variances would implicitly apply different penalties on their corresponding readout weights.</p>
<p>We fit a separate GLM for each convolutional layer of VGG-19. The GLM consists of linear fully connected weights <italic>w</italic><sub><italic>ijk</italic></sub> for each neuron that compute a dot product with the input feature maps Φ<sub><italic>ijk</italic></sub>(<italic>x</italic>), a static output nonlinearity <italic>f</italic> (also known as the inverse of the link function), and a Poisson noise model used for training. Here, <italic>i</italic> and <italic>j</italic> index space, while <italic>k</italic> indexes feature maps (denoted as depth in <xref ref-type="fig" rid="pcbi.1006897.g003">Fig 3</xref>). The spiking rate of a given neuron <italic>r</italic> will follow:
<disp-formula id="pcbi.1006897.e006"><alternatives><graphic id="pcbi.1006897.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mo>∑</mml:mo> <mml:msub><mml:mo>Φ</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>Additionally, three regularization terms were applied to the weights:</p>
<list list-type="order">
<list-item>
<p><bold>Sparsity</bold>: Most weights need to be zero since we expect the spatial pooling to be localized. We use the <italic>L</italic><sub>1</sub> norm of the weights:
<disp-formula id="pcbi.1006897.e007"><alternatives><graphic id="pcbi.1006897.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mtext>sparse</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mtext>sparse</mml:mtext></mml:msub> <mml:mo>∑</mml:mo> <mml:mrow><mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
</list-item>
<list-item>
<p><bold>Spatial smoothness</bold>: Together with sparseness, spatial smoothness encourages spatial locality by imposing continual regular changes in space. We computed this by an <italic>L</italic><sub>2</sub> penalty on the Laplacian of the weights:
<disp-formula id="pcbi.1006897.e008"><alternatives><graphic id="pcbi.1006897.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mtext>Laplace</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mtext>Laplace</mml:mtext></mml:msub> <mml:mroot><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mo>:</mml:mo> <mml:mo>,</mml:mo> <mml:mo>:</mml:mo> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>*</mml:mo> <mml:mi>L</mml:mi> <mml:msubsup><mml:mo>)</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow/></mml:mroot> <mml:mo>,</mml:mo> <mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>L</mml:mi> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd><mml:mn>4</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
</list-item>
<list-item>
<p><bold>Group sparsity</bold> encourages our model to pool from a small set of feature maps to explain each neuron’s responses:
<disp-formula id="pcbi.1006897.e009"><alternatives><graphic id="pcbi.1006897.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mtext>group</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mtext>group</mml:mtext></mml:msub> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mroot><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow/></mml:mroot></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
</list-item>
</list>
<p>Considering the recorded image-response pair (<italic>x</italic>, <italic>y</italic>) for one neuron, the resulting loss function is given by:
<disp-formula id="pcbi.1006897.e010"><alternatives><graphic id="pcbi.1006897.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mo>∑</mml:mo> <mml:mi>y</mml:mi><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mtext>sparse</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mtext>Laplace</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mtext>group</mml:mtext></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where the sum runs over samples (image, response pairs).</p>
<p>We fit the model by minimizing the loss using the Adam optimizer [<xref ref-type="bibr" rid="pcbi.1006897.ref063">63</xref>] on a training set consisting of 80% of the data, and reported performance on the remaining 20%. We cross-validated the hyperparameters λ<sub>sparse</sub>, λ<sub>Laplace</sub>, λ<sub>group</sub> for each neuron independently by performing a grid search over four logarithmically spaced values for each hyperparameter. The validation was done on 20% of the training data. The optimal hyperparameter values obtained on the validation set where λ<sub>Laplace</sub> = 0.1, λ<sub>sparse</sub> = 0.01, λ<sub>group</sub> = 0.001. When fitting models, we used the same split of data for training, validation, and testing across all models.</p>
</sec>
<sec id="sec022">
<title>Data-driven convolutional neural network model</title>
<p>We followed the results of [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>] and use their best-performing architecture that obtained state-of-the-art performance on a public dataset [<xref ref-type="bibr" rid="pcbi.1006897.ref038">38</xref>]. Like our VGG-based model, this model also consisted of convolutional feature extraction followed by a GLM, the difference being that here the convolutional feature space was learned from neural data instead of having been trained on object recognition. The feature extraction architecture consisted of convolutional layers with filters of size 13 × 13 px for the first layer and 3 × 3 px for the subsequent layers. Each layer had 32 feature maps (<xref ref-type="fig" rid="pcbi.1006897.g007">Fig 7A</xref>) and exponential linear units (ELU [<xref ref-type="bibr" rid="pcbi.1006897.ref064">64</xref>])
<disp-formula id="pcbi.1006897.e011"><alternatives><graphic id="pcbi.1006897.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mi>L</mml:mi> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>x</mml:mi></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>x</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>x</mml:mi></mml:mrow> <mml:mo>≤</mml:mo> <mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
as nonlinearities with batch normalization [<xref ref-type="bibr" rid="pcbi.1006897.ref065">65</xref>] to facilitate training in between the layers. As in the original publication [<xref ref-type="bibr" rid="pcbi.1006897.ref040">40</xref>], we regularized the convolutional filters by imposing smoothness constraints on the first layer and group sparseness on the subsequent layers. A notable difference to our VGG-based GLM is that here the readout weights are factorized in space and feature maps:
<disp-formula id="pcbi.1006897.e012"><alternatives><graphic id="pcbi.1006897.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>v</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>u</italic><sub><italic>ij</italic></sub> is a spatial mask and <italic>v</italic><sub><italic>k</italic></sub> a set of feature pooling weights. We fitted models with increasing number of convolutional layers (one to five). We found that optimizing the final nonlinearity, <italic>f</italic>(<italic>x</italic>), of each neuron was important for optimal performance of the data-driven CNN. To do so, we took the following approach: we split <italic>f</italic>(<italic>x</italic>) into two components:
<disp-formula id="pcbi.1006897.e013"><alternatives><graphic id="pcbi.1006897.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>h</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <italic>g</italic>(<italic>x</italic>) is ELU shifted to the right and up by one unit (to make it non-negative—firing rates are non-negative):
<disp-formula id="pcbi.1006897.e014"><alternatives><graphic id="pcbi.1006897.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mi>L</mml:mi> <mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
and <italic>h</italic> is a non-negative, piecewise linear function:
<disp-formula id="pcbi.1006897.e015"><alternatives><graphic id="pcbi.1006897.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>h</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
Here, <italic>α</italic><sub><italic>i</italic></sub> are parameters learned jointly with the remaining weights of the network and the <italic>t</italic><sub><italic>i</italic></sub> are a set of ‘tent’ basis functions to create a piecewise linear function with interpolation points <italic>x</italic><sub><italic>i</italic></sub> = −3, −2.82, …, 6 (i.e. Δ<italic>x</italic> = 0.18):
<disp-formula id="pcbi.1006897.e016"><alternatives><graphic id="pcbi.1006897.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mspace width="2pt"/><mml:mtext>min</mml:mtext><mml:mspace width="2pt"/> <mml:mo>(</mml:mo><mml:mtext>max</mml:mtext><mml:mspace width="2pt"/> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mspace width="2pt"/><mml:mtext>max</mml:mtext><mml:mspace width="2pt"/> <mml:mo>(</mml:mo> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mi>x</mml:mi></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mstyle> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
We regularize the output nonlinearity by penalizing the <italic>L</italic><sub>2</sub> norm of the first and second discrete finite differences of <italic>α</italic><sub><italic>i</italic></sub> to encourage <italic>h</italic> to be close to 1 and smooth:
<disp-formula id="pcbi.1006897.e017"><alternatives><graphic id="pcbi.1006897.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
Note that we applied this optimization of the output nonlinearity only to the data-driven model, as doing the same for the VGG-based model did not improve performance. One potential reason for this difference is that the VGG-based model has a much larger number of feature maps (256 for layer conv3_1) that each neuron can pool from.</p>
</sec>
<sec id="sec023">
<title>Linear nonlinear poisson model (LNP)</title>
<p>We implemented a simple regularized LNP Model [<xref ref-type="bibr" rid="pcbi.1006897.ref046">46</xref>]. This model is fitted for each neuron separately and consists of two simple stages: The first one is a linear filter <bold>w</bold> with the same dimensions as the input images. The second is a pointwise exponential function as nonlinearity that converts the filter output into a non-negative spike rate. The LNP assumes spike count generation through a Poisson process, so we minimize a Poisson loss (negative log-likelihood) to obtain the kernels of each neuron (see first term of <xref ref-type="disp-formula" rid="pcbi.1006897.e018">Eq 17</xref> below). Additionally, we imposed two regularization constraints that we cross-validated: smoothness (<xref ref-type="disp-formula" rid="pcbi.1006897.e008">Eq 7</xref>) and sparsity (<xref ref-type="disp-formula" rid="pcbi.1006897.e007">Eq 6</xref>). With the same <italic>M</italic> image-response pairs (<bold>x</bold>, <italic>y</italic>) of the training set that we used for all other models, we optimized the following loss function:
<disp-formula id="pcbi.1006897.e018"><alternatives><graphic id="pcbi.1006897.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>L</mml:mi> <mml:mi>N</mml:mi> <mml:mi>P</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>s</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>a</mml:mi> <mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>a</mml:mi> <mml:mi>c</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula></p>
</sec>
<sec id="sec024">
<title>Gabor filter bank model (GFB)</title>
<p>Varying versions of the Gabor filter bank model (GFB) have been used in classical work on system identification [<xref ref-type="bibr" rid="pcbi.1006897.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006897.ref066">66</xref>]. This model convolves the image with quadrature pairs of Gabor filters with varying scales, frequencies, aspect ratios, and orientations. Each quadrature pair consists of an ‘even’ (cosine/symmetric) and an ‘odd’ (sine/antisymmetric) Gabor filter and produces three feature maps: the results of the convolution with the two filters (‘even’ and ‘odd’ features) and an ‘energy’ feature, which is the spectral power of each pair (i.e. sum of the squares). Thus, this model allows for modeling simple and complex cells and linear combinations thereof.</p>
<p>The Gabor filters obeyed the following equations with <italic>x</italic> and <italic>y</italic> representing spatial dimensions:
<disp-formula id="pcbi.1006897.e019"><alternatives><graphic id="pcbi.1006897.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>g</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>φ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi> <mml:mrow><mml:mo>′</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>′</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>]</mml:mo><mml:mspace width="2pt"/><mml:mtext>cos</mml:mtext><mml:mspace width="2pt"/><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mi>f</mml:mi> <mml:msup><mml:mi>x</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
where
<disp-formula id="pcbi.1006897.e020"><alternatives><graphic id="pcbi.1006897.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>x</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msup><mml:mi>y</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mspace width="2pt"/><mml:mtext>cos</mml:mtext><mml:mspace width="2pt"/><mml:mi>θ</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mo form="prefix">sin</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo form="prefix">sin</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mspace width="2pt"/><mml:mtext>cos</mml:mtext><mml:mspace width="2pt"/><mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
The standard deviation (<italic>σ</italic>) represents the scale of the Gaussian envelope, the aspect ratio (<italic>γ</italic>) specifies the ellipticity of the envelope, the spatial frequency (<italic>f</italic>) quantifies the number of sinusoidal cycles divided by the width of the Gaussian aperture (∼4<italic>σ</italic>), The sinusoidal grating is determined by an orientation (<italic>θ</italic>) and phase (<italic>φ</italic>). To form quadrature pairs we set <italic>φ</italic> to 0 and <italic>π</italic>/2 for even and odd filters, respectively. We set the kernel size of every Gabor filter to the minimum of the input image size and the closest integer to 4<italic>σ</italic>/<italic>γ</italic> for both spatial dimensions.</p>
<p>To compute the even and odd feature maps, we convolved the input image with each Gabor filter using strided convolutions (⊗) with stride <italic>s</italic>:
<disp-formula id="pcbi.1006897.e021"><alternatives><graphic id="pcbi.1006897.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:msub><mml:mi>E</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:mo>⊗</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
<disp-formula id="pcbi.1006897.e022"><alternatives><graphic id="pcbi.1006897.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:mo>⊗</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>φ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>π</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
We then computed the energy features as follows:
<disp-formula id="pcbi.1006897.e023"><alternatives><graphic id="pcbi.1006897.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
The full feature space of this model Φ<sub><italic>Gabor</italic></sub>(<italic>x</italic>) is a concatenation of triplets of even, odd, and energy features of every Gabor filter. The filter bank consisted of <italic>N</italic><sub><italic>s</italic></sub> filter sizes, <italic>N</italic><sub><italic>f</italic></sub> spatial frequencies per size, <italic>N</italic><sub><italic>θ</italic></sub> orientations and <italic>N</italic><sub><italic>γ</italic></sub> aspect ratios. The spatial frequencies depended on the size of the envelope: <italic>f</italic><sub><italic>n</italic></sub> = <italic>n</italic>/4<italic>σ</italic> with <italic>n</italic> = 1, …, <italic>N</italic><sub><italic>f</italic></sub>, which means for <italic>n</italic> = 3 we used Gabors with 1, 2 and 3 cycles. Aspect ratios ranged from 0.5 to 1 with equal spacing, except for <italic>N</italic><sub><italic>γ</italic></sub> = 1 where we used an aspect ratio of <italic>γ</italic> = 1. As for the GLM with VGG features, we fit a linear readout on top of this feature space (<xref ref-type="disp-formula" rid="pcbi.1006897.e006">Eq 5</xref>), followed by a shifted ELU output nonlinearity (<xref ref-type="disp-formula" rid="pcbi.1006897.e014">Eq 13</xref>).</p>
<p>To train the model, we minimized a Poisson loss and regularized the readout weights to be sparse (i.e. first two terms in <xref ref-type="disp-formula" rid="pcbi.1006897.e010">Eq 9</xref>); we did not enforce smoothness or group sparsity here, as they mainly improve interpretability but do not affect performance. We determined the hyperparameters of the Gabor filter bank by running a search over a number of parameter combinations and evaluating the performance of each model on the validation set. We converged to the following values: three different sizes (<italic>N</italic><sub><italic>s</italic></sub> = 3: 6 px/0.17°, 11 px/0.31° and 21 px/0.60°), three different spatial frequencies (<italic>N</italic><sub><italic>f</italic></sub> = 3: <italic>f</italic> = 1, 2, 3) per size, one aspect ratio (<italic>N</italic><sub><italic>γ</italic></sub> = <italic>γ</italic> = 1), eight orientations (<italic>N</italic><sub><italic>θ</italic></sub> = 8), convolutional stride of 6 for all filters, and <italic>L</italic><sub>1</sub> regularization parameter <italic>α</italic> = 0.05. Notably, including multiple different aspect ratios did not improve performance, presumably because it increased the dimensionality of the feature space which harmed generalization.</p>
</sec>
<sec id="sec025">
<title>Number of parameters to be learned</title>
<p>The parameters we fit for each of the models belong either to a shared set for all neurons (the core), or are specific to each neuron (the readout). <xref ref-type="table" rid="pcbi.1006897.t002">Table 2</xref> shows the number of parameters for each of the models and how many belong to either core or readout. For both the LNP and GFB models, we learn only a readout from a fixed feature space for each neuron plus a bias. For the LNP we learn one channel of pixel intensities (40 × 40 + 1). For the GFB model, we have for each size <italic>N</italic><sub><italic>f</italic></sub> <italic>N</italic><sub><italic>θ</italic></sub> channels (3 × 8 = 24), and each filter produced a feature output of size ⌊1 + (40 − <italic>size</italic>)/<italic>stride</italic>⌋. With <italic>N</italic><sub><italic>s</italic></sub> = 3 we got sizes 6, 11, and 21 so the output features have size 6, 5, and 4, respectively. Since each Gabor filter quadrature pair produces odd, even, and energy feature spaces, the total dimensionality is 3 × 24 × (6<sup>2</sup> + 5<sup>2</sup> + 4<sup>2</sup>) + 1 = 5545. For the three-layer CNN, we have 32 channels in all layers (32 × 3 biases) and filters with sizes 13 × 13 × 32, 3 × 3 × 32 × 32, and 3 × 3 × 32 × 32, resulting in 23, 963 core parameters. The output feature space for an image is 28 × 28 × 32 (reduced from 40 × 40 due to the padding of the convolutions: no padding in first layer, zero padding in second and third). With a factorized readout and a bias, the readout per neuron is then 28 × 28 + 32 plus a bias. In addition, our point-wise output nonlinearity has 50 parameters. Thus, overall we have 867 readout parameters per neuron for this CNN model.</p>
<p>For the VGG-based model, although we do not learn the feature space, we do learn batch normalization parameters at the output of the last convolutional layer. For the model that used conv3_1 (256 feature channels) this means learning scale and bias parameters common to all neurons: 2 × 256 = 512 for the core. For a 40 × 40 input, the output of the feature space is 10 × 10 × 256 (due to downsampling twice via max pooling). Here, we learn a dense readout and a bias, so the readout per neuron has 10 × 10 × 256 + 1 = 25, 601 parameters.</p>
</sec>
<sec id="sec026">
<title>Performance evaluation</title>
<p>We measured the performance of all models with the fraction of explainable variance explained (<italic>FEV</italic>). That is, the ratio between the variance accounted for by the model (variance explained) and the explainable variance (numerator in <xref ref-type="disp-formula" rid="pcbi.1006897.e003">Eq 3</xref>). The explainable variance is lower than the total variance, because observation noise prevents even a perfect model from accounting for all variance. We compute <italic>FEV</italic> as
<disp-formula id="pcbi.1006897.e024"><alternatives><graphic id="pcbi.1006897.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi> <mml:mi>E</mml:mi> <mml:mi>V</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>∑</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>noise</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:mi>V</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>noise</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
where <inline-formula id="pcbi.1006897.e025"><alternatives><graphic id="pcbi.1006897.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> represents the model predictions, <italic>y</italic> the observed spike counts, and the level of observation noise, <inline-formula id="pcbi.1006897.e026"><alternatives><graphic id="pcbi.1006897.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006897.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mtext>noise</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> is defined in <xref ref-type="disp-formula" rid="pcbi.1006897.e005">Eq 4</xref> above.</p>
</sec>
<sec id="sec027">
<title>Implementation details</title>
<p>We implemented all models in TensorFlow [<xref ref-type="bibr" rid="pcbi.1006897.ref067">67</xref>]. We optimized them with Adam [<xref ref-type="bibr" rid="pcbi.1006897.ref063">63</xref>] using mini-batches of size 256, and early stopping: we evaluated performance on the validation set every 100 training steps, and after ten iterations of no improvement, we decayed the learning rate by a factor of three and repeated this three times. The learning rate at the beginning of the optimization was cross-validated for the goal-driven models and set to 1e-4 for the others as this value always worked best.</p>
<sec id="sec028">
<title>Tools</title>
<p>We managed our data and kept track of models, parameters, and performance using DataJoint [<xref ref-type="bibr" rid="pcbi.1006897.ref068">68</xref>]. In addition, we used Numpy/Scipy [<xref ref-type="bibr" rid="pcbi.1006897.ref069">69</xref>], Matplotlib [<xref ref-type="bibr" rid="pcbi.1006897.ref070">70</xref>], Seaborn [<xref ref-type="bibr" rid="pcbi.1006897.ref071">71</xref>], Jupyter [<xref ref-type="bibr" rid="pcbi.1006897.ref072">72</xref>], Tensorflow [<xref ref-type="bibr" rid="pcbi.1006897.ref067">67</xref>], and Docker [<xref ref-type="bibr" rid="pcbi.1006897.ref073">73</xref>]. The code to fit all models is available in this repository: \url{<ext-link ext-link-type="uri" xlink:href="https://github.com/sacadena/Cadena2019PlosCB" xlink:type="simple">https://github.com/sacadena/Cadena2019PlosCB</ext-link>}.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Philipp Berens and James Cotton for valuable discussions and help during the early stages of this project. We thank Tori Shinn for help with animal training and neural recordings.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006897.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Do we know what the early visual system does?</article-title> <source>The Journal of neuroscience</source>. <year>2005</year>;<volume>25</volume>(<issue>46</issue>):<fpage>10577</fpage>–<lpage>10597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3726-05.2005" xlink:type="simple">10.1523/JNEUROSCI.3726-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16291931</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>. <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of physiology</source>. <year>1959</year>;<volume>148</volume>(<issue>3</issue>):<fpage>574</fpage>–<lpage>591</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1959.sp006308" xlink:type="simple">10.1113/jphysiol.1959.sp006308</ext-link></comment> <object-id pub-id-type="pmid">14403679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>. <article-title>Receptive fields and functional architecture of monkey striate cortex</article-title>. <source>The Journal of physiology</source>. <year>1968</year>;<volume>195</volume>(<issue>1</issue>):<fpage>215</fpage>–<lpage>243</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1968.sp008455" xlink:type="simple">10.1113/jphysiol.1968.sp008455</ext-link></comment> <object-id pub-id-type="pmid">4966457</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jones</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>LA</given-names></name>. <article-title>An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</article-title>. <source>Journal of neurophysiology</source>. <year>1987</year>;<volume>58</volume>(<issue>6</issue>):<fpage>1233</fpage>–<lpage>1258</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1987.58.6.1233" xlink:type="simple">10.1152/jn.1987.58.6.1233</ext-link></comment> <object-id pub-id-type="pmid">3437332</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Half-squaring in responses of cat striate cells</article-title>. <source>Visual neuroscience</source>. <year>1992</year>;<volume>9</volume>(<issue>05</issue>):<fpage>427</fpage>–<lpage>443</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S095252380001124X" xlink:type="simple">10.1017/S095252380001124X</ext-link></comment> <object-id pub-id-type="pmid">1450099</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Adelson</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Bergen</surname> <given-names>JR</given-names></name>. <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>JOSA A</source>. <year>1985</year>;<volume>2</volume>(<issue>2</issue>):<fpage>284</fpage>–<lpage>299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1364/JOSAA.2.000284" xlink:type="simple">10.1364/JOSAA.2.000284</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Complex Pattern Selectivity in Macaque Primary Visual Cortex Revealed by Large-Scale Two-Photon Imaging</article-title>. <source>Current Biology</source>. <year>2018</year>;<volume>28</volume>(<issue>1</issue>):<fpage>38</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2017.11.039" xlink:type="simple">10.1016/j.cub.2017.11.039</ext-link></comment> <object-id pub-id-type="pmid">29249660</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>How close are we to understanding V1?</article-title> <source>Neural computation</source>. <year>2005</year>;<volume>17</volume>(<issue>8</issue>):<fpage>1665</fpage>–<lpage>1699</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/0899766054026639" xlink:type="simple">10.1162/0899766054026639</ext-link></comment> <object-id pub-id-type="pmid">15969914</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Talebi</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <article-title>Natural versus synthetic stimuli for estimating receptive field models: a comparison of predictive robustness</article-title>. <source>The Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>5</issue>):<fpage>1560</fpage>–<lpage>1576</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4661-12.2012" xlink:type="simple">10.1523/JNEUROSCI.4661-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22302799</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eichhorn</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sinz</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>Natural image coding in V1: how much use is orientation selectivity?</article-title> <source>PLoS computational biology</source>. <year>2009</year>;<volume>5</volume>(<issue>4</issue>):<fpage>e1000336</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000336" xlink:type="simple">10.1371/journal.pcbi.1000336</ext-link></comment> <object-id pub-id-type="pmid">19343216</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>What Is the Goal of Sensory Coding?</article-title> <source>Neural Computation</source>. <year>1994</year>;<volume>6</volume>(<issue>4</issue>):<fpage>559</fpage>–<lpage>601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1994.6.4.559" xlink:type="simple">10.1162/neco.1994.6.4.559</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <etal>et al</etal>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Normalization of Cell Responses in Cat Striate Cortex</article-title>. <source>Visual neuroscience</source>. <year>1992</year>;<volume>9</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>197</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0952523800009640" xlink:type="simple">10.1017/S0952523800009640</ext-link></comment> <object-id pub-id-type="pmid">1504027</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Bethge M, Simoncelli EP, Sinz FH. Hierarchical Modeling of Local Image Features through <italic>L</italic>_<italic>p</italic>-Nested Symmetric Distributions. In: Advances in neural information processing systems; 2009. p. 1696–1704.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cavanaugh</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Bair</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Nature and interaction of signals from the receptive field center and surround in macaque V1 neurons</article-title>. <source>Journal of neurophysiology</source>. <year>2002</year>;<volume>88</volume>(<issue>5</issue>):<fpage>2530</fpage>–<lpage>2546</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00692.2001" xlink:type="simple">10.1152/jn.00692.2001</ext-link></comment> <object-id pub-id-type="pmid">12424292</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cavanaugh</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Bair</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Selectivity and spatial distribution of signals from the receptive field surround in macaque V1 neurons</article-title>. <source>Journal of neurophysiology</source>. <year>2002</year>;<volume>88</volume>(<issue>5</issue>):<fpage>2547</fpage>–<lpage>2556</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00693.2001" xlink:type="simple">10.1152/jn.00693.2001</ext-link></comment> <object-id pub-id-type="pmid">12424293</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bair</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Cavanaugh</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>. <article-title>Time course and time-distance relationships for surround suppression in macaque V1 neurons</article-title>. <source>Journal of Neuroscience</source>. <year>2003</year>;<volume>23</volume>(<issue>20</issue>):<fpage>7690</fpage>–<lpage>7701</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.23-20-07690.2003" xlink:type="simple">10.1523/JNEUROSCI.23-20-07690.2003</ext-link></comment> <object-id pub-id-type="pmid">12930809</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2012</year>;<volume>13</volume>(<issue>1</issue>):<fpage>51</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3136" xlink:type="simple">10.1038/nrn3136</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>(<issue>6</issue>):<fpage>945</fpage>–<lpage>956</lpage>. <object-id pub-id-type="pmid">15953422</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Touryan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Felsen</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>. <article-title>Spatial structure of complex cell receptive fields measured with natural images</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>45</volume>(<issue>5</issue>):<fpage>781</fpage>–<lpage>791</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.01.029" xlink:type="simple">10.1016/j.neuron.2005.01.029</ext-link></comment> <object-id pub-id-type="pmid">15748852</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vintch</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>A convolutional subunit model for neuronal responses in macaque V1</article-title>. <source>The Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>44</issue>):<fpage>14829</fpage>–<lpage>14841</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2815-13.2015" xlink:type="simple">10.1523/JNEUROSCI.2815-13.2015</ext-link></comment> <object-id pub-id-type="pmid">26538653</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Willmore</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Prenger</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>MCK</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>The berkeley wavelet transform: a biologically inspired orthogonal wavelet transform</article-title>. <source>Neural computation</source>. <year>2008</year>;<volume>20</volume>(<issue>6</issue>):<fpage>1537</fpage>–<lpage>1564</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2007.05-07-513" xlink:type="simple">10.1162/neco.2007.05-07-513</ext-link></comment> <object-id pub-id-type="pmid">18194102</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title>. <source>Nature</source>. <year>1988</year>;<volume>331</volume>(<issue>6158</issue>):<fpage>679</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/331679a0" xlink:type="simple">10.1038/331679a0</ext-link></comment> <object-id pub-id-type="pmid">3344044</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4244" xlink:type="simple">10.1038/nn.4244</ext-link></comment> <object-id pub-id-type="pmid">26906502</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Donahue J, Jia Y, Vinyals O, Hoffman J, Zhang N, Tzeng E, et al. Decaf: A deep convolutional activation feature for generic visual recognition. In: International conference on machine learning; 2014. p. 647–655.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2014. p. 1717–1724.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems; 2012. p. 1097–1105.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations; 2015.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 770–778.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Huang G, Liu Z, Weinberger KQ, van der Maaten L. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 4700–4708.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref031">
<label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Kümmerer M, Theis L, Bethge M. Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet. In: ICLR Workshop; 2015.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ardila</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <etal>et al</etal>. <article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>12</issue>):<fpage>e1003963</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003963" xlink:type="simple">10.1371/journal.pcbi.1003963</ext-link></comment> <object-id pub-id-type="pmid">25521294</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MA</given-names></name>. <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>The Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>10014</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Seibert D, Yamins DL, Ardila D, Hong H, DiCarlo JJ, Gardner JL. A performance-optimized model of neural responses across the ventral visual stream. bioRxiv. 2016; p. 036475.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Prenger</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>MCK</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Nonlinear V1 responses to natural scenes revealed by neural network analysis</article-title>. <source>Neural Networks</source>. <year>2004</year>;<volume>17</volume>(<issue>5</issue>):<fpage>663</fpage>–<lpage>679</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2004.03.008" xlink:type="simple">10.1016/j.neunet.2004.03.008</ext-link></comment> <object-id pub-id-type="pmid">15288891</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Antolík</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hofer</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Bednar</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name>. <article-title>Model Constrained by Visual Hierarchy Improves Prediction of Neural Responses to Natural Scenes</article-title>. <source>PLOS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>6</issue>):<fpage>e1004927</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004927" xlink:type="simple">10.1371/journal.pcbi.1004927</ext-link></comment> <object-id pub-id-type="pmid">27348548</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref039">
<label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Batty E, Merel J, Brackbill N, Heitman A, Sher A, Litke A, et al. Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses. 2016.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Klindt D, Ecker AS, Euler T, Bethge M. Neural system identification for large populations separating “what” and “where”. In Advances in Neural Information Processing Systems; 2017. p. 3506-3516.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">McIntosh L, Maheswaranathan N, Nayebi A, Ganguli S, Baccus S. Deep learning models of the retinal response to natural scenes. In: Advances in Neural Information Processing Systems; 2016. p. 1369–1377.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Kindel WF, Christensen ED, Zylberberg J. Using deep learning to reveal the neural code for images in primary visual cortex. arXiv preprint arXiv:170606208. 2017.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Tang</surname> <given-names>S</given-names></name>. <article-title>Convolutional Neural Network Models of V1 Responses to Complex Patterns</article-title>. <source>Journal of computational neuroscience</source>. <year>2018</year>; p. <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Russakovsky</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Krause</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Satheesh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Imagenet large scale visual recognition challenge</article-title>. <source>International Journal of Computer Vision</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>–<lpage>252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11263-015-0816-y" xlink:type="simple">10.1007/s11263-015-0816-y</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref045">
<label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Gatys L, Ecker AS, Bethge M. Texture synthesis using convolutional neural networks. In: Advances in Neural Information Processing Systems; 2015. p. 262–270.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>. <article-title>Characterization of neural responses with stochastic stimuli</article-title>. <source>The cognitive neurosciences</source>. <year>2004</year>;<volume>3</volume>:<fpage>327</fpage>–<lpage>338</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daugman</surname> <given-names>JG</given-names></name>. <article-title>Two-dimensional spectral analysis of cortical receptive field profiles</article-title>. <source>Vision research</source>. <year>1980</year>;<volume>20</volume>(<issue>10</issue>):<fpage>847</fpage>–<lpage>856</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0042-6989(80)90065-6" xlink:type="simple">10.1016/0042-6989(80)90065-6</ext-link></comment> <object-id pub-id-type="pmid">7467139</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>WATSON</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The cortex transform- Rapid computation of simulated neural images</article-title>. <source>Computer vision, graphics, and image processing</source>. <year>1987</year>;<volume>39</volume>(<issue>3</issue>):<fpage>311</fpage>–<lpage>327</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0734-189X(87)80184-6" xlink:type="simple">10.1016/S0734-189X(87)80184-6</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Movshon</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>D</given-names></name>. <article-title>Receptive field organization of complex cells in the cat’s striate cortex</article-title>. <source>The Journal of physiology</source>. <year>1978</year>;<volume>283</volume>:<fpage>79</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1978.sp012488" xlink:type="simple">10.1113/jphysiol.1978.sp012488</ext-link></comment> <object-id pub-id-type="pmid">722592</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Gallant J, David S. The Neural Prediction Challenge;. <ext-link ext-link-type="uri" xlink:href="http://neuralprediction.berkeley.edu/" xlink:type="simple">http://neuralprediction.berkeley.edu/</ext-link>, last accessed on 10/02/2018.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hawken</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>R</given-names></name>. <article-title>Dynamics of Orientation Tuning in Macaque Primary Visual Cortex</article-title>. <source>Nature</source>. <year>1997</year>;<volume>387</volume>(<issue>6630</issue>):<fpage>281</fpage>–<lpage>284</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/387281a0" xlink:type="simple">10.1038/387281a0</ext-link></comment> <object-id pub-id-type="pmid">9153392</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Hawken</surname> <given-names>MJ</given-names></name>. <article-title>Orientation Selectivity in Macaque V1: Diversity and Laminar Dependence</article-title>. <source>The Journal of Neuroscience</source>. <year>2002</year>;<volume>22</volume>(<issue>13</issue>):<fpage>5639</fpage>–<lpage>5651</lpage>. <object-id pub-id-type="pmid">12097515</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Scientific reports</source>. <year>2016</year>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep27755" xlink:type="simple">10.1038/srep27755</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title>. <source>Annual Review of Vision Science</source>. <year>2015</year>;<volume>1</volume>:<fpage>417</fpage>–<lpage>446</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-vision-082114-035447" xlink:type="simple">10.1146/annurev-vision-082114-035447</ext-link></comment> <object-id pub-id-type="pmid">28532370</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Heeger</surname> <given-names>D</given-names></name>. <article-title>Computational model of cat striate physiology</article-title>. <source>Computational models of visual perception</source>. <year>1991</year>; p. <fpage>119</fpage>–<lpage>133</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Spillmann</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dresp-Langley</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tseng</surname> <given-names>Ch</given-names></name>. <article-title>Beyond the Classical Receptive Field: The Effect of Contextual Stimuli</article-title>. <source>Journal of Vision</source>. <year>2015</year>;<volume>15</volume>(<issue>9</issue>):<fpage>7</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.9.7" xlink:type="simple">10.1167/15.9.7</ext-link></comment> <object-id pub-id-type="pmid">26200888</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref057">
<label>57</label>
<mixed-citation publication-type="other" xlink:type="simple">Zeiler MD, Krishnan D, Taylor GW, Fergus R. Deconvolutional Networks. In: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference On. IEEE; 2010. p. 2528–2535.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Denfield</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Shinn</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Attentional fluctuations induce shared variability in macaque primary visual cortex</article-title>. <source>Nature communications</source>; <year>2018</year>, <volume>vol. 9</volume>, <issue>no 1</issue>, p. <fpage>2654</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Keliris</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Decorrelated neuronal firing in cortical microcircuits</article-title>. <source>science</source>. <year>2010</year>;<volume>327</volume>(<issue>5965</issue>):<fpage>584</fpage>–<lpage>587</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1179867" xlink:type="simple">10.1126/science.1179867</ext-link></comment> <object-id pub-id-type="pmid">20110506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Subramaniyan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Denfield</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Cadwell</surname> <given-names>CR</given-names></name>, <etal>et al</etal>. <article-title>State dependence of noise correlations in macaque primary visual cortex</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>(<issue>1</issue>):<fpage>235</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.02.006" xlink:type="simple">10.1016/j.neuron.2014.02.006</ext-link></comment> <object-id pub-id-type="pmid">24698278</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Calabrese</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Kalman filter mixture model for spike sorting of non-stationary data</article-title>. <source>Journal of neuroscience methods</source>. <year>2011</year>;<volume>196</volume>(<issue>1</issue>):<fpage>159</fpage>–<lpage>169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2010.12.002" xlink:type="simple">10.1016/j.jneumeth.2010.12.002</ext-link></comment> <object-id pub-id-type="pmid">21182868</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shan</surname> <given-names>KQ</given-names></name>, <name name-style="western"><surname>Lubenov</surname> <given-names>EV</given-names></name>, <name name-style="western"><surname>Siapas</surname> <given-names>AG</given-names></name>. <article-title>Model-based spike sorting with a mixture of drifting t-distributions</article-title>. <source>Journal of neuroscience methods</source>, <year>2017</year>, <volume>vol. 288</volume>, p. <fpage>82</fpage>–<lpage>98</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref063">
<label>63</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma D, Ba J. Adam: A method for stochastic optimization. In: International Conference on Learning Representations; 2015.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref064">
<label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Clevert DA, Unterthiner T, Hochreiter S. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:151107289. 2015.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref065">
<label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: International conference on machine learning; 2015. p. 448–456.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Willmore</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Prenger</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Neural representation of natural images in visual area V2</article-title>. <source>The Journal of neuroscience</source>. <year>2010</year>;<volume>30</volume>(<issue>6</issue>):<fpage>2102</fpage>–<lpage>2114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4099-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4099-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20147538</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref067">
<label>67</label>
<mixed-citation publication-type="other" xlink:type="simple">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al. TensorFlow: A System for Large-Scale Machine Learning. In: OSDI. vol. 16; 2016. p. 265–283.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref068">
<label>68</label>
<mixed-citation publication-type="other" xlink:type="simple">Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, et al. DataJoint: managing big scientific data using MATLAB or Python; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/031658" xlink:type="simple">http://biorxiv.org/lookup/doi/10.1101/031658</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">Walt Svd, <name name-style="western"><surname>Colbert</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Varoquaux</surname> <given-names>G</given-names></name>. <article-title>The NumPy array: a structure for efficient numerical computation</article-title>. <source>Computing in Science &amp; Engineering</source>. <year>2011</year>;<volume>13</volume>(<issue>2</issue>):<fpage>22</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/MCSE.2011.37" xlink:type="simple">10.1109/MCSE.2011.37</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hunter</surname> <given-names>JD</given-names></name>. <article-title>Matplotlib: A 2D graphics environment</article-title>. <source>Computing in science &amp; engineering</source>. <year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/MCSE.2007.55" xlink:type="simple">10.1109/MCSE.2007.55</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006897.ref071">
<label>71</label>
<mixed-citation publication-type="other" xlink:type="simple">Waskom M, Botvinnik O, O’Kane D, Hobson P, Lukauskas S, Gemperline DC, et al. mwaskom/seaborn: v0.8.1 (September 2017); 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.883859" xlink:type="simple">https://doi.org/10.5281/zenodo.883859</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref072">
<label>72</label>
<mixed-citation publication-type="other" xlink:type="simple">Kluyver T, Ragan-Kelley B, Pérez F, Granger B, Bussonnier M, Frederic J, et al. Jupyter Notebooks—a publishing format for reproducible computational workflows. In: Loizides F, Schmidt B, editors. Positioning and Power in Academic Publishing: Players, Agents and Agendas. IOS Press; 2016. p. 87—90.</mixed-citation>
</ref>
<ref id="pcbi.1006897.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Merkel</surname> <given-names>D</given-names></name>. <article-title>Docker: Lightweight Linux Containers for Consistent Development and Deployment</article-title>. <source>Linux J</source>. <year>2014</year>;<volume>2014</volume>(<issue>239</issue>).</mixed-citation>
</ref>
</ref-list>
</back>
</article>