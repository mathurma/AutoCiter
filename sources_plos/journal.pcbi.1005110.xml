<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-02071</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005110</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Computational linguistics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Database and informatics methods</subject><subj-group><subject>Bioinformatics</subject><subj-group><subject>Sequence analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Statistical mechanics</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Zipf’s Law Arises Naturally When There Are Underlying, Unobserved Variables</article-title>
<alt-title alt-title-type="running-head">Zipf’s Law Arises When There Are Underlying, Unobserved Variables</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3681-4607</contrib-id>
<name name-style="western">
<surname>Aitchison</surname> <given-names>Laurence</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Corradi</surname> <given-names>Nicola</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8713-9328</contrib-id>
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Weill Medical College, Cornell University, New York, New York, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Sporns</surname> <given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Indiana University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> LA PEL.</p>
</list-item>
<list-item>
<p><bold>Data curation:</bold> LA.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> LA.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> PEL.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> LA NC PEL.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> LA PEL.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> PEL.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> LA PEL.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> LA.</p>
</list-item>
<list-item>
<p><bold>Supervision:</bold> PEL.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> LA.</p>
</list-item>
<list-item>
<p><bold>Visualization:</bold> LA PEL.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> LA PEL.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> LA PEL.</p>
</list-item>
</list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">laurence.aitchison@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>12</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>20</day>
<month>12</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>12</issue>
<elocation-id>e1005110</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>11</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>8</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Aitchison et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005110"/>
<abstract>
<p>Zipf’s law, which states that the probability of an observation is inversely proportional to its rank, has been observed in many domains. While there are models that explain Zipf’s law in each of them, those explanations are typically domain specific. Recently, methods from statistical physics were used to show that a fairly broad class of models does provide a general explanation of Zipf’s law. This explanation rests on the observation that real world data is often generated from underlying causes, known as latent variables. Those latent variables mix together multiple models that do not obey Zipf’s law, giving a model that does. Here we extend that work both theoretically and empirically. Theoretically, we provide a far simpler and more intuitive explanation of Zipf’s law, which at the same time considerably extends the class of models to which this explanation can apply. Furthermore, we also give methods for verifying whether this explanation applies to a particular dataset. Empirically, these advances allowed us extend this explanation to important classes of data, including word frequencies (the first domain in which Zipf’s law was discovered), data with variable sequence length, and multi-neuron spiking activity.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Datasets ranging from word frequencies to neural activity all have a seemingly unusual property, known as Zipf’s law: when observations (e.g., words) are ranked from most to least frequent, the frequency of an observation is inversely proportional to its rank. Here we demonstrate that a single, general principle underlies Zipf’s law in a wide variety of domains, by showing that models in which there is a latent, or hidden, variable controlling the observations can, and sometimes must, give rise to Zipf’s law. We illustrate this mechanism in three domains: word frequency, data with variable sequence length, and neural data.</p>
</abstract>
<funding-group>
<funding-statement>PEL and LA are funded by the Gatsby Charitable Foundation (gatsby.org.uk; grant GAT3214). NC is funded by the National Eye Institute (nei.nih.gov), part of the National Institute of Health (nih.gov; grant 5R01EY012978; title "Population Coding in the Retina"). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="32"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Both natural and artificial systems often exhibit a surprising degree of statistical regularity. One such regularity is Zipf’s law. Originally formulated for word frequency [<xref ref-type="bibr" rid="pcbi.1005110.ref001">1</xref>], Zipf’s law has since been observed in a broad range of domains, including city size [<xref ref-type="bibr" rid="pcbi.1005110.ref002">2</xref>], firm size [<xref ref-type="bibr" rid="pcbi.1005110.ref003">3</xref>], mutual fund size [<xref ref-type="bibr" rid="pcbi.1005110.ref004">4</xref>], amino acid sequences [<xref ref-type="bibr" rid="pcbi.1005110.ref005">5</xref>], and neural activity [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref007">7</xref>].</p>
<p>Zipf’s law is a relation between rank order and frequency of occurrence: it states that when observations (e.g., words) are ranked by their frequency, the frequency of a particular observation is inversely proportional to its rank,
<disp-formula id="pcbi.1005110.e001"><alternatives><graphic id="pcbi.1005110.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:mtext>Frequency</mml:mtext> <mml:mo>∝</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>Rank</mml:mtext></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula></p>
<p>Partly because it is so unexpected, a great deal of effort has gone into explaining Zipf’s law. So far, almost all explanations are either domain specific or require fine-tuning. For language, there are a variety of domain-specific models, beginning with the suggestion that Zipf’s law could be explained by imposing a balance between the effort of the listener and speaker [<xref ref-type="bibr" rid="pcbi.1005110.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref010">10</xref>]. Other explanations include minimizing the number of letters (or phonemes) necessary to communicate a message [<xref ref-type="bibr" rid="pcbi.1005110.ref011">11</xref>], or by considering the generation of random words [<xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>]. There are also domain-specific models for the distribution of city and firm sizes. These models propose a process in which cities or firms grow by random amounts [<xref ref-type="bibr" rid="pcbi.1005110.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref013">13</xref>], with a fixed total population or wealth and a fixed minimum size. Other explanations of Zipf’s law require fine tuning. For instance, there are many mechanisms that can generate power laws [<xref ref-type="bibr" rid="pcbi.1005110.ref014">14</xref>], and these can be fine tuned to give an exponent of −1. Possibly the most important fine-tuned proposal is the notion that some systems sit at a highly unusual thermodynamic state—a critical point [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>].</p>
<p>Only very recently has there been an explanation, by Schwab and colleagues [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>], that does not require fine tuning. This explanation exploits the fact that most real-world datasets have hidden structure that can be described using an unobserved variable. For such models—commonly called latent variable models—the unobserved (or latent) variable, <italic>z</italic>, is drawn from a distribution, <italic>P</italic> (<italic>z</italic>), and the observation, <italic>x</italic>, is drawn from a conditional distribution, <italic>P</italic> (<italic>x</italic>|<italic>z</italic>). The distribution over <italic>x</italic> is therefore given by
<disp-formula id="pcbi.1005110.e002"><alternatives><graphic id="pcbi.1005110.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>z</mml:mi> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
For example, for neural data the latent variable could be the underlying firing rate or the time since stimulus onset.</p>
<p>While Schwab <italic>et al.</italic>’s result was a major advance, it came with some restrictions: the observations, <italic>x</italic>, had to be a high dimensional vector, and the conditional distribution, <italic>P</italic> (<italic>x</italic>|<italic>z</italic>), had to lie in the exponential family with a small number of natural parameters. In addition, the result relied on nontrivial concepts from statistical physics, making it difficult to gain intuition into why latent variable models generally lead to Zipf’s law, and, just as importantly, why they sometimes do not. Here we use the same starting point as Schwab <italic>et al.</italic> (<xref ref-type="disp-formula" rid="pcbi.1005110.e002">Eq 2</xref>), but take a very different theoretical approach—one that considerably extends our theoretical and empirical understanding of the relationship between latent variable models and Zipf’s law. This approach not only gives additional insight into the underlying mechanism by which Zipf’s law emerges, but also gives insight into where and how that mechanism breaks down. Moreover, our theoretical approach relaxes the restrictions inherent in Schwab <italic>et al.</italic>’s model [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>] (high dimensional observations and an exponential family distribution with a small number of natural parameters). Consequently, we are able to apply our theory to three important types of data, all of which are inaccessible under Schwab <italic>et al.</italic>’s model: word frequencies, models where the latent variable is the sequence length, and complex datasets with high-dimensional observations.</p>
<p>For word frequencies—the domain in which Zipf’s law was originally discovered—we show that taking the latent variable to be the part of speech (e.g. noun/verb) can explain Zipf’s law. As part of this explanation, we show that if we take only one part of speech (e.g. only nouns) then Zipf’s law does not emerge—a phenomenon that is not, to our knowledge, taken into account by any other explanation of Zipf’s law for words. For models in which the latent variable is sequence length (i.e. observations in which the dimension of the vector, <italic>x</italic>, is variable), we show that Zipf’s law emerges under very mild conditions. Finally, for models that are high dimensional and sufficiently realistic and complex that the conditional distribution, <italic>P</italic> (<italic>x</italic>|<italic>z</italic>), falls outside Schwab <italic>et al.</italic>’s model class, we show that Zipf’s law still emerges very naturally, again under mild conditions. In addition, we introduce a quantity that allows us to assess how much a given latent variable contributes to the observation of Zipf’s law in a particular dataset. This is important because it allows us to determine, quantitatively, whether a particular latent variable really does contribute significantly to Zipf’s law.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Under Zipf’s law (<xref ref-type="disp-formula" rid="pcbi.1005110.e001">Eq 1</xref>) frequency falls off relatively slowly with rank. This means, loosely, that rare observations are more common than one would typically expect. Consequently, under Zipf’s law, one should observe a fairly broad range of frequencies. This is the case, for instance, for words—just look at the previous sentence: there are some very common words (e.g. “a”, “of”), and other words that are many orders of magnitude rarer (e.g. “frequencies”, “consequently”). This is a remarkable property: you might initially expect to see rare words only rarely. However, while a particular rare word (e.g. “frequencies”) is far less likely to occur than a particular common word (e.g. “a”), there are far more rare words than common words, and these factors balance almost exactly, so that a random word drawn from a body of text is roughly equally likely to be rare, like “frequencies” as it is to be common, like “a”.</p>
<p>Our explanation of Zipf’s law consists of two parts. The first part is the above observation—that Zipf’s law implies a broad range of frequencies. This notion was quantified by Mora and Bialek, who showed that a perfectly flat distribution over a range of frequencies is mathematically equivalent to Zipf’s law over that range [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>]—a result that applies in any and all domains. However, it is important to understand the realistic case: how a finite range of frequencies with an uneven distribution might lead to something similar to, but not exactly, Zipf’s law. We therefore extend Mora and Bialek’s result, and derive a general relationship that quantifies deviations from Zipf’s law for arbitrary distributions over frequency—from very broad to very narrow, and even to multi-modal distributions. That relationship tells us that Zipf’s law emerges when the distribution over frequency is sufficiently broad, even if it is not very flat. We complete the explanation of Zipf’s law by showing that latent variables can, but do not have to, induce a broad range of frequencies. Finally, we demonstrate theoretically and empirically that, in a variety of important domains, it is indeed latent variables that give rise to a broad range of frequencies, and hence Zipf’s law. In particular, we explain Zipf’s law in three domains by showing that, in each of them, the existence of a latent variable leads to a broad range of frequencies. Furthermore, we demonstrate that data with both a varying number of dimensions, and fixed but high dimension, leads to Zipf’s law under very mild conditions.</p>
<sec id="sec003">
<title>A broad range of frequencies implies Zipf’s law</title>
<p>By “a broad range of frequencies”, we mean the frequency varies by many orders of magnitude, as is the case, for instance, for words: “a” is indeed many orders of magnitude more common than “frequencies”. It is therefore convenient to work with the energy, defined by
<disp-formula id="pcbi.1005110.e003"><alternatives><graphic id="pcbi.1005110.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log Frequency</mml:mtext> <mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced> <mml:mo>+</mml:mo> <mml:mtext>const</mml:mtext></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where, as above, <italic>x</italic> is an observation, and we have switched from frequency to probability. To translate Zipf’s law from observations to energy, we take the log of both sides of <xref ref-type="disp-formula" rid="pcbi.1005110.e001">Eq (1)</xref> and use <xref ref-type="disp-formula" rid="pcbi.1005110.e003">Eq (3)</xref> for the energy; this gives us
<disp-formula id="pcbi.1005110.e004"><alternatives><graphic id="pcbi.1005110.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mtext>Zipf</mml:mtext> <mml:mo>’</mml:mo> <mml:mtext>s</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>law</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>holds</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>exactly</mml:mtext> <mml:mspace width="4.pt"/><mml:mspace width="4pt"/><mml:mo>⟺</mml:mo> <mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mtext>log </mml:mtext><mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mtext>const</mml:mtext> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <inline-formula id="pcbi.1005110.e005"><alternatives><graphic id="pcbi.1005110.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the rank of an observation whose energy is <inline-formula id="pcbi.1005110.e006"><alternatives><graphic id="pcbi.1005110.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mi mathvariant="script">E</mml:mi></mml:math></alternatives></inline-formula>.</p>
<p>Given, as discussed above, that Zipf’s law implies a broad range of frequencies, we expect Zipf’s law to hold whenever the low and high energies (which translate into high and low frequencies) have about the same probability. Indeed, previous work [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>] showed that when the distribution over energy, <inline-formula id="pcbi.1005110.e007"><alternatives><graphic id="pcbi.1005110.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, is perfectly constant over a broad range, Zipf’s law holds exactly in that range. However, in practice the distribution over energy is never perfectly constant; the real world is simply not that neat. Consequently, to understand Zipf’s law in real-world data, it is necessary to understand how deviations from a perfectly flat distribution over energy affect Zipf plots. For that we need to find the exact relationship between the distribution over energy and the rank.</p>
<p>To find this exact relationship, we note, using an approach similar to [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>], that if we were to plot rank versus energy, we would see a stepwise increase at the energy of each observation, <italic>x</italic>. Consequently, the gradient of the rank is 0 almost everywhere, and a delta-function at the location of each step,
<disp-formula id="pcbi.1005110.e008"><alternatives><graphic id="pcbi.1005110.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>δ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
The right hand side is closely related to the probability distribution over energy. That distribution can be thought of as a sum of delta-functions, each one located at the energy associated with a particular <italic>x</italic> and weighted by its probability,
<disp-formula id="pcbi.1005110.e009"><alternatives><graphic id="pcbi.1005110.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced> <mml:mi>δ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:msup> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>δ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
with the second equality following from <xref ref-type="disp-formula" rid="pcbi.1005110.e003">Eq (3)</xref>. This expression says that the probability distribution over energy is proportional to <inline-formula id="pcbi.1005110.e010"><alternatives><graphic id="pcbi.1005110.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:msup> <mml:mo>×</mml:mo> <mml:mspace width="0.166667em"/></mml:mrow></mml:math></alternatives></inline-formula> the density of states, a standard result from statistical physics [<xref ref-type="bibr" rid="pcbi.1005110.ref020">20</xref>]. Comparing Eqs <xref ref-type="disp-formula" rid="pcbi.1005110.e008">(5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005110.e009">(6)</xref>, we see that
<disp-formula id="pcbi.1005110.e011"><alternatives><graphic id="pcbi.1005110.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mi mathvariant="script">E</mml:mi></mml:msup> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
Integrating both sides from −∞ to <inline-formula id="pcbi.1005110.e012"><alternatives><graphic id="pcbi.1005110.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mi mathvariant="script">E</mml:mi></mml:math></alternatives></inline-formula> and taking the logarithm gives
<disp-formula id="pcbi.1005110.e013"><alternatives><graphic id="pcbi.1005110.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:mtext>log </mml:mtext><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where <inline-formula id="pcbi.1005110.e014"><alternatives><graphic id="pcbi.1005110.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is <inline-formula id="pcbi.1005110.e015"><alternatives><graphic id="pcbi.1005110.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> smoothed with an exponential kernel,
<disp-formula id="pcbi.1005110.e016"><alternatives><graphic id="pcbi.1005110.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>∞</mml:mi></mml:mrow> <mml:mi mathvariant="script">E</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mfenced> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>Comparing Eqs <xref ref-type="disp-formula" rid="pcbi.1005110.e013">(8)</xref> to <xref ref-type="disp-formula" rid="pcbi.1005110.e004">(4)</xref>, we see that for Zipf’s law to hold exactly over some range (i.e. <inline-formula id="pcbi.1005110.e017"><alternatives><graphic id="pcbi.1005110.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mtext>log </mml:mtext><mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mtext>const</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>, or <italic>r</italic>(x) ∝ 1/<italic>P</italic> (x)), we need <inline-formula id="pcbi.1005110.e018"><alternatives><graphic id="pcbi.1005110.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mtext>const</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula> over that range. This is not new; it was shown previously by Mora and Bialek using essentially the same arguments we used here [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>]. What is new is the exact relationship between <inline-formula id="pcbi.1005110.e019"><alternatives><graphic id="pcbi.1005110.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005110.e020"><alternatives><graphic id="pcbi.1005110.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> given in <xref ref-type="disp-formula" rid="pcbi.1005110.e013">Eq (8)</xref>, which is valid whether or not Zipf’s law holds exactly. This is important because the distribution over energy is never perfectly flat, so we need to reason about how deviations from <inline-formula id="pcbi.1005110.e021"><alternatives><graphic id="pcbi.1005110.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mtext>const</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula> affect Zipf plots—something that our analysis allows us to do. In particular, <xref ref-type="disp-formula" rid="pcbi.1005110.e013">Eq (8)</xref> tells us that departures from Zipf’s law are due solely to variations in <inline-formula id="pcbi.1005110.e022"><alternatives><graphic id="pcbi.1005110.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Consequently, Zipf’s law emerges if variations in <inline-formula id="pcbi.1005110.e023"><alternatives><graphic id="pcbi.1005110.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are small compared to the range of observed energies. This requires the distribution over energy to be broad, but not necessarily very flat (see <xref ref-type="disp-formula" rid="pcbi.1005110.e084">Eq (22)</xref> and surrounding text for an explicit example). Much of the focus of this paper is on showing that latent variable models typically produce sufficient broadening in the distribution over energy for Zipf’s law to emerge.</p>
<sec id="sec004">
<title>Narrow distributions over energy are typical</title>
<p>The analysis in the previous section can be used to tell us why a broad (i.e. Zipfian) distribution over energy is special, and a narrow distribution over energy is generic. Integrating <xref ref-type="disp-formula" rid="pcbi.1005110.e009">Eq (6)</xref> over a small range (from <inline-formula id="pcbi.1005110.e024"><alternatives><graphic id="pcbi.1005110.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mi mathvariant="script">E</mml:mi></mml:math></alternatives></inline-formula> to <inline-formula id="pcbi.1005110.e025"><alternatives><graphic id="pcbi.1005110.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>) we see that
<disp-formula id="pcbi.1005110.e026"><alternatives><graphic id="pcbi.1005110.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="4.pt"/><mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>≈</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:msup> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="4.pt"/><mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where <inline-formula id="pcbi.1005110.e027"><alternatives><graphic id="pcbi.1005110.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="4.pt"/><mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the number of states with energy between <inline-formula id="pcbi.1005110.e028"><alternatives><graphic id="pcbi.1005110.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mi mathvariant="script">E</mml:mi></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005110.e029"><alternatives><graphic id="pcbi.1005110.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. As we just saw, for a broad, Zipfian distribution over energy, we require <inline-formula id="pcbi.1005110.e030"><alternatives><graphic id="pcbi.1005110.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> to be nearly constant. Thus, <xref ref-type="disp-formula" rid="pcbi.1005110.e026">Eq (10)</xref> tells us that for Zipf’s law to emerge, we must have <inline-formula id="pcbi.1005110.e031"><alternatives><graphic id="pcbi.1005110.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext> <mml:mspace width="4.pt"/><mml:mi mathvariant="script">E</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∝</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mi mathvariant="script">E</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (an observation that has been made previously, but couched in terms of entropy rather than density of states [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>]). However, there is no reason for the number of states to take this particular form, so we do not, in general, see Zipf’s law. Moreover, because of the exponential term in <xref ref-type="disp-formula" rid="pcbi.1005110.e026">Eq (10)</xref>, whenever the range of energies is large, even small imbalances between the number of states and the energy lead to highly peaked probabilities. Thus, narrow distributions over energy are generic—a standard result from statistical physics [<xref ref-type="bibr" rid="pcbi.1005110.ref020">20</xref>].</p>
<p>The fact that broad distributions are not generic tells us that Zipf’s law is not generic. However, the above analysis suggests a natural way to induce Zipf’s law: stack together many narrow distributions, each with a peak at a different energy. In the following sections we expand on this idea.</p>
</sec>
</sec>
<sec id="sec005">
<title>Latent variables lead to a broad range of frequencies</title>
<p>We now demonstrate that latent variables can broaden the distribution over energy sufficiently to give Zipf’s law. We begin with generic arguments showing that latent variables typically broaden the distribution over energy. We then show empirically that, in three domains of interest, this broadening leads to Zipf’s law. We also show that Zipf’s law emerges generically in data with varying dimensions and in latent variable models describing data with fixed, but high, dimension.</p>
<sec id="sec006">
<title>General principles</title>
<p>To obtain Zipf’s law, we need a dataset displaying a broad range of frequencies (or energies). It is straightforward to see how latent variables might help: if the energy depends strongly on the latent variable, then mixing across many different settings of the latent variable leads to a broad range of energies. We can formalise this intuition by noting that for a latent variable model, the distribution over <italic>x</italic> is found by integrating <italic>P</italic> (<italic>x</italic>|<italic>z</italic>) over the latent variable, <italic>z</italic> (<xref ref-type="disp-formula" rid="pcbi.1005110.e002">Eq 2</xref>). Likewise, the distribution over energy is found by integrating <inline-formula id="pcbi.1005110.e032"><alternatives><graphic id="pcbi.1005110.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> over the latent variable,
<disp-formula id="pcbi.1005110.e033"><alternatives><graphic id="pcbi.1005110.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>z</mml:mi> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
Therefore, mixing multiple narrow (and hence non-Zipfian) distributions, <inline-formula id="pcbi.1005110.e034"><alternatives><graphic id="pcbi.1005110.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, with sufficiently different means (e.g., coloured lines in <xref ref-type="fig" rid="pcbi.1005110.g001">Fig 1A</xref>) gives rise to a broad (and hence Zipfian) distribution, <inline-formula id="pcbi.1005110.e035"><alternatives><graphic id="pcbi.1005110.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (solid black line <xref ref-type="fig" rid="pcbi.1005110.g001">Fig 1A</xref>). This tells us something very important: “special” Zipfian distributions, with a broad range of energies, can be constructed merely by combining many “generic” non-Zipfian distributions, each with a narrow range of energies. Critically, to achieve large broadening, the mean energy, and thus the typical frequency, of an observation must depend on the latent variable; i.e. the mean of the conditional distribution, <inline-formula id="pcbi.1005110.e036"><alternatives><graphic id="pcbi.1005110.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, must depend on <italic>z</italic>. Taking words as an example, one setting of the latent variable should lead mainly to common (and thus low energy) words, like “a”, whereas another setting of the latent variable should lead mainly to rare (and thus high energy) words, like “frequencies”.</p>
<fig id="pcbi.1005110.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005110.g001</object-id>
<label>Fig 1</label>
<caption>
<title>PEEV measures the average width of <inline-formula id="pcbi.1005110.e037"><alternatives><graphic id="pcbi.1005110.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> relative to <inline-formula id="pcbi.1005110.e038"><alternatives><graphic id="pcbi.1005110.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</title>
<p>PEEV is close to 0 if the widths are the same, and close to 1 if <inline-formula id="pcbi.1005110.e039"><alternatives><graphic id="pcbi.1005110.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is, on average, much narrower that <inline-formula id="pcbi.1005110.e040"><alternatives><graphic id="pcbi.1005110.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. In all panels, the black line is <inline-formula id="pcbi.1005110.e041"><alternatives><graphic id="pcbi.1005110.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and the coloured lines are <inline-formula id="pcbi.1005110.e042"><alternatives><graphic id="pcbi.1005110.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for three different settings of the latent variable, <italic>z</italic>. <bold>A</bold>. For high PEEV, the conditional distributions, <inline-formula id="pcbi.1005110.e043"><alternatives><graphic id="pcbi.1005110.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, are narrow, and have very different means. <bold>B</bold>. For intermediate PEEV, the conditional distributions are broader, and their means are more similar. <bold>C</bold>. For low PEEV, the conditional distributions are very broad, and their means are very similar.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.g001" xlink:type="simple"/>
</fig>
<p>Our mechanism (mixing together many narrow distributions over energy to give a broad distribution) is one of many possible ways that Zipf’s law could emerge in real datasets. It is thus important to be able to tell whether Zipf’s law in a particular dataset emerges because of our mechanism, or another one. Critically, if our mechanism is operative, even though the full dataset displays Zipf’s law (and hence has a broad distribution over energy), the subset of the data associated with any particular setting of the latent variable will be non-Zipfian (and hence have a narrow distribution over energy). In this case, a broad distribution over energy, and hence Zipf’s law, emerges because of the mixing of multiple narrow, non-Zipfian distributions (each with a different setting of the latent variable). To complete the explanation of Zipf’s law, we only need to explain why, in that particular dataset, it is reasonable for there to be a latent variable that controls the location of the peak in the energy distribution.</p>
<p>Of course there is, in reality, a continuum—there are two contributions to the width of <inline-formula id="pcbi.1005110.e044"><alternatives><graphic id="pcbi.1005110.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. One, corresponding to our mechanism, comes from changes in the mean of <inline-formula id="pcbi.1005110.e045"><alternatives><graphic id="pcbi.1005110.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as the latent variable changes; the other comes from the width of <inline-formula id="pcbi.1005110.e046"><alternatives><graphic id="pcbi.1005110.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. To quantify the contribution of each mechanism towards an observation of Zipf’s law, we use the standard formula for the proportion of explained variance (or <italic>R</italic><sup>2</sup>) to define the proportion of explained energy variance (PEEV; see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>PEEV, and the law of total variance</italic> for further details). PEEV gives the proportion of the total energy variance that can be explained by changes in the mean of <inline-formula id="pcbi.1005110.e047"><alternatives><graphic id="pcbi.1005110.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as the latent variable, <italic>z</italic>, changes. PEEV ranges from 0, indicating that <italic>z</italic> explains none of the energy variance, so the latent variable does not contribute to the observation of Zipf’s law, to 1, indicating that <italic>z</italic> explains all of the energy variance, so our mechanism is entirely responsible for the observation of Zipf’s law. As an example, we plot energy distributions with a range of values for PEEV (<xref ref-type="fig" rid="pcbi.1005110.g001">Fig 1</xref>). The black line is <inline-formula id="pcbi.1005110.e048"><alternatives><graphic id="pcbi.1005110.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and the coloured lines are <inline-formula id="pcbi.1005110.e049"><alternatives><graphic id="pcbi.1005110.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for different settings of <italic>z</italic>. For high values of PEEV, the distributions <inline-formula id="pcbi.1005110.e050"><alternatives><graphic id="pcbi.1005110.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are narrow, but have very different means (<xref ref-type="fig" rid="pcbi.1005110.g001">Fig 1A</xref>). In contrast, for low values of PEEV, the distributions <inline-formula id="pcbi.1005110.e051"><alternatives><graphic id="pcbi.1005110.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are broad, yet have very similar means, so the width of <inline-formula id="pcbi.1005110.e052"><alternatives><graphic id="pcbi.1005110.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> comes mainly from the width of <inline-formula id="pcbi.1005110.e053"><alternatives><graphic id="pcbi.1005110.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1005110.g001">Fig 1C</xref>).</p>
</sec>
</sec>
<sec id="sec007">
<title>Categorical data (word frequencies)</title>
<p>It has been known for many decades that word frequencies obey Zipf’s law [<xref ref-type="bibr" rid="pcbi.1005110.ref001">1</xref>], and many explanations for this finding have been suggested [<xref ref-type="bibr" rid="pcbi.1005110.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>]. However, none of these explanations accounts for the observation that, while word frequencies overall display Zipf’s law (solid black line, <xref ref-type="fig" rid="pcbi.1005110.g002">Fig 2B</xref>), word frequencies for individual parts of speech (e.g. nouns vs conjunctions) do not (coloured lines, <xref ref-type="fig" rid="pcbi.1005110.g002">Fig 2B</xref>; except perhaps for verbs, which we discuss below). We can see directly from these plots that the mechanism discussed in the previous section gives rise to Zipf’s law: different parts of speech have narrow distributions over energy (coloured lines, <xref ref-type="fig" rid="pcbi.1005110.g002">Fig 2A</xref>), and they have different means. Mixing across different parts of speech therefore gives a broad range of energies (solid black line, <xref ref-type="fig" rid="pcbi.1005110.g002">Fig 2A</xref>), and hence Zipf’s law. In practice, the fact that different parts of speech have different mean energies implies that some parts of speech (e.g. nouns, like “ream”) consist of many different words, each of which is relatively rare, whereas other parts of speech (e.g. conjunctions, like “and”) consist of only a few words, each of which is relatively common. We can therefore conclude that Zipf’s law for words emerges because there is a latent variable, the part-of-speech, and the latent variable controls the mean energy. We can confirm quantitatively that Zipf’s law arises primarily through our mechanism by noting that PEEV is relatively high, 0.58 (for details on how we compute PEEV, see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Computing PEEV</italic>).</p>
<fig id="pcbi.1005110.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005110.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Zipf’s law for word frequencies, split by part of speech (data from [<xref ref-type="bibr" rid="pcbi.1005110.ref021">21</xref>]).</title>
<p>The coloured lines are for individual parts of speech, the black line is for all the words. <bold>A</bold>. The distribution over energy is broad for words in general, but the distribution over energy for individual parts of speech is narrow. <bold>B</bold>. Therefore, words in general obey Zipf’s law, but individual parts of speech do not (except for verbs, which too can be divided into classes [<xref ref-type="bibr" rid="pcbi.1005110.ref022">22</xref>]). The red line has a slope of −1, and closely matches the combined data.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.g002" xlink:type="simple"/>
</fig>
<p>We have demonstrated that Zipf’s law for words emerges because of the combination of different parts of speech with different characteristic frequencies. However, to truly explain Zipf’s law for words, we have to explain why different parts of speech have such different characteristic frequencies. While this is really a task for linguists, we can speculate. One potential explanation is that different parts of speech have different functions within the sentence. For instance, words with a purely grammatical function (e.g. conjunctions, like “and”) are common, because they can be used in a sentence describing anything. In contrast, words denoting something in the world (e.g. nouns, like “ream”) are more rare, because they can be used only in the relatively few sentences about that object. Mixing together these two classes of words gives a broad range of frequencies, or energies, and hence, Zipf’s law. Finally, using similar arguments, we can see why verbs have a broader range of frequencies than other parts of speech—some verbs (like “is”) can be used in almost any context (and one might argue that they have a grammatical function) whereas other verbs (like “gather”) refer to a specific type of action, and hence can only be used in a few contexts. In fact, verbs, like words in general, fall into classes [<xref ref-type="bibr" rid="pcbi.1005110.ref022">22</xref>].</p>
</sec>
<sec id="sec008">
<title>Data with variable dimension</title>
<p>Two models in which the data consists of sequences with variable length have been shown to give rise to Zipf’s law [<xref ref-type="bibr" rid="pcbi.1005110.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>]. These models fit easily into our framework, as there is a natural latent variable, the sequence length. We show that if the distribution over sequence length is sufficiently broad, Zipf’s law emerges.</p>
<p>First, Li [<xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>] noted that randomly generated words with different lengths obey Zipf’s law. Here “randomly generated” means the following: a word is generated by randomly selecting a symbol that can be either one of <italic>M</italic> letters or a space, all with equal probability; the symbols are concatenated; and the word is terminated when a space is encountered. We can turn this into a latent variable model by first drawing the sequence length, <italic>z</italic>, from a distribution, then choosing <italic>z</italic> letters randomly. Thus, the sequence length, <italic>z</italic>, is “latent”, as it is chosen first, before the data are generated—it does not matter that in this particular case, the latent variable can be inferred perfectly from an observation.</p>
<p>Second, Mora <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1005110.ref005">5</xref>] found that amino acid sequences in the D region of Zebrafish IgM obey Zipf’s law. The latent variable is again <italic>z</italic>, the length of the amino acid sequence. The authors found that, conditioned on length, the data was well fit by an Ising-like model with translation-invariant coupling,
<disp-formula id="pcbi.1005110.e054"><alternatives><graphic id="pcbi.1005110.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>∝</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>z</mml:mi></mml:munderover> <mml:mi>h</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>z</mml:mi></mml:munderover> <mml:msub><mml:mi>J</mml:mi> <mml:mfenced close="|" open="|" separators=""><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi></mml:mfenced></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where <bold>x</bold> denotes a vector, <bold>x</bold> = (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>z</italic></sub>), and <italic>x</italic><sub><italic>i</italic></sub> represents a single amino acid (of which there were 21).</p>
<p>The basic principle underlying Zipf’s law in models with variable sequence length is that there are few short sequences, so each short sequence has a high probability and hence a low energy. In contrast, there are many long sequences, so each long sequence has a low probability and hence a high energy. Mixing together short and long sequences therefore gives a broad distribution over energy and hence Zipf’s law.</p>
<p>Models in which sequence length is the latent variable are particularly easy to analyze because there is a simple relationship between the total and conditional distributions,
<disp-formula id="pcbi.1005110.e055"><alternatives><graphic id="pcbi.1005110.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi>z</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
The first equality holds because <italic>z</italic>, the length of the word, is a deterministic function of <bold>x</bold>, so <italic>P</italic> (<italic>z</italic>|<bold>x</bold>) = 1 (as long as <italic>z</italic> is the length of the vector <bold>x</bold>, which is what we assume here); the second follows from Bayes theorem. To illustrate the general approach, we use this to analyze Li’s model (as it is relatively simple). For that model, each element of <bold>x</bold> is drawn from a uniform, independent distribution with <italic>M</italic> elements, so the probability of observing any particular configuration with a sequence length of <italic>z</italic> is <italic>M</italic><sup>−<italic>z</italic></sup>. Consequently
<disp-formula id="pcbi.1005110.e056"><alternatives><graphic id="pcbi.1005110.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msup> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
Taking the log of both sides of this expression and negating gives us the energy of a particular configuration,
<disp-formula id="pcbi.1005110.e057"><alternatives><graphic id="pcbi.1005110.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mi>z</mml:mi> <mml:mtext> log </mml:mtext><mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>≈</mml:mo> <mml:mi>z</mml:mi> <mml:mtext> log </mml:mtext><mml:mi>M</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
The approximation holds because log <italic>P</italic> (<italic>z</italic>) varies little with <italic>z</italic> (in this case its variance cannot be greater than (<italic>M</italic> + 1)/<italic>M</italic>, and in the worst case its variance is <inline-formula id="pcbi.1005110.e058"><alternatives><graphic id="pcbi.1005110.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow/></mml:msub> <mml:mo>[</mml:mo> <mml:mi>z</mml:mi> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>; see <xref ref-type="sec" rid="sec013">Methods</xref> Var [log <italic>P</italic> (<italic>z</italic>)] <italic>is</italic> <inline-formula id="pcbi.1005110.e059"><alternatives><graphic id="pcbi.1005110.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow/></mml:msub> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mi>z</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>). Therefore, the variance of the energy is approximately proportional to the variance of the sequence length, <italic>z</italic>,
<disp-formula id="pcbi.1005110.e060"><alternatives><graphic id="pcbi.1005110.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow/></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>≈</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mtext>log </mml:mtext><mml:mi>M</mml:mi></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow/></mml:msub> <mml:mfenced close="]" open="["><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
If there is a broad range of sequence lengths (meaning the standard deviation of <italic>z</italic> is large), then the energy has a broad range, and Zipf’s law emerges. More quantitatively, our analysis for high-dimensional data below suggests that in the limit of large average sequence length, Zipf’s law emerges when the standard deviation of z is on the order of the average sequence length. For Li’s model [<xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>], the standard deviation and mean of <italic>z</italic> both scale with <italic>M</italic>, so we expect Zipf’s law to emerge when <italic>M</italic> is large. To check this, we simulated random words with <italic>M</italic> = 4. Even for this relatively modest value, <inline-formula id="pcbi.1005110.e061"><alternatives><graphic id="pcbi.1005110.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (black line, <xref ref-type="fig" rid="pcbi.1005110.g003">Fig 3A</xref>) is relatively flat over a broad range, but the distributions for individual word lengths (coloured lines, <xref ref-type="fig" rid="pcbi.1005110.g003">Fig 3A</xref>) are extremely narrow. Therefore, data for a single word length does not give Zipf’s law (coloured lines, <xref ref-type="fig" rid="pcbi.1005110.g003">Fig 3B</xref>), but combining across different word lengths does give Zipf’s law (black line, <xref ref-type="fig" rid="pcbi.1005110.g003">Fig 3B</xref>; though with steps, because all words with the same sequence length have the same energy).</p>
<fig id="pcbi.1005110.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005110.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Li’s model of random words displays Zipf’s law because it mixes words of different lengths.</title>
<p><bold>A</bold>. The distribution over energy. <bold>B</bold>. Zipf plot. In both plots the black lines use all the data and each coloured line corresponds to a different word length. The red line has a slope of −1, and so corresponds to Zipf’s law.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.g003" xlink:type="simple"/>
</fig>
<p>Of course, this derivation becomes more complex for models, like the antibody data, in which elements of the sequence are not independently and identically distributed. However, even in such models the basic intuition holds: there are few short sequences, so each short sequence has high probability and low energy, whereas the opposite is true for longer sequences. In fact, the energy is still approximately proportional to sequence length, as it was in <xref ref-type="disp-formula" rid="pcbi.1005110.e057">Eq (15)</xref>, because the number of possible configurations is exponential in the sequence length, and the energy is approximately the logarithm of that number (see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Models in which the latent variable is the sequence length</italic>, for a more principled explanation). Consequently, in general a broad range of sequence lengths gives a broad distribution over energy, and hence Zipf’s law.</p>
<p>However, as discussed above, just because a latent variable could give rise to Zipf’s law does not mean it is entirely responsible for Zipf’s law in a particular dataset. To quantify the role of sequence length in Mora <italic>et al.</italic>’s antibody data, we computed PEEV (the proportion of the variance of the energy explained by sequence length) for the 14 datasets used in their analysis. As can be seen in <xref ref-type="fig" rid="pcbi.1005110.g004">Fig 4A</xref>, PEEV is generally small: less than 0.5 in 12 out of the 14 datasets. And indeed, for the dataset with the smallest PEEV (0.07), Zipf’s law is obeyed at each sequence length (<xref ref-type="fig" rid="pcbi.1005110.g004">Fig 4B</xref>). This in fact turns out to hold for all the datasets, even the one with the highest PEEV (0.72; <xref ref-type="fig" rid="pcbi.1005110.g004">Fig 4C</xref>).</p>
<fig id="pcbi.1005110.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005110.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Re-analysis of amino acid sequences in the D region of 14 Zebrafish.</title>
<p><bold>A</bold>. Proportion of the variance explained by sequence length (PEEV) for the 14 datasets. Most are low, and all but two are less than 0.5. <bold>B</bold> and <bold>C</bold>. Zipf plots for the dataset with the lowest (B) and highest (C) PEEV. In both plots the black line uses all the data and the coloured lines correspond to sequence lengths ranging from 1 to 7. The red line has a slope of −1, and so corresponds to Zipf’s law. Data from Ref. [<xref ref-type="bibr" rid="pcbi.1005110.ref005">5</xref>], kindly supplied by Thierry Mora. (Note that <inline-formula id="pcbi.1005110.e062"><alternatives><graphic id="pcbi.1005110.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mi mathvariant="script">E</mml:mi></mml:math></alternatives></inline-formula> increases downward on the <italic>y</italic>-axis, in keeping with standard conventions).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.g004" xlink:type="simple"/>
</fig>
<p>The fact that Zipf’s law is observed at each sequence length complicates the interpretation of this data. Our mechanism—adding together many distributions, each at different mean energy—plays only a small role in producing Zipf’s law over the whole dataset. And indeed, an additional mechanism has been found: a recent study showed that antibody data is well modelled by random growth and decay processes [<xref ref-type="bibr" rid="pcbi.1005110.ref023">23</xref>], which leads to Zipf’s law at each sequence length.</p>
</sec>
<sec id="sec009">
<title>High-dimensional data</title>
<p>A very important class of models are those where the data is high-dimensional. We show two things for this class. First, the distribution over energy is broadened by latent variables—more specifically, for latent variable models, the variance typically scales as <italic>n</italic><sup>2</sup>. Second, the <italic>n</italic><sup>2</sup> scaling is sufficiently large that deviations from Zipf’s law become negligible in the large <italic>n</italic> limit.</p>
<p>The reasoning is the same as it was above: we can obtain a broad distribution over energy by mixing together multiple, narrowly peaked (and thus non-Zipfian) distributions. Intuitively, if the peaks of those distributions cover a broad enough range of energies, Zipf’s law should emerge. To quantify this intuition, we use the law of total variance [<xref ref-type="bibr" rid="pcbi.1005110.ref024">24</xref>],
<disp-formula id="pcbi.1005110.e063"><alternatives><graphic id="pcbi.1005110.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi mathvariant="bold">x</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
where again <bold>x</bold> is a vector, this time with <italic>n</italic>, rather than <italic>z</italic>, elements. This expression tells us that the variance of the energy (the left hand side) must be greater than the variance of the mean energy (the first term on the right hand side). (As an aside, this decomposition is the essence of PEEV; see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>PEEV, and the law of total variance</italic>).</p>
<p>As discussed above, the reason latent variable models often lead to Zipf’s law is that the latent variable typically has a strong effect on the mean energy (see in particular <xref ref-type="fig" rid="pcbi.1005110.g001">Fig 1</xref>). We thus focus on the first term in <xref ref-type="disp-formula" rid="pcbi.1005110.e063">Eq (17)</xref>, the variance of the mean energy. We show next that it is typically <inline-formula id="pcbi.1005110.e064"><alternatives><graphic id="pcbi.1005110.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and that this is sufficiently broad to induce Zipf’s law.</p>
<p>The mean energy is given by
<disp-formula id="pcbi.1005110.e065"><alternatives><graphic id="pcbi.1005110.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e065" xlink:type="simple"/><mml:math display="block" id="M65"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
This is somewhat unfamiliar, but can be converted into a very standard quantity by noting that in the large <italic>n</italic> limit we may replace <italic>P</italic> (<bold>x</bold>) with <italic>P</italic> (<bold>x</bold>|<italic>z</italic>), which converts the mean energy to the entropy of <italic>P</italic> (<bold>x</bold>|<italic>z</italic>). To see why, we write
<disp-formula id="pcbi.1005110.e066"><alternatives><graphic id="pcbi.1005110.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e066" xlink:type="simple"/><mml:math display="block" id="M66"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mtext>log </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced></mml:mrow> <mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
For low dimensional latent variable models (more specifically, for models in which <italic>z</italic> is <italic>k</italic> dimensional with <italic>k</italic> ≪ <italic>n</italic>), the second term on the right hand side is <inline-formula id="pcbi.1005110.e067"><alternatives><graphic id="pcbi.1005110.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn><mml:mtext> log </mml:mtext><mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Loosely, that’s because it’s positive and its expectation over <italic>z</italic> is the mutual information between <bold>x</bold> and <italic>z</italic>, which is typically <inline-formula id="pcbi.1005110.e068"><alternatives><graphic id="pcbi.1005110.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn><mml:mtext> log </mml:mtext><mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1005110.ref025">25</xref>]. Here, and in almost all of our analysis, we consider low dimensional latent variables; in this regime, the second term on the right hand side is small compared to the energy, which is <inline-formula id="pcbi.1005110.e069"><alternatives><graphic id="pcbi.1005110.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (recall, from the previous section, that the energy is proportional to the sequence length, which here is <italic>n</italic>). Thus, in the large <italic>n</italic> and small <italic>k</italic> limit—the limit of interest—the second term can be ignored, and the mean energy is approximately equal to the entropy of <italic>P</italic> (<bold>x</bold>|<italic>z</italic>),
<disp-formula id="pcbi.1005110.e070"><alternatives><graphic id="pcbi.1005110.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e070" xlink:type="simple"/><mml:math display="block" id="M70"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>≈</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>≡</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
Approximating the energy by the entropy is convenient because the latter is intuitive, and often easy to estimate. This approximation breaks down (as does the <inline-formula id="pcbi.1005110.e071"><alternatives><graphic id="pcbi.1005110.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn><mml:mtext> log </mml:mtext><mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> scaling [<xref ref-type="bibr" rid="pcbi.1005110.ref025">25</xref>]) for high dimensional latent variables, those for which <italic>k</italic> is on the same order as <italic>n</italic>. However, the approximation is not critical to any of our arguments, so we can use our framework to show that high dimensional latent variables can also lead to Zipf’s law; see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>High dimensional latent variables</italic>.</p>
<p>At least in the simple case in which each element of <bold>x</bold> is independent and identically distributed conditioned on <italic>z</italic>, it is straightforward to show that the variance of the entropy is <inline-formula id="pcbi.1005110.e072"><alternatives><graphic id="pcbi.1005110.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. That is because the entropy is <italic>n</italic> times the entropy of one element <inline-formula id="pcbi.1005110.e073"><alternatives><graphic id="pcbi.1005110.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mtext>x</mml:mtext> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>n</mml:mi> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, so the variance of the total entropy is <italic>n</italic><sup>2</sup> times the variance of the entropy of one element,
<disp-formula id="pcbi.1005110.e074"><alternatives><graphic id="pcbi.1005110.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e074" xlink:type="simple"/><mml:math display="block" id="M74"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
which is <inline-formula id="pcbi.1005110.e075"><alternatives><graphic id="pcbi.1005110.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and hence the variance of the energy is also <inline-formula id="pcbi.1005110.e076"><alternatives><graphic id="pcbi.1005110.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Importantly, to obtain this scaling, all we need is that <inline-formula id="pcbi.1005110.e077"><alternatives><graphic id="pcbi.1005110.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e077" xlink:type="simple"/><mml:math display="inline" id="M77"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>In the slightly more complex case in which each element of <bold>x</bold> is independent, but not identically distributed conditioned on <italic>z</italic>, the total entropy is still the sum of the element-wise entropies: <inline-formula id="pcbi.1005110.e078"><alternatives><graphic id="pcbi.1005110.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mtext>x</mml:mtext> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Now, though, each of the <inline-formula id="pcbi.1005110.e079"><alternatives><graphic id="pcbi.1005110.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be different. In this case, for the variance to scale as <italic>n</italic><sup>2</sup>, the element-wise entropies must covary, with <inline-formula id="pcbi.1005110.e080"><alternatives><graphic id="pcbi.1005110.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and, on average, positive, covariance. Intuitively, the latent variable must control the entropy, such that for some settings of the latent variable the entropy of most of the elements is high, and for other settings the entropy of most of the elements is low.</p>
<p>For the completely general case, in which the elements of <italic>x</italic><sub><italic>i</italic></sub> are not independent, essentially the same reasoning holds: for Zipf’s law to emerge the entropies of each element (suitably defined; see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Latent variable models with high dimensional non-conditionally independent data</italic>) must covary, with <inline-formula id="pcbi.1005110.e081"><alternatives><graphic id="pcbi.1005110.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and, on average, positive, covariance. This result—that the variance of the energy scales as <italic>n</italic><sup>2</sup> when the elementwise entropies covary—has been confirmed empirically for multi-neuron spiking data [<xref ref-type="bibr" rid="pcbi.1005110.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>] (though they did not assess Zipf’s law).</p>
<p>We have shown that the variance of the energy is typically <inline-formula id="pcbi.1005110.e082"><alternatives><graphic id="pcbi.1005110.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e082" xlink:type="simple"/><mml:math display="inline" id="M82"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. But is that broad enough to produce Zipf’s law? The answer is yes, for the following reason. For Zipf’s law to emerge, we need the distribution over energy to be broad over the whole range of ranks. For high-dimensional data, the number of possible observations, and hence the range of possible ranks, increases with <italic>n</italic>. In particular, the number of possible observations scales exponentially with <italic>n</italic> (e.g. if each element of the observation is binary, the number of possible observations is 2<italic><sup>n</sup></italic>), so the logarithm of the number of possible observations, and hence the range of possible log-ranks, scales with <italic>n</italic>. Therefore, to obtain Zipf’s law, the distribution over energy must be roughly constant over a region that scales with <italic>n</italic>. But that is exactly what latent variable models give us: the variance scales as <italic>n</italic><sup>2</sup>, so the width of the distribution is proportional to <italic>n</italic>, matching the range of log-ranks. Thus, the fact that the variance scales as <italic>n</italic><sup>2</sup> means that Zipf’s law is, very generically, likely to emerge for latent variable models in which the data is high dimensional.</p>
<p>We can, in fact, show that when the variance of the energy is <inline-formula id="pcbi.1005110.e083"><alternatives><graphic id="pcbi.1005110.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e083" xlink:type="simple"/><mml:math display="inline" id="M83"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, Zipf’s law is obeyed ever more closely as <italic>n</italic> increases. Rewriting <xref ref-type="disp-formula" rid="pcbi.1005110.e013">Eq (8)</xref>, but normalizing by <italic>n</italic>, we have
<disp-formula id="pcbi.1005110.e084"><alternatives><graphic id="pcbi.1005110.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e084" xlink:type="simple"/><mml:math display="block" id="M84"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi mathvariant="script">E</mml:mi> <mml:mi>n</mml:mi></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
The normalized log-rank and normalized energy now vary across an <inline-formula id="pcbi.1005110.e085"><alternatives><graphic id="pcbi.1005110.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e085" xlink:type="simple"/><mml:math display="inline" id="M85"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> range, so if <inline-formula id="pcbi.1005110.e086"><alternatives><graphic id="pcbi.1005110.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e086" xlink:type="simple"/><mml:math display="inline" id="M86"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the last term will be small, and Zipf’s law will emerge. If the variance of the energy is <inline-formula id="pcbi.1005110.e087"><alternatives><graphic id="pcbi.1005110.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e087" xlink:type="simple"/><mml:math display="inline" id="M87"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, then <inline-formula id="pcbi.1005110.e088"><alternatives><graphic id="pcbi.1005110.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> typically has this scaling. For example, consider a Gaussian distribution, for which <inline-formula id="pcbi.1005110.e089"><alternatives><graphic id="pcbi.1005110.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e089" xlink:type="simple"/><mml:math display="inline" id="M89"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∼</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Because, as we have seen, the energy is proportional to <italic>n</italic>, the numerator and denominator both scale with <italic>n</italic><sup>2</sup>, giving <inline-formula id="pcbi.1005110.e090"><alternatives><graphic id="pcbi.1005110.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e090" xlink:type="simple"/><mml:math display="inline" id="M90"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> the required <inline-formula id="pcbi.1005110.e091"><alternatives><graphic id="pcbi.1005110.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> scaling. This argument is not specific to Gaussian distributions: if the variance of the energy is <inline-formula id="pcbi.1005110.e092"><alternatives><graphic id="pcbi.1005110.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, we expect <inline-formula id="pcbi.1005110.e093"><alternatives><graphic id="pcbi.1005110.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e093" xlink:type="simple"/><mml:math display="inline" id="M93"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> to display only <inline-formula id="pcbi.1005110.e094"><alternatives><graphic id="pcbi.1005110.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e094" xlink:type="simple"/><mml:math display="inline" id="M94"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> changes as the energy changes by an <inline-formula id="pcbi.1005110.e095"><alternatives><graphic id="pcbi.1005110.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e095" xlink:type="simple"/><mml:math display="inline" id="M95"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> amount.</p>
<p>This result turns out to be very robust. For instance, as we show in Methods <italic>Peaks in <inline-formula id="pcbi.1005110.e096"><alternatives><graphic id="pcbi.1005110.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e096" xlink:type="simple"/><mml:math display="inline" id="M96"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> do not disrupt Zipf’s law</italic>, even delta-function spikes in the distribution over energy (<xref ref-type="fig" rid="pcbi.1005110.g005">Fig 5A</xref>) do not disrupt the emergence of Zipf’s law as <italic>n</italic> increases (<xref ref-type="fig" rid="pcbi.1005110.g005">Fig 5B</xref>). (The distribution over energy is, of course, always a sum of delta-functions, as can be seen in <xref ref-type="disp-formula" rid="pcbi.1005110.e009">Eq (6)</xref>. However, the delta-functions in <xref ref-type="disp-formula" rid="pcbi.1005110.e009">Eq (6)</xref> are typically very close together, and each one is weighted by a very small number, <inline-formula id="pcbi.1005110.e097"><alternatives><graphic id="pcbi.1005110.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>. Here we are considering a delta-function with a large weight, as shown by the large spike in <xref ref-type="fig" rid="pcbi.1005110.g005">Fig 5A</xref>). However, “holes” in the probability distribution of the energy (i.e. regions of 0 probability, as in <xref ref-type="fig" rid="pcbi.1005110.g005">Fig 5C</xref>) do disrupt the Zipf plot. That is because in regions where <inline-formula id="pcbi.1005110.e098"><alternatives><graphic id="pcbi.1005110.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is low, the energy decreases rapidly without the rank changing; this makes <inline-formula id="pcbi.1005110.e099"><alternatives><graphic id="pcbi.1005110.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e099" xlink:type="simple"/><mml:math display="inline" id="M99"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> very large and negative, disrupting Zipf’s law (<xref ref-type="fig" rid="pcbi.1005110.g005">Fig 5D</xref>). Between holes, however, we expect Zipf’s law to be obeyed, as illustrated in <xref ref-type="fig" rid="pcbi.1005110.g005">Fig 5D</xref>.</p>
<fig id="pcbi.1005110.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005110.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The relationship between <inline-formula id="pcbi.1005110.e100"><alternatives><graphic id="pcbi.1005110.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (left panel) and Zipf plots (<inline-formula id="pcbi.1005110.e101"><alternatives><graphic id="pcbi.1005110.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:mrow><mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> versus log-rank, right panel).</title>
<p>As in <xref ref-type="fig" rid="pcbi.1005110.g004">Fig 4</xref>, <italic>ε</italic> increases downward on the <italic>y</italic>-axis in panels B and D. <bold>A</bold> and <bold>B</bold>. We bypassed an explicit latent variable model, and set <inline-formula id="pcbi.1005110.e102"><alternatives><graphic id="pcbi.1005110.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mtext>Uniform</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>30</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:mn>15</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The deviation from Zipf’s law, shown as a blip around <inline-formula id="pcbi.1005110.e103"><alternatives><graphic id="pcbi.1005110.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:mn>15</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, is small. This is general: as we show in Methods <italic>Peaks in <inline-formula id="pcbi.1005110.e104"><alternatives><graphic id="pcbi.1005110.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> do not disrupt Zipf’s law</italic>, departures from Zipf’s law scale as 1/<italic>n</italic> even for large delta-function perturbations. <bold>C</bold> and <bold>D</bold>. We again bypassed an explicit latent variable model, and set <inline-formula id="pcbi.1005110.e105"><alternatives><graphic id="pcbi.1005110.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mtext>Uniform</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>10</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>+</mml:mo> <mml:mtext>Uniform</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>;</mml:mo> <mml:mn>20</mml:mn> <mml:mo>,</mml:mo> <mml:mn>30</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The resulting hole between <inline-formula id="pcbi.1005110.e106"><alternatives><graphic id="pcbi.1005110.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:mn>10</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and 20 causes a large deviation from Zipf’s law.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.g005" xlink:type="simple"/>
</fig>
<p>Importantly, we can now see why a model in which there is no latent variable, so the variance of the energy is <inline-formula id="pcbi.1005110.e107"><alternatives><graphic id="pcbi.1005110.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, does not give Zipf’s law. (To see why the <inline-formula id="pcbi.1005110.e108"><alternatives><graphic id="pcbi.1005110.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e108" xlink:type="simple"/><mml:math display="inline" id="M108"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> scaling of the variance is generic, see [<xref ref-type="bibr" rid="pcbi.1005110.ref020">20</xref>]). In this case, the range of energies is <inline-formula id="pcbi.1005110.e109"><alternatives><graphic id="pcbi.1005110.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e109" xlink:type="simple"/><mml:math display="inline" id="M109"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msqrt><mml:mi>n</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. This is much smaller than the <inline-formula id="pcbi.1005110.e110"><alternatives><graphic id="pcbi.1005110.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e110" xlink:type="simple"/><mml:math display="inline" id="M110"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> range of the log ranks, and so Zipf’s law will not emerge.</p>
<p>We have shown that high dimensional latent variable models lead to Zipf’s law under two relatively mild conditions. First, the average entropy of each individual element of the data, <bold>x</bold>, must covary as <italic>z</italic> changes, and the average covariance must be <inline-formula id="pcbi.1005110.e111"><alternatives><graphic id="pcbi.1005110.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e111" xlink:type="simple"/><mml:math display="inline" id="M111"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (again, see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Latent variable models with high dimensional non-conditionally independent data</italic>, for the definition of elementwise entropy for non-independent models). Second, <inline-formula id="pcbi.1005110.e112"><alternatives><graphic id="pcbi.1005110.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e112" xlink:type="simple"/><mml:math display="inline" id="M112"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> cannot have holes; that is, it cannot have large regions where the probability approaches zero between regions of non-zero probability. These conditions are typically satisfied for real world data.</p>
<sec id="sec010">
<title>Neural data</title>
<p>Neural data has been shown, in some cases, to obey Zipf’s law [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref007">7</xref>]. Here the data, which consists of spike trains from <italic>n</italic> neurons, is converted to binary vectors, <bold>x</bold>(<italic>t</italic>) = (<italic>x</italic><sub>1</sub>(<italic>t</italic>), <italic>x</italic><sub>2</sub>(<italic>t</italic>), …), with <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) = 1 if neuron <italic>i</italic> spiked in timestep <italic>t</italic> and <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) = 0 if there was no spike. The time index is then ignored, and the vectors are treated as independent draws from a probability distribution.</p>
<p>To model data of this type, we follow [<xref ref-type="bibr" rid="pcbi.1005110.ref007">7</xref>] and assume that each cell has its own probability of firing, which we denote <italic>p</italic><sub><italic>i</italic></sub>(<italic>z</italic>). Here <italic>z</italic>, the latent variable, is the time since stimulus onset. This results in a model in which the distribution over each element conditioned on the latent variable is given by
<disp-formula id="pcbi.1005110.e113"><alternatives><graphic id="pcbi.1005110.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e113" xlink:type="simple"/><mml:math display="block" id="M113"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msup> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
The entropy of an individual element of <bold>x</bold> is, therefore,
<disp-formula id="pcbi.1005110.e114"><alternatives><graphic id="pcbi.1005110.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e114" xlink:type="simple"/><mml:math display="block" id="M114"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mtext>log </mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
The entropy is high when <italic>p</italic><sub><italic>i</italic></sub>(<italic>z</italic>) is close to 1/2, and low when <italic>p</italic><sub><italic>i</italic></sub>(<italic>z</italic>) is close to 0 or 1. Because time bins are typically sufficiently small that the probability of a spike is less than 1/2, probability and entropy are positively correlated. Thus, if the latent variable (time since stimulus onset) strongly and coherently modulates most cells’ firing probabilities—with high probabilities soon after stimulus onset (giving high entropy), and low probabilities long after stimulus onset (giving low entropy)—then the changes in entropy across different cells will reinforce, giving an <inline-formula id="pcbi.1005110.e115"><alternatives><graphic id="pcbi.1005110.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e115" xlink:type="simple"/><mml:math display="inline" id="M115"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> change in entropy, and thus <inline-formula id="pcbi.1005110.e116"><alternatives><graphic id="pcbi.1005110.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> variance.</p>
<p>In our data, we do indeed see that firing rates are strongly and coherently modulated by the stimulus—firing rates are high just after stimulus onset, but they fall off as time goes by (<xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6A</xref>). Thus, when we combine data across all times, we see a broad distribution over energy (black line in <xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6B</xref>), and hence Zipf’s law (black line in <xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6C</xref>). However, in any one time bin the firing rates do not vary much from one presentation of the stimulus to another, and so the energy distribution is relatively narrow (coloured lines in <xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6B</xref>). Consequently, Zipf’s law is not obeyed (or at least is obeyed less strongly; coloured lines in <xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6C</xref>).</p>
<fig id="pcbi.1005110.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005110.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Neural data recorded from 30 mouse retinal ganglion cells stimulated by full-field illumination; see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Experimental methods</italic>, for details.</title>
<p><bold>A</bold>. Spike trains from all 30 neurons. Note that the firing rates are strongly correlated across time. <bold>B</bold>. <inline-formula id="pcbi.1005110.e117"><alternatives><graphic id="pcbi.1005110.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (coloured lines) when time relative to stimulus onset is the latent variable (see text and <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Experimental methods</italic>). The thick black line is <inline-formula id="pcbi.1005110.e118"><alternatives><graphic id="pcbi.1005110.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e118" xlink:type="simple"/><mml:math display="inline" id="M118"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <bold>C</bold>. Zipf plots for the data conditioned on time (coloured lines) and for all the data (black line). The red lines have slope −1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.g006" xlink:type="simple"/>
</fig>
<p>In our model of the neural data, <xref ref-type="disp-formula" rid="pcbi.1005110.e113">Eq (23)</xref>, and in the neural data itself (<xref ref-type="sec" rid="sec013">Methods</xref> <italic>Experimental methods</italic>), we assumed that the <italic>x</italic><sub><italic>i</italic></sub> were independent conditioned on the latent variable. However, the independence assumption was not critical; it was made primarily to simplify the analysis. What is critical is that there is a latent variable that controls the population averaged firing rate, such that variations in the population averaged firing rate are <inline-formula id="pcbi.1005110.e119"><alternatives><graphic id="pcbi.1005110.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e119" xlink:type="simple"/><mml:math display="inline" id="M119"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>—much larger than expected for neurons that are either independent or very weakly correlated. When that happens, the variance of the energy scales as <italic>n</italic><sup>2</sup> (as has been observed [<xref ref-type="bibr" rid="pcbi.1005110.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>]), and Zipf’s law emerges (see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>High dimensional latent variables</italic>).</p>
</sec>
</sec>
<sec id="sec011">
<title>Exponential family latent variable models</title>
<p>Recently, Schwab <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>] showed that a relatively broad class of models for high-dimensional data, a generalization of a so-called superstatistical latent variable model [<xref ref-type="bibr" rid="pcbi.1005110.ref026">26</xref>],
<disp-formula id="pcbi.1005110.e120"><alternatives><graphic id="pcbi.1005110.e120g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e120" xlink:type="simple"/><mml:math display="block" id="M120"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">g</mml:mi></mml:mfenced> <mml:mo>∝</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mi>g</mml:mi> <mml:mi>μ</mml:mi></mml:msub> <mml:msub><mml:mi>O</mml:mi> <mml:mi>μ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
can give rise to Zipf’s law. Importantly, in Schwab’s model, when they refer to “latent variables,” they are not referring to our fully general latent variables (which we call <italic>z</italic>) but to <italic>g</italic><sub><italic>μ</italic></sub>, the natural parameters of an exponential family distribution. To make this explicit, and to also make contact with our model, we rewrite <xref ref-type="disp-formula" rid="pcbi.1005110.e120">Eq (25)</xref> as
<disp-formula id="pcbi.1005110.e121"><alternatives><graphic id="pcbi.1005110.e121g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e121" xlink:type="simple"/><mml:math display="block" id="M121"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>∝</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mi>g</mml:mi> <mml:mi>μ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>O</mml:mi> <mml:mi>μ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
where the dimensionality of <italic>z</italic> can be lower than <italic>m</italic> (see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Exponential family latent variable models: technical details</italic> for the link between Eqs <xref ref-type="disp-formula" rid="pcbi.1005110.e120">(25)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005110.e121">(26)</xref>).</p>
<p>If <italic>m</italic> were allowed to be arbitrarily large, <xref ref-type="disp-formula" rid="pcbi.1005110.e121">Eq (26)</xref> could describe any distribution <italic>P</italic> (<bold>x</bold>|<italic>z</italic>). However, under Schwab <italic>et al.</italic>’s model <italic>m</italic> can’t be arbitrarily large; it must be much less than <italic>n</italic> (as we show explicitly in Methods <italic>Exponential family latent variable models: technical details</italic>). This puts several restrictions on Schwab <italic>et al.</italic>’s model class. In particular, it does not include many flexible models that have been fit to data. A simple example is our model of neural data (<xref ref-type="disp-formula" rid="pcbi.1005110.e113">Eq (23)</xref>). Writing this distribution in exponential family form gives
<disp-formula id="pcbi.1005110.e122"><alternatives><graphic id="pcbi.1005110.e122g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e122" xlink:type="simple"/><mml:math display="block" id="M122"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>∝</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mtext>log </mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>p</mml:mi> <mml:mi>μ</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>μ</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
Even though there is only one “real” latent variable, <italic>z</italic> (the time since stimulus onset), there are <italic>n</italic> natural parameters, <italic>g</italic><sub><italic>μ</italic></sub> = log(<italic>p</italic><sub><italic>μ</italic></sub>(<italic>z</italic>)<sup>−1</sup> − 1). Consequently, this distribution falls outside of Schwab <italic>et al.</italic>’s model class. This is but one example; more generally, any distribution with <italic>n</italic> natural parameters <italic>g</italic><sub><italic>μ</italic></sub>(<italic>z</italic>) falls outside of Schwab <italic>et al.</italic>’s model class whenever the <italic>g</italic><sub><italic>μ</italic></sub>(<italic>z</italic>) have a nontrivial dependence on <italic>μ</italic> and <italic>z</italic> (as they did in <xref ref-type="disp-formula" rid="pcbi.1005110.e122">Eq (27)</xref>). This includes models in which sequence length is the latent variable, as these models require a large number of natural parameters (something that is not immediately obvious; see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Exponential family latent variable models: technical details</italic>).</p>
<p>The restriction to a small number of natural parameters also rules out high dimensional latent variable models—models in which the number of latent variable is on the order of <italic>n</italic>. That is because such models would require at least <inline-formula id="pcbi.1005110.e123"><alternatives><graphic id="pcbi.1005110.e123g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e123" xlink:type="simple"/><mml:math display="inline" id="M123"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> natural parameters, much more than are allowed by Schwab <italic>et al.</italic>’s analysis. Although we have so far restricted our analysis to low dimensional latent variable models, our framework can easily handle high dimensional ones. In fact, the restriction to low dimensional latent variables was needed only to approximate the mean energy by the entropy. That approximation, however, was not necessary; we can instead reason directly: as long as changes in the latent variable (now a high dimensional vector) lead to <inline-formula id="pcbi.1005110.e124"><alternatives><graphic id="pcbi.1005110.e124g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e124" xlink:type="simple"/><mml:math display="inline" id="M124"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> changes in the mean energy—more specifically, as long as the variance of the mean energy with respect to the latent variable is <inline-formula id="pcbi.1005110.e125"><alternatives><graphic id="pcbi.1005110.e125g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e125" xlink:type="simple"/><mml:math display="inline" id="M125"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>—Zipf’s law will emerge. Alternatively, whenever we can reduce a model with a high dimensional latent variable to a model with a low dimensional latent variable, we can use the framework we developed for low dimensional latent variables (see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Exponential family latent variable models: technical details</italic>). The same reduction cannot be carried out on Schwab <italic>et al.</italic>’s model, as in general that will take it out of the exponential family with a small number of natural parameters (see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Exponential family latent variable models: technical details</italic>).</p>
<p>Besides the restrictions associated with a small number of natural parameters, there are two further restrictions; both prevent Schwab <italic>et al.</italic>’s model from applying to word frequencies. First, the observations must be high-dimensional vectors. However, words have no real notion of dimension. In contrast, our theory is applicable even in cases for which there is no notion of dimension (here we are referring to the theory in earlier sections; the later sections on data with variable and high-dimension are only applicable in those cases). Second, the latent variable must be continuous, or sufficiently dense that it can be treated as continuous. However, the latent variable for words is categorical, with a fixed, small number of categories (the part-of-speech).</p>
<p>Finally, our analysis makes it is relatively easy to identify scenarios in which Zipf’s law does not emerge, something that can be hard to do under Schwab <italic>et al.</italic>’s framework. Consider, for example, the following model of data consisting of <italic>n</italic>-dimensional binary vectors,
<disp-formula id="pcbi.1005110.e126"><alternatives><graphic id="pcbi.1005110.e126g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e126" xlink:type="simple"/><mml:math display="block" id="M126"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>∝</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mi>h</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi> <mml:mo form="prefix">cos</mml:mo> <mml:mi>z</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo form="prefix">cos</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi> <mml:mo form="prefix">sin</mml:mo> <mml:mi>z</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo form="prefix">sin</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula>
where <italic>θ</italic><sub><italic>i</italic></sub> ≡ 2<italic>πi</italic>/<italic>n</italic>, <italic>h</italic> and <italic>A</italic> are constant, and <italic>z</italic> ranges from 0 to 2<italic>π</italic>. Although this is in Schwab <italic>et al.</italic>’s model class, it does not display Zipf’s law. To see why, note that it can be written
<disp-formula id="pcbi.1005110.e127"><alternatives><graphic id="pcbi.1005110.e127g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e127" xlink:type="simple"/><mml:math display="block" id="M127"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>∝</mml:mo> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mi>h</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi> <mml:mo form="prefix">cos</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula>
This is a model of place fields on a ring: the activity of neuron <italic>i</italic> is largest when its preferred orientation, <italic>θ</italic><sub><italic>i</italic></sub>, is equal to <italic>z</italic>, and smallest when its preferred orientation is <italic>z</italic> + <italic>π</italic>. Because of the high symmetry of the model, the entropy is almost independent of <italic>z</italic>. In particular, changes in <italic>z</italic> produce <inline-formula id="pcbi.1005110.e128"><alternatives><graphic id="pcbi.1005110.e128g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e128" xlink:type="simple"/><mml:math display="inline" id="M128"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> variations in the entropy (see <xref ref-type="sec" rid="sec013">Methods</xref> <italic>Exponential family latent variable models: technical details</italic>); much smaller than the <inline-formula id="pcbi.1005110.e129"><alternatives><graphic id="pcbi.1005110.e129g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e129" xlink:type="simple"/><mml:math display="inline" id="M129"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> variations needed to produce Zipf’s law.</p>
<p>This example suggests that any model in which changes in the latent variable cause uniform translation of place fields, without changing their height or shape, should not display Zipf’s law. And indeed, non-Zipfian behaviour was found in a numerical study of Gaussian place fields in one dimension [<xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>]. Note, though, that if the amplitude of the place fields (<italic>A</italic> in our model) or the overall firing rate (<italic>h</italic> in our model) depends on a latent variable, then the population would exhibit Zipf’s law. These conclusions emerge easily from our framework, but are harder to extract from that of Schwab <italic>et al.</italic></p>
<p>In conclusion, while Schwab <italic>et al.</italic>’s approach is extremely valuable, it does have some constraints. We were able to relax those constraints, and thus show that latent variables induce Zipf’s law in a wide array of practically relevant cases (word frequencies, data with variable sequence length, and simultaneously recorded neural data). Notably, all of these lie outside the class that Schwab <italic>et al.</italic>’s approach can handle. In addition, our analysis allowed us to easily identify scenarios in which the latent variable model lies in Schwab <italic>et al.</italic>’s model class, but Zipf’s law does not emerge.</p>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>We have shown that it is possible to understand, and explain, Zipf’s law in a variety of domains. Our explanation consists of two parts. First, we derived an exact relationship between the shape of a distribution over log frequencies (energies) and Zipf’s law. In particular, we showed that the broader the distribution, the closer the data comes to obeying Zipf’s law. This was an extension of previous work showing that if a dataset has a broad, and perfectly flat, distribution over log frequencies (e.g. if a random draw gives very common elements, like “a” and rare elements, like “frequencies” the same proportion of the time), then Zipf’s law must emerge [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>]. Importantly, our extension allowed us to reason about how deviations from a perfectly flat distribution over energy manifest in Zipf plots. Second, we showed that if there is a latent variable that controls the typical frequency of observations, then mixing together different settings of the latent variable gives a broad range of frequencies, and hence Zipf’s law. This is true even if the distributions over frequency conditioned on the latent variable are very narrow. Thus, Zipf’s law can emerge when we mix together multiple non-Zipfian distributions. This is important because non-Zipfian distributions are the typical case, and are thus easy to understand.</p>
<p>When Zipf’s law is observed, it is an empirical question whether or not it is due to our mechanism. Motivated by this observation, we derive a measure (percentage of explained variance, or PEEV) that allows us to separate out, and account for, the contribution of different latent variables to the observation of Zipf’s law. We found that our mechanism was indeed operative in three domains: word frequencies, data with variable sequence length, and neural data. We were also able to show that while variable sequence length can give rise to Zipf’s law on it’s own, it was not the primary cause of Zipf’s law in an antibody sequence dataset.</p>
<p>For words, the latent variable is the part of speech. As we described, parts of speech with a grammatical function (e.g. conjunctions, like “a”) have a few, common words, whereas parts of speech that denote something in the world (e.g. nouns, like “frequencies”) have many, rare words. Varying the latent variable therefore induces a broad range of characteristic energies (or frequencies), giving rise to Zipf’s law.</p>
<p>For data with variable sequence length, we take the latent variable to be the sequence length itself. There are many possible long sequences, so each long sequence is rare (high-energy). In contrast, there are few possible short sequences, so each short sequence is common (low-energy). Mixing across short and long sequences, and everything in between, gives a broad range of energies, and hence Zipf’s law. We examined the role of sequence length in two datasets: randomly generated words and antibody sequences, both of which display Zipf’s law [<xref ref-type="bibr" rid="pcbi.1005110.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>]. For the former, randomly generated words, sequence length was wholly responsible for Zipf’s law. For the latter, antibody sequences, it formed only a small contribution. We were able to make these assessments quantitative, by computing the percentage of explained variance, or PEEV. And indeed, a recent model by Desponds <italic>et al.</italic> indicates that for antibodies, Zipf’s law at each sequence length is most likely due to random growth and decay processes [<xref ref-type="bibr" rid="pcbi.1005110.ref023">23</xref>].</p>
<p>For high-dimensional data, small changes in the energy (or entropy) of each element of the observation can reinforce to give a large change overall, and hence Zipf’s law. As an example, we considered multi-neuron spiking data, for which the latent variable is the time since stimulus onset. Just after stimulus onset, the firing rate of almost every cell (and hence the energy associated with those cells), is elevated. In contrast, long after stimulus onset, the firing rate of almost every cell (and hence the energy associated with those cells) is lower. As all the cells’ energies change in the same direction (high just after stimulus onset, and low long after stimulus onset), the changes reinforce, and so produce <inline-formula id="pcbi.1005110.e130"><alternatives><graphic id="pcbi.1005110.e130g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e130" xlink:type="simple"/><mml:math display="inline" id="M130"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> changes in the total energy. Consequently, whenever the population firing rate varies with time, Zipf’s law will almost always appear. This is true regardless of what is causing the variation: it could be a stimulus, or it could be low dimensional internal network dynamics. Thus, our framework is consistent with the recent observation that in salamander retina the variance of the energy scales as <italic>n</italic><sup>2</sup> (the scaling needed for Zip’s law to emerge), with higher variance when the stimulus induces larger covariation in the firing rates [<xref ref-type="bibr" rid="pcbi.1005110.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>]. This does not, of course, imply that the retina implements an uninteresting transformation from stimulus to neural response. However, our findings do have implications for the interpretation of observations of Zipf’s law.</p>
<p>Our work shows that there are two types of datasets in which we expect Zipf’s law to emerge generically. First, for the reason mentioned above, any dataset in which the sequence length varies (and is thus a latent variable) will display Zipf’s law if the distribution over sequence length is sufficiently broad. Second, any high-dimensional dataset will display Zipf’s law if the entropy of each element of the observation changes with the latent variable, and if those changes are correlated.</p>
<p>Previous authors have pointed out that latent variables models have interesting properties when the data is high-dimensional. As we discussed, Schwab <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>] were the first to show that a relatively broad class of latent variable models describing high-dimensional data give rise to Zipf’s law. Their result, however, carries some restrictions: it applies only to exponential family distributions with continuous latent variables and a small number of natural parameters. We took a far more general approach that relaxes all of these restrictions: it does not require high-dimensional data, continuous latent variables, or an exponential family distribution with a small number of latent variables. Importantly, none of the datasets that we considered lie within the class considered by Schwab <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>]. However, the fact that Schwab <italic>et al.</italic>’s analysis applies to a restricted class of models should not detract from its importance: they were the first that we know of to show that Zipf’s law could arise without fine tuning.</p>
<p>In addition, in work that anticipated some forms of latent variable models, Macke and colleagues examined models with common input [<xref ref-type="bibr" rid="pcbi.1005110.ref027">27</xref>], similar to the model in <xref ref-type="disp-formula" rid="pcbi.1005110.e113">Eq (23)</xref>, as well as simple feedforward spiking neuron models [<xref ref-type="bibr" rid="pcbi.1005110.ref028">28</xref>]. They showed that both exhibit diverging heat capacity, for which the variance of the energy is <inline-formula id="pcbi.1005110.e131"><alternatives><graphic id="pcbi.1005110.e131g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e131" xlink:type="simple"/><mml:math display="inline" id="M131"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Although they did not explicitly explore the connection to Zipf’s law, in the latter study [<xref ref-type="bibr" rid="pcbi.1005110.ref028">28</xref>] they noted that the diverging heat capacity should lead to Zipf’s law.</p>
<p>These findings have important implications in fields as diverse as biology and linguistics. In biology, one explanation for Zipf’s law is that biological systems sit at a special thermodynamic state, the critical point [<xref ref-type="bibr" rid="pcbi.1005110.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>]. However, our findings indicate that Zipf’s law emerges from phenomena much more familiar to biologists: unobserved states that influence the observed data. In fact, as mentioned above, for neural data our analysis shows that Zipf’s law will emerge whenever the average firing rate in a population of neurons varies over time. Such time variation is common in neural systems, and can be due to external stimuli, low dimensional internal dynamics, or both.</p>
<p>For words, we showed that individual parts of speech do not obey Zipf’s law; it is only by mixing together different parts of speech with different characteristic frequencies that Zipf’s law emerges. This has an important consequence for other explanations of Zipf’s law in language. In particular, the observation that individual parts of speech do not obey Zipf’s law is inconsistent with any explanation of Zipf’s law that fails to distinguish between parts of speech [<xref ref-type="bibr" rid="pcbi.1005110.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref029">29</xref>].</p>
<p>In all of these domains, the observation of Zipf’s law is important because it may point to the existence of some latent variable structure. It is that structure, not Zipf’s law itself, that is likely to provide insight into statistical regularities in the world.</p>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Ethics statement</title>
<p>All procedures were performed under the regulation of the Institutional Animal Care and Use Committee of Weill Cornell Medical College (protocol #0807-769A) and in accordance with NIH guidelines.</p>
</sec>
<sec id="sec015">
<title>Experimental methods</title>
<p>The neural data in <xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6</xref> was acquired by electrophysiological recordings of 3 isolated mouse retinas, yielding 30 ganglion cells. The recordings were performed on a multielectrode array using the procedure described in [<xref ref-type="bibr" rid="pcbi.1005110.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005110.ref031">31</xref>]. Full field flashes were presented on a Sony LCD computer monitor, delivering intermittent flashes (2 s of light followed by 2 s of dark, repeated 30 times) of white light to the retina [<xref ref-type="bibr" rid="pcbi.1005110.ref032">32</xref>]. All procedures were performed under the regulation of the Institutional Animal Care and Use Committee of Weill Cornell Medical College (protocol #0807-769A) and in accordance with NIH guidelines.</p>
<p>Spikes were binned at 20 ms, and <italic>x</italic><sub><italic>i</italic></sub> was set to 1 if cell <italic>i</italic> spiked in a bin and zero otherwise. To give us enough samples to plot Zipf’s law, we estimated <italic>p</italic><sub><italic>i</italic></sub>(<italic>z</italic>), the probability that neuron <italic>i</italic> spikes in bin <italic>z</italic>, from data using the model in <xref ref-type="disp-formula" rid="pcbi.1005110.e113">Eq (23)</xref>, and drew 10<sup>6</sup> samples from that model. To construct the distributions of energy conditioned on the latent variable—the coloured lines in <xref ref-type="fig" rid="pcbi.1005110.g006">Fig 6B and 6C</xref>—we treated samples that occurred within 100 ms as if they had the same latent variable (so, for example, <inline-formula id="pcbi.1005110.e132"><alternatives><graphic id="pcbi.1005110.e132g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e132" xlink:type="simple"/><mml:math display="inline" id="M132"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>=</mml:mo> <mml:mn>300</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is shorthand for the smoothed distribution over energy for spike trains in the five bins between 300 and 400 ms). Finally, to reduce clutter, we plotted lines only for <italic>z</italic> = 0 ms, <italic>z</italic> = 300 ms etc.</p>
</sec>
<sec id="sec016">
<title>PEEV, and the law of total variance</title>
<p>The law of total variance [<xref ref-type="bibr" rid="pcbi.1005110.ref024">24</xref>] is well known in statistics; it decomposes the total variance into the sum of two terms. Here we briefly review this law in the context of latent variable models, and then discuss how it is related to PEEV.</p>
<p>The energy, <inline-formula id="pcbi.1005110.e133"><alternatives><graphic id="pcbi.1005110.e133g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e133" xlink:type="simple"/><mml:math display="inline" id="M133"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, can be trivially decomposed as
<disp-formula id="pcbi.1005110.e134"><alternatives><graphic id="pcbi.1005110.e134g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e134" xlink:type="simple"/><mml:math display="block" id="M134"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>+</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula>
where the first term, <inline-formula id="pcbi.1005110.e135"><alternatives><graphic id="pcbi.1005110.e135g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e135" xlink:type="simple"/><mml:math display="inline" id="M135"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, is the mean energy conditioned on <italic>z</italic>,
<disp-formula id="pcbi.1005110.e136"><alternatives><graphic id="pcbi.1005110.e136g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e136" xlink:type="simple"/><mml:math display="block" id="M136"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mi>d</mml:mi> <mml:mi>x</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(31)</label></disp-formula>
The two terms in <xref ref-type="disp-formula" rid="pcbi.1005110.e134">Eq (30)</xref>, <inline-formula id="pcbi.1005110.e137"><alternatives><graphic id="pcbi.1005110.e137g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e137" xlink:type="simple"/><mml:math display="inline" id="M137"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005110.e138"><alternatives><graphic id="pcbi.1005110.e138g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e138" xlink:type="simple"/><mml:math display="inline" id="M138"><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, are uncorrelated, so the variance of <inline-formula id="pcbi.1005110.e139"><alternatives><graphic id="pcbi.1005110.e139g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e139" xlink:type="simple"/><mml:math display="inline" id="M139"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the sum of their variances,
<disp-formula id="pcbi.1005110.e140"><alternatives><graphic id="pcbi.1005110.e140g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e140" xlink:type="simple"/><mml:math display="block" id="M140"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>x</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mi>z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(32)</label></disp-formula>
where Var<sub><italic>x</italic></sub>[…] is the variance with respect to <italic>P</italic> (<italic>x</italic>) and Var<sub><italic>x</italic>,<italic>z</italic></sub>[…] is the variance with respect to <italic>P</italic> (<italic>x</italic>, <italic>z</italic>). As is straightforward to show, the second term can be rearranged to give the law of total variance,
<disp-formula id="pcbi.1005110.e141"><alternatives><graphic id="pcbi.1005110.e141g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e141" xlink:type="simple"/><mml:math display="block" id="M141"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>x</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(33)</label></disp-formula>
This is the same as <xref ref-type="disp-formula" rid="pcbi.1005110.e063">Eq (17)</xref> of the main text, except here we use <italic>x</italic> rather than <bold>x</bold>.</p>
<p>We can identify two contributions to the variance. The first, <inline-formula id="pcbi.1005110.e142"><alternatives><graphic id="pcbi.1005110.e142g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e142" xlink:type="simple"/><mml:math display="inline" id="M142"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, is the variance of the expected energy, <inline-formula id="pcbi.1005110.e143"><alternatives><graphic id="pcbi.1005110.e143g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e143" xlink:type="simple"/><mml:math display="inline" id="M143"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, induced by changes in the latent variable, <italic>z</italic>. This represents the contribution to the total energy variance from the latent variable (i.e. the contribution from changes in the peak of <inline-formula id="pcbi.1005110.e144"><alternatives><graphic id="pcbi.1005110.e144g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e144" xlink:type="simple"/><mml:math display="inline" id="M144"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as <italic>z</italic> changes) and, under our mechanism, is the contribution that gives rise to Zipf’s law. The second, <inline-formula id="pcbi.1005110.e145"><alternatives><graphic id="pcbi.1005110.e145g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e145" xlink:type="simple"/><mml:math display="inline" id="M145"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, is the variance of the energy, <inline-formula id="pcbi.1005110.e146"><alternatives><graphic id="pcbi.1005110.e146g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e146" xlink:type="simple"/><mml:math display="inline" id="M146"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, for a fixed setting of the latent variable, averaged over the latent variable, <italic>z</italic>. This represents the contribution from the width of <inline-formula id="pcbi.1005110.e147"><alternatives><graphic id="pcbi.1005110.e147g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e147" xlink:type="simple"/><mml:math display="inline" id="M147"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The proportion of explained energy variance (PEEV)—that is, the portion explained by the first contribution—is the ratio of the first quantity to the total variance of the energy,
<disp-formula id="pcbi.1005110.e148"><alternatives><graphic id="pcbi.1005110.e148g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e148" xlink:type="simple"/><mml:math display="block" id="M148"><mml:mrow><mml:mtext>PEEV</mml:mtext> <mml:mo>≡</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mfenced></mml:mrow> <mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>x</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(34)</label></disp-formula>
This quantity ranges from 0, indicating that <italic>z</italic> explains none of the energy variance, to 1, indicating that <italic>z</italic> explains all of the energy variance. PEEV therefore describes how much the latent variable contributes to the observation of Zipf’s law, though it should be remembered that PEEV may be large even if the total energy variance is narrow, and hence Zipf’s law is not obeyed.</p>
<sec id="sec017">
<title>Computing PEEV</title>
<p>To compute PEEV, we need to estimate, from data, the distribution over energy given the latent variable, and the distribution over the latent variable. Here we consider the case in which the latent variable is category, and each observation, <italic>x</italic>, falls into a single, known, category. In more realistic cases, <italic>P</italic> (<italic>z</italic>|<italic>x</italic>) must be estimated from a model and <italic>P</italic> (<italic>x</italic>) from data, from which <italic>P</italic> (<italic>x</italic>|<italic>z</italic>) and <italic>P</italic> (<italic>z</italic>) can be obtained using Bayes’ theorem.</p>
<p>The starting point is the number of observations, and the category, of each possible value of <italic>x</italic>. For instance, for words, we took a list of words, their frequencies, and their parts of speech from [<xref ref-type="bibr" rid="pcbi.1005110.ref021">21</xref>]. We then used the frequencies to estimate the probability of each observation, and, finally, turned those into an energy via <xref ref-type="disp-formula" rid="pcbi.1005110.e003">Eq (3)</xref>: <inline-formula id="pcbi.1005110.e149"><alternatives><graphic id="pcbi.1005110.e149g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e149" xlink:type="simple"/><mml:math display="inline" id="M149"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The empirical distribution over energy, <inline-formula id="pcbi.1005110.e150"><alternatives><graphic id="pcbi.1005110.e150g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e150" xlink:type="simple"/><mml:math display="inline" id="M150"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and over energy given the latent variable, <inline-formula id="pcbi.1005110.e151"><alternatives><graphic id="pcbi.1005110.e151g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e151" xlink:type="simple"/><mml:math display="inline" id="M151"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, was therefore a set of delta functions, with each delta-function weighted by the probability of its corresponding observation,
<disp-formula id="pcbi.1005110.e152"><alternatives><graphic id="pcbi.1005110.e152g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e152" xlink:type="simple"/><mml:math display="block" id="M152"><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mi>x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>−</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(35)</label></disp-formula> <disp-formula id="pcbi.1005110.e153"><alternatives><graphic id="pcbi.1005110.e153g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e153" xlink:type="simple"/><mml:math display="block" id="M153"><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mi>z</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mi>x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mi>z</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>−</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(36)</label></disp-formula>
The first equation is the same as <xref ref-type="disp-formula" rid="pcbi.1005110.e009">Eq (6)</xref>; it is repeated here for convenience.</p>
<p>To compute the terms relevant to PEEV (<xref ref-type="disp-formula" rid="pcbi.1005110.e148">Eq (34)</xref>), we need moments of both the total energy and the energy conditioned on <italic>z</italic>. These are given, respectively, by
<disp-formula id="pcbi.1005110.e154"><alternatives><graphic id="pcbi.1005110.e154g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e154" xlink:type="simple"/><mml:math display="block" id="M154"><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mi>x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(37)</label></disp-formula> <disp-formula id="pcbi.1005110.e155"><alternatives><graphic id="pcbi.1005110.e155g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e155" xlink:type="simple"/><mml:math display="block" id="M155"><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mi>x</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(38)</label></disp-formula>
Then, to compute the variances required for PEEV, we use
<disp-formula id="pcbi.1005110.e156"><alternatives><graphic id="pcbi.1005110.e156g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e156" xlink:type="simple"/><mml:math display="block" id="M156"><mml:msub><mml:mtext>Var</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(39)</label></disp-formula> <disp-formula id="pcbi.1005110.e157"><alternatives><graphic id="pcbi.1005110.e157g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e157" xlink:type="simple"/><mml:math display="block" id="M157"><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(40)</label></disp-formula>
where
<disp-formula id="pcbi.1005110.e158"><alternatives><graphic id="pcbi.1005110.e158g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e158" xlink:type="simple"/><mml:math display="block" id="M158"><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>x</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(41)</label></disp-formula> <disp-formula id="pcbi.1005110.e159"><alternatives><graphic id="pcbi.1005110.e159g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e159" xlink:type="simple"/><mml:math display="block" id="M159"><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mi>k</mml:mi></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mi>z</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>k</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(42)</label></disp-formula></p>
</sec>
</sec>
<sec id="sec018">
<title>Var[log <italic>P</italic> (<italic>z</italic>)] is <inline-formula id="pcbi.1005110.e160"><alternatives><graphic id="pcbi.1005110.e160g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e160" xlink:type="simple"/><mml:math display="inline" id="M160"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>log Var</mml:mtext> <mml:mo>[</mml:mo> <mml:mi>z</mml:mi> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></title>
<p>To compute the variance of the energy for variable length data, we stated that the variance of log <italic>P</italic> (<italic>z</italic>) is small compared to the variance of <italic>z</italic> (see in particular <xref ref-type="disp-formula" rid="pcbi.1005110.e057">Eq (15)</xref>). Here we first show that for Li’s model [<xref ref-type="bibr" rid="pcbi.1005110.ref012">12</xref>], the variance of log <italic>P</italic> (<italic>z</italic>) is <inline-formula id="pcbi.1005110.e161"><alternatives><graphic id="pcbi.1005110.e161g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e161" xlink:type="simple"/><mml:math display="inline" id="M161"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>; we then show that in general the variance of log <italic>P</italic> (<italic>z</italic>) is at most <inline-formula id="pcbi.1005110.e162"><alternatives><graphic id="pcbi.1005110.e162g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e162" xlink:type="simple"/><mml:math display="inline" id="M162"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>log</mml:mtext><mml:msub><mml:mtext> Var</mml:mtext> <mml:mrow/></mml:msub> <mml:mo>[</mml:mo> <mml:mi>z</mml:mi> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>For Li’s model, the probability of observing a sequence of length <italic>z</italic> is proportional to the probability of drawing <italic>z</italic> letters followed by a blank. For an alphabet with <italic>M</italic> letters, this is given by
<disp-formula id="pcbi.1005110.e163"><alternatives><graphic id="pcbi.1005110.e163g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e163" xlink:type="simple"/><mml:math display="block" id="M163"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>M</mml:mi></mml:mfrac> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mi>M</mml:mi> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced> <mml:mi>z</mml:mi></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(43)</label></disp-formula>
The leading factor of 1/<italic>M</italic> ensures that the distribution is properly normalized (note that <italic>z</italic> ranges from 1 to ∞). Given this distribution, it is straightforward to show that
<disp-formula id="pcbi.1005110.e164"><alternatives><graphic id="pcbi.1005110.e164g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e164" xlink:type="simple"/><mml:math display="block" id="M164"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:mi>M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>M</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mtext>log </mml:mtext><mml:mfenced close="]" open="[" separators=""><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>M</mml:mi></mml:mfrac></mml:mfenced></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(44)</label></disp-formula>
Using the fact that log(1 + <italic>ϵ</italic>) ≤ <italic>ϵ</italic>, we see that the right hand side is bounded by (<italic>M</italic> + 1)/<italic>M</italic>. Thus, for Li’s model, Var<sub><italic>z</italic></sub>[log <italic>P</italic> (<italic>z</italic>)] is indeed <inline-formula id="pcbi.1005110.e165"><alternatives><graphic id="pcbi.1005110.e165g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e165" xlink:type="simple"/><mml:math display="inline" id="M165"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>To understand how the variance of log <italic>P</italic> (<italic>z</italic>) scales in general, we note that the variance is bounded by the second moment,
<disp-formula id="pcbi.1005110.e166"><alternatives><graphic id="pcbi.1005110.e166g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e166" xlink:type="simple"/><mml:math display="block" id="M166"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>z</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>]</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:munder><mml:mo>∑</mml:mo> <mml:mi>z</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>≤</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>z</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>]</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(45)</label></disp-formula>
Shortly we’ll maximize the second moment with the variance of <italic>z</italic> fixed. When we do that, we find that the second moment is small compared to <inline-formula id="pcbi.1005110.e167"><alternatives><graphic id="pcbi.1005110.e167g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e167" xlink:type="simple"/><mml:math display="inline" id="M167"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, the variance of <italic>z</italic>. However, the analysis is somewhat complicated, so first we provide the intuition.</p>
<p>The main idea is to note that for unimodal distributions, the number of sequence lengths with appreciable probability is proportional to the standard deviation of <italic>z</italic>. If we make the (rather crude) approximation that <italic>P</italic> (<italic>z</italic>) is nonzero only for <italic>n</italic><sub>0</sub> sequence lengths, where <italic>n</italic><sub>0</sub> ∝ <italic>σ</italic><sub><italic>z</italic></sub>, then the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005110.e166">Eq (45)</xref> is maximum when <italic>P</italic> (<italic>z</italic>) = 1/<italic>n</italic><sub>0</sub>, and the corresponding value is (log <italic>n</italic><sub>0</sub>)<sup>2</sup>. Consequently, the second moment of log <italic>P</italic> (<italic>z</italic>) is at most <inline-formula id="pcbi.1005110.e168"><alternatives><graphic id="pcbi.1005110.e168g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e168" xlink:type="simple"/><mml:math display="inline" id="M168"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, giving us the very approximate bound
<disp-formula id="pcbi.1005110.e169"><alternatives><graphic id="pcbi.1005110.e169g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e169" xlink:type="simple"/><mml:math display="block" id="M169"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>≤</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mtext>log </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mfenced> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives> <label>(46)</label></disp-formula>
where we used <inline-formula id="pcbi.1005110.e170"><alternatives><graphic id="pcbi.1005110.e170g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e170" xlink:type="simple"/><mml:math display="inline" id="M170"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>log </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>This does indeed turn out to be the correct bound. To show that rigorously, we take the usual approach: we use Lagrange multipliers to maximize the second moment of log <italic>P</italic> (<italic>z</italic>) with constraints on the total probability and the variance. This gives us
<disp-formula id="pcbi.1005110.e171"><alternatives><graphic id="pcbi.1005110.e171g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e171" xlink:type="simple"/><mml:math display="block" id="M171"><mml:mrow><mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced></mml:mrow></mml:mfrac> <mml:mfenced close="]" open="[" separators=""><mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mfenced> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mfenced></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mfenced> <mml:msup><mml:mrow><mml:msup><mml:mi>z</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(47)</label></disp-formula>
where <italic>μ</italic> is the mean value of <italic>z</italic>,
<disp-formula id="pcbi.1005110.e172"><alternatives><graphic id="pcbi.1005110.e172g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e172" xlink:type="simple"/><mml:math display="block" id="M172"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>≡</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>z</mml:mi></mml:munder> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mi>z</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(48)</label></disp-formula>
We use <italic>γ</italic><sup>2</sup>+<italic>α</italic><sup>2</sup> − 1 and <italic>γ</italic><sup>2</sup> <italic>Z</italic><sup>2</sup>/<italic>e</italic><sup>2</sup> as our Lagrange multiplier to simplify later expressions. As is straightforward to show (taking into account the fact that <italic>μ</italic> depends on <italic>P</italic> (<italic>z</italic>)), <xref ref-type="disp-formula" rid="pcbi.1005110.e171">Eq (47)</xref> is satisfied when <italic>P</italic> (<italic>z</italic>) is given by
<disp-formula id="pcbi.1005110.e173"><alternatives><graphic id="pcbi.1005110.e173g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e173" xlink:type="simple"/><mml:math display="block" id="M173"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mfenced> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(49)</label></disp-formula></p>
<p>The parameters <italic>γ</italic>, <italic>α</italic> and <italic>Z</italic> must be chosen so that <italic>P</italic> (<italic>z</italic>) is normalized to 1 and has variance <inline-formula id="pcbi.1005110.e174"><alternatives><graphic id="pcbi.1005110.e174g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e174" xlink:type="simple"/><mml:math display="inline" id="M174"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. However, because <italic>z</italic> is a positive integer, finding these parameters analytically is, as far as we know, not possible. We can, though, make two approximations that ultimately do yield analytic expressions. The first is to allow <italic>z</italic> to be continuous. This turns sums (which are needed to compute moments) into integrals, and results in an error in those sums that scales as 1/<italic>σ</italic><sub><italic>z</italic></sub>. That error is negligible in the limit that <italic>σ</italic><sub><italic>z</italic></sub> is large (the limit of interest here). The second is to allow <italic>z</italic> to be negative. This will increase the maximum second moment of log <italic>P</italic> (<italic>z</italic>) at fixed <inline-formula id="pcbi.1005110.e175"><alternatives><graphic id="pcbi.1005110.e175g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e175" xlink:type="simple"/><mml:math display="inline" id="M175"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> (because we are expanding the space of probability distributions), and so result in a slightly looser bound. But the bound will be sufficiently tight for our purposes.</p>
<p>The problem of choosing the parameters <italic>γ</italic>, <italic>α</italic> and <italic>Z</italic> is now much simpler, as we can do integrals rather than sums. We proceed in three steps: first, we show that none of the relevant moments depend on <italic>μ</italic>, so we set it to zero and at the same time eliminate <italic>α</italic>; second, we use the fact that <italic>P</italic> (<italic>z</italic>) must be properly normalized to express <italic>Z</italic> in terms of <italic>γ</italic>; and third, we explicitly compute the second moment of log <italic>P</italic> (<italic>z</italic>) and the variance of <italic>σ</italic><sup>2</sup>.</p>
<p>To see that the second moment of <italic>P</italic> (<italic>z</italic>) and the variance of <italic>z</italic> do not depend on <italic>μ</italic>, make the change of variables <italic>z</italic> = <italic>z</italic>′ + <italic>μ</italic> and let <italic>α</italic><sup>2</sup> = <italic>γ</italic><sup>2</sup> <italic>Z</italic><sup>2</sup> <italic>μ</italic><sup>2</sup>/<italic>e</italic><sup>2</sup>. That yields a distribution <italic>P</italic> (<italic>z</italic>′) that is independent of <italic>μ</italic>. Thus, <italic>μ</italic> does not effect either the second moment of log <italic>P</italic> (<italic>z</italic>) or the variance of <italic>z</italic>, and so without loss of generality we can set both <italic>μ</italic> and <italic>α</italic> to zero. We thus have
<disp-formula id="pcbi.1005110.e176"><alternatives><graphic id="pcbi.1005110.e176g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e176" xlink:type="simple"/><mml:math display="block" id="M176"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfenced> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(50)</label></disp-formula>
It is convenient to make the change of variables <italic>z</italic> = <italic>ye</italic>/<italic>Z</italic>, yielding
<disp-formula id="pcbi.1005110.e177"><alternatives><graphic id="pcbi.1005110.e177g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e177" xlink:type="simple"/><mml:math display="block" id="M177"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>y</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup> <mml:mrow><mml:mi>Z</mml:mi> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(51)</label></disp-formula>
where <italic>Z</italic>, which now depends on <italic>γ</italic> to ensure that <italic>P</italic> (<italic>z</italic>) (and thus <italic>P</italic> (<italic>y</italic>)) is properly normalized, is given by
<disp-formula id="pcbi.1005110.e178"><alternatives><graphic id="pcbi.1005110.e178g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e178" xlink:type="simple"/><mml:math display="block" id="M178"><mml:mrow><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>y</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(52)</label></disp-formula></p>
<p>In terms of <italic>P</italic> (<italic>y</italic>), the two quantities of interest are
<disp-formula id="pcbi.1005110.e179"><alternatives><graphic id="pcbi.1005110.e179g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e179" xlink:type="simple"/><mml:math display="block" id="M179"><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtext>log</mml:mtext> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(53)</label></disp-formula> <disp-formula id="pcbi.1005110.e180"><alternatives><graphic id="pcbi.1005110.e180g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e180" xlink:type="simple"/><mml:math display="block" id="M180"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:msup><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>y</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(54)</label></disp-formula>
These expectations can be expressed as modified Bessel functions of the second kind (as can be seen by making the change of variables <italic>y</italic> = sinh <italic>θ</italic>). However, the resulting expressions are not very useful, so instead, we consider two easy limits: large and small <italic>γ</italic>. In the large <italic>γ</italic> limit, <italic>P</italic> (<italic>y</italic>) is Gaussian, yielding
<disp-formula id="pcbi.1005110.e181"><alternatives><graphic id="pcbi.1005110.e181g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e181" xlink:type="simple"/><mml:math display="block" id="M181"><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow> <mml:mrow><mml:mi>γ</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtext>log</mml:mtext> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:mn>3</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(55)</label></disp-formula> <disp-formula id="pcbi.1005110.e182"><alternatives><graphic id="pcbi.1005110.e182g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e182" xlink:type="simple"/><mml:math display="block" id="M182"><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow> <mml:mrow><mml:mi>γ</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:munder> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>γ</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(56)</label></disp-formula>
And in the small <italic>γ</italic> limit, <italic>P</italic> (<italic>y</italic>) is Laplacian, and we have
<disp-formula id="pcbi.1005110.e183"><alternatives><graphic id="pcbi.1005110.e183g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e183" xlink:type="simple"/><mml:math display="block" id="M183"><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow> <mml:mrow><mml:mi>γ</mml:mi> <mml:mo>→</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:munder> <mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtext>log</mml:mtext> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>5</mml:mn> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(57)</label></disp-formula> <disp-formula id="pcbi.1005110.e184"><alternatives><graphic id="pcbi.1005110.e184g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e184" xlink:type="simple"/><mml:math display="block" id="M184"><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow> <mml:mrow><mml:mi>γ</mml:mi> <mml:mo>→</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:munder> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:math></alternatives> <label>(58)</label></disp-formula>
As is straightforward to show, in both limits the second moment of log <italic>P</italic> (<italic>z</italic>) obeys the inequality
<disp-formula id="pcbi.1005110.e185"><alternatives><graphic id="pcbi.1005110.e185g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e185" xlink:type="simple"/><mml:math display="block" id="M185"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfenced> <mml:mo>≤</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>log </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>z</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mfenced> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives> <label>(59)</label></disp-formula>
where
<disp-formula id="pcbi.1005110.e186"><alternatives><graphic id="pcbi.1005110.e186g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e186" xlink:type="simple"/><mml:math display="block" id="M186"><mml:mrow><mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msqrt><mml:mn>20</mml:mn></mml:msqrt> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>≈</mml:mo> <mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mn>58</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(60)</label></disp-formula>
We verified numerically that the inequality in <xref ref-type="disp-formula" rid="pcbi.1005110.e185">Eq (59)</xref> is satisfied over the whole range of <italic>γ</italic>, from 0 to ∞. Thus, although very naive arguments were used to derive the bound given in <xref ref-type="disp-formula" rid="pcbi.1005110.e169">Eq (46)</xref>, it is substantially correct.</p>
</sec>
<sec id="sec019">
<title>Models in which the latent variable is the sequence length</title>
<p>For models in which the sequence length is the latent variable, for Zipf’s law to hold the energy must be proportional to the sequence length, <italic>z</italic>; that is, the energy must be <inline-formula id="pcbi.1005110.e187"><alternatives><graphic id="pcbi.1005110.e187g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e187" xlink:type="simple"/><mml:math display="inline" id="M187"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. To determine whether this scaling holds, we start with <xref ref-type="disp-formula" rid="pcbi.1005110.e055">Eq (13)</xref> of the main text, which tells us that when the latent variable is sequence length, the total distribution is a simple function of the latent variables: <italic>P</italic> (<bold>x</bold>) = <italic>P</italic> (<bold>x</bold>|<italic>z</italic>)<italic>P</italic> (<italic>z</italic>) where <italic>z</italic> is the dimension of <bold>x</bold> (the sequence length). Thus, the energy is given by
<disp-formula id="pcbi.1005110.e188"><alternatives><graphic id="pcbi.1005110.e188g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e188" xlink:type="simple"/><mml:math display="block" id="M188"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>z</mml:mi></mml:munderover> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(61)</label></disp-formula>
where
<disp-formula id="pcbi.1005110.e189"><alternatives><graphic id="pcbi.1005110.e189g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e189" xlink:type="simple"/><mml:math display="block" id="M189"><mml:mrow><mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>…</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(62)</label></disp-formula>
Assuming the value of <italic>x</italic><sub><italic>i</italic></sub> isn’t perfectly determined by the values of <italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub><italic>i</italic>−1</sub> (the typical case), each term in the sum over <italic>z</italic> is <inline-formula id="pcbi.1005110.e190"><alternatives><graphic id="pcbi.1005110.e190g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e190" xlink:type="simple"/><mml:math display="inline" id="M190"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and so the first term in <xref ref-type="disp-formula" rid="pcbi.1005110.e188">Eq (61)</xref> is <inline-formula id="pcbi.1005110.e191"><alternatives><graphic id="pcbi.1005110.e191g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e191" xlink:type="simple"/><mml:math display="inline" id="M191"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. As we saw in the previous section, the variance of log <italic>P</italic> (<italic>z</italic>) is small compared to the variance of <italic>z</italic>. Consequently, the energy is <inline-formula id="pcbi.1005110.e192"><alternatives><graphic id="pcbi.1005110.e192g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e192" xlink:type="simple"/><mml:math display="inline" id="M192"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec020">
<title>Latent variable models with high dimensional non-conditionally independent data</title>
<p>In the main text we argued that for a conditionally independent model—a model in which each element of <bold>x</bold> is independent conditioned on <italic>z</italic>—the variance of the entropy typically scales as <italic>n</italic><sup>2</sup>. Extending this argument to complex joint distribution is straightforward, and, in fact, follows closely the method used in the previous section.</p>
<p>The first step is to note that, just as in the conditionally independent case, log <italic>P</italic> (<bold>x</bold>|<italic>z</italic>) can be written as a sum over each element of <italic>x</italic><sub><italic>i</italic></sub>,
<disp-formula id="pcbi.1005110.e193"><alternatives><graphic id="pcbi.1005110.e193g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e193" xlink:type="simple"/><mml:math display="block" id="M193"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>,</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(63)</label></disp-formula>
Taking the expectation with respect to <italic>P</italic> (<bold>x</bold>|<italic>z</italic>) (and negating) gives the entropy, which consists of a sum of <italic>n</italic> terms,
<disp-formula id="pcbi.1005110.e194"><alternatives><graphic id="pcbi.1005110.e194g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e194" xlink:type="simple"/><mml:math display="block" id="M194"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(64)</label></disp-formula>
where <italic>h</italic><sub><italic>i</italic></sub>(<italic>z</italic>) is the entropy of <italic>P</italic> (<italic>x</italic><sub><italic>i</italic></sub>|<italic>z</italic>, <italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>i</italic>−1</sub>), averaged over <italic>x</italic><sub>1</sub> to <italic>x</italic><sub><italic>i</italic>−1</sub>, with <italic>z</italic> fixed,
<disp-formula id="pcbi.1005110.e195"><alternatives><graphic id="pcbi.1005110.e195g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e195" xlink:type="simple"/><mml:math display="block" id="M195"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>,</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(65)</label></disp-formula>
The variance of the entropy is thus given by
<disp-formula id="pcbi.1005110.e196"><alternatives><graphic id="pcbi.1005110.e196g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e196" xlink:type="simple"/><mml:math display="block" id="M196"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(66)</label></disp-formula>
Just as in the main text, if the individual entropies (the <italic>h</italic><sub><italic>i</italic></sub>) have, on average, <inline-formula id="pcbi.1005110.e197"><alternatives><graphic id="pcbi.1005110.e197g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e197" xlink:type="simple"/><mml:math display="inline" id="M197"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> covariance as <italic>z</italic> changes, then the variance of the entropy is <inline-formula id="pcbi.1005110.e198"><alternatives><graphic id="pcbi.1005110.e198g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e198" xlink:type="simple"/><mml:math display="inline" id="M198"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. This illuminates a special case in which we do not see Zipf’s law: if the <italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>i</italic>−1</sub> determine the value of <italic>x</italic><sub><italic>i</italic></sub> when <italic>i</italic> &gt; <italic>i</italic><sub>0</sub> (independent of <italic>n</italic>), then the entropy, <italic>h</italic><sub><italic>i</italic></sub>, is zero whenever <italic>i</italic> &gt; <italic>i</italic><sub>0</sub>. If this were to happen, the variance of the entropy would scale at most as <inline-formula id="pcbi.1005110.e199"><alternatives><graphic id="pcbi.1005110.e199g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e199" xlink:type="simple"/><mml:math display="inline" id="M199"><mml:msubsup><mml:mi>i</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, independent of <italic>n</italic>; far smaller than the required <inline-formula id="pcbi.1005110.e200"><alternatives><graphic id="pcbi.1005110.e200g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e200" xlink:type="simple"/><mml:math display="inline" id="M200"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> scaling. However, for most types of data, including neural data, each neuron has considerable independent noise (due, for instance, to synaptic failures [<xref ref-type="bibr" rid="pcbi.1005110.ref033">33</xref>]), so the <italic>h</italic><sub><italic>i</italic></sub> typically remain finite for all <italic>i</italic>.</p>
<p>For complex joint distribution, the <italic>h</italic><sub><italic>i</italic></sub>(<italic>z</italic>) can be hard to reason about and/or compute. However, here we argue that it is possible to reason about the scaling of the covariance of the <italic>h</italic><sub><italic>i</italic></sub>(<italic>z</italic>)’s based on the scaling of the covariance of the elementwise entropies <inline-formula id="pcbi.1005110.e201"><alternatives><graphic id="pcbi.1005110.e201g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e201" xlink:type="simple"/><mml:math display="inline" id="M201"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which are much simpler quantities. To see this, note that the <italic>h</italic><sub><italic>i</italic></sub> can be written
<disp-formula id="pcbi.1005110.e202"><alternatives><graphic id="pcbi.1005110.e202g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e202" xlink:type="simple"/><mml:math display="block" id="M202"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(67)</label></disp-formula>
where, as in the main text, the first term is the elementwise entropy,
<disp-formula id="pcbi.1005110.e203"><alternatives><graphic id="pcbi.1005110.e203g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e203" xlink:type="simple"/><mml:math display="block" id="M203"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mfenced> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(68)</label></disp-formula>
and the second term is the mutual information between <italic>x</italic><sub><italic>i</italic></sub> and <italic>x</italic><sub>1</sub> to <italic>x</italic><sub><italic>i</italic>−1</sub>, conditioned on z,
<disp-formula id="pcbi.1005110.e204"><alternatives><graphic id="pcbi.1005110.e204g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e204" xlink:type="simple"/><mml:math display="block" id="M204"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow> <mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>,</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi> <mml:mo>,</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(69)</label></disp-formula>
Combining <xref ref-type="disp-formula" rid="pcbi.1005110.e202">Eq (67)</xref> with <xref ref-type="disp-formula" rid="pcbi.1005110.e194">Eq (64)</xref>, we see that
<disp-formula id="pcbi.1005110.e205"><alternatives><graphic id="pcbi.1005110.e205g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e205" xlink:type="simple"/><mml:math display="block" id="M205"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:mn>2</mml:mn> <mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(70)</label></disp-formula></p>
<p>If the <inline-formula id="pcbi.1005110.e206"><alternatives><graphic id="pcbi.1005110.e206g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e206" xlink:type="simple"/><mml:math display="inline" id="M206"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> covary, then the first term is <inline-formula id="pcbi.1005110.e207"><alternatives><graphic id="pcbi.1005110.e207g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e207" xlink:type="simple"/><mml:math display="inline" id="M207"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. In this situation it would require very precise cancellation for the whole expression to be <inline-formula id="pcbi.1005110.e208"><alternatives><graphic id="pcbi.1005110.e208g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e208" xlink:type="simple"/><mml:math display="inline" id="M208"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Such cancellation could occur if, for instance, <inline-formula id="pcbi.1005110.e209"><alternatives><graphic id="pcbi.1005110.e209g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e209" xlink:type="simple"/><mml:math display="inline" id="M209"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>+</mml:mo><mml:mtext> const</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>. However, unless the constant were zero, so <italic>x</italic><sub><italic>i</italic>−1</sub>…<italic>x</italic><sub>1</sub> determine the value of <italic>x</italic><sub><italic>i</italic></sub> (see <xref ref-type="disp-formula" rid="pcbi.1005110.e204">Eq (69)</xref>), it is unclear how this could occur. Thus, as claimed in the main text, except in cases in which there is highly precise cancellation, if the elementwise entropies <inline-formula id="pcbi.1005110.e210"><alternatives><graphic id="pcbi.1005110.e210g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e210" xlink:type="simple"/><mml:math display="inline" id="M210"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>z</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> covary (with <inline-formula id="pcbi.1005110.e211"><alternatives><graphic id="pcbi.1005110.e211g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e211" xlink:type="simple"/><mml:math display="inline" id="M211"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> covariance), the variance of the total entropy will scale as <italic>n</italic><sup>2</sup>.</p>
</sec>
<sec id="sec021">
<title>High dimensional latent variables</title>
<p>So far we have restricted our analysis to low dimensional latent variables. However, this is not absolutely necessary, and in fact high dimensional latent variable can induce Zipf’s law the same way low dimensional ones can: if different settings of the latent variable result in <inline-formula id="pcbi.1005110.e212"><alternatives><graphic id="pcbi.1005110.e212g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e212" xlink:type="simple"/><mml:math display="inline" id="M212"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> differences in the mean energy, Zipf’s law will emerge. The main difference in the analysis is that we can no longer approximate the mean energy by the entropy, as we did in <xref ref-type="disp-formula" rid="pcbi.1005110.e070">Eq (20)</xref>. However, it is not actually necessary to make this approximation; it is merely convenient, as it allows us to work with the entropy, an intuitive, well-understood quantity. Indeed, if we work directly with the mean energy, <xref ref-type="disp-formula" rid="pcbi.1005110.e065">Eq (18)</xref>, we can see that covariation in the individual energies leads to Zipf’s law—just as the covariation in the individual entropies led to Zipf’s law in the previous section.</p>
<p>To show this explicitly, we break <xref ref-type="disp-formula" rid="pcbi.1005110.e065">Eq (18)</xref> into one term for each element of <bold>x</bold>,
<disp-formula id="pcbi.1005110.e213"><alternatives><graphic id="pcbi.1005110.e213g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e213" xlink:type="simple"/><mml:math display="block" id="M213"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msub><mml:mi>l</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(71)</label></disp-formula>
where
<disp-formula id="pcbi.1005110.e214"><alternatives><graphic id="pcbi.1005110.e214g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e214" xlink:type="simple"/><mml:math display="block" id="M214"><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(72)</label></disp-formula>
Then, writing the variance of the mean energy in terms of the <italic>l</italic><sub><italic>i</italic></sub>, we have
<disp-formula id="pcbi.1005110.e215"><alternatives><graphic id="pcbi.1005110.e215g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e215" xlink:type="simple"/><mml:math display="block" id="M215"><mml:mrow><mml:msub><mml:mtext>Var</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mtext>Cov</mml:mtext> <mml:mi>z</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>l</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>l</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(73)</label></disp-formula>
If the <italic>l</italic><sub><italic>i</italic></sub> have <inline-formula id="pcbi.1005110.e216"><alternatives><graphic id="pcbi.1005110.e216g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e216" xlink:type="simple"/><mml:math display="inline" id="M216"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and positive, covariance, the variance of the energy is <inline-formula id="pcbi.1005110.e217"><alternatives><graphic id="pcbi.1005110.e217g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e217" xlink:type="simple"/><mml:math display="inline" id="M217"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and Zipf’s law emerges. The intuition is that each element of <bold>x</bold> contributes to the energy, −log <italic>P</italic> (<bold>x</bold>). These contributions (or their expected values) change with the latent variable, and if they all change in the same direction, then the overall change in the energy is <inline-formula id="pcbi.1005110.e218"><alternatives><graphic id="pcbi.1005110.e218g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e218" xlink:type="simple"/><mml:math display="inline" id="M218"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, so the variance is <inline-formula id="pcbi.1005110.e219"><alternatives><graphic id="pcbi.1005110.e219g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e219" xlink:type="simple"/><mml:math display="inline" id="M219"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>While the above analysis provides the underlying intuition, in practical situations the <italic>l</italic><sub><italic>i</italic></sub> may be difficult to compute. We therefore provide an alternative approach. For definiteness, we’ll set the dimension of the latent variable to the dimension of the data, <italic>n</italic>; to make this explicit, we’ll replace <italic>z</italic> by <bold>z</bold> (≡ <italic>z</italic><sub>1</sub>, <italic>z</italic><sub>2</sub>, …, <italic>z</italic><sub><italic>n</italic></sub>). In addition, we’ll assume, without loss of generality, that each latent variable—each <italic>z</italic><sub><italic>i</italic></sub>—has an <inline-formula id="pcbi.1005110.e220"><alternatives><graphic id="pcbi.1005110.e220g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e220" xlink:type="simple"/><mml:math display="inline" id="M220"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> range. We’ll also assume that each latent variable has an <inline-formula id="pcbi.1005110.e221"><alternatives><graphic id="pcbi.1005110.e221g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e221" xlink:type="simple"/><mml:math display="inline" id="M221"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> effect on the mean energy; this ensures that the average energy has sensible scaling with <italic>n</italic>.</p>
<p>Because each of the latent variables has a small effect, they need to act together to produce the <inline-formula id="pcbi.1005110.e222"><alternatives><graphic id="pcbi.1005110.e222g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e222" xlink:type="simple"/><mml:math display="inline" id="M222"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> variability in the mean energy that is required for Zipf’s law. Specifically, if any two latent variables, say <italic>z</italic><sub><italic>i</italic></sub> and <italic>z</italic><sub><italic>j</italic></sub>, have the same effect on the average energy (either both increasing it or both decreasing it), they need to be positively correlated; if they have the opposite effect (one increasing it and the other decreasing it), they need to be negatively correlated. When this doesn’t hold—when correlations are essentially arbitrary, or non-existent—variations in <bold>z</bold> have an <inline-formula id="pcbi.1005110.e223"><alternatives><graphic id="pcbi.1005110.e223g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e223" xlink:type="simple"/><mml:math display="inline" id="M223"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msqrt><mml:mi>n</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> effect on the average energy. In this regime, the variance of the average energy is <inline-formula id="pcbi.1005110.e224"><alternatives><graphic id="pcbi.1005110.e224g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e224" xlink:type="simple"/><mml:math display="inline" id="M224"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and Zipf’s law does not emerge. We thus conclude, at least tentatively (and perhaps not surprisingly) that the <italic>z</italic><sub><italic>i</italic></sub> must to be correlated for Zipf’s law to emerge.</p>
<p>To see this more quantitatively, we make a first-order Taylor series expansion of the expected energy,
<disp-formula id="pcbi.1005110.e225"><alternatives><graphic id="pcbi.1005110.e225g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e225" xlink:type="simple"/><mml:math display="block" id="M225"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>≈</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mfenced close="|" open="" separators=""><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>z</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:mi>μ</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(74)</label></disp-formula>
Because each of the <italic>z</italic><sub><italic>i</italic></sub> has an <inline-formula id="pcbi.1005110.e226"><alternatives><graphic id="pcbi.1005110.e226g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e226" xlink:type="simple"/><mml:math display="inline" id="M226"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> range and an <inline-formula id="pcbi.1005110.e227"><alternatives><graphic id="pcbi.1005110.e227g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e227" xlink:type="simple"/><mml:math display="inline" id="M227"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> effect on the mean energy, each term in the sum is <inline-formula id="pcbi.1005110.e228"><alternatives><graphic id="pcbi.1005110.e228g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e228" xlink:type="simple"/><mml:math display="inline" id="M228"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Thus, if the higher order terms in <xref ref-type="disp-formula" rid="pcbi.1005110.e225">Eq (74)</xref> can be neglected, the <italic>z</italic><sub><italic>i</italic></sub> have to be correlated for the variance of the average energy to scale as <inline-formula id="pcbi.1005110.e229"><alternatives><graphic id="pcbi.1005110.e229g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e229" xlink:type="simple"/><mml:math display="inline" id="M229"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>; if they are not correlated, the variance is <inline-formula id="pcbi.1005110.e230"><alternatives><graphic id="pcbi.1005110.e230g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e230" xlink:type="simple"/><mml:math display="inline" id="M230"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Of course, ignoring higher order terms in high dimensions is dangerous, as the number of terms grows rapidly with <italic>n</italic> (the number of <inline-formula id="pcbi.1005110.e231"><alternatives><graphic id="pcbi.1005110.e231g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e231" xlink:type="simple"/><mml:math display="inline" id="M231"><mml:msup><mml:mi>k</mml:mi> <mml:mtext>th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula> order terms is proportional to <italic>n</italic><sup><italic>k</italic></sup>). However, it turns out to give the right intuition: the Efron-Stein inequality [<xref ref-type="bibr" rid="pcbi.1005110.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1005110.ref036">36</xref>], along with the assumption that each latent variable has an <inline-formula id="pcbi.1005110.e232"><alternatives><graphic id="pcbi.1005110.e232g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e232" xlink:type="simple"/><mml:math display="inline" id="M232"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> effect on the energy, ensures that if the <italic>z</italic><sub><italic>i</italic></sub> are independent, the variance of the energy is indeed <inline-formula id="pcbi.1005110.e233"><alternatives><graphic id="pcbi.1005110.e233g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e233" xlink:type="simple"/><mml:math display="inline" id="M233"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Thus, a necessary condition for Zipf’s law to emerge is that the <italic>z</italic><sub><italic>i</italic></sub> are correlated, as has been pointed out previously [<xref ref-type="bibr" rid="pcbi.1005110.ref018">18</xref>] (in Supporting Information).</p>
<p>The fact that correlations are necessary to produce Zipf’s law provides a natural approach to understanding models with high dimensional latent variables. The approach relies on the observation that sufficiently correlated variables have a “long” direction—a direction along which the typical size of |<bold>z</bold>| is <inline-formula id="pcbi.1005110.e234"><alternatives><graphic id="pcbi.1005110.e234g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e234" xlink:type="simple"/><mml:math display="inline" id="M234"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (rather than <inline-formula id="pcbi.1005110.e235"><alternatives><graphic id="pcbi.1005110.e235g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e235" xlink:type="simple"/><mml:math display="inline" id="M235"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:msqrt><mml:mi>n</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, as it is for uncorrelated latent variables). We can, therefore, construct a low dimensional latent variable that measures distance along that direction, and then use the analysis developed above for low dimensional latent variables.</p>
<p>Here we illustrate this idea for binary variables, <italic>x</italic><sub><italic>i</italic></sub> = 0 or 1. For definiteness, and because it makes the ideas more intuitively accessible, we consider a concrete setting: neural data, with as many latent variables as neurons. As in the main text, <italic>x</italic><sub><italic>i</italic></sub> = 1 corresponds to one or more spikes in a small time bin and <italic>x</italic><sub><italic>i</italic></sub> = 0 corresponds to no spikes. Because the long direction in latent variable space depends on the distribution <italic>P</italic> (<bold>z</bold>), it would seem difficult to make general statements. However, in this example the data comes from neural spike trains, and so we can make use of the fact that firing rates of neurons often covary. Thus, a very natural low dimensional latent variable, which we denote <italic>ν</italic>, is the population averaged firing rate,
<disp-formula id="pcbi.1005110.e236"><alternatives><graphic id="pcbi.1005110.e236g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e236" xlink:type="simple"/><mml:math display="block" id="M236"><mml:mrow><mml:mi>ν</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(75)</label></disp-formula>
where <italic>p</italic><sub><italic>i</italic></sub>(<bold>z</bold>) is the probability that <italic>x</italic><sub><italic>i</italic></sub> = 1 given <bold>z</bold>,
<disp-formula id="pcbi.1005110.e237"><alternatives><graphic id="pcbi.1005110.e237g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e237" xlink:type="simple"/><mml:math display="block" id="M237"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mtext>E</mml:mtext> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">z</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(76)</label></disp-formula>
For this model the element-wise entropies have a very simple form,
<disp-formula id="pcbi.1005110.e238"><alternatives><graphic id="pcbi.1005110.e238g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e238" xlink:type="simple"/><mml:math display="block" id="M238"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mtext>log </mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(77)</label></disp-formula>
We’ll assume that all the <italic>p</italic><sub><italic>i</italic></sub>(<bold>z</bold>) are less than 1/2, something that is satisfied for realistic spike trains if the time bins aren’t too large. Consequently, increasing <italic>p</italic><sub><italic>i</italic></sub>(<bold>z</bold>) increases the element-wise entropy of neuron <italic>i</italic>.</p>
<p>We need two conditions for Zipf’s law to emerge: the variance of <italic>ν</italic> must be <inline-formula id="pcbi.1005110.e239"><alternatives><graphic id="pcbi.1005110.e239g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e239" xlink:type="simple"/><mml:math display="inline" id="M239"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1005110.e240"><alternatives><graphic id="pcbi.1005110.e240g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e240" xlink:type="simple"/><mml:math display="inline" id="M240"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> changes in <italic>ν</italic> must lead to <inline-formula id="pcbi.1005110.e241"><alternatives><graphic id="pcbi.1005110.e241g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e241" xlink:type="simple"/><mml:math display="inline" id="M241"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and positively correlated, changes in the element-wise entropies (assuming, as discussed in the previous section, there isn’t very precise cancellation). So long as the firing rates go up and down together, both conditions are satisfied, and Zipf’s law emerges. If, on the other hand, the firing rates are not positively correlated on average, the variance of <italic>ν</italic> is <inline-formula id="pcbi.1005110.e242"><alternatives><graphic id="pcbi.1005110.e242g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e242" xlink:type="simple"/><mml:math display="inline" id="M242"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msqrt><mml:mi>n</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and the population averaged firing rate provides no information about Zipf’s law. This is an important example, as the population averaged firing rate is easy to estimate from data.</p>
<p>In summary, high dimensional latent variables are, from a conceptual point of view, no different than low dimensional ones: both lead to Zipf’s law if different settings of the latent variables lead to average energies that differ by <inline-formula id="pcbi.1005110.e243"><alternatives><graphic id="pcbi.1005110.e243g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e243" xlink:type="simple"/><mml:math display="inline" id="M243"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. However, in the high dimensional case, each latent variable has a small effect on the energy, so a necessary condition for Zipf’s law to emerge is that the latent variables are correlated. This turns out to be helpful: the correlations can lead naturally to a low dimensional latent variable, for which our analysis of low dimensional latent variables applies.</p>
</sec>
<sec id="sec022">
<title>Peaks in <inline-formula id="pcbi.1005110.e244"><alternatives><graphic id="pcbi.1005110.e244g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e244" xlink:type="simple"/><mml:math display="inline" id="M244"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> do not disrupt Zipf’s law</title>
<p>In the main text, we noted that while holes in the distribution over energy, <inline-formula id="pcbi.1005110.e245"><alternatives><graphic id="pcbi.1005110.e245g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e245" xlink:type="simple"/><mml:math display="inline" id="M245"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, disrupt Zipf’s law, peaks in this distribution do not. To see this explicitly, take an extreme case: <inline-formula id="pcbi.1005110.e246"><alternatives><graphic id="pcbi.1005110.e246g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e246" xlink:type="simple"/><mml:math display="inline" id="M246"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is composed of a delta function at <inline-formula id="pcbi.1005110.e247"><alternatives><graphic id="pcbi.1005110.e247g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e247" xlink:type="simple"/><mml:math display="inline" id="M247"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, weighted by <italic>α</italic>, combined with a smooth component, <inline-formula id="pcbi.1005110.e248"><alternatives><graphic id="pcbi.1005110.e248g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e248" xlink:type="simple"/><mml:math display="inline" id="M248"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, that integrates to 1 − <italic>α</italic>. Here <italic>α</italic> may be any number between 0 and 1, and in particular it need not be exponentially small in the energy, as it is in <xref ref-type="disp-formula" rid="pcbi.1005110.e009">Eq (6)</xref>. For this case, we can compute <inline-formula id="pcbi.1005110.e249"><alternatives><graphic id="pcbi.1005110.e249g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e249" xlink:type="simple"/><mml:math display="inline" id="M249"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> explicitly using <xref ref-type="disp-formula" rid="pcbi.1005110.e016">Eq (9)</xref>,
<disp-formula id="pcbi.1005110.e250"><alternatives><graphic id="pcbi.1005110.e250g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e250" xlink:type="simple"/><mml:math display="block" id="M250"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:mfenced close="]" open="[" separators=""><mml:mi>α</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:msup> <mml:mo>Θ</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(78)</label></disp-formula>
where <italic>f</italic><sub><italic>S</italic></sub> is <italic>f</italic> smoothed by an exponential kernel, Θ is the Heaviside step function, and we have normalized by <italic>n</italic> to give us the quantity relevant for determining the size of departures from Zipf’s law (see <xref ref-type="disp-formula" rid="pcbi.1005110.e084">Eq (22)</xref>). The term <inline-formula id="pcbi.1005110.e251"><alternatives><graphic id="pcbi.1005110.e251g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e251" xlink:type="simple"/><mml:math display="inline" id="M251"><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>Θ</mml:mo> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">E</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> ranges from 0 to 1, so <inline-formula id="pcbi.1005110.e252"><alternatives><graphic id="pcbi.1005110.e252g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e252" xlink:type="simple"/><mml:math display="inline" id="M252"><mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be bounded above and below,
<disp-formula id="pcbi.1005110.e253"><alternatives><graphic id="pcbi.1005110.e253g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e253" xlink:type="simple"/><mml:math display="block" id="M253"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>f</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>≤</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="script">E</mml:mi></mml:mfenced> <mml:mo>≤</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:mtext>log </mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(79)</label></disp-formula>
Assuming the distribution <inline-formula id="pcbi.1005110.e254"><alternatives><graphic id="pcbi.1005110.e254g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e254" xlink:type="simple"/><mml:math display="inline" id="M254"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is such that the first term vanishes in the large <italic>n</italic> limit (so that without the delta function Zipf’s law would hold), then the last term must also vanish in the large <italic>n</italic> limit. Thus, even delta-function singularities do not prevent convergence to Zipf’s law, so long as they occur on top of a finite baseline.</p>
</sec>
<sec id="sec023">
<title>Exponential family latent variable models: technical details</title>
<p>Schwab <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>] showed that Zipf’s law emerges for a model in which the distribution over <bold>x</bold> given the latent variable is in the exponential family. By itself, the fact that the distribution is in the exponential family places no restrictions on the class of models. However, their derivation required other conditions to be satisfied, and those conditions do induce restrictions. In particular, their analysis does not apply to models with a large number of natural parameters (it thus does not apply when the latent variable is high dimensional), models in which the latent variable is discrete, and models in which the latent variable is the dimension of the data. Here we show this explicitly.</p>
<sec id="sec024">
<title>The relationship between Schwab <italic>et al.</italic>’s model and our model</title>
<p>Schwab <italic>et al.</italic> formulated their model as a latent variable model conditioned on natural parameters, as written in the main text, <xref ref-type="disp-formula" rid="pcbi.1005110.e120">Eq (25)</xref>. Hidden in <xref ref-type="disp-formula" rid="pcbi.1005110.e120">Eq (25)</xref> is the fact that the <italic>g</italic><sub><italic>μ</italic></sub> can be “tied”: the parameters <italic>g</italic><sub><italic>μ</italic></sub> are drawn from a distribution that allows delta-functions, such as <italic>δ</italic>(<italic>g</italic><sub>1</sub> − <italic>f</italic>(<italic>g</italic><sub>2</sub>)) for some function <italic>f</italic>, or even <inline-formula id="pcbi.1005110.e255"><alternatives><graphic id="pcbi.1005110.e255g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e255" xlink:type="simple"/><mml:math display="inline" id="M255"><mml:mrow><mml:mi>δ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>g</mml:mi> <mml:mn>3</mml:mn> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. To make this explicit, and to also make contact with our model, we rewrote <xref ref-type="disp-formula" rid="pcbi.1005110.e120">Eq (25)</xref> as a latent variable model conditioned on <italic>z</italic> (<xref ref-type="disp-formula" rid="pcbi.1005110.e121">Eq (26)</xref>), where <italic>z</italic> is a <italic>k</italic>-dimensional latent variable. Under this model it is easy to tie variables; for instance, letting <italic>g</italic><sub>1</sub> = <italic>z</italic> and <italic>g</italic><sub>2</sub> = <italic>f</italic>(<italic>z</italic>) (with <italic>z</italic> one-dimensional) enforces the constraint <italic>δ</italic>(<italic>g</italic><sub>1</sub> − <italic>f</italic>(<italic>g</italic><sub>2</sub>)).</p>
</sec>
<sec id="sec025">
<title>Number of latent variables</title>
<p>Here we show that the number of natural parameters (<italic>m</italic> in Eqs <xref ref-type="disp-formula" rid="pcbi.1005110.e120">(25)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005110.e121">(26)</xref>) must be small compared to the dimension of the data, <italic>n</italic>. We start by sketching Schwab <italic>et al.</italic>’s [<xref ref-type="bibr" rid="pcbi.1005110.ref019">19</xref>] derivation, including many steps that were left to the reader in their paper. Their starting point is the expression for the energy of an observation,
<disp-formula id="pcbi.1005110.e256"><alternatives><graphic id="pcbi.1005110.e256g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e256" xlink:type="simple"/><mml:math display="block" id="M256"><mml:mrow><mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi>z</mml:mi> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(80)</label></disp-formula>
We have written the right hand side using the form given in <xref ref-type="disp-formula" rid="pcbi.1005110.e121">Eq (26)</xref>, except that we explicitly include the partition function (<xref ref-type="disp-formula" rid="pcbi.1005110.e258">Eq (82)</xref> below), and we use dot products instead of sums. This integral is evaluated using the saddle-point method,
<disp-formula id="pcbi.1005110.e257"><alternatives><graphic id="pcbi.1005110.e257g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e257" xlink:type="simple"/><mml:math display="block" id="M257"><mml:mrow><mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>≈</mml:mo> <mml:mi>n</mml:mi> <mml:mi mathvariant="bold">g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(81)</label></disp-formula>
where <italic>z</italic>* maximizes the integrand. For the saddle point method to work—that is, for the above approximation to hold—the number of latent variables, dim(<italic>z</italic>), must be subextensive in <italic>n</italic> (i.e., dim(<italic>z</italic>)/<italic>n</italic> → 0 as <italic>n</italic> goes to infinity; see [<xref ref-type="bibr" rid="pcbi.1005110.ref037">37</xref>] for details).</p>
<p>The condition dim(<italic>z</italic>) ≪ <italic>n</italic> does not place any restrictions on the number of natural parameters (the dimension of <bold>g</bold>). But the next step in their derivation, computing the partition function (which is necessary for finding the energy of an observation), does. The log of the partition function is given by the usual expression,
<disp-formula id="pcbi.1005110.e258"><alternatives><graphic id="pcbi.1005110.e258g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e258" xlink:type="simple"/><mml:math display="block" id="M258"><mml:mrow><mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>log </mml:mtext><mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(82)</label></disp-formula>
In the large <italic>n</italic> limit, the sum can be approximated as an integral over <bold>O</bold>,
<disp-formula id="pcbi.1005110.e259"><alternatives><graphic id="pcbi.1005110.e259g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e259" xlink:type="simple"/><mml:math display="block" id="M259"><mml:mrow><mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>log </mml:mtext><mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">O</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(83)</label></disp-formula>
where <italic>S</italic>(<bold>O</bold>) is the entropy at fixed <bold>O</bold>,
<disp-formula id="pcbi.1005110.e260"><alternatives><graphic id="pcbi.1005110.e260g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e260" xlink:type="simple"/><mml:math display="block" id="M260"><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>S</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>x</mml:mi></mml:munder> <mml:mi>δ</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">O</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(84)</label></disp-formula>
Note that <bold>O</bold> is in fact a discrete variable. However, <italic>e</italic><sup><italic>S</italic>(<bold>O</bold>)</sup> becomes progressively denser as <italic>n</italic> increases, and as <italic>n</italic> → ∞, it becomes continuous. As with <xref ref-type="disp-formula" rid="pcbi.1005110.e256">Eq (80)</xref>, the integral can be computed using the saddle point method, yielding
<disp-formula id="pcbi.1005110.e261"><alternatives><graphic id="pcbi.1005110.e261g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e261" xlink:type="simple"/><mml:math display="block" id="M261"><mml:mrow><mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mi mathvariant="bold">g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold">O</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">O</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(85)</label></disp-formula>
For this approximation to be valid, the dimension of <bold>O</bold>, and hence the dimension of <bold>g</bold> (which is <italic>m</italic>), must be subextensive in <italic>n</italic>. Thus, Schwab <italic>et al.</italic>’s method applies to model in which <italic>m</italic> ≪ <italic>n</italic> (more technically, <italic>m</italic>/<italic>n</italic> → 0 as <italic>n</italic> → ∞). This restricts it to a relatively small number of natural parameters.</p>
<p>In sum, because Schwab <italic>et al.</italic>’s method involves an <italic>m</italic>-dimensional saddle-point integral over <bold>O</bold>, it requires the dimensionality of <bold>O</bold> (and hence <bold>g</bold>) to be small (i.e. <italic>m</italic>/<italic>n</italic> → 0 as <italic>n</italic> → ∞; again, see [<xref ref-type="bibr" rid="pcbi.1005110.ref037">37</xref>] for details). There are additional steps in their derivation. However, they are not trivial, and they do not lead to additional constraints on their model, so we do not consider them further.</p>
<p>Although high dimensional natural parameters are ruled out by Schwab <italic>et al.</italic>’s method, there are many interesting cases (e.g., models of neural data), in which the elements of <bold>g</bold> covary. In those cases, one might think that it would be possible to reduce a high-dimensional latent variable to a low-dimensional one, as we did in previously. While such a reduction is always possible, doing so typically takes the model out of Schwab <italic>et al.</italic>’s class. To see this in a simple setting, we reduce a model with one low-dimensional natural parameter, <italic>g</italic>, and one high-dimensional natural parameter, <bold>g</bold>, to a model with just the low-dimensional natural parameter. (Here <italic>g</italic> might represent the overall firing rate, and the other natural parameters, <bold>g</bold>, might represent fluctuations around that rate). The model is written
<disp-formula id="pcbi.1005110.e262"><alternatives><graphic id="pcbi.1005110.e262g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e262" xlink:type="simple"/><mml:math display="block" id="M262"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">g</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>g</mml:mi> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(86)</label></disp-formula>
where <italic>Z</italic>(<italic>g</italic>,<bold>g</bold>) is the partition function,
<disp-formula id="pcbi.1005110.e263"><alternatives><graphic id="pcbi.1005110.e263g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e263" xlink:type="simple"/><mml:math display="block" id="M263"><mml:mrow><mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi mathvariant="bold">x</mml:mi></mml:munder> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>g</mml:mi> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(87)</label></disp-formula>
Marginalizing over <bold>g</bold>, we have
<disp-formula id="pcbi.1005110.e264"><alternatives><graphic id="pcbi.1005110.e264g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e264" xlink:type="simple"/><mml:math display="block" id="M264"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>g</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:mi mathvariant="bold">g</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>g</mml:mi> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mtext>log </mml:mtext><mml:mi>Z</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">g</mml:mi> <mml:mo>|</mml:mo> <mml:mi>g</mml:mi></mml:mfenced> <mml:mo>≡</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>g</mml:mi> <mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>ψ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(88)</label></disp-formula></p>
<p>The function <italic>ψ</italic>(<italic>g</italic>,<bold>O</bold>(<bold>x</bold>)) typically has an extremely complicated dependence on <italic>g</italic> and <bold>x</bold>. In fact, for all but the simplest model it is not even possible to calculate it analytically, as the partition function cannot be calculated analytically. Thus, <italic>P</italic> (<bold>x</bold>|<italic>g</italic>) can’t be written in the exponential family with a single natural parameter. It can, of course, be written in the exponential family with an exponential number of natural parameters,
<disp-formula id="pcbi.1005110.e265"><alternatives><graphic id="pcbi.1005110.e265g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e265" xlink:type="simple"/><mml:math display="block" id="M265"><mml:mrow><mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:munder> <mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">O</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(89)</label></disp-formula>
where <italic>δ</italic>(<bold>x</bold> − <bold>x</bold>′) is the Kronecker delta, but this clearly takes it out of Schwab <italic>et al.</italic>’s model class. This is closely related to the fact that exponential family distributions are not closed under marginalisation [<xref ref-type="bibr" rid="pcbi.1005110.ref038">38</xref>].</p>
</sec>
<sec id="sec026">
<title>Latent variable is the sequence length</title>
<p>To show that a model with sequence length as the latent variable is outside of Schwab’s class, we begin by writing the distribution in exponential family form. The simplest way to do that is to write
<disp-formula id="pcbi.1005110.e266"><alternatives><graphic id="pcbi.1005110.e266g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e266" xlink:type="simple"/><mml:math display="block" id="M266"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">lim</mml:mo> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:munder> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mtext>dim</mml:mtext> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(90)</label></disp-formula>
where <italic>δ</italic><sub><italic>ij</italic></sub> is the Kronecker delta (<italic>δ</italic><sub><italic>ij</italic></sub> = 1 if <italic>i</italic> = <italic>j</italic> and 0 otherwise) and, as above, dim(⋅) denotes dimension (in this case the number of elements in <bold>x</bold>). This distribution allows only values of <bold>x</bold> which have the correct length: if dim(<bold>x</bold>) = <italic>z</italic>, the second term in the exponent is zero, giving <italic>P</italic> (<bold>x</bold>|<italic>z</italic>) = <italic>P</italic> (<bold>x</bold>); in contrast, if dim(<bold>x</bold>) ≠ <italic>z</italic>, the second term in the exponent is −<italic>L</italic>, giving a large negative contribution to the energy, and sending <italic>P</italic> (<bold>x</bold>|<italic>z</italic> ≠ dim(<bold>x</bold>)) → 0.</p>
<p>This distribution is not in the exponential family form, because the term, <italic>δ</italic><sub>dim(<bold>x</bold>), <italic>z</italic></sub> is not written as the product of a natural parameter (in this case a function of <italic>z</italic>), and a sufficient statistic (in this case a function of <bold>x</bold>). It is not possible to write it as a single product, but it can be written as the sum of multiple products,
<disp-formula id="pcbi.1005110.e267"><alternatives><graphic id="pcbi.1005110.e267g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e267" xlink:type="simple"/><mml:math display="block" id="M267"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mtext>dim</mml:mtext> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>dim</mml:mtext> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(91)</label></disp-formula>
This is now in the required form, because each term in the sum is the product of a natural parameter (<italic>δ</italic><sub><italic>z</italic>,<italic>i</italic></sub>, which is function of <italic>z</italic>), and a sufficient statistic, (<italic>δ</italic><sub><italic>i</italic>,dim(<bold>x</bold>)</sub>, which is a function of <bold>x</bold>). Inserting this into <xref ref-type="disp-formula" rid="pcbi.1005110.e266">Eq (90)</xref> gives
<disp-formula id="pcbi.1005110.e268"><alternatives><graphic id="pcbi.1005110.e268g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e268" xlink:type="simple"/><mml:math display="block" id="M268"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix" movablelimits="true">lim</mml:mo> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>→</mml:mo> <mml:mi>∞</mml:mi></mml:mrow></mml:munder> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mtext>log </mml:mtext><mml:msub><mml:mi>P</mml:mi> <mml:mrow/></mml:msub> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>L</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mtext>dim</mml:mtext> <mml:mfenced close=")" open="("><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(92)</label></disp-formula>
This is in the exponential family. However, there are <inline-formula id="pcbi.1005110.e269"><alternatives><graphic id="pcbi.1005110.e269g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e269" xlink:type="simple"/><mml:math display="inline" id="M269"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> terms in the sum, where <italic>n</italic> is the mean sequence length, so it is not in Schwab <italic>et al.</italic>’s model class.</p>
</sec>
<sec id="sec027">
<title>Entropy of a place field model</title>
<p>Here we compute the entropy, at fixed <italic>z</italic>, of the place field model in <xref ref-type="disp-formula" rid="pcbi.1005110.e127">Eq (29)</xref>, and show that it depends very weakly on <italic>z</italic>. Because the distribution over <bold>x</bold> is conditionally independent given <italic>z</italic>, the entropy has a simple form,
<disp-formula id="pcbi.1005110.e270"><alternatives><graphic id="pcbi.1005110.e270g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e270" xlink:type="simple"/><mml:math display="block" id="M270"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>H</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(93)</label></disp-formula>
where <italic>p</italic>(<italic>z</italic> − <italic>θ</italic><sub><italic>i</italic></sub>) is the probability that <italic>x</italic><sub><italic>i</italic></sub> = 1 given <italic>z</italic>,
<disp-formula id="pcbi.1005110.e271"><alternatives><graphic id="pcbi.1005110.e271g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e271" xlink:type="simple"/><mml:math display="block" id="M271"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>h</mml:mi> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi> <mml:mo form="prefix">cos</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:msup> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>h</mml:mi> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi> <mml:mo form="prefix">cos</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(94)</label></disp-formula>
and <italic>H</italic><sub><italic>B</italic></sub>(<italic>p</italic>) is the entropy (in nats) of a Bernoulli random variable,
<disp-formula id="pcbi.1005110.e272"><alternatives><graphic id="pcbi.1005110.e272g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e272" xlink:type="simple"/><mml:math display="block" id="M272"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mtext> log </mml:mtext><mml:mi>p</mml:mi> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>log </mml:mtext><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(95)</label></disp-formula></p>
<p>To understand how this scales with <italic>z</italic>, we make the change of variables
<disp-formula id="pcbi.1005110.e273"><alternatives><graphic id="pcbi.1005110.e273g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e273" xlink:type="simple"/><mml:math display="block" id="M273"><mml:mrow><mml:mi>z</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:mi>z</mml:mi></mml:mrow></mml:math></alternatives> <label>(96)</label></disp-formula>
where <italic>θ</italic><sub><italic>j</italic></sub> is chosen to minimize |<italic>δz</italic>|. The mean value theorem tells us that for any smooth function <italic>f</italic>(<italic>z</italic>),
<disp-formula id="pcbi.1005110.e274"><alternatives><graphic id="pcbi.1005110.e274g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e274" xlink:type="simple"/><mml:math display="block" id="M274"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:mi>z</mml:mi> <mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(97)</label></disp-formula>
where prime denotes derivative and <italic>z</italic>* is between <italic>z</italic> and <italic>z</italic> + <italic>δz</italic>. Consequently, for some <italic>z</italic>* close to <italic>θ</italic><sub><italic>j</italic></sub>,
<disp-formula id="pcbi.1005110.e275"><alternatives><graphic id="pcbi.1005110.e275g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e275" xlink:type="simple"/><mml:math display="block" id="M275"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>|</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>H</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi> <mml:mi>z</mml:mi> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:msub><mml:mi>H</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mfenced close=")" open="(" separators=""><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>z</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msup><mml:mi>z</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(98)</label></disp-formula>
Because the <italic>θ</italic><sub><italic>i</italic></sub> are evenly spaced, the first term is independent of <italic>z</italic>. Except at <italic>p</italic> = 0 or 1 (which are not allowed if <italic>h</italic> and <italic>A</italic> are finite), the sum over <italic>i</italic> of the second term is <inline-formula id="pcbi.1005110.e276"><alternatives><graphic id="pcbi.1005110.e276g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e276" xlink:type="simple"/><mml:math display="inline" id="M276"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The spacing between adjacent <italic>θ</italic><sub><italic>i</italic></sub> is 2<italic>π</italic>/<italic>n</italic>, so <inline-formula id="pcbi.1005110.e277"><alternatives><graphic id="pcbi.1005110.e277g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e277" xlink:type="simple"/><mml:math display="inline" id="M277"><mml:mrow><mml:mo>|</mml:mo> <mml:mi>δ</mml:mi> <mml:mi>z</mml:mi> <mml:mo>|</mml:mo> <mml:mo>≤</mml:mo> <mml:mi>π</mml:mi> <mml:mo>/</mml:mo> <mml:mi>n</mml:mi> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Consequently, the second term in <xref ref-type="disp-formula" rid="pcbi.1005110.e275">Eq (98)</xref> scales as <inline-formula id="pcbi.1005110.e278"><alternatives><graphic id="pcbi.1005110.e278g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e278" xlink:type="simple"/><mml:math display="inline" id="M278"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo> <mml:mo>×</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and so <inline-formula id="pcbi.1005110.e279"><alternatives><graphic id="pcbi.1005110.e279g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e279" xlink:type="simple"/><mml:math display="inline" id="M279"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> changes in <italic>z</italic> produce <inline-formula id="pcbi.1005110.e280"><alternatives><graphic id="pcbi.1005110.e280g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005110.e280" xlink:type="simple"/><mml:math display="inline" id="M280"><mml:mrow><mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> changes in the entropy.</p>
</sec>
</sec>
</sec>
<sec id="sec028">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005110.s001" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005110.s001" xlink:type="simple">
<label>S1 Code/Data</label>
<caption>
<title>We include a supporting information file containing the code and data used to obtain our results.</title>
<p>Please see the “README” file for details.</p>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank Yasser Roudi, John Hertz, David Schwab.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005110.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Zipf</surname> <given-names>GK</given-names></name>. <source>Selected studies of the principle of relative frequency in language</source>. <publisher-name>Harvard Univ. Press</publisher-name>; <year>1932</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gabaix</surname> <given-names>X</given-names></name>. <article-title>Zipf’s law for cities: an explanation</article-title>. <source>The Quarterly Journal of Economics</source>. <year>1999</year>;<volume>114</volume>:<fpage>739</fpage>–<lpage>767</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/003355399556133" xlink:type="simple">10.1162/003355399556133</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Axtell</surname> <given-names>RL</given-names></name>. <article-title>Zipf distribution of US firm sizes</article-title>. <source>Science</source>. <year>2001</year>;<volume>293</volume>:<fpage>1818</fpage>–<lpage>1820</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1062081" xlink:type="simple">10.1126/science.1062081</ext-link></comment> <object-id pub-id-type="pmid">11546870</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gabaix</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Gopikrishnan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Plerou</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Stanley</surname> <given-names>HE</given-names></name>. <article-title>A theory of power-law distributions in financial market fluctuations</article-title>. <source>Nature</source>. <year>2003</year>;<volume>423</volume>:<fpage>267</fpage>–<lpage>270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature01624" xlink:type="simple">10.1038/nature01624</ext-link></comment> <object-id pub-id-type="pmid">12748636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mora</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Walczak</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Callan</surname> <given-names>CG</given-names></name>. <article-title>Maximum entropy models for antibody diversity</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year>;<volume>107</volume>:<fpage>5405</fpage>–<lpage>5410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1001705107" xlink:type="simple">10.1073/pnas.1001705107</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mora</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Are biological systems poised at criticality?</article-title> <source>Journal of Statistical Physics</source>. <year>2011</year>;<volume>144</volume>:<fpage>268</fpage>–<lpage>302</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10955-011-0229-4" xlink:type="simple">10.1007/s10955-011-0229-4</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tyrcha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Roudi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Marsili</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>. <article-title>The effect of nonstationarity on models inferred from neural data</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>. <year>2013</year>; p. <fpage>03005</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1742-5468/2013/03/P03005" xlink:type="simple">10.1088/1742-5468/2013/03/P03005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref008">
<label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Zipf</surname> <given-names>GK</given-names></name>. <source>Human behavior and the principle of least effort</source>. <publisher-name>Addison-Wesley</publisher-name>; <year>1949</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cancho i</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Solé</surname> <given-names>RV</given-names></name>. <article-title>Least effort and the origins of scaling in human language</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2003</year>;<volume>100</volume>:<fpage>788</fpage>–<lpage>791</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0335980100" xlink:type="simple">10.1073/pnas.0335980100</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Corominas-Murtra</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fortuny</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Solé</surname> <given-names>RV</given-names></name>. <article-title>Emergence of Zipf’s law in the evolution of communication</article-title>. <source>Physical Review E</source>. <year>2011</year>;<volume>83</volume>:<fpage>036115</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.83.036115" xlink:type="simple">10.1103/PhysRevE.83.036115</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref011">
<label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Mandelbrot B. An informational theory of the statistical structure of languages. In: Jackson BW, editor. Communication Theory; 1953. p. 486–502.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>. <article-title>Random texts exhibit Zipf’s-law-like word frequency distribution</article-title>. <source>IEEE Transactions on Information Theory</source>. <year>1992</year>;<volume>38</volume>:<fpage>1842</fpage>–<lpage>1845</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/18.165464" xlink:type="simple">10.1109/18.165464</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ioannides</surname> <given-names>YM</given-names></name>, <name name-style="western"><surname>Overman</surname> <given-names>HG</given-names></name>. <article-title>Zipf’s law for cities: an empirical examination</article-title>. <source>Regional Science and Urban Economics</source>. <year>2003</year>;<volume>33</volume>:<fpage>127</fpage>–<lpage>137</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0166-0462(02)00006-6" xlink:type="simple">10.1016/S0166-0462(02)00006-6</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Newman</surname> <given-names>ME</given-names></name>. <article-title>Power laws, Pareto distributions and Zipf’s law</article-title>. <source>Contemporary Physics</source>. <year>2005</year>;<volume>46</volume>:<fpage>323</fpage>–<lpage>351</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/00107510500052444" xlink:type="simple">10.1080/00107510500052444</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Saremi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Hierarchical model of natural images and the origin of scale invariance</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2013</year>;<volume>110</volume>:<fpage>3071</fpage>–<lpage>3076</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1222618110" xlink:type="simple">10.1073/pnas.1222618110</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Saremi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>On criticality in high-dimensional data</article-title>. <source>Neural Computation</source>. <year>2014</year>;<volume>26</volume>:<fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00607" xlink:type="simple">10.1162/NECO_a_00607</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref017">
<label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Tkačik G, Mora T, Marre O, Amodei D, Berry II MJ, Bialek W. Thermodynamics for a network of neurons: Signatures of criticality. arXiv. 2014;1407.5946.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tkačik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mora</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Marre</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Amodei</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <etal>et al</etal>. <article-title>Thermodynamics and signatures of criticality in a network of neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>:<fpage>11508</fpage>–<lpage>11513</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1514188112" xlink:type="simple">10.1073/pnas.1514188112</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwab</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Nemenman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Mehta</surname> <given-names>P</given-names></name>. <article-title>Zipf’s law and criticality in multivariate data without fine-tuning</article-title>. <source>Physical Review Letters</source>. <year>2014</year>;<volume>113</volume>:<fpage>068102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.113.068102" xlink:type="simple">10.1103/PhysRevLett.113.068102</ext-link></comment> <object-id pub-id-type="pmid">25148352</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref020">
<label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Pathria</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Beale</surname> <given-names>PD</given-names></name>. <source>Statistical Mechanics</source>. <edition>3rd ed</edition>. <publisher-name>Elsivier</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref021">
<label>21</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Leech</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rayson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>A</given-names></name>. <source>Word frequencies in written and spoken English: based on the British National Corpus</source>. <publisher-loc>Harlow</publisher-loc>: <publisher-name>Longman</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref022">
<label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Levin</surname> <given-names>B</given-names></name>. <source>English verb classes and alternations: A preliminary investigation</source>. <publisher-name>University of Chicago press</publisher-name>; <year>1993</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Desponds</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mora</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Walczak</surname> <given-names>AM</given-names></name>. <article-title>Fluctuating fitness shapes the clone-size distribution of immune repertoires</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;<volume>113</volume>:<fpage>274</fpage>–<lpage>279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1512977112" xlink:type="simple">10.1073/pnas.1512977112</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref024">
<label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Weiss</surname> <given-names>NA</given-names></name>. <source>A Course in Probability</source>. <publisher-name>Addison-Wesley</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Nemenman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Tishby</surname> <given-names>N</given-names></name>. <article-title>Predictability, complexity, and learning</article-title>. <source>Neural Computation</source>. <year>2001</year>;<volume>13</volume>:<fpage>2409</fpage>–<lpage>2463</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976601753195969" xlink:type="simple">10.1162/089976601753195969</ext-link></comment> <object-id pub-id-type="pmid">11674845</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beck</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>EGD</given-names></name>. <article-title>Superstatistics</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2003</year>;<volume>322</volume>:<fpage>267</fpage>–<lpage>275</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0378-4371(03)00019-0" xlink:type="simple">10.1016/S0378-4371(03)00019-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Macke</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Opper</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>Common input explains higher-order correlations and entropy in a simple model of neural population activity</article-title>. <source>Physical Review Letters</source>. <year>2011</year>;<volume>106</volume>(<issue>20</issue>):<fpage>208102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.106.208102" xlink:type="simple">10.1103/PhysRevLett.106.208102</ext-link></comment> <object-id pub-id-type="pmid">21668265</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Nonnenmacher M, Behrens C, Berens P, Bethge M, Macke JH. Signatures of criticality arise in simple neural population models with correlations. arXiv;1603.00097.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Price</surname> <given-names>D</given-names></name>. <article-title>A general theory of bibliometric and other cumulative advantage processes</article-title>. <source>Journal of the American Society for Information Science</source>. <year>1976</year>;<volume>27</volume>:<fpage>292</fpage>–<lpage>306</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/asi.4630270505" xlink:type="simple">10.1002/asi.4630270505</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bomash</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Roudi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nirenberg</surname> <given-names>S</given-names></name>. <article-title>A virtual retina for studying population coding</article-title>. <source>PloS One</source>. <year>2013</year>;<volume>8</volume>:<fpage>e53363</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0053363" xlink:type="simple">10.1371/journal.pone.0053363</ext-link></comment> <object-id pub-id-type="pmid">23341940</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nirenberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pandarinath</surname> <given-names>C</given-names></name>. <article-title>Retinal prosthetic strategy with the capacity to restore normal vision</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;<volume>109</volume>:<fpage>15012</fpage>–<lpage>15017</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1207035109" xlink:type="simple">10.1073/pnas.1207035109</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nirenberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name>. <article-title>The light response of retinal ganglion cells is truncated by a displaced amacrine circuit</article-title>. <source>Neuron</source>. <year>1997</year>;<volume>18</volume>:<fpage>637</fpage>–<lpage>650</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(00)80304-9" xlink:type="simple">10.1016/S0896-6273(00)80304-9</ext-link></comment> <object-id pub-id-type="pmid">9136772</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Staras</surname> <given-names>K</given-names></name>. <article-title>The probability of neurotransmitter release: variability and feedback control at single synapses</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2009</year>;<volume>10</volume>:<fpage>373</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2634" xlink:type="simple">10.1038/nrn2634</ext-link></comment> <object-id pub-id-type="pmid">19377502</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Efron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Stein</surname> <given-names>C</given-names></name>. <article-title>The jackknife estimate of variance</article-title>. <source>Annals of Statistics</source>. <year>1981</year>;<volume>9</volume>:<fpage>586</fpage>–<lpage>596</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176345462" xlink:type="simple">10.1214/aos/1176345462</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Steele</surname> <given-names>JM</given-names></name>. <article-title>An Efron-Stein inequality for nonsymmetric statistics</article-title>. <source>Annals of Statistics</source>. <year>1986</year>;<volume>14</volume>:<fpage>753</fpage>–<lpage>758</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176349952" xlink:type="simple">10.1214/aos/1176349952</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref036">
<label>36</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Boucheron</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bousquet</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Lugosi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Massart</surname> <given-names>P</given-names></name>. <source>Concentration Inequalities: A Nonasymptotic Theory of Independence</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2013</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/acprof:oso/9780199535255.001.0001" xlink:type="simple">10.1093/acprof:oso/9780199535255.001.0001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005110.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shun</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>McCullagh</surname> <given-names>P</given-names></name>. <article-title>Laplace approximation of high dimensional integrals</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1995</year>; p. <fpage>749</fpage>–<lpage>760</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005110.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Seeger M. Expectation propagation for exponential families. 2005;(EPFL-REPORT-161464).</mixed-citation>
</ref>
</ref-list>
</back>
</article>