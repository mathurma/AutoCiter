<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006205</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-02085</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject><subj-group><subject>Normal distribution</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject><subj-group><subject>Statistical dispersion</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability density</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Operator theory</subject><subj-group><subject>Kernel functions</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kernel methods</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kernel methods</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Instance-based generalization for human judgments about uncertainty</article-title>
<alt-title alt-title-type="running-head">Instance-based generalization under uncertainty</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5349-9082</contrib-id>
<name name-style="western">
<surname>Schustek</surname>
<given-names>Philipp</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Moreno-Bote</surname>
<given-names>Rubén</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Center for Brain and Cognition and Department of Information and Communications Technologies, Pompeu Fabra University, Barcelona, Spain</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">philipp.schustek@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>4</day>
<month>6</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>6</issue>
<elocation-id>e1006205</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>12</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Schustek, Moreno-Bote</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006205"/>
<abstract>
<p>While previous studies have shown that human behavior adjusts in response to uncertainty, it is still not well understood how uncertainty is estimated and represented. As probability distributions are high dimensional objects, only constrained families of distributions with a low number of parameters can be specified from finite data. However, it is unknown what the structural assumptions are that the brain uses to estimate them. We introduce a novel paradigm that requires human participants of either sex to explicitly estimate the dispersion of a distribution over future observations. Judgments are based on a very small sample from a centered, normally distributed random variable that was suggested by the framing of the task. This probability density estimation task could optimally be solved by inferring the dispersion parameter of a normal distribution. We find that although behavior closely tracks uncertainty on a trial-by-trial basis and resists an explanation with simple heuristics, it is hardly consistent with parametric inference of a normal distribution. Despite the transparency of the simple generating process, participants estimate a distribution biased towards the observed instances while still strongly generalizing beyond the sample. The inferred internal distributions can be well approximated by a nonparametric mixture of spatially extended basis distributions. Thus, our results suggest that fluctuations have an excessive effect on human uncertainty judgments because of representations that can adapt overly flexibly to the sample. This might be of greater utility in more general conditions in structurally uncertain environments.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Are three heavy tropical storms this year compelling evidence for climate change? A suspicious clustering of events may reflect a real change of the environment or might be due to random fluctuations because our world is uncertain. To generalize well, we should build a probability distribution over our observations defined in terms of latent causes. If data is scarce we are forced to make strong assumptions about the shape of the distribution ideally incorporating our prior knowledge. In our task, human behavior is consistent with probabilistic inference but reveals a tendency to generalize based on observed instances enhancing the effect of random patterns on behavioral judgments. The decreased reliance on available constraints through prior knowledge corresponds to a dominance of bottom-up sensory information. Maintaining a balance with expectation-driven top-down information is crucial for proper generalization. Our work provides evidence for the necessity to include graded instance-based generalization into the mathematical formulation of cognitive models. The investigation of the determinants and neural substrates of this inferential bias is expected to give insights into the richness but also fallibility of human inferences.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>MINECO (Spain)</institution>
</funding-source>
<award-id>PSI2013-44811-P</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Moreno-Bote</surname>
<given-names>Rubén</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>MINECO (Spain)</institution>
</funding-source>
<award-id>FLAGERA-PCIN-2015-162-C02-02</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Moreno-Bote</surname>
<given-names>Rubén</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000011</institution-id>
<institution>Howard Hughes Medical Institute</institution>
</institution-wrap>
</funding-source>
<award-id>55008742</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Moreno-Bote</surname>
<given-names>Rubén</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>FI-AGAUR scholarship of the Secretariat for Universities and Research of the Ministry of Business and Knowledge of the Government of Catalonia and the European Social Fund</institution>
</funding-source>
<award-id>G62978689</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5349-9082</contrib-id>
<name name-style="western">
<surname>Schustek</surname>
<given-names>Philipp</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>PS was supported by a FI-AGAUR scholarship of the Secretariat for Universities and Research of the Ministry of Business and Knowledge of the Government of Catalonia and the European Social Fund (G62978689, agaur.gencat.cat). RM-B is supported by PSI2013-44811-P and FLAGERA-PCIN-2015-162-C02-02 from MINECO (Spain) and Howard Hughes Medical Institute (HHMI), ref 55008742. This work was supported by CERCA Programme / Generalitat de Catalunya. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-06-14</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Determining from limited data when observations reflect a consistently appearing pattern or when they are merely the result of randomness is important to faithfully represent the environment (e.g. [<xref ref-type="bibr" rid="pcbi.1006205.ref001">1</xref>]). Suppose you want to assess the skill of a dart player in throwing darts at the bullseye (center) of the board. For a single bad throw, it is hard to discern whether it was due to bad luck or to the general inability of the player. For several throws, however, the dispersion of the darts around the center should more closely reflect the skill of the player.</p>
<p>To represent uncertainty of our knowledge in this and more general situations, normative considerations suggest that an agent should explicitly represent knowledge as probability distributions instead of point estimates [<xref ref-type="bibr" rid="pcbi.1006205.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref003">3</xref>]. Several studies have shown that under certain conditions humans behave as if the uncertainty about a task-relevant variable was available to them as a distribution over its possible values [<xref ref-type="bibr" rid="pcbi.1006205.ref004">4</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref005">5</xref>].</p>
<p>For instance, judging the skill of the dart player corresponds to estimating the spread of the distribution around the observed values. This requires constraining structural assumptions about the ‘shape’ of the underlying probability distribution (e.g. a parameterized function such as a Laplacian or Gaussian). However, it is generally unknown what assumptions are used by humans when dealing with uncertainty. Ideally, previous knowledge about the data generation process, such as an expectation for the darts to cluster around the center corresponding to the goal in the example, is incorporated. As opposed to visuo-motor uncertainty [<xref ref-type="bibr" rid="pcbi.1006205.ref006">6</xref>], there is little evidence for the shape of inferred trial-by-trial perceptual representations in the small sample limit. In several previous studies such as cue combination [<xref ref-type="bibr" rid="pcbi.1006205.ref007">7</xref>], distributional estimates are taken to be normally distributed. While this may be justifiable under certain conditions [<xref ref-type="bibr" rid="pcbi.1006205.ref008">8</xref>], we challenge the general validity of this assumption.</p>
<p>To generalize from sparse data, one inevitably must make assumptions about the distribution. In other words, we have to choose a suitable model for probabilistic inference. In the most elementary case, probability density must be assigned to the vicinity of an observed point in some internal psychological space defining a metric of similarity between possible occurrences [<xref ref-type="bibr" rid="pcbi.1006205.ref009">9</xref>].</p>
<p>In doing so, weak assumptions give more freedom to the observed instances of the data to determine the inferred distribution. The resulting generalizations are similarity-based and have been used to explain certain characteristics of how humans learn continuous functions [<xref ref-type="bibr" rid="pcbi.1006205.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref011">11</xref>]. Similarly, such instance-based or exemplar methods were suggested to describe the representations that underlie human categorizations [<xref ref-type="bibr" rid="pcbi.1006205.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006205.ref014">14</xref>]. If such inferences are formulated in probabilistic terms, this is commonly implemented by nonparametric methods, such as kernel density estimation [<xref ref-type="bibr" rid="pcbi.1006205.ref015">15</xref>].</p>
<p>Stronger assumptions, on the other hand, may allow for more powerful generalizations [<xref ref-type="bibr" rid="pcbi.1006205.ref016">16</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref017">17</xref>] if they are based on appropriate prior knowledge about the task structure [<xref ref-type="bibr" rid="pcbi.1006205.ref018">18</xref>]. Correspondingly, a more restricted class of parametric probability distributions is used. The function learning literature refers to the more constrained case as rule-based [<xref ref-type="bibr" rid="pcbi.1006205.ref019">19</xref>] because humans appear to learn explicit functions of some family, such as polynomials [<xref ref-type="bibr" rid="pcbi.1006205.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref021">21</xref>]. Similarly, strong assumptions can be incorporated into models of categorization by positing a prototype for each category [<xref ref-type="bibr" rid="pcbi.1006205.ref022">22</xref>]. Critically, we emphasize that inferential methods are not limited to the extremes of strong and weak assumptions but may exist as combinations along a continuum [<xref ref-type="bibr" rid="pcbi.1006205.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref023">23</xref>].</p>
<p>Here we asked what kind of internal structural assumptions humans employ to generalize from sparse observations. Human participants are asked to quantify uncertainty about future events by estimating the dispersion of a normally distributed random variable. Although the instructions and the framing of the task suggested a simple, centered, unimodal, bell-shaped distribution, human behavior was not consistent with structural assumptions based on a close to normal probability distribution. Instead, human behavior was better explained by instance-based generalization whereby observed samples were used to build an internal representation of the underlying probability distribution, not necessarily unimodal or symmetric. The resulting internal representation is a mixture of several components and hence less sparse than necessary. Our participants demonstrated faithful trial-by-trial estimates of uncertainty which are suggested to originate from internal uncertainty representations, as the opportunity to learn suitable stimulus-response associations from feedback was avoided in our task design [<xref ref-type="bibr" rid="pcbi.1006205.ref003">3</xref>]. All alternative heuristic explanations proved insufficient to explain the complex and consistently accurate estimates. Hence, our results support the notion that approximate probabilistic processing underlies behavior.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We asked human participants to estimate the dispersion of future events from a small sample by indicating a range in which they predicted 65% of all future events to fall. The task instructions alluded to judging the ability of a dart player to hit the target based only on the outcome of previous attempts (<xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1</xref>). More specifically, participants were asked to judge the unknown accuracy of a “dart player” to hit the center of the board (<xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1A</xref>). On a given trial, of a total of 320 trials, the participants are shown four points representing the “darts” thrown by one unobserved player of unknown accuracy to hit the center of the board. Based on the four observed “darts”, participants must predict where future darts might strike the board. Specifically, participants were asked to capture 65% of all future imaginary darts from the same unobserved player by adjusting the width of the rectangular frame of size 2<italic>y</italic> symmetrically about the center (<italic>y</italic> is the horizontal, one-sided distance of the lateral borders of the rectangle to the center). Only the horizontal dispersion of the dots is relevant to estimate the accuracy of the dart player, while vertical displacements are added just to improve visibility of the samples. The choice of 65% is convenient as it does not depend on an accurate estimate of the distribution’s tail and conveniently allows to examine a limiting case of instanced-based generalization. Participants were informed that they would see a new player of unknown and fixed accuracy to hit the center in every trial, that there would be just as many amateur as expert level players and that the order of appearance is unpredictable.</p>
<fig id="pcbi.1006205.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Human participants perform a task consisting in estimating the dispersion of future events based on a few observations.</title>
<p><bold>(A)</bold> Schematic of one trial of the task. Participants were asked to judge the unknown accuracy of a “dart player” to hit the center of the board (gray rectangle). Based on the four observed “darts” (white dots), participants must predict where future darts might strike the board. Specifically, participants were asked to capture 65% of all future imaginary darts by adjusting the width of the rectangular frame (colored frames, see below). Only the horizontal dispersion of the dots is relevant to estimate the accuracy of the dart player, while vertical displacements are added just to improve visibility of the samples. <bold><italic>(B)</italic></bold> Based on the observed samples, the participant might infer a predictive probability distribution over the position of the next sample. Two hypothetical predictive distributions are shown, representing different structural assumptions about how the samples might have been generated, corresponding to maximum likelihood estimation based on a Gaussian distribution (blue) or a generalized normal distribution with shape parameter <italic>p</italic> = 10 (orange) (see <xref ref-type="sec" rid="sec011">Methods</xref>). Based on the predictive probability distribution, the participant can set the frame’s width so that it matches the target percentage of 65% (colored frames in panel A). Note that for the assumption of a generalized normal distribution, the posterior is more sensitive to data points far from the center and hence a larger frame is chosen. <bold>(C)</bold> The horizontal positions of the points with respect to the center were generated as follows. First, all samples <bold><italic>r</italic></bold> <italic>= (r</italic><sub>1</sub>,…,<italic>r</italic><sub>4</sub>) were generated independently from a standard normal distribution. Second, the samples were scaled by the factor <italic>ν</italic>/<italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>r</italic></bold>), where <inline-formula id="pcbi.1006205.e001"><alternatives><graphic id="pcbi.1006205.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula> is the maximum likelihood estimator (MLE) for a normal distribution centered at zero and <italic>ν</italic> is drawn from a uniform probability distribution over the range of [10,140] pixels. The scaled samples <bold><italic>d</italic></bold> = <italic>ν</italic>/<italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>r</italic></bold>) ⋅ <bold><italic>r</italic></bold> feature a MLE given by <inline-formula id="pcbi.1006205.e002"><alternatives><graphic id="pcbi.1006205.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:mi>ν</mml:mi></mml:math></alternatives></inline-formula>. This method allows choosing any desired distribution of <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) by setting <italic>ν</italic> correspondingly. <bold>(D)</bold> Histogram of <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) across 320 trials (blue). For comparison, the red histogram indicates the results for a sample scaling <bold><italic>d</italic></bold> = <italic>ν</italic> ⋅ <bold><italic>r</italic></bold> without normalizing by <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>r</italic></bold>). Both samples have a comparable mean, but the red distribution features few but extremely outlying values, which are avoided by our scaling method.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g001" xlink:type="simple"/>
</fig>
<p>Ideally, this task could be accomplished by inferring the dispersion of the generative distribution which in accordance to the task and its instructions was chosen to be Gaussian. Based on the observed samples, a probabilistic agent would infer a predictive probability distribution over the position of the next sample to accurately estimate the size of the frame that would capture 65% of the imaginary darts thrown by the very same dart player with the same abilities. Inference requires the specification of a generative model of the observed data. However, the actual generative model in the environment, controlled by the experimenter, and the model that the agent uses for inference are generally different. Nevertheless, in order that inference is optimal for this task, the agent’s probabilistic model needs to match the generative process. Exploiting knowledge that a normal distribution <italic>d</italic><sub><italic>n</italic></sub> ~ <italic>N</italic>(<italic>μ</italic> = 0,<italic>σ</italic>) centered at zero is responsible for the <italic>N</italic> = 4 observations <bold><italic>d</italic></bold> = (<italic>d</italic><sub>1</sub>,…,<italic>d</italic><sub><italic>N</italic></sub>), estimation of the predictive density <italic>p</italic>(<italic>x</italic>|<bold><italic>d</italic></bold>) over an unseen event <italic>x</italic> amounts to inference of the only unknown quantity, the standard deviation <italic>σ</italic>, parameterizing the zero mean Gaussian. Maximizing the likelihood function <italic>p</italic>(<bold><italic>d</italic></bold>|<italic>σ</italic>) with respect to <italic>σ</italic> yields <inline-formula id="pcbi.1006205.e003"><alternatives><graphic id="pcbi.1006205.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> which corresponds to the expression for the standard deviation with a known mean of zero. The predictive distribution may be directly based on the specific value determined by maximum likelihood estimation (MLE) <italic>p</italic>(<italic>x</italic>|<italic>σ</italic> = <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>)), which is illustrated in <xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1B</xref>. However, given the observations it is not possible to determine <italic>σ</italic> with certainty. The maximum likelihood estimator <italic>σ</italic><sub><italic>ML</italic></sub> and the number of observations <italic>N</italic> can only be regarded as sufficient statistics for <italic>σ</italic>.</p>
<p>The Bayesian treatment explicitly acknowledges this uncertainty by computing the posterior distribution <italic>p</italic>(<italic>σ</italic>|<bold><italic>d</italic></bold>) over possible values of <italic>σ</italic>.
<disp-formula id="pcbi.1006205.e004">
<alternatives>
<graphic id="pcbi.1006205.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
Additionally, this requires the specification of the prior distribution <italic>p</italic>(<italic>σ</italic>) which is part of the agent’s subjective knowledge. However, to be task-optimal, it must equal the actual distribution over <italic>σ</italic> in the environment, i.e. the base rate at which the hidden variable <italic>σ</italic> occurs. For this task, it should be a uniform distribution over the range of [0,140] pixels. To then predict the probability of the next event at position <italic>x</italic> given <bold><italic>d</italic></bold>, <italic>σ</italic> has to be marginalized out. The predictive distribution results from the probabilistic model <italic>N</italic>(<italic>x</italic>|0, <italic>σ</italic>) weighted by the posterior over <italic>σ</italic>.
<disp-formula id="pcbi.1006205.e005">
<alternatives>
<graphic id="pcbi.1006205.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>σ</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
More generally, the predictive distribution <italic>p</italic>(<italic>x</italic>|<bold><italic>d</italic></bold>) corresponds to the belief about future events after observing data <bold><italic>d</italic></bold>.</p>
<p>Now, we turn to the problem of how the agent might set the frame in a principled way based on the estimated predictive probability distribution. For a given setting of the rectangular frame <italic>z</italic>, one can determine the fraction of future events within that interval, the capture probability <italic>c</italic>, by calculating the integral
<disp-formula id="pcbi.1006205.e006">
<alternatives>
<graphic id="pcbi.1006205.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
More generally, the inferred distribution in (3) provides an objective to determine the response <italic>y</italic> (half-frame size) on a trial-by-trial basis. To match the target probability of 65%, the frame size <italic>z</italic> should be optimized such that the capture probability matches the target probability (<xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1B</xref>). In other words, the response <italic>y</italic> is the optimized frame size that matches
<disp-formula id="pcbi.1006205.e007">
<alternatives>
<graphic id="pcbi.1006205.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.65</mml:mn></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
This inference procedure of a normal distribution whose width is assumed to vary parametrically across trials is devised as a reference model (benchmark) for comparison with behavior. It follows the inference procedure of Eqs (<xref ref-type="disp-formula" rid="pcbi.1006205.e004">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006205.e005">2</xref>) and assumes a uniform prior over the range of [0,140] pixels corresponding to the task instructions. The Bayesian benchmark model was chosen as reference for motivational feedback and bonus payments to incentivize engagement in the task (see <xref ref-type="sec" rid="sec011">Methods</xref>).</p>
<p>However, to generate the data <bold><italic>d</italic></bold> that was presented to our participants, we used a slightly modified sampling scheme which reduces response noise and keeps outlying conditions to a minimum translating into improved discriminatory power for model comparison (see <xref ref-type="sec" rid="sec011">Methods</xref>). This was achieved by renormalization of the raw samples <bold><italic>r</italic></bold> (<xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1C</xref>). Therefore, a draw <italic>ν</italic> from the uniform distribution over the desired range of dispersions directly determines the sufficient statistic <italic>σ</italic><sub><italic>ML</italic></sub>. Omitting sample renormalization instead corresponds to sampling from a Gaussian whose width parameter <italic>σ</italic> is drawn from a uniform distribution. This would have led to a long-tailed <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) distribution with undesirable properties (s. <xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1D</xref>) which is avoided by our approach.</p>
<p>The goal of the study is to determine which inductive biases participants employ for generalization and whether that conforms to the structural assumptions suggested by the framing of the task. More specifically, we attempted to distinguish between inference of a centered, unimodal, bell-shaped distribution, such as a Gaussian (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2A</xref>), and variants of instance-based generalization (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2B–2D</xref>) which make only very few assumptions about the distribution to be inferred.</p>
<fig id="pcbi.1006205.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Generalization beyond the observed sample is governed by the parametric assumptions of the distribution.</title>
<p>Each row shows examples of probability densities (black lines) for a different sample (green and blue dots, four observations) in units of its root mean squared deviation (RMSD). <bold>(A)</bold> A zero-centered unimodal Gaussian distribution is used to account for the whole sample. All point positions <bold><italic>d</italic></bold> = (<italic>d</italic><sub>1</sub>,…,<italic>d</italic><sub>4</sub>) enter via the estimated standard deviation parameter, <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) (RMSD), determined by probabilistic inference. Whereas for instanced-based generalization the sample points effectively enter as parameters themselves. <bold>(B-D)</bold> Different additive basis distributions (red) can be used to cover the observation space. The tiling model covers the space with adjacent non-overlapping uniform basis distributions resulting in a compressed distribution around spatially proximal points <bold>(B)</bold>. Additionally, models can be constructed from simpler components by centering a Gaussian kernel on each observation (see <xref ref-type="sec" rid="sec011">Methods</xref>). In the limit of vanishing kernel widths <bold>(C)</bold> there is no generalization beyond the sample while for larger widths <bold>(D)</bold> a smoothed density over the whole domain is obtain due to overlapping basis distributions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g002" xlink:type="simple"/>
</fig>
<p>We will test several models of the latter class to obtain more information about the specific characteristics of the internal representations. The tiling model constructs a normalized histogram under the constraint that an observed point only exhibits a local effect on the constructed density by tiling the domain into non-overlapping basis distributions (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2B</xref>). Similar representations were, for instance, suggested to underlie the representation of visuo-motor errors [<xref ref-type="bibr" rid="pcbi.1006205.ref024">24</xref>]. The degree of generalization critically depends on how far away from a sample’s position the inferred density is affected [<xref ref-type="bibr" rid="pcbi.1006205.ref025">25</xref>]. Therefore, we use a kernel density estimation method [<xref ref-type="bibr" rid="pcbi.1006205.ref022">22</xref>] with Gaussian, and hence spatially extended, basis distributions. The width of these basis distributions critically governs the locality of the influence of the sample on the internal representation and will be inferred from behavior. For very narrow Gaussians (<italic>δ</italic>-KDE, <xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2C</xref>) generalization is weak whereas large and overlapping kernels indicate stronger generalizations beyond the sample (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2D</xref>). We furthermore investigated whether participants might derive their behavior from an internal representation of a probability distribution. Alternatively, any measure that correlates with the dispersion to be estimated might serve to inform behavior. These heuristics are primarily chosen to facilitate processing and not to achieve a more accurate representation of the environment. Our task allows explicit testing of some heuristic short-cuts to the task.</p>
<sec id="sec003">
<title>Faithful tracking of trial-by-trial uncertainty</title>
<p>First, we tested whether participants demonstrate the ability to faithfully estimate the dispersion of the centered normal distribution assumed to be responsible for the observations. The MLE of the Gaussian, <italic>σ</italic><sub><italic>ML</italic></sub> (<xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3A</xref>, red), is the sufficient statistic to inform the optimal response (green).</p>
<fig id="pcbi.1006205.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Human behavior closely tracks trial-by-trial uncertainty of future events.</title>
<p><bold>(A)</bold> Mean responses across participants plotted as a function of the MLE of the sample, <italic>σ<sub>ML</sub>(<bold>d</bold>)</italic>, in ten equally spaced bins (black; error bars, 95% CI). Basing behavior on a Gaussian estimated by ML (red, <italic>N(x|0,σ<sub>ML</sub>(<bold>d</bold>))</italic>) results in responses proportional to the estimate. The prior distribution that is assumed by the devised Bayesian benchmark model (green) biases responses towards intermediate values (see <xref ref-type="sec" rid="sec011">Methods</xref>). <bold>(B)</bold> Individual response curves of all 23 participants tested (gray lines). Three participants displaying poor compliance with the instructed task (dotted) were excluded from further analysis. The average across the remaining participants is superimposed (black).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g003" xlink:type="simple"/>
</fig>
<p>The averaged mean response across participants (black) is related to the ML approach in a approximately linear relationship (Methods). Assuming that participants use the Gaussian distribution for inference (Methods, normal model) yields good predictive performance and accounts for a substantial amount of the variance (regression, cross-validated median <italic>R</italic><sup>2</sup> = 0.80, 95%-CI (0.73,0.82), across participants). Hence their judgments correlated tightly with the uncertainty about the abilities of the supposed dart players. Such uncertainty tracking is also apparent on an individual participant level (<xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3B</xref>) (cross-validated median <italic>R</italic><sup>2</sup> ranging from 0.47 to 0.93). On average, the responses appear to be systematically biased toward intermediate values with respect to the ML approach (<xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3A</xref>, red) resembling the effect of a prior distribution (green) incorporating knowledge about the range of dispersions across trials. To quantify this effect, we used two variants of a model (Methods, Weighting model) that effectively determines whether judgments may still be predicted well if they are assumed to be proportional to the estimated dispersion (<xref ref-type="disp-formula" rid="pcbi.1006205.e036">Eq 7</xref>). However, this was found to be strongly inferior to a linear relationship (Methods, <xref ref-type="disp-formula" rid="pcbi.1006205.e026">Eq 5</xref>), even on an individual level (cross validation log likelihood (CVLL) difference Δ ≥ 20 for 12 participants, Δ ≥ 10 for 17 participants).</p>
</sec>
<sec id="sec004">
<title>Evidence for an internal trial-by-trial objective</title>
<p>Next, behavior is examined with respect to the objective participants were instructed to obey. Namely, if their estimates are quantitatively accurate and correspond to the 65% target percentage. For independent trials, participants must infer the dispersion anew on each trial. Inferring a probability distribution over future events allows behavior to be derived from a principled trial-by-trial objective regarding the target percentage (see <xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1B</xref> and <xref ref-type="sec" rid="sec011">Methods</xref>, Eqs <xref ref-type="disp-formula" rid="pcbi.1006205.e006">3</xref> and <xref ref-type="disp-formula" rid="pcbi.1006205.e007">4</xref>). By construction, our task objective demands a quantification of the relative frequency of all future events and was intended to require participants to approximate distributional estimates.</p>
<p>To examine how well participants performed with respect to the devised optimal inference strategy, we calculated the capture percentage by evaluating (<xref ref-type="disp-formula" rid="pcbi.1006205.e006">Eq 3</xref>) with respect to the optimally inferred probability distribution (Eqs <xref ref-type="disp-formula" rid="pcbi.1006205.e004">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006205.e005">2</xref>). The distribution of the per participant median capture percentage across all trials is clustered close to the target of 65% (<xref ref-type="fig" rid="pcbi.1006205.g004">Fig 4A</xref>). In this measure opposing deviations cancel, so that it evidences an overall compliance to the target percentage across all trials. The median across participants is close to the target percentage, which indicates that participants quantify uncertainty in a quantitatively similar manner as the probabilistic benchmark model. The median of the absolute deviation per response is 6.54% (95% CI, (5.83,7.28) %) with respect to the external objective of the task. However, it is possible that behavior has been produced from an internal objective (see <xref ref-type="disp-formula" rid="pcbi.1006205.e007">Eq 4</xref>) in which the percentage is matched much more closely to 65%. There are at least two contributions that inflate the deviation from the external measure (<xref ref-type="fig" rid="pcbi.1006205.g004">Fig 4A</xref>). First, there is intrinsic response noise which would even occur for fixed stimuli on the screen, e.g. through motor-related variability. Second, there are deviations due to mismatched inference with respect to our benchmark model [<xref ref-type="bibr" rid="pcbi.1006205.ref026">26</xref>]. The latter are deterministic and the result of e.g. different prior knowledge from the one assumed by our benchmark model. Altogether, the median absolute deviation (<xref ref-type="fig" rid="pcbi.1006205.g004">Fig 4A</xref>) is a conservative upper bound estimate for an internal trial-by-trial objective of the capture percentage such that the quantitative match with the target percentage can be considered high.</p>
<fig id="pcbi.1006205.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Behavior is consistent with participants possessing a subjective but well calibrated trial-by-trial internal objective that remains stable over the experiment.</title>
<p>(<bold>A</bold>) Across trials participants tend to comply well to the objective despite per trial deviations due to systematic biases and response noise, as the capture percentage <italic>c</italic> is typically around the target value 65% (vertical axis) and the median deviance is relatively small (horizontal axis). Histograms correspond to marginal distributions. (<bold>B</bold>) Participants display stable behavior throughout the experiment, as they do not appear to adjust their responses closer to the task objective over time. Median capture percentages <italic>c</italic> are calculated separately for the first and second halves of the experimental session.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g004" xlink:type="simple"/>
</fig>
<p>If participants did not possess an internal trial-by-trial objective, they could instead associate stimuli with suitable responses by a learning a behavioral function. Next, we tested whether behavior is consistent with this alternative approach, which should result in across-trial and feedback dependencies. Even though barely informative, the feedback may have been used to adjust behavior. Remarkably, however, the median capture percentage appears not to adjust closer to the target percentage as indicated by similar values calculated separately for the first and second half of the experimental session for each participant (<xref ref-type="fig" rid="pcbi.1006205.g004">Fig 4B</xref>). The absolute difference of the median capture deviation is small and not significantly different from zero (right-tailed Wilcoxon signed rank test, <italic>p</italic> = 0.48) despite the fact that the trial-averaged feedback about the capture percentage in the experimental session may have allowed to derive some global adjustments. Accordingly, too high a capture percentage on average should subsequently lead to the choice of smaller response frames. Hence, a decrease of the feedback error would be expected over time. The results, on the other hand, suggest that participants did not even use feedback to calibrate their probability estimates. We also confirmed that the previously presented feedback about the capture percentage did not influence behavior (regression, exceedance probability 2.04 ⋅ 10<sup>−4</sup> compared to baseline model, see <xref ref-type="sec" rid="sec011">Methods</xref>). Similarly, no considerable dependencies across trials were found (Methods). Consequently, it appears unlikely that the feedback scheme had an important influence on behavior.</p>
<p>Overall, participants typically predict the dispersion of future darts in a quantitatively accurate manner. They appear to have relied on an internal trial-by-trial objective regarding the target percentage as they largely conform to trial independence, feature stable processing across time and virtually ignore feedback. This is consistent with internal probabilistic processing.</p>
</sec>
<sec id="sec005">
<title>Systematic deviations from inference of a Gaussian</title>
<p>Thus far, behavior appears to be close to the optimal inference strategy defined by the benchmark model, but we have also observed deviations (Figs <xref ref-type="fig" rid="pcbi.1006205.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1006205.g004">4A</xref>). If behavior follows from inference of a normal distribution, it can only depend on the sample via the sufficient statistic, <inline-formula id="pcbi.1006205.e008"><alternatives><graphic id="pcbi.1006205.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula>. This means that the squared position of each point should contribute equally to the final estimate. We tested this with a weighting model that generalizes <italic>σ</italic><sub><italic>ML</italic></sub> by assigning a tunable weight <italic>ω</italic><sub><italic>n</italic></sub> to each input depending on its excentricity, <inline-formula id="pcbi.1006205.e009"><alternatives><graphic id="pcbi.1006205.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula>. Excentricity refers to the distance from the center irrespective of the side where the sample occurs.</p>
<p>Experimentally, the weights of the individual points tend to take unequal values when we index them with respect to their distance from the center (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5A</xref>). Participants put more emphasis on the third most excentric point and down-weigh the first and the fourth point. We also tested whether other models of behavior, such as the KDE model, are able to reproduce this pattern (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>). For that purpose, we used those fitted models to generate surrogate responses for every actual experimental response. Subsequently, for the comparison, the weighting model was fitted to the surrogate responses. In the following, models will be compared by both the (i) weighting pattern (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>) as well as their (ii) overall ability to predict behavior (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>). Consistent with the weighting pattern observed in our data, the normal model (nm) is far from providing the best predictions of behavior. This can be seen from the pairwise model comparison matrix (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>). There the binomial probability that the model indexing the row (vs. the model indexing the column) is more likely to account for the data of a randomly chosen participant is depicted as color code. Additionally, entries with high exceedance probabilities are considered significant (Methods) and marked with asterisks. For instance, the comparison between the weighting model in row (wgt) to the normal model in column (nm) shows that the latter is clearly rejected (<italic>p</italic><sub><italic>exc</italic></sub> &gt; 0.999). Beyond the group level, the normal model can be decisively ruled out individually for many participants despite the fact that generally different participants are best described by different models.</p>
<fig id="pcbi.1006205.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The weighting pattern of the observed samples deviates from inference of a close-to-normal distribution and matches kernel density estimation (KDE).</title>
<p>Evaluation of the normalized weights <italic>ω<sub>n</sub></italic> of the weighting-model <inline-formula id="pcbi.1006205.e010"><alternatives><graphic id="pcbi.1006205.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula> as a generalization of the MLE of a zero centered Gaussian. The points are indexed according to their distance from the center. <bold>(A)</bold> Input weight that each participant (gray lines) assigns as a function of the weight index. If participants followed optimal MLE based on a Gaussian centered at zero, all input weights should be equal (black line). Fitting of the weighting model (see <xref ref-type="sec" rid="sec011">Methods</xref>) shows a systematic deviation of the across participant median (red, error bars, 95% CI). Participants tend to overweigh the third most extreme value compared to the others. <bold>(B)</bold> Among all models tested, only KDE (blue) qualitatively matches the characteristics of the experimental weighting pattern (red, same as panel A). The other models fail to capture the behavioral weighting pattern (fits of the weighting model to the other indicated models’ output). Model abbreviations: kde—kernel density estimation, tlg—tiling, gnm—generalized normal, max–maximum.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g005" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006205.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Pairwise model comparison evidences an inclination to resort to instance-based generalization, indicating that fluctuations have a profound effect on the inferred representations.</title>
<p>Summarized results of a hierarchical Bayesian model comparison procedure that estimates probability distributions over models. Pairwise comparisons (each square) are performed to evidence relative differences in prediction for models with different features. The color code over each square shows estimates of the parameter of the binomial distribution governing the probability by which the model indexed by the row is more likely than the one indexed by the column. This corresponds to the expectation value that a given model is considered responsible for generating the data of a randomly chosen participant. Superimposed are large differences of the exceedance probability (<inline-formula id="pcbi.1006205.e011"><alternatives><graphic id="pcbi.1006205.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mi>*</mml:mi><mml:mover accent="true"><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mn>0.99</mml:mn><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.95</mml:mn><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>; <inline-formula id="pcbi.1006205.e012"><alternatives><graphic id="pcbi.1006205.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mi>*</mml:mi><mml:mi>*</mml:mi><mml:mover accent="true"><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.99</mml:mn></mml:math></alternatives></inline-formula>) which quantifies the belief that the row model is more likely to have generated the data of a randomly chosen participant compared to the column model. Model abbreviations: gpr—Gaussian process regression, wgt—weighting, kde—kernel density estimation, gnm—generalized normal, tlg—tiling, nm—normal, max—maximum, rng–range.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g006" xlink:type="simple"/>
</fig>
<p>We tested whether generalizations of the Gaussian can account for the systematic deviations that were observed before. The generalized normal model (gnm) allows for more freedom in the representation of the inferred density through a shape parameter governing its kurtosis (see <xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1B</xref>) by generalizing the square in the exponential function to other powers than two leading to an unequal weighting pattern of the samples (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>). This model predicts significantly better than the Normal-model (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>, <italic>p</italic><sub><italic>exc</italic></sub> &gt; 0.999) by making use of the additional shape parameter to represent heavier tailed distributions (quartiles across participants <italic>Q</italic> = (0.79,1.24,1.68)). Heavier tailed distributions discount outlying and enhance the influence of inlying points on judgments (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>, black line). The experimental pattern (red) is not matched well suggesting that it does not reflect how participants behave. In addition, the weighting model still outperforms the generalized normal model (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>).</p>
</sec>
<sec id="sec006">
<title>Simple heuristics are poor predictors</title>
<p>We determined above that responses are on average relatively close to optimal but that the finer-grained behavioral patterns are inconsistent with inference of a Gaussian. That raises the question whether simpler, heuristic strategies that unequally weigh sample information might offer a better account of behavior.</p>
<p>We first tested the established heuristic models that use perceptually simple statistics and only a subset of the available information. The maximum model (max) only depends on the most excentric point which leads to a weighting pattern (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>, yellow) which is highly inconsistent with the experimental one (red). The participants’ weighting is more balanced and typically features weights smaller than four (normalization to number of sample points). The range model (rng) is based on the sample's range and predicts worse than the maximum model (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>). On the group level, both are clearly refuted by all other models.</p>
<p>Another heuristic strategy is attending to just one point when sorting them according to their excentricity. In particular, the third most excentric point is important as it closely corresponds to the target percentage of 65% on the sample and is the response in the limiting case of pure instance-based generalization (see <italic>δ</italic>-KDE model, <xref ref-type="sec" rid="sec011">Methods</xref>). Participants typically take all point positions into account. The four unnormalized weights (<italic>w</italic><sub>1</sub>,…,<italic>w</italic><sub>4</sub>) are significantly different from zero for the respective number of (14, 20, 20, 19) of all 20 participants (Weighting model, 10000-fold permutation test, <italic>p</italic>-value threshold 0.05). Furthermore, for each individual at most one weight is non-significant showing that it is not an effect of grouping. Consistent with integration of the whole sample, the maximum of the normalized weights is considerably lower than four (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5A</xref>).</p>
<p>Altogether, this is evidence that among all participants few exploit heuristics. The clear majority however resorted to some more sophisticated weighting inconsistent with the simple heuristics tested.</p>
</sec>
<sec id="sec007">
<title>Behavior relies on instance-based generalization</title>
<p>So far, participants appear to violate the assumptions of a close to Gaussian distribution centered at zero that was suggested by the task instructions and the dart metaphor. Alternatively, the probability distribution to be inferred may be directly constructed from the observed instances by imposing only minimal structural constraints on the data. That corresponds to the assumption that the sample is representative of the unknown population to be estimated.</p>
<p>Our tiling model (tlg) implements such an approach with spatially confined basis distributions. It places a uniform distribution in between observations and hence the resulting density is increased around clusters and reduced elsewhere (Methods). It adapts to the fluctuations which are present in the sample. Consequently, the target capture percentage of 65% is by construction very close to the third most excentric point. As a result, this model emphasizes the third most excentric point (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>, green) and thus captures an important characteristic of behavior (red).</p>
<p>The kernel density estimation (KDE) model uses Gaussian basis functions to implement instance-based generalization. It centers a Gaussian distribution on each data point and thus assigns density to its vicinity depending on the standard deviation parameter. The experimental weighting pattern (black) is closely captured by KDE (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>, blue). It is very successful at predicting behavior and superior to both the normal and the generalized normal model considered before (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>). The small and insignificant difference of the model probability (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>, wgt vs. kde) indicates that KDE predicts on a similar level as the weighting model even though the latter has more adaptable parameters and thus may be considered more flexible. The weighting model does not explicitly construct a probability density but can be viewed as a functional approximation that can capture similar dependencies of behavior on the sample.</p>
<p>In summary, participants do not sufficiently exploit the structural constraints suggested by the task but instead give more freedom to the specific instances of the observations to determine their responses. The tendency to assume that even small samples are representative of the population could be well captured by nonparametric kernel density estimation.</p>
</sec>
<sec id="sec008">
<title>Inferred representations feature overlapping and redundant kernels</title>
<p>Probability distributions over perceptual variables should be embedded in the context of more general knowledge of the task’s context. From a causal inference perspective, they should be attributed to the causal variables already known to exist. Treating all observations as if they originate from their own cause, i.e. as new causal variables, makes purely nonparametric methods seem of limited applicability in wider contexts. In this sense, KDE itself may be considered a heuristic approach as it largely ignores prior (structural) knowledge. Examining the inferred representations, we argue here that there is reason to believe that behavior is not purely nonparametric but can rather be conceived of as an instance-based modulation, or bias, to causal inference.</p>
<p>If we infer very narrow kernel functions for our participants that indicates that there is very little generalization from the sample. For close to orthogonal kernel functions with virtually no overlap (e.g. delta-distributions) the output reduces to a mere counting of observations. First, we tested how strong this instance-based bias is on the level of raw responses by comparing them to the predictions of <italic>δ</italic>-KDE (<xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7A</xref>). Both axes are normalized to the MLE, <italic>σ</italic><sub><italic>ML</italic></sub>, of the sample (i.e. the draws from the standard normal distribution, see <xref ref-type="sec" rid="sec011">Methods</xref>). All responses are plotted as a function of the <italic>δ</italic>-KDE output. Thus, by construction, predictions of <italic>δ</italic>-KDE (green) itself follow the unity line while predictions of inference using a Gaussian likelihood function follow a constant line of slope zero (red). Values of the optimal benchmark model would fluctuate because of varying prior beliefs that average to a constant independent of the sample given the MLE. The slope of a linear function fitted to the experimental responses is far from one as expected from <italic>δ</italic>-KDE (<xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7A</xref>, regression, median slope across participants 0.27, 95%-CI (0.24,0.38)). As opposed to the <italic>δ</italic>-KDE model, the KDE model (cyan) can predict the behavioral pattern (black) well because its kernel width parameter takes large values (<xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7B</xref>, red) (median across participants 0.40, 95%-CI (0.35,0.59), in units of <italic>σ</italic><sub><italic>ML</italic></sub>). Participants capture a varying number of points with the response frame (<xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7A</xref>, inset) which is only possible if the constructed density is a non-local function of the specific sample configuration on the screen. This slope pattern is not entirely inconsistent with inference of a Gaussian likelihood function as responses actually vary around its value as a function of the sample configuration. On the contrary, the normal model reaches high predictive performance in absolute values as shown before. However, additional to the responses derived from Gaussian inference, there are subtle instance-based variations which can be captured by the KDE model. At the level of the responses, behavior may be understood as inference of a normal distribution that is modulated by KDE.</p>
<fig id="pcbi.1006205.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Strong generalization is consistent with the possibility of integrating prior knowledge about the task structure.</title>
<p><bold>(A)</bold> Responses (black) show higher consistency with inference of a single Gaussian than with approaches generalizing only weakly beyond the sample such as <italic>δ</italic>-KDE (limit of vanishing kernel widths; third most excentric sample point). The plot shows aggregated (median across participants, 95% CI) bin medians of the responses (normalized by <italic>σ</italic><sub><italic>ML</italic></sub>) and the fitted KDE model (cyan) as a function of the <italic>δ</italic>-KDE output (approximately equally filled bins). By construction, inference of a Gaussian results in a horizontal line (red) while <italic>δ</italic>-KDE (green) yields a linear function of slope one. The experimental curves are less steep indicating a rather moderate instance-based modulation compared to a Gaussian model. The inset is a zoomed-out version additionally showing the relationship of the responses to the distribution of sample points (median of absolute value within each bin). <bold>(B)</bold> The KDE model infers internal distributions that are smoothed and spatially extended around the sample points. The mean probability density function across participants (black, 95% CI) is shown for four different samples (blue circles). The inferred density is smooth featuring fewer modes than the number of basis distributions (red curves). This is a consequence of the large fitted Gaussian kernel widths which lead to substantial overlap of the basis distributions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.g007" xlink:type="simple"/>
</fig>
<p>Interestingly, KDE predicts behavior significantly better than the tiling model (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>). The main difference is that the tiling model relies on spatially confined basis functions while Gaussian kernels are spatially extended. The weighting pattern shows that the tiling model (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>, green) overweighs the third most excentric point even more than behavior (red). The tiling model too closely resembles the purely instance-based approach of <italic>δ</italic>-KDE while behavior is not so strongly influenced by the third most excentric point. Of all models tested KDE (blue) best captures the weighting pattern (<xref ref-type="fig" rid="pcbi.1006205.g005">Fig 5B</xref>) because the large kernel width exhibits a non-local effect so that the positions of all points influence judgments leading to a more balanced pattern.</p>
<p>A large kernel width makes spatially extended basis distributions overlap (<xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7B</xref>, red). Accordingly, we typically find fewer than four modes in the inferred densities of the participants (median of the per participant mean across trials 2.0375,95% <italic>CI</italic> = (1.50,2.27)). Thus, increasing the kernel width may be understood as a reduction of the effective number of components in the mixture distribution as measured by the number of modes (Pearson correlation coefficient, <italic>ρ</italic> = −0.95, <italic>p</italic> = 6.01 ⋅ 10<sup>−11</sup>). Our data requires the KDE model to perform close to a regime where it must approximate inference of some smooth distribution which is closer to unimodal. Despite being the best approximation explored, it is nevertheless possible that the inference method used by our participants is structurally more constrained than KDE and uses some prior knowledge of the task structure.</p>
<p>From a representational point of view, the large overlap of the basis distributions (<xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7B</xref>, red) is a rather redundant and thus inefficient way of representing the whole distribution. For a large degree of overlap, several kernel functions could be well represented by a single kernel function whose free parameters are tuned to accommodate all their contributions. Bayesian nonparametric mixture models [<xref ref-type="bibr" rid="pcbi.1006205.ref027">27</xref>] can effectively reduce the number of redundant mixture components and minimize shared responsibility to account for the data points. The number of components can adapt to the position and number of data points in the sample. It gives less freedom to the data than KDE but implements soft and gradual constraints towards sparsity. A preference for sparser or denser representations can be specified by a prior. Likewise, prior knowledge such as a zero-centered population may be included in this way. We suggest this as a connection to theoretical principles.</p>
<p>We found that participants show different preferences for instance-based generalization. The average number of modes of the inferred densities according to the KDE-model almost covers the full range of possible values (minimum 1.01, median 2.04, maximum 3.63, across participants). Even with wide kernels, KDE is limited in its ability to represent unimodal near-Gaussian distributions. Correspondingly, the difference in predictive performance (CVLL) between the KDE and the normal model is larger for smaller kernel widths (linear correlation coefficient, <italic>ρ</italic> = −0.54,<italic>p</italic> = 0.0075). Consistent with previous results, the slope in <xref ref-type="fig" rid="pcbi.1006205.g007">Fig 7A</xref> decreases with the kernel width (Pearson correlation coefficient, <italic>ρ</italic> = −0.66, <italic>p</italic> = 7.30 ⋅ 10<sup>−4</sup>). The determinants of the participants' preferences are unclear from this experiment. We remark however, that participants who infer more redundant densities tend to respond faster (Spearman correlation coefficient, <italic>ρ</italic> = 0.35,<italic>p</italic> = 0.064) although the result does not reach significance.</p>
<p>In summary, using KDE we found very wide overlapping kernels leading to densities which could be more sparsely represented. This hints at a more sophisticated inference approach than pure instance-based generalization. It may be considered a modulation of causal inference by a kernel-based approach. We suggest a connection to Bayesian nonparametric methods in statistics that allow to incorporate prior knowledge and sparsity constraints.</p>
</sec>
<sec id="sec009">
<title>Explanation close to ceiling level</title>
<p>There are many possible ways in which this task might be approached by our participants. Thus, we attempt to estimate an upper bound of the predictable structure in the data regardless of how the task was solved by the participant. Gaussian process regression (GPR) is used to find a low-bias functional approximation between input <bold><italic>d</italic></bold> and behavior <italic>y</italic>. Hence, if a model reaches similar predictive levels, this is indication that it captures the most relevant computational operations. GPR is indeed found to be the best model (<xref ref-type="fig" rid="pcbi.1006205.g006">Fig 6</xref>) on the group level. However, the differences to the KDE-model are not disconcertingly large (median CVLL difference across participants, 13.3 dHart, 95%-CI, (−3.1, 23.5) dHart). Overall, KDE can predict on a comparable level as GPR. This is remarkable as for interpretable models, all factors need to be specified explicitly. For instance, even motor related variations with <bold><italic>d</italic></bold> would have to be incorporated. Moreover, as probability densities are high dimensional and subjective, the achieved match is not trivial.</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>This study attempted to elucidate how sensory representations of uncertainty are constructed from sparse data. We have described a new experimental task that allows us to measure quantitative judgments of uncertainty in response to a noisy stimulus with high precision. We find that (1) participants give faithful judgments about uncertainty on a trial-by-trial basis which are irreducible to simple heuristics. (2) Their behavior is not in agreement with the structural assumptions of a Gaussian suggested by the framing of the task. Instead, according to their behavior, participants are biased to judge the sample as representative of the population and that random fluctuations in the sample will reproduce in the long run. A connection to Bayesian nonparametric models is suggested to model this inclination towards instance-based generalization. (3) Furthermore, behavior is consistent with the idea that participants internally represent the variable of interest probabilistically as a normalized distribution over its possible values.</p>
<p>The idea that perception constitutes some form of (probabilistic) inference process was suggested long ago [<xref ref-type="bibr" rid="pcbi.1006205.ref028">28</xref>]. It has a particular appeal for deriving subjective estimates of uncertainty as it emerges naturally from the knowledge representation itself, i.e. from the posterior distribution, without requiring a meta-representation [<xref ref-type="bibr" rid="pcbi.1006205.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1006205.ref031">31</xref>].</p>
<p>Experimentally, one must elicit the read-out of a suitable summary statistic of the sensory representation. In previous work, participants are typically asked to report their confidence that the latent variable to be inferred lies beyond some fixed decision boundary [<xref ref-type="bibr" rid="pcbi.1006205.ref032">32</xref>]. Instead, we allowed participants to freely estimate the dispersion of the inferred density. There is virtually no demand on working memory and participants do not need to resort to language to perform the task. Both aspects are believed to be critical for promoting rational behavior [<xref ref-type="bibr" rid="pcbi.1006205.ref033">33</xref>]. In addition to being intuitive, this task requires an ability to deal with uncertainty to construct an internal objective on a trial-by-trial level regarding the target percentage.</p>
<p>Critically, this task was designed to minimize sensory and motor noise to obtain a sensitive probe of behavioral variations of dispersion estimates. As opposed to prior work, e.g. using the random dot motion stimulus [<xref ref-type="bibr" rid="pcbi.1006205.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref035">35</xref>], here mainly the task-relevant stimulus dimensions drive behavior. This study more specifically investigates the process of density estimation that is embedded in other (hierarchical) tasks. Previously, several studies tested how multiple inferred sensory representations are combined. The reliability based weighting of conflicting cues from different modalities suggests that distributional estimates are provided by each modality [<xref ref-type="bibr" rid="pcbi.1006205.ref007">7</xref>]. Another study also supplied evidence by means of a dot cloud [<xref ref-type="bibr" rid="pcbi.1006205.ref004">4</xref>] but assumed that participants know that the observations are normally distributed when making inference. Many previous studies made the strong assumption that participants know the generative process of the task. Very often it is chosen to be a normal distribution [<xref ref-type="bibr" rid="pcbi.1006205.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref036">36</xref>]. It may be a reasonably good proxy to model cognitive processes for simple, nonlinear and low-dimensional stimulus tasks with abundant evidence. However, we challenge the adequacy for inference in complex environments or sparse observations. These assumptions evade the deeper question of choosing a suitable model that the agent faces. In hierarchical models and depending on context, the upper levels provide constraints as to what the important causal factors are. We framed the task by alluding to a commonly known random process of throwing darts conforming to prior structural assumptions of a centered, unimodal and bell-shaped distribution that is close to Gaussian.</p>
<p>Nevertheless, we find that most participants fall short of these assumptions but rather give systematically biased estimates. Because of the low number of samples, our task allows testing what inductive biases [<xref ref-type="bibr" rid="pcbi.1006205.ref037">37</xref>] participants exhibit. They appear to give more freedom to the model’s structure to adapt to the sample. Thus, their judgments seem to assume that fluctuations in the sample are representative of the population [<xref ref-type="bibr" rid="pcbi.1006205.ref038">38</xref>]. However, we found evidence that their inferences are somewhat more constrained than purely instance-based estimates leading to potentially sparser representations. We propose to view this in the framework of Bayesian nonparametric mixture models [<xref ref-type="bibr" rid="pcbi.1006205.ref027">27</xref>,<xref ref-type="bibr" rid="pcbi.1006205.ref039">39</xref>] which may infer the appropriate complexity for each sample based on a prior expressing a preference for the sparsity of the final estimate (the number of components). In this context, the bias towards instanced-based generalization can be considered a prior that favors more complex solutions. This is reminiscent of findings in the literature where human abilities to learn functions are described by a hybrid of instance-based, nonparametric and rule-based, parametric approaches [<xref ref-type="bibr" rid="pcbi.1006205.ref010">10</xref>]. We believe that these ideas merit further exploration and extension to more complex causal structures and tests with different sample sizes. For a simple, monocausal generative model, as in our case, we would expect that the number of components of the internal representations becomes sparser as more data is provided, because there are no more features to be captured. Furthermore, it is intriguing to ask if a similar probabilistic inference perspective may be helpful to explain the decreased reliance on outlying evidence in a decision task between two stimulus categories [<xref ref-type="bibr" rid="pcbi.1006205.ref040">40</xref>]. This was originally attributed to robust estimation which may be seen as inference of a mixture distribution in which additional components are used to explain observations that are far too outlying to be considered part of the main process. Correspondingly, their contribution to an estimate of the main process would be reduced.</p>
<p>We can only speculate about the reasons behind this inductive bias. First, it might be due to considering the cost of computing [<xref ref-type="bibr" rid="pcbi.1006205.ref041">41</xref>] in an attempt to simplify judgments. However, we found a tendency towards more complex representations whereas sparser representations are typically believed to be more economical. For example, decomposing high-dimensional objects such as continuous probability density functions of human visuo-motor errors into simple non-overlapping (uniform) basis distributions was suggested to be a solution to complexity by obtaining a sparser representation [<xref ref-type="bibr" rid="pcbi.1006205.ref006">6</xref>]. Instead, we speculate that the bias towards instanced-based generalization might be related to structural uncertainty about the causes of their observations. Structural uncertainty has been shown to lead to model-free learning [<xref ref-type="bibr" rid="pcbi.1006205.ref042">42</xref>]. Similarly, a sensitivity to small alterations in the task setting has been found to affect optimality of behavior [<xref ref-type="bibr" rid="pcbi.1006205.ref043">43</xref>]. Furthermore, we might be equipped with a more fundamental bias to perceive causes behind patterns even for little evidence [<xref ref-type="bibr" rid="pcbi.1006205.ref044">44</xref>].</p>
<p>By construction, our task objective only applies to a normalized distribution over future outcomes regardless of its functional shape. Various studies have claimed that internal processing is probabilistic or at least demonstrated a “lower bound for the sophistication of confidence evaluation” [<xref ref-type="bibr" rid="pcbi.1006205.ref045">45</xref>]. Typical approaches derive an optimal solution to the task and show that behavior is reasonably close to it. However, strong claims require preconditions [<xref ref-type="bibr" rid="pcbi.1006205.ref046">46</xref>] such as testing alternative models [<xref ref-type="bibr" rid="pcbi.1006205.ref047">47</xref>] for non-trivial optimal processing. We do not claim optimal processing but emphasize systematic deviations that nevertheless might originate from internal probabilistic computations. Often as in our case, a clearly suboptimal strategy yields near-optimal results.</p>
<p>In fact, instead of a trial-by-trial objective for the target percentage derived from a density estimate, a learnt stimulus-responses mapping might be used instead. Our task design minimized the possibility to optimize a reward measure through trial-and-error over trials by omitting informative feedback. Consequently, the chances of acquiring a stimulus-response mapping are minimized. Furthermore, simple heuristic approximations [<xref ref-type="bibr" rid="pcbi.1006205.ref048">48</xref>] to behavior have been ruled out explicitly. Additionally, we found that the implementation of instance-based generalization by KDE is within reasonable bounds of an estimate of the predictable structure in behavior [<xref ref-type="bibr" rid="pcbi.1006205.ref046">46</xref>] suggesting that we have captured the important computations.</p>
<p>Ultimately, the degree to which claims to probabilistic processing seem substantiated depends on the propensity to belief that the task could alternatively be solved by a well-tuned mapping or heuristic estimator acquired prior to the experiment. This task is rather artificial, and humans are seldom prompted to state or give error intervals in terms of percentages. Accordingly, the situations to learn from are sparse. Uncertainty about (latent) variables is rarely made explicit, especially in numerical terms, but rather implicitly used by the agent to integrate and update beliefs. Generally, there is little information about the frequency with which events happen in our world across instances of the same situation. Even though learning calibrated mappings from specific situations is in principle possible, it is highly uneconomical and thus regarded unlikely. Likewise, it seems unrealistic that evolutionary training across generations has provided us with well-tuned heuristics for specific situations such as this task. After all, we deem it more plausible to assume that most participants estimated some (approximate) probabilistic distribution to derive their judgments.</p>
<p>In conclusion, our results suggest that human judgments about uncertainty are guided by an internal probabilistic objective. However, there is a tendency to identify fluctuations in the sample as representative for judgments about the population. This may be captured by a representation endowed with a preference to adapt overly flexibly to the observed instances.</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec012">
<title>Ethics statement</title>
<p>Comité Ético de Investigación Clínica, Parc de Salut MAR, Barcelona Spain, 2013/5464/I, titulado “Del laboratorio a la calle: El impacto de la integración multisensorial en la vida cotidiana”. Written informed consent was obtained from all participants.</p>
</sec>
<sec id="sec013">
<title>Sampling scheme to generate observations</title>
<p>On each of the 320 trials, the horizontal positions of the points with respect to the center were generated as follows (<xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1C</xref>). First, always <italic>N</italic> = 4 sample values <bold><italic>r</italic></bold> = (<italic>r</italic><sub>1</sub>,…,<italic>r</italic><sub>4</sub>) are independently drawn from a standard normal distribution <italic>r</italic><sub><italic>n</italic></sub> ~ <italic>N</italic>(0,1). Second, the samples were scaled by the factor <italic>ν</italic>/<italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>r</italic></bold>), where <inline-formula id="pcbi.1006205.e013"><alternatives><graphic id="pcbi.1006205.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula> is the maximum likelihood estimator (MLE) for a normal distribution centered at zero of the samples <bold><italic>r</italic></bold> and <italic>ν</italic> is drawn from a uniform probability distribution over the range of [10,140] pixels. The scaled sample <bold><italic>d</italic></bold> = <italic>ν</italic>/<italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>r</italic></bold>) ⋅ <bold><italic>r</italic></bold> always has a MLE given by <inline-formula id="pcbi.1006205.e014"><alternatives><graphic id="pcbi.1006205.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:mi>ν</mml:mi></mml:math></alternatives></inline-formula>. This method allows choosing any desired value of <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) by setting <italic>ν</italic> correspondingly. Setting <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) directly, which is the main determinant for inference, has the advantage that observations <bold><italic>d</italic></bold> and the MLE <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) take less extreme values which translates into increased numerical stability for model comparison. Defining an explicit latent <italic>σ</italic>-variable over a finite range instead would have led to a long-tailed <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) distribution with undesirable properties (s. <xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1D</xref>). The ability to tell apart models with similar predictions is enhanced if response noise and outlying conditions are kept at a minimum.</p>
<p>However, because of this way of generating the dots, the optimal inference model with respect to the actual generative model in the environment is not readily defined. Nevertheless, participants do not know these alterations to how the dots were generated. The best they can do is to follow the instructions and their prior knowledge suggested by the dart metaphor to explain the data. We do not define the optimal model with respect to the generative model in the environment. Instead, we define it as an optimal inference strategy based on a normal distribution whose width varies parametrically across trials. It follows the inference strategy of Eqs (<xref ref-type="disp-formula" rid="pcbi.1006205.e004">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006205.e005">2</xref>) and assumes a uniform prior over the range of [0,140] pixels. As this prior arguably matches the task instructions it was chosen as a basis for our Bayesian benchmark model and the feedback in the experiment.</p>
</sec>
<sec id="sec014">
<title>Participants and experimental procedure</title>
<p>In total 23 participants (15 female, 8 male) were recruited mainly among students from the Pompeu Fabra University in Barcelona. We accepted all healthy adults with normal or corrected to normal vision. We obtained written confirmation of informed consent to the conditions and the payment modalities of the task. The training and the experimental session were carried out on a single appointment that nominally lasted 75 min. First, participants read detailed written instructions of the task. In a brief training session, they were given 40 trials to familiarize with the handling of the task through a short interactive session with feedback after every trial. The feedback consisted of the actual percentage <italic>c</italic><sub><italic>t</italic></sub> (using Eqs <xref ref-type="disp-formula" rid="pcbi.1006205.e004">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1006205.e006">3</xref>) they would have captured in trial <italic>t</italic> according their response <italic>y</italic><sub><italic>t</italic></sub> and our benchmark model. In addition, they were given a deviation score (mean squared error (MSE)) from the target percentage <italic>δ</italic><sub><italic>t</italic></sub> = (<italic>c</italic><sub><italic>t</italic></sub> − 0.65)<sup>2</sup> ⋅ 1000.</p>
<p>In principle, a subject could learn how a pair consisting of observations <bold><italic>d</italic></bold> together with his response <italic>y</italic>, (<bold><italic>d</italic></bold>,<italic>y</italic>), relates to the capture probability <italic>p</italic> from experience in the 40 training trials. For a given learned mapping (<bold><italic>d</italic></bold>,<italic>y</italic>) → <italic>p</italic> he would have to adjust <italic>y</italic> such that <italic>p</italic> = 0.65. We regard this as unlikely for the following reasons. First, 40 trials do not provide a lot of data to learn from. Second, the mapping is high-dimensional and nonlinear which makes it hard to learn and susceptible to the specific instantiations of <bold><italic>d</italic></bold> across trials–as well as the choice of <italic>y</italic>. (<bold><italic>d</italic></bold>,<italic>y</italic>) and <italic>p</italic> are never simultaneously visible on the screen. And finally, batch learning requires memorizing all presented pairs which seems infeasible for participants. While on-line learning is possible, it typically suffers from slower convergence rates.</p>
<p>Participants could ask any questions to the experimenter prior to the experiment. The subsequent experimental session consisted of 320 trials with pauses together with feedback after every 5 trials. In the experiment, the feedback consisted of 5-trial averages of the quantities <italic>c</italic><sub><italic>t</italic></sub> and <italic>δ</italic><sub><italic>t</italic></sub> above that were computed since the last pause. Participants were supposed to minimize the deviation score and were payed more compensation when having a smaller deviation score to incentivize optimization. This supposedly promoted high motivation to prevent participants from resorting to computationally cheaper heuristic shortcuts. The task circumvents risk aversion since there is practically nothing that the participant can do to prevent losses other than stating the response as accurately as possible.</p>
<p>The bonus payment was determined by the mean of their final deviation score after removing the eight worst trials. The payment was determined by comparison to an array of five thresholds that were set according to the {0.1,0.2,0.3,0.4,0.5} cumulative quantiles of the empirical deviation score distribution across prior participants. A lower score corresponds to a better performance so that participants were payed an additional bonus of {5,4,3,2,1} € if their final deviation score was less or equal to the quantile thresholds. This is a relative way of rewarding their efforts to optimize their responses. Irrespective of their performance they were paid 10 € and hence on average received 11,50 € per session. The experiment was carried out with 23 participants. Later we excluded three of them because their behavior had little dependence on the stimulus.</p>
<p>The task was presented with Matlab Psychtoolbox 3.0.12. Participants made input with an USB-mouse that allowed them to precisely adjust the width of the response frame and confirm it with a click. Immediately after trial onset, they were presented with the dots and could start to expand/shrink the frame from a random initial width by moving the mouse up/down-wards. The points were visible throughout the entire time until the participant confirmed his response with a click. The program then either proceeded to the next trial or to the feedback/pause screen that indicates the averages over the five last trials of the percentage the participant would have captured as well as the numerical deviation score. In addition, information about the how many of all trials have already been completed was presented. The participant could proceed at his own pace.</p>
</sec>
<sec id="sec015">
<title>Computational models</title>
<p>We attempt to examine whether the behavior of the participants can be described by inference of probability distributions. More specifically, we attempt to infer whether their internal structural assumptions correspond to unimodal near-Gaussian distributions (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2A</xref>) or might be better described by instance-based, nonparametric approaches (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2B–2D</xref>) such as kernel density estimation. In addition, we checked whether selected heuristics can also account for the behavioral data.</p>
<sec id="sec016">
<title>Response mapping accounts for nuisance factors</title>
<p>Behavior is influenced by various factors and subjective assumptions of the participant which are difficult to model explicitly. Among these are subjective prior knowledge and probability distortion. Even for a probabilistic agent there exists some mathematical freedom as to what prior distribution over the latent variables to use. We did not explicitly include prior knowledge into our models but instead endowed the model with flexibility to approximately account for such effects.</p>
<p>We make use of the fact that ultimately, behavior such as the one derived from a probabilistic inference model just amounts to a specific mapping <inline-formula id="pcbi.1006205.e015"><alternatives><graphic id="pcbi.1006205.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> from inputs onto the response <inline-formula id="pcbi.1006205.e016"><alternatives><graphic id="pcbi.1006205.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. Generally, for probabilistic models the mapping <inline-formula id="pcbi.1006205.e017"><alternatives><graphic id="pcbi.1006205.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> can be written in two steps. (i) Computing the sufficient statistic <inline-formula id="pcbi.1006205.e018"><alternatives><graphic id="pcbi.1006205.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> which is then (ii) mapped onto the response, <inline-formula id="pcbi.1006205.e019"><alternatives><graphic id="pcbi.1006205.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, such as <inline-formula id="pcbi.1006205.e020"><alternatives><graphic id="pcbi.1006205.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> for the Gaussian. We use <inline-formula id="pcbi.1006205.e021"><alternatives><graphic id="pcbi.1006205.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> to refer to any dispersion estimate and call <inline-formula id="pcbi.1006205.e022"><alternatives><graphic id="pcbi.1006205.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> the response mapping. For non-probabilistic estimators, it just allows for additional tuning of the dispersion estimate. The introduction of the response mapping permits the construction of computationally simple models that may accommodate subjective knowledge of latent variables like <italic>σ</italic> in the second step.</p>
<p>This is illustrated in <xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3A</xref> for the theoretical response curves (red, green). For maximum likelihood estimation (MLE) the response (red) is nothing but a linear mapping of the sufficient statistic <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) onto its output <inline-formula id="pcbi.1006205.e023"><alternatives><graphic id="pcbi.1006205.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. The Bayesian benchmark model (green) also takes the sample size <italic>N</italic> = 4 and a uniform prior distribution over <italic>σ</italic> into account. Compared to MLE, its main effect is a bias of the responses towards intermediate values. The effect of a different prior on <italic>σ</italic> would merely manifest as a somewhat different mapping onto the response because <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) and <italic>N</italic> are sufficient statistics for <italic>σ</italic>. In other words, the model will produce the same results even when input <bold><italic>d</italic></bold> changes as long as the sufficient statistics remain the same. They compactly sum up all the information that is to be known about the hidden variables of a probabilistic model from the sample <bold><italic>d</italic></bold>. Hence, distributions such as the posterior <italic>p</italic>(<italic>σ</italic>|<bold><italic>d</italic></bold>) or the prior <italic>p</italic>(<italic>σ</italic>) do not have to be explicitly represented in our model. Instead they are implicitly considered through the effects they exert on the response by allowing for additional freedom through a mapping. Apart from that, the mapping <inline-formula id="pcbi.1006205.e024"><alternatives><graphic id="pcbi.1006205.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> also depends on the target percentage that the model is required to capture. A larger target percentage leads to a larger dependence on <italic>σ</italic><sub><italic>ML</italic></sub>(<bold><italic>d</italic></bold>) and would e.g. manifest as a larger slope of the ML response (<xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3A</xref>, red). The model may however account for the fact that participants suffer from probability distortion such that their internal target probability does not exactly match the one of a probabilistic agent (<xref ref-type="disp-formula" rid="pcbi.1006205.e007">Eq 4</xref>).</p>
<p>The response mapping from the dispersion estimate to the response, <inline-formula id="pcbi.1006205.e025"><alternatives><graphic id="pcbi.1006205.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, is chosen to be the same for all models and is intended to be flexible enough to account for these implicit effects. Empirically we found that a quadratic polynomial is only minimally better than a linear mapping (using the weighting-model, below). The improvements on the group level are significant (increased median cross-validation log likelihood (CVLL) across participants, Wilcoxon sign rank test, <italic>p</italic> = 0.0027) but small in absolute terms (median CVLL difference 3.66 dHart, 95% CI (0.34, 7.15) dHart, below). For this weak nonlinearity and to obtain a sparse model formulation, we consider a polynomial of first order to be a sufficiently good approximation to represent the response mapping.</p>
<disp-formula id="pcbi.1006205.e026">
<alternatives>
<graphic id="pcbi.1006205.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
<p>The models that we consider differ only in how they compute the dispersion measure <inline-formula id="pcbi.1006205.e027"><alternatives><graphic id="pcbi.1006205.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. They may introduce additional parameters which are detailed below. We start by describing approximative models that do not make use of distributions first. We will explicitly consider heuristic models. In general, heuristics are not linked to optimal responses in a principled way but nevertheless might yield satisfactory results. Every estimator that correlates with <italic>σ</italic><sub><italic>ML</italic></sub> contains some useful information about the dispersion and may thus be used. As heuristics are frequently associated with less effortful processing, we consider simple and visually salient quantities that may be readily assessed by the participants. As another approximate model, we test a weighting model that emphasizes certain stimulus features. We will then describe probabilistic models that derive responses from different distributional estimates and conclude with a predictive model intended to serve as an estimator for the upper bound on predictability given our data.</p>
</sec>
<sec id="sec017">
<title>Maximum model</title>
<p>This model uses the distance of the point that is farthest away from the center, that is, <inline-formula id="pcbi.1006205.e028"><alternatives><graphic id="pcbi.1006205.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. This function can be considered a simple heuristic approach because it reduces the input information to be processed, but as this distance strongly correlates with <italic>σ</italic><sub><italic>ML</italic></sub> it is expected to be predictive of behavior.</p>
</sec>
<sec id="sec018">
<title>Range model</title>
<p>This model uses an estimate of dispersion based on the difference between the leftmost and rightmost point <inline-formula id="pcbi.1006205.e029"><alternatives><graphic id="pcbi.1006205.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">min</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. Again, this quantity is correlated with <italic>σ</italic><sub><italic>ML</italic></sub>.</p>
</sec>
<sec id="sec019">
<title>Weighting model</title>
<p>The maximum likelihood estimator <bold><italic>σ</italic><sub><italic>ML</italic></sub></bold> can be generalized in that it assigns different weights to individual points when calculating the root mean square deviation. The observations <bold><italic>d</italic></bold> are indexed according to their excentricity, i.e. their absolute deviation from zero such that |<italic>d</italic><sub><italic>n</italic></sub>| ≥ |<italic>d</italic><sub><italic>m</italic></sub>| for <italic>n</italic> &gt; <italic>m</italic>.
<disp-formula id="pcbi.1006205.e030">
<alternatives>
<graphic id="pcbi.1006205.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
The parameter <italic>β</italic><sub>1</sub> of the response mapping <inline-formula id="pcbi.1006205.e031"><alternatives><graphic id="pcbi.1006205.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1006205.e026">Eq 5</xref>) is factored into the <italic>ω</italic><sub><italic>n</italic></sub> and set to one to avoid under-constrained solutions for regression. We may enforce the summation constraint, ∑<sub><italic>n</italic></sub><italic>ω</italic><sub><italic>n</italic></sub> = <italic>N</italic>, on the weights after fitting to interpret the weights as relative contributions with respect to the case of <italic>ω</italic><sub><italic>n</italic></sub> = 1, which corresponds to inference of a Gaussian. This can be done by factoring out a term <inline-formula id="pcbi.1006205.e032"><alternatives><graphic id="pcbi.1006205.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msqrt><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula> which can be formally assigned to <italic>β</italic><sub>1</sub>. We consider the equal weighting of the square of each point’s position <inline-formula id="pcbi.1006205.e033"><alternatives><graphic id="pcbi.1006205.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:math></alternatives></inline-formula> a non-trivial pattern of inference of a normal distribution.</p>
<p>Within this model, we also test the heuristic of considering just one out of all <italic>n</italic> = 1,…,<italic>N</italic> points, <inline-formula id="pcbi.1006205.e034"><alternatives><graphic id="pcbi.1006205.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:math></alternatives></inline-formula>. In this case, just one of the four weights should be four while the others will become zero due to the summation constraint. The task is constructed such that the position of the third most excentric point closely corresponds to the target percentage. Yet, we found that this heuristic is evidently exploited by just one participant (normalized <italic>ω</italic><sub>3</sub>′ = 0.95, <italic>d</italic><sub>3</sub> almost explains full variance, <italic>R</italic><sup>2</sup> = 0.96).</p>
<p>Because of the generality and the computational ease with which optimization can be performed for this model, we use it to test variants of the response mapping <xref ref-type="disp-formula" rid="pcbi.1006205.e026">Eq 5</xref>. We test whether participants behave in accordance to a prior belief about the range of dispersions across trials. A pure ML approach ignores prior knowledge and leads to responses proportional to the dispersion estimate <inline-formula id="pcbi.1006205.e035"><alternatives><graphic id="pcbi.1006205.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3A</xref>, red). If that was sufficient to predict behavior, a model whose output is restricted to be proportional to the dispersion estimate (omitting constant term in <xref ref-type="disp-formula" rid="pcbi.1006205.e026">Eq 5</xref>) should perform equally well.
<disp-formula id="pcbi.1006205.e036">
<alternatives>
<graphic id="pcbi.1006205.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e036" xlink:type="simple"/>
<mml:math display="block" id="M36">
<mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
Likewise, a model which additionally features a quadratic term <inline-formula id="pcbi.1006205.e037"><alternatives><graphic id="pcbi.1006205.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is used to test for the nonlinearity of the response mapping. The weighting model is chosen for these tests as it can flexibly account for other systematic biases in behavior that are not related to prior knowledge.</p>
</sec>
<sec id="sec020">
<title>Normal model</title>
<p>Making inference using a normal distribution is equivalent to the mapping <inline-formula id="pcbi.1006205.e038"><alternatives><graphic id="pcbi.1006205.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> in which <inline-formula id="pcbi.1006205.e039"><alternatives><graphic id="pcbi.1006205.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the sufficient statistic and the MLE of the Gaussian. To match the responses of our benchmark model, the response mapping <inline-formula id="pcbi.1006205.e040"><alternatives><graphic id="pcbi.1006205.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> must equal the green curve in <xref ref-type="fig" rid="pcbi.1006205.g003">Fig 3A</xref>. The chosen response mapping for regression (<xref ref-type="disp-formula" rid="pcbi.1006205.e026">Eq 5</xref>) can only provide a linear approximation to this curve but was chosen based on considerations regarding model sparsity and the empirical evidence to be sufficient to capture behavior.</p>
</sec>
<sec id="sec021">
<title>Generalized normal model</title>
<p>The dart metaphor and the task instructions suggest that the distribution of darts follows some symmetric and bell-shaped curve centered at zero. As a perfect match between the true and assumed distributions by the participants is not expected, we consider a generalized normal distribution which has an additional shape parameter <italic>p</italic> &gt; 0 so that it can represent a family of distributions.
<disp-formula id="pcbi.1006205.e041">
<alternatives>
<graphic id="pcbi.1006205.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e041" xlink:type="simple"/>
<mml:math display="block" id="M41">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>α</mml:mi><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mo>/</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
It effectively generalizes the exponent of the normal distribution for which it takes a value of <italic>p</italic> = 2. For small <italic>p</italic>, the distribution is more peaked whereas it approximates a plateau like distribution for larger values (<xref ref-type="fig" rid="pcbi.1006205.g001">Fig 1B</xref>). We assume that the exponent parameter <italic>p</italic> is constant across trials and treat it as an additional fitting parameter. For a known mean of zero, μ = 0, the maximum likelihood estimator for <italic>α</italic> is <inline-formula id="pcbi.1006205.e042"><alternatives><graphic id="pcbi.1006205.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mroot><mml:mrow><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mroot></mml:math></alternatives></inline-formula> which we identify with the dispersion estimate <inline-formula id="pcbi.1006205.e043"><alternatives><graphic id="pcbi.1006205.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. In the limit of <italic>p</italic> → ∞ it corresponds to the heuristic maximum model above. We also tested a generalized normal model which infers <italic>μ</italic> on a trial-by-trial basis for a given exponent <italic>p</italic> to test whether dropping the assumption of a centered distribution can better explain behavior. In this case, <xref ref-type="disp-formula" rid="pcbi.1006205.e007">Eq 4</xref> is explicitly solved, and its result is assigned to <inline-formula id="pcbi.1006205.e044"><alternatives><graphic id="pcbi.1006205.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. As it was found to be worse than the centered normalized distribution on the group-level (exceedance probability <italic>p</italic><sub><italic>exc</italic></sub> &gt; 0.999), we chose to only report results using a centered distribution.</p>
</sec>
<sec id="sec022">
<title>Gaussian kernel density estimation model</title>
<p>If one imposes only minimal structural constraints, more freedom is given to the data to determine the inferred density. One may assume that even small samples represent the population well and that future observations will cluster around the already observed instances. One way to do so is to estimate <italic>p</italic>(<italic>x</italic>|<bold><italic>d</italic></bold>) over future events <italic>x</italic> based on a kernel method. It generalizes observed data points <italic>d</italic><sub><italic>n</italic></sub> by assigning probability density proportional to a kernel function <italic>k</italic>(<italic>x</italic>,<italic>d</italic><sub><italic>n</italic></sub>) to their vicinity and thus constitutes a data smoothing problem (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2D</xref>). For the whole training set <bold><italic>d</italic></bold>, kernel density estimation centers a kernel on each observation and sums up their contributions to determine <italic>p</italic>(<italic>x</italic>|<bold><italic>d</italic></bold>) as:
<disp-formula id="pcbi.1006205.e045">
<alternatives>
<graphic id="pcbi.1006205.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e045" xlink:type="simple"/>
<mml:math display="block" id="M45">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
It is a nonparametric method because it does not assume a certain parameterized family of probability distributions for <italic>p</italic>(<italic>x</italic>) apart from the kernel. The kernel function <italic>k</italic> typically decays with the distance between <italic>x</italic> and <italic>d</italic><sub><italic>n</italic></sub>. Here we assume that it has the shape of a normal distribution <italic>k</italic>(<italic>x</italic>|<italic>d</italic><sub><italic>n</italic></sub>,<italic>η</italic>) = <italic>N</italic>(<italic>x</italic>|<italic>d</italic><sub><italic>n</italic></sub>,<italic>η</italic>). The kernel width <italic>η</italic> = <italic>η</italic>(<bold><italic>d</italic></bold>) is in principle a free parameter but needs to be sensibly chosen with respect to the dispersion of the data. Manual testing revealed that <italic>η</italic> = <italic>a</italic> ⋅ (<italic>d</italic><sub>3</sub> + <italic>d</italic><sub>4</sub>)/2 is a reasonably good approximation to the unknown <italic>η</italic>(<bold><italic>d</italic></bold>) function. Thus, potentially even better performance might be achievable than the one reported here. The model’s dispersion estimate, <inline-formula id="pcbi.1006205.e046"><alternatives><graphic id="pcbi.1006205.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, regarding the 65% capture probability is determined by inserting the inferred distribution <xref ref-type="disp-formula" rid="pcbi.1006205.e045">Eq 9</xref> into <xref ref-type="disp-formula" rid="pcbi.1006205.e006">Eq 3</xref> and then solving <xref ref-type="disp-formula" rid="pcbi.1006205.e007">Eq 4</xref>.</p>
<p>In the limit of vanishing kernel widths <italic>η</italic> → 0 (<italic>δ</italic>-distributions) the response for the target percentage of <italic>p</italic><sub><italic>t</italic></sub> = 0.65% converges to the third most excentric point. We refer to this approach as <italic>δ</italic>-KDE (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2C</xref>). In this limiting case, one would merely capture the target fraction <italic>p</italic><sub><italic>t</italic></sub> of observed points on the screen, thus replacing an estimation of the target fraction <italic>p</italic><sub><italic>t</italic></sub> of the population with a corresponding estimation of <italic>p</italic><sub><italic>t</italic></sub> on the sample.</p>
<p><bold>Tiling model.</bold> To capture a certain percentage of points of the sample, one must have some sort of quantile function that outputs the region containing the desired percentage. Explicit density models such as KDE entail a quantile function. A simple alternative is to construct some normalized histogram. We attempt to do so with the constraint that an observation point only exhibits a local effect on the constructed density (<xref ref-type="fig" rid="pcbi.1006205.g002">Fig 2B</xref>). Specifically, the contribution to the overall density of one data point only depends on its own position and on the position of its adjacent points.</p>
<p>More formally, this can be achieved by tiling the space between observations into rectangular, adjacent but non-overlapping basis distributions. We adhere to the additional constraint that the <italic>N</italic> ordered points correspond to the (0.5/<italic>N</italic>,1.5/<italic>N</italic>,…,(<italic>N</italic> − 0.5)/<italic>N</italic>) cumulative quantiles. Hence each basis distribution spanned between points has to be normalized by <italic>N</italic>. To assign the remaining probability 0.5/<italic>N</italic> below the lowest point <italic>d</italic><sub>1</sub> we use a uniform distribution U(<italic>d</italic><sub>1</sub> − <italic>d</italic><sub>2</sub>,<italic>d</italic><sub>1</sub>) whose support equals the distance to its only adjacent point <italic>d</italic><sub>2</sub> (and likewise for the largest point). Representations of probability densities based on orthogonal basis functions are suggested as a solution to tractably represent complex densities [<xref ref-type="bibr" rid="pcbi.1006205.ref006">6</xref>].</p>
<p><bold>Gaussian process regression.</bold> Gaussian Process Regression (GPR) [<xref ref-type="bibr" rid="pcbi.1006205.ref049">49</xref>] is used to estimate the upper bound on predictability of the participants’ behavior. It does not lend itself readily to an interpretation of how participants solve the problem on a given trial. It is however very flexible and successful in prediction by exploiting consistency between input <bold><italic>d</italic></bold> and output <italic>y</italic> across pairs of trials (<italic>i</italic>,<italic>j</italic>). We used GPR since it is a bias free estimator of the distribution <italic>p</italic>(<italic>y</italic>|<bold><italic>d</italic></bold>) which is assumed to be normally distributed with a constant intrinsic noise parameter <italic>σ</italic><sub><italic>I</italic></sub>. We chose a Gaussian kernel function
<disp-formula id="pcbi.1006205.e047">
<alternatives>
<graphic id="pcbi.1006205.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e047" xlink:type="simple"/>
<mml:math display="block" id="M47">
<mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>θ</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
that defines a scalar measure of similarity and the entries of the covariance matrix of the GP as <inline-formula id="pcbi.1006205.e048"><alternatives><graphic id="pcbi.1006205.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. Input pairs (<bold><italic>d</italic></bold><sub><italic>i</italic></sub>,<bold><italic>d</italic></bold><sub><italic>j</italic></sub>) that are considered similar in this sense should result in comparable responses (<italic>y</italic><sub><italic>i</italic></sub>,<italic>y</italic><sub><italic>j</italic></sub>) if the process <italic>p</italic>(<italic>y</italic>|<bold><italic>d</italic></bold>) is consistent. Prediction is more strongly influenced by those trials’ responses <italic>y</italic> for which (<bold><italic>d</italic></bold><sub><italic>i</italic></sub>,<bold><italic>d</italic></bold><sub><italic>j</italic></sub>) are similar. To make predictions for a new input <bold><italic>d</italic></bold><sub><italic>ν</italic></sub>, we evaluate the mean of the predictive distribution <inline-formula id="pcbi.1006205.e049"><alternatives><graphic id="pcbi.1006205.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi></mml:math></alternatives></inline-formula>. Here <bold><italic>k</italic></bold> has the entries <italic>k</italic>(<bold><italic>d</italic></bold><sub><italic>i</italic></sub>,<bold><italic>d</italic></bold><sub><italic>ν</italic></sub>) with <italic>i</italic> indexing all trials in the training data. Likewise, <italic>C</italic> and <bold><italic>y</italic></bold> are constructed from all the training data used to derive predictions. For each trial <bold><italic>d</italic></bold><sub><italic>t</italic></sub> = (<italic>d</italic><sub><italic>t</italic>1</sub>,…,<italic>d</italic><sub><italic>tN</italic></sub>), symmetry is exploited by sorting the points in ascending order of excentricity. To set the hyperparameters of the GP, (<italic>θ</italic>,<italic>σ</italic><sub>1</sub>,…,<italic>σ</italic><sub><italic>N</italic></sub>,<italic>σ</italic><sub><italic>I</italic></sub>), its generalization error is minimized. To do so, the mean of the test sets of <xref ref-type="disp-formula" rid="pcbi.1006205.e061">Eq 13</xref> of a 5-fold cross validation (CV) procedure is calculated. This procedure is part of training the GPR. We also attempted to predict behavior using a simple 1-hidden-layer feedforward neural network. Despite being a successful predictor, its performance was inferior to the GPR which is why we chose to only report the latter.</p>
<p><bold>Baseline model.</bold> The baseline model is chosen to provide a simple lower bound estimate for predictability that is independent of the trial-by-trial variations of the stimulus. This model calculates the mean of the responses of all its input <bold><italic>y</italic></bold><sub><italic>in</italic></sub> (training set). It thus makes the same prediction on every trial <italic>t</italic>.</p>
<disp-formula id="pcbi.1006205.e050">
<alternatives>
<graphic id="pcbi.1006205.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e050" xlink:type="simple"/>
<mml:math display="block" id="M50">
<mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
<p><bold>Inter-trial and feedback dependence.</bold> We investigated the influence of other quantities on behavior that participants might have (erroneously) utilized to guide their responses. To test for a dependence on the preceding trial, the estimator <inline-formula id="pcbi.1006205.e051"><alternatives><graphic id="pcbi.1006205.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is chosen to be the previously stated response.
<disp-formula id="pcbi.1006205.e052">
<alternatives>
<graphic id="pcbi.1006205.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e052" xlink:type="simple"/>
<mml:math display="block" id="M52">
<mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
There is a significant effect with respect to baseline (exceedance probability, <italic>p</italic><sub><italic>exc</italic></sub> &gt; 0.99), yet the effect on behavior is virtually negligible as the overall predictive performance is very low (median cross-validation log likelihood across participants −318 dHart, 95%-CI (−356,−300) dHart, with respect to the best model for each participant). The influence of the previously presented feedback about the capture percentage is similarly tested but its effect is found to be even weaker (−327 dHart, 95%-CI (−368,−312) dHart). Together with the evidence that participants did not adjust closer to the target capture percentage of the task (<xref ref-type="fig" rid="pcbi.1006205.g004">Fig 4B</xref>), we consider it unlikely that feedback affected behavior to a considerable extent.</p>
<p><bold>Overview of model parameters.</bold> The models used have a different number of parameters depending on the dispersion estimate <inline-formula id="pcbi.1006205.e053"><alternatives><graphic id="pcbi.1006205.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. The ones reported in the main text are summarized in <xref ref-type="table" rid="pcbi.1006205.t001">Table 1</xref>.</p>
<table-wrap id="pcbi.1006205.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006205.t001</object-id>
<label>Table 1</label> <caption><title>Overview of model parameters.</title></caption>
<alternatives>
<graphic id="pcbi.1006205.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Abbreviation</th>
<th align="center" colspan="6">Fitting parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><bold>Maximum</bold></td>
<td align="center">max</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center"><italic>β</italic><sub>1</sub></td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center"><bold>Range</bold></td>
<td align="center">rng</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center"><italic>β</italic><sub>1</sub></td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center"><bold>Weighting</bold></td>
<td align="center">wgt</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center">-</td>
<td align="center"><italic>w</italic><sub>1</sub></td>
<td align="center"><italic>w</italic><sub>2</sub></td>
<td align="center"><italic>w</italic><sub>3</sub></td>
<td align="center"><italic>w</italic><sub>4</sub></td>
</tr>
<tr>
<td align="center"><bold>Normal</bold></td>
<td align="center">nm</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center"><italic>β</italic><sub>1</sub></td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center"><bold>Generalized normal</bold></td>
<td align="center">gnm</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center"><italic>β</italic><sub>1</sub></td>
<td align="center"><italic>p</italic></td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center"><bold>Kernel density estimation</bold></td>
<td align="center">kde</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center"><italic>β</italic><sub>1</sub></td>
<td align="center"><italic>a</italic></td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center"><bold>Tiling</bold></td>
<td align="center">tlg</td>
<td align="center"><italic>β</italic><sub>0</sub></td>
<td align="center"><italic>β</italic><sub>1</sub></td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center"><bold>GPR</bold></td>
<td align="center">gpr</td>
<td align="center" colspan="6">Nonparametric, hyperparameters: (<italic>θ</italic>,<italic>σ</italic><sub>1</sub>,…,<italic>σ</italic><sub><italic>N</italic></sub>,<italic>σ</italic><sub><italic>I</italic></sub>)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p><bold>The response distribution.</bold> The probability of obtaining the response <italic>y</italic><sub><italic>t</italic></sub> on trial <italic>t</italic> conditional on the data <bold><italic>d</italic></bold><sub><italic>t</italic></sub> and the model parameters is assumed to be a mixture distribution of two contributions. The first and dominant term is a normal distribution centered on the model prediction <inline-formula id="pcbi.1006205.e054"><alternatives><graphic id="pcbi.1006205.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, modeling task-intrinsic noise around the estimates. Upon preliminary inspection of the data we found considerable heteroscedasticity with higher response variability for larger sample dispersions.</p>
<p>To take this feature of the response data into account, we assume that the standard deviation (SD), <italic>θ</italic>, of the distribution over response <italic>y</italic><sub><italic>t</italic></sub>, <inline-formula id="pcbi.1006205.e055"><alternatives><graphic id="pcbi.1006205.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, is a function of the model output <inline-formula id="pcbi.1006205.e056"><alternatives><graphic id="pcbi.1006205.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. The model output is denoted by <inline-formula id="pcbi.1006205.e057"><alternatives><graphic id="pcbi.1006205.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> to distinguish it from the response <italic>y</italic> of the participant which is formally represented by a draw from the response distribution to account for behavioral variability. Instead of assuming a parametric relationship and the need of further parameters to be fitted in the model, we make a parameter free estimate by assuming a discretized function, as follows. We divide the whole model output <inline-formula id="pcbi.1006205.e058"><alternatives><graphic id="pcbi.1006205.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> into <italic>Q</italic> equally filled quantiles <italic>q</italic> ∈ {1,…,<italic>Q</italic>} by assigning trial <italic>t</italic> to quantile <italic>q</italic><sub><italic>t</italic></sub>. For every quantile <italic>q</italic>, the SD is estimated separately by calculating <inline-formula id="pcbi.1006205.e059"><alternatives><graphic id="pcbi.1006205.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> (<italic>j</italic> = 1,…,<italic>J</italic> indexes trials belonging to quantile <italic>q</italic>). Hence, whenever there is heteroscedasticity, the true function <inline-formula id="pcbi.1006205.e060"><alternatives><graphic id="pcbi.1006205.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mi>θ</mml:mi><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> is approximated by the estimated bin values. For homoscedasticity all <italic>θ</italic><sub><italic>q</italic></sub> are the same and collapsing bins would make no difference. The resolution of the function is higher when many quantile divisions are used provided the <italic>θ</italic><sub><italic>q</italic></sub> can still be estimated faithfully. We consider <italic>Q</italic> = 5 a suitable choice for our problem.</p>
<p>As our data might be contaminated by processes other than dispersion estimation, such as lapses, we take precaution against far outlying responses. We calculate a trimmed standard deviation, i.e. before calculating <italic>θ</italic><sub><italic>q</italic></sub> we remove values below or above two interquartile ranges from the lower or upper quartile respectively. However, this applies to <italic>θ</italic><sub><italic>q</italic></sub> estimation only. No points are removed from calculating the response likelihood
<disp-formula id="pcbi.1006205.e061">
<alternatives>
<graphic id="pcbi.1006205.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e061" xlink:type="simple"/>
<mml:math display="block" id="M61">
<mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
Additionally, to prevent isolated points from being assigned virtually zero probability, we generally add a small probability of <italic>ϵ</italic> = 1.34 × 10<sup>−4</sup> to all. This corresponds to the probability of a point at four standard deviations from the standard normal distribution. For non-outlying points this alteration is considered negligible.</p>
<p><bold>Estimating model evidence.</bold> The evidence that each participant’s data lends to each model is derived from predictive performance in terms of the cross-validation log likelihood (CVLL). For training, we maximized the logarithm of the response likelihood (<xref ref-type="disp-formula" rid="pcbi.1006205.e061">Eq 13</xref>). To maximize the chances of finding the global maximum even for non-convex problems or shallow gradients, every training run first uses a genetic algorithm and then refines its estimate with gradient based search (MATLAB ga, fmincon). The CVLL for each participant and model is summarized by the mean of the logarithm of the response likelihood (<xref ref-type="disp-formula" rid="pcbi.1006205.e061">Eq 13</xref>) on the test set across all cross validation (CV) folds.</p>
<p>As cross validation is a computationally expensive method, we use a random 5-fold split of data into training and test sets such that each training point is used four times for training and once for testing. However, to make splits more representative of the sample we use a stratified version of CV by ensuring that the mean target variable is approximately equal in all folds. This is done by assigning data points to one of the 8-quantiles of the distribution of the target variable. We constructed slices that contain one value from each quantile. Subsequently, we sampled strata to create the 5-fold CV splits. To improve the reliability of per participant estimates of the model evidence (CVLL) we repeated this procedure with different random splits and aggregated the output so that in total 10 CV splits are performed for each participant and model.</p>
<p>Differences in model evidence, Δ, are reported on a log-scale in decibans (also decihartleys, abbreviated dHart) that may be used to interpret the significance of the results of individual participants. According to standard conventions, we consider a value of 5 &gt; Δ barely worth mentioning, 10 &gt; Δ ≥ 5 substantial, 15 &gt; Δ ≥ 10 strong, 20 &gt; Δ ≥ 15 very strong and Δ ≥ 20 decisive.</p>
<p><bold>Group level comparison.</bold> Instead of making the assumption that all participants can be described by the same model, we use a hierarchical Bayesian model selection method (BMS) [<xref ref-type="bibr" rid="pcbi.1006205.ref050">50</xref>] that assigns probabilities to the models themselves. This way, we assume that participants may be described by different models. That is a more suitable approach for group heterogeneity and outliers which are certainly present in the data. The algorithm operates on the CVLL for each participant (<italic>p</italic> = {1,…,<italic>P</italic>}) and each model (<italic>m</italic> = {1,…,<italic>M</italic>}) under consideration and estimates a Dirichlet distribution Dir(<bold><italic>r</italic></bold>|<italic>α</italic><sub>1</sub>,…,<italic>α</italic><sub><italic>M</italic></sub>) that acts as a prior for the multinomial model switches <italic>u</italic><sub><italic>pm</italic></sub>. The latter are represented individually for each subject by a draw from a multinomial distribution <italic>u</italic><sub><italic>pm</italic></sub> ~ Mult(1,<bold><italic>r</italic></bold>) whose parameters are <italic>r</italic><sub><italic>m</italic></sub> = <italic>α</italic><sub><italic>m</italic></sub>/(<italic>α</italic><sub>1</sub>+…+<italic>α</italic><sub><italic>M</italic></sub>). We use the CVLL and assume an uninformative Dirichlet prior <bold><italic>α</italic><sub>0</sub></bold> = <bold>1</bold> on the model probabilities. Later, for model comparison, exceedance probabilities, <inline-formula id="pcbi.1006205.e062"><alternatives><graphic id="pcbi.1006205.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006205.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mi>Beta</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, are calculated corresponding to the belief that a given model is more likely to have generated the data than any other model under consideration. High exceedance probabilities indicate large differences on the group level. We consider values of <italic>p</italic><sub><italic>exc</italic></sub> ≥ 0.95 significant (marked with *) and values of <italic>p</italic><sub><italic>exc</italic></sub> ≥ 0.99 very significant (marked with **).</p>
</sec>
</sec>
</sec>
<sec id="sec023">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006205.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006205.s001" xlink:type="simple">
<label>S1 Dataset</label>
<caption>
<title>Participants’ experimental data.</title>
<p>All data used for the analysis is available as a Matlab data file.</p>
<p>(MAT)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006205.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalman</surname> <given-names>RE</given-names></name>. <article-title>A New Approach to Linear Filtering and Prediction Problems</article-title>. <source>J Basic Eng</source>. <year>1960</year>;<volume>82</volume>: <fpage>35</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1115/1.3662552" xlink:type="simple">10.1115/1.3662552</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006205.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>: <fpage>1170</fpage>–<lpage>1178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3495" xlink:type="simple">10.1038/nn.3495</ext-link></comment> <object-id pub-id-type="pmid">23955561</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>. <article-title>Neural Coding of Uncertainty and Probability</article-title>. <source>Annu Rev Neurosci</source>. <year>2014</year>;<volume>37</volume>: <fpage>205</fpage>–<lpage>220</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-071013-014017" xlink:type="simple">10.1146/annurev-neuro-071013-014017</ext-link></comment> <object-id pub-id-type="pmid">25032495</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source>. <year>2004</year>;<volume>427</volume>: <fpage>244</fpage>–<lpage>247</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02169" xlink:type="simple">10.1038/nature02169</ext-link></comment> <object-id pub-id-type="pmid">14724638</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trommershäuser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gepshtein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Optimal Compensation for Changes in Task-Relevant Movement Variability</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>7169</fpage>–<lpage>7178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1906-05.2005" xlink:type="simple">10.1523/JNEUROSCI.1906-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16079399</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Human representation of visuo-motor uncertainty as mixtures of orthogonal basis distributions</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>1152</fpage>–<lpage>1158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4055" xlink:type="simple">10.1038/nn.4055</ext-link></comment> <object-id pub-id-type="pmid">26120962</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>: <fpage>429</fpage>–<lpage>433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name>. <article-title>Multidimensional models of perception and cognition</article-title>. (Ed.) <source>FGA</source>. <year>1992</year>; <fpage>449</fpage>–<lpage>483</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepard</surname> <given-names>R</given-names></name>. <article-title>Toward a universal law of generalization for psychological science</article-title>. <source>Science</source>. <year>1987</year>;<volume>237</volume>: <fpage>1317</fpage>–<lpage>1323</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.3629243" xlink:type="simple">10.1126/science.3629243</ext-link></comment> <object-id pub-id-type="pmid">3629243</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lucas</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kalish</surname> <given-names>M</given-names></name>. <article-title>A rational model of function learning</article-title>. <source>Psychon Bull Rev</source>. <year>2015</year>;<volume>22</volume>(<issue>5</issue>): <fpage>1193</fpage>–<lpage>215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13423-015-0808-5" xlink:type="simple">10.3758/s13423-015-0808-5</ext-link></comment> <object-id pub-id-type="pmid">25732094</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeLosh</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>McDaniel</surname> <given-names>MA</given-names></name>. <article-title>Extrapolation: The sine qua non of abstraction in function learning</article-title>. <source>J Exp Psychol Learn Mem Cogn</source>. <year>1997</year>;<volume>23</volume>: <fpage>968</fpage>–<lpage>986</lpage>. <object-id pub-id-type="pmid">9231439</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref012"><label>12</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Murphy</surname> <given-names>G</given-names></name>. <chapter-title>The big book of concepts</chapter-title>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge</publisher-loc>; <year>2002</year>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jäkel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wichmann</surname> <given-names>FA</given-names></name>. <article-title>Generalization and similarity in exemplar models of categorization: Insights from machine learning</article-title>. <source>Psychon B Rev</source>. <year>2008</year>;<volume>15</volume>: <fpage>256</fpage>–<lpage>271</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name>. <source>Models of Categorization</source>. <name name-style="western"><surname>Sun</surname> <given-names>R</given-names></name>, editor. <year>2008</year>; <fpage>267</fpage>–<lpage>301</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name>, <name name-style="western"><surname>Alfonso-Reese</surname> <given-names>LA</given-names></name>. <article-title>Categorization as Probability Density Estimation</article-title>. <source>J Math Psychol</source>. <year>1995</year>;<volume>39</volume>: <fpage>216</fpage>–<lpage>233</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/jmps.1995.1021" xlink:type="simple">10.1006/jmps.1995.1021</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006205.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Navarro</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>MD</given-names></name>. <article-title>Sampling Assumptions in Inductive Generalization</article-title>. <source>Cogn Sci</source>. <year>2011</year>;<volume>36</volume>: <fpage>187</fpage>–<lpage>223</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1551-6709.2011.01212.x" xlink:type="simple">10.1111/j.1551-6709.2011.01212.x</ext-link></comment> <object-id pub-id-type="pmid">22141440</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>. <article-title>Generalization, similarity, and Bayesian inference</article-title>. <source>Behav Brain Sci</source>. <year>2001</year>;<volume>24</volume>: <fpage>629</fpage>–<lpage>640</lpage>–629–640. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X01000061" xlink:type="simple">10.1017/S0140525X01000061</ext-link></comment> <object-id pub-id-type="pmid">12048947</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kemp</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>The discovery of structural form</article-title>. <source>Proc Natl Acad Sci</source>. <year>2008</year>;<volume>105</volume>: <fpage>10687</fpage>–<lpage>10692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0802631105" xlink:type="simple">10.1073/pnas.0802631105</ext-link></comment> <object-id pub-id-type="pmid">18669663</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koh</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>DE</given-names></name>. <article-title>Function learning: Induction of continuous stimulus-response relations</article-title>. <source>J Exp Psychol Learn Mem Cogn</source>. <year>1991</year>;<volume>17</volume>: <fpage>811</fpage>–<lpage>811</lpage>. <object-id pub-id-type="pmid">1834766</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carroll</surname> <given-names>JD</given-names></name>. <article-title>Functional learning: The learning of continuous functional mappings relating stimulus and response continua</article-title>. <source>ETS Res Rep Ser</source>. <year>1963</year>;1963.</mixed-citation></ref>
<ref id="pcbi.1006205.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mcdaniel</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>. <article-title>The conceptual basis of function learning and extrapolation: Comparison of rule-based and associative-based models</article-title>. <source>Psychon Bull Rev</source>. <year>2005</year>;<volume>12</volume>: <fpage>24</fpage>–<lpage>42</lpage>. <object-id pub-id-type="pmid">15948282</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name>, <name name-style="western"><surname>Alfonso-Reese</surname> <given-names>LA</given-names></name>. <article-title>Categorization as Probability Density Estimation</article-title>. <source>J Math Psychol</source>. <year>1995</year>;<volume>39</volume>: <fpage>216</fpage>–<lpage>233</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/jmps.1995.1021" xlink:type="simple">10.1006/jmps.1995.1021</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006205.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Navarro</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>MD</given-names></name>. <article-title>Sampling Assumptions in Inductive Generalization</article-title>. <source>Cogn Sci</source>. <year>2011</year>;<volume>36</volume>: <fpage>187</fpage>–<lpage>223</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1551-6709.2011.01212.x" xlink:type="simple">10.1111/j.1551-6709.2011.01212.x</ext-link></comment> <object-id pub-id-type="pmid">22141440</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>. <article-title>Human representation of visuo-motor uncertainty as mixtures of orthogonal basis distributions</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>1152</fpage>–<lpage>1158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4055" xlink:type="simple">10.1038/nn.4055</ext-link></comment> <object-id pub-id-type="pmid">26120962</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepard</surname> <given-names>R</given-names></name>. <article-title>Toward a universal law of generalization for psychological science</article-title>. <source>Science</source>. <year>1987</year>;<volume>237</volume>: <fpage>1317</fpage>–<lpage>1323</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.3629243" xlink:type="simple">10.1126/science.3629243</ext-link></comment> <object-id pub-id-type="pmid">3629243</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Not Noisy, Just Wrong: The Role of Suboptimal Inference in Behavioral Variability</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>: <fpage>30</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.03.016" xlink:type="simple">10.1016/j.neuron.2012.03.016</ext-link></comment> <object-id pub-id-type="pmid">22500627</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Blei</surname> <given-names>DM</given-names></name>. <article-title>A tutorial on Bayesian nonparametric models</article-title>. <source>J Math Psychol</source>. <year>2012</year>;<volume>56</volume>: <fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jmp.2011.08.004" xlink:type="simple">10.1016/j.jmp.2011.08.004</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006205.ref028"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Helmholtz H von. Handbuch der physiologischen Optik. Voss; 1867.</mixed-citation></ref>
<ref id="pcbi.1006205.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Drugowitsch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Churchland</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>The Cost of Accumulating Evidence in Perceptual Decision Making</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>3612</fpage>–<lpage>3628</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4010-11.2012" xlink:type="simple">10.1523/JNEUROSCI.4010-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22423085</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fleming</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name>. <article-title>Metacognition: computation, biology and function</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2012</year>;<volume>367</volume>: <fpage>1280</fpage>–<lpage>1286</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2012.0021" xlink:type="simple">10.1098/rstb.2012.0021</ext-link></comment> <object-id pub-id-type="pmid">22492746</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>. <article-title>Decision Confidence and Uncertainty in Diffusion Models with Partially Correlated Neuronal Integrators</article-title>. <source>Neural Comput</source>. <year>2010</year>;<volume>22</volume>: <fpage>1786</fpage>–<lpage>1811</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2010.12-08-930" xlink:type="simple">10.1162/neco.2010.12-08-930</ext-link></comment> <object-id pub-id-type="pmid">20141474</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mainen</surname> <given-names>ZF</given-names></name>. <article-title>A computational framework for the study of confidence in humans and animals</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2012</year>;<volume>367</volume>: <fpage>1322</fpage>–<lpage>1237</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2012.0037" xlink:type="simple">10.1098/rstb.2012.0037</ext-link></comment> <object-id pub-id-type="pmid">22492750</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oaksford</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>S</given-names></name>. <article-title>On the Source of Human Irrationality</article-title>. <source>Trends Cogn Sci</source>. <year>2016</year>;<volume>20</volume>: <fpage>336</fpage>–<lpage>344</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2016.03.002" xlink:type="simple">10.1016/j.tics.2016.03.002</ext-link></comment> <object-id pub-id-type="pmid">27105669</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Corthell</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Choice Certainty Is Informed by Both Evidence and Decision Time</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>84</volume>: <fpage>1329</fpage>–<lpage>1342</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.015" xlink:type="simple">10.1016/j.neuron.2014.12.015</ext-link></comment> <object-id pub-id-type="pmid">25521381</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Purcell</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>. <article-title>Hierarchical decision processes that operate over distinct timescales underlie choice and changes in strategy</article-title>. <source>Proc Natl Acad Sci</source>. <year>2016</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1524685113" xlink:type="simple">10.1073/pnas.1524685113</ext-link></comment> <object-id pub-id-type="pmid">27432960</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sanders</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Hangya</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>. <article-title>Signatures of a Statistical Computation in the Human Sense of Confidence</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>: <fpage>499</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.03.025" xlink:type="simple">10.1016/j.neuron.2016.03.025</ext-link></comment> <object-id pub-id-type="pmid">27151640</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Chater</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kemp</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Perfors</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Probabilistic models of cognition: exploring representations and inductive biases</article-title>. <source>Trends Cogn Sci</source>. <year>2010</year>;<volume>14</volume>: <fpage>357</fpage>–<lpage>364</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2010.05.004" xlink:type="simple">10.1016/j.tics.2010.05.004</ext-link></comment> <object-id pub-id-type="pmid">20576465</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tversky</surname> <given-names>A</given-names></name>. <article-title>Subjective probability: A judgment of representativeness</article-title>. <source>Cognit Psychol</source>. <year>1972</year>;<volume>3</volume> (<issue>3</issue>): <fpage>430</fpage>–<lpage>454</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Austerweil</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>. <article-title>Structure and Flexibility in Bayesian Models of Cognition</article-title>. <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Townsend</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Eidels</surname> <given-names>A</given-names></name>, editors. <year>2015</year>; <fpage>187</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Gardelle</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Summerfield</surname> <given-names>C</given-names></name>. <article-title>Robust averaging during perceptual judgment</article-title>. <source>Proc Natl Acad Sci</source>. <year>2011</year>;<volume>108</volume>: <fpage>13341</fpage>–<lpage>13346</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1104517108" xlink:type="simple">10.1073/pnas.1104517108</ext-link></comment> <object-id pub-id-type="pmid">21788517</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Horvitz</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</article-title>. <source>Science</source>. <year>2015</year>;<volume>349</volume>: <fpage>273</fpage>–<lpage>278</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aac6076" xlink:type="simple">10.1126/science.aac6076</ext-link></comment> <object-id pub-id-type="pmid">26185246</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Payzan-LeNestour</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bossaerts</surname> <given-names>P</given-names></name>. <article-title>Risk, Unexpected Uncertainty, and Estimation Uncertainty: Bayesian Learning in Unstable Settings</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>: <fpage>e1001048</fpage>–<lpage>e1001048</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1001048" xlink:type="simple">10.1371/journal.pcbi.1001048</ext-link></comment> <object-id pub-id-type="pmid">21283774</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aitchison</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bahrami</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Doubly Bayesian Analysis of Confidence in Perceptual Decision-Making</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>e1004519</fpage>–<lpage>e1004519</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004519" xlink:type="simple">10.1371/journal.pcbi.1004519</ext-link></comment> <object-id pub-id-type="pmid">26517475</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name>. <article-title>Thinking, fast and slow</article-title>. <source>Farrar, Straus and Giroux</source>; <year>2011</year>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barthelmé</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mamassian</surname> <given-names>P</given-names></name>. <article-title>Flexible mechanisms underlie the evaluation of visual confidence</article-title>. <source>Proc Natl Acad Sci</source>. <year>2010</year>;<volume>107</volume>: <fpage>20834</fpage>–<lpage>20839</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1007704107" xlink:type="simple">10.1073/pnas.1007704107</ext-link></comment> <object-id pub-id-type="pmid">21076036</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>. <article-title>A detailed comparison of optimality and simplicity in perceptual decision making.</article-title> <source>Am Psychol Assoc</source>. <year>2016</year>;<volume>123</volume>(<issue>4</issue>): <fpage>452</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/rev0000028" xlink:type="simple">10.1037/rev0000028</ext-link></comment> <object-id pub-id-type="pmid">27177259</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bowers</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>CJ</given-names></name>. <article-title>Bayesian just-so stories in psychology and neuroscience</article-title>. <source>Psychol Bull</source>. <year>2012</year>;<volume>138</volume>: <fpage>389</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0026450" xlink:type="simple">10.1037/a0026450</ext-link></comment> <object-id pub-id-type="pmid">22545686</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gigerenzer</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gaissmaier</surname> <given-names>W</given-names></name>. <article-title>Heuristic Decision Making</article-title>. <source>Annu Rev Psychol</source>. <year>2011</year>;<volume>62</volume>: 451–82–451–82. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-psych-120709-145346" xlink:type="simple">10.1146/annurev-psych-120709-145346</ext-link></comment> <object-id pub-id-type="pmid">21126183</object-id></mixed-citation></ref>
<ref id="pcbi.1006205.ref049"><label>49</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rasmussen</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>CKI</given-names></name>. <source>Gaussian Processes for Machine Learning</source>. <publisher-name>MIT Press</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1006205.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Bayesian model selection for group studies</article-title>. <source>NeuroImage</source>. <year>2009</year>;<volume>46</volume>: <fpage>1004</fpage>–<lpage>1017</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.03.025" xlink:type="simple">10.1016/j.neuroimage.2009.03.025</ext-link></comment> <object-id pub-id-type="pmid">19306932</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>