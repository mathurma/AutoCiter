<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01414</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003406</article-id>
    <article-categories><subj-group subj-group-type="heading"><subject>Perspective</subject></subj-group>
        
        <subj-group subj-group-type="Discipline-v2">
            <subject>Biology</subject>
            <subj-group><subject>Biophysics</subject>
                <subj-group><subject>Biophysics simulations</subject>
                </subj-group></subj-group></subj-group>
            
            <subj-group><subject>Computational biology</subject>
                <subj-group><subject>Biophysic al simulations</subject>
                </subj-group></subj-group>
        
        <subj-group subj-group-type="Discipline-v2"><subject>Chemistry</subject>
            <subj-group><subject>Computational chemistry</subject>
                <subj-group><subject>Molecular mechanics</subject>
                </subj-group></subj-group></subj-group>
        
        <subj-group subj-group-type="Discipline-v2">
            <subject>Physics</subject>
            <subj-group>
                <subject>Biophysics</subject>
                <subj-group><subject>Biophysics simulations</subject>
                </subj-group>
            </subj-group>
        </subj-group>
            
            <subj-group><subject>Physical laws and principles</subject>
                <subj-group><subject>Thermodynamics</subject>
                    <subj-group><subject>Entropy</subject>
                    </subj-group>
                </subj-group>
            </subj-group>
        
       
    </article-categories>
<title-group>
<article-title>Combining Experiments and Simulations Using the Maximum Entropy Principle</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Boomsma</surname><given-names>Wouter</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Ferkinghoff-Borg</surname><given-names>Jesper</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lindorff-Larsen</surname><given-names>Kresten</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Structural Biology and NMR Laboratory, Department of Biology, University of Copenhagen, Copenhagen, Denmark</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Cellular Signal Integration Group, Center for Biological Sequence Analysis, Technical University of Denmark, Lyngby, Denmark</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Levitt</surname><given-names>Michael</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Stanford University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">wb@bio.ku.dk</email> (WB); <email xlink:type="simple">jesperfb@cbs.dtu.dk</email> (JFB); <email xlink:type="simple">lindorff@bio.ku.dk</email> (KLL)</corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>2</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>20</day><month>2</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>2</issue>
<elocation-id>e1003406</elocation-id><permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Boomsma et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>A key component of computational biology is to compare the results of computer modelling with experimental measurements. Despite substantial progress in the models and algorithms used in many areas of computational biology, such comparisons sometimes reveal that the computations are not in quantitative agreement with experimental data. The principle of maximum entropy is a general procedure for constructing probability distributions in the light of new data, making it a natural tool in cases when an initial model provides results that are at odds with experiments. The number of maximum entropy applications in our field has grown steadily in recent years, in areas as diverse as sequence analysis, structural modelling, and neurobiology. In this Perspectives article, we give a broad introduction to the method, in an attempt to encourage its further adoption. The general procedure is explained in the context of a simple example, after which we proceed with a real-world application in the field of molecular simulations, where the maximum entropy procedure has recently provided new insight. Given the limited accuracy of force fields, macromolecular simulations sometimes produce results that are at not in complete and quantitative accordance with experiments. A common solution to this problem is to explicitly ensure agreement between the two by perturbing the potential energy function towards the experimental data. So far, a general consensus for how such perturbations should be implemented has been lacking. Three very recent papers have explored this problem using the maximum entropy approach, providing both new theoretical and practical insights to the problem. We highlight each of these contributions in turn and conclude with a discussion on remaining challenges.</p>
</abstract>
<funding-group><funding-statement>WB and KLL were supported by a Hallas MÃ¸ller stipend from the Novo Nordisk Foundation (<ext-link ext-link-type="uri" xlink:href="http://www.novonordiskfonden.dk" xlink:type="simple">www.novonordiskfonden.dk</ext-link>). The funders had no role in the preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Picture this scenario: you have spent years developing an elaborate model for a particular scientific phenomenon. Now, new experimental data have been measured for the same phenomenon, and the data disagree with your model. How do you proceed? This is a reasonable question to pose in any scientific discipline, but perhaps particularly in that of computational biology, where models are constantly developed and refined to encompass the ever-growing databases of biological data.</p>
<p>Bayesian inference is commonly put forward as an answer to this question. It provides a simple recipe for how to produce a new model (posterior) by modifying an existing model (prior) after observing a new set of data. There are, however, situations where the Bayesian formalism is not easily applicable. For instance, it is traditionally assumed that all our prior knowledge about the measured quantities can be expressed in terms of probability distributions. Often, however, we only obtain information about the average value of these quantities from experiments. This information must somehow be turned into a probability distribution concerning the system under study before we can apply the Bayesian machinery, but intuitively it seems unreasonable to assume knowledge about an entire distribution when all we know is a single value. It is an underdetermined problem, in the sense that there may be an infinite number of possible prior distributions that are compatible with this piece of data. A simple, but general solution to this type of problem was provided by Jaynes in 1957, who proposed that among all the models fulfilling the constraints from the data, one should select the model containing the least amount of information <xref ref-type="bibr" rid="pcbi.1003406-Jaynes1">[1]</xref>. This <italic>maximum entropy</italic> principle has proven to be extremely powerful, having applications in a wide variety of scientific disciplines.</p>
<p>In computational biology, maximum entropy approaches are also becoming increasingly common. Examples include the formulation of models of collective neural stimuli <xref ref-type="bibr" rid="pcbi.1003406-GranotAtedgi1">[2]</xref>, reconstruction of protein signaling networks <xref ref-type="bibr" rid="pcbi.1003406-Locasale1">[3]</xref>, optimization of force fields for molecular simulation <xref ref-type="bibr" rid="pcbi.1003406-Carmichael1">[4]</xref>, and modelling covariation among sites in protein sequences <xref ref-type="bibr" rid="pcbi.1003406-Lapedes1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Morcos1">[6]</xref>. The general applicability of the principle suggests that there is a significant potential for other relevant applications in this field. With this Perspectives article, we will highlight the approach in some detail, hopefully communicating the elegance of the procedure and encouraging further work in this direction.</p>
<p>As a concrete example, we will focus our attention on a recent application in the field of structural biology, namely, the problem of conducting molecular simulations under restraints from experimental data. In this specific case, the force field can be considered as the model. Decades of research have gone into the development and fine-tuning of these force fields, and they have proven useful in a multitude of applications <xref ref-type="bibr" rid="pcbi.1003406-Klepeis1">[7]</xref>. <xref ref-type="fig" rid="pcbi-1003406-g001">Figure 1</xref> Despite their success, it is, however, still a common scenario that the results obtained through simulations do not quantitatively match those obtained from experiments. A relevant question is then how one can make use of additional information obtained through experiments to improve the quality of a simulation. Although efficient algorithms exist for improving molecular force fields based on experimental data <xref ref-type="bibr" rid="pcbi.1003406-Norgaard1">[8]</xref>, a common approach is to introduce a system-specific modification to the energy function, and thereby modify the structural ensemble to become in agreement with the experimental data. Various techniques for direct combination of experiment and simulation exist, but the theoretical underpinnings of these approaches have remained elusive. Three recent papers <xref ref-type="bibr" rid="pcbi.1003406-Pitera1">[9]</xref>â<xref ref-type="bibr" rid="pcbi.1003406-Cavalli1">[11]</xref>, have explored the assumptions underlying existing methods in the light of the maximum entropy principle, leading to suggestions for new avenues to optimally utilize the complementary information available from experiments and molecular simulations. We here review these developments and suggest areas that are in need of further study. In particular, we discuss the complications that may arise when using the technique in practice, including the fact that all experiments contain various, sometimes unknown, sources of noise.</p>
<fig id="pcbi-1003406-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003406.g001</object-id><label>Figure 1</label><caption>
<title>Jaynes' die problem: Maximum entropy probability distributions for a die, after observing the average outcome.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003406.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s2" sec-type="methods">
<title>Jaynes' Principle of Maximum Entropy</title>
<p>Jaynes originally proposed the maximum entropy principle to establish a link between Shannon's information theory <xref ref-type="bibr" rid="pcbi.1003406-Shannon1">[12]</xref> and statistical mechanics <xref ref-type="bibr" rid="pcbi.1003406-Jaynes1">[1]</xref>. The goal is to construct the probability distribution that best represents the state of knowledge after observing a set of quantities of a system. The central idea is that among all the infinite number of distributions that are compatible with the data, one should select the distribution which maintains the largest degree of uncertainty about the variables of interest, thus ensuring that the data has been used as conservatively as possible. The natural quantity for expressing uncertainty in a distribution is Shannon's entropy <xref ref-type="bibr" rid="pcbi.1003406-Shannon1">[12]</xref>:<disp-formula id="pcbi.1003406.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e001" xlink:type="simple"/></disp-formula>Finding the correct probability distribution thus becomes a matter of maximizing this expression under the constraint that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e002" xlink:type="simple"/></inline-formula> sums to one (i.e. it should be a probability distribution), combined with the constraints obtained from the observed data. Typically, the Lagrange formalism is used to enforce these constraints.</p>
<p>As an example, <xref ref-type="sec" rid="pcbi-1003406-box001">Box 1</xref> contains a primer of the basic maximum entropy procedure on the simple problem of inferring the probability of the different outcomes of a (possibly biased) die, given only information about the average observed after a large number of throws. Following the exact same procedure, with just a few lines of calculation, the principle of maximum entropy also predicts the well-known Boltzmann distribution in statistical physics as the correct distribution reflecting your knowledge of a system when only the mean energy is observed <xref ref-type="bibr" rid="pcbi.1003406-Jaynes1">[1]</xref>. In fact, one of Jaynes' great achievements was to demonstrate that many results in statistical mechanics could be derived by the simple application of this principle.</p>
<boxed-text id="pcbi-1003406-box001" position="float"><sec id="s2a1">
<title>Box 1. A Primer to the Principle of Maximum Entropy (Adapted from Ref. <xref ref-type="bibr" rid="pcbi.1003406-Jaynes3">[17]</xref>)</title>
<sec id="s2a1a">
<title>Jaynes' die problem</title>
<p>A die has been tossed many times, and we are provided with the information that the average outcome was some value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e003" xlink:type="simple"/></inline-formula>, rather than the 3.5 that one would expect from a fair die. Based on this information alone, what is our estimate of the probabilities of the different outcomes for this die?</p>
<p>According to the Principle of Maximum Entropy, we should maximize the entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e004" xlink:type="simple"/></inline-formula> of the discrete probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e005" xlink:type="simple"/></inline-formula>. This should be done under the constraints <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e006" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e007" xlink:type="simple"/></inline-formula>.</p>
<p>In general, the solution to this type of optimization problem takes the form:<disp-formula id="pcbi.1003406.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e008" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e009" xlink:type="simple"/></inline-formula> runs over the number of constraints, and Z is the partition function, which ensures proper normalization. The following identity conveniently relates the derivatives of the partition function to the observed expectation values:<disp-formula id="pcbi.1003406.e010"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e010" xlink:type="simple"/><label>(10)</label></disp-formula>For our die problem, we have only a single constraint, and since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e011" xlink:type="simple"/></inline-formula>, we have:<disp-formula id="pcbi.1003406.e012"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e012" xlink:type="simple"/><label>(11)</label></disp-formula>from which we obtain the following five-degree polynomial in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e013" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003406.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e014" xlink:type="simple"/><label>(12)</label></disp-formula><xref ref-type="fig" rid="pcbi-1003406-g001">Figure 1</xref> shows the single, real-valued solution to this polynomial for various values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e015" xlink:type="simple"/></inline-formula>. Notice how the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e016" xlink:type="simple"/></inline-formula> produces the uniform distribution as we expect, while higher or lower values produce gradually more skewed distributions.</p>
</sec></sec></boxed-text>
<p>In the scenario drawn up in the beginning of this article, we already have a model, and are interested in finding the necessary modifications to make it compatible with the new data. In this case, it is more convenient to consider the <italic>relative entropy</italic>, or <italic>Kullback-Leibler divergence</italic> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e017" xlink:type="simple"/></inline-formula>) instead <xref ref-type="bibr" rid="pcbi.1003406-Kullback1">[13]</xref>:<disp-formula id="pcbi.1003406.e018"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e018" xlink:type="simple"/></disp-formula>By minimizing this expression under the constraints from the experimental data, we find the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e019" xlink:type="simple"/></inline-formula> that is as close as possible to the original distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e020" xlink:type="simple"/></inline-formula>, but is now compatible with the data. This procedure is sometimes referred to as the principle of <italic>minimum discrimination information</italic> or <italic>minimum cross entropy</italic>, but can be seen as a natural extension of the maximum entropy approach.</p>
<p>There is a substantial literature on the foundations of maximum entropy and why it is an appropriate framework for inference <xref ref-type="bibr" rid="pcbi.1003406-Jaynes1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Jaynes2">[14]</xref>â<xref ref-type="bibr" rid="pcbi.1003406-Shore1">[16]</xref>. Although a full treatment is beyond the scope of this paper, one intuitive argument for its validity comes from combinatorics: the principle of maximum entropy will provide the solution which is realizable in the most ways. For our example, if we consider all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e021" xlink:type="simple"/></inline-formula> possible outcomes of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e022" xlink:type="simple"/></inline-formula> throws of a die, only a subset of these would be compatible with a given model. Maximizing the entropy ensures that this subset is as large as possible given the observed average value, not ruling out any more realizations than strictly necessary. For a clear illustration of this point, we again refer to Jaynes, who explicitly calculates this multiplicity for different assignments of probabilities to the die <xref ref-type="bibr" rid="pcbi.1003406-Jaynes3">[17]</xref>.</p>
<p>With this brief introduction, we hope that we have conveyed the general applicability of the principle of maximum entropy. In particular, in combination with Bayesian inference, it is a powerful tool for consistent reasoning in the light of new data. For the remainder of the paper, we focus on a particular problem in computational biology which has recently been the subject of substantial activity, in the pursuit of a practically workable maximum entropy solution to replace (or validate) the currently used approaches.</p>
</sec><sec id="s3">
<title>Macromolecular Structure Determination</title>
<p>Molecular simulations typically utilize either molecular dynamics (MD) or Monte Carlo (MC) methods to sample conformations according to an energy function, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e023" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e024" xlink:type="simple"/></inline-formula> represents the structure of a molecule and possibly also solvent molecules and other co-factors, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e025" xlink:type="simple"/></inline-formula> represents a mathematical function that relates the structure to the âenergyâ of the system. In the context of physics-based simulations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e026" xlink:type="simple"/></inline-formula> mimics the physical energy of the system; in such cases <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e027" xlink:type="simple"/></inline-formula> is also often referred to by its derivative, and thus called a molecular mechanics âforce fieldâ (herein termed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e028" xlink:type="simple"/></inline-formula>). When MD or MC methods are used to sample protein conformations they typically give rise to an <italic>ensemble</italic> of conformations that are distributed according the celebrated Boltzmann distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e029" xlink:type="simple"/></inline-formula>, that relates the probability of observing a given conformation to the energy of that conformation. In this equation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e030" xlink:type="simple"/></inline-formula> is a normalization constant and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e031" xlink:type="simple"/></inline-formula> is the Boltzmann factor.</p>
<sec id="s3a">
<title>Traditional structure determination methods</title>
<p>Despite recent substantial developments in the accuracy of molecular energy functions <xref ref-type="bibr" rid="pcbi.1003406-LindorffLarsen1">[18]</xref>â<xref ref-type="bibr" rid="pcbi.1003406-Best1">[20]</xref> it is still not possible routinely and consistently to use molecular simulations to predict or refine the structure of proteins <xref ref-type="bibr" rid="pcbi.1003406-Tyka1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Raval1">[22]</xref>. Protein structures are therefore typically determined through hybrid methods that combine experiments and simulations. The most common technique is to combine a physical force field, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e032" xlink:type="simple"/></inline-formula> with an experimentally derived âbiasing potential,â <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e033" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e034" xlink:type="simple"/></inline-formula>. The function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e035" xlink:type="simple"/></inline-formula> acts to bias simulations to provide structures that are compatible with experiments and typically takes the form of a harmonic potential that penalizes protein structures that are not in agreement with experiments: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e036" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e037" xlink:type="simple"/></inline-formula> is the set of experimental measurements (e.g., NMR-derived NOE intensities or structure factors from X-ray scattering); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e038" xlink:type="simple"/></inline-formula> are the corresponding calculated quantities calculated from the structure, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e039" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e040" xlink:type="simple"/></inline-formula> is a force constant that provides a scale for the energetic penalty for deviations between experimental and calculated values. In this way, only conformations where the back-calculated data are close to the experimental values will have a low overall value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e041" xlink:type="simple"/></inline-formula>. When combined with a force field, this produces structures that simultaneously agree with the experimental data and have structural features that conform to our current understanding of the physical principles that govern protein stability, encoded in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e042" xlink:type="simple"/></inline-formula> (e.g., well-packed hydrophobic cores and regular secondary structural elements stabilized by hydrogen bonds). When implemented in this fashion, it is important to note that the simulations are not forced to agree perfectly with the experimental data. Instead, the level of agreement is now governed by the weight of the biasing energy term. Consequently, the experimental data is typically referred to as <italic>restraints</italic>, rather than the term <italic>constraints</italic> used when complete agreement is the goal. One of the challenges associated with these hybrid energies is choosing such weights and other parameters for the biasing potential. Often these parameters are tuned manually. An alternative, Bayesian approach, called inferential structure determination, however, provides an elegant solution to this problem, by treating such unknown quantities as ânuisance parametersâ and integrating them out <xref ref-type="bibr" rid="pcbi.1003406-Rieping1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Habeck1">[24]</xref>.</p>
<p>A direct consequence of the hybrid energy approach described above is that all of the sampled structures are <italic>individually</italic> in agreement with the experimental data. Although this superficially sounds reasonableâindeed the idea of the biasing potential is to bring the conformations to be in agreement with experimentsâit brings with it some additional consequences. The basic problem arises because the experimental data, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e043" xlink:type="simple"/></inline-formula>, are typically averaged over a very large number of molecules as well as averaged over timescales that are long compared to those typical of macromolecular fluctuations. Thus, there is no reason to expect that individual conformations should be in exact agreement with the data as long as the entire ensemble of conformations is (e.g., even if a biased die produces an average of four, one would not expect that each throw produced this result). Thus, in the approach outlined above for structure determination, one is effectively making additional assumptions about both the structure and the conformational variability of a protein that are neither directly derived from the experimental data nor from the physical force field.</p>
</sec><sec id="s3b">
<title>Simulating replicas</title>
<p>One intuitive strategy to overcome this problem is to simultaneously simulate several replicas of the system and apply restraints on the <italic>average</italic> of the back-calculated experimental values, rather than on the individual structures <xref ref-type="bibr" rid="pcbi.1003406-Vendruscolo1">[25]</xref>. This strategy has a long history and is often known as âensemble averaged refinementâ in the context of NMR structure determination <xref ref-type="bibr" rid="pcbi.1003406-Scheek1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Kim1">[27]</xref> or âmulticonformer refinementâ in X-ray structure determination <xref ref-type="bibr" rid="pcbi.1003406-Kuriyan1">[28]</xref>â<xref ref-type="bibr" rid="pcbi.1003406-Burnley1">[30]</xref>. This approach has, for instance, been used to study the structural dynamics of folded proteins <xref ref-type="bibr" rid="pcbi.1003406-LindorffLarsen2">[31]</xref>â<xref ref-type="bibr" rid="pcbi.1003406-Roux2">[33]</xref>, unfolded proteins <xref ref-type="bibr" rid="pcbi.1003406-LindorffLarsen3">[34]</xref>, membrane proteins <xref ref-type="bibr" rid="pcbi.1003406-Jo1">[35]</xref>, and intrinsically disordered proteins <xref ref-type="bibr" rid="pcbi.1003406-Dedmon1">[36]</xref>.</p>
<p>In such ensemble simulations, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e044" xlink:type="simple"/></inline-formula> replicas do not physically interact, but are coupled via the experimental data. The total system of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e045" xlink:type="simple"/></inline-formula> conformations is governed by the energy function:<disp-formula id="pcbi.1003406.e046"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e046" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<p>The first term is simply the sum of the force field energies of the replicas. The second term acts to enforce that the simulation is in agreement with the experiments, but penalizing the entire ensemble only when the <italic>ensemble averaged</italic> quantities, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e047" xlink:type="simple"/></inline-formula> deviate from experiment. For linearly averaged quantities, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e048" xlink:type="simple"/></inline-formula>. In this way, the calculated quantities in individual conformation (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e049" xlink:type="simple"/></inline-formula>) may differ from experiment as long as their ensemble average, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e050" xlink:type="simple"/></inline-formula>, matches the experiment within a scale that is implicitly determined by the force constant, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e051" xlink:type="simple"/></inline-formula>.</p>
<p>Obviously, this method introduces a new parameter to the problem, namely the number of parallel replicas, and it is not immediately clear what this parameter should be set to. One approach to explore this problem is first to use synthetic data that have themselves been generated from simulations and compare the restrained ensemble with the ensemble used to generate the data <xref ref-type="bibr" rid="pcbi.1003406-Richter1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-LindorffLarsen4">[38]</xref>. With real-world experimental data, a suitable value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e052" xlink:type="simple"/></inline-formula> can be determined by cross-validating with independent data not used in the structure determination <xref ref-type="bibr" rid="pcbi.1003406-Burling1">[39]</xref>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e053" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e054" xlink:type="simple"/></inline-formula> both affect the level of agreement between experiment and simulation, their optimal values are interdependent.</p>
<p>One critique of the method relates to the ratio of the number of free parameters (atomic coordinates) to the number of experimental data points. In ânormalâ (non-ensemble) structure determination, there are typically fewer experimental data than atomic positions to be determined; the problem is underdetermined and additional (prior) information, e.g., from a force field, is needed to determine structures. In ensemble refinement, the same number of data points are available, but now these are used to determine <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e055" xlink:type="simple"/></inline-formula> times as many atomic positions, leaving the underdetermination even worse. Although it can be argued that ensemble simulations provide a more natural way to match ensemble-averaged experimental data with simulations, the lack of a clear theoretical underpinning is problematic and it has been argued that the method can lead to an increased risk of drawing erroneous conclusions <xref ref-type="bibr" rid="pcbi.1003406-vanGunsteren1">[40]</xref>.</p>
</sec><sec id="s3c">
<title>Maximum entropy approach</title>
<p>Given the possibility of both overfitting experimental data and underrestraining by unfavourable data-to-parameter ratios, it would be preferable to have a theoretically well-founded method for combining experiment and simulation. Incorporating experimental data into a simulation is essentially a matter of updating a probability distribution (the original Boltzmann distribution defined by the force field) in the light of new data. As described above, these data are typically both time and ensemble averages of an underlying quantity, and it is therefore an obvious choice to use the principle of maximum entropy to infer a suitable model. Among all possible models compatible with the new data, this will be the one that is the least biased. Or alternatively phrased, this will be the model that is as close as possible to the original distribution, while taking the new data into account. As an important special case, we stress that if the force field is <italic>already</italic> compatible with the observed data, no modifications to the distribution are made. While this sounds like a trivial principle, it is actually violated by many existing methods.</p>
<p>Conceptually, the maximum entropy procedure is simple, and we proceed exactly as we did for the die example. The probability distribution takes the form<disp-formula id="pcbi.1003406.e056"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e056" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e057" xlink:type="simple"/></inline-formula> is the structural state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e058" xlink:type="simple"/></inline-formula> is the force field energy of this state, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e059" xlink:type="simple"/></inline-formula> represent an experimental observable back-calculated from the structure, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e060" xlink:type="simple"/></inline-formula> are the corresponding Lagrange multipliers, whose values should be determined to enforce agreement between experiment and simulations. Technical issues, however, seem to have hindered a practically useful implementation of the method. Although maximum entropy approaches have been explored in certain aspects of X-ray scattering data <xref ref-type="bibr" rid="pcbi.1003406-Bricogne1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Rycki1">[42]</xref> and NMR <xref ref-type="bibr" rid="pcbi.1003406-Groth1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Massad1">[44]</xref>, applications in the context of molecular simulation have been surprisingly few. One of the main practical issues is that one needs to numerically determine the optimal values for the Lagrange multiplier corresponding to each constraint. Since experimental data will easily provide hundreds of these constraints, this optimization is a formidable task.</p>
<p>In the last year, three papers have brought a practical application of the maximum entropy principle for this problem considerably closer <xref ref-type="bibr" rid="pcbi.1003406-Pitera1">[9]</xref>â<xref ref-type="bibr" rid="pcbi.1003406-Cavalli1">[11]</xref>. Pitera and Chodera made the intriguing observation of a potential link between the maximum entropy solution and the solution obtained by the replica-averaged ensemble technique described above, suggesting that as the number of replicas in a simulation grows, the ensemble-restrained solution would gradually approach that obtained by the principle of maximum entropy <xref ref-type="bibr" rid="pcbi.1003406-Pitera1">[9]</xref>.</p>
<p>The relationship between replica-based simulations and the maximum entropy formalism was clarified and mathematically proven in papers by Roux and Weare <xref ref-type="bibr" rid="pcbi.1003406-Roux1">[10]</xref> and Cavalli et al. <xref ref-type="bibr" rid="pcbi.1003406-Cavalli1">[11]</xref>, both of which demonstrated that a replica-based approach is equivalent to the maximum entropy solution. In addition to establishing this link, their result provides a solution to one of the primary problems associated with the maximum entropy problem in molecular simulation: the challenge of estimating the Lagrange multipliers numerically. In particular, they showed that the maximum entropy solution appears as a limit of the replica method when the harmonic potential enforcing the replica-averaged restraint becomes infinitely narrow. More precisely, <italic>the distribution of each replica in a replica-averaged ensemble simulation (</italic><xref ref-type="disp-formula" rid="pcbi.1003406.e046"><italic>Eq. 1</italic></xref><italic>) will approach the maximum entropy distribution if both </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e061" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e062" xlink:type="simple"/></inline-formula>. Using Dirac's <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e063" xlink:type="simple"/></inline-formula>-function over the averaged restraint violations, this can be written as<disp-formula id="pcbi.1003406.e064"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e064" xlink:type="simple"/><label>(3)</label></disp-formula>As above, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e065" xlink:type="simple"/></inline-formula> denotes the average ensemble average over the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e066" xlink:type="simple"/></inline-formula>'th restraint, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e067" xlink:type="simple"/></inline-formula> is the experimentally observed data.</p>
<p>This result has immediate practical applications. Rather than determining Lagrange multipliers for all experimental observations, it is sufficient to conduct an ensemble-averaged simulation with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e068" xlink:type="simple"/></inline-formula>-function constraints with a large number of replicas. In practice, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e069" xlink:type="simple"/></inline-formula>-functions are difficult to work with and are often replaced with a steep potential, for instance a harmonic term. <xref ref-type="fig" rid="pcbi-1003406-g002">Figure 2</xref> illustrates the procedure on a simple 2D bivariate Gaussian mixture model with two components, with a single restraint in one of the dimensions (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e070" xlink:type="simple"/></inline-formula>). The top-left plot is the unperturbed potential, while the top-right plot shows the maximum entropy solution with a numerically optimized Lagrange multiplier. The plots in the matrix show the behavior of different combinations of the force constant of the harmonic potential and the number of replicas used in the simulation. Note how two opposite forces are at play: an increase in the force constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e071" xlink:type="simple"/></inline-formula> will pull the distribution toward the restrained value, while an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e072" xlink:type="simple"/></inline-formula> will increase the variance, pulling it back to the original distribution. For sufficiently large values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e073" xlink:type="simple"/></inline-formula> the harmonic term mimics a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e074" xlink:type="simple"/></inline-formula>-function and when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e075" xlink:type="simple"/></inline-formula> is increased for such values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e076" xlink:type="simple"/></inline-formula> the distribution converges towards the maximum entropy solution <italic>without explicitly determining any Lagrange multipliers</italic>.</p>
<fig id="pcbi-1003406-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003406.g002</object-id><label>Figure 2</label><caption>
<title>The effect of different methods for incorporating experimental data on a simple example consisting of a mixture of two bivariate normal distributions.</title>
<p>In this example, we only have experimental data regarding the y-dimension of the distribution (target value indicated by dotted line). The top row contains the unperturbed and maximum entropy distributions. The matrix shows various combinations of force constant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e123" xlink:type="simple"/></inline-formula>) and number of replicas (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e124" xlink:type="simple"/></inline-formula>) when enforcing the restraint through a harmonic potential. In these calculations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e125" xlink:type="simple"/></inline-formula> corresponds to the standard method for structure calculation, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e126" xlink:type="simple"/></inline-formula> corresponds to ensemble refinement. In each plot we also show the mean in the y-direction (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e127" xlink:type="simple"/></inline-formula>), and the entropy of the distribution (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e128" xlink:type="simple"/></inline-formula>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003406.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s3d">
<title>Remaining challenges</title>
<p>The replica-averaged approach described in the previous section is a remarkably elegant, easily implementable technique that provides the least-biased distribution consistent with any observed expectation values over the data. From our perspective, it represents a significant step forward in our understanding of how experimental data should be used in molecular simulations. There are, however, still some remaining issues, which must be resolved before we can claim a full understanding of the problem and a practically useful implementation. We will highlight the most important ones here.</p>
<sec id="s3d1">
<title>Ensuring convergence</title>
<p>There are some challenges involved in determining optimal values for the number of replicas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e077" xlink:type="simple"/></inline-formula> and the force constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e078" xlink:type="simple"/></inline-formula>. The question of how quickly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e079" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e080" xlink:type="simple"/></inline-formula> should grow with respect to each other was investigated in some detail for the 1D harmonic system by Roux and Weare, who stressed the importance of letting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e081" xlink:type="simple"/></inline-formula> more rapidly then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e082" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003406-Roux1">[10]</xref> but also noted that the exact details could differ for more complex models. Ideally, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e083" xlink:type="simple"/></inline-formula> should be chosen as large as possible, but as illustrated by <xref ref-type="fig" rid="pcbi-1003406-g002">Figure 2</xref>, this will impose a requirement for a larger number of replicas as well. This observation suggests an iterative approach of alternating increases of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e084" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e085" xlink:type="simple"/></inline-formula>. Higher values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e086" xlink:type="simple"/></inline-formula> will draw the mean value closer to the restraint, while increasing values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e087" xlink:type="simple"/></inline-formula> will increase the variance.</p>
<p>Convergence can be assessed by simultaneously probing the violation of the expectation values relative to the restraints and the entropy of the distribution (or the entropy corresponding to the restrained subspace). It is, however, very difficult to get converged entropy estimates for the high dimensional conformational space of a molecular simulation <xref ref-type="bibr" rid="pcbi.1003406-Genheden1">[45]</xref>. It remains to be seen whether this poses a significant problem for the application of this method in practice.</p>
</sec><sec id="s3d2">
<title>Estimating Lagrange multipliers</title>
<p>Despite the convenience of the replica-averaged method, it remains unclear whether this method is always preferable to an approach that estimates the Lagrange multipliers explicitly. Although there can be hundreds of parameters to estimate, there are mitigating circumstances, such as convexity in the case of independent restraints, which might make the search problem less complex. Roux and Weare point out that even when successfully finding all Lagrange multipliers, one still has to run an entire simulation. Similar problems seem to assert themselves for the replica-case, where production runs can only be conducted once convergence in entropy has been ensured.</p>
<p>One potential compromise could be the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e088" xlink:type="simple"/></inline-formula> approach described below, which assumes that different restraints share the same Lagrange multiplier, and therefore requires fewer parameters to be estimated. Whether this approximation in practice proves more efficient than finding local-optima in the full restraint Lagrange problem remains to be seen. One direction that is worth pursuing further in this respect is to develop a replica analogy to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e089" xlink:type="simple"/></inline-formula> approach, alleviating the need for the numerical determination of the Lagrange multiplier.</p>
</sec><sec id="s3d3">
<title>Dealing with uncertainties</title>
<p>The previous sections assumed that the experimentally observed values were obtained with perfect accuracy. In any real-world scenario there will, however, be some level of noise or uncertainty associated with such experimental data. As an example, consider the case of the die in <xref ref-type="sec" rid="pcbi-1003406-box001">Box 1</xref>: the experiment from which the averages are observed will always consist of a finite number of tosses, and the average <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e090" xlink:type="simple"/></inline-formula> is therefore only determined within some uncertainty. How should this uncertainty be taken into account? Note that there is an important difference between adding a constraint on the second moment (the variance) and incorporating knowledge about the error of the first moment (error of the mean). The former can easily be dealt with using the maximum entropy principle, while the latter is more problematic.</p>
<p>One potential solution to the problem is to replace a single, exact constraint with two constraints that act as a lower and upper bound, respectively <xref ref-type="bibr" rid="pcbi.1003406-Roux1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Thomas1">[46]</xref>. This solution, however, assumes that the experimental noise can be interpreted as âhardâ limits and does not represent the fact that the experimental measurement is just an estimate of the underlying true value.</p>
<p>As an alternative solution to this problem, Cavalli et al. propose a combination of the maximum entropy principle and Bayesian inference with a prior distribution that reflects the uncertainty of the measured quantity. We briefly sketch the idea behind the resulting derivation here, referring to the measured data points as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e091" xlink:type="simple"/></inline-formula> and the (unknown) actual values as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e092" xlink:type="simple"/></inline-formula>. For compactness, boldface is used to denote a sets of replicas or restraints (e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e093" xlink:type="simple"/></inline-formula>). Assuming independence between the restraints, we have:<disp-formula id="pcbi.1003406.e094"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e094" xlink:type="simple"/><label>(4)</label></disp-formula>Assuming flat priors, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e095" xlink:type="simple"/></inline-formula>, and assuming independent Gaussian distributions on the latter,<disp-formula id="pcbi.1003406.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e096" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e097" xlink:type="simple"/></inline-formula> again is the calculated ensemble averaged quantity of data <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e098" xlink:type="simple"/></inline-formula>. Note how this is simply the product of a noise-free maximum entropy expression on the exact but unknown quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e099" xlink:type="simple"/></inline-formula> and a noise term that models the uncertainty of our observable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e100" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e101" xlink:type="simple"/></inline-formula> are ânuisance parametersâ that can now be integrated out:<disp-formula id="pcbi.1003406.e102"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e102" xlink:type="simple"/><label>(6)</label></disp-formula>This expression now only includes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e103" xlink:type="simple"/></inline-formula>, the experimentally determined quantity that is an estimate of the true, underlying value. The equation above, derived by Cavalli et al. is quite striking, in the sense that it corresponds exactly to the form used in classic ensemble simulations (<xref ref-type="disp-formula" rid="pcbi.1003406.e046">Eq. 1</xref>), except that the force constant that can normally be tuned freely is now determined uniquely by the uncertainty in the observed experimental values.</p>
<p>While this expression is highly appealing, it also presents a potential complication: because the force constant is now fixed (by the experimental uncertainty), the replicas will decouple in the limit of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e104" xlink:type="simple"/></inline-formula>, and the influence of the data will decrease as the ensemble approaches the unperturbed distribution provided by the force field. In the context of the example in <xref ref-type="fig" rid="pcbi-1003406-g002">Figure 2</xref>, it is clear that if the force constant is too low, such as in the first row, increasing the number of replicas does not lead to a distribution that mimics the maximum entropy solution. While it is clear that in the presence of experimental noise one would not expect to recover the standard maximum entropy result (which is valid only for exactly known quantities), one would expect that the experimental uncertainty, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e105" xlink:type="simple"/></inline-formula>, should set the scale for how large deviations can be tolerated between the final ensemble and the experimental value. The effect can also be understood in the detailed analysis by Roux and Weare of a 1D harmonic potential with a harmonic restraint. In particular, their calculations show that when the number of replicas is increased for a fixed force constant, the mean of the restrained ensemble converges to that of the prior reference distribution while the variance increases to its correct value. In the standard maximum entropy setting, the problem of the mean reverting to the reference value can be alleviated by simply increasing the force constant. When the force constant is determined from the experimental noise, this is no longer possible, suggesting that rather than the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e106" xlink:type="simple"/></inline-formula> limit, an intermediate value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e107" xlink:type="simple"/></inline-formula> might be more appropriate in order to provide a balance between matching the mean and the variance. Although this leaves open exactly which procedure is most appropriate to determine the optimal value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e108" xlink:type="simple"/></inline-formula>, it does provide insight into the problems associated with choosing a value that is either too low or too high.</p>
<p>The problem of maximum entropy in the context of noisy data has been addressed numerous times in other fields, leading to various forms of generalized maximum entropy procedures <xref ref-type="bibr" rid="pcbi.1003406-Thomas1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Golan1">[47]</xref> and regularization approaches <xref ref-type="bibr" rid="pcbi.1003406-Dudk1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Donoho1">[49]</xref>. Unfortunately, as of yet there seems to be no universally accepted solution to this problem. One approach that we foresee could be potentially useful in molecular simulation was proposed by Gull and Daniell in the context of image reconstruction <xref ref-type="bibr" rid="pcbi.1003406-Gull1">[50]</xref>. The idea is to replace the many individual constraints with a single constraint on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e109" xlink:type="simple"/></inline-formula> statistic over all data, only matching them up to their experimental uncertainty:<disp-formula id="pcbi.1003406.e110"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e110" xlink:type="simple"/><label>(7)</label></disp-formula>The expectation of this statistic is the number of data points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e111" xlink:type="simple"/></inline-formula>. Maximizing the entropy with respect to this single constraint we obtain:<disp-formula id="pcbi.1003406.e112"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003406.e112" xlink:type="simple"/><label>(8)</label></disp-formula>This approach only requires a single Lagrange multiplier to be determined (by matching the calculated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e113" xlink:type="simple"/></inline-formula> with its expectation value) and, thus, scales considerably better with the number of observed data points. The resulting expression, however, relies on the averages <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e114" xlink:type="simple"/></inline-formula>, which are not know a priori. A possible strategy would be to estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e115" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e116" xlink:type="simple"/></inline-formula> iteratively, by repeatedly estimating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e117" xlink:type="simple"/></inline-formula> from a simulation, adjusting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e118" xlink:type="simple"/></inline-formula> to match the calculated and expected value for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e119" xlink:type="simple"/></inline-formula>, and then rerunning the simulation (or reweighting the statistics from the previous one <xref ref-type="bibr" rid="pcbi.1003406-Norgaard1">[8]</xref>). It might also be possible to use ensemble simulations to provide an initial ensemble to help determine <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e120" xlink:type="simple"/></inline-formula>. To our knowledge, this method has not yet been applied to molecular simulation, and the practical applicability of the approach therefore remains to be established.</p>
<p>Finally, we note that an alternative approach has very recently been suggested to derive structural ensembles from noisy, ensemble-averaged experimental data <xref ref-type="bibr" rid="pcbi.1003406-Olsson1">[51]</xref>. This method is an extension of the Bayesian inferential structure determination method that includes ensemble averaging via a hierarchical model, and it attempts to find an ensemble that is least biased compared to prior knowledge (e.g., a force field) and that simultaneously is compatible with the experimental data.</p>
</sec></sec></sec><sec id="s4">
<title>Discussion</title>
<p>The three new papers highlighted in this Perspectives article have provided substantial new insights to the field of molecular simulation under experimental restraints. Of particular interest is the result that the current common practice of replica-averaged simulations is tightly linked to the solution prescribed by the maximum entropy formalism. This link provides an attractive way forward for the field. Replica-averaged simulations have a substantial track record and have in many cases been shown to improve the quality of structural ensemblesâe.g., measured through cross-validation with unrelated experimental dataâand to provide new biological insights. The relationship with the maximum entropy solution suggests that the restrained ensemble can be regarded as the proper thermodynamic ensemble that represents a system when both the energy and some additional experimental data are known. As such, the system-specific force field correction introduced by the restraints, when applied appropriately, may be viewed as a natural extension of the Boltzmann ensemble when one is provided with additional information beyond the energy.</p>
<p>In addition to the theoretical developments highlighted in this article, an important area for future studies is how best to implement them in practice. In the case of data without noise it is, for example, not clear how many replicas are needed in practice to recover an ensemble that is close to the maximum entropy solution. Another question is related to the steepness of the potential used to implement the restraint: how narrow should the potential be to mimic the appropriate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e121" xlink:type="simple"/></inline-formula>-function that will ensure the maximum entropy correspondence? Previous work has either found optimal values of the number of replicas by cross-validation, or simply chosen a sufficiently large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003406.e122" xlink:type="simple"/></inline-formula> to obtain convergence. As both illustrated by theoretical results <xref ref-type="bibr" rid="pcbi.1003406-Roux1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Cavalli1">[11]</xref> and our own simulations (<xref ref-type="fig" rid="pcbi-1003406-g002">Figure 2</xref>), it is, however, necessary to choose the force constant sufficiently large to converge to the maximum entropy solution. The results also show that that one can reach apparent convergence at lower values of the force constant, but that the resulting distribution in this case will not be the maximum entropy solution. For these and related problems, we also need better methods to check for convergence, both to study the effect of varying these restraint-parameters and to monitor and ensure sufficient sampling of the ensembles.</p>
<p>Another topic that remains incompletely understood is how best to deal with uncertainties in the observed data. Cavalli et al. provide a possible path in this direction, and in this paper, we have sketched out a few potential alternatives. From a theoretical viewpoint, it seems desirable to combine Bayesian inference, which provides a robust toolbox for dealing with noisy data, with the maximum entropy principle for deriving probability distributions in underdetermined systems. There already exists a large literature on these topics in other disciplines, but further studies and applications on real systems are necessary to shed further light on which methods are most useful in biological simulations.</p>
<p>As we have here hinted, the problem of uncertainties in the data appears to be related to the problem of determining the relative weight between force field and restraint-potential. A relevant question in this context is whether such a weight can be meaningfully defined and assigned without considering the inherent accuracy of the force field itself. Intuitively, if the force field in question is a preliminary implementation, it should be weighed lower than if it has been carefully parameterized against a large amount of data. This degree of trust is currently not encoded in the force fields commonly employed in simulations. In principle, this information could be specified by providing distributions (or at least variances) for all estimated parameters in the force field, in the spirit of Bayesian inference, allowing the inference machinery to deduce or integrate out the relevant weights. In this way, a force field would no longer be characterized by a single set of parameters, but instead as a âdistribution of force fields.â There are significant challenges associated with estimating and sampling from such models, but recent work provides hope for the eventual feasibility of such an approach. First, advances in techniques for force field optimization <xref ref-type="bibr" rid="pcbi.1003406-Norgaard1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Wang1">[52]</xref> allow for a Bayesian approach to integrate experimental data and, e.g., quantum-level data, bringing us closer to the ability to probe the uncertainties associated with individual parameters. Second, on the sampling front, inferential structure determination has demonstrated how (small numbers of) parameters can be successfully integrated out during a simulation <xref ref-type="bibr" rid="pcbi.1003406-Rieping1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Habeck1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003406-Olsson2">[53]</xref>. Thus, we envisage that in future applications it might be possible to integrate out not only experimental noise and ânuisance parameters,â but potentially also the uncertainty associated with the parameterization of a force field. We note that distributed computing platforms may be particularly well suited to sample from such models as one might need to perform multiple, independent simulations that differ only slightly in the force field used.</p>
<p>We also point out that ensemble simulations inherently have more unfavourable data-to-parameter ratios than standard methods for structure determination. As such, they may in particular benefit from improved force fields, and we expect that as force fields continue to improve it should become possible to study more complex systems with less experimental information. Importantly, a consistent theoretical framework should allow us to transition smoothly between traditional, mostly data-driven methods for structure determination and molecular simulations in the absence of any experimental data.</p>
<p>Finally, we note that although the developments described here have focused on restraining molecular simulations with experimental data, maximum entropy methods have a broad range of applications both in biology and beyond. We envisage that new theoretical developments, such as the link between ensemble simulations and maximum entropy solutions, can be directly applicable in other fields. Similarly, new methods for deriving modified models in the context of noisy data should have broad applicability. For example, the recent advances in predicting structural contacts from a maximum entropyâbased analysis of the covariation of sites in a multiple sequence alignment <xref ref-type="bibr" rid="pcbi.1003406-Morcos1">[6]</xref> should benefit greatly from improved techniques for handling the uncertainty associated with limited sequence numbers.</p>
</sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003406-Jaynes1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name> (<year>1957</year>) <article-title>Information theory and statistical mechanics</article-title>. <source>Phys Rev</source> <volume>106</volume>: <fpage>620</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-GranotAtedgi1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Granot-Atedgi</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Tkaik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>Stimulus-dependent maximum entropy models of neural population codes</article-title>. <source>PLOS Comput Biol</source> <volume>9</volume>: <fpage>e1002922</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002922" xlink:type="simple">10.1371/journal.pcbi.1002922</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003406-Locasale1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Locasale</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Wolf-Yadlin</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>Maximum entropy reconstructions of dynamic signaling networks from quantitative proteomics data</article-title>. <source>PLOS One</source> <volume>4</volume>: <fpage>e6522</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0006522" xlink:type="simple">10.1371/journal.pone.0006522</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003406-Carmichael1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carmichael</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Shell</surname><given-names>MS</given-names></name> (<year>2012</year>) <article-title>A new multiscale algorithm and its application to coarse-grained peptide models for self-assembly</article-title>. <source>J Phys Chem B</source> <volume>116</volume>: <fpage>8383</fpage>â<lpage>8393</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Lapedes1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lapedes</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Giraud</surname><given-names>BG</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Stormo</surname><given-names>GD</given-names></name> (<year>1999</year>) <article-title>Correlated mutations in models of protein sequences: phylogenetic and structural effects</article-title>. <source>Lect Notes Monogr Ser</source> <volume>33</volume>: <fpage>236</fpage>â<lpage>256</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Morcos1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morcos</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Pagnani</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lunt</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Bertolino</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Marks</surname><given-names>DS</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>108</volume>: <fpage>E1293</fpage>â<lpage>E1301</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Klepeis1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klepeis</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dror</surname><given-names>RO</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>DE</given-names></name> (<year>2009</year>) <article-title>Long-timescale molecular dynamics simulations of protein structure and function</article-title>. <source>Curr Opin Struct Biol</source> <volume>19</volume>: <fpage>120</fpage>â<lpage>127</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Norgaard1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Norgaard</surname><given-names>AB</given-names></name>, <name name-style="western"><surname>Ferkinghoff-Borg</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name> (<year>2008</year>) <article-title>Experimental parameterization of an energy function for the simulation of unfolded proteins</article-title>. <source>Biophys J</source> <volume>94</volume>: <fpage>182</fpage>â<lpage>192</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Pitera1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pitera</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Chodera</surname><given-names>JD</given-names></name> (<year>2012</year>) <article-title>On the use of experimental observations to bias simulated ensembles</article-title>. <source>J Chem Theory Comput</source> <volume>8</volume>: <fpage>3445</fpage>â<lpage>3451</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Roux1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roux</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Weare</surname><given-names>J</given-names></name> (<year>2013</year>) <article-title>On the statistical equivalence of restrained-ensemble simulations with the maximum entropy method</article-title>. <source>J Chem Phys</source> <volume>138</volume>: <fpage>084107</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Cavalli1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cavalli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Camilloni</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Vendruscolo</surname><given-names>M</given-names></name> (<year>2013</year>) <article-title>Molecular dynamics simulations with replica averaged structural restraints generate structural ensembles according to the maximum entropy principle</article-title>. <source>J Chem Phys</source> <volume>138</volume>: <fpage>094112</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Shannon1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shannon</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Weaver</surname><given-names>W</given-names></name> (<year>1948</year>) <article-title>A mathematical theory of communication</article-title>. <source>Bell System Technical Journal</source> <volume>27</volume>: <fpage>379</fpage>â<lpage>423, 623â656</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Kullback1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kullback</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Leibler</surname><given-names>RA</given-names></name> (<year>1951</year>) <article-title>On information and sufficiency</article-title>. <source>Ann Math Statist</source> <volume>22</volume>: <fpage>79</fpage>â<lpage>86</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Jaynes2"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name> (<year>1968</year>) <article-title>Prior probabilities</article-title>. <source>IEEE Trans Syst Sci Cybern</source> <volume>4</volume>: <fpage>227</fpage>â<lpage>241</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Hobson1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hobson</surname><given-names>A</given-names></name> (<year>1969</year>) <article-title>A new theorem of information theory</article-title>. <source>J Stat Phys</source> <volume>1</volume>: <fpage>383</fpage>â<lpage>391</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Shore1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shore</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>R</given-names></name> (<year>1980</year>) <article-title>Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy</article-title>. <source>IEEE Trans Inf Theory</source> <volume>26</volume>: <fpage>26</fpage>â<lpage>37</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Jaynes3"><label>17</label>
<mixed-citation publication-type="book" xlink:type="simple">Jaynes E (1979) Where do we stand on maximum entropy? In: Levine R, Tribus M, editors. The Maximum Entropy Formalism. Cambridge, MA: MIT Press. pp. 1â104.</mixed-citation>
</ref>
<ref id="pcbi.1003406-LindorffLarsen1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Maragakis</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Piana</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Eastwood</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Dror</surname><given-names>RO</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Systematic validation of protein force fields against experimental data</article-title>. <source>PLOS One</source> <volume>7</volume>: <fpage>e32131</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0032131" xlink:type="simple">10.1371/journal.pone.0032131</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003406-LeaverFay1"><label>19</label>
<mixed-citation publication-type="book" xlink:type="simple">Leaver-Fay A, O'Meara MJ, Tyka M, Jacak R, Song Y, <etal>et al</etal>.. (2013) Chapter six - scientific benchmarks for guiding macromolecular energy function improvement. In: Keating AE, editor. Methods in Protein Design, Academic Press, Volume 523 of Methods in Enzymology. pp. 109â143.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Best1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Best</surname><given-names>RB</given-names></name>, <name name-style="western"><surname>Hummer</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>Optimized molecular dynamics force fields applied to the helix-coil transition of polypeptides</article-title>. <source>J Phys Chem B</source> <volume>113</volume>: <fpage>9004</fpage>â<lpage>9015</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Tyka1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tyka</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Jung</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Efficient sampling of protein conformational space using fast loop building and batch minimization on highly parallel computers</article-title>. <source>J Comput Chem</source> <volume>33</volume>: <fpage>2483</fpage>â<lpage>2491</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Raval1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raval</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Piana</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Eastwood</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Dror</surname><given-names>RO</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>DE</given-names></name> (<year>2012</year>) <article-title>Refinement of protein structure homology models via long, all-atom molecular dynamics simulations</article-title>. <source>Proteins</source> <volume>80</volume>: <fpage>2071</fpage>â<lpage>2079</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Rieping1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rieping</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Habeck</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nilges</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Inferential structure determination</article-title>. <source>Science</source> <volume>309</volume>: <fpage>303</fpage>â<lpage>306</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Habeck1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Habeck</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rieping</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Nilges</surname><given-names>M</given-names></name> (<year>2006</year>) <article-title>Weighting of experimental evidence in macromolecular structure determination</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>: <fpage>1756</fpage>â<lpage>1761</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Vendruscolo1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vendruscolo</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Determination of conformationally heterogeneous states of proteins</article-title>. <source>Curr Opin Struct Biol</source> <volume>17</volume>: <fpage>15</fpage>â<lpage>20</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Scheek1"><label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">Scheek RM, Torda AE, Kemmink J, van Gunsteren WF (1991) Structure determination by NMR: The modeling of NMR parameters as ensemble averages. In: Hoch JC, Poulsen FM, Redfield C, editors. Computational aspects of the Study of Biological Macromolecules by Nuclear Magnetic Resonance Spectroscopy. New York: Plenum Press. pp. 209â217.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Kim1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Prestegard</surname><given-names>J</given-names></name> (<year>1989</year>) <article-title>A dynamic model for the structure of acyl carrier protein in solution</article-title>. <source>Biochemistry</source> <volume>28</volume>: <fpage>8792</fpage>â<lpage>8797</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Kuriyan1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuriyan</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ãsapay</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Burley</surname><given-names>SK</given-names></name>, <name name-style="western"><surname>BrÃ¼nger</surname><given-names>AT</given-names></name>, <name name-style="western"><surname>Hendrickson</surname><given-names>WA</given-names></name>, <etal>et al</etal>. (<year>1991</year>) <article-title>Exploration of disorder in protein structures by x-ray restrained molecular dynamics</article-title>. <source>Proteins</source> <volume>10</volume>: <fpage>340</fpage>â<lpage>358</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Levin1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levin</surname><given-names>EJ</given-names></name>, <name name-style="western"><surname>Kondrashov</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Wesenberg</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>GN</given-names></name> (<year>2007</year>) <article-title>Ensemble refinement of protein crystal structures: validation and application</article-title>. <source>Structure</source> <volume>15</volume>: <fpage>1040</fpage>â<lpage>1052</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Burnley1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burnley</surname><given-names>BT</given-names></name>, <name name-style="western"><surname>Afonine</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>PD</given-names></name>, <name name-style="western"><surname>Gros</surname><given-names>P</given-names></name> (<year>2012</year>) <article-title>Modelling dynamics in protein crystal structures by ensemble refinement</article-title>. <source>eLife</source> <volume>1</volume>: <fpage>e00311</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-LindorffLarsen2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Best</surname><given-names>RB</given-names></name>, <name name-style="western"><surname>DePristo</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Dobson</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Vendruscolo</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Simultaneous determination of protein structure and dynamics</article-title>. <source>Nature</source> <volume>433</volume>: <fpage>128</fpage>â<lpage>132</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Lange1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lange</surname><given-names>OF</given-names></name>, <name name-style="western"><surname>Lakomek</surname><given-names>NA</given-names></name>, <name name-style="western"><surname>FarÃ¨s</surname><given-names>C</given-names></name>, <name name-style="western"><surname>SchrÃ¶der</surname><given-names>GF</given-names></name>, <name name-style="western"><surname>Walter</surname><given-names>KF</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Recognition dynamics up to microseconds revealed from an rdc-derived ubiquitin ensemble in solution</article-title>. <source>Science</source> <volume>320</volume>: <fpage>1471</fpage>â<lpage>1475</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Roux2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roux</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Islam</surname><given-names>SM</given-names></name> (<year>2013</year>) <article-title>Restrained-ensemble molecular dynamics simulations based on distance histograms from double electronâelectron resonance spectroscopy</article-title>. <source>J Phys Chem B</source> <volume>117</volume>: <fpage>4733</fpage>â<lpage>4739</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-LindorffLarsen3"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kristjansdottir</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Teilum</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Fieber</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dobson</surname><given-names>CM</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Determination of an ensemble of structures representing the denatured state of the bovine acyl-coenzyme A binding protein</article-title>. <source>J Am Chem Soc</source> <volume>126</volume>: <fpage>3291</fpage>â<lpage>3299</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Jo1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jo</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Im</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Transmembrane helix orientation and dynamics: insights from ensemble dynamics with solid-state nmr observables</article-title>. <source>Biophys J</source> <volume>100</volume>: <fpage>2913</fpage>â<lpage>2921</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Dedmon1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dedmon</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Christodoulou</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Vendruscolo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dobson</surname><given-names>CM</given-names></name> (<year>2005</year>) <article-title>Mapping long-range interactions in Î±-synuclein using spin-label NMR and ensemble molecular dynamics simulations</article-title>. <source>J Am Chem Soc</source> <volume>127</volume>: <fpage>476</fpage>â<lpage>477</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Richter1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Richter</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Gsponer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>VÃ¡rnai</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Salvatella</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Vendruscolo</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>The MUMO (minimal underrestraining minimal over-restraining) method for the determination of native state ensembles of proteins</article-title>. <source>J Biomol NMR</source> <volume>37</volume>: <fpage>117</fpage>â<lpage>135</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-LindorffLarsen4"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindorff-Larsen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ferkinghoff-Borg</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Similarity measures for protein ensembles</article-title>. <source>PLOS One</source> <volume>4</volume>: <fpage>e4203</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0004203" xlink:type="simple">10.1371/journal.pone.0004203</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003406-Burling1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burling</surname><given-names>FT</given-names></name>, <name name-style="western"><surname>Weis</surname><given-names>WI</given-names></name>, <name name-style="western"><surname>Flaherty</surname><given-names>KM</given-names></name>, <name name-style="western"><surname>Brunger</surname><given-names>AT</given-names></name> (<year>1996</year>) <article-title>Direct observation of protein salvation and discrete disorder with experimental crystallographic phases</article-title>. <source>Science</source> <volume>271</volume>: <fpage>72</fpage>â<lpage>77</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-vanGunsteren1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Gunsteren</surname><given-names>WF</given-names></name>, <name name-style="western"><surname>Dolenc</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mark</surname><given-names>AE</given-names></name> (<year>2008</year>) <article-title>Molecular simulation as an aid to experimentalists</article-title>. <source>Curr Opin Struct Biol</source> <volume>18</volume>: <fpage>149</fpage>â<lpage>153</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Bricogne1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bricogne</surname><given-names>G</given-names></name> (<year>1984</year>) <article-title>Maximum entropy and the foundations of direct methods</article-title>. <source>Acta Crystallogr A</source> <volume>40</volume>: <fpage>410</fpage>â<lpage>445</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Rycki1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>RÃ³z`ycki</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>YC</given-names></name>, <name name-style="western"><surname>Hummer</surname><given-names>G</given-names></name> (<year>2011</year>) <article-title>SAXS ensemble refinement of ESCRT-III CHMP3 conformational transitions</article-title>. <source>Structure</source> <volume>19</volume>: <fpage>109</fpage>â<lpage>116</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Groth1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groth</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Malicka</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Czaplewski</surname><given-names>C</given-names></name>, <name name-style="western"><surname>OÅdziej</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Åankiewicz</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>1999</year>) <article-title>Maximum entropy approach to the determination of solution conformation of exible polypeptides by global conformational analysis and NMR spectroscopy â application to DNS<sup>1</sup>-c-[D-A<sub>2</sub>bu<sup>2</sup>, Trp<sup>4</sup>, Leu<sup>5</sup>]-enkephalin and DNS<sup>1</sup>-c-[D-A<sub>2</sub>bu<sup>2</sup>, Trp<sup>4</sup>, D-Leu<sup>5</sup>] enkephalin</article-title>. <source>J Biomol NMR</source> <volume>15</volume>: <fpage>315</fpage>â<lpage>330</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Massad1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Massad</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Jarvet</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tanner</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Tomson</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Smirnova</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Maximum entropy reconstruction of joint <italic>Ï</italic>, <italic>Ï</italic>-distribution with a coil-library prior: the backbone conformation of the peptide hormone motilin in aqueous solution from <italic>Ï</italic> and <italic>Ï</italic>-dependent J-couplings</article-title>. <source>J Biomol NMR</source> <volume>38</volume>: <fpage>107</fpage>â<lpage>123</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Genheden1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Genheden</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ryde</surname><given-names>U</given-names></name> (<year>2012</year>) <article-title>Will molecular dynamics simulations of proteins ever reach equilibrium?</article-title> <source>Phys Chem Chem Phys</source> <volume>14</volume>: <fpage>8662</fpage>â<lpage>8677</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Thomas1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thomas</surname><given-names>MU</given-names></name> (<year>1979</year>) <article-title>Technical note â a generalized maximum entropy principle</article-title>. <source>Oper Res</source> <volume>27</volume>: <fpage>1188</fpage>â<lpage>1196</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Golan1"><label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">Golan A, Judge G, Miller D (1996) Maximum entropy econometrics: robust estimation with limited data. Series in financial economics and quantitative analysis. San Francisco: Wiley. 307 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Dudk1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DudÃ­k</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Schapire</surname><given-names>RE</given-names></name> (<year>2007</year>) <article-title>Maximum entropy density estimation with generalized regularization and an application to species distribution modeling</article-title>. <source>J Mach Learn Res</source> <volume>8</volume>: <fpage>1217</fpage>â<lpage>1260</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Donoho1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donoho</surname><given-names>DL</given-names></name>, <name name-style="western"><surname>Johnstone</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Hoch</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Stern</surname><given-names>AS</given-names></name> (<year>1992</year>) <article-title>Maximum entropy and the nearly black object</article-title>. <source>J R Stat Soc Series B Stat Methodol</source> <volume>54</volume>: <fpage>41</fpage>â<lpage>81</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Gull1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gull</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Daniell</surname><given-names>G</given-names></name> (<year>1978</year>) <article-title>Image reconstruction from incomplete and noisy data</article-title>. <source>Nature</source> <volume>272</volume>: <fpage>686</fpage>â<lpage>690</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Olsson1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olsson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Frellsen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Boomsma</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Mardia</surname><given-names>KV</given-names></name>, <name name-style="western"><surname>Hamelryck</surname><given-names>T</given-names></name> (<year>2013</year>) <article-title>Inference of structure ensembles from sparse, averaged data</article-title>. <source>PLOS One</source> <volume>8</volume>: <fpage>e79439</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/101371/journal.pone0079439" xlink:type="simple">101371/journal.pone0079439</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003406-Wang1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>LP</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Van Voorhis</surname><given-names>T</given-names></name> (<year>2012</year>) <article-title>Systematic parametrization of polarizable force fields from quantum chemistry data</article-title>. <source>J Chem Theory Comput</source> <volume>9</volume>: <fpage>452</fpage>â<lpage>460</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003406-Olsson2"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olsson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Boomsma</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Frellsen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bottaro</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Harder</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Generative probabilistic models extend the scope of inferential structure determination</article-title>. <source>Journal of Magnetic Resonance</source> <volume>213</volume>: <fpage>182</fpage>â<lpage>186</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>