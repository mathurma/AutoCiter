<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01525</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005508</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal to noise ratio</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical noise</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Approximation methods</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</article-title>
<alt-title alt-title-type="running-head">Representational models</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0264-8532</contrib-id>
<name name-style="western">
<surname>Diedrichsen</surname> <given-names>Jörn</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7433-9005</contrib-id>
<name name-style="western">
<surname>Kriegeskorte</surname> <given-names>Nikolaus</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Brain and Mind Institute, Department for Computer Science, Department for Statistical and Actuarial Science, Western University, London, Canada</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Cognitive and Brain Sciences Unit, Cambridge University, Cambridge, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Cichy</surname> <given-names>Radoslaw</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Freie Universitat Berlin, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Data curation:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Visualization:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> JD NK.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> JD NK.</p>
</list-item>
</list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">jdiedric@uwo.ca</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>24</day>
<month>4</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>4</issue>
<elocation-id>e1005508</elocation-id>
<history>
<date date-type="received">
<day>20</day>
<month>9</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>4</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Diedrichsen, Kriegeskorte</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005508"/>
<abstract>
<p>Representational models specify how activity patterns in populations of neurons (or, more generally, in multivariate brain-activity measurements) relate to sensory stimuli, motor responses, or cognitive processes. In an experimental context, representational models can be defined as hypotheses about the distribution of activity profiles across experimental conditions. Currently, three different methods are being used to test such hypotheses: encoding analysis, pattern component modeling (PCM), and representational similarity analysis (RSA). Here we develop a common mathematical framework for understanding the relationship of these three methods, which share one core commonality: all three evaluate the second moment of the distribution of activity profiles, which determines the representational geometry, and thus how well any feature can be decoded from population activity. Using simulated data for three different experimental designs, we compare the power of the methods to adjudicate between competing representational models. PCM implements a likelihood-ratio test and therefore provides the most powerful test if its assumptions hold. However, the other two approaches—when conducted appropriately—can perform similarly. In encoding analysis, the linear model needs to be appropriately regularized, which effectively imposes a prior on the activity profiles. With such a prior, an encoding model specifies a well-defined distribution of activity profiles. In RSA, the unequal variances and statistical dependencies of the dissimilarity estimates need to be taken into account to reach near-optimal power in inference. The three methods render different aspects of the information explicit (e.g. single-response tuning in encoding analysis and population-response representational dissimilarity in RSA) and have specific advantages in terms of computational demands, ease of use, and extensibility. The three methods are properly construed as complementary components of a single data-analytical toolkit for understanding neural representations on the basis of multivariate brain-activity data.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Modern neuroscience can measure activity of many neurons or the local blood oxygenation of many brain locations simultaneously. As the number of simultaneous measurements grows, we can better investigate how the brain represents and transforms information, to enable perception, cognition, and behavior. Recent studies go beyond showing <italic>that</italic> a brain region is involved in some function. They use representational models that specify <italic>how</italic> different perceptions, cognitions, and actions are encoded in brain-activity patterns. In this paper, we provide a general mathematical framework for such representational models, which clarifies the relationships between three different methods that are currently used in the neuroscience community. All three methods evaluate the same core feature of the data, but each has distinct advantages and disadvantages. Pattern component modelling (PCM) implements the most powerful test between models, and is analytically tractable and expandable. Representational similarity analysis (RSA) provides a highly useful summary statistic (the dissimilarity) and enables model comparison with weaker distributional assumptions. Finally, encoding models characterize individual responses and enable the study of their layout across cortex. We argue that these methods should be considered components of a larger toolkit for testing hypotheses about the way the brain represents information.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>James S. McDonnell Foundation (US)</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0264-8532</contrib-id>
<name name-style="western">
<surname>Diedrichsen</surname> <given-names>Jörn</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000038</institution-id>
<institution>Natural Sciences and Engineering Research Council of Canada</institution>
</institution-wrap>
</funding-source>
<award-id>RGPIN-2016-04890</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0264-8532</contrib-id>
<name name-style="western">
<surname>Diedrichsen</surname> <given-names>Jörn</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>European Research Council (BE)</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7433-9005</contrib-id>
<name name-style="western">
<surname>Kriegeskorte</surname> <given-names>Nikolaus</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by: James S McDonnell Foundation, Scholar award for understanding human cognition to JD; Natural Sciences and Engineering Research Council of Canada, Discovery grant, RGPIN-2016-04890 to JD; European Research Council Starting Grant, ERC-2010-StG 261352 to NK, and the UK Medical Research Council, Programme MC-A060- 5PR20 to NK. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="2"/>
<page-count count="33"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-05-08</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data and algorithms are available in the github repositories rsa_toolbox and pcm_toolbox.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The measurement of brain activity is rapidly advancing in terms of spatial and temporal resolution, and in terms of the number of responses that can be measured simultaneously [<xref ref-type="bibr" rid="pcbi.1005508.ref001">1</xref>]. Modern electrode arrays and calcium imaging enable the recording of hundreds of neurons in parallel. Electrophysiological signals that reflect summaries of the population activity can be recorded using both invasive (e.g. the local field potential, LFP) and non-invasive techniques (e.g. scalp electrophysiological measurements) at increasingly high spatial resolution. Modern functional magnetic resonance imaging (fMRI) enables us to measure hemodynamic activity in hundreds of thousands of voxels across the entire human brain at sub-millimeter resolution.</p>
<p>In order to translate advances in brain-activity measurement into advances in computational theory [<xref ref-type="bibr" rid="pcbi.1005508.ref002">2</xref>], researchers increasingly seek to test representational models that capture both what information is represented in a population of neurons, and how it is represented. Knowing the content and format of representations provides strong constraints for computational models of brain information processing. We refer to hypotheses about the content and format of brain representations as <italic>representational models</italic>, and address here the important methodological question of how to best test such models.</p>
<p>Referring to an activity pattern as a “representation” constitutes a functional interpretation [<xref ref-type="bibr" rid="pcbi.1005508.ref003">3</xref>], which requires not only that the represented variable (such as a perceptual property, some cognitive content, or an action parameter) is encoded in the pattern of activity in a format that can be read out by downstream neurons, but also that the information is actually used by other brain regions and, thus, <italic>serves a functional purpose</italic> [<xref ref-type="bibr" rid="pcbi.1005508.ref004">4</xref>]. The representational interpretation therefore ultimately needs to be supported by evidence for a cause-and-effect relationship between the activity and downstream neural and behavioral responses. Testing causal effects of activity patterns is beyond the scope of the observational methods considered in this paper. However, we note that a good brain-computational model must, as a necessary condition, be able to explain the format in which information it is encoded in the task-relevant brain regions.</p>
<p>For a population code to constitute an <italic>explicit representation</italic>, another area must be able to read out the represented variable directly using a neurobiologically plausible readout mechanism, such as linear or radial-basis-function decoding [<xref ref-type="bibr" rid="pcbi.1005508.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref006">6</xref>]. Note that this definition of explicit does not restrict us to highly localized codes, such as the “grandmother neuron” [<xref ref-type="bibr" rid="pcbi.1005508.ref007">7</xref>], but encompasses widely distributed codes.</p>
<p>An example of an implicit representation is the representation of object category in the retina. The retina clearly contains information about object category, and an aspect of its function is to convey this information. However, it does not <italic>explicitly represent</italic> object category. Multiple stages of nonlinear tranformation along the ventral visual stream are required to render the category of an object explicit. Inferior temporal cortex contains a representation of object category [<xref ref-type="bibr" rid="pcbi.1005508.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref009">9</xref>], along with representations of much additional information [<xref ref-type="bibr" rid="pcbi.1005508.ref010">10</xref>].</p>
<p>Many researchers have used linear decoding methods to reveal explicit information in neural representations [<xref ref-type="bibr" rid="pcbi.1005508.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref013">13</xref>]. Representational models, as considered here, go one step further: they fully characterize the representational geometry, defining all represented features in a region, how strongly each of them is represented (signal to noise ratio), and how the activity patterns associated with different features relate to each other. Representational models therefore fully specify the representational content of an area.</p>
<p>To define representational models formally, we need to consider two complementary perspectives on activity data, as illustrated in <xref ref-type="fig" rid="pcbi.1005508.g001">Fig 1</xref>. The activity of many neurons, or more generally <italic>measurement channels</italic> (neurons, electrodes, or fMRI voxels), can be measured across a range of <italic>experimental conditions</italic> (stimuli, movements, or tasks). Thus, each channel will have an <italic>activity profile</italic>, which can be plotted as a point in the space spanned by the experimental conditions (<xref ref-type="fig" rid="pcbi.1005508.g001">Fig 1B</xref>). A representational model specifies a probability distribution of activity profiles in the space spanned by the experimental conditions. It treats the true activity profiles as a random variable and predicts, for each possible activity profile, the probability of observing a measurement channel exhibiting that profile. It does not predict the activity profile for each individual channel actually measured. The motivation for this approach derives from the idea that the computational function of a region does not depend on specific neurons having specific response properties, but on the fact that certain features can be read out from the population by downstream neurons. The probability distribution over activity profiles determines which features can be read out from the code and the signal-to-noise ratio of the readout. By basing further analyses on the probability distribution of the activity profiles, we disregard (1) which neuron fulfills which function, and (2) where neurons are located within a cortical area. Furthermore, by focusing on the second moment of the distribution we ignore (3) the degree to which the information about a given represented feature is concentrated in a few neurons (as in single-cell selectivity for a represented feature) or spread out over the population. Ignoring these aspects may be viewed as an advantage or a disadvantage, depending on the level of description that a researcher is interested in. We argue that treating activity profiles as random vectors is a simplification that is useful for drawing computational insights from population activity measurements.</p>
<fig id="pcbi.1005508.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Two complementary perspectives on population activity.</title>
<p>(<bold>A</bold>) The multivariate activity data can be viewed as a set of activity profiles (columns) or as a set of activity patterns (rows). An activity profile is a vector of responses of a single channel across experimental conditions. An activity pattern is a vector of responses across all channels for a single condition. Activity data can be visualized by plotting activity profiles as points in a space defined by the experimental conditions (B,D), or by plotting the activity patterns as points in a space defined by the measurement channels (C,E). (<bold>B</bold>) If the activities are uncorrelated between conditions, then (<bold>C</bold>) the corresponding activity patterns of all three conditions are equidistant to each other, and can be equally well distinguished. (<bold>D</bold>) If the activities are positively correlated for two conditions that elicit similar regional-mean activation (conditions 2 and 3 here), then (<bold>E</bold>) the activity patterns for these conditions are closer to each other and can be less well distinguished.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g001" xlink:type="simple"/>
</fig>
<p>In this paper, we show that the multivariate <italic>second moment</italic> of the activity profiles fully defines the representational geometry and with it all the information that can linearly or nonlinearly decoded. In particular, under the assumption of Gaussian noise the second moment determines the signal-to-noise ratio with which any feature can be decoded. We discuss three established methods for adjudicating between representational models: encoding analysis, pattern-component modeling (PCM) and representational similarity analysis (RSA, see <xref ref-type="table" rid="pcbi.1005508.t001">Table 1</xref>). We show that these three techniques all exclusively rely on information contained in the second moment. This core commonality enables us to consider these methods in the same formal framework.</p>
<table-wrap id="pcbi.1005508.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.t001</object-id>
<label>Table 1</label>
<caption>
<title>Comparison of encoding analysis with regularization, pattern component modelling (PCM), and representational dissimilarity analysis (RSA).</title>
</caption>
<alternatives>
<graphic id="pcbi.1005508.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left">Encoding analysis</th>
<th align="left">PCM</th>
<th align="left">RSA</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Model definition</td>
<td align="left">Model-feature matrix <bold>M</bold>, regularization / prior</td>
<td align="left">Predicted second-moments matrix (<bold>G</bold>)</td>
<td align="left">Representational dissimilarity matrix (RDM)</td>
</tr>
<tr>
<td align="left">First-level parameters (characterizing individual activity profiles)</td>
<td align="left">One weight per feature and measurement channel</td>
<td align="left">None; integrated out in the likelihood</td>
<td align="left">None; integrated out when calculating dissimilarities</td>
</tr>
<tr>
<td align="left">Second-level parameters (characterizing the distribution of activity profiles)</td>
<td align="left">Regularization / Ridge coefficient (determined by noise / signal ratio)</td>
<td align="left">Scale parameter s, Noise variance</td>
<td align="left">Scaling between predicted and observed distances (s)</td>
</tr>
<tr>
<td align="left">Prediction target</td>
<td align="left">Responses to test conditions</td>
<td align="left">Distribution of measurement channels in activity-profile space</td>
<td align="left">Dissimilarities among activity patterns</td>
</tr>
<tr>
<td align="left">Training data required</td>
<td align="left">always</td>
<td align="left">not for fixed models, only if additional second-level parameters are to be fitted</td>
<td align="left">not for fixed models, only if additional second-level parameters are to be fitted</td>
</tr>
<tr>
<td align="left">Explicit likelihood for fitting additional model parameters</td>
<td align="left">No—need to do nested within crossvalidation</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
</tr>
<tr>
<td align="left">Fitting algorithms for model parameters</td>
<td align="left">-</td>
<td align="left">EM Gradient descent Newton-Raphson</td>
<td align="left">Linear and non-negative regression IRLS</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In <italic>encoding analysis</italic> [<xref ref-type="bibr" rid="pcbi.1005508.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref015">15</xref>], representational models are defined in terms of the underlying <italic>features</italic> (<xref ref-type="fig" rid="pcbi.1005508.g002">Fig 2A</xref>). Each activity profile can be characterized by a linear combination of such features. Examples include Gabor filters [<xref ref-type="bibr" rid="pcbi.1005508.ref016">16</xref>] (for a low-level visual representation), abstract semantic dimensions [<xref ref-type="bibr" rid="pcbi.1005508.ref017">17</xref>] (for a cognitive representation), and force, direction or hand position [<xref ref-type="bibr" rid="pcbi.1005508.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref020">20</xref>] (for a movement representation). The importance of each feature in each channel is measured by a feature weight. Feature weights are considered first-level parameters in our framework, as they describe the individual activity profiles, as opposed to second-level parameters that describe the distribution of the activity profiles (<xref ref-type="table" rid="pcbi.1005508.t001">Table 1</xref>). The large number of parameters (number of features in the model times number of channels in the measurements) engenders a danger of overfitting. Encoding models are therefore commonly evaluated using cross-validation: The feature weights are estimated on a training set, and the model is evaluated in terms of its performance at predicting left-out data [<xref ref-type="bibr" rid="pcbi.1005508.ref014">14</xref>]. The test data may consist in a sample of experimental conditions not used in training, so as to test the model’s generalization performance [<xref ref-type="bibr" rid="pcbi.1005508.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref016">16</xref>]. While many studies use simple linear regression to estimate the weights [<xref ref-type="bibr" rid="pcbi.1005508.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref021">21</xref>], it is increasingly common to use a regularization penalty (for example the L2 norm of the vector of weights) [<xref ref-type="bibr" rid="pcbi.1005508.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref017">17</xref>]. We will show that regularization is not merely a technical trick used in fitting a given model. Instead, the regularization (and its implicit distributional assumptions) are an essential part of the representational hypothesis that is tested. Without it, encoding models do not specify a probability distribution with a finite second moment and thus do not define the linear decodability of different features.</p>
<fig id="pcbi.1005508.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Three approaches to testing representational models.</title>
<p>(<bold>A</bold>) In encoding analysis, the distribution of activity profiles is described by the underlying features (red vectors). The direction of a feature vector determines the associated activity profile, and the length the strength of the feature encoding in the representation. (<bold>B</bold>) PCM models the distribution of the activity profiles as a multivariate Gaussian. This model is parametrized by the second moment of the activity profiles, which determines at what signal-to-noise ratio any feature is linearly decodable from the population. (<bold>C</bold>) RSA uses the representational distances (or, more generally, dissimilarities) between activity patterns as a summary statistic to describe decodability and hence the second moment of the underlying distribution.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g002" xlink:type="simple"/>
</fig>
<p><italic>Pattern component modeling</italic> [<xref ref-type="bibr" rid="pcbi.1005508.ref022">22</xref>] is based on an explicit generative model of the process that produced the data and can be considered a Bayesian approach. The true activity profiles are assumed to have a multivariate Gaussian distribution in the space spanned by the experimental conditions (<xref ref-type="fig" rid="pcbi.1005508.g002">Fig 2B</xref>). This formulation enables us to evaluate the marginal likelihood of the observed activity profiles under the probability distribution specified by the model. Thus, we do not fit any first-level parameters (feature weights) and hence reduce the risk of overfitting. This enables us to compare models with different numbers of features without having to correct for model complexity. If the assumptions of the generative model hold, PCM implements the likelihood-ratio test between models [<xref ref-type="bibr" rid="pcbi.1005508.ref023">23</xref>], which by the Neyman-Pearson lemma [<xref ref-type="bibr" rid="pcbi.1005508.ref024">24</xref>], is the most powerful test of its size. In theory, therefore, PCM should yield more accurate inferences than any of its competitors, that is it should be able to more sensitively adjudicate among competing models.</p>
<p>Finally, <italic>representational similarity analysis</italic> (RSA [<xref ref-type="bibr" rid="pcbi.1005508.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref026">26</xref>]) approaches the problem from a complementary perspective. Rather than considering the activity profiles of the measurement channels as points in the space spanned by the conditions (<xref ref-type="fig" rid="pcbi.1005508.g001">Fig 1B and 1D</xref>), it considers the activity patterns associated with the experimental conditions as points in the space spanned by the measurement channels (<xref ref-type="fig" rid="pcbi.1005508.g001">Fig 1C and 1E</xref>). RSA then uses the representational distances (<xref ref-type="fig" rid="pcbi.1005508.g002">Fig 2C</xref>) between the conditions as a summary statistic. We will see that these distances again exclusively depend on the second moment of the distribution of activity profiles. Having obtained a matrix of dissimilarities between activity patterns (the representational dissimilarity matrix, RDM), RSA then tests models by comparing the observed distances to the distances predicted by each representational model. This can be done by calculating rank-based correlations [<xref ref-type="bibr" rid="pcbi.1005508.ref027">27</xref>] or Pearson correlations [<xref ref-type="bibr" rid="pcbi.1005508.ref028">28</xref>]. Here we show that for near-optimal inferences it is important to take the co-dependence structure of the distance estimates into account, for example by using a multivariate normal approximation to the joint distribution of the cross-validated Mahalanobis distances [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref030">30</xref>].</p>
<p>In the remainder of the paper, we first introduce the second moment of the activity profiles and explain why it is the sufficient statistic of the representational geometry and thus of linear and nonlinear decodability. We then define the three methods in detail, and show how they related to the second moment. Finally, using simulated data and models taken from our fMRI work, we assess the statistical efficiency, i.e. how well these methods adjudicate between two or more competing representational models given limited data. We also compare the methods in terms of their computational efficiency.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec003">
<title>Basic definitions</title>
<p>All symbols used in the following derivations are summarized in <xref ref-type="table" rid="pcbi.1005508.t002">Table 2</xref>. First, we define <bold>U</bold> to be the matrix of noiseless activity profiles with <italic>K</italic> (number of experimental conditions) rows and <italic>P</italic> (number of measurement channels) columns. Each row of this matrix is an activity pattern, the response of the whole population to a single condition. Each column of this matrix is an activity profile (<xref ref-type="fig" rid="pcbi.1005508.g001">Fig 1A</xref>).</p>
<table-wrap id="pcbi.1005508.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.t002</object-id>
<label>Table 2</label>
<caption>
<title>Notation used.</title><p>For non-scalars, the second column indicates the vector / matrix size.</p>
</caption>
<alternatives>
<graphic id="pcbi.1005508.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<tbody>
<tr>
<td align="left"><italic>K</italic></td>
<td align="left"/>
<td align="left">Number of conditions</td>
</tr>
<tr>
<td align="left"><italic>M</italic></td>
<td align="left"/>
<td align="left">Number of independent partitions (imaging runs)</td>
</tr>
<tr>
<td align="left"><italic>P</italic></td>
<td align="left"/>
<td align="left">Number of measurement channels (voxels, electrodes,neurons)</td>
</tr>
<tr>
<td align="left"><italic>N</italic></td>
<td align="left"/>
<td align="left">Overall number of measurements (<italic>N<sub>m</sub></italic> × <italic>M</italic>)</td>
</tr>
<tr>
<td align="left"><italic>Q</italic></td>
<td align="left"/>
<td align="left">Number of features in model</td>
</tr>
<tr>
<td align="left"><bold>U</bold></td>
<td align="left"><italic>K</italic> × <italic>P</italic></td>
<td align="left">Matrix of true activation patterns</td>
</tr>
<tr>
<td align="left"><bold>u</bold><sub><bold>i</bold>,.</sub></td>
<td align="left"><italic>1</italic> × <italic>P</italic></td>
<td align="left">Activation pattern for condition <italic>i</italic>; <italic>i</italic><sup>th</sup> row of <bold>U</bold></td>
</tr>
<tr>
<td align="left"><bold>u</bold><sub>.,<bold>j</bold></sub></td>
<td align="left"><italic>K</italic> × <italic>1</italic></td>
<td align="left">Activation profile for measurement channel <italic>j</italic>; <italic>j</italic><sup><italic>th</italic></sup> column of <bold>U</bold></td>
</tr>
<tr>
<td align="left">
<inline-formula id="pcbi.1005508.e001">
<alternatives>
<graphic id="pcbi.1005508.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e001" xlink:type="simple"/>
<mml:math display="inline" id="M1">
<mml:msup>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="bold">U</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>m</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="left"><italic>K</italic> × <italic>P</italic></td>
<td align="left">Matrix of estimated activity patterns, based on data from partition <italic>m</italic></td>
</tr>
<tr>
<td align="left">
<inline-formula id="pcbi.1005508.e002">
<alternatives>
<graphic id="pcbi.1005508.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e002" xlink:type="simple"/>
<mml:math display="inline" id="M2">
<mml:msup>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="bold">U</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mo>∼</mml:mo>
<mml:mi>m</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="left"><italic>K</italic> × <italic>P</italic></td>
<td align="left">Model prediction for activity patterns, based on data independent of <italic>m</italic></td>
</tr>
<tr>
<td align="left"><bold>M</bold></td>
<td align="left"><italic>K</italic> × <italic>Q</italic></td>
<td align="left">Matrix of model features for all condition</td>
</tr>
<tr>
<td align="left"><bold>W</bold></td>
<td align="left"><italic>Q</italic> × <italic>P</italic></td>
<td align="left">Matrix of voxel weights for each feature</td>
</tr>
<tr>
<td align="left">Ω</td>
<td align="left"><italic>Q</italic> × <italic>Q</italic></td>
<td align="left">Second moment of <bold>W</bold></td>
</tr>
<tr>
<td align="left"><bold>Y</bold></td>
<td align="left"><italic>N</italic> × <italic>P</italic></td>
<td align="left">Matrix of brain measurements, concatenated activity estimates or time series data</td>
</tr>
<tr>
<td align="left"><bold>Z</bold></td>
<td align="left"><italic>N</italic> × <italic>K</italic></td>
<td align="left">Design matrix, indicating how measurements relate to activity patterns</td>
</tr>
<tr>
<td align="left"><bold>X</bold></td>
<td align="left"><italic>N</italic> × <italic>R</italic></td>
<td align="left">Design matrix containing <italic>n</italic> regressors of no-interest</td>
</tr>
<tr>
<td align="left"><bold>G</bold></td>
<td align="left"><italic>K</italic> × <italic>K</italic></td>
<td align="left">Second moment of <bold>U</bold></td>
</tr>
<tr>
<td align="left">d<sub>i,k</sub></td>
<td align="left"/>
<td align="left">Distance between condition <italic>i</italic> and <italic>k</italic></td>
</tr>
<tr>
<td align="left"><italic>J</italic></td>
<td align="left"/>
<td align="left">Number of distances, normally <italic>K</italic>(<italic>K</italic>−1)/2</td>
</tr>
<tr>
<td align="left"><bold>D</bold></td>
<td align="left"><italic>K</italic> × <italic>K</italic></td>
<td align="left">Representational dissimilarity matrix of all pairwise distances</td>
</tr>
<tr>
<td align="left"><bold>d</bold></td>
<td align="left"><italic>J</italic> × 1</td>
<td align="left">Vector of all pairwise distances</td>
</tr>
<tr>
<td align="left">
<inline-formula id="pcbi.1005508.e003">
<alternatives>
<graphic id="pcbi.1005508.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e003" xlink:type="simple"/>
<mml:math display="inline" id="M3">
<mml:mover accent="true">
<mml:mi mathvariant="bold">d</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="left"><italic>J</italic> × 1</td>
<td align="left">Vector of predicted distances</td>
</tr>
<tr>
<td align="left"><bold>C</bold></td>
<td align="left"><italic>J</italic> × <italic>K</italic></td>
<td align="left">Contrast matrix, defining the J pairwise differences between conditions</td>
</tr>
<tr>
<td align="left"><bold>Σ</bold><sub><italic>P</italic></sub></td>
<td align="left"><italic>P</italic> × <italic>P</italic></td>
<td align="left">Variance-covariance matrix between the <italic>P</italic> voxels</td>
</tr>
<tr>
<td align="left"><bold>Σ</bold><sub><italic>K</italic></sub></td>
<td align="left"><italic>K</italic> × <italic>K</italic></td>
<td align="left">Variance-covariance matrix of the columns of <inline-formula id="pcbi.1005508.e004"><alternatives><graphic id="pcbi.1005508.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left"><bold>V</bold></td>
<td align="left"><italic>N</italic> × <italic>N</italic></td>
<td align="left">Variance-covariance matrix of <bold>Y</bold></td>
</tr>
<tr>
<td align="left"><bold>S</bold></td>
<td align="left"><italic>J</italic> × <italic>J</italic></td>
<td align="left">Variance-covariance matrix of all pair-wise distances</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Because we are interested in the distribution of activity profiles, but not in the activity profiles per se, we consider the columns of <bold>U</bold> to be a random variable. This is an essential step underlying our common framework, which is justified by the fact that, for the purpose of reading out information, the different measurement channels are exchangeable (see <xref ref-type="sec" rid="sec001">introduction</xref>). We assume that the activity profiles are repeatedly measured, with the data consisting of <italic>M</italic> independent partitions, each containing at least one activity measurement for each condition and measurement channel. In the context of fMRI, a partition will consist of a separate phase of data acquisition, e.g. a scanner run. The activity estimates <inline-formula id="pcbi.1005508.e005"><alternatives><graphic id="pcbi.1005508.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> of partition <italic>m</italic> are the true patterns <bold>U</bold> plus noise <bold>E</bold><sup>(<italic>m</italic>)</sup>. The noise captures both neural trial-by-trial variability of the activity pattern in a single condition, as well as measurement noise.
<disp-formula id="pcbi.1005508.e006"><alternatives><graphic id="pcbi.1005508.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">U</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi mathvariant="bold">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula></p>
<p>For the purposes of this paper, we assume that the noise is Gaussian, and independent and identically distributed (i.i.d.) across conditions and partitions (homoscedasticity). Possible dependence within each partition, however, can be easily accounted for [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref031">31</xref>].</p>
</sec>
<sec id="sec004">
<title>Dependence between measurement channels</title>
<p>The discussion below further assumes that the noise is also i.i.d. across different measurement channels (isotropicity). However, noise in fMRI, MEG, and even invasive electrophysiology exhibits strong correlations between neighboring locations in the brain. To account for these dependencies, we employ multivariate noise normalization (i.e. spatial prewhitening), which has been shown to increase the reliability of inference [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>]. Across all measurement channels, we estimate the <italic>P</italic> × <italic>P</italic> variance-covariance matrix across trials, <bold>Σ</bold><sub><italic>P</italic></sub> and then regularize the estimate by shrinking it towards a diagonal matrix [<xref ref-type="bibr" rid="pcbi.1005508.ref033">33</xref>]. In the context of fMRI, we can use the residual time series from the fitting of the time-series model to estimate noise covariance [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref034">34</xref>]. We then post-multiply our activity estimates by <inline-formula id="pcbi.1005508.e007"><alternatives><graphic id="pcbi.1005508.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mover accent="true"><mml:mo>Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>P</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, rendering the model errors in the channels approximately uncorrelated. If multivariate noise normalization is not performed or is incomplete, inference will be suboptimal in all three methods (for details see [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>]).</p>
</sec>
<sec id="sec005">
<title>Second moment matrix and decodeability</title>
<p>In this section, we show that, under the assumption of Gaussian noise, the <italic>second moment</italic> of the activity profiles fully characterizes the decodability of any feature that describes the experimental conditions. The fact that the second moment determines what can be decoded provides a motivation, from the perspective of brain computation, for using the second moment matrix as a summary statistic.</p>
<p>The <italic>n</italic><sup><italic>th</italic></sup> moment of a scalar random variable <italic>u</italic> is <italic>E</italic>(<italic>u</italic><sup><italic>n</italic></sup>), where <italic>E</italic>() denotes the expected value. Here we use a multivariate extension of the concept, with the second moment of the random vector <bold>u</bold> defined as the matrix <italic>E</italic>(<bold>u</bold><bold>u</bold><sup><italic>T</italic></sup>), the expected outer product of the activity profiles, where the expectation is across the measurement channels. The second-moment matrix of the activity profiles is given by
<disp-formula id="pcbi.1005508.e008"><alternatives><graphic id="pcbi.1005508.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>≡</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>P</mml:mi></mml:munderover> <mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow> <mml:mo>/</mml:mo> <mml:mi>P</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">U</mml:mi> <mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:mi>P</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>Thus, each cell of this matrix contains the scaled inner product of two activity patterns.</p>
<p>Before calculating <bold>G</bold>, some investigators subtract the mean activity across measurement channels for each condition from the data. In this case, <xref ref-type="disp-formula" rid="pcbi.1005508.e008">Eq 2</xref> becomes the variance-covariance matrix of the activity profiles –the second moment around the mean activity profile. Here we do not remove the mean, but use the second moment around zero. From the perspective of a neuron that reads out the activity pattern of an area, any difference between activity patterns across conditions can be used to decode information. Some features (for example, stimulus intensity) may be encoded in the mean activity over all measurement channels. Other properties (for example, stimulus identity) may be encoded in relative activity differences, with some measurement channels responding more to one condition, and others to a different condition. The second moment around zero captures both of these potentially meaningful differences.</p>
<p>Any feature of the conditions that we might want to decode can be defined by a <italic>K</italic> × 1 vector <bold>f</bold> with one entry per condition, which describes how the feature varies across conditions. To obtain a linear read-out estimate <inline-formula id="pcbi.1005508.e009"><alternatives><graphic id="pcbi.1005508.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mover accent="true"><mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for the feature <italic>f</italic><sub><italic>i</italic></sub> for a given condition <italic>i</italic>, we weight each channel’s observed activity using the <italic>P</italic> × 1 read-out vector <bold>v</bold>:
<disp-formula id="pcbi.1005508.e010"><alternatives><graphic id="pcbi.1005508.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:msub> <mml:mi mathvariant="bold">v</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>We would like the estimate <inline-formula id="pcbi.1005508.e011"><alternatives><graphic id="pcbi.1005508.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mover accent="true"><mml:mi>f</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> to have very different values for two trials that differ on the feature value, while showing small differences for trials that have the same feature value. <bold>f</bold><sup><italic>T</italic></sup> <bold>U</bold> is the pattern that encodes the feature. We are looking for the readout vector <bold>v</bold> that maximizes the ratio <italic>S</italic> between the sum-of-squares of the readout of the feature and the sum-of-square of the readout of the noise:
<disp-formula id="pcbi.1005508.e012"><alternatives><graphic id="pcbi.1005508.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mtext mathvariant="bold">ff</mml:mtext> <mml:mi>T</mml:mi></mml:msup> <mml:mtext mathvariant="bold">Uv</mml:mtext></mml:mrow> <mml:mrow><mml:msup><mml:mi mathvariant="bold">v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">E</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mtext mathvariant="bold">ff</mml:mtext> <mml:mi>T</mml:mi></mml:msup> <mml:mtext mathvariant="bold">Ev</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>The solution to this equation is commonly known as Fisher’s linear discriminant [<xref ref-type="bibr" rid="pcbi.1005508.ref035">35</xref>], which, under the assumption of homoscedastic Gaussian noise, is the best achievable linear decoder. If the noise is isotropic (or the data is adequately pre-whitened), then <bold>E</bold><sup><italic>T</italic></sup> <bold>f</bold><bold>f</bold><sup><italic>T</italic></sup> <bold>E</bold> = <bold>I</bold><italic>b</italic>, where <italic>b</italic> is a constant. The denominator then depends only on the norm of the read-out vector <bold>v</bold>, not on its direction, and can be ignored when <bold>v</bold> is constrained to have a norm of 1. The best readout vector <bold>v</bold> is then given by the first eigenvector of the matrix <bold>U</bold><sup><italic>T</italic></sup> <bold>f</bold><bold>f</bold><sup><italic>T</italic></sup> <bold>U</bold>, and the quality of the best readout is determined by the corresponding eigenvalue.</p>
<p>Non-zero eigenvalues (<italic>eig</italic>) of a square matrix are invariant to cyclic permutations of the product order:
<disp-formula id="pcbi.1005508.e013"><alternatives><graphic id="pcbi.1005508.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>e</mml:mi> <mml:mi>i</mml:mi> <mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mtext mathvariant="bold">ff</mml:mtext> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">U</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mi>e</mml:mi> <mml:mi>i</mml:mi> <mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mrow><mml:msup><mml:mi mathvariant="bold">f</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mtext mathvariant="bold">UU</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">f</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mspace width="4pt"/><mml:mi>e</mml:mi> <mml:mi>i</mml:mi> <mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi mathvariant="bold">f</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mtext mathvariant="bold">Gf</mml:mtext></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>Therefore, the quality of the best linear decoder for <italic>any</italic> feature (as defined by <bold>f</bold>) is fully characterized by <bold>f</bold><sup><italic>T</italic></sup> <bold>G</bold><bold>f</bold>.</p>
<p>By the same logic, the second moment determines decodeability in general. Assume you want to use a non-linear decoder to estimate feature <bold>f</bold>. For this to be possible, one has to ensure that one can decode the difference between any two conditions that have different feature values. Once that distinction is made, one can use a arbitrary non-linear function (i.e. table lookup) to read out the feature for each condition. Because the second moment defines the decodeability of any pair of two experimental conditions, it also defines whether a feature can be read out non-linearly.</p>
<p>Importantly, the second moment is only a sufficient statistic for decodeability under the assumption that the readout neurons can integrate information from the entire population that constitutes the code, i.e., it as capable of any arbitrary linear transform of the input data. If the readout neuron is only partially connected, it becomes important to what extent particular information is concentrated in restricted sets of neurons. This information is captured in the higher moments of the activity profile distribution, a point to which we will return in the Discussion. For a fully connected readout neuron that can weight activities in any arbitrary way, the second moment is a sufficient statistic of the decodable information.</p>
</sec>
<sec id="sec006">
<title>Representational analysis in the context of fMRI</title>
<p>The methods in this paper were first developed in the context of fMRI data analysis, and our examples will come from this domain. A simple way to apply the analyses to fMRI data is to use as activity estimates (<inline-formula id="pcbi.1005508.e014"><alternatives><graphic id="pcbi.1005508.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>) the regression coefficients, or “beta”-weights, from a first-level time series analysis [<xref ref-type="bibr" rid="pcbi.1005508.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref037">37</xref>]. The time-series model accounts for the hemodynamic lag and the temporal autocorrelation of the noise. The activity estimates usually express the difference in activity during a condition relative to rest. Activity estimates commonly co-vary together across fMRI imaging runs, because all activity estimates within a partition are measured relative to the same resting baseline. This positive correlation can be reduced by subtracting, within each partition, the mean activity pattern (across conditions) from each activity pattern. This makes the mean of each measurement channel (across condition) zero and thus centers the ensemble of points in activity-pattern space that is centered on the origin.</p>
<p>Rather than using the concatenated activity estimates from different partitions, encoding analysis and PCM can also be applied directly to time series data. As a universal notation that encompasses both situations, we can use a standard linear mixed model [<xref ref-type="bibr" rid="pcbi.1005508.ref038">38</xref>]:
<disp-formula id="pcbi.1005508.e015"><alternatives><graphic id="pcbi.1005508.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>=</mml:mo> <mml:mtext mathvariant="bold">ZU</mml:mtext> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">XB</mml:mtext> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
where <bold>Y</bold> is an <italic>N</italic> × <italic>P</italic> matrix of all activity measurements, <bold>Z</bold> the <italic>N</italic> × <italic>K</italic> design matrix, which relates the activity measurements to the <italic>K</italic> experimental conditions, and <bold>X</bold> is a second design matrix for nuisance variables. <bold>U</bold> is the <italic>K</italic> × <italic>P</italic> matrix of activity patterns (the random effects), <bold>B</bold> are the regression coefficients for these nuisance variables (the fixed effects), and <bold>E</bold> is the matrix of measurement errors. If the data <bold>Y</bold> are the concatenated activity estimates, the nuisance variables typically only model the mean pattern for each run. If <bold>Y</bold> consists of time-series data, the nuisance variables typically capture additional effects such as time-series drifts and residual head-motion-related artifacts.</p>
</sec>
<sec id="sec007">
<title>Representational analysis in the context of neurophysiological recordings</title>
<p>All three methods can also be applied to recordings of single cell activity or neurophysiological potentials [<xref ref-type="bibr" rid="pcbi.1005508.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref025">25</xref>]. The activity estimates can then be firing rates estimated over a temporal window for each trial, or the power in different frequency bands over time. Because the trial-by-trial variability of firing rates will usually increase with the mean firing rate, it is advisable to use the square root of firing rates to make the data conform better to the assumption that the variance of the noise is independent of the signal [<xref ref-type="bibr" rid="pcbi.1005508.ref039">39</xref>].</p>
<p>Here we focus on models that treat the activity patterns <bold>U</bold> as static snapshots. To exploit the temporal detail provided by electrophysiological recordings, the analyses can be either performed using a sliding window over the time course of the trial [<xref ref-type="bibr" rid="pcbi.1005508.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref042">42</xref>], or by “stacking” the time series and conditions, resulting in a activity matrix with <italic>TK</italic> rows [<xref ref-type="bibr" rid="pcbi.1005508.ref043">43</xref>].</p>
</sec>
<sec id="sec008">
<title>Encoding analysis</title>
<p>An encoding model characterizes the structure of the representation in terms of a set of features [<xref ref-type="bibr" rid="pcbi.1005508.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref017">17</xref>]. We will show in the following that encoding models are representational models as definied by the second moment of the activity profiles. For this to be the case, however, the use of regularized regression is a crticial factor. We will therefore first present the encoding apporach in general, and then show why regularisation is important to test for distributions with a definied second moment.</p>
<p>In general. the value of each feature for each experimental condition is coded in the <italic>model matrix</italic> <bold>M</bold> (<italic>K</italic> conditions by <italic>Q</italic> features). The <italic>feature weight matrix</italic> <bold>W</bold> (<italic>Q</italic> features by <italic>P</italic> channels) then determines how the different model features contribute to the activity profiles of different measurement channels to produce the predicted activity patterns <bold>U</bold>:
<disp-formula id="pcbi.1005508.e016"><alternatives><graphic id="pcbi.1005508.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>=</mml:mo> <mml:mtext mathvariant="bold">MW</mml:mtext> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>Geometrically, we can think of the features as the basis vectors of the subspace, in which the activity profiles reside (<xref ref-type="fig" rid="pcbi.1005508.g002">Fig 2A</xref>).</p>
<sec id="sec009">
<title>Encoding analysis without regularization</title>
<p>To adjudicate among encoding models of different numbers of features—and hence different numbers of free first-level parameters—most researchers use independent test sets [<xref ref-type="bibr" rid="pcbi.1005508.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref017">17</xref>]. A training data set is used to estimate the feature weights for each channel, and the resulting prediction is then evaluated on a held-out test data set. This can be implemented in a statistically efficient manner by using cross-validation, which is usually performed by holding out a single partition (e.g. fMRI imaging run) as a test set, and using the remaining <italic>M</italic>-1 partitions as the training set. Each partition is held out as the test set once and prediction performance is averaged across the <italic>M</italic> folds of cross-validation. Encoding models can also make predictions about conditions that are not in the training set (<xref ref-type="sec" rid="sec023">Discussion</xref>). However, we focus our simulations on cases, in which training and test sets include the same experimental conditions.</p>
<p>The weights can be chosen to minimize the sum of squared errors on the training data, i.e. using linear regression:
<disp-formula id="pcbi.1005508.e017"><alternatives><graphic id="pcbi.1005508.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mstyle> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>M</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>M</mml:mi></mml:mstyle></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>M</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mover accent="true"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle> <mml:mo stretchy="true">^</mml:mo></mml:mover> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mo>~</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:math></alternatives> <label>(8)</label></disp-formula>
where we define <inline-formula id="pcbi.1005508.e018"><alternatives><graphic id="pcbi.1005508.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>∼</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> to be the average activity estimates from all partitions except <italic>m</italic>. The prediction for the left-out test data of run <italic>m</italic> is
<disp-formula id="pcbi.1005508.e019"><alternatives><graphic id="pcbi.1005508.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>∼</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">M</mml:mi> <mml:mover accent="true"><mml:mi mathvariant="bold">W</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>The accuracy of the prediction can be assessed by relating the residual sums-of-squares (SSR) of the prediction to the total sums-of-squares (SST) of the observed activities, summed over all partitions, conditions, and voxels
<disp-formula id="pcbi.1005508.e020"><alternatives><graphic id="pcbi.1005508.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mtext>SSR</mml:mtext> <mml:mtext>SST</mml:mtext></mml:mfrac> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow/></mml:msubsup> <mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>∼</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow/></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula></p>
<p>Alternatively, we can evaluate the prediction by correlating the predicted and observed activity patterns across all conditions and channels. Assuming that the mean of each channel across all conditions is zero (given mean pattern subtraction), the correlation is given by
<disp-formula id="pcbi.1005508.e021"><alternatives><graphic id="pcbi.1005508.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mfenced close=")" open="("><mml:mi>m</mml:mi></mml:mfenced></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:mo>∼</mml:mo> <mml:mi>m</mml:mi></mml:mfenced></mml:msubsup></mml:mrow></mml:mrow> <mml:msqrt><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mfenced close=")" open="("><mml:mi>m</mml:mi></mml:mfenced> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow/></mml:msubsup> <mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mo>∼</mml:mo> <mml:mi>m</mml:mi></mml:mfenced> <mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula></p>
<p>The correlation introduces an arbitrary scaling factor between prediction and observations and, in contrast to <xref ref-type="disp-formula" rid="pcbi.1005508.e020">Eq 10</xref>, allows the model to over- or under-predict the data by a scalar factor without penalty. Encoding analysis can also be applied directly to the time-series data without an intervening model (<xref ref-type="disp-formula" rid="pcbi.1005508.e015">Eq 6</xref>).</p>
<p>To understand how encoding analysis adjudicates between models, consider the graphical representation of the estimation process (<xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3</xref>). In this example, the training data the activity profile of a single measurement channel, which can be visualized as a point in activity-profile space (black cross). Regression analysis can be understood as the orthogonal projection of the measured activity profile onto the linear subspace spanned by the features of the model. The two models depicted in <xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3A and 3B</xref> have different features (blue arrows) that define different subspaces (planes with blue outlines). Therefore, the training data is projected onto two different planes and the prediction for the test data differs between the two models. The model with a subspace that better describes the cloud of activation profiles will make better predictions overall across the measurement channels, show lower cross-validation error, and will hence be more likely selected as the winning model.</p>
<fig id="pcbi.1005508.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Adjudicating between encoding models with and without regularization.</title>
<p>The axes of the three-dimensional space are formed by the response to three experimental conditions. The activity profile of each unit defines a point in this space. Models are defined by their features (blue arrows) and (when using regularization) a prior distribution of the weights for these features. The features and the prior, together, define a distribution of activity profiles (ellipsoids indicate an iso-probability-density contours of the Gaussian distributions). To predict the activity profile of a single measurement channel, the model is fitted to the training data set (cross). Simple regression finds the shortest projection (black dot) onto the subspace defined by the features, whereas regression with regularization (red dot) biases the prediction towards the predicted distribution. Two models (<bold>A, B</bold>) with features that span different model subspaces are distinguishable using regression without regularization. (<bold>C</bold>) This model spans the same subspace as model A. Unregularized regression results in the same projection as for model A, whereas regression with regularization leads to a different projection. (<bold>D</bold>) A saturated model with as many features as conditions. Unregularized regression can perfectly fit any data point (cross and black dot coincide). With regularization, the prediction is biased towards the predicted distribution (iso-probability-density ellipsoid).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g003" xlink:type="simple"/>
</fig>
<p>Importantly, encoding analysis without regularization compares the subspaces of the competing models, but not their probability distributions. For example, the model depicted in <xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3C</xref> predicts a different distribution than the one in <xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3A</xref>. The features of these two models, however, span the same subspace. Therefore, without regularization, the predictions of these two models are identical (black dots) and the models indistinguishable.</p>
</sec>
<sec id="sec010">
<title>Encoding analysis with regularization</title>
<p>When using regularized regression, encoding analysis evaluates models according to their predicted distribution of activity profiles. From a Bayesian perspective, regularization can be motivated by assuming a prior probability distribution on the weight vectors <bold>w</bold><sub>.,<italic>i</italic></sub> the columns of <bold>W</bold>. Specifically, L2-norm (Tikhonov) regularization is equivalent to assuming a multivariate Gaussian prior with zero mean and variance-covariance matrix <bold>Ω</bold>. Under this assumption, the predicted second moment of the activity profiles is
<disp-formula id="pcbi.1005508.e022"><alternatives><graphic id="pcbi.1005508.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mtext mathvariant="bold">MWW</mml:mtext> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">M</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:mi>P</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">M</mml:mi> <mml:mo>Ω</mml:mo> <mml:msup><mml:mi mathvariant="bold">M</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula></p>
<p>Thus, the model features together with the prior distributional assumption on the feature weights define a probability distribution over activity profiles. For example, a representational model of motor cortical activity could be defined by assuming that the features are individual units with cosine-tuning for different movement directions [<xref ref-type="bibr" rid="pcbi.1005508.ref018">18</xref>], and that (as a prior) the preferred directions of the units are uniformly distributed.</p>
<p>In practice, we allow a scalar factor, <italic>s</italic>, between the predicted and measured second moment. This accounts for the fact that different subjects or regions will have different signal levels and that hence the distribution of activity profiles have different widths. Under the assumption that the feature weights come from a multivariate Gaussian distribution with variance <bold>Ω</bold><italic>s</italic>, the best linear unbiased predictor (BLUP, [<xref ref-type="bibr" rid="pcbi.1005508.ref044">44</xref>]), i.e. the predictor that minimizes the squared error on the held-out data is:
<disp-formula id="pcbi.1005508.e023"><alternatives><graphic id="pcbi.1005508.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mstyle> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>M</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>M</mml:mi></mml:mstyle> <mml:mo>+</mml:mo> <mml:msup><mml:mo>Ω</mml:mo> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ϵ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>M</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mover accent="true"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>U</mml:mi></mml:mstyle> <mml:mo stretchy="true">^</mml:mo></mml:mover> <mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mo>~</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives> <label>(13)</label></disp-formula>
where <inline-formula id="pcbi.1005508.e024"><alternatives><graphic id="pcbi.1005508.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>ϵ</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the noise variance on the observations. The strength of regularization is determined by the ratio of this noise variance and the variance of the signal <bold>Ω</bold><italic>s</italic>, consistent with Bayesian inference of the weights on the basis of the prior and the data.</p>
<p>After assuming a prior on the model weights, the two models depicted in <xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3A and 3C</xref> predict different distributions of the activation profiles. When estimating the weights (<xref ref-type="disp-formula" rid="pcbi.1005508.e023">Eq 13</xref>), the activity profiles are projected onto the space spanned by <bold>M</bold>, but this time biased (red dot) towards the denser part of the model-predicted distribution of activity profiles. As a result, the two models make different predictions. An accurate prior will help the model generalize to the held-out data; an inaccurate prior will hurt generalization performance. The model with the distribution that is closest to the true distribution of activity profiles will yield the best cross-validation performance (as measured by <italic>R</italic><sup>2</sup> or <italic>r</italic>). When using regularized regression (<xref ref-type="disp-formula" rid="pcbi.1005508.e023">Eq 13</xref>), models can also have as many features as conditions (<xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3D</xref>), or even more features than conditions. When using unregularized regression, such <italic>saturated</italic> models are indistinguishable from each other. They become distinct only after adding weight-distributional priors.</p>
<p>Because regularization is equivalent to imposing a prior on the feature weights, it is not just a technical trick for estimation. Instead the prior is an integral part of the hypothesis being evaluated as it co-determines (together with the features) the probability distribution over activity profiles that the model predicts. Therefore, we will refer to encoding models evaluated using regularized regression analysis in the following as “encoding models with a prior”.</p>
<p>One important consequence of <xref ref-type="disp-formula" rid="pcbi.1005508.e022">Eq 12</xref> is that the same representational model can be defined using different feature sets. Because a representational model is defined by its second moment, two feature sets <bold>M</bold><sub>1</sub> and <bold>M</bold><sub>2</sub>, combined with corresponding second moment matrices of the weights, <bold>Ω</bold><sub><bold>1</bold></sub> and <bold>Ω</bold><sub><bold>2</bold></sub>, define the same representational model, if
<disp-formula id="pcbi.1005508.e025"><alternatives><graphic id="pcbi.1005508.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">M</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mo>Ω</mml:mo> <mml:mn>1</mml:mn></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">M</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">M</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mo>Ω</mml:mo> <mml:mn>2</mml:mn></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">M</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula></p>
<p>Thus, an important caveat when using encoding models is that one does not compare different feature sets per se—but rather different distributions (when using regularization) or different subspaces of activity profiles (when not using regularization). The winning model in either case can be equivalently re-expressed using a different feature set. Interpretation, therefore, must consider the model-predicted distributions or subspaces of activity profiles, not the particular feature basis set chosen (as the latter is not unique for any given representational model).</p>
<p>Technically, this also means that regression with a Gaussian prior can be implemented using ridge regression [<xref ref-type="bibr" rid="pcbi.1005508.ref045">45</xref>]. The equivalence is established by scaling and rotating the model matrix <bold>M</bold> in such a way that <bold>Ω</bold> becomes the identity matrix. Any representational model can be brought into this diagonal form by setting the columns of <bold>M</bold> to the eigenvectors of <bold>G</bold>, each one multiplied by the square root of the corresponding eigenvalue:
<disp-formula id="pcbi.1005508.e026"><alternatives><graphic id="pcbi.1005508.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi> <mml:mo>=</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msqrt><mml:msub><mml:mo>λ</mml:mo> <mml:mn>1</mml:mn></mml:msub></mml:msqrt> <mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mo>…</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold">v</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msqrt><mml:msub><mml:mo>λ</mml:mo> <mml:mn>2</mml:mn></mml:msub></mml:msqrt> <mml:mspace width="4pt"/></mml:mfenced> <mml:mi mathvariant="bold">G</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">M</mml:mi> <mml:msup><mml:mi mathvariant="bold">M</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula></p>
<p>The strength of the regularization is determined by a scalar ridge coefficient defined by <inline-formula id="pcbi.1005508.e027"><alternatives><graphic id="pcbi.1005508.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ε</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. For an encoding model with regularization, the ridge coefficient still needs to be determined for each cross-validation fold. This can be done again by nested cross-validation [<xref ref-type="bibr" rid="pcbi.1005508.ref016">16</xref>], generalized cross-validation [<xref ref-type="bibr" rid="pcbi.1005508.ref046">46</xref>], or restricted maximum-likelihood estimation (<xref ref-type="disp-formula" rid="pcbi.1005508.e032">Eq 18</xref>). To save time, it is also possible to use a constant regularization coefficient. For our simulations, we estimated the optimal <inline-formula id="pcbi.1005508.e028"><alternatives><graphic id="pcbi.1005508.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ε</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> by maximizing <xref ref-type="disp-formula" rid="pcbi.1005508.e032">Eq 18</xref> for the training set (across all voxels). Generalized cross-validation [<xref ref-type="bibr" rid="pcbi.1005508.ref046">46</xref>] yielded very similar results.</p>
</sec>
</sec>
<sec id="sec011">
<title>Pattern component modeling</title>
<p>An alternative to cross-validation is to evaluate the likelihood of the measured activity profiles under the representational model. This approach is taken in pattern-component modeling [<xref ref-type="bibr" rid="pcbi.1005508.ref022">22</xref>]. We start with a generative model of the activity profiles (<xref ref-type="disp-formula" rid="pcbi.1005508.e015">Eq 6</xref>). We consider the activity profiles (columns of <bold>U</bold>) to come from a multivariate Gaussian distribution with zero mean and second-moment matrix <bold>G</bold>. To account for other nuisance effects (mean activity for each partition, low-frequency drift, etc), the model also has some fixed-effects regressors (<bold>B</bold>). We are not interested in fitting <bold>U</bold> per se, but simply want to evaluate the likelihood of the data under different models, marginalized over all possible values of <bold>U</bold>. The marginal distribution for each channel (columns of matrix <bold>Y</bold>) takes the form of a multivariate normal:
<disp-formula id="pcbi.1005508.e029"><alternatives><graphic id="pcbi.1005508.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mrow><mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>∼</mml:mo> <mml:mi>N</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">X</mml:mi> <mml:msub><mml:mi mathvariant="bold">b</mml:mi> <mml:mrow><mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">V</mml:mi> <mml:mfenced close=")" open="("><mml:mi>θ</mml:mi></mml:mfenced></mml:mfenced></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi> <mml:mfenced close=")" open="("><mml:mi>θ</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mtext mathvariant="bold">ZG</mml:mtext> <mml:mi>s</mml:mi> <mml:msup><mml:mi mathvariant="bold">Z</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">I</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>=</mml:mo> <mml:mfenced close="}" open="{" separators=""><mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>The predicted covariance matrix of the activity measurements for each person is the function of the model (as encoded in the second-moment matrix <bold>G</bold>) and two second-level parameters (<italic>θ</italic>): one that determines the strength of the signal (<italic>s</italic>) and one that determines the variance of the noise (<inline-formula id="pcbi.1005508.e030"><alternatives><graphic id="pcbi.1005508.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>). In determining the likelihood, we remove the fixed effects using the residual forming matrix
<disp-formula id="pcbi.1005508.e031"><alternatives><graphic id="pcbi.1005508.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle> <mml:mo>=</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle> <mml:mo>−</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle> <mml:mrow><mml:mo>−</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:msup> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>V</mml:mi></mml:mstyle> <mml:mrow><mml:mo>−</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:msup></mml:math></alternatives> <label>(17)</label></disp-formula></p>
<p>We need to then account for the removal of these fixed effects by evaluating the restricted likelihood <italic>l</italic>(<bold>Y</bold> <bold>|</bold> <bold>G</bold>,<italic>θ</italic>) [<xref ref-type="bibr" rid="pcbi.1005508.ref047">47</xref>]:
<disp-formula id="pcbi.1005508.e032"><alternatives><graphic id="pcbi.1005508.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>N</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mfenced> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>P</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mfenced close="|" open="|"><mml:mi mathvariant="bold">V</mml:mi></mml:mfenced></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>trace</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">R</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">V</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mtext mathvariant="bold">RY</mml:mtext></mml:mfenced> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>P</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi mathvariant="bold">X</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">V</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mi mathvariant="bold">X</mml:mi></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>To evaluate the fit of a model, the scaling and noise parameters need to be determined. For fMRI data, these two parameters can vary widely between different brain regions and individuals, and are not meaningful in themselves. We therefore replace <italic>θ</italic> with point estimates that maximize <xref ref-type="disp-formula" rid="pcbi.1005508.e032">Eq 18</xref>—i.e., the approach uses Empirical Bayes, or Type-II maximum likelihood for model comparison [<xref ref-type="bibr" rid="pcbi.1005508.ref045">45</xref>]. Because every model has the same two free second-level parameters, even models that are based on different numbers of features can be compared directly. An efficient implementation of this algorithm can be found in the open-source Matlab package for PCM [<xref ref-type="bibr" rid="pcbi.1005508.ref048">48</xref>].</p>
</sec>
<sec id="sec012">
<title>Representational similarity analysis</title>
<sec id="sec013">
<title>Relationship between representational dissimilarities and second-moment matrices</title>
<p>In RSA, representational models are conceptualized in terms of the dissimilarities between the activity patterns elicited across channels by the experimental conditions (<xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3C</xref>). One important dissimilarity measure is the Euclidean distance, which is closely related to the second-moment matrix <bold>G</bold>. The squared Euclidean distance between the true activity patterns for condition <italic>i</italic> and <italic>k</italic> (normalized by the number of measurement channels) is
<disp-formula id="pcbi.1005508.e033"><alternatives><graphic id="pcbi.1005508.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:mi>P</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:mn>2</mml:mn> <mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mtext> </mml:mtext> <mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula></p>
<p>The Euclidean distance matrix is therefore a function the second moment of the activity profiles. The generalization of the Euclidean distances to non-isotropic noise is the Mahalanobis distance (see below). Correlation distances, another class of popular dissimilarity measures, can also be computed from the second-moment matrix. The cosine angle distance is defined as
<disp-formula id="pcbi.1005508.e034"><alternatives><graphic id="pcbi.1005508.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow> <mml:msqrt><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mi>k</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msqrt><mml:mrow><mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold">G</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mfrac> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula></p>
<p>Here we focus on Euclidean and Mahalanobis distances, as they are independent of the resting baseline and generally easier to interpret [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>].</p>
<p>In the following, we either represent these distances as a <italic>K × K</italic> representational dissimilarity matrix (RDM) <bold>D</bold>, or a <italic>K(K-1)/2</italic> vector <bold>d</bold> that contains all unique pairwise dissimilarities (the lower triangular entries of <bold>D</bold>). The vector of all pairwise dissimilarities can be obtained from <bold>G</bold> by defining a contrast matrix <bold>C</bold>, with each row encoding one of the pairwise contrasts, with a 1 and a −1 for the contrasted conditions and zeros elsewhere:
<disp-formula id="pcbi.1005508.e035"><alternatives><graphic id="pcbi.1005508.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>=</mml:mo> <mml:mi>d</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mtext mathvariant="bold">CG</mml:mtext> <mml:msup><mml:mi mathvariant="bold">C</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>The distances contain the same information as the second moment matrix—however, we are losing the distance of each pattern to the baseline, which was encoded on the diagonal of <bold>G</bold>. Thus, in order to go from a distance matrix to a second-moment matrix, we need to re-set the origin of the coordinate system. An obvious choice is to define the mean activity pattern across all conditions to be the baseline. This is equivalent making the sum of all rows and columns of <bold>G</bold> zero, which can be achieved by defining the centering matrix <bold>H</bold> = <bold>I</bold><sub><italic>K</italic></sub> − <bold>1</bold><sub><italic>K</italic></sub>/<italic>K</italic>, with <bold>1</bold><sub>K</sub> being a square matrix of ones. Under these conditions, <bold>G</bold> can be computed from <bold>D</bold> as
<disp-formula id="pcbi.1005508.e036"><alternatives><graphic id="pcbi.1005508.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext mathvariant="bold">HDH</mml:mtext> <mml:mspace width="4pt"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula></p>
<p>This yields the <bold>G</bold> that would be obtained if the patterns in <bold>U</bold> were centered about the origin, as can be achieved by subtracting the mean pattern from each pattern.</p>
</sec>
<sec id="sec014">
<title>Multivariate noise normalization and cross-validation: The crossnobis estimator</title>
<p>A particularly useful dissimilarity measure is the cross-validated, squared Mahalanobis distance estimator (or crossnobis estimator for short). This estimator has superior characteristics in terms of reliability and interpretability as compared to other dissimilarity measures [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>].</p>
<p>The crossnobis estimator uses multivariate noise normalization (see section Spatial dependence) to make the errors of different measurement channels approximately independent of each other. Euclidean distances (<xref ref-type="disp-formula" rid="pcbi.1005508.e033">Eq 19</xref>) computed on these pre-whitened activity estimates are equivalent to the Mahalanobis distance defined by the error-covariance matrix between channels (for details see [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>]).</p>
<p>The crossnobis estimator is cross-validated to yield an unbiased estimate of the Mahalanobis distance (assuming that the error covariance is correctly estimated). Conventional distances, which are non-negative by definition, are positively biased when estimated on noisy data: When one replaces the true activity patterns in <xref ref-type="disp-formula" rid="pcbi.1005508.e033">Eq 19</xref> with their noisy estimates, the expected value of the Euclidean distance will be always higher than the true distances, because the noise terms are squared and summed. We can obtain an unbiased estimate of the true distance by computing the difference vectors between the two activity patterns from two independent data partitions and taking the inner product of the difference vectors. Thus, if we have <italic>M</italic> independent partitions, the crossnobis estimator can be computed using a leave-one-out cross-validation scheme:
<disp-formula id="pcbi.1005508.e037"><alternatives><graphic id="pcbi.1005508.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>M</mml:mi> <mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mstyle> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:mo>−</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mstyle> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mstyle> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>~</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup> <mml:mo>−</mml:mo> <mml:msubsup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mstyle> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>~</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:mi>P</mml:mi></mml:math></alternatives> <label>(23)</label></disp-formula>
where <inline-formula id="pcbi.1005508.e038"><alternatives><graphic id="pcbi.1005508.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the prewhitened pattern for condition <italic>i</italic> measured on partition <italic>m</italic>, and <inline-formula id="pcbi.1005508.e039"><alternatives><graphic id="pcbi.1005508.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>∼</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is same activity pattern determined from the data of all other partitions. The expected value of this estimator matches the true Mahalanobis distance [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>], except for a multiplicative bias caused by inaccuracies of the error covariance. In particular, if the patterns of two conditions only differ by noise, then the expected value of this measure will be zero. We will see below that the interpretable zero point can be advantageous for adjudicating among representational models.</p>
</sec>
<sec id="sec015">
<title>Model comparison</title>
<p>In RSA, different representational models are evaluated by comparing the predicted to the observed dissimilarities. The overall magnitude of the Mahalanobis distances can vary considerably between subjects. The inter-subject variation is caused by differences in physiological responsiveness, physiological noise, and head movements—in short, by all the factors contributing to signal strength or the noise distribution, by which the Mahalanobis distance is scaled. Therefore, it is advisable to introduce a subject-specific scaling factor between observed and predicted distances, relying on the ratios between distances to distinguish models.</p>
<p>The unknown scaling of the observed dissimilarities is usually accounted for by calculating the correlation between the predicted and observed representational dissimilarity vectors (not to be confused with the use of correlation distance as an activity-pattern dissimilarity measure, <xref ref-type="disp-formula" rid="pcbi.1005508.e034">Eq 20</xref>).</p>
<p>The most cautious approach is to assume that we can only predict the rank ordering of distances [<xref ref-type="bibr" rid="pcbi.1005508.ref025">25</xref>]. It is then only appropriate to use Spearman correlation, or (in the case any of the models predict equal ranks for different pairs of conditions) Kendall’s <italic>τ</italic><sub><italic>a</italic></sub> [<xref ref-type="bibr" rid="pcbi.1005508.ref027">27</xref>]. Evaluating models based on their ordinal dissimilarity predictions is conservative in terms of assumptions. However, the lesser reliance on assumptions comes at the cost of reduced sensitivity to certain differences between models. For more quantitative models, it may be appropriate to assume that distance predictions can be made on an interval scale. The assumption of a linear relationship between the predicted and measured distances motivates the use of Pearson correlation [<xref ref-type="bibr" rid="pcbi.1005508.ref028">28</xref>]. It may be justifiable in certain cases and can increase our sensitivity to differences between representational models.</p>
<p>Both rank-based and linear correlation coefficients not only allow an arbitrary scaling factor between observed and predicted distances, but also an arbitrary additive constant due the intercept of regression. However, the crossnobis estimator has an interpretable zero point: If a model predicts a zero distance for two conditions, then a brain region explained by the model should not be sensitive to the difference between the two conditions. This is a very meaningful prediction, which we can exploit to discriminate among models. Pearson and rank-based correlation coefficients discard this information. This suggest the use of a normalized inner product, a quantity analogous to a correlation coefficient, but in which the predictions and the data are not centered about their mean:
<disp-formula id="pcbi.1005508.e040"><alternatives><graphic id="pcbi.1005508.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi mathvariant="bold">d</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>/</mml:mo> <mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:msup><mml:mi mathvariant="bold">d</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula></p>
<p>This amounts to a linear regression model between the predicted and observed distances, where the regression line is constrained to pass through the origin [<xref ref-type="bibr" rid="pcbi.1005508.ref049">49</xref>]:
<disp-formula id="pcbi.1005508.e041"><alternatives><graphic id="pcbi.1005508.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
Here <italic>s</italic> is a scaling factor that is estimated from the data by minimizing the sum-of-squared errors between predicted and observed values.</p>
<p>
<xref ref-type="disp-formula" rid="pcbi.1005508.e040">Eq 24</xref> would provide optimal inference, if all distances estimates were independent and of equal variance. However, for the crossnobis estimator (and for most other dissimilarity measures), the assumptions of independence and equal variance are both violated. Estimated squared distances with larger true values are estimated with higher variability. Furthermore, the estimated distance between conditions A and B is not independent from the estimated distances between A and C [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>]. To account for these factors, we need to know the predicted probability distribution of representational dissimilarity matrix estimates given a model. While the exact distribution of the vector of <italic>K(K-1)/2</italic> crossnobis estimates is difficult to obtain, we have shown that their distribution is well approximated by a multivariate normal distribution [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>]:
<disp-formula id="pcbi.1005508.e042"><alternatives><graphic id="pcbi.1005508.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mrow><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>N</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">S</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:mfenced></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula></p>
<p>The mean of the distribution is the true distance matrix, scaled by a parameter relating to the signal strength in this subject (<italic>s</italic>). In [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>], we showed that that the variance-covariance matrix of <bold>d</bold> is given by
<disp-formula id="pcbi.1005508.e043"><alternatives><graphic id="pcbi.1005508.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mrow><mml:mi mathvariant="bold">S</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>K</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>P</mml:mi></mml:msub></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mn>4</mml:mn> <mml:mfrac><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mi>s</mml:mi> <mml:msup><mml:mtext mathvariant="bold">CGC</mml:mtext> <mml:mi>T</mml:mi></mml:msup></mml:mfenced> <mml:mo>∘</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="bold">C</mml:mi> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>K</mml:mi></mml:msub> <mml:msup><mml:mi mathvariant="bold">C</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mfenced></mml:mrow> <mml:mi>M</mml:mi></mml:mfrac> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mfrac><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="bold">C</mml:mi> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>K</mml:mi></mml:msub> <mml:msup><mml:mi mathvariant="bold">C</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mfenced> <mml:mo>∘</mml:mo> <mml:mfenced close="]" open="[" separators=""><mml:mi mathvariant="bold">C</mml:mi> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>K</mml:mi></mml:msub> <mml:msup><mml:mi mathvariant="bold">C</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mfenced></mml:mrow> <mml:mrow><mml:mi>M</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mfrac></mml:mfenced> <mml:mfrac><mml:mrow><mml:mtext>trace</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mo>Σ</mml:mo> <mml:mi>P</mml:mi></mml:msub> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>P</mml:mi></mml:msub></mml:mfenced></mml:mrow> <mml:msup><mml:mi>P</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula></p>
<p>Where <bold>G</bold> is the predicted second-moment matrix of the patterns, <bold>C</bold> the contrast matrix that transforms the second-moment matrix into distances, and ∘ refers to the element-by-element multiplication of two matrices. <bold>Σ</bold><sub><italic>K</italic></sub> is the condition-by-condition covariance matrix of the estimates of the activation profiles across partitions, which can be estimated from the variability of the activity patterns around their mean (<inline-formula id="pcbi.1005508.e044"><alternatives><graphic id="pcbi.1005508.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>):
<disp-formula id="pcbi.1005508.e045"><alternatives><graphic id="pcbi.1005508.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mo>Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover> <mml:mi>K</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi></mml:mrow> <mml:mrow/></mml:munderover> <mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfenced> <mml:mfenced close=")" open="" separators=""><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfenced></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>/</mml:mo> <mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula></p>
<p><bold>Σ</bold><sub><italic>P</italic></sub> is the voxel-by-voxel correlation matrix of the activation estimates. If multivariate noise-normalization [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>] was completely successful, then this would be the identity matrix. However, given the shrinkage of the noise-covariance matrix used for noise-normalization, some residual correlations will remain; for accurate predictions of the variance, these must be estimated and accounted for [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>].</p>
<p>Based on this approximation we can now express the log-likelihood of the measured distances <bold>d</bold> under the model predictions <inline-formula id="pcbi.1005508.e046"><alternatives><graphic id="pcbi.1005508.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mover accent="true"><mml:mi mathvariant="bold">d</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>.
<disp-formula id="pcbi.1005508.e047"><alternatives><graphic id="pcbi.1005508.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mi>l</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>|</mml:mo> <mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:mfrac><mml:mi>D</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>S</mml:mi></mml:mstyle> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>−</mml:mo> <mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>S</mml:mi></mml:mstyle> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>−</mml:mo> <mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula></p>
<p>To evaluate the likelihood, we first need to estimate the scaling coefficient between predicted and observed distances by choosing <italic>s</italic> to maximize the likelihood. This can be done efficiently using iteratively-reweighted least squares (IRLS): Given a starting estimate of <bold>S</bold>, we can obtain the generalized least squares estimate of <italic>s</italic>,<disp-formula id="pcbi.1005508.e048"><alternatives><graphic id="pcbi.1005508.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>S</mml:mi></mml:mstyle> <mml:mrow><mml:mo>−</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:msup> <mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mover><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>S</mml:mi></mml:mstyle> <mml:mrow><mml:mo>−</mml:mo> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:msup> <mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>d</mml:mi></mml:mstyle></mml:math></alternatives> <label>(30)</label>
</disp-formula>
re-estimate <bold>S</bold> according to <xref ref-type="disp-formula" rid="pcbi.1005508.e043">Eq 27</xref>, and iterate until convergence.</p>
</sec>
<sec id="sec016">
<title>Simulated example data sets</title>
<p>We use simulated data sets here to evaluate and compare the three analysis techniques in a situation where the ground-truth is known. The three simulated example data sets are inspired by real fMRI studies. The first two examples are motivated by a paper investigating the representational structure of finger movements in primary motor and sensory cortex [<xref ref-type="bibr" rid="pcbi.1005508.ref028">28</xref>]. The structure of the empirically measured distances between movements of the five fingers was highly reliable across different individuals. The main question was whether this invariant structure is best explained by the correlations of finger movements in every-day life—i.e. the natural statistics of movement [<xref ref-type="bibr" rid="pcbi.1005508.ref050">50</xref>], or by the patterns of muscle activity required for these movements. Rather than hypothesizing that certain features form the basis set generating the activity profiles distribution, we could directly predict the second-moment matrices, and hence the RDMs, from the correlations between naturally occurring movements, or the correlations of muscle activity patterns. The predicted RDM for individuated movements of the five fingers (Exp. 1) is shown in <xref ref-type="fig" rid="pcbi.1005508.g004">Fig 4A and 4B</xref>. The second example comes from experiment 3 in the same paper, this time looking at 31 different finger movements, which span the whole space of possible “piano-chord” combinations (<xref ref-type="fig" rid="pcbi.1005508.g004">Fig 4C and 4D</xref>).</p>
<fig id="pcbi.1005508.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Representational dissimilarity matrices (RDMs) for the models used in simulation.</title>
<p>Each entry of an RDM shows the dissimilarity between the patterns associated with two experimental conditions. RDMs are symmetric about a diagonal of zeros. Note that while zero is meaningfully defined (no difference between conditions), the scaling of the distances is arbitrary. For Experiment 1, the distance between the activity patterns for the five fingers are predicted from the structure of (<bold>A</bold>) muscle activity and (<bold>B</bold>) the natural statists of movement. In Experiment 2 (<bold>C, D</bold>) the same models predict the representational dissimilarities between finger movements for 31 piano-like chords. For Experiment 3 (<bold>E, F</bold>), model predictions come from the activity of the seven layers of a deep convolutional neural network in response to 96 visual stimuli. The 1<sup>st</sup> convolutional layer and the 1<sup>st</sup> fully connected layer are shown as examples.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g004" xlink:type="simple"/>
</fig>
<p>The third example uses an experiment investigating the response of the human inferior temporal cortex to 96 images, including animate and inanimate objects [<xref ref-type="bibr" rid="pcbi.1005508.ref009">9</xref>]. The model predictions are derived from a convolutional deep neural network model—with each of the 7 layers providing a separate representational model. The bitmap images were presented to the deep neural network and the internal activity patterns used as representational models.</p>
<p>All data sets where simulated with 8 runs, 160 voxels, and independent noise on the observations. The noise variance was set to <italic>σ</italic><sup>2</sup> = 1. We first normalized the model predictions, such that the norm of the predicted squared Euclidean distances was 1. We then derived the second moment matrix (<bold>G</bold>) for each model using <xref ref-type="disp-formula" rid="pcbi.1005508.e036">Eq 22</xref> and created true activity patterns that were normally distributed with second moment <bold>U</bold> <bold>U</bold><sup><italic>T</italic></sup>/<italic>P</italic> = <bold>G</bold><italic>s</italic>. The observation for each run were then generated by adding normally distributed random noise with unit variance to the data (<xref ref-type="disp-formula" rid="pcbi.1005508.e006">Eq 1</xref>). The signal-strength parameter <italic>s</italic> was varied systematically starting from 0 (pure noise data).</p>
<p>We generated 3,000 data sets for each experiment, parameter setting, and model. Each data set was generated by one model (ground truth) and was analyzed so as to infer the data-generating model, using each of the inference methods. To evaluate how well the methods adjudicated between the models, we compared the fit of the true model (i.e. the model that generated that particular data set) with each alternative model by counting the number of instances, in which the method decided in favor of the correct model. Thus, even though there were 7 alternative models in Experiment 3, chance performance for the pairwise comparisons was always 50%. The percentage of correct decisions over all possible model pairs and simulation was used as a measure of model-selection accuracy.</p>
</sec>
</sec>
</sec>
<sec id="sec017" sec-type="results">
<title>Results</title>
<p>Our simulations illustrate the three main points of this paper: (1) Encoding approaches only provide a powerful test of representational models when using regularization that defines a prior distribution on the feature weights. (2) For the best possible inference using RSA, it is important to take the unequal variances and covariances between he distance estimates into account. (3) While PCM performs optimal model selection if the model assumptions are met, the other two approaches provide close approximations to the theoretical maximum. We will now discuss these results in turn.</p>
<sec id="sec018">
<title>Encoding analysis without regularization</title>
<p>When evaluating encoding models without using regularization, one compares the subspaces spanned by the respective model features. To make different models distinguishable, one typically needs to reduce the dimensionality of the model matrix <bold>M,</bold> for example by using only the eigenvectors with the <italic>n</italic> highest eigenvalues of the predicted second-moment matrix. The decision to use a given number regressors is somewhat arbitrary: For example, Leo et al. [<xref ref-type="bibr" rid="pcbi.1005508.ref021">21</xref>] used 5 “synergies” (i.e. principal components of the kinematic data of 20 movements), as these explained 90% of the variance of the behavioral data.</p>
<p>Here we explore systematically how the number of principal components influences model selection. For each experiment, we simulated data sets with a fixed signal-to-noise ratio (Exp. 1 and Exp. 3: <italic>s</italic> = 0.3, Exp 2: <italic>s</italic> = 0.1; <inline-formula id="pcbi.1005508.e049"><alternatives><graphic id="pcbi.1005508.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ε</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> = 1), and compared model selection accuracies using a number of principal components ranging between one and the maximum number. We used both cross-validated performance measures, <inline-formula id="pcbi.1005508.e050"><alternatives><graphic id="pcbi.1005508.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1005508.e020">Eq 10</xref>) and <italic>r</italic> (the correlation between predicted and observed values; <xref ref-type="disp-formula" rid="pcbi.1005508.e021">Eq 11</xref>) to perform model selection.</p>
<p><xref ref-type="fig" rid="pcbi.1005508.g005">Fig 5A–5C</xref> shows the percentage of correct model selections for Experiments 1-3. Results for encoding analysis without regularization are shown in blue. The dimensionality that differentiated best between competing models was 2, 3, and 5 features, respectively. As more features were included, the number of correct model selections declined. When the number of features was the same as the number of conditions minus 1 (due to the mean subtraction), i.e. the models became saturated, model selection accuracy fell to chance. This is expected, as two saturated models span exactly the same subspace and hence make identical predictions (<xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3D</xref>).</p>
<fig id="pcbi.1005508.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Dependence of encoding model analysis on regularization and the number of included model features.</title>
<p>(<bold>A-C</bold>) Percent correct model selections using either <inline-formula id="pcbi.1005508.e051"><alternatives><graphic id="pcbi.1005508.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> (solid line) or correlation (dashed line) for encoding models without a prior (blue lines) and with a prior (red line). (<bold>D-F</bold>) Correlation between predicted and observed patterns. (<bold>G-I</bold>) Predictive <inline-formula id="pcbi.1005508.e052"><alternatives><graphic id="pcbi.1005508.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> for the encoding models with prior. All <inline-formula id="pcbi.1005508.e053"><alternatives><graphic id="pcbi.1005508.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> values for models without prior are negative, and therefore not visible.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g005" xlink:type="simple"/>
</fig>
<p>Using correlations as selection criterion led to more accurate decisions than using <inline-formula id="pcbi.1005508.e054"><alternatives><graphic id="pcbi.1005508.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. Correlations (<xref ref-type="fig" rid="pcbi.1005508.g005">Fig 5D–5F</xref>, blue lines) were generally positive and peaked at a number of features that was slightly higher than the optimal dimensionality for model selection. <inline-formula id="pcbi.1005508.e055"><alternatives><graphic id="pcbi.1005508.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> values for encoding without a prior were all negative (and are therefore not visible), because the approach does not account for the noise in the data and hence leads to predictions that are too extreme—i.e. the approach over-predicts the scale of the data. Correlations are insensitive to this problem as they allow for arbitrary scaling between predicted and observed values.</p>
</sec>
<sec id="sec019">
<title>Encoding approaches with regularization</title>
<p>From a Bayesian perspective, employing regularization (<xref ref-type="disp-formula" rid="pcbi.1005508.e023">Eq 13</xref>) is equivalent to adding a prior to the feature weights. Note that this changes the representational hypotheses tested. For example, the models for Experiment 3, based on the neural network representations, now predicted not only that some weighted combination of the neural network features can account for the data, but more specifically that the distribution of activity profiles should match the distribution of activity profiles of the original neural network simulation. In the model matrix, we scaled each principal component of <bold>G</bold> with the square root of the eigenvalue (<xref ref-type="disp-formula" rid="pcbi.1005508.e026">Eq 15</xref>), such that we could employ ridge regression to obtain the best linear unbiased predictor for the held-out data patterns.</p>
<p>For encoding models with a prior, model selection performance increased with increasing number of features (red lines, <xref ref-type="fig" rid="pcbi.1005508.g005">Fig 5A–5C</xref>). Thus, dimensionality reduction of the model is not necessary here. Furthermore, model selection was always more powerful with than without a prior when correlation was used for model selection. This reflects the fact that the prior provides additional information about the models to be compared. It enables us to compare well-defined distributions of activity profiles instead of just subspaces.</p>
<p>For Experiments 2 and 3, the <inline-formula id="pcbi.1005508.e056"><alternatives><graphic id="pcbi.1005508.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> criterion performed substantially worse than the correlation between predicted and observed activity patterns. The difference between the two criteria arises from the fact that correlations allow for an arbitrary scaling between predicted and observed activity patterns, whereas <inline-formula id="pcbi.1005508.e057"><alternatives><graphic id="pcbi.1005508.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> penalizes deviation in scale. The scaling of the prediction in turn strongly depends on the choice of the scalar regularization coefficient. This fact is illustrated in <xref ref-type="fig" rid="pcbi.1005508.g006">Fig 6</xref>, where we simulated data from Exp. 2 with a fixed noise and signal strength, and varied the regularization coefficient systematically. While <inline-formula id="pcbi.1005508.e058"><alternatives><graphic id="pcbi.1005508.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> is highly sensitive to the choice of the regularization coefficient, the correlation criterion is not. Because the regularization coefficient is determined separately for each cross-validation fold and model, deviations from the optimal ridge will decrease model selection accuracy for the cross-validated <inline-formula id="pcbi.1005508.e059"><alternatives><graphic id="pcbi.1005508.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> criterion, but not for the correlation criterion.</p>
<fig id="pcbi.1005508.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Sensitivity of the <inline-formula id="pcbi.1005508.e060"><alternatives><graphic id="pcbi.1005508.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:msubsup><mml:mi mathvariant="bold">R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> (solid line) and correlation (dashed line) to the choice of the regularization coefficient.</title>
<p>Simulations come from Experiment 2 with a true signal strength of s = 0.2 and a noise variance of 1. For this combination the optimal regularization coefficient is <inline-formula id="pcbi.1005508.e061"><alternatives><graphic id="pcbi.1005508.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>ε</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (dashed vertical line). The correlation criterion is generally robust against non-optimal choice of regularization coefficient.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g006" xlink:type="simple"/>
</fig>
<p>In sum, using regularization improves model selection performance, even if the encoding model has fewer features than conditions or measurements. Rather than just comparing subspaces, the implicit prior on the weights means that a more specific hypothesis is being tested. From this perspective, it is unsurprising that we can adjudicate between these hypotheses with greater accuracy. Furthermore, the use of correlation instead of the predictive <inline-formula id="pcbi.1005508.e062"><alternatives><graphic id="pcbi.1005508.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> makes model selection more robust against variations in the regularization coefficient.</p>
</sec>
<sec id="sec020">
<title>Representational similarity analysis</title>
<p>When evaluating models with RSA, there is no need to restrict the model to a specific number of features—the second-moment matrix from all features can determine the predicted distances. As an empirical dissimilarity measure, we used the crossnobis estimator [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>] and compared the predicted to the measured RDM. To select the winning model, we used rank-based correlation of dissimilarities [<xref ref-type="bibr" rid="pcbi.1005508.ref027">27</xref>], Pearson correlation, correlation with a fixed intercept (<xref ref-type="disp-formula" rid="pcbi.1005508.e040">Eq 24</xref>), and the likelihood of the observed distances under the normal approximation (<xref ref-type="disp-formula" rid="pcbi.1005508.e042">Eq 26</xref>) using the full variance-covariance matrix of the estimated dissimilarities.</p>
<p>For Experiment 1 (<xref ref-type="fig" rid="pcbi.1005508.g007">Fig 7</xref>), rank-based correlation performed substantially worse than the other criteria. The lower performance of rank correlation may have been exacerbated here by the fact that the two models predict relatively similar dissimilarity ranks. However, we expect lower performance for rank correlation in general, because this approach does not use all the information in the measured RDMs. It forgoes the assumption of a linear relationship between predicted and measured dissimilarities and therefore does not exploit the information in the continuous magnitudes of the dissimilarities. Likelihood-based RSA yielded the best decisions; slightly better than Pearson correlation and fixed-intercept correlation.</p>
<fig id="pcbi.1005508.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g007</object-id>
<label>Fig 7</label>
<caption>
<title>RSA model selection accuracies for different criteria of RDM fit.</title>
<p>Data sets for all three experiments were generated with varying signal strength (horizontal axis). The percentage of correct decisions using different criteria is shown (dotted line). Models were selected based on the Spearman rank correlation (purple), Pearson correlation (green), fixed intercept correlation (blue) or likelihood under the multinormal approximation (red). For comparison, the model selection accuracy for PCM is shown in the dotted line.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g007" xlink:type="simple"/>
</fig>
<p>The advantage of the likelihood-based approach was clearer for Exp. 2 and 3. Here, it led to about 10 percentage points greater accuracy of the decisions than the next-best RSA approach. This advantage is likely due to the fact that Pearson correlations and especially fixed-intercept correlations (<xref ref-type="disp-formula" rid="pcbi.1005508.e040">Eq 24</xref>) are sensitive to the observed value for the largest predicted dissimilarities, as these data points have a large leverage on the estimated regression line. Indeed, some of the models for Exp. 2 and 3 contain a few especially large dissimilarities, which will influence the model fit strongly. The likelihood-based approach incorporates the knowledge that large dissimilarities are measured with substantially larger variability [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>], and hence discounts their influence. Notably, rank-based correlation performed relatively well on these models as compared to Pearson correlation, likely because rank correlation is robust to outliers and less dominated by the large predicted distances.</p>
<p>In sum, these simulations show that it is advantageous to take the covariance structure of the measured dissimilarities into account whenever the additional assumptions this requires are justified.</p>
</sec>
<sec id="sec021">
<title>Pattern component modeling</title>
<p>In the same simulations, we also applied the direct likelihood-ratio test, as implemented by PCM. As all the assumptions of the generative model behind PCM are met in the simulation, we would expect, by the Neyman-Pearson lemma [<xref ref-type="bibr" rid="pcbi.1005508.ref024">24</xref>], that this method should provide us with highest achievable model selection accuracy. Model selection performance (dotted line in <xref ref-type="fig" rid="pcbi.1005508.g007">Fig 7</xref>) was indeed systematically higher than for the best RSA-based method. For direct comparison of the so far best methods—PCM, likelihood-based RSA, and encoding analysis with regularization (using correlations as a model selection criterion)—we simulated the three Experiments at a single signal strength (<xref ref-type="fig" rid="pcbi.1005508.g008">Fig 8</xref>).</p>
<fig id="pcbi.1005508.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005508.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Model selection accuracy and execution time for likelihood-based RSA, PCM, and encoding analysis with regularization.</title>
<p>(<bold>A-C</bold>) Model-selection accuracy was inferentially compared between the three techniques on the basis of N = 3,000 simulations, using a likelihood-ratio test of counts of correct model decisions [<xref ref-type="bibr" rid="pcbi.1005508.ref051">51</xref>]. The signal-strength parameter for the simulation was set to s = 0.3 for Exp. 1, s = 0.15 for Exp. 2, and s = 0.5 for Exp. 3. All resulting significant differences (two-tailed, p&lt;0.01, uncorrected) are indicated by a horizontal line above the bars. (<bold>D-F</bold>) Execution times for the evaluation of a single data set under a single model. For encoding, the time is split into the time required to estimate regression coefficients (dark blue) and the time to determine the regularization constant (light blue).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005508.g008" xlink:type="simple"/>
</fig>
<p>In this simulation, PCM resulted in 1.48, 3.01 and 2.86 percentage points (for Exp. 1-3, respectively) better model selection accuracy than likelihood-based RSA, and 1.98, 1.17 and 0.85 percentage points higher model selection accuracies than an encoding analysis using correlations. PCM never performed worse than another method and performed significantly better than the other two approaches in 4 of 6 total comparisons across the three experiments (<xref ref-type="fig" rid="pcbi.1005508.g008">Fig 8</xref>). There were no significant performance differences between RSA and encoding analysis. Overall, however, all three methods were very close in performance.</p>
</sec>
<sec id="sec022">
<title>Computational cost</title>
<p>A practical concern is the speed at which the model comparison can be performed. This is usually not important when evaluating the model fit on a small number of participants or ROIs. However, if a larger number of models is evaluated continuously over the cortical surface using a searchlight approach [<xref ref-type="bibr" rid="pcbi.1005508.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref053">53</xref>], or in data sets with large numbers of participants, computational cost becomes a practical issue. While we cannot treat this issue exhaustively, we provide here a brief overview over the computation time required for the three methods for our specific examples and implementation. In general, the computation time will of course depend on the number of conditions, the number of channels, the exact variant of each technique. Our goal here is simply to give the reader a starting point for making a choice for a particular application, trading off computational and statistical efficiency.</p>
<p>Both RSA and PCM operate on the inner product matrix of the activity estimates, thus the computational costs for these approaches is virtually independent of the number of voxels. PCM works on the <italic>MK</italic> × <italic>MK</italic> inner product matrix of the activity estimates, whereas RSA operates on a <italic>K</italic> × <italic>K</italic> matrix of distances between conditions. For a small number of conditions, this explains the favorable computational cost of RSA. However, when using likelihood-based RSA, the covariance matrix of the distances needs to be calculated and inverted. The size of this matrix is (<italic>K</italic>(<italic>K</italic>-1)/2)<sup>2</sup> and it therefore grows with the 4<sup>th</sup> power of the number of conditions <italic>K</italic>. For Exp. 3 (<xref ref-type="fig" rid="pcbi.1005508.g008">Fig 8F</xref>) with <italic>K</italic> = 96, this is computationally costly, whereas PCM still only needs to invert matrices of size (<italic>MK</italic>)<sup>2</sup>. Using RDM-correlation-based model selection (whether rank, Pearson or fixed-intercept), RSA is much more computationally efficient (not shown).</p>
<p>For encoding models, conducting the actual ridge regression for each cross-validation fold (dark blue area) is extremely fast and efficient. The main cost of the technique lies in the determination of the optimal ridge coefficient (light blue area). In our simulations, we use restricted maximum likelihood estimation (<xref ref-type="disp-formula" rid="pcbi.1005508.e032">Eq 18</xref>) to do so—therefore this cost is always <italic>M</italic> times higher than for PCM alone. Depending on the implementation, generalized cross-validation [<xref ref-type="bibr" rid="pcbi.1005508.ref046">46</xref>] may offer a considerable speed-up. If very high speeds are required, one could use a constant ridge coefficient and accept the possible loss in model selection accuracy. In sum, while PCM is computationally feasible across the three experiments, encoding models were less efficient in the present implementation and likelihood-based RSA was less efficient than PCM for the condition-rich scenario of Experiment 3. Alternative variants of encoding models (with fixed ridge coefficient) and RSA (with correlation-based model selection) are less statistically efficient, but beat PCM in terms of computational efficiency.</p>
</sec>
</sec>
<sec id="sec023" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper, we defined representational models as formal hypotheses about the distribution of the activity profiles in the space defined by the experimental conditions. That is, a representational model specifies, which features are represented in a brain region, and how strongly they are represented. The “strength” of representation of a feature has two aspects: the number of responses (e.g. neurons) dedicated to a feature and the scaling of their activity profiles relative to the noise. The second-moment matrix of the activity profiles captures the combined effect of these aspects of feature strength. Two distinct representations with identical second-moment matrices therefore support linear decoding of any given feature at the same signal-to-noise ratio. This holds independent of the question whether the distribution of true activity profiles is Gaussian. It motivates using the second moment as a summary statistic for characterizing representations. RSA, PCM and encoding models offer different tests of representational models, but all three depend, explicitly or implicitly, on the second-moment matrix to characterize each representational hypothesis. Thus, these methods are deeply related and should be understood as part of the same multivariate toolbox. The main characteristics of the three methods are summarized in <xref ref-type="table" rid="pcbi.1005508.t001">Table 1</xref>.</p>
<sec id="sec024">
<title>Encoding models without prior define subspaces, not distributions of activity profiles</title>
<p>There is a fundamental difference between encoding models with and without weight priors. Without a prior on the feature weights, encoding models test how well the subspace spanned by the model features captures the observed activity profiles. For models to be discriminable, the dimensionality (i.e. the number of features) of each model must be substantially lower than the number of experimental conditions. As the number of model dimensions increases, the subspaces of competing models increasingly overlap. Once the number of features matches the number of experimental conditions, their subspaces comprise the entire space of activity profiles, each perfectly fits the training data, and their predictions for unseen data become identical.</p>
<p>A subspace specifies what activity profiles are possible and what activity profiles are impossible (though they might still arise as estimates because of the noise). A subspace might be conceptualized as an infinite flat distribution over the subspace dimensions, with 0 probability outside the subspace. However, a uniform distribution on an infinite interval has an infinite second moment and hence does not specify the neural representation uniquely.</p>
<p>L2-norm regularization (i.e. ridge regression) is equivalent to imposing a Gaussian prior on the regression weights. With such a prior, the representational model specifies a probability distribution with a finite second moment. When changing the form of regularization, one also changes the implicit prior, and hence the representational model that is being tested. Thus, regularization is not simply a trick for stabilizing the fit. Instead, the weight prior forms an integral part of the model, which determines the strength with which each feature is encoded according to the model. Choosing a specific form of regularisation therefore constitutes a decision about the neuroscientific hypothesis to be tested rather than a methodological consideration.</p>
</sec>
<sec id="sec025">
<title>Encoding models tests hypotheses about activity profile distributions, not features sets</title>
<p>Encoding models do not support inferences about the particular feature set generating a representation, because infinitely many feature sets can span the same space. Even when using a prior, the feature set that characterizes a given representational model is not unique. Features should not in general be constrained to be orthogonal in the space of experimental conditions, because the structure of the model is not usually meant to depend on the experimental conditions chosen. Whether the features chosen are orthogonal or not, there is an infinite number of basis sets of features that express the same representational model (inducing the same second moment of activity profiles, <xref ref-type="disp-formula" rid="pcbi.1005508.e010">Eq 3</xref>). For example, two equally long correlated feature vectors can equally well describe a distribution with elliptical isoprobability-density contours (<xref ref-type="fig" rid="pcbi.1005508.g003">Fig 3A</xref>) as two orthogonal features, with one vector longer than the other. Thus, when one representational model is shown to be superior to others, it does not imply anything special about the feature set chosen to express that model. These complications need to be kept in mind in the interpretation of the results of encoding model analyses. It is very tempting to attribute meaning to the particular features, especially when they are mapped onto the cortical surface [<xref ref-type="bibr" rid="pcbi.1005508.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref021">21</xref>]. When interpreting these maps, one needs to remember that a feature set only describes a distribution of activity profiles, and that very different maps can emerge when the same distribution is described by a rotated set. In PCM and RSA, the equivalence of different feature sets is made explicit, as they lead to the same second-moment and representational dissimilarity matrices.</p>
</sec>
<sec id="sec026">
<title>Likelihood-based RSA is more sensitive than correlation-based RSA</title>
<p>When using RSA to test representational models, the crossnobis estimator provides a highly reliable measure of dissimilarity with the added advantage of having an interpretable zero-point [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>]. Rank-based, Pearson, and fixed-intercept correlation provide fast and straightforward ways of measuring the correspondence between predicted and observed distances, so as to select the representational model most consistent with the data. However, using simple correlations ignores the dependence of the distance estimates, as well as their unequal variances. In other words, the sampling distribution of the estimated RDM in the space spanned by the dissimilarities (one dimension per pair of conditions) is not isotropic. This problem is addressed in likelihood-based RSA, which uses a multivariate-normal approximation to the sampling distribution of the crossnobis RDM estimate [<xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>]. The approximation provides an analytical expression for the statistical dependency of distance estimates, as well as their signal-dependent variances. In the simulations, likelihood-based RSA was shown to be more powerful than correlation-based RSA. Its model-selection accuracy was only slightly below the theoretical upper bound, as established by PCM. Likelihood-based RSA might therefore become the approach of choice when comparing representational models using crossnobis estimates.</p>
<p>There are situations, however, in which the models are not specific enough to support ratio-scale predictions of representational dissimilarities. Moreover, for measurement modalities like fMRI, it might be undesirable to assume a linear relationship between predicted and measured representational dissimilarities. Rank-correlation-based RSA [<xref ref-type="bibr" rid="pcbi.1005508.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref027">27</xref>] provides a robust method that is not dependent on the assumption of a linear reflection of the underlying neural dissimilarities in the data RDM. It is also more computationally efficient in the context of condition-rich designs. Likelihood-based RSA becomes computationally expensive as the number of conditions increases. A practical compromise might be to only use the diagonal of the variance-covariance matrix, which would dramatically reduce computational complexity at the expense of neglecting dependencies among dissimilarity estimates.</p>
</sec>
<sec id="sec027">
<title>Which method is best?</title>
<p>For all simulations, model selection using PCM [<xref ref-type="bibr" rid="pcbi.1005508.ref022">22</xref>] was better than competing methods. This is not surprising, as the data were simulated exactly according to the generative model underlying this approach (Gaussian distribution of noise and signal, independence across voxels). In this case, PCM implements the likelihood-ratio test, which by the Neyman-Pearson lemma [<xref ref-type="bibr" rid="pcbi.1005508.ref024">24</xref>] is the most powerful test. Beyond confirming what we know from theory, the simulations were important because they revealed how close the other two techniques come to the theoretical upper bound established by PCM. Results showed that encoding models with a prior and likelihood-based RSA perform near-optimally. In practice, we therefore expect these three techniques to provide similar answers. When its assumptions hold, PCM has clear advantages for model comparison, providing optimal power at reasonable computational cost. However, the other two techniques have other advantages that make them attractive for specific applications.</p>
<p>RSA using RDM correlation for model selection gives up statistical efficiency for computational efficiency, and beats PCM at the latter. When rank correlation is used to compare RDMs, the inference does not rely on a linear relationship between the true dissimilarities and the estimated dissimilarities, an assumption that might be violated in many contexts. RSA also provides readily interpretable intermediate statistics (cross-validated distances), which are closely related to linear decoders for all pairs of stimuli. These statistics can be used to test whether two conditions have different activity patterns [<xref ref-type="bibr" rid="pcbi.1005508.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref029">29</xref>], or whether the dissimilarity is larger for one pair than for another pair of conditions. Multidimensional scaling of the stimuli on the basis of their representational dissimilarities also provides an intuitive visualization of the representational structure [<xref ref-type="bibr" rid="pcbi.1005508.ref025">25</xref>], which can be very helpful in the generation of novel representational hypotheses.</p>
<p>In contrast, PCM sometimes demands complicated approaches to answer simple questions: For example, to test the hypothesis that a difference between two conditions is encoded, one would need to fit one model that allows for separate patterns and one model that does not—and then compare the marginal likelihood of these models. Furthermore, PCM requires the noise to be explicitly modeled, whereas RSA removes the bias arising from noise through cross-validated distances.</p>
<p>Encoding analysis explicitly estimates the first-level parameters that describe the response for each individual voxel. This enables the mapping of the estimated features onto the cortical surface to study their spatial distribution [<xref ref-type="bibr" rid="pcbi.1005508.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref021">21</xref>].</p>
<p>In sum, the three methods are deeply related in that they test hypotheses about the second moment of the activity profiles. However, each method constitutes a unique perspective on the data and supports different kinds of exploratory analyses. We view the methods as complementary tools that are part of a single coherent toolkit for analyzing representations.</p>
</sec>
<sec id="sec028">
<title>Single-voxel vs. multi-voxel inference</title>
<p>An important issue, which we have not touched upon so far, is whether to perform model comparison on single or multiple voxels. While RSA and PCM are usually applied to groups of voxels (such as for ROIs or searchlights), encoding models are often compared on the single-voxel level. This tendency, however, is not strictly inherent in methodological constraints: The searchlight approach for RSA and PCM can be reduced to single voxels, and encoding models can be combined with multi-voxel searchlights. Analyses with coarser granularity give up some spatial precision of the map in exchange for greater statistical power. Searchlight mapping boosts power (1) by locally combining the evidence, (2) by enabling the use of a multivariate noise normalization, and (3) by reducing the effective number of multiple comparisons [<xref ref-type="bibr" rid="pcbi.1005508.ref054">54</xref>]. There is no reason to assume that a single-voxel searchlight is always the optimal choice when balancing spatial precision and power. Based on our previous results [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>], we expect that ignoring voxel dependencies will entail a loss of sensitivity when making inferences on representational models for regions of interest comprising multiple responses.</p>
</sec>
<sec id="sec029">
<title>Testing models without overfitting to the noise and to the sample of experimental conditions</title>
<p>Whenever a model is fitted using experimental data, its parameters will necessarily be overfitted to the data to some extent. Assessing the performance of a fitted model therefore requires independent test data. An important question is whether the test data should consist in independent measurements for the same experimental conditions or in measurements for a fresh sample of experimental conditions (e.g. a different sample of visual images). The simple answer is that it depends on the inference we would like to make. If our hypothesis is restricted to the present set of conditions (e.g. five finger movements), we need only account for overfitting to the noise in the data and require different measurements for the same conditions. If our hypothesis is about a population of conditions (e.g. all natural images), we need to account for overfitting to the condition sample and require measurements for an independent random sample of conditions from the population of conditions covered by our hypothesis.</p>
<p>However, overfitting only needs to be accounted for when the model being tested had parameters fitted in the first place. Encoding models always require independent test sets to account for the over-fitting of the first-level parameters of the representational model (feature weights). RSA and PCM, by contrast, rely explicitly on summary statistics of the responses. Therefore, only second-level parameters related to the strength of the signal and noise need to be fitted (see <xref ref-type="table" rid="pcbi.1005508.t001">Table 1</xref>). Because the representational models considered here had the same number of such second- level parameters, they could be compared directly.</p>
</sec>
<sec id="sec030">
<title>What about decoding approaches?</title>
<p>Decoding is widely used in multivariate analysis of brain imaging data [<xref ref-type="bibr" rid="pcbi.1005508.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref013">13</xref>]. Can it serve us also as a tool for comparing representational models? While one can use standard decoding approaches to determine whether specific features are represented in an area or not, it does not lend itself to the comparison of full representational models (as defined here). Representational models determine (via the second moment matrix) the decodability of any linear feature, not just a restricted set of features. This is most obvious in RSA, where the RDM assembles all pairwise condition discriminabilities. It is of course possible to use decoding in the context of the methods considered here. For example, some studies have used encoding models to decode stimuli [<xref ref-type="bibr" rid="pcbi.1005508.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref021">21</xref>]. Decoding accuracy on held-out data can then serve, instead of correlation or <inline-formula id="pcbi.1005508.e063"><alternatives><graphic id="pcbi.1005508.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005508.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:msubsup><mml:mi>R</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>, as a performance evaluation of an encoding model. While this approach is motivated by the intuitive demonstration of mind reading, it does not provide a particularly natural or powerful approach to adjudicating between representational models. Alternatively, we could use classification accuracy as a measure of dissimilarity between two conditions in the context of RSA [<xref ref-type="bibr" rid="pcbi.1005508.ref055">55</xref>]. However, classification essentially converts a continuous measure of dissimilarity into a binary label of correct / incorrect. It is therefore expected to be less informative than the underlying continuous measure, and we have shown previously that this entails a loss of sensitivity in practice [<xref ref-type="bibr" rid="pcbi.1005508.ref032">32</xref>]. In sum, decoding is not particularly useful for the evaluation of representational models [<xref ref-type="bibr" rid="pcbi.1005508.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref023">23</xref>] and should therefore be limited to situations, in which the quality of the decoding itself is the measure of interest.</p>
</sec>
<sec id="sec031">
<title>Flexible representational models</title>
<p>All models considered here were “fixed”, i.e., they did not include free parameters that would change the predicted second-moment matrix. In many applications, however, the relative importance of different features (for example encoding strength for orientation and color) are unknown. In this case, the predicted second moment can be expressed as the weighted sum of different pattern components, i.e. <bold>G</bold> = ∑<sub><italic>i</italic></sub> <italic>ω<sub>i</sub></italic> <bold>C<sub>i</sub></bold> [<xref ref-type="bibr" rid="pcbi.1005508.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref056">56</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref058">58</xref>], with the weights being free second-level parameters. In other situations, <bold>G</bold> is a nonlinear function of free model parameters: For example, <bold>G</bold> depends non-linearly on the spatial tuning width in population receptive field modeling [<xref ref-type="bibr" rid="pcbi.1005508.ref059">59</xref>]. Both RSA and PCM already provide a mechanism to estimate such parameters, as both approaches need to estimate the signal strength parameters <italic>s</italic> by maximizing the respective likelihood function (Eqs <xref ref-type="disp-formula" rid="pcbi.1005508.e031">17</xref> and <xref ref-type="disp-formula" rid="pcbi.1005508.e045">28</xref>)—and the analytical derivatives of the likelihood (Eqs <xref ref-type="disp-formula" rid="pcbi.1005508.e031">17</xref> and <xref ref-type="disp-formula" rid="pcbi.1005508.e045">28</xref>) with respect to the parameters are easily obtained. In the context of encoding approaches using ridge regression, free model parameters that change the model structure would result in independent scaling of different features, rotations, or extensions of the model matrix <bold>M</bold>. At the time of writing there are no published examples of such parameter optimization in the context of cross-validated encoding models that we know of.</p>
<p>The inclusion of free parameters into the model also enables the specification of measurement models. Representational models ideally test hypotheses about the distribution of activation profiles of the core computational elements—i.e. neurons. When using indirect measures of brain activity such as fMRI or MEG, the distribution of activity profiles across measurement channels is also influenced by the measurement process, which samples and mixes neuronal activity signals in the measurement channels [<xref ref-type="bibr" rid="pcbi.1005508.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005508.ref060">60</xref>–<xref ref-type="bibr" rid="pcbi.1005508.ref063">63</xref>]. As the underlying brain computational models become more specific and detailed, the corresponding measurement models will also have to be improved.</p>
</sec>
<sec id="sec032">
<title>Higher-order moments of the activity profiles</title>
<p>We focused on approaches that characterize the distribution of activity profiles by its second moment. If the true distribution of the activity profiles is a multivariate Gaussian, then the second moment fully defines the distribution of activity profiles. However, a representational hypothesis may not only predict that the response for condition A is uncorrelated to the response for condition B, but, for example, that channels either respond to A or B, but not to both A and B. Such tuning is for example prevalent in primary visual cortex, where neurons (and voxels) respond to a stimulus in a <italic>one</italic> specific part of the visual field, but less often two or more disparate locations [<xref ref-type="bibr" rid="pcbi.1005508.ref059">59</xref>]. This would correspond to a non-Gaussian prior on the feature weights. In a recent publication, Norman-Haignere and colleagues [<xref ref-type="bibr" rid="pcbi.1005508.ref064">64</xref>] suggested a likelihood-based method, in which the Gaussian prior on the feature weights <bold>W</bold> is replaced with a Gamma distribution, essentially providing a non-Gaussian extension of PCM. It will be interesting to determine to what degree such non-Gaussian distributions are present in fMRI or single-cell data, and what computational function these may serve.</p>
<p>It is important to stress that the approaches considered here are still appropriate when the distribution of activity profiles is truly non-Gaussian. Even in the non-Gaussian case, the second moment determines the representational geometry and thus the decodability of all possible features. It therefore remains essential for characterizing the representation. Taking into account higher moments of the activity profile distribution would enable us to distinguish between representations that afford the same decoding of features (assuming that readout neurons have access to the entire code), but achieve this by distinct population codes.</p>
</sec>
<sec id="sec033">
<title>Conclusions</title>
<p>If advances in brain-activity measurements are to yield theoretical insights into brain computation, they need to be complemented by analytical methods to test computational models of information processing [<xref ref-type="bibr" rid="pcbi.1005508.ref065">65</xref>]. The main purpose of this paper was to provide a clear definition of one important class of models—representational models—and to compare three important approaches of testing these. We have shown that PCM, RSA and encoding analysis are all closely related, testing hypotheses about the distribution of activity profiles. Moreover, all three approaches, in their dominant implementations, are sensitive only to distinctions between representations that are reflected in the second moment of the activity profiles. Thus, these three methods are properly understood as components of a single analytical framework. Each of the three methods has particular advantages and disadvantages and preferred areas of application.</p>
<list list-type="order">
<list-item>
<p>PCM provides an analytic expression for the marginal likelihood of the data under the model, and therefore constitutes the most powerful test for adjudicating between representational models if the assumptions hold. Its analytical tractability and relative computational efficiency are further attractive features, especially when considering models with increasing numbers of free parameters.</p>
</list-item>
<list-item>
<p>RSA provides highly interpretable intermediate statistics and is therefore ideally suited for the visualization and exploratory analysis. Furthermore, simple models are often more easily tested than with PCM. The normal approximation to the distribution of estimated distances enables inference that is nearly as powerful as the likelihood-ratio test provided by PCM. Finally, dissimilarity-rank-based RSA, though less sensitive, provides a means of inference that does not rely on the assumption of a linear relationship between predicted and measured dissimilarities and is computationally efficient even for condition-rich designs.</p>
</list-item>
<list-item>
<p>Encoding approaches enable the voxel-wise mapping of model features onto the cortical surface. They therefore are the natural choice when the spatial distribution of features or the voxel-wise comparison of representational models is the main interest.</p>
</list-item>
</list>
<p>We hope that the general framework presented here will enable researchers to combine these approaches to make progress revealing the computational mechanisms of biological brains.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005508.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stevenson</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>. <article-title>How advances in neural recording affect data analysis</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2731" xlink:type="simple">10.1038/nn.2731</ext-link></comment> <object-id pub-id-type="pmid">21270781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>. <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source>. <year>2012</year>;<volume>73</volume>(<issue>3</issue>):<fpage>415</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment> <object-id pub-id-type="pmid">22325196</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dennett</surname> <given-names>DC</given-names></name>. <source>The intentional stance</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>The MIT Press</publisher-name>; <year>1987</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005508.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>deCharms</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Zador</surname> <given-names>A</given-names></name>. <article-title>Neural representation and the cortical code</article-title>. <source>Annu Rev Neurosci</source>. <year>2000</year>;<volume>23</volume>:<fpage>613</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.23.1.613" xlink:type="simple">10.1146/annurev.neuro.23.1.613</ext-link></comment> <object-id pub-id-type="pmid">10845077</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>. <article-title>Untangling invariant object recognition</article-title>. <source>Trends Cogn Sci</source>. <year>2007</year>;<volume>11</volume>(<issue>8</issue>):<fpage>333</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2007.06.010" xlink:type="simple">10.1016/j.tics.2007.06.010</ext-link></comment> <object-id pub-id-type="pmid">17631409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Pattern-information analysis: from stimulus decoding to computational-model testing</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>56</volume>(<issue>2</issue>):<fpage>411</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.01.061" xlink:type="simple">10.1016/j.neuroimage.2011.01.061</ext-link></comment> <object-id pub-id-type="pmid">21281719</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Reddy</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name>. <article-title>Invariant visual representation by single neurons in the human brain</article-title>. <source>Nature</source>. <year>2005</year>;<volume>435</volume>(<issue>7045</issue>):<fpage>1102</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature03687" xlink:type="simple">10.1038/nature03687</ext-link></comment> <object-id pub-id-type="pmid">15973409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hung</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title>. <source>Science</source>. <year>2005</year>;<volume>310</volume>(<issue>5749</issue>):<fpage>863</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1117593" xlink:type="simple">10.1126/science.1117593</ext-link></comment> <object-id pub-id-type="pmid">16272124</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Esteky</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>60</volume>(<issue>6</issue>):<fpage>1126</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.10.043" xlink:type="simple">10.1016/j.neuron.2008.10.043</ext-link></comment> <object-id pub-id-type="pmid">19109916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Majaj</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>4</issue>):<fpage>613</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4247" xlink:type="simple">10.1038/nn.4247</ext-link></comment> <object-id pub-id-type="pmid">26900926</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haxby</surname> <given-names>JV</given-names></name>, <name name-style="western"><surname>Gobbini</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Furey</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Ishai</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schouten</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Pietrini</surname> <given-names>P</given-names></name>. <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source>. <year>2001</year>;<volume>293</volume>(<issue>5539</issue>):<fpage>2425</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1063736" xlink:type="simple">10.1126/science.1063736</ext-link></comment> <object-id pub-id-type="pmid">11577229</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Norman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Polyn</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Detre</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Haxby</surname> <given-names>JV</given-names></name>. <article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>. <source>Trends Cogn Sci</source>. <year>2006</year>;<volume>10</volume>(<issue>9</issue>):<fpage>424</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2006.07.005" xlink:type="simple">10.1016/j.tics.2006.07.005</ext-link></comment> <object-id pub-id-type="pmid">16899397</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Mitchell</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>. <article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title>. <source>Neuroimage</source>. <year>2009</year>;<volume>45</volume>(<issue>1 Suppl</issue>):<fpage>S199</fpage>–<lpage>209</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2008.11.007" xlink:type="simple">10.1016/j.neuroimage.2008.11.007</ext-link></comment> <object-id pub-id-type="pmid">19070668</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Nishimoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Encoding and decoding in fMRI</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>56</volume>(<issue>2</issue>):<fpage>400</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2010.07.073" xlink:type="simple">10.1016/j.neuroimage.2010.07.073</ext-link></comment> <object-id pub-id-type="pmid">20691790</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mitchell</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Shinkareva</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Malave</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Mason</surname> <given-names>RA</given-names></name>, <etal>et al</etal>. <article-title>Predicting human brain activity associated with the meanings of nouns</article-title>. <source>Science</source>. <year>2008</year>;<volume>320</volume>(<issue>5880</issue>):<fpage>1191</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1152876" xlink:type="simple">10.1126/science.1152876</ext-link></comment> <object-id pub-id-type="pmid">18511683</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kay</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source>. <year>2008</year>;<volume>452</volume>(<issue>7185</issue>):<fpage>352</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature06713" xlink:type="simple">10.1038/nature06713</ext-link></comment> <object-id pub-id-type="pmid">18322462</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huth</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>de Heer</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title>. <source>Nature</source>. <year>2016</year>;<volume>532</volume>(<issue>7600</issue>):<fpage>453</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature17637" xlink:type="simple">10.1038/nature17637</ext-link></comment> <object-id pub-id-type="pmid">27121839</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Georgopoulos</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Kettner</surname> <given-names>RE</given-names></name>. <article-title>Neuronal population coding of movement direction</article-title>. <source>Science</source>. <year>1986</year>;<volume>233</volume>(<issue>4771</issue>):<fpage>1416</fpage>–<lpage>1419</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.3749885" xlink:type="simple">10.1126/science.3749885</ext-link></comment> <object-id pub-id-type="pmid">3749885</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sergio</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Hamel-Paquet</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kalaska</surname> <given-names>JF</given-names></name>. <article-title>Motor cortex neural correlates of output kinematics and kinetics during isometric-force and arm-reaching tasks</article-title>. <source>J Neurophysiol</source>. <year>2005</year>;<volume>94</volume>(<issue>4</issue>):<fpage>2353</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00989.2004" xlink:type="simple">10.1152/jn.00989.2004</ext-link></comment> <object-id pub-id-type="pmid">15888522</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sergio</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Kalaska</surname> <given-names>JF</given-names></name>. <article-title>Systematic changes in directional tuning of motor cortex cell activity with hand location in the workspace during generation of static isometric forces in constant spatial directions</article-title>. <source>J Neurophysiol</source>. <year>1997</year>;<volume>78</volume>(<issue>2</issue>):<fpage>1170</fpage>–<lpage>4</lpage>. <object-id pub-id-type="pmid">9307146</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Leo</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Handjaras</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bianchi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Marino</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gabiccini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Guidi</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>A synergy-based hand control is encoded in human motor cortical areas</article-title>. <source>Elife</source>. <year>2016</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.13420" xlink:type="simple">10.7554/eLife.13420</ext-link></comment> <object-id pub-id-type="pmid">26880543</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ridgway</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Wiestler</surname> <given-names>T</given-names></name>. <article-title>Comparing the similarity and spatial structure of neural representations: a pattern-component model</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>55</volume>(<issue>4</issue>):<fpage>1665</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.01.044" xlink:type="simple">10.1016/j.neuroimage.2011.01.044</ext-link></comment> <object-id pub-id-type="pmid">21256225</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Mourao-Miranda</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hulme</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Bayesian decoding of brain images</article-title>. <source>Neuroimage</source>. <year>2008</year>;<volume>39</volume>(<issue>1</issue>):<fpage>181</fpage>–<lpage>205</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2007.08.013" xlink:type="simple">10.1016/j.neuroimage.2007.08.013</ext-link></comment> <object-id pub-id-type="pmid">17919928</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Neyman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pearson</surname> <given-names>ES</given-names></name>. <article-title>On the problem of the most efficient test of statistical hypotheses</article-title>. <source>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</source>. <year>1933</year>;<volume>231</volume>:<fpage>289</fpage>–<lpage>337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-1-4612-0919-5_6" xlink:type="simple">10.1007/978-1-4612-0919-5_6</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Representational similarity analysis—connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>;<volume>2</volume>:<fpage>4</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment> <object-id pub-id-type="pmid">19104670</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kievit</surname> <given-names>RA</given-names></name>. <article-title>Representational geometry: integrating cognition, computation, and the brain</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year>;<volume>17</volume>(<issue>8</issue>):<fpage>401</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2013.06.007" xlink:type="simple">10.1016/j.tics.2013.06.007</ext-link></comment> <object-id pub-id-type="pmid">23876494</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>A toolbox for representational similarity analysis</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>4</issue>):<fpage>e1003553</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment> <object-id pub-id-type="pmid">24743308</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ejaz</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hamada</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>. <article-title>Hand use predicts the structure of representations in sensorimotor cortex</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>(<issue>7</issue>):<fpage>1034</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4038" xlink:type="simple">10.1038/nn.4038</ext-link></comment> <object-id pub-id-type="pmid">26030847</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zareamoghaddam</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Provost</surname> <given-names>S</given-names></name>. <article-title>The distribution of crossvalidated mahalanobis distances</article-title>. <source>ArXiv</source>. <year>2016</year>;</mixed-citation>
</ref>
<ref id="pcbi.1005508.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>. <article-title>Inferring brain-computational mechanisms with models of activity measurements</article-title>. <source>Proceedings of the Royal Society</source>. <year>2016</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2016.0278" xlink:type="simple">10.1098/rstb.2016.0278</ext-link></comment> <object-id pub-id-type="pmid">27574316</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cai</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Schuck</surname> <given-names>NW</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>A Bayesian method for reducing bias in neural representational similarity analysis</article-title>. <source>BioRxiv</source>. <year>2016</year>;</mixed-citation>
</ref>
<ref id="pcbi.1005508.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ejaz</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Alink</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>. <article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title>. <source>Neuroimage</source>. <year>2016</year>;<volume>137</volume>:<fpage>188</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2015.12.012" xlink:type="simple">10.1016/j.neuroimage.2015.12.012</ext-link></comment> <object-id pub-id-type="pmid">26707889</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ledoit</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>M</given-names></name>. <article-title>Improved estimation of the covariance matrix of stock returns with an application to portfolio selection</article-title>. <source>Journal of Empirical Finance</source>. <year>2003</year>;<volume>10</volume>(<issue>5</issue>)(<fpage>603</fpage>–<lpage>621</lpage>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0927-5398(03)00007-0" xlink:type="simple">10.1016/S0927-5398(03)00007-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Misaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>Neuroimage</source>. <year>2010</year>;<volume>53</volume>(<issue>1</issue>):<fpage>103</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2010.05.051" xlink:type="simple">10.1016/j.neuroimage.2010.05.051</ext-link></comment> <object-id pub-id-type="pmid">20580933</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fisher</surname> <given-names>RA</given-names></name>. <article-title>The use of multiple measurements in taxonomic problems</article-title>. <source>Annals of Eugenics</source>. <year>1936</year>;<volume>7</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>188</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1469-1809.1936.tb02137.x" xlink:type="simple">10.1111/j.1469-1809.1936.tb02137.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Poline</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Grasby</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Frackowiak</surname> <given-names>RS</given-names></name>, <etal>et al</etal>. <article-title>Analysis of fMRI time-series revisited</article-title>. <source>Neuroimage</source>. <year>1995</year>;<volume>2</volume>(<issue>1</issue>):<fpage>45</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/nimg.1995.1007" xlink:type="simple">10.1006/nimg.1995.1007</ext-link></comment> <object-id pub-id-type="pmid">9343589</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Worsley</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Analysis of fMRI time-series revisited—again</article-title>. <source>Neuroimage</source>. <year>1995</year>;<volume>2</volume>(<issue>3</issue>):<fpage>173</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/nimg.1995.1023" xlink:type="simple">10.1006/nimg.1995.1023</ext-link></comment> <object-id pub-id-type="pmid">9343600</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Ware</surname> <given-names>JH</given-names></name>. <article-title>Random-effects models for longitudinal data</article-title>. <source>Biometrics</source>. <year>1982</year>;<volume>38</volume>(<issue>4</issue>):<fpage>963</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2529876" xlink:type="simple">10.2307/2529876</ext-link></comment> <object-id pub-id-type="pmid">7168798</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>. <article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title>. <source>J Neurophysiol</source>. <year>2009</year>;<volume>102</volume>(<issue>1</issue>):<fpage>614</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90941.2008" xlink:type="simple">10.1152/jn.90941.2008</ext-link></comment> <object-id pub-id-type="pmid">19357332</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carlson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tovar</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Alink</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Representational dynamics of object vision: the first 1000 ms</article-title>. <source>J Vis</source>. <year>2013</year>;<volume>13</volume>(<issue>10</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/13.10.1" xlink:type="simple">10.1167/13.10.1</ext-link></comment> <object-id pub-id-type="pmid">23908380</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wardle</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Grootswagers</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>TA</given-names></name>. <article-title>Perceptual similarity of visual patterns predicts dynamic neural activation patterns measured with MEG</article-title>. <source>Neuroimage</source>. <year>2016</year>;<volume>132</volume>:<fpage>59</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2016.02.019" xlink:type="simple">10.1016/j.neuroimage.2016.02.019</ext-link></comment> <object-id pub-id-type="pmid">26899210</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Resolving human object recognition in space and time</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>(<issue>3</issue>):<fpage>455</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3635" xlink:type="simple">10.1038/nn.3635</ext-link></comment> <object-id pub-id-type="pmid">24464044</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kobak</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brendel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Constantinidis</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Feierstein</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mainen</surname> <given-names>ZF</given-names></name>, <etal>et al</etal>. <article-title>Demixed principal component analysis of neural population data</article-title>. <source>Elife</source>. <year>2016</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.10989" xlink:type="simple">10.7554/eLife.10989</ext-link></comment> <object-id pub-id-type="pmid">27067378</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Robinson</surname> <given-names>GK</given-names></name>. <article-title>That BLUP is a Good Thing: The Estimation of Random Effects</article-title>. <source>Statistical Science</source>. <year>1991</year>;<volume>6</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/ss/1177011933" xlink:type="simple">10.1214/ss/1177011933</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref045">
<label>45</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Murphy</surname> <given-names>KP</given-names></name>. <source>Machine Learning: A probabilistic perspective</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT press</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005508.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Golub</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Heath</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wahba</surname> <given-names>G</given-names></name>. <article-title>Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter</article-title>. <source>Technometrics</source>. <year>1979</year>;<volume>21</volume>(<issue>2</issue>):<fpage>215</fpage>–<lpage>223</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/00401706.1979.10489751" xlink:type="simple">10.1080/00401706.1979.10489751</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Harville</surname> <given-names>DA</given-names></name>. <article-title>Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems</article-title>. <source>Journal of the American Statistical Association</source>. <volume>1977</volume>;<volume>72</volume>(<issue>358</issue>):<fpage>320</fpage>–<lpage>338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2286798" xlink:type="simple">10.2307/2286798</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Diedrichsen J, Yokoi A, Arbuckle S. Pattern component modeling toolbox. 2016. <ext-link ext-link-type="uri" xlink:href="https://github.com/jdiedrichsen/pcm_toolbox" xlink:type="simple">https://github.com/jdiedrichsen/pcm_toolbox</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005508.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eisenhauer</surname> <given-names>JG</given-names></name>. <article-title>Regression through the origin</article-title>. <source>Teaching Statistics</source>. <year>2003</year>;<volume>25</volume>(<issue>3</issue>):<fpage>76</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1467-9639.00136" xlink:type="simple">10.1111/1467-9639.00136</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ingram</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Howard</surname> <given-names>IS</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>. <article-title>The statistics of natural hand movements</article-title>. <source>Exp Brain Res</source>. <year>2008</year>;<volume>188</volume>(<issue>2</issue>):<fpage>223</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-008-1355-3" xlink:type="simple">10.1007/s00221-008-1355-3</ext-link></comment> <object-id pub-id-type="pmid">18369608</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref051">
<label>51</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sokal</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Rohlf</surname> <given-names>FJ</given-names></name>. <source>Biometry: the principles and practice of statistics in biological research</source>. <edition>2nd ed</edition>. <publisher-loc>San Fransisco</publisher-loc>: <publisher-name>W. H. Freeman</publisher-name>; <year>1981</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005508.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Oosterhof</surname> <given-names>NN</given-names></name>, <name name-style="western"><surname>Wiestler</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Downing</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>. <article-title>A comparison of volume-based and surface-based multi-voxel pattern analysis</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>56</volume>(<issue>2</issue>):<fpage>593</fpage>–<lpage>600</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2010.04.270" xlink:type="simple">10.1016/j.neuroimage.2010.04.270</ext-link></comment> <object-id pub-id-type="pmid">20621701</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Goebel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Information-based functional brain mapping</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2006</year>;<volume>103</volume>(<issue>10</issue>):<fpage>3863</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0600244103" xlink:type="simple">10.1073/pnas.0600244103</ext-link></comment> <object-id pub-id-type="pmid">16537458</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Analyzing for information, not activation, to exploit high-resolution fMRI</article-title>. <source>Neuroimage</source>. <year>2007</year>;<volume>38</volume>(<issue>4</issue>):<fpage>649</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2007.02.022" xlink:type="simple">10.1016/j.neuroimage.2007.02.022</ext-link></comment> <object-id pub-id-type="pmid">17804260</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>O’Toole</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Abdi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Haxby</surname> <given-names>JV</given-names></name>. <article-title>Partially distributed representations of objects and faces in ventral temporal cortex</article-title>. <source>J Cogn Neurosci</source>. <year>2005</year>;<volume>17</volume>(<issue>4</issue>):<fpage>580</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0898929053467550" xlink:type="simple">10.1162/0898929053467550</ext-link></comment> <object-id pub-id-type="pmid">15829079</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jozwik</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>. <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>. <source>Neuropsychologia</source>. <year>2016</year>;<volume>83</volume>:<fpage>201</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuropsychologia.2015.10.023" xlink:type="simple">10.1016/j.neuropsychologia.2015.10.023</ext-link></comment> <object-id pub-id-type="pmid">26493748</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Henriksson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Fixed versus mixed RSA: Explaining visual representations by fixed and mixed feature sets from shallow and deep computational models</article-title>. <source>BioRxiv</source>. <year>2016</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jmp.2016.10.007" xlink:type="simple">10.1016/j.jmp.2016.10.007</ext-link></comment> <object-id pub-id-type="pmid">28298702</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dumoulin</surname> <given-names>SO</given-names></name>, <name name-style="western"><surname>Wandell</surname> <given-names>BA</given-names></name>. <article-title>Population receptive field estimates in human visual cortex</article-title>. <source>Neuroimage</source>. <year>2008</year>;<volume>39</volume>(<issue>2</issue>):<fpage>647</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2007.09.034" xlink:type="simple">10.1016/j.neuroimage.2007.09.034</ext-link></comment> <object-id pub-id-type="pmid">17977024</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Cusack</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>How does an fMRI voxel sample the neuronal activity pattern: compact-kernel or complex spatiotemporal filter?</article-title> <source>Neuroimage</source>. <year>2010</year>;<volume>49</volume>(<issue>3</issue>):<fpage>1965</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2009.09.059" xlink:type="simple">10.1016/j.neuroimage.2009.09.059</ext-link></comment> <object-id pub-id-type="pmid">19800408</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kamitani</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tong</surname> <given-names>F</given-names></name>. <article-title>Decoding the visual and subjective contents of the human brain</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>(<issue>5</issue>):<fpage>679</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1444" xlink:type="simple">10.1038/nn1444</ext-link></comment> <object-id pub-id-type="pmid">15852014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chaimow</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Yacoub</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ugurbil</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shmuel</surname> <given-names>A</given-names></name>. <article-title>Modeling and analysis of mechanisms underlying fMRI-based decoding of information conveyed in cortical columns</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>56</volume>(<issue>2</issue>):<fpage>627</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2010.09.037" xlink:type="simple">10.1016/j.neuroimage.2010.09.037</ext-link></comment> <object-id pub-id-type="pmid">20868757</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramirez</surname> <given-names>FM</given-names></name>, <name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Allefeld</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Haynes</surname> <given-names>JD</given-names></name>. <article-title>The neural code for face orientation in the human fusiform face area</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>36</issue>):<fpage>12155</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3156-13.2014" xlink:type="simple">10.1523/JNEUROSCI.3156-13.2014</ext-link></comment> <object-id pub-id-type="pmid">25186759</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Norman-Haignere</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>. <article-title>Distinct Cortical Pathways for Music and Speech Revealed by Hypothesis-Free Voxel Decomposition</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>88</volume>(<issue>6</issue>):<fpage>1281</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.11.035" xlink:type="simple">10.1016/j.neuron.2015.11.035</ext-link></comment> <object-id pub-id-type="pmid">26687225</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005508.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kording</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jonas</surname> <given-names>E</given-names></name>. <article-title>Could a neuroscientist understand a microprocessor?</article-title> <source>BioRxiv</source>. <year>2016</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1005268" xlink:type="simple">10.1371/journal.pcbi.1005268</ext-link></comment> <object-id pub-id-type="pmid">28081141</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>