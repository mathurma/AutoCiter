<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00444</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002219</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
                <subject>Sensory systems</subject>
                <subject>Single neuron function</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Receptive Field Inference with Localized Priors</article-title><alt-title alt-title-type="running-head">Receptive Field Inference with Localized Priors</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Park</surname>
            <given-names>Mijung</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Pillow</surname>
            <given-names>Jonathan W.</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, Texas, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Center for Perceptual Systems, Department of Psychology and Section of Neurobiology, The University of Texas at Austin, Austin, Texas, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">mjpark@mail.utexas.edu</email> (MP); <email xlink:type="simple">pillow@mail.utexas.edu</email> (JWP)</corresp>
        <fn fn-type="con">
          <p>Analyzed the data: MP JWP. Wrote the paper: MP JWP. Designed the software used in analysis: MP JWP.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>10</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>27</day>
        <month>10</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>10</issue><elocation-id>e1002219</elocation-id><history>
        <date date-type="received">
          <day>4</day>
          <month>4</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>8</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Park, Pillow</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>The linear receptive field describes a mapping from sensory stimuli to a one-dimensional variable governing a neuron's spike response. However, traditional receptive field estimators such as the spike-triggered average converge slowly and often require large amounts of data. Bayesian methods seek to overcome this problem by biasing estimates towards solutions that are more likely <italic>a priori</italic>, typically those with small, smooth, or sparse coefficients. Here we introduce a novel Bayesian receptive field estimator designed to incorporate <italic>locality</italic>, a powerful form of prior information about receptive field structure. The key to our approach is a hierarchical receptive field model that flexibly adapts to localized structure in both spacetime and spatiotemporal frequency, using an inference method known as empirical Bayes. We refer to our method as <italic>automatic locality determination</italic> (ALD), and show that it can accurately recover various types of smooth, sparse, and localized receptive fields. We apply ALD to neural data from retinal ganglion cells and V1 simple cells, and find it achieves error rates several times lower than standard estimators. Thus, estimates of comparable accuracy can be achieved with substantially less data. Finally, we introduce a computationally efficient Markov Chain Monte Carlo (MCMC) algorithm for fully Bayesian inference under the ALD prior, yielding accurate Bayesian confidence intervals for small or noisy datasets.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>A central problem in systems neuroscience is to understand how sensory neurons convert environmental stimuli into spike trains. The receptive field (RF) provides a simple model for the first stage in this encoding process: it is a linear filter that describes how the neuron integrates the stimulus over time and space. A neuron's RF can be estimated using responses to white noise or naturalistic stimuli, but traditional estimators such as the spike-triggered average tend to be noisy and require large amounts of data to converge. Here, we introduce a novel estimator that can accurately determine RFs with far less data. The key insight is that RFs tend to be localized in spacetime and spatiotemporal frequency. We introduce a family of prior distributions that flexibly incorporate these tendencies, using an approach known as empirical Bayes. These methods will allow experimentalists to characterize RFs more accurately and more rapidly, freeing more time for other experiments. We argue that locality, which is a structured form of sparsity, may play an important role in a wide variety of biological inference problems.</p>
      </abstract><funding-group><funding-statement>This work was supported by the UT Austin Center for Perceptual Systems. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="16"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>A fundamental problem in systems neuroscience is to determine how sensory stimuli are functionally related to a neuron's response. A popular mathematical description of this encoding relationship is the “cascade” model, which consists of a linear filter followed by a noisy nonlinear spiking process. The linear stage in this model is commonly identified as the neuron's <italic>spatiotemporal receptive field</italic>, which we will refer to simply as the receptive field (RF) or “filter”. The RF describes how a neuron sums up its inputs across space and time. It can also be conceived as the spatiotemporal stimulus pattern that optimally drives the neuron to spike. A large body of literature in sensory neuroscience has addressed the problem of estimating a neuron's RF from its responses to a rapidly fluctuating stimulus, a problem known generally as “neural characterization” <xref ref-type="bibr" rid="pcbi.1002219-Lee1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Wu1">[17]</xref>.</p>
      <p>Here we focus on a highly simplified encoding model that describes neural responses in terms of a linear filter and additive Gaussian noise <xref ref-type="bibr" rid="pcbi.1002219-Jones1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Korenberg1">[18]</xref>. Although this model gives an imperfect description of real neural responses, the RF estimators that arise from it (such as the spike-triggered average) are consistent under a much larger class of models <xref ref-type="bibr" rid="pcbi.1002219-Chichilnisky1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Bussgang1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Paninski2">[20]</xref>. The maximum likelihood filter estimate under the linear-Gaussian model is the whitened spike-triggered average (STA), also known as <italic>linear regression</italic>, <italic>reverse correlation</italic>, or the <italic>first-order Weiner kernel</italic> <xref ref-type="bibr" rid="pcbi.1002219-Lee1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Marmarelis1">[3]</xref>. The STA has an extensive history in neuroscience and has been used to characterize RFs in a wide variety of areas, including retina <xref ref-type="bibr" rid="pcbi.1002219-Victor1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Chichilnisky1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Simoncelli1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Victor2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Meister1">[22]</xref>, lateral geniculate nucleus <xref ref-type="bibr" rid="pcbi.1002219-Reid1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Reid2">[24]</xref>, primary visual cortex <xref ref-type="bibr" rid="pcbi.1002219-Jones1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-DeAngelis1">[25]</xref>, and peripheral as well as central auditory brain areas <xref ref-type="bibr" rid="pcbi.1002219-Depireux1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Theunissen1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Eggermont1">[26]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Sahani2">[28]</xref>.</p>
      <p>The STA is often high-dimensional (containing tens to hundreds of parameters) and generally requires large amounts of data to converge. With naturalistic stimuli, the whitened STA is often corrupted by high-frequency noise because natural scenes contain little power at high frequencies. A common solution is to regularize the filter estimate by penalizing unlikely parameter settings, generally by biasing parameters towards zero (also known as “shrinkage”). Statisticians have long known that biased estimators can achieve substantially lower error rates in high-dimensional inference problems <xref ref-type="bibr" rid="pcbi.1002219-James1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Efron1">[30]</xref>, and Bayesian methods formalize such biases in terms of a prior distribution over the parameter space. In neuroscience applications, priors for sparse (having many zeros) or smooth (having small pairwise differences) filter coefficients have been used to obtain substantially more accurate RF estimates <xref ref-type="bibr" rid="pcbi.1002219-Theunissen1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Smyth1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-David1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Stevenson1">[31]</xref>.</p>
      <p>However, neural receptive fields are more than simply sparse or smooth. They are <italic>localized</italic> in both spacetime and spatiotemporal frequency. This is a structured form of sparsity: RFs contain many zeros, but these zeros are not uniformly distributed across the filter. Rather, the zeros tend to occur outside some region of spacetime and, in the Fourier domain, outside some region of spatiotemporal frequency. Although this property of receptive fields is well-known <xref ref-type="bibr" rid="pcbi.1002219-DeAngelis2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-deCharms1">[33]</xref>, it has not to our knowledge been previously exploited for receptive field inference. Here we introduce a family of priors that can flexibly encode locality. Our approach is to first estimate a localized prior from the data, and then find the maximum a posteriori (MAP) filter estimate under this prior. This general approach is known in statistics as parametric empirical Bayes <xref ref-type="bibr" rid="pcbi.1002219-Casella1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Kass1">[35]</xref>. Our method is directly inspired by previous empirical Bayes estimators designed to incorporate sparsity <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref> and smoothness <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>. We show that locality can be an even more powerful source of prior information about neural receptive fields, and introduce a method for simultaneously inferring locality in two different bases, yielding filter estimates that are both sparse (local in a spacetime basis) and smooth (local in a Fourier basis).</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>The <xref ref-type="sec" rid="s2">results</xref> section is organized as follows. First, we will describe the linear-Gaussian encoding model and the empirical Bayes framework for receptive field estimation. Second, we will review several previous empirical Bayes RF estimators, to which we will compare our method. Third, we will derive three new receptive field estimators that we collectively refer to as <italic>automatic locality determination</italic> (ALD). We will apply ALD to simulated data and to neural data recorded in primate V1 and primate retina. Finally, we will describe an extension from empirical Bayes to “fully Bayesian” inference under the ALD prior.</p>
      <sec id="s2a">
        <title>Model-based receptive field estimation</title>
        <p>A typical neural characterization experiment involves rapidly presenting stimuli from some statistical ensemble and recording the neuron's response in discrete time bins. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e001" xlink:type="simple"/></inline-formula> denote the (vector) stimulus and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e002" xlink:type="simple"/></inline-formula> the neuron's (scalar) spike response at time bin <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e003" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e004" xlink:type="simple"/></inline-formula> is a vector of spacetime stimulus intensities over some preceding time window that affects the spike response at time bin <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e005" xlink:type="simple"/></inline-formula>.</p>
        <p>We will model the neuron's response as a linear function of the stimulus plus Gaussian noise:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e006" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e007" xlink:type="simple"/></inline-formula> denotes the neuron's receptive field and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e008" xlink:type="simple"/></inline-formula> is a sample of zero-mean, independent Gaussian noise with variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e009" xlink:type="simple"/></inline-formula>. This model is the simplest type of cascade encoding model (depicted in <xref ref-type="fig" rid="pcbi-1002219-g001">Fig. 1 A</xref>), and plays an important role in the theory of neural encoding and decoding <xref ref-type="bibr" rid="pcbi.1002219-Jones1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Wu1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sahani2">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Warland1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Pillow2">[38]</xref>. For a complete dataset with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e010" xlink:type="simple"/></inline-formula> stimulus-response pairs, likelihood is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e011" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e012" xlink:type="simple"/></inline-formula> is a column vector of neural responses and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e013" xlink:type="simple"/></inline-formula> is the stimulus design matrix, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e014" xlink:type="simple"/></inline-formula>'th row equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e015" xlink:type="simple"/></inline-formula>. The maximum likelihood (ML) receptive field estimate is:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e016" xlink:type="simple"/><label>(3)</label></disp-formula>This estimate, also known as the whitened spike-triggered average, and is proportional to the ordinary spike-triggered average if the stimulus ensemble is uncorrelated, meaning <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e017" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002219-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Neural encoding model and empirical Bayes receptive field inference.</title>
            <p>(<bold>A</bold>) Linear Gaussian encoding model: the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e018" xlink:type="simple"/></inline-formula> is projected on the receptive field <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e019" xlink:type="simple"/></inline-formula> and Gaussian noise is added to produce the neural response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e020" xlink:type="simple"/></inline-formula>. (<bold>B</bold>) Graphical model for a hierarchical Bayesian receptive field model. The hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e021" xlink:type="simple"/></inline-formula> specify a prior over the receptive field <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e022" xlink:type="simple"/></inline-formula>, which together with stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e023" xlink:type="simple"/></inline-formula> determines the conditional probability of neural response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e024" xlink:type="simple"/></inline-formula>. Circles indicate variables, arrows indicate conditional dependence, and the square denotes a pair of variables (stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e025" xlink:type="simple"/></inline-formula> and response <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e026" xlink:type="simple"/></inline-formula>) that are observed many times. (<bold>C</bold>) Empirical Bayes involves a two-stage inference procedure: first, maximize the evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e027" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e028" xlink:type="simple"/></inline-formula> (<italic>left</italic>), which can be computed by integrating out <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e029" xlink:type="simple"/></inline-formula> from the generative model in (B); second, maximize the posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e030" xlink:type="simple"/></inline-formula> given the data and estimated hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e031" xlink:type="simple"/></inline-formula> (<italic>right</italic>). See text for details.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g001" xlink:type="simple"/>
        </fig>
        <p>A major drawback of the maximum likelihood estimator is that it typically requires large amounts of data to converge, especially when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e032" xlink:type="simple"/></inline-formula> is high-dimensional. This problem is exacerbated for correlated or naturalistic stimulus ensembles, because the high-frequency components of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e033" xlink:type="simple"/></inline-formula> are not well constrained by the data. In the Bayesian framework, regularization is formalized in terms of a prior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e034" xlink:type="simple"/></inline-formula>, which tells us that we should bias our estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e035" xlink:type="simple"/></inline-formula> toward regions of parameter space that are more probable <italic>a priori</italic>. The posterior distribution, which captures the combination of likelihood and prior information, is given by Bayes' rule:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e036" xlink:type="simple"/><label>(4)</label></disp-formula>The most probable filter given the data and prior is known as the <italic>maximum a posteriori</italic> (MAP) estimator:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e037" xlink:type="simple"/><label>(5)</label></disp-formula>The log prior behaves as a “penalty” on the solution to an ordinary least-squares problem, forcing a tradeoff between minimizing the sum of squared prediction errors and maximizing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e038" xlink:type="simple"/></inline-formula>.</p>
        <p>Biased estimators can achieve substantial improvements over the maximum likelihood, particularly for high-dimensional problems, without giving up desirable features such as consistency (i.e., converging to the correct value in the limit of infinite data). However, the important question arises: how should one select a prior distribution? (Choosing the <italic>wrong</italic> prior can certainly lead to a worse estimate!)</p>
        <p>One common method is to set the prior (or “penalty”) by cross-validation. This involves dividing the data into a “training” and “test” set, and selecting the prior for which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e039" xlink:type="simple"/></inline-formula> (estimated on the training set) achieves maximal performance on the test set. However, this approach is computationally expensive and may be intractable for a prior with multiple hyperparameters. Empirical Bayes is an alternative method for prior selection that does not require separate training and test data.</p>
      </sec>
      <sec id="s2b">
        <title>Empirical Bayes</title>
        <p>Empirical Bayes can be viewed as a maximum-likelihood procedure for estimating the prior distribution from data. It is also known in the literature as <italic>evidence optimization</italic>, <italic>Type II maximum likelihood</italic>, and <italic>maximum marginal likelihood</italic> <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Casella1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Robbins1">[39]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Bishop1">[41]</xref>. The basic idea is that we can compute the probability of the data given a set of hyperparameters governing the prior by “integrating out” the model parameters. This probability is really just a likelihood function for the hyperparameters, so maximizing it results in a maximum-likelihood estimate for the hyperparameters. (Technically, this is <italic>parametric empirical Bayes</italic>, since we will assume a particular parametric form for the prior; see <xref ref-type="bibr" rid="pcbi.1002219-Casella1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Kass1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Raphan1">[42]</xref> for a more general discussion).</p>
        <p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e040" xlink:type="simple"/></inline-formula> denote a set of hyperparameters controlling the prior distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e041" xlink:type="simple"/></inline-formula>, which we will henceforth denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e042" xlink:type="simple"/></inline-formula>. The posterior distribution over the RF (eq.4) can now be written:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e043" xlink:type="simple"/><label>(6)</label></disp-formula>The denominator in this expression is known as the <italic>evidence</italic> or <italic>marginal likelihood</italic>. (Note that we ignored this denominator when finding the MAP estimate (eq.5), since it does not involve <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e044" xlink:type="simple"/></inline-formula>). The evidence is the probability of the responses <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e045" xlink:type="simple"/></inline-formula> given the stimuli <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e046" xlink:type="simple"/></inline-formula> and the hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e047" xlink:type="simple"/></inline-formula>, which we can compute by integrating the numerator (eq.6) with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e048" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e049" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e050" xlink:type="simple"/></inline-formula> is the parameter space for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e051" xlink:type="simple"/></inline-formula>. Maximizing the evidence for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e052" xlink:type="simple"/></inline-formula> therefore amounts to a maximum likelihood estimate of the hyperparameters. The MAP estimate for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e053" xlink:type="simple"/></inline-formula> under this prior is an empirical Bayes estimate, since the prior is learned “empirically” from the data.</p>
        <p>Empirical Bayes can therefore be described as a two-stage procedure: <bold>(1)</bold> Maximize the evidence to obtain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e054" xlink:type="simple"/></inline-formula>; <bold>(2)</bold> Find the MAP estimate for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e055" xlink:type="simple"/></inline-formula> under the prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e056" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1002219-g001">Fig. 1</xref> shows a diagram for this hierarchical receptive field model the steps for empirical Bayesian inference.</p>
      </sec>
      <sec id="s2c">
        <title>Zero-mean Gaussian priors</title>
        <p>Following earlier work <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Faul1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Wipf1">[44]</xref>, we will take the prior distribution to be a Gaussian centered at zero:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e057" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e058" xlink:type="simple"/></inline-formula> is a covariance matrix that depends on hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e059" xlink:type="simple"/></inline-formula> in some yet-to-be-specified manner. This Gaussian prior together with a Gaussian likelihood (eq.2) ensures the posterior is also Gaussian:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e060" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e061" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e062" xlink:type="simple"/></inline-formula> are the posterior mean and covariance. The MAP filter estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e063" xlink:type="simple"/></inline-formula> is simply the posterior mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e064" xlink:type="simple"/></inline-formula>, since the mean and maximum of a Gaussian are the same. Moreover, the evidence (eq.7) can be computed in closed form, since it is the integral of a product of two Gaussians. This allows for rapid optimization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e065" xlink:type="simple"/></inline-formula>. We will in practice maximize the log-evidence, given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e066" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e067" xlink:type="simple"/></inline-formula> is the number of samples (rows) in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e069" xlink:type="simple"/></inline-formula>. All that remains is to specify the prior covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e070" xlink:type="simple"/></inline-formula>, which we will explore in detail below.</p>
        <p>Before continuing, we wish to distinguish two distinct notions of “dimensionality” for a receptive field. First, dimensionality may refer to the number of parameters or coefficients in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e071" xlink:type="simple"/></inline-formula>. We will refer to this as the <italic>parameter dimensionality</italic> of the filter, denoted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e072" xlink:type="simple"/></inline-formula>. Second, dimensionality may refer to the dimensionality of the coordinate space in which the filter is defined. In this sense, a filter with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e073" xlink:type="simple"/></inline-formula> elements arranged as a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e074" xlink:type="simple"/></inline-formula> vector is 1-dimensional (e.g., a temporal filter), while a filter with the same number of elements arranged in a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e075" xlink:type="simple"/></inline-formula> matrix is 2-dimensional (e.g., an image filter). We will refer to this as the <italic>coordinate dimensionality</italic> of the filter, denoted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e076" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2d">
        <title>Previous methods</title>
        <p>We will examine three empirical Bayes RF estimators from the literature: ridge regression <xref ref-type="bibr" rid="pcbi.1002219-MacKay1">[45]</xref>, Automatic Relevance Determination (ARD) <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Faul1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Wipf1">[44]</xref>, and Automatic Smoothness Determination (ASD) <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>. <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref> provides an illustrative comparison of these methods, using a simulated example consisting with a 100-element vector filter (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e077" xlink:type="simple"/></inline-formula>), stimulated with correlated (“1/F”) Gaussian noise stimuli. The true filter was a difference of two Gaussians, and the maximum likelihood estimate (middle left) is badly corrupted by high frequency noise.</p>
        <fig id="pcbi-1002219-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Comparison of estimators for 1D simulated example.</title>
            <p>A 1D difference-of-Gaussians receptive field with 100 elements was stimulated with 2000 samples of correlated (1/F) Gaussian noise. <bold>Left column:</bold> True filter (top), maximum likelihood (linear regression) estimate (middle), and empirical Bayes ridge regression (L2-penalized) estimate (bottom). <bold>Middle:</bold> Lasso (L1-penalized) estimate (top) and ARD (middle) produce sparse estimates but fail to capture smoothness. The ASD estimate (bottom) captures smoothness, but exhibits spurious oscillations in the tails. <bold>Right column</bold>: Three variants of automatic locality determination (ALD): Spacetime localization (ALDs, top), which identifies a spatial region in which the filter coefficients are large; frequency localization (ALDf, middle), which identifies a local region of the frequency domain in which Fourier coefficients are large, leading to a smooth estimate that closely resembles ASD; and joint localization in spacetime and frequency (ALDsf, bottom), which simultaneously identifies a local region in spacetime and frequency, yielding an estimate that is both smooth and sparse.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g002" xlink:type="simple"/>
        </fig>
        <p>First, <italic>ridge regression</italic> assumes a prior with covariance matrix proportional to the identity matrix: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e078" xlink:type="simple"/></inline-formula>. This treats the filter coefficients as drawn <italic>i.i.d.</italic> from a zero-mean Gaussian prior with precision (“inverse variance”) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e079" xlink:type="simple"/></inline-formula>. Ridge regression is penalized least-squares estimate with a penalty (eq.5) on the squared <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e080" xlink:type="simple"/></inline-formula> norm of the filter, given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e081" xlink:type="simple"/></inline-formula>. This penalty shrinks the coefficients of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e082" xlink:type="simple"/></inline-formula> towards zero. Larger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e083" xlink:type="simple"/></inline-formula> yields smaller filter coefficients, and in the limit of infinite <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e084" xlink:type="simple"/></inline-formula>, the MAP estimate shrinks to all-zeros. Set correctly, the ridge prior can provide substantial improvement over maximum likelihood, especially when the stimulus autocovariance is ill-conditioned, as it is for naturalistic stimuli (see <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>). Ridge regression is perhaps the most popular and well-known regularization method. Although it is not usually employed in an empirical Bayes framework, it is straightforward (and fast) to maximize the evidence for the ridge parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e085" xlink:type="simple"/></inline-formula> using a fixed-point rule <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-MacKay1">[45]</xref>. (See <xref ref-type="sec" rid="s4">Methods</xref>).</p>
        <p>Second, <italic>Automatic Relevance Determination</italic> (ARD) <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref> assumes a diagonal prior covariance matrix with a distinct hyperparameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e086" xlink:type="simple"/></inline-formula> for each element of the diagonal. This resembles the ridge prior covariance except that the prior variance of each filter coefficient is set independently. The prior covariance matrix can be written <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e087" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e088" xlink:type="simple"/></inline-formula> ranges over the number of elements in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e089" xlink:type="simple"/></inline-formula>. It would be intractable to use cross-validation to estimate all the elements in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e090" xlink:type="simple"/></inline-formula> (a 100-element vector in <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>), so empirical Bayes plays a critical role for inference. In practice, evidence maximization drives many of the prior variances to zero, making the posterior a delta function at zero for those coefficients. The MAP estimate for these coefficients is therefore zero, making the ARD estimate sparse. The ARD estimate can be computed rapidly using fixed-point methods, expectation-maximization, or variational methods <xref ref-type="bibr" rid="pcbi.1002219-Faul1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Wipf1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Tipping2">[46]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Wipf3">[49]</xref>. <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref> (middle column) shows the ARD and the <italic>lasso</italic> estimate <xref ref-type="bibr" rid="pcbi.1002219-Tibshirani1">[50]</xref>, the latter of which is the MAP estimate under an exponential (or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e091" xlink:type="simple"/></inline-formula>) prior. We set the lasso parameter here by cross-validation. Both estimates are sparse. The ARD estimate is actually sparser and less biased towards zero for large coefficients, but both fail to provide a close match to the smooth filter used in this example.</p>
        <p>Third, <italic>Automatic Smoothness Determination</italic> (ASD) <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref> assumes a non-diagonal prior covariance, given by a Gaussian kernel <xref ref-type="bibr" rid="pcbi.1002219-Rasmussen1">[51]</xref>, which is parametrized so that the correlation between filter coefficients falls off as a function of their separation distance. The rationale here is that RFs are smooth in both space and time, so nearby coefficients should be highly correlated, while more distant ones should be more nearly independent. For a 1D filter, the ASD prior covariance takes the form of a “fuzzy ridge”, with Gaussian decay on either side of the diagonal. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e092" xlink:type="simple"/></inline-formula>'th element is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e093" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e094" xlink:type="simple"/></inline-formula> is the squared distance between the filter coefficients <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e095" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e096" xlink:type="simple"/></inline-formula> in pixel space, and the hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e097" xlink:type="simple"/></inline-formula> control the scale (analogous to the ridge parameter) and smoothness (the width of the fuzzy ridge), respectively. For filters with higher coordinate dimension (e.g., a 2D spatial filter), the hyperparameters include additional hyperparameters to control smoothness in each direction. Optimization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e098" xlink:type="simple"/></inline-formula> can be achieved by gradient ascent of the log-evidence (see <xref ref-type="sec" rid="s4">Methods</xref>). For our simulated example (<xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>, bottom middle), the ASD estimate is indeed smooth due to the correlations in the inferred prior.</p>
        <p>Note that for smooth RFs, the ASD prior covariance matrix becomes ill-conditioned, as some of its eigenvalues are very close to zero. This implies that the ASD estimate is sparse, but (unlike ARD) it is not sparse in the pixel basis. Rather, the ASD estimate is sparse in a basis that depends on the hyperparameters (since the eigenvectors of the ASD prior covariance vary with the hyperparameters). The small-eigenvalue eigenvectors tend to have high-frequency oscillations, meaning that the ASD estimate is sparse in a Fourier-like basis, with the prior variance of high-frequency modes set near to zero. In our view, ASD is the current state-of-the-art method for linear filter estimation and indeed (as shown in <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>) it performs far better than previous methods for realistic neural RFs.</p>
      </sec>
      <sec id="s2e">
        <title>Automatic Locality Determination (ALD)</title>
        <p>The motivation for our approach is the observation that neural receptive fields tend to be localized in space, time, and spatiotemporal frequency (i.e., Fourier space). Neurons in the visual pathway, for example, tend to integrate light only within some restricted region of visual space and some finite window of time, and respond only to some finite range of spatiotemporal frequencies <xref ref-type="bibr" rid="pcbi.1002219-DeAngelis1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-DeAngelis2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Ohzawa1">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>. This is tantamount to a structured form of sparsity: large groups of coefficients (e.g., those outside some spacetime region) that fall to zero in a dependent manner. Here we describe three prior distributions for exploiting this structure. We refer to these methods collectively as <italic>automatic locality determination (ALD)</italic>.</p>
        <sec id="s2e1">
          <title>Locality in spacetime (ALDs)</title>
          <p>First we formulate a prior covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e099" xlink:type="simple"/></inline-formula> that can capture the tendency for RFs to have a limited extent in space and time. We can achieve this with a diagonal covariance matrix, but instead of using a constant diagonal (as in ridge regression) or a vector of hyperparameters along the diagonal (as in ARD), we use a functional form for the diagonal that allows the prior variance to be large for coefficients within some region, and small (decaying to zero) for coefficients outside that region.</p>
          <p>We parametrize the local region with a Gaussian form, so that prior variance of each filter coefficient is determined by its Mahalanobis distance (in coordinate space) from some mean location <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e100" xlink:type="simple"/></inline-formula> under a symmetric positive semi-definite matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e101" xlink:type="simple"/></inline-formula>. The diagonal prior covariance matrix is given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e102" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e103" xlink:type="simple"/></inline-formula> is the spacetime location (i.e., filter coordinates) of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e104" xlink:type="simple"/></inline-formula>'th filter coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e105" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e106" xlink:type="simple"/></inline-formula> is a covariance matrix determining the shape and extent of the local region, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e107" xlink:type="simple"/></inline-formula> sets the overall scale of the prior variance (as in ASD). We refer to this method as ALDs, for automatic locality determination in <italic>spacetime coordinates</italic>. The hyperparameters governing the ALDs prior are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e108" xlink:type="simple"/></inline-formula>, which can specify an arbitrary elliptical region of coordinate space where prior variance is large.</p>
          <p><xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref> shows the ALDs estimate for the 1D example discussed above. As expected, the RF coefficients are large within a central region, and decay to zero outside it. <xref ref-type="fig" rid="pcbi-1002219-g003">Fig. 3</xref> (top row) shows the prior variance underlying this estimate (i.e., the diagonal of the prior covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e109" xlink:type="simple"/></inline-formula>) at the maximum-evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e110" xlink:type="simple"/></inline-formula>. The method can be extended to filters of higher coordinate dimensionality <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e111" xlink:type="simple"/></inline-formula>. In this case, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e112" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e113" xlink:type="simple"/></inline-formula> vector and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e114" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e115" xlink:type="simple"/></inline-formula> symmetric, positive definite matrix specified by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e116" xlink:type="simple"/></inline-formula> parameters.</p>
          <fig id="pcbi-1002219-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Estimated filters and prior covariances for ALD methods.</title>
              <p>(Same example filter as shown in <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>). Left column shows the true filter (dotted black) and ALD estimates (red) replotted from the right-most column of <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>. <bold>Top:</bold> Space-localized estimate. The estimated prior variance (black trace, middle) is a Gaussian form that controls the falloff in amplitude of filter coefficients (red) as a function of position. The prior covariance (right) is a diagonal matrix with this Gaussian along the diagonal. The prior is thus independent with location-dependent variance. <bold>Middle:</bold> Frequency-localized estimate. A Gaussian form (reflected around the origin due to symmetries of the Fourier transform) specifies the prior variance as a function of frequency (black trace, middle). The Fourier power of the filter estimate (red) drops quickly to zero outside the estimated region. The prior covariance matrix (right) is diagonal in the Fourier domain, meaning the Fourier coefficients are independent with frequency-dependent variance. <bold>Bottom:</bold> Space and frequency localized estimate. The estimated prior covariance matrix is not diagonal in spacetime or frequency, but takes the form of a “sandwich matrix” that combines the prior covariances from ALDs and ALDf (see text). The resulting prior covariance matrix can be visualized in either the spacetime domain (left) or the Fourier domain (right). It is localized (has a local region of large prior variance) in both coordinate frames, but has strong dependencies (off-diagonal elements), particularly across space.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g003" xlink:type="simple"/>
          </fig>
          <p>Computationally, ALDs is faster than ASD because, although its parametrization is similar, the prior covariance matrix is diagonal. As the localized region described by the hypearparemeters becomes smaller, the prior variance of outer filter pixels falls arbitrarily close to zero, and we can prune these coefficients (as in ARD) because the prior effectively pins them to zero. This reduces the dimensionality of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e117" xlink:type="simple"/></inline-formula>, making it sparse in pixel space, and making evaluation of the log-evidence (eq.10) faster. The key difference from ARD, however, is that pruning does not take place independently for each coefficient, but occurs systematically as a function of distance from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e118" xlink:type="simple"/></inline-formula>, the center of some spatiotemporal region.</p>
          <p>Note that the ALDs estimator does not assume any functional form for the filter itself. Rather, it seeks to determine (via evidence optimization) only whether there is some elliptical region beyond which the filter coefficients fall to zero. If an RF is <italic>not</italic> localized, the evidence will be maximal when the width of the region specified by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e119" xlink:type="simple"/></inline-formula> becomes much larger than the area covered by the RF coefficients. In this limit, the diagonal of the prior covariance will be nearly constant, where the ALDs prior is equivalent to the ridge regression prior.</p>
          <p>Although ALDs correctly identifies spacetime locality in simulated examples, the estimates it provides are not smooth. The use of a diagonal prior covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e120" xlink:type="simple"/></inline-formula> means that the filter coefficients are independent <italic>a priori</italic> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e121" xlink:type="simple"/></inline-formula>. We can address this shortcoming by considering a different basis for the RF coefficients.</p>
        </sec>
        <sec id="s2e2">
          <title>Locality in frequency (ALDf)</title>
          <p>Neural receptive fields are localized in spatiotemporal frequency as well as in spacetime, which is apparent from their Fourier power spectra <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>. That is, a neuron typically responds to sine waves over some limited range of spatiotemporal frequencies, and is insensitive beyond this range. We can design a prior covariance matrix to capture this structure by employing the ALDs prior in the Fourier domain. We refer to this as the ALDf, for automatic locality determination in <italic>frequency coordinates</italic>.</p>
          <p>We can define a Gaussian prior over the Fourier-transformed RF coefficients <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e122" xlink:type="simple"/></inline-formula> using a diagonal covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e123" xlink:type="simple"/></inline-formula> with diagonal:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e124" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e125" xlink:type="simple"/></inline-formula> denotes the frequency coordinates for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e126" xlink:type="simple"/></inline-formula>'th coefficient of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e127" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e128" xlink:type="simple"/></inline-formula> is a symmetric matrix, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e129" xlink:type="simple"/></inline-formula> describes the mean of (symmetric) elliptical regions in Fourier space. The absolute value ensures reflection symmetry through the origin, a property of the Fourier transform of any real signal, while allowing localized Fourier energy to exhibit orientations in spacetime and to extend over different frequency ranges for different coordinate dimensions. The hyperparameters are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e130" xlink:type="simple"/></inline-formula>. (See <xref ref-type="sec" rid="s4">Methods</xref> for details).</p>
          <p>The ALDf estimate can be computed efficiently by taking the discrete Fourier transform of stimuli <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e131" xlink:type="simple"/></inline-formula>, maximizing evidence for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e132" xlink:type="simple"/></inline-formula> under the diagonal ALDf prior (eq.10), computing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e133" xlink:type="simple"/></inline-formula> in the Fourier domain (eq.9), and then taking the inverse Fourier transform to obtain the spacetime filter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e134" xlink:type="simple"/></inline-formula>. (See <xref ref-type="sec" rid="s4">Methods</xref>). Note that a filter in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e135" xlink:type="simple"/></inline-formula> coordinate dimensions requires the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e136" xlink:type="simple"/></inline-formula>-dimensional Fourier transform. <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref> shows the ALDf estimate for the simulated 1D example, and <xref ref-type="fig" rid="pcbi-1002219-g003">Fig. 3</xref> (second row) shows the diagonal of the estimated prior covariance and Fourier spectrum of the ALDf estimate, which exhibits modes at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e137" xlink:type="simple"/></inline-formula> Hz. Note that this filter is more sparse in the Fourier domain than the space domain, and that the ALDf estimate exhibits correspondingly smaller error than the ALDs. Thus, locality in frequency is more useful than locality in spacetime for smooth RFs.</p>
          <p>Although the ASD and ALDf estimates look similar for this 1D example, the latter achieves slightly lower error due to the fact that it also suppresses low frequencies (e.g., the DC component), which are also small for this filter. The ASD prior, in contrast, always assigns highest prior variance to the lowest frequency Fourier components. (This can be seen by inspecting the ASD prior covariance matrix in the Fourier basis). ALDf can be expected to outperform ASD whenever the Fourier spectrum is not a monotonically decreasing function of frequency; however, for realistic examples we considered, the two perform very similarly. The main limitation of both methods is a failure to account for locality in spacetime, which is evident in the ripples present in the tails of both estimates (<xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>).</p>
        </sec>
        <sec id="s2e3">
          <title>Locality in spacetime and frequency (ALDsf)</title>
          <p>The two methods described above exploit locality by estimating a diagonal prior covariance matrix in either a spacetime basis (ALDs) or a Fourier basis (ALDf). However, neural receptive fields generally exhibit both kinds of locality at once. One would therefore like to design a prior that simultaneously captures both forms of locality. We can accomplish this by forming a “sandwich” matrix out of the two prior covariance matrices defined above. We define the prior covariance to be:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e138" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e139" xlink:type="simple"/></inline-formula> is the square root of the diagonal ALDs prior covariance (eq.11), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e140" xlink:type="simple"/></inline-formula> is the diagonal ALDf prior covariance matrix (eq.12), and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e141" xlink:type="simple"/></inline-formula> is an orthogonal basis matrix for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e142" xlink:type="simple"/></inline-formula>-dimensional discrete Fourier transform. (That is, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e143" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e144" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e145" xlink:type="simple"/></inline-formula>.) This formulation effectively imposes the two forms of locality in series: first, the spacetime prior covariance (outer matrix); then Fourier transform and the frequency domain covariance (inner matrix). Although there are other combination schemes are possible (see <xref ref-type="sec" rid="s3">Discussion</xref>), we found this one to give the best performance on simulated data. We call the resulting estimate ALDsf, for automatic locality determination in <italic>spacetime and frequency</italic>.</p>
          <p>The hyperparameters for ALDsf are union of the ALDs and ALDf hyperparameters, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e146" xlink:type="simple"/></inline-formula>, where subscripts <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e147" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e148" xlink:type="simple"/></inline-formula> indicate parameters for the spatial and frequency domain matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e149" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e150" xlink:type="simple"/></inline-formula>, respectively. We perform evidence optimization over this full set of hyperparameters, although it is helpful to initialize with the values estimated for each of the two above methods individually to avoid sub-optimal local maxima. <xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref> (bottom right) shows the ALDsf estimate for our 1D example, which is nearly indistinguishable from the true filter. <xref ref-type="fig" rid="pcbi-1002219-g003">Fig. 3</xref> shows the estimated prior covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e151" xlink:type="simple"/></inline-formula>, represented in both pixel and Fourier bases. As expected, the prior covariance exhibits locality in both coordinates (bases), but is no longer diagonal in either. This indicates that the resulting prior covariance imposes dependencies between neighboring coefficients in both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e152" xlink:type="simple"/></inline-formula> and its Fourier transform <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e153" xlink:type="simple"/></inline-formula>.</p>
          <p>One useful feature of ALDsf is that it defaults to ALDf if the filter is not localized in space, to ALDs if not localized in frequency, or to ridge regression if not localized in either basis. When the filter is not localized, the evidence will favor regions that are sufficiently broad (i.e., sufficiently large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e154" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e155" xlink:type="simple"/></inline-formula>) that the matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e156" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e157" xlink:type="simple"/></inline-formula> (or both) will approximate the identity matrix, eliminating the prior preference for locality in the corresponding basis. When both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e158" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e159" xlink:type="simple"/></inline-formula> are the identity matrix, the resulting covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e160" xlink:type="simple"/></inline-formula> corresponds to the ridge regression prior.</p>
        </sec>
      </sec>
      <sec id="s2f">
        <title>Application to simulated data</title>
        <p>To compare performance with previous receptive field estimators, we began with simulated data. We generated six different 2D spatial receptive fields with varying degrees of locality in space and frequency. Each filter consisted of a 2D array of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e161" xlink:type="simple"/></inline-formula> pixels, making for a parameter space of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e162" xlink:type="simple"/></inline-formula> dimensions. Noisy responses were simulated using 1600 samples of 1/F correlated Gaussian noise according to (eq.1). Results are shown in <xref ref-type="fig" rid="pcbi-1002219-g004">Fig. 4</xref>.</p>
        <fig id="pcbi-1002219-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Menagerie of simulated examples.</title>
            <p>Noisy responses to 1600 random 1/F Gaussian stimuli were simulated and used for training. The leftmost column shows the true filter (a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e163" xlink:type="simple"/></inline-formula> pixel image), while subsequent columns show various estimates. The mean squared error of each estimate is indicated below in red. Filters shown include: (<bold>A</bold>) Oriented Gabor filter, typical of a V1 simple cell; (<bold>B</bold>) Smaller Gabor filter; (<bold>C</bold>) center-surround “difference-of-Gaussians” filter, typical of retinal ganglion cells; <bold>D</bold>) grid cell with multiple non-zero regions (localized in the Fourier domain but not in space); (<bold>E</bold>) circularly windowed Gaussian white noise (localized in space but not in frequency); (<bold>F</bold>) full field Gaussian noise (not localized in space or frequency). ALDsf performs at or near the optimum for all examples we examined.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g004" xlink:type="simple"/>
        </fig>
        <p>Each row of <xref ref-type="fig" rid="pcbi-1002219-g004">Fig. 4</xref> shows one of the six filters, and the estimates provided by maximum likelihood (ML), ridge regression, ARD, ASD, and (highlighted in blue) ALDsf. The numbers in red below each estimate indicate the mean squared error between the true filter and the estimate. (We did not show ALDs or ALDf because ALDsf always performed best of the three new methods). The simulated examples included: (A) a large Gabor filter; (B) a small Gabor filter; (C) a retina-like center-surround RF; (D) a grid cell RF with several non-zero regions; (E) circularly windowed Gaussian white noise; and (F) a pure Gaussian white noise filter. The grid cell filter did not exhibit strong locality in space, while the windowed white noise did not exhibit locality in frequency, and the pure white noise filter did not exhibit locality in either space nor frequency. Nevertheless, the ALDsf estimate had the smallest error by a substantial margin for all examples except the white noise filter. For the white noise filter, the ridge prior (i.i.d. zero-mean Gaussian) was in fact the “correct” prior. For this example, the ASD and ALDsf estimates were not distinguishable from the ridge regression estimate, consistent with the expectation that both should default to the ridge prior when the evidence did not favor smoothness (ASD) nor locality (ALDsf).</p>
        <p>We examined the convergence properties of the various estimators as a function of the amount of data collected. We simulated responses from the first filter in <xref ref-type="fig" rid="pcbi-1002219-g004">Fig. 4A</xref> according to (eq.1), using two kinds of stimuli: Gaussian white noise, and 1/F correlated Gaussian noise, which more closely resembles natural stimuli. The results (<xref ref-type="fig" rid="pcbi-1002219-g005">Fig. 5</xref>) show that the ALDsf estimate achieved the smallest error for both kinds of stimuli, regardless of the number of training samples. The upper plots in <xref ref-type="fig" rid="pcbi-1002219-g005">Fig. 5</xref> show that for white noise stimuli, traditional estimators (ML and ridge regression) needed more than four times more data than ALDsf to achieve the same error rate. For naturalistic stimuli, traditional estimators needed twenty to thirty times more data. The bottom row of plots shows the ratio of the average mean-squared error (MSE) for each estimate to the average MSE for the ALDsf estimate, showing that the next best method (ASD) exhibits errors nearly 1.8 times larger than ALDsf.</p>
        <fig id="pcbi-1002219-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Comparison of error rates on simulated data.</title>
            <p>Responses of a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e164" xlink:type="simple"/></inline-formula> pixel Gabor filter (shown in <xref ref-type="fig" rid="pcbi-1002219-g004">Fig. 4 A</xref>) were simulated using white noise stimuli (left) or “naturalistic” 1/F Gaussian stimuli (right). (<bold>A</bold>): Filter error using white noise stimuli, for varying amounts of training data (See <xref ref-type="sec" rid="s4">Methods</xref>). (<bold>B</bold>) Average filter error under each method. (<bold>C–D</bold>) Analogous to A–B, but for 1/F stimuli. For both kinds of stimuli, ALDsf achieved error rates almost 2 times smaller than ASD, the next best method. By examining horizontal slices through panels (A) and (C), it is apparent that traditional methods (ML and ridge regression) required four times more data on white noise stimuli, and twenty to thirty times more data on 1/F stimuli, to achieve the same error rate as ALDsf.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g005" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2g">
        <title>Application to neural data</title>
        <p>Next, we compared the various estimators using neural data recorded from simple cells in primate V1 <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>. The stimuli consisted of 16 “flickering bars” aligned with each cell's preferred orientation. We took the receptive field to have a length of 16 time bins, resulting in a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e165" xlink:type="simple"/></inline-formula> filter with two coordinate dimensions (space<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e166" xlink:type="simple"/></inline-formula>time), resulting in a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e167" xlink:type="simple"/></inline-formula>-dimensional parameter space. Because the “true” filter was not known, we quantified performance using relative cross-validation error, defined as the prediction error on an 8-minute test set (See <xref ref-type="sec" rid="s4">Methods</xref>). We varied the amount of data used for training, and performed 100 repetitions with randomly selected subsets of the full training data to obtain accurate estimates for each size training set.</p>
        <p><xref ref-type="fig" rid="pcbi-1002219-g006">Fig. 6</xref> (left) shows ML, ridge regression and ALDsf estimates for an example cell with a 1, 2 or 4 minutes of training data. Numbers in red indicate the average cross-validation error of each estimate. Note that with only 1 minute of data, ALDsf performed nearly as well as ML and ridge regression with 4 minutes of data. The middle panel shows a summary of cross-validation error for each of the five empirical Bayes estimators discussed previously, as a function of the amount of training data. ALDsf once again achieved substantially lower error than other methods. The right panel shows how many times more data were required to achieve the same level of cross-validation error as ALDsf. On average, ALDsf required 1.7 times less data than the next best method (ASD) and five times less data than maximum likelihood.</p>
        <fig id="pcbi-1002219-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Receptive field estimates for V1 simple cells.</title>
            <p>(Data from <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>). <bold>Left</bold>: Filter estimates obtained by ML, ridge regression, and ALDsf, for three different amounts of training data (1, 2, and 4 min). Numbers in red beneath each filter indicate relative cross-validation error. <bold>Middle</bold>: Relative cross validation error for each method, averaged across 16 neurons. ALDsf achieved the lowest average error, for all amounts of training data. <bold>Right</bold>: Number of times more training data required by each method to obtain the same error level of as ALDsf with 30s of training data. On average, the ML estimator required 5 times more training data, while ASD required 1.7 times more training data to match the performance of ALDsf.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g006" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pcbi-1002219-g007">Fig. 7</xref> shows the ML and ALDsf estimates for all 16 V1 simple cells in the population obtained with 1 minute of training data, as well as the ML estimate obtained using all the data available for each cell (40 minutes of data, on average). Note that for ALDsf recovers the qualitative structure of these RFs even when the underlying RF structure is barely discernible in the 1-minute ML estimate. Also note that the population exhibits substantial variability in RF shape, with many neurons whose RFs would not be well described by a fixed parametric form such as a Gabor filter.</p>
        <fig id="pcbi-1002219-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Receptive field estimates for the full set of sixteen V1 simple cells analyzed.</title>
            <p>(Data from <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>). <bold>Left</bold>: ML filter estimates from 1 minute of training data. <bold>Middle</bold>: ALDsf estimates from 1 minute of training data. <bold>Right</bold>: ML estimates from all data (an average of approximately 40 minutes of data per cell). Note the heterogeneity across cells, and that ALDsf captures the qualitative RF structure even when the 1-minute ML estimate is nearly indistinguishable from noise.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g007" xlink:type="simple"/>
        </fig>
        <p>We examined a second dataset of retinal ganglion cells (RGCs) in primate retina, which stimulated with 2D spatiotemporal white noise (“binary flicker”) <xref ref-type="bibr" rid="pcbi.1002219-Shlens1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Pillow3">[55]</xref>. The RFs considered had 3 coordinate dimensions (space<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e168" xlink:type="simple"/></inline-formula>space<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e169" xlink:type="simple"/></inline-formula>time), and a 2500-dimensional parameter space (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e170" xlink:type="simple"/></inline-formula> pixels in space<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e171" xlink:type="simple"/></inline-formula>25 8.33 ms-bins in time). <xref ref-type="fig" rid="pcbi-1002219-g008">Fig. 8</xref> shows the spatial (2D) and the temporal (1D) slices through the estimated 3D RFs (schematized at left). Even with only 1 minute of training data, the ALDsf estimate recovered the qualitative structure of the RF at all time points, including the filters' departure from spacetime separability (i.e., the center pixel has different timecourse than surround). By contrast, the ML estimate is indistinguishable from noise in many places, indicating that ALDsf can reveal qualitative structure that is not visible in the ML estimate. We examined 3 ON and 3 OFF RGCs, and found that error was 18 times higher in ML estimates and 6 times higher in ridge regression estimates than in ALDsf (where error was computed with respect to the ML estimate using a full 20 minutes of data).</p>
        <fig id="pcbi-1002219-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Comparison of 3D receptive field estimates for retinal data.</title>
            <p>(Data from Chichilnisky lab, <xref ref-type="bibr" rid="pcbi.1002219-Pillow3">[55]</xref>). <bold>Top row</bold>: Maximum likelihood and ALDsf estimates for an OFF retinal ganglion cell (RGC) receptive field, stimulated using 1 minute of binary spatiotemporal white noise. Left column shows a schematic of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e172" xlink:type="simple"/></inline-formula> pixel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e173" xlink:type="simple"/></inline-formula>25 time bin receptive field, containing 2500 total coefficients. Each time bin was 8.33 ms, corresponding to a frame rate of 120 Hz. Colored lines indicate specific pixels whose timecourses shown at right, and spatial time-slices, depicted as images at right (taken at the 4th and 8th time bins, indicated by green and purple arrows, respectively). The ML and ALDsf estimates with 1 minute of training data are shown alongside the ML estimate computed from 20 minutes of data. Pixel time-courses were rescaled to be unit vectors, so that differences in temporal profiles (i.e., spacetime non-separability of filter) can be observed. <bold>Bottom row</bold>: Similar plots for an ON RGC, with spatial profiles shown for the 5th and 8th time bins. In both cases, the ALDsf accurately recovered the shape and timecourse of the RF, while the ML estimate was often indistinguishable from noise. We examined RF estimates from 3 ON and 3 OFF cells, and found that, with 1 minute of training data, the average mean-squared-error between each estimate and a reference estimate (the ML estimate computed with 20 minutes of data) was 18 times larger for ML and 6.6 times larger for ridge regression than for ALDsf.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g008" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2h">
        <title>Quantifying uncertainty: Bayesian confidence intervals</title>
        <p>How can we quantify uncertainty in a receptive field estimate? The error bars shown in <xref ref-type="fig" rid="pcbi-1002219-g005">Figs. 5</xref> and <xref ref-type="fig" rid="pcbi-1002219-g006">6</xref> represent variability in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e174" xlink:type="simple"/></inline-formula> across resampled or permuted datasets. However, we would like to be able to measure the uncertainty in a single estimate given a single set of training data. Given the hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e175" xlink:type="simple"/></inline-formula>, the model specifies a Gaussian posterior (eq.9) with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e176" xlink:type="simple"/></inline-formula> and covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e177" xlink:type="simple"/></inline-formula>. The diagonal of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e178" xlink:type="simple"/></inline-formula> specifies the posterior variance for each element of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e179" xlink:type="simple"/></inline-formula>, giving us 95% credible intervals (Bayesian confidence intervals) of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e180" xlink:type="simple"/><label>(14)</label></disp-formula>The interpretation of these credible intervals is that, given the data and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e181" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e182" xlink:type="simple"/></inline-formula>. More generally, for any unit vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e183" xlink:type="simple"/></inline-formula>, the credible interval of size (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e184" xlink:type="simple"/></inline-formula>) for the projection <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e185" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e186" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e187" xlink:type="simple"/></inline-formula> is the inverse normal cumulative density function.</p>
        <p>However, these credible intervals, and the associated Gaussian posterior for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e188" xlink:type="simple"/></inline-formula>, are conditioned on maximum-evidence estimate of the hyper-parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e189" xlink:type="simple"/></inline-formula>. These intervals fail to take into account uncertainty in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e190" xlink:type="simple"/></inline-formula>, which may be substantial if the evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e191" xlink:type="simple"/></inline-formula> is not tightly concentrated around its maximum. The true uncertainty in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e192" xlink:type="simple"/></inline-formula> will therefore generally be greater than that captured by the posterior covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e193" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2i">
        <title>Fully Bayesian inference</title>
        <p>To accurately quantify uncertainty, we may wish to perform fully Bayesian inference under the priors introduced above. Empirical Bayes (EB) inference can be interpreted as an approximate form of fully Bayesian (FB) inference in a hierarchical model <xref ref-type="bibr" rid="pcbi.1002219-Kass1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-MacKay1">[45]</xref>. If we incorporate a prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e194" xlink:type="simple"/></inline-formula> over the hyperparameters at the top level of the graphical model shown in <xref ref-type="fig" rid="pcbi-1002219-g001">Fig. 1 B</xref>, also known as a <italic>hyperprior</italic>, we will have a complete hierarchical model of the neural response. The difference between EB and FB inference for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e195" xlink:type="simple"/></inline-formula> comes down to the fact that the FB prior involves marginalizing over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e196" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e197" xlink:type="simple"/><label>(15)</label></disp-formula>while the EB prior is just the conditional distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e198" xlink:type="simple"/></inline-formula>. When are these priors equivalent or, more importantly, when do the EB and FB estimates agree?</p>
        <p>The relationship between EB and FB inference can be understood by examining the posterior distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e199" xlink:type="simple"/></inline-formula>. The full posterior is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e200" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e201" xlink:type="simple"/></inline-formula> is the posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e202" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e203" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e204" xlink:type="simple"/></inline-formula> is proportional to the evidence (i.e. the exponential of (eq.10)) times the hyperprior:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e205" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e206" xlink:type="simple"/></inline-formula> is a normalizing constant. Note that if the evidence is proportional to a delta function at its maximum, then the posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e207" xlink:type="simple"/></inline-formula> is itself a delta function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e208" xlink:type="simple"/></inline-formula>. The full posterior then reduces to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e209" xlink:type="simple"/><label>(18)</label></disp-formula>which is the EB posterior (i.e., the posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e210" xlink:type="simple"/></inline-formula> conditioned on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e211" xlink:type="simple"/></inline-formula>). Thus, EB and FB inference are identical when the evidence is proportional to a delta function, and the two methods will in general give similar results whenever the evidence is highly concentrated around its maximum <xref ref-type="bibr" rid="pcbi.1002219-MacKay1">[45]</xref>. In general, EB and FB estimates will always agree given enough data, since by central limit theorem, the evidence will concentrate around its maximum with variance that falls as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e212" xlink:type="simple"/></inline-formula>. However, for finite datasets, the two may differ.</p>
        <p>To examine the proximity of EB and FB estimates and credible intervals, we developed a sampling-based algorithm to perform FB inference under the ALD prior. The factorization shown in (eq.16) suggests an efficient method for sampling from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e213" xlink:type="simple"/></inline-formula> via Markov Chain Monte Carlo (MCMC), using a Markov chain over the space of the hyperparameters whose stationary distribution is proportional to the evidence. The summary of the algorithm for sampling <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e214" xlink:type="simple"/></inline-formula> is as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e215" xlink:type="simple"/><label>(19)</label></disp-formula>A nice feature of this approach is that the hyperparameters live in relatively low-dimension (e.g., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e216" xlink:type="simple"/></inline-formula> for a 1D filter and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e217" xlink:type="simple"/></inline-formula> for a 2D filter under ALDsf). The Markov Chain therefore only has to explore this low-dimensional space, instead of the high-dimensional space of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e218" xlink:type="simple"/></inline-formula>, which contains tens to thousands of parameters in typical cases <xref ref-type="bibr" rid="pcbi.1002219-Schummers1">[57]</xref>. Samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e219" xlink:type="simple"/></inline-formula> are obtained by drawing from the Gaussian conditioned on each MCMC sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e220" xlink:type="simple"/></inline-formula>. These samples may be averaged to the posterior mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e221" xlink:type="simple"/></inline-formula>, also known as the <italic>Bayes Least-Squares</italic> estimate, and their quantiles provide credible intervals. (See Method).</p>
        <p><xref ref-type="fig" rid="pcbi-1002219-g009">Fig. 9</xref> shows a comparison of EB and FB estimates and credible intervals for the 1D simulated example shown previously. The hyperprior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e222" xlink:type="simple"/></inline-formula> was taken to be uniform over a large region (See <xref ref-type="sec" rid="s4">Methods</xref>). For a small dataset, the FB credible intervals were noticeably larger than the EB credible intervals, as expected, owing to the effects of uncertainty in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e223" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002219-Kass1">[35]</xref>. For larger datasets, this discrepancy was much smaller, and was smaller in general for ALDsf than ALDs or ALDf intervals. The EB and FB (Bayes least-squares) filter estimates, however, did not differ noticeably even for small amounts of data. <xref ref-type="fig" rid="pcbi-1002219-g010">Fig. 10</xref> shows a comparison of EB and FB inference for the V1 neural data presented in <xref ref-type="fig" rid="pcbi-1002219-g006">Fig. 6</xref>. For small datasets, the FB credible intervals were larger than EB intervals, but cross-validation error did not differ noticeably across dataset sizes. This suggests that the higher computational cost of FB inference may not be justified unless one is interested in obtaining accurate quantification of uncertainty from a small or noisy dataset.</p>
        <fig id="pcbi-1002219-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Empirical Bayes (EB) and fully Bayes (FB) credible intervals on simulated data.</title>
            <p><bold>Left</bold>: FB and EB 95% credible intervals, computed from 100 samples of training data, for ALDs (above), ALDf (middle), and ALDsf (bottom). The true filter is shown in black. FB intervals are larger than EB intervals, due to the incorporation of uncertainty in the hyperparameters under fully Bayesian inference. <bold>Right</bold>: Credible intervals computed from 500 samples of training data. As the amount of training data increases, the FB and EB credible regions became indistinguishable, indicating that the evidence is tightly constrained around its maximum. For both amounts of training data, the posterior mean under FB and EB were virtually identical.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g009" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002219-g010" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002219.g010</object-id>
          <label>Figure 10</label>
          <caption>
            <title>Empirical Bayes (EB) and fully Bayesian (FB) estimates on V1 data.</title>
            <p>(<bold>A</bold>) ALDsf estimates for a single V1 simple cell under EB and FB inference, from 30 seconds (above) and 4 minutes (below) of training data. There was no significant difference in cross-validation error (numbers below in red, averaged over 100 resampled training sets). (<bold>B</bold>) Marginal posterior variance of RF coefficients, averaged across pixels and across all 16 cells, under EB and FB inference. As expected, FB estimates of the posterior variance were higher, especially for small datasets, reflecting the effects of posterior uncertainty in the hyperparameters. (<bold>C</bold>) Average cross-validation error across 16 cells for FB and EB estimates. For all amounts of training data, error rates were nearly identical, indicating that the FB posterior mean (computed via MCMC) is not superior to the more computationally inexpensive EB estimate.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.g010" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have described a new family of priors for Bayesian receptive field estimation that seek to simultaneously exploit locality in spacetime and spatiotemporal frequency. We have shown that empirical Bayes estimates under a localized prior are more accurate than those obtained under alternative priors designed to incorporate sparsity and smoothness. Although the ALD prior does not explicitly impose sparseness or smoothness, the estimates obtained with realistic neural data were both sparse and smooth. Sparsity arises from the fact that pixels outside a central region fall to zero, while smoothness arises from the fact that Fourier coefficients outside some low-frequency region fall to zero. However, for a receptive field dominated by high frequency components, ALD should outperform ASD and other smoothed estimates (e.g., smooth RVM <xref ref-type="bibr" rid="pcbi.1002219-Schmolck1">[47]</xref>, fused lasso <xref ref-type="bibr" rid="pcbi.1002219-Tibshirani2">[58]</xref>), since it can also select regions centered on high frequencies.</p>
      <p>We have also derived an algorithm for performing fully Bayesian inference under ALD, ASD, and ridge regression priors. The algorithm exploits the low-dimensionality of the hyperparameter space and the tractability of the evidence to perform MCMC sampling of the posterior over hyperparameters. The full prior takes the form of a <italic>Gaussian scale mixture</italic> <xref ref-type="bibr" rid="pcbi.1002219-Wainwright1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Park1">[60]</xref>, a mixture of zero-mean Gaussians with covariances <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e224" xlink:type="simple"/></inline-formula> and mixing weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e225" xlink:type="simple"/></inline-formula>, resulting in a Gaussian posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e226" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e227" xlink:type="simple"/></inline-formula> that is trivial to sample. MCMC sampling allows for the calculation of fully Bayesian credible intervals over RF coefficients, which we found to be systematically larger than empirical Bayesian intervals. Nevertheless, we found no differences in the quantitative performance of EB and FB receptive field estimates with either simulated or real neural data (<xref ref-type="fig" rid="pcbi-1002219-g009">Figs. 9</xref> and <xref ref-type="fig" rid="pcbi-1002219-g010">10</xref>). Of course, both intervals rely on the linear-Gaussian model of the neural response, which may be inaccurate in cases where the neural response noise is highly non-Gaussian (e.g., heavy-tailed).</p>
      <p>More generally, this work highlights the advantages of locality as an additional source of prior information in biological inference problems. Shrinkage and sparsity have attracted considerable attention in statistics, and they have advantageous properties for a variety of high-dimensional inference problems <xref ref-type="bibr" rid="pcbi.1002219-James1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Tibshirani1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Donoho1">[61]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Donoho2">[62]</xref>. ALD exploits a stronger form of prior information, assuming that large groups of coefficients go to zero in a correlated manner. This may not hold for generic regression problems; for a sparse filter with randomly distributed non-zero coefficients, the ARD estimate substantially outperforms ALD (not shown), but such filters are unlikely to arise in neural systems.</p>
      <p>Two general ideas that arise from ALD may be useful for thinking about statistical inference in other biological and non-biological systems. The first is the idea of exploiting an underlying coordinate system or topography. Whenever the regression coefficients can be arranged topographically (e.g., temporally, spatially, spectrally), it may be possible to design a prior that exploits dependencies within this topography using a small number of hyperparameters. This idea is central to ALD as well as to ASD, which uses the distances between RF pixels to set their prior correlation. But other coordinates and prior parameterizations are possible. For example, although ALD performs reasonably well for a simulated grid cell (<xref ref-type="fig" rid="pcbi-1002219-g004">Fig. 4 D</xref>), locality in space does not hold for grid cells, and a prior that exploits the “natural” parameters of grid cell responses (e.g., grid spacing, size, orientation, phase) might perform even better. Optimizing the hyperparameters governing such a prior is tractable with empirical Bayes. The second idea that arises from ALD is that of simultaneously constraining a set of regression coefficients in two (or more) different bases. The ALDsf method combines a local prior in a spacetime basis and a local prior in Fourier basis via a “sandwich matrix” (eq.13), which effectively applies prior constraints in series: first in spacetime and then in frequency. Another solution would be to combine the two priors symmetrically, e.g., using prior covariance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e228" xlink:type="simple"/></inline-formula>. (This is the covariance that results from taking the product of the ALDs and ALDf Gaussian priors). We found this formulation to perform slightly worse on test data, but results were similar. Note that the sum of prior covariances <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e229" xlink:type="simple"/></inline-formula> would <italic>not</italic> achieve the desired goal of imposing the prior constraints simultaneously, since it would prune only those coefficients in the (effective) null space of both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e230" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e231" xlink:type="simple"/></inline-formula>. A large literature has examined regularization and feature selection in overcomplete dictionaries (e.g., “basis pursuit”) <xref ref-type="bibr" rid="pcbi.1002219-Donoho2">[62]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Mineault1">[65]</xref>, but combining structured prior information defined in different bases poses an intriguing open problem.</p>
      <p>One potential criticism of ALD is that the linear-Gaussian encoding model (eq.1) is overly simplistic. Despite its simplicity, this model has a long history in the neural characterization literature <xref ref-type="bibr" rid="pcbi.1002219-Jones1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Korenberg1">[18]</xref>, and the estimators considered here are consistent (i.e., converge asymptotically) for responses generated by any linear-nonlinear response model, so long as the stimuli are elliptically symmetric and the expected STA is non-zero <xref ref-type="bibr" rid="pcbi.1002219-Paninski2">[20]</xref>. We addressed whether the linear-Gaussian modeling assumption undermines our results by re-analyzing the V1 simple cell data with maximally informative dimensions (MID) <xref ref-type="bibr" rid="pcbi.1002219-Sharpee1">[66]</xref>, an information-theoretic estimator that incorporates neural nonlinearities and Poisson spiking. The results (shown in Supporting Information (<xref ref-type="supplementary-material" rid="pcbi.1002219.s002">Text S1</xref>), <xref ref-type="supplementary-material" rid="pcbi.1002219.s001">Fig. S1</xref>), indicate that MID errors were large, comparable in size to those of the maximum likelihood (linear regression) estimate. Even when comparing to the MID filter computed from test data, ALDsf outperformed MID by a substantial margin. This shows that the limitations of the linear-Gaussian model do not substantially undermine its performance on simple cells. However, we have applied ALD only to neurons whose responses exhibit a quasi-linear relationship to the stimulus. ALD would indeed fail for a neuron with a symmetric nonlinearity (e.g., squaring) and cannot recover multiple filters (e.g., those driving a complex cell). A variety of techniques exist estimating multi-dimensional feature spaces (e.g., spike-triggered covariance (STC) <xref ref-type="bibr" rid="pcbi.1002219-deRuytervanSteveninck1">[67]</xref>–<xref ref-type="bibr" rid="pcbi.1002219-Schwartz1">[69]</xref>, MID <xref ref-type="bibr" rid="pcbi.1002219-Paninski2">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Sharpee1">[66]</xref>, iSTAC <xref ref-type="bibr" rid="pcbi.1002219-Pillow4">[70]</xref>, spike-triggered ICA <xref ref-type="bibr" rid="pcbi.1002219-Saleem1">[71]</xref>). However, the “kernel trick” <xref ref-type="bibr" rid="pcbi.1002219-Wu1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Bishop1">[41]</xref>, which involves using linear methods on nonlinearly transformed stimuli, provides the simplest method for extending ALD to nonlinear response models. Many nonlinear transformations (e.g., transforming the stimulus to its Fourier power <xref ref-type="bibr" rid="pcbi.1002219-David2">[72]</xref>) preserve the topography of the underlying stimulus, making this approach directly applicable to ALD.</p>
      <p>One advantage of the linear-Gaussian model is its computational tractability. ALD is fast because the evidence can be calculated and optimized entirely from the sufficient statistics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e232" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e233" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e234" xlink:type="simple"/></inline-formula> (the raw stimulus covariance, the STA, and sum of squared responses, respectively). This means that the computational cost does not scale with the amount of data (unlike MID and maximum-likelihood point process methods). Evidence optimization is also much faster than cross-validation, particularly with the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e235" xlink:type="simple"/></inline-formula> hyperparameters employed by ALDsf. The computational cost of ALD is still at least <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e236" xlink:type="simple"/></inline-formula> in the number of filter coefficients, since evidence evaluation requires left-division by matrices of size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e237" xlink:type="simple"/></inline-formula>. However, the number of approximately zero coefficients often falls considerably during optimization, and eliminating these coefficients by thresholding small eigenvalues of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e238" xlink:type="simple"/></inline-formula> can speed convergence considerably.</p>
      <p>Given the hyperparameters, the log-posterior over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e239" xlink:type="simple"/></inline-formula> is concave, with a single maximum that can be computed in closed form (eq.5). Although the log-evidence (eq.10) is <italic>not</italic> concave in the hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e240" xlink:type="simple"/></inline-formula>, there are far fewer hyperparameters than parameters, making ALD far easier than non-convex optimization in the full space of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e241" xlink:type="simple"/></inline-formula> (e.g., as in MID). We can maximize the evidence more rapidly by using its first and second derivatives, which we can compute analytically (see <xref ref-type="sec" rid="s4">Methods</xref>). We also exploit a heuristic strategy for initializing the ALDsf hyperparameters using the estimates from ridge regression (to identify the scale), ALDs (to identify a spatiotemporal region) and ALDf (to identify a Fourier region). Although it is substantially more computationally expensive, the fully Bayesian estimate based on MCMC avoids the issue of local maxima because it explores the entire evidence surface, not just its modes.</p>
      <p>However, we do not ultimately view ALD and other model-based or information-based methods as in conflict. Rather, we regard ALD as providing a prior distribution over RFs that can be combined with any likelihood. Computing and optimizing the evidence under nonlinear models with non-Gaussian noise represents an important direction for future work. We suggest that locality is a general feature of neural information processing and anticipate that it will be useful for neural characterization in a wide variety of brain areas, including those where response properties are not yet well understood <xref ref-type="bibr" rid="pcbi.1002219-David3">[73]</xref>. We expect hierarchical models and empirical and fully Bayesian inference methods to find application to a wide range of problems where structured prior information can be usefully defined.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Implementation of RF estimators</title>
        <sec id="s4a1">
          <title>Ridge regression</title>
          <p>For simulated and real datasets, we computed the empirical Bayes ridge regression estimate as follows. First, we initialized the noise variance to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e242" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e243" xlink:type="simple"/></inline-formula> was the ML estimate, and the inverse prior variance to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e244" xlink:type="simple"/></inline-formula>. Then, we ran an iterative fixed-point algorithm <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-MacKay1">[45]</xref> to optimize the evidence for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e245" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e246" xlink:type="simple"/></inline-formula>,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e247" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e248" xlink:type="simple"/></inline-formula> is the parameter dimensionality (number of RF coefficients), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e249" xlink:type="simple"/></inline-formula> is the number of samples, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e250" xlink:type="simple"/></inline-formula> is the posterior mean (eq.9), and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e251" xlink:type="simple"/></inline-formula> are the diagonal elements of the posterior covariance. The posterior mean and covariance are recomputed after each update to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e252" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e253" xlink:type="simple"/></inline-formula>. Note that (following <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref>) we treat the noise variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e254" xlink:type="simple"/></inline-formula> as a hyperparameter, maximizing instead of integrating it out, which is computationally more tractable, although technically it appears in the likelihood rather than the prior.</p>
        </sec>
        <sec id="s4a2">
          <title>Automatic Relevance Determination (ARD)</title>
          <p>We initialized the noise variance and ARD hyperparameters using the maximum-evidence values obtained from the ridge regression prior: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e255" xlink:type="simple"/></inline-formula> and inverse prior variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e256" xlink:type="simple"/></inline-formula>. Then, we updated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e257" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e258" xlink:type="simple"/></inline-formula> using the fixed-point rule given in <xref ref-type="bibr" rid="pcbi.1002219-Tipping1">[36]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e259" xlink:type="simple"/><label>(21)</label></disp-formula></p>
        </sec>
        <sec id="s4a3">
          <title>Lasso</title>
          <p>We computed the Lasso estimate using the algorithm introduced in <xref ref-type="bibr" rid="pcbi.1002219-Friedman1">[74]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Friedman2">[75]</xref>, using software available at <ext-link ext-link-type="uri" xlink:href="http://www-stat.stanford.edu/~tibs/glmnet-matlab" xlink:type="simple">http://www-stat.stanford.edu/~tibs/glmnet-matlab</ext-link>. This implementation performs cyclical coordinate descent in a pathwise fashion. We used a test dataset with 2000 samples to find the optimal value of the lasso parameter (<xref ref-type="fig" rid="pcbi-1002219-g002">Fig. 2</xref>).</p>
        </sec>
        <sec id="s4a4">
          <title>Automatic Smoothness Determination (ASD)</title>
          <p>We computed the ASD estimate by gradient ascent of the log-evidence function, following the methods in <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>. Briefly, we initialized using the hyperparameter estimates from ridge regression: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e260" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e261" xlink:type="simple"/></inline-formula>, initialized the smoothness parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e262" xlink:type="simple"/></inline-formula> to 1, then minimized the negative log-evidence for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e263" xlink:type="simple"/></inline-formula> using analytically computed gradients (provided in <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>) and Hessians, which we derive below. We performed minimization using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e264" xlink:type="simple"/></inline-formula> in MATLAB, with boundary conditions for the hyperparameters and the noise variance set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e265" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e266" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e267" xlink:type="simple"/></inline-formula>, which we selected to be far larger than the range of probable values.</p>
        </sec>
        <sec id="s4a5">
          <title>Automatic Locality Determination (ALD)</title>
          <p>We computed ALD estimates by numerical optimization of the log-evidence using the analytically computed gradient and Hessian (second derivative matrix). For notational convenience, we will denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e268" xlink:type="simple"/></inline-formula>, the first derivative of a quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e269" xlink:type="simple"/></inline-formula> with respect to a parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e270" xlink:type="simple"/></inline-formula>, as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e271" xlink:type="simple"/></inline-formula>, and denote the second derivative <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e272" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e273" xlink:type="simple"/></inline-formula>.</p>
          <p>The first derivatives of the log-evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e274" xlink:type="simple"/></inline-formula> with respect to the hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e275" xlink:type="simple"/></inline-formula> and the observation noise <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e276" xlink:type="simple"/></inline-formula> are given by <xref ref-type="bibr" rid="pcbi.1002219-Sahani1">[11]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e277" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e278" xlink:type="simple"/></inline-formula> is the derivative of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e279" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e280" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e281" xlink:type="simple"/></inline-formula> is the number of training samples, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e282" xlink:type="simple"/></inline-formula> is dimensionality of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e283" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e284" xlink:type="simple"/></inline-formula> is the squared residual error, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e285" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e286" xlink:type="simple"/></inline-formula> are the posterior covariance and mean, respectively (eq.9). The corresponding second derivatives are given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e287" xlink:type="simple"/><label>(23)</label></disp-formula>These expressions involve the derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e288" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e289" xlink:type="simple"/></inline-formula> and with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e290" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e291" xlink:type="simple"/></inline-formula>, which are matrices and vectors of the same size as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e292" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e293" xlink:type="simple"/></inline-formula>, given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e294" xlink:type="simple"/><label>(24)</label></disp-formula></p>
          <p>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e295" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e296" xlink:type="simple"/></inline-formula> is numerically unstable when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e297" xlink:type="simple"/></inline-formula> becomes ill-conditioned. Thus we never compute the inverse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e298" xlink:type="simple"/></inline-formula> explicitly. Instead, we exploit the Woodbury matrix identity to compute the evidence and other quantities using matrices that are well-conditioned. The resulting expressions involve matrix left division instead of inversion (computed via the backslash operator in Matlab; see Supplementary Information for details). Code is available from the last authors website (<ext-link ext-link-type="uri" xlink:href="http://pillowlab.cps.utexas.edu/code.html" xlink:type="simple">http://pillowlab.cps.utexas.edu/code.html</ext-link>). Below, we provide the partial derivatives <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e299" xlink:type="simple"/></inline-formula> for the various ALD hyperparameters, which are all that is required for computing the gradient and Hessian of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e300" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s4a6">
          <title>ALD in spacetime (ALDs)</title>
          <p>The hyperparameters governing the ALDs prior covariance matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e301" xlink:type="simple"/></inline-formula> are (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e302" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e303" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e304" xlink:type="simple"/></inline-formula>), where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e305" xlink:type="simple"/></inline-formula> defines the mean of the localized RF, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e306" xlink:type="simple"/></inline-formula> defines its elliptical extent, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e307" xlink:type="simple"/></inline-formula> defines the scale of the prior variance (as in ASD). For filters with coordinate dimension <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e308" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e309" xlink:type="simple"/></inline-formula> is a vector and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e310" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e311" xlink:type="simple"/></inline-formula> matrix that we parametrized (e.g., for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e312" xlink:type="simple"/></inline-formula>) as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e313" xlink:type="simple"/><label>(25)</label></disp-formula>For a one-dimensional filter, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e314" xlink:type="simple"/></inline-formula>, while for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e315" xlink:type="simple"/></inline-formula>, we have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e316" xlink:type="simple"/></inline-formula> defined in terms of six hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e317" xlink:type="simple"/></inline-formula>. The first and second derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e318" xlink:type="simple"/></inline-formula> with respect to these hyperparameters (for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e319" xlink:type="simple"/></inline-formula>) are given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e320" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e321" xlink:type="simple"/></inline-formula> is a matrix of differences between pixel coordinates in spacetime and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e322" xlink:type="simple"/></inline-formula>.</p>
          <p>During optimization, local maxima can be a problem if one initializes with a prior region that does not cover the location where the receptive field is largest. To avoid local maxima, we initialized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e323" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e324" xlink:type="simple"/></inline-formula> to the noise variance from ridge regression and to the center of mass of the ridge regression estimate, respectively. We used a coarse grid search for initializing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e325" xlink:type="simple"/></inline-formula>, with off-diagonal terms <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e326" xlink:type="simple"/></inline-formula>. From the best initial point, we then descended the negative log-evidence using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e327" xlink:type="simple"/></inline-formula> in MATLAB, with analytically computed gradients and Hessians given above. Hyperparameters were constrained to fall within the ranges <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e328" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e329" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e330" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e331" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e332" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e333" xlink:type="simple"/></inline-formula> is the number of filter elements along the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e334" xlink:type="simple"/></inline-formula>'th coordinate dimension.</p>
        </sec>
        <sec id="s4a7">
          <title>ALD in spatiotemporal frequency (ALDf)</title>
          <p>We implemented ALDf using the method similar to that described above for ALDs, after performing an orthogonal fast Fourier transform (FFT) on the stimuli <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e335" xlink:type="simple"/></inline-formula>, which amounts to a change-of-basis (i.e., multiplication by a unitary matrix).</p>
          <p>As described in <xref ref-type="sec" rid="s2">Results</xref>, for filters with coordinate dimension <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e336" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e337" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e338" xlink:type="simple"/></inline-formula> vector. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e339" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e340" xlink:type="simple"/></inline-formula> symmetric matrix, parametrized as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e341" xlink:type="simple"/><label>(27)</label></disp-formula></p>
          <p>The first and second derivatives of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e342" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e343" xlink:type="simple"/></inline-formula> with respect to these hyperparameters are given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e344" xlink:type="simple"/><label>(28)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e345" xlink:type="simple"/></inline-formula> is a matrix of differences between pixel coordinates in frequency and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e346" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e347" xlink:type="simple"/></inline-formula>.</p>
          <p>Analogously to ALDs, to avoid local maxima, we initialized <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e348" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e349" xlink:type="simple"/></inline-formula> to the noise variance from ridge regression and to the centroid of the region of maximal power in the Fourier transform of the ridge-regression estimate, respectively. We used a coarse grid search for initializing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e350" xlink:type="simple"/></inline-formula>. From the best initial point, we then performed optimization as described above, with boundary conditions for the noise variance and hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e351" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e352" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e353" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e354" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e355" xlink:type="simple"/></inline-formula> is the number of filter elements along the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e356" xlink:type="simple"/></inline-formula>'th coordinate dimension. Once we found the filter estimate in Fourier domain, we projected it back to the spacetime domain via the inverse FFT.</p>
        </sec>
        <sec id="s4a8">
          <title>ALD in spacetime and frequency (ALDsf)</title>
          <p>For the jointly localized prior, we first obtained the maximum-evidence estimates for the ALDs and ALDf covariance matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e357" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e358" xlink:type="simple"/></inline-formula> (eqs. 11 and 12). We then performed the optimization of the log-evidence for the full set of ALDsf hyperparameters using <italic>fmincon</italic> in MATLAB, using analytic gradient and Hessian (introduced above), with the boundary conditions for the noise variance and hyperparameters set to the same values as above.</p>
        </sec>
      </sec>
      <sec id="s4b">
        <title>Application to simulated and real neural data</title>
        <p>For the simulated data shown in <xref ref-type="fig" rid="pcbi-1002219-g005">Fig. 5</xref> , we used a 2-dimensional Gabor filter (shown in <xref ref-type="fig" rid="pcbi-1002219-g004">Fig. 4 A</xref>) and two types of stimuli: Gaussian white noise and “naturalistic spectrum” noise–Gaussian noise with a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e359" xlink:type="simple"/></inline-formula> power spectrum. Simulations were carried out with various numbers of stimulus samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e360" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e361" xlink:type="simple"/></inline-formula>, noise variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e362" xlink:type="simple"/></inline-formula>, signal variance of 1, and a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e363" xlink:type="simple"/></inline-formula> pixel filter (coordinate dimension <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e364" xlink:type="simple"/></inline-formula>, filter dimension <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e365" xlink:type="simple"/></inline-formula>). To quantify performance, we defined the filter error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e366" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e367" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e368" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e369" xlink:type="simple"/></inline-formula> is the true filter and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e370" xlink:type="simple"/></inline-formula> is an estimate. To obtain reliable estimates of mean error, we ran 100 simulations at each sample size. To calculate the relative error (<xref ref-type="fig" rid="pcbi-1002219-g005">Fig. 5 B and D</xref>), we computed the error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e371" xlink:type="simple"/></inline-formula> for each method, and then computed the geometric mean of the error ratio <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e372" xlink:type="simple"/></inline-formula> across datasets.</p>
        <p>For V1 data shown in <xref ref-type="fig" rid="pcbi-1002219-g006">Fig. 6</xref> , the data and experimental methods are described in <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>. Briefly, cells were stimulated with 1D spatiotemporal binary white noise stimuli (“flickering bars”) aligned with each neuron's preferred orientation. Stimuli were presented at a frame rate of 100 Hz. The number of bars <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e373" xlink:type="simple"/></inline-formula> varied for different neurons, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e374" xlink:type="simple"/></inline-formula>. The linear receptive field was assumed to extend over a time window of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e375" xlink:type="simple"/></inline-formula> frames before a spike (a 160 ms time interval). The full dimensionality of the filter was thus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e376" xlink:type="simple"/></inline-formula>, ranging from 192 to 384 parameters.</p>
        <p>For retinal ganglion cell data shown in <xref ref-type="fig" rid="pcbi-1002219-g008">Fig. 8</xref> , the data and experimental methods are described in <xref ref-type="bibr" rid="pcbi.1002219-Shlens1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Pillow3">[55]</xref>. Briefly, cells were stimulated with the spatiotemporal binary white noise stimuli presented at a frame rate of 120 Hz, contained in 10×10 pixels in space. We assumed the size of the linear receptive field to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e377" xlink:type="simple"/></inline-formula> pixel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e378" xlink:type="simple"/></inline-formula>25 time bin, making for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e379" xlink:type="simple"/></inline-formula> total coefficients in the RF.</p>
        <p>We used cross-validation to quantify the performance of the various estimators (<xref ref-type="fig" rid="pcbi-1002219-g006">Fig. 6</xref>), and resampled the training data to examine performance as a function of training sample size. To quantify error reliably, we performed 100 repetitions for each sample size, drawing the training data randomly without replacement in blocks of size 2s, which helped to minimize the effects of non-stationarities in the data. To quantify cross-validation performance, we used relative cross-validation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e380" xlink:type="simple"/></inline-formula>, defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e381" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e382" xlink:type="simple"/></inline-formula> is the number of samples of test data, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e383" xlink:type="simple"/></inline-formula> is a spike count in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e384" xlink:type="simple"/></inline-formula>'th time bin in the test set, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e385" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e386" xlink:type="simple"/></inline-formula>th row of the design matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e387" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e388" xlink:type="simple"/></inline-formula> is the RF estimate obtained by each method (from training data), and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e389" xlink:type="simple"/></inline-formula> is the ML estimate obtained on the test data. Essentially, this is the ordinary test error minus the error of the ML estimator trained on test data (which provides an absolute lower bound on the performance of any linear model). We computed the relative cross-validation errors from five methods (ML, Ridge, ARD, ASD, and ALDsf) using 8 minutes of test data. In <xref ref-type="fig" rid="pcbi-1002219-g006">Fig. 6</xref>, we normalized the errors by dividing them by maximum average error across methods (the ML estimate using 30 seconds of data yielded the maximum cross-validation error). We computed the standard deviation of the normalized cross-validation error across 100 different training sets for each dataset size.</p>
      </sec>
      <sec id="s4c">
        <title>Fully Bayesian inference (MCMC)</title>
        <p>To perform fully Bayesian inference, we used Metropolis-Hastings (MH) sampling to sample from the distribution over hyperparameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e390" xlink:type="simple"/></inline-formula> given the data <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e391" xlink:type="simple"/></inline-formula>. We used an isotropic Gaussian proposal distribution with variance given by the largest eigenvalue of inverse Hessian of the log-evidence around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e392" xlink:type="simple"/></inline-formula>. (More advanced proposal distributions and sampling methods are found in <xref ref-type="bibr" rid="pcbi.1002219-Neal1">[76]</xref>, <xref ref-type="bibr" rid="pcbi.1002219-Ahmadian1">[77]</xref>, but this simple proposal sufficed for our purposes and mixed reasonably quickly). Thus, we first optimized the evidence to obtain the mode <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e393" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e394" xlink:type="simple"/></inline-formula>, which is the mode of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e395" xlink:type="simple"/></inline-formula>. We assumed a non-informative hyperprior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e396" xlink:type="simple"/></inline-formula>, taken to be uniform over the range of values permitted during constrained optimization of the log-evidence (see above).</p>
        <p>To carry out MH sampling, we sampled from the Gaussian proposal distribution centered on the current state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e397" xlink:type="simple"/></inline-formula> of the Markov chain, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e398" xlink:type="simple"/></inline-formula>, then computed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e399" xlink:type="simple"/></inline-formula>, with the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e400" xlink:type="simple"/></inline-formula>. We accepted the proposal randomly with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e401" xlink:type="simple"/></inline-formula>, setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e402" xlink:type="simple"/></inline-formula>, and otherwise rejected it, setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e403" xlink:type="simple"/></inline-formula>. Given each sample <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e404" xlink:type="simple"/></inline-formula>, we drew a sample of the receptive field <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002219.e405" xlink:type="simple"/></inline-formula>. These samples were averaged to compute the posterior mean (or Bayes Least Squares estimator). Their quantiles were used to compute credible intervals for each filter coefficient.</p>
        <p>In <xref ref-type="fig" rid="pcbi-1002219-g010">Fig. 10</xref> , we compared fully Bayesian (FB) and empirical Bayes (EB) filter estimates obtained from V1 simple cell data <xref ref-type="bibr" rid="pcbi.1002219-Rust1">[53]</xref>. For each set of training data, we drew 5000 samples using MH to compute the posterior mean and credible intervals. The average acceptance rate of the MH sampler was 0.12. For <xref ref-type="fig" rid="pcbi-1002219-g010">Fig. 10 A</xref>, we computed the average of the EB and FB error from 100 repetitions with independently drawn sets of training data. We computed the average cross-validation error of both estimates of the example cell (in red). For <xref ref-type="fig" rid="pcbi-1002219-g010">Fig. 10 B</xref>, we computed the average posterior variance by averaging the posterior variances in the estimates from the 100 iterations in each cell, which we then averaged across all 16 cells. For <xref ref-type="fig" rid="pcbi-1002219-g010">Fig. 10 C</xref>, we computed the average cross-validation error by averaging the errors from the 100 iterations in each cell, and we averaged these across 16 cells. The same 8 minutes of held out test data was used for cross-validation, for all training iterations.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002219.s001" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p><xref ref-type="supplementary-material" rid="pcbi.1002219.s001">Figure S1</xref> shows the comparison of ALD and MID estimates.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002219.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002219.s002" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Supporting information to 1) compare the performance of the ALDsf estimator under the linear Gaussian model to the MID estimator which is equivalent to the maximum likelihood estimator under the linear-nonlinear Poisson cascade model; 2) provide expressions for the quantities for computing the log-evidence.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank NC Rust and JA Movshon for V1 data, and thank J Shlens, AM Litke, A Sher, and EJ Chichilnisky for retinal data. We are grateful to L Paninski, M Sahani, and EP Simoncelli for helpful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002219-Lee1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>Y</given-names></name><name name-style="western"><surname>Schetzen</surname><given-names>M</given-names></name></person-group>             <year>1965</year>             <article-title>Measurement of the wiener kernels of a non-linear system by crosscorrelation (Wiener kernels of nonlinear system based on cross-correlation techniques).</article-title>             <source>Int J Control</source>             <volume>2</volume>             <fpage>237</fpage>             <lpage>254</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-deBoer1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>deBoer</surname><given-names>E</given-names></name><name name-style="western"><surname>Kuyper</surname><given-names>P</given-names></name></person-group>             <year>1968</year>             <article-title>Triggered correlation.</article-title>             <source>IEEE T Bio-Med Eng</source>             <volume>15</volume>             <fpage>169</fpage>             <lpage>179</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Marmarelis1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Marmarelis</surname><given-names>PZ</given-names></name><name name-style="western"><surname>Naka</surname><given-names>K</given-names></name></person-group>             <year>1972</year>             <article-title>White-noise analysis of a neuron chain: an application of the Wiener theory.</article-title>             <source>Science</source>             <volume>175</volume>             <fpage>1276</fpage>             <lpage>1278</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Victor1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name><name name-style="western"><surname>Shapley</surname><given-names>RM</given-names></name></person-group>             <year>1979</year>             <article-title>Receptive field mechansims of cat x and y retinal ganglion cells.</article-title>             <source>J Gen Physiol</source>             <volume>74</volume>             <fpage>275</fpage>             <lpage>298</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Jones1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>JP</given-names></name><name name-style="western"><surname>Palmer</surname><given-names>LA</given-names></name></person-group>             <year>1987</year>             <article-title>The two-dimensional spatial structure of simple receptive fields in the cat striate cortex.</article-title>             <source>J Neurophysiol</source>             <volume>58</volume>             <fpage>1187</fpage>             <lpage>11211</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Ringach1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ringach</surname><given-names>D</given-names></name><name name-style="western"><surname>Shapiro</surname><given-names>G</given-names></name><name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name></person-group>             <year>1997</year>             <article-title>A subspace reverse correlation technique for the study of visual neurons.</article-title>             <source>Vision Res</source>             <volume>37</volume>             <fpage>2455</fpage>             <lpage>2464</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Chichilnisky1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group>             <year>2001</year>             <article-title>A simple white noise analysis of neuronal light responses.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>199</fpage>             <lpage>213</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Depireux1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Depireux</surname><given-names>D</given-names></name><name name-style="western"><surname>Simon</surname><given-names>J</given-names></name><name name-style="western"><surname>Klein</surname><given-names>D</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name></person-group>             <year>2001</year>             <article-title>Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex.</article-title>             <source>J Neurophysiol</source>             <volume>85</volume>             <fpage>1220</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Theunissen1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Theunissen</surname><given-names>F</given-names></name><name name-style="western"><surname>David</surname><given-names>S</given-names></name><name name-style="western"><surname>Singh</surname><given-names>N</given-names></name><name name-style="western"><surname>Hsu</surname><given-names>A</given-names></name><name name-style="western"><surname>Vinje</surname><given-names>W</given-names></name><etal/></person-group>             <year>2001</year>             <article-title>Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>289</fpage>             <lpage>316</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Ringach2">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ringach</surname><given-names>D</given-names></name><name name-style="western"><surname>Hawken</surname><given-names>M</given-names></name><name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name></person-group>             <year>2002</year>             <article-title>Receptive field structure of neurons in monkey primary visual cortex revealed by stimulation with natural image sequences.</article-title>             <source>J Vision</source>             <volume>2</volume>             <fpage>12</fpage>             <lpage>24</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Sahani1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name><name name-style="western"><surname>Linden</surname><given-names>J</given-names></name></person-group>             <year>2003</year>             <article-title>Evidence optimization techniques for estimating stimulus-response functions.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>15</volume>             <fpage>301</fpage>             <lpage>308</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Smyth1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smyth</surname><given-names>D</given-names></name><name name-style="western"><surname>Willmore</surname><given-names>B</given-names></name><name name-style="western"><surname>Baker</surname><given-names>G</given-names></name><name name-style="western"><surname>Thompson</surname><given-names>I</given-names></name><name name-style="western"><surname>Tolhurst</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>The receptive-field organization of simple cells in primary visual cortex of ferrets under natural scene stimulation.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>4746</fpage>             <lpage>4759</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Simoncelli1">
        <label>13</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name></person-group>             <year>2004</year>             <source>The Cognitive Neurosciences</source>             <publisher-name>MIT Press, 3 edition</publisher-name>             <fpage>327</fpage>             <lpage>338</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Paninski1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2004</year>             <article-title>Maximum likelihood estimation of cascade point-process neural encoding models.</article-title>             <source>Network</source>             <volume>15</volume>             <fpage>243</fpage>             <lpage>262</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-David1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>S</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J</given-names></name></person-group>             <year>2005</year>             <article-title>Predicting neuronal responses during natural vision.</article-title>             <source>Network</source>             <volume>16</volume>             <fpage>239</fpage>             <lpage>260</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Pillow1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2003</year>             <article-title>Biases in white noise analysis due to non-Poisson spike generation.</article-title>             <source>Neurocomputing</source>             <volume>52</volume>             <fpage>109</fpage>             <lpage>115</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Wu1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>M</given-names></name><name name-style="western"><surname>David</surname><given-names>S</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J</given-names></name></person-group>             <year>2006</year>             <article-title>Complete functional characterization of sensory neurons by system identification.</article-title>             <source>Annu Rev Neurosci</source>             <volume>29</volume>             <fpage>477</fpage>             <lpage>505</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Korenberg1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Korenberg</surname><given-names>M</given-names></name><name name-style="western"><surname>Hunter</surname><given-names>I</given-names></name></person-group>             <year>1996</year>             <article-title>The identification of nonlinear biological systems: Volterra kernel approaches.</article-title>             <source>Ann Biomed Eng</source>             <volume>24</volume>             <fpage>250</fpage>             <lpage>268</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Bussgang1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bussgang</surname><given-names>J</given-names></name></person-group>             <year>1952</year>             <article-title>Crosscorrelation functions of amplitude-distorted gaussian signals.</article-title>             <source>RLE Technical Reports</source>             <volume>216</volume>             <fpage>14</fpage>             <lpage>30</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Paninski2">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2003</year>             <article-title>Convergence properties of some spike-triggered analysis techniques.</article-title>             <source>Network</source>             <volume>14</volume>             <fpage>437</fpage>             <lpage>464</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Victor2">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name></person-group>             <year>1987</year>             <article-title>The dynamics of the cat retinal x cell centre.</article-title>             <source>J Physiol</source>             <volume>386</volume>             <fpage>219</fpage>             <lpage>246</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Meister1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Meister</surname><given-names>M</given-names></name><name name-style="western"><surname>Pine</surname><given-names>J</given-names></name><name name-style="western"><surname>Baylor</surname><given-names>DA</given-names></name></person-group>             <year>1994</year>             <article-title>Multi-neuronal signals from the retina: acquisition and analysis.</article-title>             <source>J Neurosci Methods</source>             <volume>51</volume>             <fpage>95</fpage>             <lpage>106</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Reid1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reid</surname><given-names>R</given-names></name><name name-style="western"><surname>Alonso</surname><given-names>J</given-names></name></person-group>             <year>1995</year>             <article-title>Specificity of monosynaptic connections from thalamus to visual cortex.</article-title>             <source>Nature</source>             <volume>378</volume>             <fpage>281</fpage>             <lpage>283</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Reid2">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reid</surname><given-names>R</given-names></name><name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name></person-group>             <year>2002</year>             <article-title>Space and time maps of cone photoreceptor signals in macaque lateral geniculate nucleus.</article-title>             <source>J Neurosci</source>             <volume>22</volume>             <fpage>6158</fpage>             <lpage>6175</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-DeAngelis1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>DeAngelis</surname><given-names>G</given-names></name><name name-style="western"><surname>Ohzawa</surname><given-names>I</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>R</given-names></name></person-group>             <year>1993</year>             <article-title>Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. ii. linearity of temporal and spatial summation.</article-title>             <source>J Neurophysiol</source>             <volume>69</volume>             <fpage>1118</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Eggermont1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Eggermont</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Johannesma</surname><given-names>PIM</given-names></name><name name-style="western"><surname>Aertsen</surname><given-names>AMHJ</given-names></name></person-group>             <year>1983</year>             <article-title>Reverse-correlation methods in auditory research.</article-title>             <source>Q Rev Biophys</source>             <volume>16</volume>             <fpage>341</fpage>             <lpage>414</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Klein1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Klein</surname><given-names>D</given-names></name><name name-style="western"><surname>Depireux</surname><given-names>D</given-names></name><name name-style="western"><surname>Simon</surname><given-names>J</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name></person-group>             <year>2000</year>             <article-title>Robust spectrotemporal reverse correlation for the auditory system: optimizing stimulus design.</article-title>             <source>J Comput Neurosci</source>             <volume>9</volume>             <fpage>85</fpage>             <lpage>111</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Sahani2">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name><name name-style="western"><surname>Linden</surname><given-names>J</given-names></name></person-group>             <year>2003</year>             <article-title>How linear are auditory cortical responses?</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>15</volume>             <fpage>125</fpage>             <lpage>132</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-James1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>James</surname><given-names>W</given-names></name><name name-style="western"><surname>Stein</surname><given-names>C</given-names></name></person-group>             <year>1960</year>             <article-title>Estimation with quadratic loss.</article-title>             <source>Proc Fourth Berkeley Symp Math Statist Probab</source>             <volume>1</volume>             <fpage>361</fpage>             <lpage>379</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Efron1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Efron</surname><given-names>B</given-names></name><name name-style="western"><surname>Morris</surname><given-names>C</given-names></name></person-group>             <year>1973</year>             <article-title>Stein's estimation rule and its competitors–an empirical Bayes approach.</article-title>             <source>J Am Stat Assoc</source>             <volume>68</volume>             <fpage>117</fpage>             <lpage>130</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Stevenson1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stevenson</surname><given-names>I</given-names></name><name name-style="western"><surname>Rebesco</surname><given-names>J</given-names></name><name name-style="western"><surname>Hatsopoulos</surname><given-names>N</given-names></name><name name-style="western"><surname>Haga</surname><given-names>Z</given-names></name><name name-style="western"><surname>Miller</surname><given-names>L</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>Bayesian inference of functional connectivity and network structure from spikes.</article-title>             <source>IEEE T Neural Syst Rehabil Eng</source>             <volume>17</volume>             <fpage>203</fpage>             <lpage>213</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-DeAngelis2">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>DeAngelis</surname><given-names>G</given-names></name><name name-style="western"><surname>Ohzawa</surname><given-names>I</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>R</given-names></name></person-group>             <year>1995</year>             <article-title>Receptive-field dynamics in the central visual pathways.</article-title>             <source>Trends Neurosci</source>             <volume>18</volume>             <fpage>451</fpage>             <lpage>458</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-deCharms1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>deCharms</surname><given-names>RC</given-names></name><name name-style="western"><surname>Blake</surname><given-names>DT</given-names></name><name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name></person-group>             <year>1998</year>             <article-title>Optimizing sound features for cortical neurons.</article-title>             <source>Science</source>             <volume>280</volume>             <fpage>1439</fpage>             <lpage>1443</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Casella1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Casella</surname><given-names>G</given-names></name></person-group>             <year>1985</year>             <article-title>An introduction to empirical Bayes data analysis.</article-title>             <source>Am Stat</source>             <fpage>83</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Kass1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kass</surname><given-names>R</given-names></name><name name-style="western"><surname>Steffey</surname><given-names>D</given-names></name></person-group>             <year>1989</year>             <article-title>Approximate Bayesian inference in conditionally independent hierarchical models (parametric empirical Bayes models).</article-title>             <source>J Am Stat Assoc</source>             <volume>84</volume>             <fpage>717</fpage>             <lpage>726</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Tipping1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tipping</surname><given-names>M</given-names></name></person-group>             <year>2001</year>             <article-title>Sparse Bayesian learning and the relevance vector machine.</article-title>             <source>J Mach Learn Res</source>             <volume>1</volume>             <fpage>211</fpage>             <lpage>244</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Warland1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name><name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name><name name-style="western"><surname>Meister</surname><given-names>M</given-names></name></person-group>             <year>1997</year>             <article-title>Decoding visual information from a population of retinal ganglion cells.</article-title>             <source>J Neurophysiol</source>             <volume>78</volume>             <fpage>2336</fpage>             <lpage>2350</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Pillow2">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Ahmadian</surname><given-names>Y</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2011</year>             <article-title>Model-based decoding, information estimation, and change-point detection techniques for multineuron spike trains.</article-title>             <source>Neural Comput</source>             <volume>23</volume>             <fpage>1</fpage>             <lpage>45</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Robbins1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robbins</surname><given-names>H</given-names></name></person-group>             <year>1956</year>             <article-title>An empirical Bayes approach to statistics.</article-title>             <source>Proc Third Berkeley Symp Math Statist Probab</source>             <volume>1</volume>             <fpage>157</fpage>             <lpage>163</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Morris1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Morris</surname><given-names>C</given-names></name></person-group>             <year>1983</year>             <article-title>Parametric empirical Bayes inference: theory and applications.</article-title>             <source>J Am Stat Assoc</source>             <volume>78</volume>             <fpage>47</fpage>             <lpage>55</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Bishop1">
        <label>41</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>CM</given-names></name></person-group>             <year>2006</year>             <source>Pattern recognition and machine learning</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Raphan1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Raphan</surname><given-names>M</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2011</year>             <article-title>Least squares estimation without priors or supervision.</article-title>             <source>Neural Comput</source>             <volume>23</volume>             <fpage>374</fpage>             <lpage>420</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Faul1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Faul</surname><given-names>A</given-names></name><name name-style="western"><surname>Tipping</surname><given-names>M</given-names></name></person-group>             <year>2002</year>             <article-title>Analysis of sparse Bayesian learning.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>14</volume>             <fpage>383</fpage>             <lpage>389</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Wipf1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wipf</surname><given-names>D</given-names></name><name name-style="western"><surname>Nagarajan</surname><given-names>S</given-names></name></person-group>             <year>2008</year>             <article-title>A new view of automatic relevance determination.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>22</volume>             <fpage>1625</fpage>             <lpage>1632</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-MacKay1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>MacKay</surname><given-names>D</given-names></name></person-group>             <year>1991</year>             <article-title>Bayesian interpolation.</article-title>             <source>Neural Comput</source>             <volume>4</volume>             <fpage>415</fpage>             <lpage>447</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Tipping2">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tipping</surname><given-names>M</given-names></name><name name-style="western"><surname>Faul</surname><given-names>AC</given-names></name></person-group>             <year>2003</year>             <article-title>Fast marginal likelihood maximisation for sparse Bayesian models.</article-title>             <source>Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</source>             <volume>9</volume>             <fpage>1</fpage>             <lpage>13</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Schmolck1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schmolck</surname><given-names>A</given-names></name><name name-style="western"><surname>Everson</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Smooth relevance vector machine: a smoothness prior extension of the RVM.</article-title>             <source>Mach learn</source>             <volume>68</volume>             <fpage>107</fpage>             <lpage>135</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Wipf2">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wipf</surname><given-names>D</given-names></name><name name-style="western"><surname>Nagarajan</surname><given-names>S</given-names></name></person-group>             <year>2009</year>             <article-title>Sparse estimation using general likelihoods and non-factorial priors.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>23</volume>             <fpage>2071</fpage>             <lpage>2079</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Wipf3">
        <label>49</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wipf</surname><given-names>D</given-names></name><name name-style="western"><surname>Nagarajan</surname><given-names>S</given-names></name></person-group>             <year>2010</year>             <article-title>Latent variable Bayesian models for promoting sparsity.</article-title>             <comment>Technical report, UCSF</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Tibshirani1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group>             <year>1996</year>             <article-title>Regression shrinkage and selection via the Lasso.</article-title>             <source>J Roy Stat Soc B Met</source>             <volume>58</volume>             <fpage>267</fpage>             <lpage>288</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Rasmussen1">
        <label>51</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rasmussen</surname><given-names>C</given-names></name><name name-style="western"><surname>Williams</surname><given-names>C</given-names></name></person-group>             <year>2006</year>             <source>Gaussian Processes for Machine Learning</source>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Ohzawa1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ohzawa</surname><given-names>I</given-names></name><name name-style="western"><surname>DeAngelis</surname><given-names>G</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>R</given-names></name></person-group>             <year>1997</year>             <article-title>The neural coding of stereoscopic depth.</article-title>             <source>NeuroReport</source>             <volume>8</volume>             <fpage>3</fpage>             <lpage>12</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Rust1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2005</year>             <article-title>Spatiotemporal elements of macaque V1 receptive fields.</article-title>             <source>Neuron</source>             <volume>46</volume>             <fpage>945</fpage>             <lpage>956</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Shlens1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name><name name-style="western"><surname>Field</surname><given-names>G</given-names></name><name name-style="western"><surname>Gauthier</surname><given-names>J</given-names></name><name name-style="western"><surname>Grivich</surname><given-names>M</given-names></name><name name-style="western"><surname>Petrusca</surname><given-names>D</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>The structure of multi-neuron firing patterns in primate retina.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>8254</fpage>             <lpage>8266</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Pillow3">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name><name name-style="western"><surname>Sher</surname><given-names>A</given-names></name><name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Spatio-temporal correlations and visual signaling in a complete neuronal population.</article-title>             <source>Nature</source>             <volume>454</volume>             <fpage>995</fpage>             <lpage>999</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Robert1">
        <label>56</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robert</surname><given-names>C</given-names></name><name name-style="western"><surname>Casella</surname><given-names>G</given-names></name></person-group>             <year>2005</year>             <source>Monte Carlo Statistical Methods</source>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Schummers1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schummers</surname><given-names>J</given-names></name><name name-style="western"><surname>Cronin</surname><given-names>B</given-names></name><name name-style="western"><surname>Wimmer</surname><given-names>K</given-names></name><name name-style="western"><surname>Stimberg</surname><given-names>M</given-names></name><name name-style="western"><surname>Martin</surname><given-names>R</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Dynamics of orientation tuning in cat V1 neurons depend on location within layers and orientation maps.</article-title>             <source>Front Neurosci</source>             <volume>1</volume>             <fpage>145</fpage>             <lpage>159</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Tibshirani2">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name><name name-style="western"><surname>Saunders</surname><given-names>M</given-names></name><name name-style="western"><surname>Rosset</surname><given-names>S</given-names></name><name name-style="western"><surname>Zhu</surname><given-names>J</given-names></name><name name-style="western"><surname>Knight</surname><given-names>K</given-names></name></person-group>             <year>2005</year>             <article-title>Sparsity and smoothness via the fused Lasso.</article-title>             <source>J Roy Stat Soc B Met</source>             <volume>67</volume>             <fpage>91</fpage>             <lpage>108</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Wainwright1">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wainwright</surname><given-names>M</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>E</given-names></name></person-group>             <year>2000</year>             <article-title>Scale mixtures of gaussians and the statistics of natural images.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>12</volume>             <fpage>855</fpage>             <lpage>861</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Park1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>T</given-names></name><name name-style="western"><surname>Casella</surname><given-names>G</given-names></name></person-group>             <year>2008</year>             <article-title>The Bayesian Lasso.</article-title>             <source>J Am Stat Assoc</source>             <volume>103</volume>             <fpage>681</fpage>             <lpage>686</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Donoho1">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Donoho</surname><given-names>D</given-names></name><name name-style="western"><surname>Johnstone</surname><given-names>I</given-names></name></person-group>             <year>1995</year>             <article-title>Adapting to unknown smoothness via wavelet shrinkage.</article-title>             <source>J Am Stat Assoc</source>             <volume>90</volume>             <fpage>1200</fpage>             <lpage>1224</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Donoho2">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Donoho</surname><given-names>D</given-names></name><name name-style="western"><surname>Elad</surname><given-names>M</given-names></name></person-group>             <year>2003</year>             <article-title>Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>100</volume>             <fpage>2197</fpage>             <lpage>2202</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Chen1">
        <label>63</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>S</given-names></name><name name-style="western"><surname>Donoho</surname><given-names>D</given-names></name><name name-style="western"><surname>Saunders</surname><given-names>M</given-names></name></person-group>             <year>1999</year>             <article-title>Atomic decomposition by basis pursuit.</article-title>             <source>SIAM J Sci Comp</source>             <volume>20</volume>             <fpage>33</fpage>             <lpage>61</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Wipf4">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wipf</surname><given-names>D</given-names></name><name name-style="western"><surname>Rao</surname><given-names>B</given-names></name></person-group>             <year>2004</year>             <article-title>Sparse Bayesian learning for basis selection.</article-title>             <source>IEEE T Signal Proces</source>             <volume>52</volume>             <fpage>2153</fpage>             <lpage>2164</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Mineault1">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mineault</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Barthelme</surname><given-names>S</given-names></name><name name-style="western"><surname>Pack</surname><given-names>CC</given-names></name></person-group>             <year>2009</year>             <article-title>Improved classification images with sparse priors in a smooth basis.</article-title>             <source>J Vision</source>             <volume>9</volume>             <fpage>1</fpage>             <lpage>24</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Sharpee1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sharpee</surname><given-names>T</given-names></name><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2004</year>             <article-title>Analyzing neural responses to natural signals: maximally informative dimensions.</article-title>             <source>Neural Comput</source>             <volume>16</volume>             <fpage>223</fpage>             <lpage>250</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-deRuytervanSteveninck1">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1988</year>             <article-title>Real-time performance of a movement-senstivive neuron in the blowy visual system: coding and information transmission in short spike sequences.</article-title>             <source>Proc R Soc Lond B</source>             <volume>234</volume>             <fpage>379</fpage>             <lpage>414</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Bialek1">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name></person-group>             <year>2005</year>             <article-title>Features and dimensions: Motion estimation in y vision.</article-title>             <source>arXiv</source>             <fpage>q–bio/0505003</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Schwartz1">
        <label>69</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2006</year>             <article-title>Spike-triggered neural characterization.</article-title>             <source>J Vis</source>             <volume>6</volume>             <fpage>484</fpage>             <lpage>507</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Pillow4">
        <label>70</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>             <year>2006</year>             <article-title>Dimensionality reduction in neural models: An informationtheoretic generalization of spike-triggered average and covariance analysis.</article-title>             <source>J Vision</source>             <volume>6</volume>             <fpage>414</fpage>             <lpage>428</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Saleem1">
        <label>71</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Saleem</surname><given-names>A</given-names></name><name name-style="western"><surname>Krapp</surname><given-names>H</given-names></name><name name-style="western"><surname>Schultz</surname><given-names>S</given-names></name></person-group>             <year>2008</year>             <article-title>Receptive field characterization by spike-triggered independent component analysis.</article-title>             <source>J Vis</source>             <volume>8</volume>             <fpage>1</fpage>             <lpage>16</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-David2">
        <label>72</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>S</given-names></name><name name-style="western"><surname>Vinje</surname><given-names>W</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J</given-names></name></person-group>             <year>2004</year>             <article-title>Natural stimulus statistics alter the receptive field structure of V1 neurons.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>6991</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-David3">
        <label>73</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>SV</given-names></name><name name-style="western"><surname>Hayden</surname><given-names>BY</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name></person-group>             <year>2006</year>             <article-title>Spectral receptive field properties explain shape selectivity in area V4.</article-title>             <source>J Neurophysiol</source>             <volume>96</volume>             <fpage>3492</fpage>             <lpage>3505</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Friedman1">
        <label>74</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Regularization paths for generalized linear models via coordinate descent.</article-title>             <source>J Stat Softw</source>             <volume>33</volume>             <fpage>1</fpage>             <lpage>22</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Friedman2">
        <label>75</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name><name name-style="western"><surname>Höing</surname><given-names>H</given-names></name><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Pathwise coordinate optimization.</article-title>             <source>Ann Appl Stat</source>             <volume>1</volume>             <fpage>302</fpage>             <lpage>332</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Neal1">
        <label>76</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Neal</surname><given-names>RM</given-names></name></person-group>             <year>2003</year>             <article-title>Slice sampling.</article-title>             <source>Ann Stat</source>             <volume>31</volume>             <fpage>705</fpage>             <lpage>741</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002219-Ahmadian1">
        <label>77</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ahmadian</surname><given-names>Y</given-names></name><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2011</year>             <article-title>Efficient Markov chain Monte Carlo methods for decoding neural spike trains.</article-title>             <source>Neural Comput</source>             <volume>23</volume>             <fpage>46</fpage>             <lpage>96</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>