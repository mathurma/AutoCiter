<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004315</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01314</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Automatic Adaptation to Fast Input Changes in a Time-Invariant Neural Circuit</article-title>
<alt-title alt-title-type="running-head">Automatic Neural Circuit Adaptation to Fast Input Changes</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Bharioke</surname>
<given-names>Arjun</given-names>
</name>
<xref rid="cor001" ref-type="corresp">*</xref>
<xref rid="aff001" ref-type="aff"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Chklovskii</surname>
<given-names>Dmitri B.</given-names>
</name>
<xref rid="currentaff001" ref-type="fn"><sup>¤</sup></xref>
<xref rid="aff001" ref-type="aff"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, Virginia, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Kording</surname>
<given-names>Konrad P.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Northwestern University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: AB DBC. Performed the experiments: AB. Analyzed the data: AB. Wrote the paper: AB DBC.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label><p>Current Address: Simons Center for Data Analysis, Simons Foundation, New York, New York, United States of America</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">bharioke@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>6</day>
<month>8</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>8</issue>
<elocation-id>e1004315</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>7</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>16</day>
<month>4</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Bharioke, Chklovskii</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004315" xlink:type="simple"/>
<abstract>
<p>Neurons must faithfully encode signals that can vary over many orders of magnitude despite having only limited dynamic ranges. For a correlated signal, this dynamic range constraint can be relieved by subtracting away components of the signal that can be predicted from the past, a strategy known as predictive coding, that relies on learning the input statistics. However, the statistics of input natural signals can also vary over very short time scales e.g., following saccades across a visual scene. To maintain a reduced transmission cost to signals with rapidly varying statistics, neuronal circuits implementing predictive coding must also rapidly adapt their properties. Experimentally, in different sensory modalities, sensory neurons have shown such adaptations within 100 ms of an input change. Here, we show first that linear neurons connected in a feedback inhibitory circuit can implement predictive coding. We then show that adding a rectification nonlinearity to such a feedback inhibitory circuit allows it to automatically adapt and approximate the performance of an optimal linear predictive coding network, over a wide range of inputs, while keeping its underlying temporal and synaptic properties unchanged. We demonstrate that the resulting changes to the linearized temporal filters of this nonlinear network match the fast adaptations observed experimentally in different sensory modalities, in different vertebrate species. Therefore, the nonlinear feedback inhibitory network can provide automatic adaptation to fast varying signals, maintaining the dynamic range necessary for accurate neuronal transmission of natural inputs.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>An animal exploring a natural scene receives sensory inputs that vary, rapidly, over many orders of magnitude. Neurons must transmit these inputs faithfully despite both their limited dynamic range and relatively slow adaptation time scales. One well-accepted strategy for transmitting signals through limited dynamic range channels–predictive coding–transmits only components of the signal that cannot be predicted from the past. Predictive coding algorithms respond maximally to unexpected inputs, making them appealing in describing sensory transmission. However, recent experimental evidence has shown that neuronal circuits adapt quickly, to respond optimally following rapid input changes. Here, we reconcile the predictive coding algorithm with this automatic adaptation, by introducing a fixed nonlinearity into a predictive coding circuit. The resulting network automatically “adapts” its linearized response to different inputs. Indeed, it approximates the performance of an optimal linear circuit implementing predictive coding, without having to vary its internal parameters. Further, adding this nonlinearity to the predictive coding circuit still allows the input to be compressed losslessly, allowing for additional downstream manipulations. Finally, we demonstrate that the nonlinear circuit dynamics match responses in both auditory and visual neurons. Therefore, we believe that this nonlinear circuit may be a general circuit motif that can be applied in different neural circuits, whenever it is necessary to provide an automatic improvement in the quality of the transmitted signal, for a fast varying input distribution.</p>
</abstract>
<funding-group>
<funding-statement>This work was funded by the Howard Hughes Medical Institute (HHMI). AB was funded as part of a joint HHMI PhD program between the University of Cambridge and Janelia Farm Research Campus, and through a Postgraduate Scholarship from the National Science and Engineering Research Council of Canada. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Early sensory processing faces the challenge of communicating sensory inputs with large dynamic range to the rest of the brain using neurons with limited dynamic range [<xref rid="pcbi.1004315.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1004315.ref006" ref-type="bibr">6</xref>]. In electrical engineering, such challenges are met by compressing the inputs using predictive coding [<xref rid="pcbi.1004315.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004315.ref011" ref-type="bibr">11</xref>]. Given this, Srinivasan et al. conjectured that the early visual systems of both vertebrates and invertebrates may also implement predictive coding [<xref rid="pcbi.1004315.ref012" ref-type="bibr">12</xref>]. More recently, predictive coding has also been conjectured to function in cortical visual and auditory processing, and attention [<xref rid="pcbi.1004315.ref013" ref-type="bibr">13</xref>–<xref rid="pcbi.1004315.ref016" ref-type="bibr">16</xref>].</p>
<p>A predictive coding circuit attempts to reduce the dynamic range of an input by subtracting a prediction of the current input value–based on past input values–from the actual current input value, and then transmitting only the difference, i.e. the prediction error (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1</xref>). Such a strategy only works if the prediction is good, which requires the existence of stationary (predictable) correlations within the input, and the ability of the algorithm to adapt to them. For example, the optimal linear prediction-error filter, that minimizes the relative power of the transmission, depends on the signal-to-noise ratio (SNR) of the inputs [<xref rid="pcbi.1004315.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004315.ref009" ref-type="bibr">9</xref>]. Indeed, in the invertebrate visual system, Srinivasan et al. observed adaptations to the prediction-error filter in response to changes in the input SNR [<xref rid="pcbi.1004315.ref012" ref-type="bibr">12</xref>].</p>
<fig id="pcbi.1004315.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Lossless compression via predictive coding.</title>
<p>Feedforward (a) and feedback (b) predictive coding circuits (as in Eq (<xref rid="pcbi.1004315.e001" ref-type="disp-formula">1</xref>)), showing the circuit structure necessary for reconstructing the input. (c) Lossless reconstruction (black dots) of an input (blue) from the transmitted output of a predictive coding circuit (red). The mean power up to each point in time (solid bars) for the input (blue) and transmission (red) demonstrates the reduction in the mean transmitted power via predictive coding.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g001" position="float" xlink:type="simple"/>
</fig>
<p>This adaptation of the response filters of a neuron to changing input properties has been explored in the literature [<xref rid="pcbi.1004315.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004315.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1004315.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1004315.ref017" ref-type="bibr">17</xref>–<xref rid="pcbi.1004315.ref021" ref-type="bibr">21</xref>]. This includes the predictive coding formulation introduced by Srinivasan et al. [<xref rid="pcbi.1004315.ref012" ref-type="bibr">12</xref>], which was also extended to the vertebrate visual system [<xref rid="pcbi.1004315.ref020" ref-type="bibr">20</xref>]. Indeed, Hosoya et al. [<xref rid="pcbi.1004315.ref020" ref-type="bibr">20</xref>] demonstrated that predictive coding can explain changes to the response filters of retinal ganglion cells, for adaptations on the time scale of seconds. Other authors [<xref rid="pcbi.1004315.ref022" ref-type="bibr">22</xref>–<xref rid="pcbi.1004315.ref024" ref-type="bibr">24</xref>] alternatively postulated that sensory systems must whiten input signals. They showed that varying response filters to whiten the input signals, for different inputs, also matches the observed changes in neurons’ responses.</p>
<p>However, the adaptation of early sensory processing circuits must also be very fast, since the input statistics often vary rapidly, e.g. across a saccade [<xref rid="pcbi.1004315.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004315.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1004315.ref025" ref-type="bibr">25</xref>], or during movements within a complex auditory environment [<xref rid="pcbi.1004315.ref026" ref-type="bibr">26</xref>]. Many of the adaptations that have been proposed in the literature vary the biophysical properties of neurons; hence, they are thought to occur at a slower time scale than the speed at which input changes occur. Nevertheless, experimental measurements of the linear filters of sensory neurons have shown that they do indeed change rapidly–often as fast as can be experimentally measured [<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>,<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>]. Therefore, to maintain optimality, linear response filters in models of sensory coding must adapt to changes in the input signal statistics–including the input SNR–rapidly, following each input change, yet prior to the next change.</p>
<p>How can a linear filter change at a high rate in response to changes in its input statistics? The addition of a time-invariant nonlinearity can allow the construction of a circuit that “instantaneously adapts” its linearized responses to input changes [<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>,<xref rid="pcbi.1004315.ref029" ref-type="bibr">29</xref>]. However, it is not clear if such a circuit exists, and how to construct it. Previous work on the effect of static nonlinearities on neurons has focused mostly on the possible computational functions of a spiking nonlinearity on the output of a single neuron [<xref rid="pcbi.1004315.ref029" ref-type="bibr">29</xref>–<xref rid="pcbi.1004315.ref032" ref-type="bibr">32</xref>], or a multiplicative nonlinearity within a motion detecting circuit [<xref rid="pcbi.1004315.ref033" ref-type="bibr">33</xref>].</p>
<p>Here, we demonstrate that a network of leaky integrator neurons, with a threshold nonlinearity, is, indeed, able to achieve automatic adaptation to changes in the ratio of the predictable component of an input to its unpredictable component. Since noise is, by definition, unpredictable, one specific case of this would be sudden changes to the input SNR. We first show that, for certain stimulus ensembles, linear leaky integrator neurons can implement linear predictive coding through both a feedback and a feedforward inhibitory circuit. We find the parameters that allow these networks to minimize the output dynamic range. Comparing these implementations, we find that the structure of the adaptation in the feedback inhibitory circuit lends itself to the construction of an automatically adapting filter. Specifically, the addition of a biologically-plausible threshold nonlinearity to the feedback inhibitory circuit allows it to approximate the performance of the optimal linear filter over a range of input SNRs.</p>
<p>We compare the responses of a nonlinear predictive coding circuit to available experimental results. The instantaneous changes to the linearized filter of the nonlinear circuit match the fast changes measured to the linear filters of neurons in different sensory modalities, in response to rapid input changes. Hence, our results support the nonlinear feedback inhibitory circuit as a circuit implementation of predictive coding that models the response of early processing in various sensory modalities, facilitating the transmission of rapidly varying, high dynamic range inputs, through slow, low dynamic range neurons.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Predictive Coding Framework</title>
<p>In the field of adaptive signal processing, predictive coding algorithms have commonly been used for signal compression [<xref rid="pcbi.1004315.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004315.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1004315.ref034" ref-type="bibr">34</xref>], including in the transmission of telecommunications signals (e.g. in the GSM standard) [<xref rid="pcbi.1004315.ref035" ref-type="bibr">35</xref>,<xref rid="pcbi.1004315.ref036" ref-type="bibr">36</xref>] and in video compression [<xref rid="pcbi.1004315.ref037" ref-type="bibr">37</xref>–<xref rid="pcbi.1004315.ref039" ref-type="bibr">39</xref>]. These algorithms compute a prediction of the current input based on previous values of the input, and then subtract it away from the actual current input (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1A and 1B</xref>). If the signal possesses some statistical structure, an accurate prediction can be made and the output transmission will have a smaller power than the input (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1C and 1D</xref>) [<xref rid="pcbi.1004315.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004315.ref011" ref-type="bibr">11</xref>].</p>
<p>In a general predictive coding algorithm, acting on an input time series {<italic>f</italic><sub><italic>t</italic></sub>}, the prediction can be constructed in two ways: (1) from past values of the input signal, or (2) from past values of the transmitted output of the algorithm. In (1), the algorithm is entirely feedforward (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1A</xref>). In contrast, in (2), the algorithm is feedback (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1B</xref>). In each case, we can write the algorithm as:
<disp-formula id="pcbi.1004315.e001">
<alternatives>
<graphic id="pcbi.1004315.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2em"/><mml:mtext>or</mml:mtext><mml:mspace width="2em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>p</italic><sub><italic>t</italic></sub> is the transmitted signal and the predictions (computed either linearly or nonlinearly) are <italic>C</italic>(⋅).</p>
<p>A crucial property of predictive coding algorithms is that they transmit information losslessly. Specifically, their function is to transmit all the input that they receive including both signal and noise. This is unlike many other algorithms commonly used in neuroscience, which separate signal from noise. Indeed, as structured in Eq (<xref rid="pcbi.1004315.e001" ref-type="disp-formula">1</xref>) and assuming that there is no communication noise in the transmission process, the output of a predictive coding network can be used to reconstruct the entire input, losslessly (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1</xref>) [<xref rid="pcbi.1004315.ref008" ref-type="bibr">8</xref>,<xref rid="pcbi.1004315.ref011" ref-type="bibr">11</xref>]. Further, this is independent of the linear or nonlinear way in which the prediction is computed (<xref rid="pcbi.1004315.s006" ref-type="supplementary-material">S1 Text</xref>). This property is important in our identification of an optimal predictive coding algorithm.</p>
<p>To formulate this optimization problem, we define:
<list list-type="order">
<list-item><p>A class of allowable predictive coding algorithms, within which to identify an optimal algorithm</p></list-item>
<list-item><p>An input ensemble over which the algorithm is optimized.</p></list-item>
<list-item><p>An optimization metric to measure the algorithm’s performance</p></list-item>
</list></p>
</sec>
<sec id="sec004">
<title>Optimizing a Linear Predictive Coding Algorithm</title>
<p>We start by defining the class of <italic>linear predictive coding algorithms</italic>–predictive coding algorithms in which the prediction is a linear combination of the past inputs:
<disp-formula id="pcbi.1004315.e002">
<alternatives>
<graphic id="pcbi.1004315.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>We have written this in the feedforward implementation. However, since the equation is linear, this results in no loss of generality; it can be rewritten recursively to obtain the feedback case.</p>
<p>Eq (<xref rid="pcbi.1004315.e002" ref-type="disp-formula">2</xref>) describes a predictive encoding constructed by a class of linear temporal filters, which vary in the way in which they compute the prediction. Each such temporal filter is defined by its set of parameters, {<italic>w</italic><sub><italic>i</italic></sub>}, causally, for all time steps, <italic>i</italic>, counting backwards through time. We now define the input ensemble, and optimization metric with which we can identify a specific temporal filter (i.e. specific set of parameters {<italic>w</italic><sub><italic>i</italic></sub>}) that is optimal.</p>
</sec>
<sec id="sec005">
<title>Naturalistic Subset of Inputs</title>
<p>Ideally, one would like to find the optimal filter over the space of natural images. However, given the complexity of this space, we chose to use a subset of such inputs. Natural image amplitudes are distributed, over time, with a power law distribution over temporal frequencies [<xref rid="pcbi.1004315.ref040" ref-type="bibr">40</xref>]. It is possible to decompose such an input (up to some high frequency roll-off) into a sum of several exponentially correlated components, each with a different time constant [<xref rid="pcbi.1004315.ref041" ref-type="bibr">41</xref>,<xref rid="pcbi.1004315.ref042" ref-type="bibr">42</xref>].</p>
<p>Therefore, we chose an input composed of one such exponentially correlated signal (with a single time constant, <italic>τ</italic><sub><italic>s</italic></sub>), combined with uncorrelated noise, combined at a particular SNR, <italic>σ</italic> (Methods):
<disp-formula id="pcbi.1004315.e003">
<alternatives>
<graphic id="pcbi.1004315.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e003" xlink:type="simple"/>
<mml:math display="block" id="M3" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:msub><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>This input provides the ensemble over which we can identify an optimal linear predictive coding filter. We believe that this subset of inputs is naturalistic, since it should be possible to combine several input subsets (constructed with different time constants) to generalize back to the space of natural images.</p>
</sec>
<sec id="sec006">
<title>Assessing Performance of Predictive Coding</title>
<p>The final part of the formulation of the optimization problem is a performance metric against which to optimize the filter. Since the goal of applying predictive coding is to reduce the dynamic range required to transmit a signal, a natural measure of performance would be the degree of reduction in the power of the transmitted signal, relative to the input power. We term this the network gain, defined as:
<disp-formula id="pcbi.1004315.e004">
<alternatives>
<graphic id="pcbi.1004315.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e004" xlink:type="simple"/>
<mml:math display="block" id="M4" overflow="scroll">
<mml:mrow><mml:mtext>Network Gain</mml:mtext><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Transmitted Power</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Input Power</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
for <italic>p</italic><sub><italic>i</italic></sub> as defined in Eq (<xref rid="pcbi.1004315.e002" ref-type="disp-formula">2</xref>), and <italic>f</italic><sub><italic>i</italic></sub> as defined in Eq (<xref rid="pcbi.1004315.e003" ref-type="disp-formula">3</xref>).</p>
<p>Ideally, any metric of performance, for a compression algorithm, would include both the degree of compression, and a measure of the information lost due to the compression, e.g. reconstruction error. However, as introduced above, predictive coding algorithms encode inputs losslessly. Hence, any reconstruction error is necessarily zero (<xref rid="pcbi.1004315.g001" ref-type="fig">Fig 1C</xref>). Therefore, we can measure the performance of different predictive coding algorithms just by using the network gain, and we now solve for the optimal linear temporal filter that minimizes this gain over the input ensemble.</p>
</sec>
<sec id="sec007">
<title>Optimal Linear Predictive Coding Filter</title>
<p>Finding the linear filter that minimizes the network gain is a specific example of a common optimization in the adaptive signal processing literature [<xref rid="pcbi.1004315.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004315.ref009" ref-type="bibr">9</xref>], but the specific derivation that we utilize is detailed in <xref rid="pcbi.1004315.s006" ref-type="supplementary-material">S1 Text</xref>.</p>
<p>Briefly, we compute the power of the filter by transforming Eq (<xref rid="pcbi.1004315.e002" ref-type="disp-formula">2</xref>) into the frequency domain, and then compute the output power by applying the transformed transfer function to the autocorrelation function of the input. We then solve for the optimal values over the set of <italic>w</italic><sub><italic>i</italic></sub> by differentiation. The solution is a function of the two parameters that characterize the input ensemble, the time constant (<italic>τ</italic><sub><italic>s</italic></sub>) and SNR (<italic>σ</italic>):
<disp-formula id="pcbi.1004315.e005">
<alternatives>
<graphic id="pcbi.1004315.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e005" xlink:type="simple"/>
<mml:math display="block" id="M5" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where
<disp-formula id="pcbi.1004315.e006">
<alternatives>
<graphic id="pcbi.1004315.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e006" xlink:type="simple"/>
<mml:math display="block" id="M6" overflow="scroll">
<mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac bevelled="true"><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1.25em"/><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>It is important to note that:
<list list-type="bullet">
<list-item><p><italic>β</italic> is dependent only on the correlation time constant of the signal component</p></list-item>
<list-item><p>Λ<sup>*</sup> is dependent both on the signal, and the SNR</p></list-item>
</list></p>
<p>Plotting each of these variables, for some values of the input parameters, provides their qualitative structure. First, <italic>β</italic> varies from 0 to 1, as the correlation time constant of the signal increases (<xref rid="pcbi.1004315.g002" ref-type="fig">Fig 2A</xref>). Second, for each fixed <italic>β</italic>, Λ<sup>*</sup> varies from 0 to 1, as the SNR increases (<xref rid="pcbi.1004315.g002" ref-type="fig">Fig 2B</xref>). This quantitative value of Λ<sup>*</sup> varies with <italic>β</italic>, but the increase from 0 to 1 always holds, qualitatively.</p>
<fig id="pcbi.1004315.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Parameters of the optimized linear predictive coding algorithm.</title>
<p>(a) <italic>β</italic> = <italic>β</italic>(<italic>τ</italic><sub><italic>s</italic></sub>) (b) Λ<sup>*</sup> = Λ<sup>*</sup>(<italic>β</italic>, <italic>σ</italic>): Eq (<xref rid="pcbi.1004315.e006" ref-type="disp-formula">6</xref>) for two values of <italic>β</italic>.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g002" position="float" xlink:type="simple"/>
</fig>
<p>Substituting Eq (<xref rid="pcbi.1004315.e005" ref-type="disp-formula">5</xref>) back into Eq (<xref rid="pcbi.1004315.e002" ref-type="disp-formula">2</xref>), we can write down the linear predictive coding filter that minimizes the power of the transmitted signal:
<disp-formula id="pcbi.1004315.e007">
<alternatives>
<graphic id="pcbi.1004315.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e007" xlink:type="simple"/>
<mml:math display="block" id="M7" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>An interesting property of the optimal linear predictive coding algorithm is its structure in the high noise, low signal regime (i.e. low SNR in <xref rid="pcbi.1004315.g002" ref-type="fig">Fig 2B</xref>). In that regime, Λ<sup>*</sup> → 0. Hence, the prediction approaches 0, and <italic>p</italic><sub><italic>t</italic></sub> → <italic>f</italic><sub><italic>t</italic></sub>. Initially, this might seem counterintuitive, since the predictive coding network is transmitting the entire, noisy input to downstream circuits. However, any prediction of an uncorrelated input will, on average, increase the power of the transmitted output. Therefore, sending out no prediction does indeed minimize the network gain.</p>
<p>Further intuition about the values of the parameters is most useful when applied to specific implementations of this algorithm. Therefore, we first show that it is possible to implement Eq (<xref rid="pcbi.1004315.e007" ref-type="disp-formula">7</xref>) using a circuit of linear leaky integrator neurons.</p>
</sec>
<sec id="sec008">
<title>Implementing Optimal Filter with Linear Neurons</title>
<p>A simple model of a biological neuron is a leaky integrator (<xref rid="pcbi.1004315.s001" ref-type="supplementary-material">S1 Fig</xref>) [<xref rid="pcbi.1004315.ref043" ref-type="bibr">43</xref>,<xref rid="pcbi.1004315.ref044" ref-type="bibr">44</xref>], whose voltage response (v) is modeled by an exponential low pass filter (Methods):
<disp-formula id="pcbi.1004315.e008">
<alternatives>
<graphic id="pcbi.1004315.e008g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e008" xlink:type="simple"/>
<mml:math display="block" id="M8" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
where the time constant of the filter, <italic>τ</italic><sub><italic>m</italic></sub>, is the membrane time constant, and g<sub>s</sub> is the synaptic conductance (measured as a fraction of the membrane conductance).</p>
<p>Comparing Eq (<xref rid="pcbi.1004315.e008" ref-type="disp-formula">8</xref>) to Eq (<xref rid="pcbi.1004315.e007" ref-type="disp-formula">7</xref>), we see that the optimal linear predictive coding filter can be implemented by a combination of two leaky integrator neurons, with different time constants. First, in the limit of <italic>τ</italic><sub><italic>m</italic></sub> → 0, Eq (<xref rid="pcbi.1004315.e008" ref-type="disp-formula">8</xref>) implies that <inline-formula id="pcbi.1004315.e009"><alternatives><graphic id="pcbi.1004315.e009g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e009" xlink:type="simple"/><mml:math display="inline" id="M9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∝</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, the first term in Eq (<xref rid="pcbi.1004315.e007" ref-type="disp-formula">7</xref>) can be implemented by a neuron with a short time constant. Second, the subtracted prediction in Eq (<xref rid="pcbi.1004315.e007" ref-type="disp-formula">7</xref>) is simply an exponential weight on the past inputs and, hence, can also be implemented by a neuron with a specific time constant.</p>
<p>This structure can be implemented with different circuits, and we explore both feedforward and feedback two-neuron circuit implementations of the predictive coding algorithm.</p>
</sec>
<sec id="sec009">
<title>Feedforward and Feedback Implementations</title>
<p>The feedforward and feedback implementations of predictive coding (<xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3</xref>) are each characterized by two parameters: the time constant of the interneuron (feedforward: <inline-formula id="pcbi.1004315.e010"><alternatives><graphic id="pcbi.1004315.e010g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e010" xlink:type="simple"/><mml:math display="inline" id="M10" overflow="scroll"><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>; feedback: <italic>α</italic>) and the loop gain (feedforward: <inline-formula id="pcbi.1004315.e011"><alternatives><graphic id="pcbi.1004315.e011g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e011" xlink:type="simple"/><mml:math display="inline" id="M11" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>; feedback: Γ), i.e. the gain of input through the interneuron relative to direct input. For each network, the two parameters may take different values to implement the optimal linear predictive filter (that minimizes network gain for the given input statistics). To derive these values, we first construct a recursive analytical form for each network’s dynamics, following the flow of information around the circuit, and adding a time step delay at the synapses leading into the interneuron (as diagrammed in <xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3</xref>) (Methods).</p>
<fig id="pcbi.1004315.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Schematic neural networks implementing predictive coding, through feedforward (a) or feedback (b) inhibition.</title>
<p>Networks’ parameters are: (a) Feedforward: <inline-formula id="pcbi.1004315.e012"><alternatives><graphic id="pcbi.1004315.e012g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e012" xlink:type="simple"/><mml:math display="inline" id="M12" overflow="scroll"><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004315.e013"><alternatives><graphic id="pcbi.1004315.e013g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e013" xlink:type="simple"/><mml:math display="inline" id="M13" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. (b) Feedback: <italic>α</italic>, Γ. In both circuits, the unit that outputs <italic>p</italic><sub><italic>t</italic></sub> is termed the <italic>principal cell</italic>, and the unit that outputs <italic>n</italic><sub><italic>t</italic></sub> the <italic>interneuron</italic>. There are two synapses onto each interneuron: (1) its external input and (2) a representation of its internal dynamics (right of the dashed line). We can describe its internal dynamics in this way because the interneuron is a leaky integrator, i.e. its internal dynamics (Eq (<xref rid="pcbi.1004315.e008" ref-type="disp-formula">8</xref>)) are an infinite sum of past outputs, discounted at each time step by a fixed multiplier. Therefore, they can be represented by a delayed input across a synapse, with a multiplicative discounting factor, as in (a) and (b).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g003" position="float" xlink:type="simple"/>
</fig>
<p>The recursive dynamics of the feedforward circuit (<xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3A</xref>) are:
<disp-formula id="pcbi.1004315.e014">
<alternatives>
<graphic id="pcbi.1004315.e014g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e014" xlink:type="simple"/>
<mml:math display="block" id="M14" overflow="scroll">
<mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where the interneuron’s time constant is characterized by a discounting factor, <inline-formula id="pcbi.1004315.e015"><alternatives><graphic id="pcbi.1004315.e015g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e015" xlink:type="simple"/><mml:math display="inline" id="M15" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, that discounts the voltage from the past time step by a fixed multiple, and the feedforward gain through the loop is <inline-formula id="pcbi.1004315.e016"><alternatives><graphic id="pcbi.1004315.e016g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e016" xlink:type="simple"/><mml:math display="inline" id="M16" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (<xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3A</xref>). Solving this recursion, we get (<xref rid="pcbi.1004315.s006" ref-type="supplementary-material">S1 Text</xref>):
<disp-formula id="pcbi.1004315.e017">
<alternatives>
<graphic id="pcbi.1004315.e017g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e017" xlink:type="simple"/>
<mml:math display="block" id="M17" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>⋅</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula></p>
<p>Comparing Eq (<xref rid="pcbi.1004315.e017" ref-type="disp-formula">10</xref>) to Eq (<xref rid="pcbi.1004315.e007" ref-type="disp-formula">7</xref>), the feedback inhibitory network will implement the optimal linear prediction filter if
<disp-formula id="pcbi.1004315.e018">
<alternatives>
<graphic id="pcbi.1004315.e018g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e018" xlink:type="simple"/>
<mml:math display="block" id="M18" overflow="scroll">
<mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1.25em"/><mml:mtext>and</mml:mtext><mml:mspace width="1.25em"/><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>For the feedback inhibitory network (<xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3B</xref>), we can identify and solve a similar pair of recursive equations (Methods). This gives us:
<disp-formula id="pcbi.1004315.e019">
<alternatives>
<graphic id="pcbi.1004315.e019g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e019" xlink:type="simple"/>
<mml:math display="block" id="M19" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:mspace width="0.15em"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where the interneuron’s discounting factor is <italic>α</italic> and the feedback gain is Γ. Comparing Eq (<xref rid="pcbi.1004315.e019" ref-type="disp-formula">12</xref>) to Eq (<xref rid="pcbi.1004315.e007" ref-type="disp-formula">7</xref>), the feedforward inhibitory network will implement the optimal linear prediction filter if:
<disp-formula id="pcbi.1004315.e020">
<alternatives>
<graphic id="pcbi.1004315.e020g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e020" xlink:type="simple"/>
<mml:math display="block" id="M20" overflow="scroll">
<mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mspace width="1.1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1.1em"/><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula></p>
<p>Summarizing the dependence of the optimal network parameters on the input statistics (from Eqs (<xref rid="pcbi.1004315.e018" ref-type="disp-formula">11</xref>) and (<xref rid="pcbi.1004315.e020" ref-type="disp-formula">13</xref>)) makes explicit the differences between the feedforward and feedback models in their adaptation to input changes:</p>
<table-wrap id="pcbi.1004315.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.t001</object-id>
<alternatives>
<graphic id="pcbi.1004315.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"/>
<th align="left" rowspan="1" colspan="1">Interneuron discounting factor</th>
<th align="left" rowspan="1" colspan="1">Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Feedforward</td>
<td align="left" rowspan="1" colspan="1"><inline-formula id="pcbi.1004315.e021"><alternatives><graphic id="pcbi.1004315.e021g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e021" xlink:type="simple"/><mml:math display="inline" id="M21" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula id="pcbi.1004315.e022"><alternatives><graphic id="pcbi.1004315.e022g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e022" xlink:type="simple"/><mml:math display="inline" id="M22" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Feedback</td>
<td align="left" rowspan="1" colspan="1"><italic>α</italic>(<italic>τ</italic><sub><italic>s</italic></sub>) = <italic>β</italic>(<italic>τ</italic><sub><italic>s</italic></sub>)</td>
<td align="left" rowspan="1" colspan="1">Γ(<italic>τ</italic><sub><italic>s</italic></sub>, <italic>σ</italic>) = Λ<sup>*</sup>(<italic>β</italic>(<italic>τ</italic><sub><italic>s</italic></sub>), <italic>σ</italic>)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Although the resulting linear prediction-error filter changes in the same way for both circuits, the mechanistic difference between the circuits places different demands on the interneurons. For example, consider the changes in response to increasing input SNR (<italic>σ</italic>), for a fixed correlation of the signal component (<italic>τ</italic><sub><italic>s</italic></sub>). For the feedforward network, <inline-formula id="pcbi.1004315.e023"><alternatives><graphic id="pcbi.1004315.e023g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e023" xlink:type="simple"/><mml:math display="inline" id="M23" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> increases without bound, while <inline-formula id="pcbi.1004315.e024"><alternatives><graphic id="pcbi.1004315.e024g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e024" xlink:type="simple"/><mml:math display="inline" id="M24" overflow="scroll"><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> approaches 0. In contrast, for the feedback network, Γ approaches 1 (<xref rid="pcbi.1004315.g002" ref-type="fig">Fig 2B</xref>), while <italic>α</italic> remains fixed to <italic>β</italic>. Therefore, whereas in the feedforward network both the gain and the interneuron time constant must vary to maintain optimality, in the feedback network, only the feedback gain must vary.</p>
<p>Focusing on the interneuron discounting factor, in the feedforward case, the interneuron gets progressively faster as noise increases, as if the interneuron is reducing the time over which it averages the signal to obtain a prediction, given cleaner inputs (with less noise). However, in the feedback case, the interneuron averages over the same time scale, perhaps to provide a matched filter to select the specific correlated signal. This emphasizes the different roles for the interneuron within each circuit.</p>
<p>Further, this difference suggests that the feedback network may lend itself more readily to the construction of an automatically adapting nonlinear network–to respond to rapidly varying input SNR. One can imagine changing the output from one component of a circuit using a nonlinearity, as is necessary for adaptation of the feedback network. However, it would seem to be quite difficult to vary the time constant of a neuron, a cellular property, using a nonlinearity, as is necessary for adaptation of the feedforward network. Therefore, we now explore the construction of such a nonlinear feedback circuit.</p>
</sec>
<sec id="sec010">
<title>Constructing a Nonlinear Circuit that Automatically Adapts to Changes in Input SNR</title>
<p>As introduced earlier, a nonlinearity can allow an invariant circuit to automatically change its linearized response to varying inputs [<xref rid="pcbi.1004315.ref029" ref-type="bibr">29</xref>–<xref rid="pcbi.1004315.ref033" ref-type="bibr">33</xref>]. Here, we consider the automatic adaptation of a feedback circuit to rapid changes of the input SNR by introducing such a nonlinearity. In the following sections, we demonstrate that its performance is close to optimal.</p>
<p>Our analysis of the optimal linear feedback circuit shows that as <italic>σ</italic> varies from 0 (pure noise) to ∞ (pure signal), the feedback gain, Γ, must increase from 0 to 1. Therefore, to automatically match the optimal filter over a range of input SNRs, the nonlinearity in the feedback inhibitory circuit must increase the strength of the output from the interneuron onto the principal neuron as the noise decreases (and vice versa).</p>
<p>Since inputs of different SNR are integrated differentially by the interneuron, we define the shape necessary for the static nonlinearity. Integrating uncorrelated noise is equivalent to a random walk. In contrast, integrating a correlated signal is equivalent to a biased random walk. Hence, on average, the output of a leaky integrator neuron, from an input with greater correlated component (i.e. higher SNR), will be larger in amplitude. Therefore, any automatic adapting nonlinearity–applied to the output of the feedback interneuron–must push the gain towards 0 for small output amplitudes, and pull the gain towards 1 for large output amplitudes.</p>
<p>One simple piecewise linear nonlinearity satisfies this requirement: the threshold or rectilinear nonlinearity, which increases linearly, from a fixed threshold (<xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4A and 4B</xref>; Methods). In response to increasing input amplitudes, the linearized feedback gain of the nonlinear circuit increases from a gain of 0, for small inputs (below the threshold), to a gain of 1, for large inputs (colored lines in <xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4A</xref>). This precisely matches the range over which Γ must vary, as the SNR of the input changes to minimize the network gain.</p>
<fig id="pcbi.1004315.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Rectification nonlinearity and its effect on the feedback inhibitory circuit.</title>
<p>(a) Rectification nonlinearity (black), with threshold at v = 1. Linearized responses in color (cyan: min; magenta: max). (b) Nonlinear feedback inhibitory network. The nonlinearity (inset) is applied to the interneuron’s output. Nonlinearities with increasing thresholds are colored (green: min; red: max) (Methods). (c) Network gain at different input frequencies (note: the frequency space is in Z-space, i.e. defined with respect to the fixed time step of the network). The three colored curves are the nonlinear network response curves (computed using describing function analysis, colored as in (b)) (Methods). The dotted lines provide extreme parameter values for the linear network: Γ = 0,1.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g004" position="float" xlink:type="simple"/>
</fig>
<p>Also termed a dead-zone nonlinearity by engineers [<xref rid="pcbi.1004315.ref045" ref-type="bibr">45</xref>], the rectification nonlinearity is biologically plausible. The nonlinearity (<xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4B</xref>) needs to respond symmetrically around 0, i.e. to both positive and negative inputs. This is biologically unreasonable for any one neuron. However, such a nonlinearity can be constructed by using a pair of neurons, with each receiving half the inputs (a single sign), and having oppositely signed output connections. The rectification can then be implemented through half-wave rectification of each of the neurons’ outputs, either through the neurons’ spiking thresholds, or through the minimum voltage required to release a vesicle [<xref rid="pcbi.1004315.ref043" ref-type="bibr">43</xref>,<xref rid="pcbi.1004315.ref044" ref-type="bibr">44</xref>].</p>
<p>Therefore, a feedback inhibitory circuit with a rectification nonlinearity applied to the feedback interneuron (<xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4B</xref>) seems to be a plausible candidate to perform automatic adaptation, and approximate the minimal network gain, across a range of input SNRs in real neural circuits. To confirm this, we first demonstrate that the nonlinear feedback inhibitory network does, indeed, change its responses as the input properties change, and that the resulting network gain has the qualitative structure necessary for automatic adaptation.</p>
</sec>
<sec id="sec011">
<title>Qualitative Understanding of Nonlinear Network Dynamics</title>
<p>To understand the operation of the nonlinear feedback circuit (<xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4B</xref>), independent of the specific value of the threshold, it is convenient to consider its network gain in the frequency domain (<xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4C</xref>). This will provide intuition for the circuits’ response to inputs with different degrees of predictability. Indeed, since low frequency inputs change slowly, they are predictable. In contrast, high frequency inputs (near the Nyquist frequency) are unpredictable and, therefore, for the purposes of the feedback circuit equivalent to noise. The optimal linear feedback circuit for each of these input regimes differs. For low frequency (predictable) inputs, the optimal linear feedback circuit would set the feedback gain to 1. In contrast, for high frequency (unpredictable) inputs, the optimal linear feedback circuit should set the feedback gain to 0.</p>
<p>We observe that, without changing any parameters, each nonlinear network (with a specific threshold) shows network gains that approach those of the Γ = 1 linear network for low frequency inputs, and those of the Γ = 0 linear network for high frequency inputs (<xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4C</xref>, dotted lines show the performance of the two linear networks). This suggests that the rectilinear feedback inhibitory circuit approaches the optimal performance in different regimes of activity, without internal adaptation, i.e. performance of the form necessary for automatic adaptation. Further, the performance is qualitatively independent of the precise value of the threshold, suggesting–as introduced above–the dynamically changing performance is a general property of the shape of the rectilinear nonlinearity.</p>
<p>We now demonstrate that this qualitative understanding is also supported quantitatively, for fast varying input statistics: comparing the performance of the nonlinear feedback network against that of optimal linear networks.</p>
</sec>
<sec id="sec012">
<title>Quantitative Performance of Nonlinear vs. Linear Circuits</title>
<p>To compare the quantitative performance of the nonlinear and the linear circuits in the regime where input properties change too fast to allow for parameter adaptation, we define a class of non-stationary inputs. Each such input–termed a mixture–is composed of two components with different SNRs, mixed in time, such that there is one component for a fixed amount of time, and then the second component for the same amount of time. In this way, we are modelling the response of the circuit to an input with a rapid change from one SNR to another, as opposed to a single input, with a fixed SNR.</p>
<p>Within this input regime, we compare the nonlinear network to two different types of linear networks:
<list list-type="bullet">
<list-item><p>Type 1: The linear network that obtains the minimal network gain over the specific mixture of two SNR inputs, i.e. the minimal network gain for a non-adapting linear predictive coding network.</p></list-item>
<list-item><p>Type 2: The linear network that has sufficient time to adapt separately to optimally transmit each component of the mixture, i.e. this network has the minimal network gain for any linear predictive coding network (over this specific input mixture).</p></list-item>
</list></p>
<p>We demonstrate (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C and 5D</xref>) that the nonlinear network (red curves) both: (i) outperforms the type 1 linear network (blue curves), and (ii) approximates the performance of the type 2 linear network (dashed black curves).</p>
<fig id="pcbi.1004315.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparing the performance (measured through network gain) of nonlinear and linear feedback inhibitory networks (measured through network gain: lower is better) (details in Methods).</title>
<p>(a) Two input mixtures, modeling rapid transition from predictable to unpredictable input components. (b) Description of simulations. Inputs constructed as in (a). At each time point, inputs are either pure predictable signal, or pure unpredictable noise, with an instantaneous transition from one type to the other, in the middle of the simulation period. (c-f) Simulation outputs (inputs shown in inset). The amplitude of the unpredictable component of the mixture varies along the x axis. Error bars are 1 std. dev. (c,e) Network gain of the linear network of type 1, optimized to the mixture (blue, non-adapted linear response), is significantly higher than that of the nonlinear network (red). In contrast, the nonlinear network gain is close to the response of the optimal linear network of type 2, which is allowed to adapt to each component of the mixture (dotted black, adapted linear response). (e) Green shading indicates region where the nonlinear response is more than one std. dev. lower than the non-adapted linear response. Diagonal hashing indicates region where the nonlinear response is within one std. dev. of the adapted linear response. (d,f) % improvement of the performance of the nonlinear network over the type 1 linear network at different amplitudes of the unpredictable component. Data taken from (c) and (e) respectively. (f) Green box indicates region where the improvement is more than one std. dev. different from 0.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g005" position="float" xlink:type="simple"/>
</fig>
<p>In more detail, the response of both the linear and nonlinear networks to a mixture of two input components (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5A</xref>) is simulated, and the network gain computed (summarized in <xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5B</xref>). By varying the network parameters, we find the network which minimizes the gain for the specific mixture (Methods).</p>
<p>To robustly test the performance of the nonlinear circuit, we simulated its response to a mixture composed of components that are as distinct as possible. Therefore, we chose the first component of the mixture to be pure, predictable, correlated signal, and the second component to be unpredictable. As defined earlier, the correlated component is exponentially correlated with a fixed time constant. For the unpredictable component of the mixture, we utilize one of two inputs: (1) input at the Nyquist frequency, or (2) Gaussian white noise. Both these inputs are–for the purposes of a nonlinear predictive coding circuit with a non-zero time constant in the feedback neuron–unpredictable. Since the input mixture transitions from one extreme SNR to another, it should provide a strong test of the ability of a fixed nonlinear circuit to respond to a range of input SNRs.</p>
<p>We first show that the best linear network of type 1 is outperformed by the nonlinear network (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C</xref>, and green region in <xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5E</xref>). Indeed, when the unpredictable component of the mixture was composed of input at the Nyquist frequency, the improvement of the nonlinear network over the type 1 linear network was particularly large (30–40%) (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5D</xref>). When the unpredictable component of the mixture was modelled by the more biologically realistic white noise stimulus, the improvement was smaller, but it was still approx. 20% (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5F</xref>).</p>
<p>It is important to note that the linear network of type 1 against which we compare the nonlinear network’s performance has the minimal network gain of any such network. We could have used a linear network adapted to the first component of the mixture, and then measured its performance over both components. This would be a natural model for the case where a network was adapted to some input statistic, which changed rapidly, and the network had had insufficient time to adapt to the new statistic. However, the type 1 linear network outperforms any such linear network. Therefore, it provides a strong baseline against which to compare the performance of the nonlinear network.</p>
<p>Our results show that the nonlinear network’s improvement over the type 1 linear network persists even if (a) the unpredictable component has larger average amplitude than the predictable (correlated) component (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C–5F</xref>), or if (b) the fraction of non-stationarity within the input is low (<xref rid="pcbi.1004315.s002" ref-type="supplementary-material">S2 Fig</xref>). In both cases, the greatest relative improvement occurs when the uncorrelated component is comparatively smaller (either in amplitude or time) than the correlated component. However, the improvement persists over a wide range of input mixtures. Therefore, mixtures of noisy inputs with cleaner ones can be dealt with automatically–and effectively–by the nonlinear network, independent of either the amplitudes of each component, or the amount of each component within the mixture (modelling the sampling distribution over the environment).</p>
<p>Continuing beyond the improvement over type 1 networks, we next observe that the performance of the nonlinear network approximates the performance of the type 2 linear network (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C</xref> and diagonal hashed region in <xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5E</xref>). The type 2 linear network is allowed to adapt independently to each component of the mixture. Therefore, its performance is the lowest possible network gain, for any linear predictive coding algorithm–assuming sufficient time to adapt to each new input SNR. The observation that the nonlinear network is able to approximate the type 2 linear performance–despite having a fixed set of network parameters–for a range of input mixtures (with different input signal distributions), is precisely the desired, quantitative, demonstration of automatic adaptation.</p>
<p>Given this, we explore the potential role of nonlinear feedback inhibitory networks in real sensory systems.</p>
</sec>
<sec id="sec013">
<title>Comparison with Biological Experiments</title>
<p>Classical experiments, such as the seminal work of J. D. Victor in cat retinal ganglion cells [<xref rid="pcbi.1004315.ref017" ref-type="bibr">17</xref>], have shown that the adaptation of neural responses to varying input stimuli occurs nonlinearly. However, these observations do not directly assess the presence of an automatically adapting circuit since, classically, the neuronal responses were measured after the input had been introduced stably, for some time. Therefore, any observed nonlinear adaptation can be explained by the presence of a second, nonlinear estimate of the input statistics, whose output is then, secondarily, used to adapt the measured linear neural response properties. Such a mechanism would have a delay but, given the experimental time scale, such a delay was not constrained. Nevertheless, simulating the responses of the nonlinear feedback inhibitory network shows that its responses do agree qualitatively with the classically observed nonlinear experiments (<xref rid="pcbi.1004315.s003" ref-type="supplementary-material">S3 Fig</xref>).</p>
<p>To test specifically for the presence of automatic adaptation, experiments must constrain the speed of the change of the neuronal response function: an automatically adapting circuit will respond to a change in the input structure with an adaptive change, on the timescales of neuronal dynamics. Experimental evidence demonstrating these fast changes to neuronal responses has been found, recently, both in the salamander visual system [<xref rid="pcbi.1004315.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>] (<xref rid="pcbi.1004315.g006" ref-type="fig">Fig 6A and 6B</xref>) and in the auditory centers of the avian brain [<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>] (<xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7A–7C</xref>). Therefore, these changes appear to be a general property of sensory systems. We demonstrate that these experimental changes match the expected changes in the response filters, within the nonlinear network.</p>
<fig id="pcbi.1004315.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Linear filters from spike-triggered correlation analyses, estimated at different times around a sudden shift from low (L) to high (H) contrast.</title>
<p>(a,b) Adapted from [<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>]. (a) Timeline of experiment. (b) Experimentally measured filters (low contrast, blue; high contrast, red). High contrast filter is computed only from the response to the first 200 ms of high contrast stimulus (see timeline in (a)). (c) Simulated response filters of the principal cell of the nonlinear predictive coding circuit (Methods), showing oscillatory structure.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g006" position="float" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004315.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004315.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Response filters of zebra finch auditory neurons ((a), adapted from [<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>]) compared to theoretically simulated response filters of a nonlinear feedback inhibitory circuit (d).</title>
<p>(a-c) Adapted from [<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>]. (a) Linear response filters for a single neuron stimulated with inputs of different mean sound amplitudes (colored to match (d)). (b) Ratio of the total positive to total negative component of neuronal filters is computed for an input with high mean amplitude, and an input with low mean amplitude. Each circle shows these values for a different recorded neuron (one example being (a)). The colored circle is derived from the simulated response filters of the principal cell of the nonlinear circuit (d). It uses the ratio of the total positive to total negative component of the simulated filters (plotted in (e)) at the highest input amplitude (red), and the lowest input amplitude (cyan). (c) The BMF (freq. of the peak of the Fourier transform of the linear response filters) for a high mean input, and a low mean input (for different neurons, one example being (a)). The colored circle is derived from the simulated responses of the principal cell of the nonlinear network (d). It uses the BMFs (shown in (f)) of the simulated filters at the highest input amplitude (red) and the lowest input amplitude (cyan). (d) Linear filters estimated to best approximate the response of the principal cell of the nonlinear circuit to white noise stimuli of different amplitudes (Methods). Curves colored to match (a). (e) The ratio of the total positive component of the curves in (d) to the total negative component. Points are colored as in (a,d). The ratios derived from inputs with highest (red) and lowest amplitude (cyan) define the colored circle in (b). (f) The BMF of the responses filters from (d). The BMF values derived from inputs with highest (red) and lowest amplitude (cyan) define the colored circle in (c).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.g007" position="float" xlink:type="simple"/>
</fig>
<p>Baccus and Meister [<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>] found that the linear filters of retinal ganglion cells (RGCs) changed as fast as could be experimentally measured–within 200ms of a sudden input change. Further, as the mean input contrast is increased, the filter shifts its maximum weights onto inputs from the more recent past (<xref rid="pcbi.1004315.g006" ref-type="fig">Fig 6A and 6B</xref>) [<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>].</p>
<p>To match the qualitative structure of the observed temporal filter, with a smooth increase from zero to the first peak, we make two biologically reasonable changes to the nonlinear model (Methods, <xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4 Fig</xref>).</p>
<list list-type="order">
<list-item><p>We introduce an additional neuron with a non-zero time constant, prior to the principal neuron (<xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4B Fig</xref>) (modelling the upstream circuitry as a low pass filter, a first-order approximation).</p></list-item>
<list-item><p>We added a time constant to the principal neuron (<xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4C Fig</xref>) (since no real neuron can have a time constant of 0).</p></list-item>
</list>
<p>This modified network produces a smoothly varying temporal filter (with zero weight at t = 0) that can be compared to experiment (Methods). These two (biologically reasonable) changes are actually necessary; subsets of this model, with fewer neurons, or fewer time constants, won’t result in a smooth temporal filter (Methods).</p>
<p>Given this model, we demonstrate that the resulting response filter for the nonlinear feedback network shifts in the same direction as that measured by Baccus and Meister [<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>], in response to sudden input mean changes (<xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C</xref>). For example, the filter shifts towards more recent inputs (i.e. speeds up) in response to a sudden increase in input amplitude. This qualitative agreement between the change in the response filter introduced automatically by the nonlinear feedback network, and the responses of the RGCs, suggests that a nonlinear circuit could underlie the observed fast adaptation.</p>
<p>Mante et al. [<xref rid="pcbi.1004315.ref021" ref-type="bibr">21</xref>] also showed a rapid shift of the linear impulse response function for neurons in the lateral geniculate nucleus (LGN). Their observed change (Figures 3 and 4 in [<xref rid="pcbi.1004315.ref021" ref-type="bibr">21</xref>]) has the same qualitative structure as that observed by Baccus and Meister [<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>] (<xref rid="pcbi.1004315.g006" ref-type="fig">Fig 6B</xref>), but is observed in response to both changing input contrast and changing input luminance. Since the shift in the filter due to the nonlinear feedback inhibitory network is independent of the specific input, the nonlinear network is able to model the observed filter changes in the LGN.</p>
<p>This independence on the precise input also allows the automatic adaptation of the nonlinear feedback inhibitory network to generalize to non-visual sensory modalities. Nagel and Doupe [<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>] measured changes to the linear filters of auditory neurons in the zebra finch forebrain, which occurred rapidly, within 100ms of changes to the input auditory stimuli (<xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7A</xref>). They quantitatively characterized these changes:
<list list-type="order">
<list-item><p>In response to increasing input amplitude, the first peak of the temporal filter decreased in amplitude, and the first valley increased in amplitude. Therefore, there was a decrease in the ratio of total positive response of the filter to total negative response, when the input amplitude changes from high to low (points below the diagonal in <xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7B</xref>).</p></list-item>
<list-item><p>The shift in the location of the peaks can be characterized by the change in the peak frequency response of the filter (best mean frequency, BMF). Therefore, comparing high to low amplitude inputs, the authors identified an increase in the BMF (points above the diagonal in <xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7C</xref>).</p></list-item>
</list></p>
<p>We demonstrate, through simulating the responses of the nonlinear feedback circuit (<xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7D</xref>) that the shift in the filter responses agrees qualitatively with the shift in the measured linear responses (<xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7A</xref>). Indeed, as the amplitude increases, one observes not only a shift in the locations of the extrema, but also the addition of a second peak, following the valley, at the highest amplitudes. Further, each of the changes to the filters measured by Nagel and Doupe [<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>] are a natural property of the nonlinear circuit:
<list list-type="order">
<list-item><p>The positive/negative ratio of the simulated filter of the nonlinear network decreases as the input amplitude increases (<xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7E</xref>). Further, the simulated ratio at high and low input amplitudes agrees quantitatively with Nagel and Doupe’s measurements (colored circle in <xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7B</xref>).</p></list-item>
<list-item><p>The BMF of the simulated temporal filters of the nonlinear network increases for increasing input amplitudes (<xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7F</xref>). Further, the value of the BMF for the simulated filters at high vs. low input amplitudes agrees quantitatively with the experimental measurements (colored circle in <xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7C</xref>).</p></list-item>
</list></p>
<p>This suggests that a nonlinear feedback circuit could underlie the observed fast adaptation in the zebra finch auditory forebrain.</p>
<p>Importantly, the changes to the response filters of the nonlinear predictive coding network are a general property of the network, and not a function of the specific parameters chosen. Indeed, it is possible to demonstrate analytically that a nonlinear model of neurons with just two time constants, assuming only that the time constant of the interneuron is longer than that of the principal neuron, already shows a shift of its single extremum towards the more recent past (as input amplitudes increase) (<xref rid="pcbi.1004315.s006" ref-type="supplementary-material">S1 Text</xref>). Therefore, the rectified feedback inhibitory circuit design automatically lends itself to fast changes of the response filters of the output neuron, matching the observed fast adaptations in many different neural circuits.</p>
</sec>
</sec>
<sec id="sec014" sec-type="conclusions">
<title>Discussion</title>
<p>Neuronal circuits must transmit input signals which vary, rapidly, by multiple orders of magnitude [<xref rid="pcbi.1004315.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004315.ref003" ref-type="bibr">3</xref>,<xref rid="pcbi.1004315.ref025" ref-type="bibr">25</xref>,<xref rid="pcbi.1004315.ref026" ref-type="bibr">26</xref>], through neuron channels that have limited dynamic ranges and slow response times [<xref rid="pcbi.1004315.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1004315.ref005" ref-type="bibr">5</xref>]. This results in two computational challenges: transmitting signals with minimal power, and responding to rapid changes in the input statistics. We demonstrated that an optimal predictive coding algorithm, that reduces the transmission power of a correlated signal (and thereby ameliorates the first challenge), can be implemented with linear leaky integrator neurons using either feedback or feedforward inhibition. Inclusion of a static nonlinearity in the feedback inhibition circuit allows it to approximate the performance of the optimal linear predictive filter, for a range of input SNRs, while keeping its circuit parameters unchanged. Such a circuit, therefore, helps to address the second challenge. Indeed, we showed that the nonlinear feedback inhibitory network’s responses are in agreement with experimentally measured rapid changes to neuronal response functions in different sensory modalities [<xref rid="pcbi.1004315.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>,<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>].</p>
<p>Our analysis distinguishing the feedforward and feedback implementations of the algorithm demonstrated the importance of the neural implementation in developing intuition about an algorithm. For example, in the feedforward inhibitory circuit, it was possible for predictive coding to be implemented by a neuron with a short time constant, for large SNR inputs. However, in the feedback circuit, the predictive neuron is matched to the properties of the signal component, and independent of the SNR. Therefore, each implementation may have differing properties, which may each prove differently useful.</p>
<p>This work also demonstrates the necessity of studying inputs with rapidly varying statistics, to understand the different constraints they place on circuit implementations of an algorithm. Both the feedforward and feedback circuits can implement optimal linear predictive coding–when the input is statistically stationary. However, by analyzing the responses of the two different implementations to non-stationary inputs, we found that the feedback implementation provides a natural way to approximate an optimal response, through the addition of a circuit nonlinearity. In contrast, the alternative, feedforward, implementation requires adaptation of its underlying cellular properties, which would be difficult to vary through a circuit nonlinearity.</p>
<p>In this report, we introduced intuition on the shape of a nonlinearity necessary to perform automatic adaptation. Given its mathematical convenience, and biological plausibility, we focused on the rectilinear nonlinearity. However, it is important to note that the rectification nonlinearity is not the only nonlinearity that can satisfy the necessary structure. Indeed, any nonlinearity with the necessary inflection point should also be able to perform an automatic adaptation. This provides an avenue for further analysis.</p>
<p>Another avenue for further exploration is how to generalize our results on nonlinear predictive coding networks to more complex stimuli. We found the optimal linear predictive coding algorithm for an exponentially correlated signal with a single time constant. However, naturalistic stimuli can be modeled as a combination of several exponentially correlated signals, with different time constants. This suggests that to respond to a naturalistic stimulus, there should be several predictive coding circuits, each adapted to one of the correlations within the signal. However, how could these different predictive coding circuits be combined to respond optimally overall? One possible solution may be the addition of mutually inhibitory connections between the parallel predictive coding circuits. This circuit design should allow each neuron to respond maximally to the input component that it was adapted for, while simultaneously removing that input component from the remaining neurons. Hence, it might allow the net response of the larger circuit to remain close to optimal. A similar network design has been shown to implement predictive coding across a spatial scene (for a non time-varying stimulus) [<xref rid="pcbi.1004315.ref046" ref-type="bibr">46</xref>]. Exploring circuits of this form, built using simpler building blocks, should prove useful in understanding the design of more complex circuits.</p>
<p>Another direction to explore is the computational function of the nonlinear circuit, beyond its linearized responses. In this work, we demonstrated that the network gain of the nonlinear feedback network approximates that of the optimal linear network. However, this does not imply the two networks have identical responses to stimuli. For example, the linear algorithm amplifies high frequency inputs (flattening the output frequency distribution, for an exponentially correlated signal with noise) (<xref rid="pcbi.1004315.s005" ref-type="supplementary-material">S5 Fig</xref>). In contrast, the nonlinear network reduces the transmission of high frequency inputs (since they are less likely to cross the threshold). This difference, where the nonlinear network may partially denoise the input, hints at additional, computational functions for nonlinear networks, which should be explored further.</p>
<p>In general, adaptation of network dynamics, causing them to respond faster when inputs are more salient, has been observed in different experiments [<xref rid="pcbi.1004315.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004315.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1004315.ref017" ref-type="bibr">17</xref>,<xref rid="pcbi.1004315.ref027" ref-type="bibr">27</xref>,<xref rid="pcbi.1004315.ref028" ref-type="bibr">28</xref>], often as a shifting of the linear kernels. Conceptually, this shift has been understood as the system responding more quickly to salient stimuli, since the relevant information can be extracted sooner [<xref rid="pcbi.1004315.ref047" ref-type="bibr">47</xref>]. Our results suggest that nonlinear feedback inhibitory circuits may provide a general neural mechanism with which to implement such a shift, automatically. In addition, the presence of fast adaptation in different species suggest that this mechanism may have been conserved over evolutionary history.</p>
<p>Finally, one long standing goal of computational neuroscience has been to develop circuit motifs, in a manner similar to electrical engineering. We believe that the nonlinear feedback inhibitory network could be one such neuronal circuit motif. It performs a specific computational function without losing information, and is stable with respect to internal disturbances (<xref rid="pcbi.1004315.s006" ref-type="supplementary-material">S1 Text</xref>). Hence, this nonlinear motif could be inserted into a larger computational circuit without affecting its function, while still maintaining stability, and lowering the internal dynamic range requirements. Further, the circuit’s structure is simple and biologically plausible. Hence, it can reasonably be implemented in many areas of the central nervous system. Therefore, we believe that the identification of this motif should provide a useful tool in the analysis of larger circuits in many neural systems.</p>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec016">
<title>Input Structure</title>
<p>The input used in optimizing the linear predictive coding circuit (as in Eq (<xref rid="pcbi.1004315.e003" ref-type="disp-formula">3</xref>)) was composed of an exponentially correlated signal and uncorrelated noise, combined with an SNR (of the power of the input) of <italic>σ</italic>. In detail, the signal component of the input, <italic>s</italic><sub><italic>t</italic></sub>, was defined as:
<disp-formula id="pcbi.1004315.e025">
<alternatives>
<graphic id="pcbi.1004315.e025g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e025" xlink:type="simple"/>
<mml:math display="block" id="M25" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>〈</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mi>〉</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>〈</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>〉</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac bevelled="true"><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>≡</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
and the noise, <italic>ε</italic><sub><italic>t</italic></sub>, as:
<disp-formula id="pcbi.1004315.e026">
<alternatives>
<graphic id="pcbi.1004315.e026g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e026" xlink:type="simple"/>
<mml:math display="block" id="M26" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>〈</mml:mi><mml:mrow><mml:msub><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mi>〉</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>〈</mml:mi><mml:mrow><mml:msub><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>〉</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mspace width="0.5em"/><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula></p>
<p>Our derivation of the optimal linear predictive coding filter did not require any constraint on the distributions (for either the signal or noise components of the input). Hence, for maximal generality, we left them unconstrained.</p>
</sec>
<sec id="sec017">
<title>Leaky Integrator Neurons</title>
<p>The predictive coding circuits were constructed with linear leaky integrator neurons (<xref rid="pcbi.1004315.s001" ref-type="supplementary-material">S1 Fig</xref>). The voltage dynamics of a single such neuron is described by:
<disp-formula id="pcbi.1004315.e027">
<alternatives>
<graphic id="pcbi.1004315.e027g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e027" xlink:type="simple"/>
<mml:math display="block" id="M27" overflow="scroll">
<mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula></p>
<p>where the synaptic conductance, g<sub>s</sub>, is measured as a fraction of the cell's membrane conductance. Discretizing Eq (<xref rid="pcbi.1004315.e027" ref-type="disp-formula">16</xref>) in time, and solving for v, we find that the resulting neuronal building block is an exponential, low pass filter, with time constant <italic>τ</italic><sub><italic>m</italic></sub>, as described in the text.</p>
<disp-formula id="pcbi.1004315.e028">
<alternatives>
<graphic id="pcbi.1004315.e028g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e028" xlink:type="simple"/>
<mml:math display="block" id="M28" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula>
</sec>
<sec id="sec018">
<title>Expanding the Feedback Recursion</title>
<p>The steps to derive the recursive equation governing the dynamics of the feedback circuit Eq (<xref rid="pcbi.1004315.e019" ref-type="disp-formula">12</xref>) are shown. The corresponding derivation for the feedforward circuit can be found in <xref rid="pcbi.1004315.s006" ref-type="supplementary-material">S1 Text</xref>.</p>
<p>The discretized feedback circuit is described by a pair of linear, recursive equations. As introduced in the text, we derive this pair of equations by computing the input to each cell in the circuit (<xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3A</xref>), and adding a delay for all inputs to the interneuron. We can ignore the corresponding delay onto inputs to the principal cell, because adding an additional delay to the inputs to <italic>p</italic><sub><italic>t</italic></sub>, i.e. having <italic>p</italic><sub><italic>t</italic></sub> = <italic>f</italic><sub><italic>t</italic>−1</sub> − <italic>n</italic><sub><italic>t</italic>−1</sub>, would not change the structure of the recursion in Eq (<xref rid="pcbi.1004315.e014" ref-type="disp-formula">9</xref>). It would simply add an additional time step of delay to everything.</p>
<p>This process gives us:
<disp-formula id="pcbi.1004315.e029">
<alternatives>
<graphic id="pcbi.1004315.e029g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e029" xlink:type="simple"/>
<mml:math display="block" id="M29" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
<disp-formula id="pcbi.1004315.e030">
<alternatives>
<graphic id="pcbi.1004315.e030g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e030" xlink:type="simple"/>
<mml:math display="block" id="M30" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula></p>
<p>Substituting Eq (<xref rid="pcbi.1004315.e029" ref-type="disp-formula">18</xref>) into Eq (<xref rid="pcbi.1004315.e030" ref-type="disp-formula">19</xref>) (at time point, t-1):
<disp-formula id="pcbi.1004315.e031">
<alternatives>
<graphic id="pcbi.1004315.e031g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e031" xlink:type="simple"/>
<mml:math display="block" id="M31" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(20)</label>
</disp-formula></p>
<p>Letting <italic>η</italic> = <italic>α</italic>(1−Γ), and then substituting Eq (<xref rid="pcbi.1004315.e031" ref-type="disp-formula">20</xref>), at an earlier time point, back into itself, we get:
<disp-formula id="pcbi.1004315.e032">
<alternatives>
<graphic id="pcbi.1004315.e032g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e032" xlink:type="simple"/>
<mml:math display="block" id="M32" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(21)</label>
</disp-formula></p>
<p>Repeating this process, we have:
<disp-formula id="pcbi.1004315.e033">
<alternatives>
<graphic id="pcbi.1004315.e033g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e033" xlink:type="simple"/>
<mml:math display="block" id="M33" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⋅</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(22)</label>
</disp-formula></p>
<p>Finally, substituting back into Eq (<xref rid="pcbi.1004315.e029" ref-type="disp-formula">18</xref>) gives the expanded recursion in the text, Eq (<xref rid="pcbi.1004315.e019" ref-type="disp-formula">12</xref>).</p>
</sec>
<sec id="sec019">
<title>Rectilinear Nonlinearity</title>
<p>The equation for the rectilinear nonlinearity (also known as a dead zone nonlinearity in the engineering literature) is as follows:
<disp-formula id="pcbi.1004315.e034">
<alternatives>
<graphic id="pcbi.1004315.e034g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004315.e034" xlink:type="simple"/>
<mml:math display="block" id="M34" overflow="scroll">
<mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mi>δ</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(23)</label>
</disp-formula></p>
</sec>
<sec id="sec020">
<title>Describing Function Approximation of Nonlinear Network</title>
<p>In <xref rid="pcbi.1004315.g003" ref-type="fig">Fig 3B</xref>, we present the Bode plots of both the linear and the nonlinear networks. However, the Bode plot for the nonlinear network is, necessarily, an approximation (since the response of any nonlinear network is dependent on its input history). The approximation used in this plot is obtained through Describing Function analysis [<xref rid="pcbi.1004315.ref045" ref-type="bibr">45</xref>], and is commonly used in control theory.</p>
<p>In this analysis, we assume that the response of the network to a single frequency of input (at a single amplitude) is linear for each such input. This linear model is allowed to vary with each different frequency (and amplitudes). We compute a look-up table for the effect of the nonlinearity by balancing the input across the nonlinear loop, for each frequency (and input amplitude). For every single frequency input to the network, however, it is necessary to make the assumption that the output of the network only produces a single frequency output (the primary component of the Fourier transform). This means that describing function analysis automatically discards any spread of the initial frequency into higher Fourier harmonics. The resulting Bode plots are, therefore, not quantitatively correct. However, it has been well established that describing function analysis does provide a reasonable, qualitatively correct result.</p>
</sec>
<sec id="sec021">
<title>Comparison of Linear to Nonlinear Network Gains</title>
<p>Response of optimal linear and nonlinear networks to varying mixtures of stimuli was simulated. To construct these plots, a 1:1 mixture of two input components was used. The first component was pure exponentially correlated signal, and the second was pure noise. Further, the amplitude of the noise component was varied (values on the x-axis of <xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C–5F</xref>), while the signal amplitude was kept constant at 1.</p>
<p>The response of three different networks, two linear and one nonlinear, was simulated for each input mixture, and the network gains computed. For all three networks, the discounting factor was matched to the time constant of the input within the signal component of the mixture. Parameter variation was then used to find the optimal value of Γ (for the linear networks) and both Γ and the threshold <italic>δ</italic> (for the nonlinear network) that allowed the corresponding network to minimize the cost function. For the type 1 linear network, Γ was only optimized once, over the entire mixture. However, for the type 2 linear network, Γ was optimized twice, for each component of the mixture, separately. Finally, for the nonlinear network, the parameters were again only optimized once, over the entire mixture. The resulting network gains, and relative % improvements are plotted in <xref rid="pcbi.1004315.g005" ref-type="fig">Fig 5C–5F</xref>.</p>
</sec>
<sec id="sec022">
<title>Comparing Nonlinear Feedback Circuit to Experiment</title>
<p>It was necessary to modify the analytically-derived nonlinear feedback circuit to obtain simulations that can be directly compared to the experimentally measured response filters of different sensory neurons. The experimentally measured filters place a low weight on inputs at t = 0, with the weight increasing to a peak, followed by a reducing oscillation between peaks and troughs (Figs <xref rid="pcbi.1004315.g005" ref-type="fig">5B</xref> and <xref rid="pcbi.1004315.g006" ref-type="fig">6D</xref>). However, with the structure of the network introduced analytically (diagrammed in <xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4A Fig</xref>), it is straightforward to show that the response filter of the corresponding linear circuit applies a maximal weight at t = 0.</p>
<p>Since real neurons must have a non-zero time constant, the first change that we considered to the model, was to add a non-zero time constant to the principal neuron. However, the linearized response filter of this modified model still has maximal weight at t = 0. An alternative change is to add a time constant to a neuron providing input to the two-neuron network (<xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4B Fig</xref>). Since real neurons are embedded in a larger network of neurons, this is reasonable. However, again, the linearized response filter of this three-neuron model, with two non-zero time constants, has maximal weight at t = 0. In contrast, combining both the above changes into a single model with three neurons, all with non-zero time constants (<xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4C Fig</xref>), provides a model that shows the same behavior as experimentally measured, with a low weight on inputs from the immediate past, increasing to a peak, followed by a decrease to a trough. This model is a simple, biologically reasonable modification to the original predictive coding model that provides a response filter that is qualitatively similar to experiment. Therefore, we used this model for the in silico comparisons with experiment in Figs <xref rid="pcbi.1004315.g005" ref-type="fig">5</xref> and <xref rid="pcbi.1004315.g006" ref-type="fig">6</xref>.</p>
</sec>
<sec id="sec023">
<title>Response Filter of Nonlinear Network for Inputs of Different Amplitude</title>
<p>The response of the nonlinear network, with three neurons with non-zero time constants (<xref rid="pcbi.1004315.s004" ref-type="supplementary-material">S4 Fig</xref>), was simulated for &gt;2000 stimulus patterns, each containing 1000 time points of uncorrelated white noise. The stimulus amplitude was varied by varying the standard deviation of the sampling distribution. The linearized filter response of the nonlinear network to the inputs was then estimated by picking a point in time, and weighting the input to the network on each trial by the output of the network at that time point. This method is an analog of the spike-triggered averaging (STA) algorithm utilized by experimentalists (and is only modified by the use of the graded output values, in the absence of an output spiking neuron). The filter responses to inputs of different amplitudes could then be compared, and appropriate parameters extracted from the simulated curves.</p>
</sec>
</sec>
<sec id="sec024">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004315.s001" xlink:href="info:doi/10.1371/journal.pcbi.1004315.s001" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Single compartment model of a linear neuron.</title>
<p>Electrically, the neuron is an RC circuit, with inputs arriving as current (g<sub>s</sub>v<sub>input</sub>).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004315.s002" xlink:href="info:doi/10.1371/journal.pcbi.1004315.s002" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>% improvement in network gains for a nonlinear network (over the corresponding linear network) while varying the fraction of the signal component to the noise component in a two component input mixture.</title>
<p>Both the linear and nonlinear networks are only allowed to adapt to the mixture (and not to each individual component).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004315.s003" xlink:href="info:doi/10.1371/journal.pcbi.1004315.s003" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Nonlinear responses of X-type retinal ganglion cells (adapted from [<xref rid="pcbi.1004315.ref017" ref-type="bibr">17</xref>]) compared to simulated responses of the nonlinear feedback inhibitory circuit.</title>
<p>(a) Firing rate of X-type retinal ganglion cells in response to a stimulus pulse of increasing contrast; dashed lines denote peak (cyan) and steady-state (red) responses. (b) Ratio of steady-state amplitude to peak amplitude for experimental (squares) and simulated model responses (diamonds). The reduction in the ratio, as measured experimentally, is qualitatively the same as the simulation. (c) Bode plots of responses of retinal ganglion cells, for sinusoidal stimuli with increasing contrast (figure adapted from [<xref rid="pcbi.1004315.ref017" ref-type="bibr">17</xref>]). As contrast increases, the peak frequency response increases in amplitude, and shifts to higher frequencies; similarly, the phase shifts rightwards. (b) Bode plots of the transfer function of the nonlinear predictive coding network, for increasing input contrasts (increasing from grey to black), computed using describing function analysis (Methods, [<xref rid="pcbi.1004315.ref045" ref-type="bibr">45</xref>]). The shifts observed are qualitatively similar to those measured experimentally (see (c)).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004315.s004" xlink:href="info:doi/10.1371/journal.pcbi.1004315.s004" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Modifications to the nonlinear feedback inhibitory network to allow comparison with experiment (Methods).</title>
<p>(a) Nonlinear predictive coding circuit (as in <xref rid="pcbi.1004315.g004" ref-type="fig">Fig 4B</xref>). (1) Principal neuron; (2) Interneuron. (b) Additional neuron (3), upstream of predictive coding circuit (with non-zero time constant,χ). Model, without nonlinearity, used in analytical analysis of shifting response filters. (c) Neuron (1) modified to include a non-zero time constant. Model used in in silico simulations shown in <xref rid="pcbi.1004315.g006" ref-type="fig">Fig 6</xref> and <xref rid="pcbi.1004315.g007" ref-type="fig">Fig 7</xref>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004315.s005" xlink:href="info:doi/10.1371/journal.pcbi.1004315.s005" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Whitening effect of the optimal linear network for inputs with different SNRs.</title>
<p>(a-e) Input power (blue) and the power within the optimal transfer function of the network (red) at different frequencies. SNR decreases from (a)–(e) (f-j) Output power at each frequency (obtained by multiplying both functions from left column). Notice the flat output power, termed whitening. Also, notice the reduction in total transmitted power. This reduction in power get progressively less as the fraction of predictable signal within the input reduces (i.e. as the SNR decreases). At the extreme case, in the final row, with pure noise, the input has the same power as the output, with no reduction of gain.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004315.s006" xlink:href="info:doi/10.1371/journal.pcbi.1004315.s006" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supplementary methods and proofs.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>AB thanks Simon Laughlin, his PhD co-supervisor, for introducing him to predictive coding, and for helpful discussions during AB’s time in Cambridge. Also, we thank Tao Hu and other members of the Chklovskii lab, for helpful discussions on adaptive signal processing.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004315.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attwell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Borges</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>M</given-names></name> (<year>1987</year>) <article-title>Signal clipping by the rod output synapse</article-title>. <source>Nature</source> <volume>328</volume>: <fpage>522</fpage>–<lpage>524</lpage>. <object-id pub-id-type="pmid">3039370</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Do we know what the early visual system does?</article-title> <source>J Neurosci</source> <volume>25</volume>: <fpage>10577</fpage>–<lpage>10597</lpage>. <object-id pub-id-type="pmid">16291931</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Rudd</surname> <given-names>ME</given-names></name> (<year>2009</year>) <article-title>The challenges natural images pose for visual adaptation</article-title>. <source>Neuron</source> <volume>64</volume>: <fpage>605</fpage>–<lpage>616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.11.028" xlink:type="simple">10.1016/j.neuron.2009.11.028</ext-link></comment> <object-id pub-id-type="pmid">20005818</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Victor</surname> <given-names>J</given-names></name> (<year>1999</year>) <article-title>Temporal aspects of neural coding in the retina and lateral geniculate</article-title>. <source>Network: Computation in Neural Systems</source> <volume>10</volume>: <fpage>R1</fpage>–<lpage>R66</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werblin</surname> <given-names>FS</given-names></name> (<year>1971</year>) <article-title>Adaptation in a vertebrate retina: intracellular recording in Necturus</article-title>. <source>Journal of Neurophysiology</source> <volume>34</volume>: <fpage>228</fpage>–<lpage>241</lpage>. <object-id pub-id-type="pmid">5545938</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name> (<year>1961</year>) <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sensory communication</source>: <fpage>217</fpage>–<lpage>234</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Jayant</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>Noll</surname> <given-names>P</given-names></name> (<year>1984</year>) <chapter-title>Digital coding of waveforms: principles and applications to speech and video</chapter-title>. <publisher-loc>Englewood Cliffs, N.J.</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>. <volume>xvi</volume>, <fpage>688</fpage> p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sayed</surname> <given-names>AH</given-names></name> (<year>2008</year>) <chapter-title>Adaptive filters</chapter-title>. <publisher-loc>Hoboken, N.J.</publisher-loc>: <publisher-name>Wiley-Interscience: IEEE Press</publisher-name>. <volume>xxx</volume>, 786 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vaidyanathan</surname> <given-names>PP</given-names></name> (<year>2007</year>) <article-title>The Theory of Linear Prediction</article-title>. <source>Synthesis Lectures on Signal Processing</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>184</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref010"><label>10</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Wiener</surname> <given-names>N</given-names></name> (<year>1949</year>) <chapter-title>Extrapolation, interpolation, and smoothing of stationary time series, with engineering applications</chapter-title>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Technology Press of the Massachusetts Institute of Technology</publisher-name>. <volume>ix</volume>, 163 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Widrow</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Stearns</surname> <given-names>SD</given-names></name> (<year>1985</year>) <chapter-title>Adaptive signal processing</chapter-title>. <publisher-loc>Englewood Cliffs, N.J.</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>. <volume>xviii</volume>, 474 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Srinivasan</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Dubs</surname> <given-names>A</given-names></name> (<year>1982</year>) <article-title>Predictive coding: a fresh view of inhibition in the retina</article-title>. <source>Proc R Soc Lond B Biol Sci</source> <volume>216</volume>: <fpage>427</fpage>–<lpage>459</lpage>. <object-id pub-id-type="pmid">6129637</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name> (<year>1999</year>) <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature neuroscience</source> <volume>2</volume>: <fpage>79</fpage>–<lpage>87</lpage>. <object-id pub-id-type="pmid">10195184</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name> (<year>2004</year>) <article-title>Probabilistic models of attention based on iconic representations and predictive coding</article-title>. <source>Neurobiology of attention</source>: <fpage>553</fpage>–<lpage>561</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vuust</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ostergaard</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pallesen</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Bailey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Roepstorff</surname> <given-names>A</given-names></name> (<year>2009</year>) <article-title>Predictive coding of music–brain responses to rhythmic incongruity</article-title>. <source>Cortex</source> <volume>45</volume>: <fpage>80</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cortex.2008.05.014" xlink:type="simple">10.1016/j.cortex.2008.05.014</ext-link></comment> <object-id pub-id-type="pmid">19054506</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name> (<year>2011</year>) <article-title>Predictive coding</article-title>. <source>Wiley Interdisciplinary Reviews: Cognitive Science</source> <volume>2</volume>: <fpage>580</fpage>–<lpage>593</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Victor</surname> <given-names>JD</given-names></name> (<year>1987</year>) <article-title>The dynamics of the cat retinal X cell centre</article-title>. <source>J Physiol</source> <volume>386</volume>: <fpage>219</fpage>–<lpage>246</lpage>. <object-id pub-id-type="pmid">3681707</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Lewen</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>van Steveninck</surname> <given-names>RRdR</given-names></name> (<year>2001</year>) <article-title>Efficiency and ambiguity in an adaptive neural code</article-title>. <source>Nature</source> <volume>412</volume>: <fpage>787</fpage>–<lpage>792</lpage>. <object-id pub-id-type="pmid">11518957</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shapley</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Victor</surname> <given-names>J</given-names></name> (<year>1978</year>) <article-title>The effect of contrast on the transfer properties of cat retinal ganglion cells</article-title>. <source>J Physiol</source> <volume>285</volume>: <fpage>275</fpage>–<lpage>298</lpage>. <object-id pub-id-type="pmid">745079</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hosoya</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>Dynamic predictive coding by the retina</article-title>. <source>Nature</source> <volume>436</volume>: <fpage>71</fpage>–<lpage>77</lpage>. <object-id pub-id-type="pmid">16001064</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Frazor</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Bonin</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Geisler</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>Independence of luminance and contrast in natural scenes and in the early visual system</article-title>. <source>Nature Neuroscience</source> <volume>8</volume>: <fpage>1690</fpage>–<lpage>1697</lpage>. <object-id pub-id-type="pmid">16286933</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Atick</surname> <given-names>JJ</given-names></name> (<year>2011</year>) <article-title>Could information theory provide an ecological theory of sensory processing?</article-title> <source>Network</source> <volume>22</volume>: <fpage>4</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3109/0954898X.2011.638888" xlink:type="simple">10.3109/0954898X.2011.638888</ext-link></comment> <object-id pub-id-type="pmid">22149669</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Hateren</surname> <given-names>JH</given-names></name> (<year>1992</year>) <article-title>A theory of maximizing sensory information</article-title>. <source>Biol Cybern</source> <volume>68</volume>: <fpage>23</fpage>–<lpage>29</lpage>. <object-id pub-id-type="pmid">1486129</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Hateren</surname> <given-names>JH</given-names></name> (<year>1993</year>) <article-title>Spatiotemporal contrast sensitivity of early vision</article-title>. <source>Vision Res</source> <volume>33</volume>: <fpage>257</fpage>–<lpage>267</lpage>. <object-id pub-id-type="pmid">8447098</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref025"><label>25</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>I︠A︡rbus</surname> <given-names>AL</given-names></name> (<year>1967</year>) <chapter-title>Eye movements and vision</chapter-title>. <publisher-loc>New York</publisher-loc>,: <publisher-name>Plenum Press</publisher-name>. <volume>xiii</volume>, 222 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Doupe</surname> <given-names>AJ</given-names></name> (<year>1998</year>) <article-title>Temporal and spectral sensitivity of complex auditory neurons in the nucleus HVc of male zebra finches</article-title>. <source>The Journal of neuroscience</source> <volume>18</volume>: <fpage>3786</fpage>–<lpage>3802</lpage>. <object-id pub-id-type="pmid">9570809</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baccus</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>M</given-names></name> (<year>2002</year>) <article-title>Fast and slow contrast adaptation in retinal circuitry</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>909</fpage>–<lpage>919</lpage>. <object-id pub-id-type="pmid">12467594</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nagel</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Doupe</surname> <given-names>AJ</given-names></name> (<year>2006</year>) <article-title>Temporal processing and adaptation in the songbird auditory forebrain</article-title>. <source>Neuron</source> <volume>51</volume>: <fpage>845</fpage>–<lpage>859</lpage>. <object-id pub-id-type="pmid">16982428</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hong</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lundstrom</surname> <given-names>BN</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name> (<year>2008</year>) <article-title>Intrinsic gain modulation and adaptive neural coding</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000119</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000119" xlink:type="simple">10.1371/journal.pcbi.1000119</ext-link></comment> <object-id pub-id-type="pmid">18636100</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Famulare</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>A</given-names></name> (<year>2010</year>) <article-title>Feature selection in simple neurons: how coding depends on spiking dynamics</article-title>. <source>Neural computation</source> <volume>22</volume>: <fpage>581</fpage>–<lpage>598</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2009.02-09-956" xlink:type="simple">10.1162/neco.2009.02-09-956</ext-link></comment> <object-id pub-id-type="pmid">19922290</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hong</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>y Arcas</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name> (<year>2007</year>) <article-title>Single neuron computation: from dynamical system to feature detector</article-title>. <source>Neural computation</source> <volume>19</volume>: <fpage>3133</fpage>–<lpage>3172</lpage>. <object-id pub-id-type="pmid">17970648</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mease</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Famulare</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gjorgjieva</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moody</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name> (<year>2013</year>) <article-title>Emergence of adaptive computation by single neurons in the developing cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>33</volume>: <fpage>12154</fpage>–<lpage>12170</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3263-12.2013" xlink:type="simple">10.1523/JNEUROSCI.3263-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23884925</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Flanagin</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name> (<year>2005</year>) <article-title>Adaptation without parameter change: Dynamic gain control in motion detection</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>102</volume>: <fpage>6172</fpage>–<lpage>6176</lpage>. <object-id pub-id-type="pmid">15833815</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harrison</surname> <given-names>C</given-names></name> (<year>1952</year>) <article-title>Experiments with linear prediction in television</article-title>. <source>Bell System Technical Journal</source> <volume>31</volume>: <fpage>764</fpage>–<lpage>783</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jarvinen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Vainio</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kapanen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Honkanen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Haavisto</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>GSM enhanced full rate speech codec</article-title>; <year>1997</year>. <source>IEEE</source>. pp. <fpage>771</fpage>–<lpage>774</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Makhoul</surname> <given-names>J</given-names></name> (<year>1975</year>) <article-title>Linear prediction: A tutorial review</article-title>. <source>Proceedings of the IEEE</source> <volume>63</volume>: <fpage>561</fpage>–<lpage>580</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Battista</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Casalino</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lande</surname> <given-names>C</given-names></name> (<year>1999</year>) <article-title>MPEG-4: a multimedia standard for the third millennium, part 1</article-title>. <source>IEEE multimedia</source> <volume>6</volume>: <fpage>74</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref038"><label>38</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Shi</surname> <given-names>YQ</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>H</given-names></name> (<year>1999</year>) <chapter-title>Image and video compression for multimedia engineering: fundamentals, algorithms, and standards</chapter-title>: <publisher-name>CRC press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiegand</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sullivan</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Bjontegaard</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Luthra</surname> <given-names>A</given-names></name> (<year>2003</year>) <article-title>Overview of the H. 264/AVC video coding standard</article-title>. <source>Circuits and Systems for Video Technology, IEEE Transactions on</source> <volume>13</volume>: <fpage>560</fpage>–<lpage>576</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dong</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Atick</surname> <given-names>JJ</given-names></name> (<year>1995</year>) <article-title>Statistics of natural time-varying images</article-title>. <source>Network: Computation in Neural Systems</source> <volume>6</volume>: <fpage>345</fpage>–<lpage>358</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2006</year>) <article-title>Models and properties of power-law adaptation in neural systems</article-title>. <source>J Neurophysiol</source> <volume>96</volume>: <fpage>826</fpage>–<lpage>833</lpage>. <object-id pub-id-type="pmid">16641386</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hausdorff</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>C</given-names></name> (<year>1996</year>) <article-title>Multiscaled randomness: A possible source of 1/f noise in biology</article-title>. <source>Phys Rev E Stat Phys Plasmas Fluids Relat Interdiscip Topics</source> <volume>54</volume>: <fpage>2154</fpage>–<lpage>2157</lpage>. <object-id pub-id-type="pmid">9965304</object-id></mixed-citation></ref>
<ref id="pcbi.1004315.ref043"><label>43</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2001</year>) <chapter-title>Theoretical neuroscience: computational and mathematical modeling of neural systems</chapter-title>. <publisher-loc>Cambridge, Mass.</publisher-loc>: <publisher-name>Massachusetts Institute of Technology Press</publisher-name>. <volume>xv</volume>, 460 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name> (<year>1999</year>) <chapter-title>Biophysics of computation: information processing in single neurons</chapter-title>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>. <volume>xxiii</volume>, 562 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref045"><label>45</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Åström</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>RM</given-names></name> (<year>2008</year>) <chapter-title>Feedback systems: an introduction for scientists and engineers</chapter-title>. <publisher-loc>Princeton</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>. <volume>xii</volume>, 396 p. p.</mixed-citation></ref>
<ref id="pcbi.1004315.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Druckmann</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name> (<year>2012</year>) <article-title>A mechanistic model of early sensory processing based on subtracting sparse representations</article-title>. <source>Advances in Neural Information Processing Systems</source>. <fpage>1979</fpage>–<lpage>1987</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004315.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeWeese</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zador</surname> <given-names>A</given-names></name> (<year>1998</year>) <article-title>Asymmetric Dynamics in Optimal Variance Adaptation</article-title>. <source>Neural Computation</source> <volume>10</volume>: <fpage>1179</fpage>–<lpage>1202</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>