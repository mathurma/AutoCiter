<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="ppub">1544-9173</issn><issn pub-type="epub">1545-7885</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PBIOLOGY-D-11-02588</article-id><article-id pub-id-type="doi">10.1371/journal.pbio.1001251</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
            </subj-group>
            <subj-group>
              <subject>Neurophysiology</subject>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Engineering</subject>
          <subj-group>
            <subject>Signal processing</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Linguistics</subject>
          </subj-group>
          <subj-group>
            <subject>Psychology</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Reconstructing Speech from Human Auditory Cortex</article-title><alt-title alt-title-type="running-head">Neural Decoding of Speech Sounds</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Pasley</surname>
            <given-names>Brian N.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>David</surname>
            <given-names>Stephen V.</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Mesgarani</surname>
            <given-names>Nima</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Flinker</surname>
            <given-names>Adeen</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Shamma</surname>
            <given-names>Shihab A.</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Crone</surname>
            <given-names>Nathan E.</given-names>
          </name>
          <xref ref-type="aff" rid="aff4">
            <sup>4</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Knight</surname>
            <given-names>Robert T.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="aff" rid="aff5">
            <sup>5</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Chang</surname>
            <given-names>Edward F.</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Helen Wills Neuroscience Institute, University of California Berkeley, Berkeley, California, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Institute for Systems Research and Department of Electrical and Computer Engineering, University of Maryland, College Park, Maryland, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Department of Neurological Surgery, University of California–San Francisco, San Francisco, California, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Department of Neurology, The Johns Hopkins University, Baltimore, Maryland, United States of America</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Department of Psychology, University of California Berkeley, Berkeley, California, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Zatorre</surname>
            <given-names>Robert</given-names>
          </name>
          <role>Academic Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">McGill University, Canada</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">bpasley@berkeley.edu</email></corresp>
        <fn fn-type="con">
          <p>The author(s) have made the following declarations about their contributions: Conceived and designed the experiments: BNP SVD. Performed the experiments: NEC EFC BNP AF. Analyzed the data: BNP SVD. Contributed reagents/materials/analysis tools: NM SVD SAS BNP. Wrote the paper: BNP SVD NM AF SAS NEC RTK EFC. Coordinated project: RTK NEC EFC.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>1</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>31</day>
        <month>1</month>
        <year>2012</year>
      </pub-date><volume>10</volume><issue>1</issue><elocation-id>e1001251</elocation-id><history>
        <date date-type="received">
          <day>24</day>
          <month>6</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>13</day>
          <month>12</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Pasley et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract abstract-type="toc">
        <p>Direct brain recordings from neurosurgical patients listening to speech reveal that the acoustic speech signals can be reconstructed from neural activity in auditory cortex.</p>
      </abstract><abstract>
        <p>How the human auditory system extracts perceptually relevant acoustic features of speech is unknown. To address this question, we used intracranial recordings from nonprimary auditory cortex in the human superior temporal gyrus to determine what acoustic information in speech sounds can be reconstructed from population neural activity. We found that slow and intermediate temporal fluctuations, such as those corresponding to syllable rate, were accurately reconstructed using a linear model based on the auditory spectrogram. However, reconstruction of fast temporal fluctuations, such as syllable onsets and offsets, required a nonlinear sound representation based on temporal modulation energy. Reconstruction accuracy was highest within the range of spectro-temporal fluctuations that have been found to be critical for speech intelligibility. The decoded speech representations allowed readout and identification of individual words directly from brain activity during single trial sound presentations. These findings reveal neural encoding mechanisms of speech acoustic parameters in higher order human auditory cortex.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Spoken language is a uniquely human trait. The human brain has evolved computational mechanisms that decode highly variable acoustic inputs into meaningful elements of language such as phonemes and words. Unraveling these decoding mechanisms in humans has proven difficult, because invasive recording of cortical activity is usually not possible. In this study, we take advantage of rare neurosurgical procedures for the treatment of epilepsy, in which neural activity is measured directly from the cortical surface and therefore provides a unique opportunity for characterizing how the human brain performs speech recognition. Using these recordings, we asked what aspects of speech sounds could be reconstructed, or decoded, from higher order brain areas in the human auditory system. We found that continuous auditory representations, for example the speech spectrogram, could be accurately reconstructed from measured neural signals. Reconstruction quality was highest for sound features most critical to speech intelligibility and allowed decoding of individual spoken words. The results provide insights into higher order neural speech processing and suggest it may be possible to readout intended speech directly from brain activity.</p>
      </abstract><funding-group><funding-statement>This research was supported by NS21135 (RTK), PO4813 (RTK), NS40596 (NEC), and K99NS065120 (EFC). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="13"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>The early auditory system decomposes speech and other complex sounds into elementary time-frequency representations prior to higher level phonetic and lexical processing <xref ref-type="bibr" rid="pbio.1001251-Young1">[1]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>. This early auditory analysis, proceeding from the cochlea to the primary auditory cortex (A1) <xref ref-type="bibr" rid="pbio.1001251-Young1">[1]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Schreiner1">[3]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Eggermont1">[6]</xref>, yields a faithful representation of the spectro-temporal properties of the sound waveform, including those acoustic cues relevant for speech perception, such as formants, formant transitions, and syllable rate <xref ref-type="bibr" rid="pbio.1001251-Shamma1">[7]</xref>. However, relatively little is known about what specific features of natural speech are represented in intermediate and higher order human auditory cortex. In particular, the posterior superior temporal gyrus (pSTG), part of classical Wernicke's area <xref ref-type="bibr" rid="pbio.1001251-Geschwind1">[8]</xref>, is thought to play a critical role in the transformation of acoustic information into phonetic and pre-lexical representations <xref ref-type="bibr" rid="pbio.1001251-Hickok1">[4]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>. PSTG is believed to participate in an “intermediate” stage of processing that extracts spectro-temporal features essential for auditory object recognition and discards nonessential acoustic features <xref ref-type="bibr" rid="pbio.1001251-Hickok1">[4]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Romanski1">[11]</xref>. To investigate the nature of this auditory representation, we directly quantified how well different stimulus representations account for observed neural responses in nonprimary human auditory cortex, including areas along the lateral surface of STG. One approach, referred to as stimulus reconstruction <xref ref-type="bibr" rid="pbio.1001251-Bialek1">[12]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Ramirez1">[15]</xref>, is to measure population neural responses to various stimuli and then evaluate how accurately the original stimulus can be reconstructed from the measured responses. Comparison of the original and reconstructed stimulus representation provides a quantitative description of the specific features that can be encoded by the neural population. Furthermore, different stimulus representations, referred to as encoding models, can be directly compared to test hypotheses about how the neural population represents auditory function <xref ref-type="bibr" rid="pbio.1001251-Wu1">[16]</xref>.</p>
      <p>In this study, we focus on whether important spectro-temporal auditory features of spoken words and continuous sentences can be reconstructed from population neural responses. Because significant information may be transformed or lost in the course of higher order auditory processing, an exact reconstruction of the physical stimulus is not expected. However, analysis of stimulus reconstruction can reveal the key auditory features that are preserved in the temporal cortex representation of speech. To investigate this, we analyzed multichannel electrode recordings obtained from the surface of human auditory cortex and examined the extent to which these population neural signals could be used for reconstruction of different auditory representations of speech sounds.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>Words and sentences from different English speakers were presented aurally to 15 patients undergoing neurosurgical procedures for epilepsy or brain tumor. All patients in this study had normal language capacity as determined by neurological exam. Cortical surface field potentials were recorded from non-penetrating multi-electrode arrays placed over the lateral temporal cortex (<xref ref-type="fig" rid="pbio-1001251-g001">Figure 1</xref>, red circles), including the pSTG. We investigated the nature of auditory information contained in temporal cortex neural responses using a stimulus reconstruction approach (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>) <xref ref-type="bibr" rid="pbio.1001251-Bialek1">[12]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Ramirez1">[15]</xref>. The reconstruction procedure is a multi-input, multi-output predictive model that is fit to stimulus-response data. It constitutes a mapping from neural responses to a multi-dimensional stimulus representation (<xref ref-type="fig" rid="pbio-1001251-g001">Figures 1</xref> and <xref ref-type="fig" rid="pbio-1001251-g002">2</xref>). This mapping can be estimated using a variety of different learning algorithms <xref ref-type="bibr" rid="pbio.1001251-Hastie1">[17]</xref>. In this study a regularized linear regression algorithm was used to minimize the mean-square error between the original and reconstructed stimulus (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Once the model was fit to a training set, it could then be used to predict the spectro-temporal content of any arbitrary sound, including novel speech not used in training.</p>
      <fig id="pbio-1001251-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Experiment paradigm.</title>
          <p>Participants listened to words (acoustic waveform, top left), while neural signals were recorded from cortical surface electrode arrays (top right, red circles) implanted over superior and middle temporal gyrus (STG, MTG). Speech-induced cortical field potentials (bottom right, gray curves) recorded at multiple electrode sites were used to fit multi-input, multi-output models for offline decoding. The models take as input time-varying neural signals at multiple electrodes and output a spectrogram consisting of time-varying spectral power across a range of acoustic frequencies (180–7,000 Hz, bottom left). To assess decoding accuracy, the reconstructed spectrogram is compared to the spectrogram of the original acoustic waveform.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g001" xlink:type="simple"/>
      </fig>
      <fig id="pbio-1001251-g002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g002</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Spectrogram reconstruction.</title>
          <p>(A) Top: spectrogram of six isolated words (deep, jazz, cause) and pseudowords (fook, ors, nim) presented aurally to an individual participant. Bottom: spectrogram-based reconstruction of the same speech segment, linearly decoded from a set of electrodes. Purple and green bars denote vowels and fricative consonants, respectively, and the spectrogram is normalized within each frequency channel for display. (B) Single trial high gamma band power (70–150 Hz, gray curves) induced by the speech segment in (A). Recordings are from four different STG sites used in the reconstruction. The high gamma response at each site is <italic>z</italic>-scored and plotted in standard deviation (SD) units. Right panel: frequency tuning curves (dark black) for each of the four electrode sites, sorted by peak frequency and normalized by maximum amplitude. Red bars overlay each peak frequency and indicate SEM of the parameter estimate. Frequency tuning was computed from spectro-temporal receptive fields (STRFs) measured at each individual electrode site. Tuning curves exhibit a range of functional forms including multiple frequency peaks (<xref ref-type="supplementary-material" rid="pbio.1001251.s001">Figures S1B and S2B</xref>). (C) The anatomical distribution of fitted weights in the reconstruction model. Dashed box denotes the extent of the electrode grid (shown in <xref ref-type="fig" rid="pbio-1001251-g001">Figure 1</xref>). Weight magnitudes are averaged over all time lags and spectrogram frequencies and spatially smoothed for display. Nonzero weights are largely focal to STG electrode sites. Scale bar is 10 mm.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g002" xlink:type="simple"/>
      </fig>
      <p>The key component in the reconstruction algorithm is the choice of stimulus representation, as this choice encapsulates a hypothesis about the neural coding strategy under study. Previous applications of stimulus reconstruction in non-human auditory systems <xref ref-type="bibr" rid="pbio.1001251-Mesgarani1">[14]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Ramirez1">[15]</xref> have focused primarily on linear models to reconstruct the auditory spectrogram. The spectrogram is a time-varying representation of the amplitude envelope at each acoustic frequency (<xref ref-type="fig" rid="pbio-1001251-g001">Figure 1</xref>, bottom left) <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. The spectrogram envelope of natural sounds is not static but rather fluctuates across both frequency and time <xref ref-type="bibr" rid="pbio.1001251-Chi2">[19]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Elliott1">[21]</xref>. Envelope fluctuations in the spectrogram are referred to as modulations <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Dau1">[22]</xref> and play an important role in the intelligibility of speech <xref ref-type="bibr" rid="pbio.1001251-Chi2">[19]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Elliott1">[21]</xref>. Temporal modulations occur at different temporal rates and spectral modulations occur at different spectral scales. For example, slow and intermediate temporal modulation rates (&lt;4 Hz) are associated with syllable rate, while fast modulation rates (&gt;16 Hz) correspond to syllable onsets and offsets. Similarly, broad spectral modulations relate to vowel formants while narrow spectral structure characterizes harmonics. In the linear spectrogram model, modulations are represented implicitly as the fluctuations of the spectrogram envelope. Furthermore, neural responses are assumed to be linearly related to the spectrogram envelope.</p>
      <p>For stimulus reconstruction, we first applied the linear spectrogram model to human pSTG responses using a stimulus set of isolated words from an individual speaker. We used a leave-one-out cross-validation fitting procedure in which the reconstruction model was trained on stimulus-response data from isolated words and evaluated by directly comparing the original and reconstructed spectrograms of the out-of-sample word. Reconstruction accuracy is quantified as the correlation coefficient (Pearson's <italic>r</italic>) between the original and reconstructed stimulus. The reconstruction procedure is illustrated in <xref ref-type="fig" rid="pbio-1001251-g002">Figure 2</xref> for one participant with a high-density (4 mm) electrode grid placed over posterior temporal cortex. For different words, the linear model yielded accurate spectrogram reconstructions at the level of single trial stimulus presentations (<xref ref-type="fig" rid="pbio-1001251-g002">Figure 2A and B</xref>; see <xref ref-type="supplementary-material" rid="pbio.1001251.s007">Figure S7</xref> and Supporting <xref ref-type="supplementary-material" rid="pbio.1001251.s009">Audio File S1</xref> for example audio reconstructions). The reconstructions captured major spectro-temporal features such as energy concentration at vowel harmonics (<xref ref-type="fig" rid="pbio-1001251-g002">Figure 2A</xref>, purple bars) and high frequency components during fricative consonants (<xref ref-type="fig" rid="pbio-1001251-g002">Figure 2A</xref>, [z] and [s], green bars). The anatomical distribution of weights in the fitted reconstruction model revealed that the most informative electrode sites within temporal cortex were largely confined to pSTG (<xref ref-type="fig" rid="pbio-1001251-g002">Figure 2C</xref>).</p>
      <p>Across the sample of participants (<italic>N</italic> = 15), cross-validated reconstruction accuracy for single trials was significantly greater than zero in all individual participants (<italic>p</italic>&lt;0.001, randomization test, <xref ref-type="fig" rid="pbio-1001251-g003">Figure 3A</xref>). At the population level, mean accuracy averaged over all participants and stimulus sets (including different word sets and continuous sentences from different speakers) was highly significant (mean accuracy <italic>r</italic> = 0.28, <italic>p</italic>&lt;10<sup>−5</sup>, one-sample <italic>t</italic> test, <italic>df</italic> = 14). As a function of acoustic frequency, mean accuracy ranged from <italic>r</italic> = ∼0.2–0.3 (<xref ref-type="fig" rid="pbio-1001251-g003">Figure 3B</xref>).</p>
      <fig id="pbio-1001251-g003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g003</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Individual participant and group average reconstruction accuracy.</title>
          <p>(A) Overall reconstruction accuracy for each participant using the linear spectrogram model. Error bars denote resampling SEM. Overall accuracy is reported as the mean over all acoustic frequencies. Participants are grouped by grid density (low or high) and stimulus set (isolated words or sentences). Statistical significance of the correlation coefficient for each individual participant was computed using a randomization test. Reconstructed trials were randomly shuffled 1,000 times and the correlation coefficient was computed for each shuffle to create a null distribution of coefficients. The <italic>p</italic> value was calculated as the proportion of elements greater than the observed correlation. (B) Reconstruction accuracy as a function of acoustic frequency averaged over all participants (<italic>N</italic> = 15) using the linear spectrogram model. Shaded region denotes SEM over participants.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g003" xlink:type="simple"/>
      </fig>
      <p>We observed that overall reconstruction quality was influenced by a number of anatomical and functional factors as described below. First, informative temporal electrodes were primarily localized to pSTG. To quantify this, we defined “informative” electrodes as those associated with parameters with high signal-to-noise ratio in the reconstruction models (<italic>t</italic> ratio&gt;2.5, <italic>p</italic>&lt;0.05, false discovery rate (FDR) correction) <xref ref-type="fig" rid="pbio-1001251-g004">Figure 4A</xref> shows the anatomical distribution of informative electrodes pooled across participants and plotted in standardized anatomical coordinates (Montreal Neurological Institute, MNI) <xref ref-type="bibr" rid="pbio.1001251-Evans1">[23]</xref>). The distribution was centered in the pSTG (x = −70, y = −29, z = 12, MNI coordinates; Brodmann area 42), and was dispersed along the anterior-posterior axis.</p>
      <fig id="pbio-1001251-g004" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g004</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Factors influencing reconstruction quality.</title>
          <p>(A) Group average <italic>t</italic> value map of informative electrodes, which are predominantly localized to posterior STG. For each participant, informative electrodes are defined as those associated with significant weights (<italic>p</italic>&lt;0.05, FDR correction) in the fitted reconstruction model. To plot electrodes in a common anatomical space, spatial coordinates of significant electrodes are normalized to the MNI (Montreal Neurological Institute) brain template (Yale BioImage Suite, <ext-link ext-link-type="uri" xlink:href="http://www.bioimagesuite.org" xlink:type="simple">www.bioimagesuite.org</ext-link>). The dashed white line denotes the extent of electrode coverage pooled over participants. (B) Reconstruction accuracy is significantly greater than zero when using neural responses within the high gamma band (∼70–170 Hz; <italic>p</italic>&lt;0.05, one sample <italic>t</italic> tests, <italic>df</italic> = 14, Bonferroni correction). Accuracy was computed separately in 10 Hz bands from 1–300 Hz and averaged across all participants (<italic>N</italic> = 15). (C) Mean reconstruction accuracy improves with increasing number of electrodes used in the reconstruction algorithm. Error bars indicate SEM over 20 cross-validated data sets of four participants with 4 mm high density grids. (D) Accuracy across participants is strongly correlated (<italic>r</italic> = 0.78, <italic>p</italic>&lt;0.001, <italic>df</italic> = 13) with tuning spread (which varied by participant depending on grid placement and electrode density). Tuning spread was quantified as the fraction of frequency bins that included one or more peaks, ranging from 0 (no peaks) to 1 (at least one peak in all frequency bins, ranging from 180–7,000 Hz).</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g004" xlink:type="simple"/>
      </fig>
      <p>Second, significant predictive power (<italic>r</italic>&gt;0) was largely confined to neural responses in the high gamma band (∼70–170 Hz; <xref ref-type="fig" rid="pbio-1001251-g004">Figure 4B</xref>; <italic>p</italic>&lt;0.01, one-sample <italic>t</italic> tests, <italic>df</italic> = 14, Bonferroni correction). Predictive power for the high gamma band (∼70–170 Hz) was significantly better compared to other neural frequency bands (<italic>p</italic>&lt;0.05, Bonferroni adjusted pair-wise comparisons between frequency bands, following significant one-way repeated measures analysis of variance (ANOVA), <italic>F</italic>(30,420) = 128.7, <italic>p</italic>&lt;10<sup>−10</sup>). This is consistent with robust speech-induced high gamma responses reported in previous intracranial studies <xref ref-type="bibr" rid="pbio.1001251-Crone1">[24]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Pei1">[29]</xref> and with observed correlations between high gamma power and local spike rate <xref ref-type="bibr" rid="pbio.1001251-Logothetis1">[30]</xref>.</p>
      <p>Third, increasing the number of electrodes used in the reconstruction improved overall reconstruction accuracy (<xref ref-type="fig" rid="pbio-1001251-g004">Figure 4C</xref>). Overall prediction quality was relatively low for participants with five or fewer responsive STG electrodes (mean accuracy <italic>r</italic> = 0.19, <italic>N</italic> = 6 participants) and was robust for cases with high density grids (mean accuracy <italic>r</italic> = 0.43, <italic>N</italic> = 4, mean of 37 responsive STG electrodes per participant).</p>
      <p>What neural response properties allow the linear model to find an effective mapping to the stimulus spectrogram? There are two major requirements as described in the following paragraphs. First, individual recording sites must exhibit reliable frequency selectivity (e.g., <xref ref-type="fig" rid="pbio-1001251-g002">Figure 2B</xref>, right column; <xref ref-type="supplementary-material" rid="pbio.1001251.s001">Figures S1B</xref>, <xref ref-type="supplementary-material" rid="pbio.1001251.s002">S2</xref>). An absence of frequency selectivity (i.e., equal neural response amplitudes to all stimulus frequencies) would imply that neural responses do not encode frequency and could not be used to differentiate stimulus frequencies. To quantify frequency tuning at individual electrodes, we used estimates of standard spectro-temporal receptive fields (STRFs) (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). The STRF is a forward modeling approach commonly used to estimate neural tuning to a wide variety of stimulus parameters in different sensory systems <xref ref-type="bibr" rid="pbio.1001251-Wu1">[16]</xref>. We found that different electrodes were sensitive to different acoustic frequencies important for speech sounds, ranging from low (∼200 Hz) to high (∼7,000 Hz). The majority of individual sites exhibited a complex tuning profile with multiple peaks (e.g., <xref ref-type="fig" rid="pbio-1001251-g002">Figure 2B</xref>, rows 2 and 3; <xref ref-type="supplementary-material" rid="pbio.1001251.s002">Figure S2B</xref>). The full range of the acoustic speech spectrum was encoded by responses from multiple electrodes in the ensemble, although coverage of the spectrum varied by participant (<xref ref-type="fig" rid="pbio-1001251-g004">Figure 4D</xref>). Across participants, total reconstruction accuracy was positively correlated with the proportion of spectrum coverage (<italic>r</italic> = 0.78, <italic>p</italic>&lt;0.001, <italic>df</italic> = 13; <xref ref-type="fig" rid="pbio-1001251-g004">Figure 4D</xref>).</p>
      <p>A second key requirement of the linear model is that the neural response must rise and fall reliably with fluctuations in the stimulus spectrogram envelope. This is because the linear model assumes a linear mapping between the response and the spectrogram envelope. This requirement for “envelope-locking” reveals a major limitation of the linear model, which is most evident at fast temporal modulation rates. This limitation is illustrated in <xref ref-type="fig" rid="pbio-1001251-g005">Figure 5A</xref> (blue curve), which plots reconstruction accuracy as a function of modulation rate. A one-way repeated measures ANOVA (<italic>F</italic>(5,70) = 13.99, <italic>p</italic>&lt;10<sup>−8</sup>) indicated that accuracy was significantly higher for slow modulation rates (≤4 Hz) compared to faster modulation rates (&gt;8 Hz) (<italic>p</italic>&lt;0.05, post hoc pair-wise comparisons, Bonferroni correction). Accuracy for slow and intermediate modulation rates (≤8 Hz) was significantly greater than zero (<italic>r</italic> = ∼0.15 to 0.42; one-sample paired <italic>t</italic> tests, <italic>p</italic>&lt;0.0005, <italic>df</italic> = 14, Bonferroni correction) indicating that the high gamma response faithfully tracks the spectrogram envelope at these rates <xref ref-type="bibr" rid="pbio.1001251-Nourski1">[26]</xref>. However, accuracy levels were not significantly greater than zero at fast modulation rates (&gt;8 Hz; <italic>r</italic> = ∼0.10; one-sample paired <italic>t</italic> tests, <italic>p</italic>&gt;0.05, <italic>df</italic> = 14, Bonferroni correction), indicating a lack of reliable envelope-locking to rapid temporal fluctuations <xref ref-type="bibr" rid="pbio.1001251-LiegeoisChauvel1">[31]</xref>.</p>
      <fig id="pbio-1001251-g005" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g005</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Comparison of linear and nonlinear coding of temporal fluctuations.</title>
          <p>(A) Mean reconstruction accuracy (<italic>r</italic>) as a function of temporal modulation rate, averaged over all participants (<italic>N</italic> = 15). Modulation-based decoding accuracy (red curve) is higher compared to spectrogram-based decoding (blue curve) for temporal rates ≥4 Hz. In addition, spectrogram-based decoding accuracy is significantly greater than zero for lower modulation rates (≤8 Hz), supporting the possibility of a dual modulation and envelope-based coding scheme for slow modulation rates. Shaded gray regions indicate SEM over participants. (B) Mean ensemble rate tuning curve across all predictive electrode sites (<italic>n</italic> = 195). Error bars indicate SEM. Overlaid histograms indicate proportion of sites with peak tuning at each rate. (C) Within-site differences between modulation and spectrogram-based tuning. Arrow indicates the mean difference across sites. Within-site, nonlinear modulation models are tuned to higher temporal modulation rates than the corresponding linear spectrogram models (<italic>p</italic>&lt;10<sup>−7</sup>, two sample paired <italic>t</italic> test, <italic>df</italic> = 194).</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g005" xlink:type="simple"/>
      </fig>
      <p>Given the failure of the linear spectrogram model to reconstruct fast modulation rates, we evaluated competing models of auditory neural encoding. We investigated an alternative, nonlinear model based on modulation (described in detail in <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>). Speech sounds are characterized by both slow and fast temporal modulations (e.g., syllable rate versus onsets) as well as narrow and broad spectral modulations (e.g., harmonics versus formants) <xref ref-type="bibr" rid="pbio.1001251-Shamma1">[7]</xref>. The modulation model represents these multi-resolution features explicitly through a complex wavelet analysis of the auditory spectrogram. Computationally, the modulation representation is generated by a population of modulation-selective filters that analyze the two-dimensional spectrogram and extract modulation energy (a nonlinear operation) at different temporal rates and spectral scales (<xref ref-type="fig" rid="pbio-1001251-g006">Figure 6A</xref>) <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. Conceptually, this transformation is similar to the modulus of a 2-D Fourier transform of the spectrogram, localized at each acoustic frequency <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. The modulation model and applications to speech processing are described in detail in <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref> and <xref ref-type="bibr" rid="pbio.1001251-Shamma1">[7]</xref>.</p>
      <fig id="pbio-1001251-g006" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g006</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Schematic of nonlinear modulation model.</title>
          <p>(A) The input spectrogram (top left) is transformed by a linear modulation filter bank (right) followed by a nonlinear magnitude operation (not shown). This nonlinear operation extracts the modulation energy of the incoming spectrogram and generates phase invariance to local fluctuations in the spectrogram envelope. The input representation is the two-dimensional spectrogram S(<italic>f,t</italic>) across frequency <italic>f</italic> and time <italic>t</italic>. The output (bottom left) is the four-dimensional modulation energy representation M(<italic>s,r,f,t</italic>) across spectral modulation scale <italic>s</italic>, temporal modulation rate <italic>r</italic>, frequency <italic>f</italic>, and time <italic>t</italic>. In the full modulation representation <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>, negative rates by convention correspond to upward frequency sweeps, while positive rates correspond to downward frequency sweeps. Accuracy for positive and negative rates was averaged unless otherwise shown. See <xref ref-type="sec" rid="s4">Materials and Methods</xref>. (B) Schematic of linear (spectrogram envelope) and nonlinear (modulation energy) temporal coding. Left: acoustic waveform (black curve) and spectrogram of a temporally modulated tone. The linear spectrogram model (top) assumes that neural responses are a linear function of the spectrogram envelope (plotted for the tone center frequency channel, top right). In this case, the instantaneous output may be high or low and does not directly indicate the modulation rate of the envelope. The nonlinear modulation model (bottom) assumes that neural responses are a linear function of modulation energy. This is an amplitude-based coding scheme (plotted for the peak modulation channel, bottom right). The nonlinear modulation model explicitly estimates the modulation rate by taking on a constant value for a constant rate <xref ref-type="bibr" rid="pbio.1001251-Adelson1">[32]</xref>.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g006" xlink:type="simple"/>
      </fig>
      <p>The nonlinear component of the model is phase invariance to the spectrogram envelope (<xref ref-type="fig" rid="pbio-1001251-g006">Figure 6B</xref>). A fundamental difference with the linear spectrogram model is that phase invariance permits a nonlinear temporal coding scheme, whereby envelope fluctuations are encoded by amplitude rather than envelope-locking (<xref ref-type="fig" rid="pbio-1001251-g006">Figure 6B</xref>). Such amplitude-based coding schemes are broadly referred to as “energy models” <xref ref-type="bibr" rid="pbio.1001251-Adelson1">[32]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Kouh1">[33]</xref>. The modulation model therefore represents an auditory analog to the classical energy model of complex cells in the visual system <xref ref-type="bibr" rid="pbio.1001251-Adelson1">[32]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Willmore1">[36]</xref>, which are invariant to the spatial phase of visual stimuli.</p>
      <p>Reconstructing the modulation representation proceeds similarly to the spectrogram, except that individual reconstructed stimulus components now correspond to modulation energy at different rates and scales instead of spectral energy at different acoustic frequencies (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>, Stimulus Reconstruction). We next compared reconstruction accuracy using the nonlinear modulation model to that of the linear spectrogram model (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5A</xref>; <xref ref-type="supplementary-material" rid="pbio.1001251.s003">Figure S3</xref>). In the group data, the nonlinear model yielded significantly higher accuracy compared to the linear model (two-way repeated measures ANOVA; main effect of model type, <italic>F</italic>(1,14) = 33.36, <italic>p</italic>&lt;10<sup>−4</sup>). This included significantly better accuracy for fast temporal modulation rates compared to the linear spectrogram model (4–32 Hz; <xref ref-type="fig" rid="pbio-1001251-g005">Figure 5A</xref>, red versus blue curves; model type by modulation rate interaction effect, <italic>F</italic>(5,70) = 3.33, <italic>p</italic>&lt;0.01; post hoc pair-wise comparisons, <italic>p</italic>&lt;10<sup>−4</sup>, Bonferroni correction).</p>
      <p>The improved performance of the modulation model suggested that this representation provided better neural sensitivity to fast modulation rates compared to the linear spectrogram. To further investigate this possibility, we estimated modulation rate tuning curves at individual STG electrode sites (<italic>n</italic> = 195) using linear and nonlinear STRFs, which are based on the spectrogram and modulation representations, respectively (<xref ref-type="supplementary-material" rid="pbio.1001251.s004">Figure S4</xref>). Consistent with prior recordings from lateral temporal human cortex <xref ref-type="bibr" rid="pbio.1001251-LiegeoisChauvel1">[31]</xref>, average envelope-locked responses exhibit prominent tuning to low rates (1–8 Hz) with a gradual loss of sensitivity at higher rates (&gt;8 Hz) (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5B and C</xref>). In contrast, the average modulation-based tuning curves preserve sensitivity to much higher rates approaching 32 Hz (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5B and C</xref>).</p>
      <p>Sensitivity to fast modulation rates at single STG electrodes is illustrated for one participant in <xref ref-type="fig" rid="pbio-1001251-g007">Figure 7A</xref>. In this example (the word “waldo”), the spectrogram envelope (blue curve, top) fluctuates rapidly between the two syllables (“wal” and “do,” ∼300 ms). The linear model assumes that neural responses (high gamma power, black curves, left) are envelope-locked and directly track this rapid change. However, robust tracking of such rapid envelope changes was not generally observed, in violation of linear model assumptions. This is illustrated for several individual electrodes in <xref ref-type="fig" rid="pbio-1001251-g007">Figure 7A</xref> (compare black curves, left, with blue curve, top). In contrast, the modulation representation encodes this fluctuation nonlinearly as an increase in energy at fast rates (&gt;8 Hz, dashed red curves, ∼300 ms, bottom two rows). This allows the model to capture energy-based modulation information in the neural response. Modulation energy encoding at these sites is quantified by the corresponding nonlinear rate tuning curves (<xref ref-type="fig" rid="pbio-1001251-g007">Figure 7A</xref>, right column). These tuning curves show neural sensitivity to a range of temporal modulations with a single peak rate. For illustrative purposes, <xref ref-type="fig" rid="pbio-1001251-g007">Figure 7A</xref> (left) compares modulation energy at the peak temporal rate (dashed red curves) with the neural responses (black curves) at each individual site. This illustrates the ability of the modulation model to account for a rapid decrease in the spectrogram envelope without a corresponding decrease in the neural response.</p>
      <fig id="pbio-1001251-g007" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g007</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Example of nonlinear modulation coding and reconstruction.</title>
          <p>(A) Top: the spectrogram of an isolated word (“waldo”) presented aurally to one participant. Blue curve plots the spectrogram envelope, summed over all frequencies. Left panels: induced high gamma responses (black curves, trial averaged) at four different STG sites. Temporal modulation energy of the stimulus (dashed red curves) is overlaid (computed from 2, 4, 8, and 16 Hz modulation filters and normalized to maximum value). Dashed black lines indicate baseline response level. Right panels: nonlinear modulation rate tuning curves for each site (estimated from nonlinear STRFs). Shaded regions and error bars indicate SEM. (B) Original spectrogram (top), modulation-based reconstruction (middle), and spectrogram-based reconstruction (bottom), linearly decoded from a fixed set of STG electrodes. The modulation reconstruction is projected into the spectrogram domain using an iterative projection algorithm and an overcomplete set of modulation filters <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. The displayed spectrogram is averaged over 100 random initializations of the algorithm.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g007" xlink:type="simple"/>
      </fig>
      <p>The effect of sensitivity to fast modulation rates can also be observed when the modulation reconstruction is viewed in the spectrogram domain (<xref ref-type="fig" rid="pbio-1001251-g007">Figure 7B</xref>, middle, see Material and Methods, Reconstruction Accuracy). The result is that dynamic spectral information (such as the upward frequency sweep at ∼400–500 ms, <xref ref-type="fig" rid="pbio-1001251-g007">Figure 7B</xref>, top) is better resolved compared to the linear spectrogram-based reconstruction (<xref ref-type="fig" rid="pbio-1001251-g007">Figure 7B</xref>, bottom). These combined results support the idea of an emergent population-level representation of temporal modulation energy in primate auditory cortex <xref ref-type="bibr" rid="pbio.1001251-Wang1">[37]</xref>. In support of this notion, subpopulations of neurons have been found that exhibit both envelope and energy-based response properties in primary auditory cortex of non-human primates <xref ref-type="bibr" rid="pbio.1001251-Wang1">[37]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Bendor1">[39]</xref>. This has led to the suggestion of a dual coding scheme in which slow fluctuations are encoded by synchronized (envelope-locked) neurons, while fast fluctuations are encoded by non-synchronized (energy-based) neurons <xref ref-type="bibr" rid="pbio.1001251-Wang1">[37]</xref>.</p>
      <p>While these results indicate that a nonlinear model is required to reliably reconstruct fast modulation rates, psychoacoustic studies have shown that slow and intermediate modulation rates (∼1–8 Hz) are most critical for speech intelligibility <xref ref-type="bibr" rid="pbio.1001251-Chi2">[19]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Elliott1">[21]</xref>. These slow temporal fluctuations carry essential phonological information such as formant transitions and syllable rate <xref ref-type="bibr" rid="pbio.1001251-Shamma1">[7]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Chi2">[19]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Elliott1">[21]</xref>. The linear spectrogram model, which also yielded good performance within this range (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5A</xref>; <xref ref-type="supplementary-material" rid="pbio.1001251.s003">Figure S3</xref>), therefore appears sufficient to reconstruct the essential range of temporal modulations. To examine this issue, we further assessed reconstruction quality by evaluating the ability to identify isolated words using the linear spectrogram reconstructions. We analyzed a participant implanted with a high-density electrode grid (4 mm spacing), the density of which provided a large set of pSTG electrodes. Compared to lower density grid cases, data for this participant included ensemble frequency tuning that covered the majority of the (speech-related) acoustic spectrum (180–7,000 Hz), a factor which we found was critical for accurate reconstruction (<xref ref-type="fig" rid="pbio-1001251-g004">Figure 4D</xref>). Spectrogram reconstructions were generated for each of 47 words, using neural responses either from single trials or averaged over 3–5 trials per word (same word set and cross-validated fitting procedure as described in <xref ref-type="fig" rid="pbio-1001251-g002">Figure 2</xref>). To identify individual words from the reconstructions, a simple speech recognition algorithm based on dynamic time warping was used to temporally align words of variable duration <xref ref-type="bibr" rid="pbio.1001251-Rabiner1">[40]</xref>. For a target word, a similarity score (correlation coefficient) was then computed between the target reconstruction and the actual spectrograms of each of the 47 words in the candidate set. The 47 similarity scores were sorted and word identification rank was quantified as the percentile rank of the correct word. (1.0 indicates the target reconstruction matched the correct word out of all candidate words; 0.0 indicates the target was least similar to the correct word among all other candidates.) The expected mean of the distribution of identification ranks is 0.5 at chance level.</p>
      <p>Word identification using averaged trials was substantially higher than chance (<xref ref-type="fig" rid="pbio-1001251-g008">Figure 8A and B</xref>, median identification rank = 0.89, <italic>p</italic>&lt;0.0001; randomization test), with correctly identified words exhibiting accurate reconstructions and poorly identified words exhibiting inaccurate reconstructions (<xref ref-type="fig" rid="pbio-1001251-g008">Figure 8C</xref>). For single trials, identification performance declined slightly but remained significant (median = 0.76, <italic>p</italic>&lt;0.0001; randomization test). In addition, for each possible word pair, we computed the similarity between the two original spectrograms and compared this to the similarity between the reconstructed and actual spectrograms (using averaged trials; <xref ref-type="fig" rid="pbio-1001251-g008">Figure 8D</xref>; <xref ref-type="supplementary-material" rid="pbio.1001251.s005">Figure S5</xref>). Acoustic and reconstruction word similarities were correlated (<italic>r</italic> = 0.41, <italic>p</italic>&lt;10<sup>−10</sup>, <italic>df</italic> = 45), suggesting that acoustic similarity of the candidate words is likely to influence identification performance (i.e., identification is more difficult when the word set contains many acoustically similar sounds).</p>
      <fig id="pbio-1001251-g008" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.1001251.g008</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Word identification.</title>
          <p>Word identification based on the reconstructed spectrograms was assessed using a set of 47 individual words and pseudowords from a single speaker in a high density 4 mm grid experiment. The speech recognition algorithm is described in the text. (A) Distribution of identification rank for all 47 words in the set. Median identification rank is 0.89 (black arrow), which is higher than 0.50 chance level (dashed line; <italic>p</italic>&lt;0.0001; randomization test). Statistical significance was assessed by a randomization test in which a null distribution of the median was constructed by randomly shuffling the word pairs 10,000 times, computing median identification rank for each shuffle, and calculating the percentile rank of the true median in the null distribution. Best performance was achieved after smoothing the spectrograms with a 2-D box filter (500 ms, 2 octaves). (B) Receiver operating characteristic (ROC) plot of identification performance (red curve). Diagonal black line indicates no predictive power. (C) Examples of accurately (right) and inaccurately (left) identified words. Left: reconstruction of pseudoword “heef” is poor and leads to a low identification rank (0.13). Right: reconstruction of pseudoword “thack” is accurate and best matches the correct word out of 46 other candidate words (identification rank = 1.0). (D) Actual and reconstructed word similarity is correlated (<italic>r</italic> = 0.41). Pair-wise similarity between the original spectrograms of individual words is correlated with pair-wise similarity between the reconstructed and original spectrograms. Plotted values are computed prior to spectrogram smoothing used in the identification algorithm. Gray points denote the similarity between identical words.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.g008" xlink:type="simple"/>
      </fig>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>These findings demonstrate that key features in continuous and novel speech signals can be accurately reconstructed from STG neural responses using both spectrogram and modulation-based auditory representations, with the latter yielding better predictions at fast temporal modulation rates. For both representations, regions of good prediction performance included the range of spectro-temporal modulations most critical to speech intelligibility <xref ref-type="bibr" rid="pbio.1001251-Chi2">[19]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Elliott1">[21]</xref>.</p>
      <p>The primary difference between the linear spectrogram and nonlinear modulation models was evident in the predictive accuracy for fast temporal modulations (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5</xref>). To understand why the nonlinear modulation model performed better at fast modulation rates, it is useful to consider how the linear and nonlinear models make different assumptions about neural coding. The linear and nonlinear models are specified by different choices of stimulus representation. The linear model assumes a linear mapping between neural responses and the auditory spectrogram. The nonlinear model assumes a linear mapping between neural responses and the modulation representation. The modulation representation itself is a nonlinear transformation of the spectrogram and is based on emergent tuning properties that have been identified in the auditory cortex <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. Choosing a nonlinear stimulus representation effectively linearizes the stimulus-response mapping and allows one to fit linear models to the new space of transformed stimulus features <xref ref-type="bibr" rid="pbio.1001251-Hastie1">[17]</xref>,<xref ref-type="bibr" rid="pbio.1001251-David1">[35]</xref>. If the nonlinear stimulus representation is a more accurate description of neural responses, its predictive accuracy will be higher. In this approach, the choice of stimulus representation for reconstruction encapsulates hypotheses about the coding strategies under study. For example, Rieke et al. <xref ref-type="bibr" rid="pbio.1001251-Rieke1">[41]</xref> reconstructed the sound pressure waveform using neural responses from the bullfrog auditory periphery, where neural responses phase-lock to fluctuations in the raw stimulus waveform <xref ref-type="bibr" rid="pbio.1001251-Joris1">[2]</xref>. In the central auditory pathway, phase-locking to the stimulus waveform is rare <xref ref-type="bibr" rid="pbio.1001251-Joris1">[2]</xref>, and waveform reconstruction would be expected to fail. Instead, many neurons phase-lock to the spectrogram envelope (a nonlinear transformation of the stimulus waveform) <xref ref-type="bibr" rid="pbio.1001251-Joris1">[2]</xref>. Consistent with these response properties, spectrogram reconstruction has been demonstrated using neural responses from mammalian primary auditory cortex <xref ref-type="bibr" rid="pbio.1001251-Mesgarani1">[14]</xref> or the avian midbrain <xref ref-type="bibr" rid="pbio.1001251-Ramirez1">[15]</xref>. Beyond primary auditory areas, further processing in intermediate and higher-order auditory cortex likely results in additional stimulus transformations <xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>. In this study, we examined human STG, a nonprimary auditory area, and found that a nonlinear modulation representation yielded the best overall reconstruction accuracy, particularly at fast modulation rates (≥4 Hz). This suggests that phase-locking to the amplitude envelope is less robust at higher temporal rates and may instead be coded by an energy-based scheme <xref ref-type="bibr" rid="pbio.1001251-Wang1">[37]</xref>. Although additional studies are needed, this is consistent with a number of results suggesting that the capacity for envelope-locking decreases along the auditory pathway, extending from the inferior colliculus (32–256 Hz), medial geniculate body (16 Hz), primary auditory cortex (8 Hz), to nonprimary auditory areas (4–8 Hz) <xref ref-type="bibr" rid="pbio.1001251-Joris1">[2]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Eggermont1">[6]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Nourski1">[26]</xref>,<xref ref-type="bibr" rid="pbio.1001251-LiegeoisChauvel1">[31]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Nourski2">[42]</xref>.</p>
      <p>Fidelity of the reconstructions was sufficient to identify individual words using a rudimentary speech recognition algorithm. However, reconstruction quality at present is not clearly intelligible to a human listener (<xref ref-type="supplementary-material" rid="pbio.1001251.s007">Figure S7</xref> and Supporting <xref ref-type="supplementary-material" rid="pbio.1001251.s009">Audio File S1</xref>). It is possible that a better signal-to-noise ratio or more comprehensive (higher density) recordings in STG could produce intelligible speech reconstructions. Alternatively, the true features represented by STG may not be readily inverted back to an intelligible acoustic waveform. For speech comprehension, it is hypothesized that intermediate and higher-order auditory areas extract or construct information-rich features of speech, while discarding nonessential low-level acoustic information <xref ref-type="bibr" rid="pbio.1001251-Hickok1">[4]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Greenberg1">[43]</xref>. In the case that STG applies a highly nonlinear stimulus transformation, an exact reconstruction of the acoustic signal from STG responses would not be possible. Instead, speech reconstruction provides an important tool to investigate the critical features that are faithfully represented at different stages of the auditory system. For example, we found that low spectro-temporal modulations (temporal modulations &lt;8 Hz, spectral modulations &lt;4 cycles/octave, <xref ref-type="supplementary-material" rid="pbio.1001251.s003">Figure S3</xref>) are accurately reconstructed from spectrogram or modulation-based models. Modulations within this range correspond to important structural features of natural speech, including formants and syllable rate <xref ref-type="bibr" rid="pbio.1001251-Chi2">[19]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Elliott1">[21]</xref>. Although more work is needed to characterize the neural representation in the STG, this suggests that such key features are preserved at this stage in auditory processing. Our results are therefore consistent with the idea of pSTG as an intermediate stage in a hierarchy of auditory object processing <xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Russ1">[44]</xref>.</p>
      <p>Hierarchical auditory object processing has been hypothesized to follow a ventral “what” pathway, with an antero-lateral gradient along the superior temporal region <xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Romanski1">[11]</xref> where stimulus selectivity increases from pure tones in primary auditory cortex to words and sentences in anterior STG <xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>. How can hypotheses about the ventral pathway be tested within the stimulus reconstruction framework? In this framework, encoding models must be developed that encapsulate hypothesized neural mechanisms. These hypotheses are then tested by comparing predictive accuracies of the competing models <xref ref-type="bibr" rid="pbio.1001251-Wu1">[16]</xref>. For example, in the current work we compared reconstruction accuracy of linear and nonlinear auditory models. An important future direction is to compare performance of these auditory models to higher level models that implement more complex stimulus selectivity. Previous work suggests that categorical representations are an important organizational principle in STG <xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Formisano1">[45]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Tsunada1">[47]</xref>. These studies found evidence of neural selectivity for entire speech categories, such as vowels or syllables. Unlike the auditory representations studied here, these neural responses were relatively insensitive to acoustic variation. At a more abstract level of representation, a recent functional imaging study also demonstrated that the semantic content of nouns could be used as an effective encoding model across multiple cortical regions <xref ref-type="bibr" rid="pbio.1001251-Mitchell1">[48]</xref>.</p>
      <p>An important application of this approach has also been demonstrated in the study of visual object recognition in the primate visual system <xref ref-type="bibr" rid="pbio.1001251-Willmore1">[36]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Kay1">[49]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Naselaris1">[50]</xref>. These studies found that structural encoding models, based on spatio-temporal visual features, yielded good performance in primary and intermediate visual areas, including visual areas V1, V2, and V3, whereas a high level encoding model based on semantic features was required to achieve good performance in higher level areas such as V4 and lateral occipital cortex <xref ref-type="bibr" rid="pbio.1001251-Naselaris1">[50]</xref>. Our results suggest that a similar approach may be usefully applied to the auditory cortex, where structural auditory models may partially account for responses in primary and intermediate areas (e.g., A1 and pSTG), but development of higher level encoding models could be required to describe more anterior areas in the ventral auditory pathway. As understanding of cortical speech representation improves, future research into speech reconstruction may also be useful for development of neural interfaces for communication, for example by revealing the content of inner speech imagery.</p>
    </sec>
    <sec id="s4" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s4a">
        <title>Participants and Neural Recordings</title>
        <p>Electrocorticographic (ECoG) recordings were obtained using subdural electrode arrays implanted in 15 patients undergoing neurosurgical procedures for epilepsy or brain tumor. All participants volunteered and gave their informed consent before testing. The experimental protocol was approved by the Johns Hopkins Hospital, Columbia University Medical Center, University of California, San Francisco and Berkeley Institutional Review Boards and Committees on Human Research. Electrode grids had center-to-center distance of either 4 mm (<italic>N</italic> = 4 participants) <xref ref-type="bibr" rid="pbio.1001251-Chang1">[46]</xref> or 10 mm (<italic>N</italic> = 11) <xref ref-type="bibr" rid="pbio.1001251-Crone1">[24]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Canolty1">[25]</xref>. Grid placement was determined entirely by clinical criteria and covered left or right fronto-temporal regions in all patients. Localization and coregistration of electrodes with the structural MRI is described in detail in <xref ref-type="bibr" rid="pbio.1001251-Dalal1">[51]</xref>. Multi-channel ECoG data were amplified and digitally recorded with sampling rate = 1,000 Hz (<italic>N</italic> = 6 participants) <xref ref-type="bibr" rid="pbio.1001251-Crone1">[24]</xref>, 2,003 Hz (<italic>N</italic> = 5) <xref ref-type="bibr" rid="pbio.1001251-Canolty1">[25]</xref>, or 3,052 Hz (<italic>N</italic> = 4) <xref ref-type="bibr" rid="pbio.1001251-Chang1">[46]</xref>. All ECoG signals were remontaged to a common average reference <xref ref-type="bibr" rid="pbio.1001251-Crone1">[24]</xref> after removal of channels with artifacts or excessive noise (including electromagnetic noise from hospital equipment and poor contact with the cortical surface). Time-varying high gamma band power (70–150 Hz) was extracted from the multi-channel ECoG signal using the Hilbert-Huang transform <xref ref-type="bibr" rid="pbio.1001251-Canolty1">[25]</xref>, converted to standardized <italic>z</italic>-scores, and used for all analyses (except <xref ref-type="fig" rid="pbio-1001251-g004">Figure 4B</xref> in which the ECoG signal was filtered into 30 bands of width 10 Hz, ranging from 1–300 Hz, in order to calculate band-specific prediction accuracy). Data from a variety of language tasks were analyzed. Tasks included passive listening (<italic>N</italic> = 5 participants), target word detection (<italic>N</italic> = 5), and word/sentence repetition (<italic>N</italic> = 5).</p>
      </sec>
      <sec id="s4b">
        <title>Speech Stimuli</title>
        <p>Speech stimuli consisted of isolated words from a single speaker (<italic>N</italic> = 10 participants) or sentences from a variety of male and female speakers (<italic>N</italic> = 5). Isolated words included nouns, verbs, proper names, and pseudowords and were recorded by a native English female speaker (0.3–1 s duration, 16 kHz sample rate). Sentences were phonetically transcribed stimuli from the Texas Instruments/Massachusetts Institute of Technology (TIMIT) database (2–4 s, 16 kHz) <xref ref-type="bibr" rid="pbio.1001251-Garofolo1">[52]</xref>. Stimuli were presented aurally at the patient's bedside using either external free-field loudspeakers or calibrated ear inserts (Etymotic ER-5A) at approximately 70–80 dB.</p>
        <p>The spectrogram representation (linear model) was generated from the speech waveform using a 128 channel auditory filter bank mimicking the auditory periphery <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>,<xref ref-type="bibr" rid="pbio.1001251-David2">[53]</xref>. Filters had logarithmically spaced center frequencies ranging from 180–7,000 Hz and bandwidth of approximately 1/12<sup>th</sup> octave. The spectrogram was subsequently downsampled to 32 frequency channels.</p>
        <p>The modulation representation (nonlinear model) was obtained by a 2-D complex wavelet transform of the 128 channel auditory spectrogram <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>, implemented by a bank of causal modulation-selective filters spanning a range of spectral scales (0.5–8 cyc/oct) and temporal rates (1–32 Hz). The modulation selective filters are idealized spectro-temporal receptive fields similar to those measured in mammalian primary auditory cortex (<xref ref-type="fig" rid="pbio-1001251-g006">Figure 6</xref>). The filter bank output constitutes a complex-valued time-varying multi-dimensional speech representation (downsampled to 32 acoustic frequency×12 rate×5 scale = 1,920 total stimulus channels) <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. The modulation representation is obtained by taking the magnitude of this complex-valued output. In specific analyses (stated in the text), reduced modulation representations were used to reduce dimensionality and to achieve an acceptable computational load, as well as to verify that tuning estimates were not affected by regularization, given the large number of fitted parameters in the full model. Reduced modulation representations included (1) rate-scale (60 total channels) and (2) rate only (six total channels). The rate-scale representation was obtained by averaging along the irrelevant dimension (frequency) prior to the nonlinear magnitude operation. The rate only representation was obtained by filtering the spectrogram with pure temporal modulation filters (described in detail in Chi et al. <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>). Note that spectro-temporal filtering of the spectrogram is directional and captures upward and downward frequency sweeps, which by convention are denoted as positive and negative rates, respectively. Pure temporal filtering in the rate-only representation is not directional and results in half the total number of rate channels. These operations are described in detail in Chi et al. <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. <xref ref-type="supplementary-material" rid="pbio.1001251.s006">Figure S6</xref> summarizes the stimulus correlations present in the linear and nonlinear representations.</p>
      </sec>
      <sec id="s4c">
        <title>Stimulus Reconstruction</title>
        <p>The stimulus reconstruction model is the linear mapping between the responses at a set of electrodes and the original stimulus representation (e.g., modulation or spectrogram representation) <xref ref-type="bibr" rid="pbio.1001251-Bialek1">[12]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Mesgarani1">[14]</xref>. For a set of <italic>N</italic> electrodes, we represent the response of electrode <italic>n</italic> at time <italic>t</italic> = 1 … <italic>T</italic> as <italic>R</italic>(<italic>t</italic>, <italic>n</italic>). The reconstruction model, <italic>g</italic>(<italic>τ</italic>, <italic>f</italic>, <italic>n</italic>), is a function that maps <italic>R</italic>(<italic>t</italic>, <italic>n</italic>) to stimulus <italic>S</italic>(<italic>t</italic>, <italic>f</italic>) as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001251.e002" xlink:type="simple"/></inline-formula> denotes the estimated stimulus representation. Equation 1 implies that the reconstruction of each channel in the stimulus representation, <italic>S<sub>f</sub></italic>(<italic>t</italic>), from the neural population is independent of the other channels (estimated using a separate set of <italic>g<sub>f</sub></italic>(<italic>t</italic>, <italic>n</italic>)). If we consider the reconstruction of one such channel, it can be written as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.e003" xlink:type="simple"/><label>(2)</label></disp-formula>The entire reconstruction function is then described as the collection of functions for each stimulus feature:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.e004" xlink:type="simple"/><label>(3)</label></disp-formula>For the spectrogram, time-varying spectral energy in 32 individual frequency channels was reconstructed. For the modulation representation, unless otherwise stated we reconstructed the reduced rate-scale representation, which consists of time-varying modulation energy in 60 rate-scale channels (defined in Speech Stimuli). We used τ = 100 temporal lags, discretized at 10 ms.</p>
      </sec>
      <sec id="s4d">
        <title>Model Fitting</title>
        <p>Prior to model fitting, stimuli and neural response data were synchronized, downsampled to 100 Hz, and standardized to zero mean and unit standard deviation. Model parameters (<italic>G</italic> in Eqn. 3) were fit to a training set of stimulus-response data (ranging from 2.5–17.5 min for different participants) using coordinate gradient descent with early stopping regularization, an iterative linear regression algorithm <xref ref-type="bibr" rid="pbio.1001251-Wu1">[16]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Willmore1">[36]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Kay1">[49]</xref>. Each data set was divided into training (80%), validation (10%), and test sets (10%). Overfitting was minimized by monitoring prediction accuracy on the validation set and terminating the algorithm after a series of 50 iterations failed to improve performance (an indication that overfitting was beginning to occur). Reconstruction accuracy was then evaluated on the independent test set. Coordinate descent produces a sparse solution in the weight vector (i.e., most weight values set to zero) and essentially performs variable selection simultaneously with model fitting <xref ref-type="bibr" rid="pbio.1001251-Hastie1">[17]</xref>. Consequently, there is no requirement to preselect electrodes for the reconstruction model. For grid sizes studied here, inclusion of all electrodes in the reconstruction model can be advantageous because the algorithm encourages irrelevant parameters to maintain zero weight, while allowing the model to capture additional variance using electrodes potentially excluded by feature selection approaches. Equal numbers of parameters are used to estimate each stimulus channel in both linear and nonlinear models. For each stimulus channel, the number of parameters in the corresponding reconstruction filter is <italic>N</italic> electrodes×100 time lags (the number of electrodes for each participant was determined by clinical criteria and therefore <italic>N</italic> varied by participant).</p>
      </sec>
      <sec id="s4e">
        <title>Cross-Validation</title>
        <p>Parameter estimation was performed by a cross-validation procedure using repeated random subsampling <xref ref-type="bibr" rid="pbio.1001251-Rokach1">[54]</xref>, also referred to as Monte Carlo cross-validation <xref ref-type="bibr" rid="pbio.1001251-Boulesteix1">[55]</xref>. This has the advantage over k-fold cross-validation in that the proportion of train/test data is independent of the number of folds. Repeated random sub-sampling is similar to a bootstrap procedure (without replacement) that ensures there is no overlap between training and test data sets. For each repeat, trials were randomly partitioned into training (80% of trials), validation (10%), and test sets (10%); model fitting was then performed using the training/validation data; and reconstruction accuracy was evaluated on the test set. This procedure is repeated multiple times (depending on computational load) and the parameters and reconstruction accuracy measures were averaged over all repeats. The forward encoding models were estimated using 20 resamples; the spectrogram and modulation reconstruction models were estimated using 10 and 3 resamples, respectively (due to increasing computational load). Identical data partitions were used for comparing predictive power for different reconstruction models (i.e., spectrogram versus modulation) to ensure potential differences were not due to different stimuli or noise levels in the evaluation data. To check stability of the generalization error estimates, we verified that estimated spectrogram reconstruction accuracy was stable as a function of the number of resamples used in the estimation (ranging from 3 to 10). The total duration of the test set equaled the length of the concatenated resampled data sets (range of ∼0.8–17.5 min across participants). Standard error of individual parameters was calculated as the standard deviation of the resampled estimates <xref ref-type="bibr" rid="pbio.1001251-Hastie1">[17]</xref>. Statistical significance of individual parameters was assessed by the <italic>t</italic>-ratio (coefficient divided by its resampled standard error estimate). Model fitting was performed with the MATLAB toolbox STRFLab (<ext-link ext-link-type="uri" xlink:href="http://strflab.berkeley.edu" xlink:type="simple">http://strflab.berkeley.edu</ext-link>).</p>
      </sec>
      <sec id="s4f">
        <title>Reconstruction Accuracy</title>
        <p>Reconstruction accuracy was quantified separately for each stimulus component by computing the correlation coefficient (Pearson's <italic>r</italic>) between the reconstructed and original stimulus component. For each participant, this yielded 32 individual correlation coefficients for the 32 channel spectrogram model and 60 correlation coefficients for the 60 channel rate-scale modulation model (defined in Speech Stimuli). Overall reconstruction accuracy is reported as the mean correlation over all stimulus components.</p>
        <p>To make a direct comparison of modulation and spectrogram-based accuracy, the reconstructions need to be compared in the same stimulus space. The linear spectrogram reconstruction was therefore projected into the rate-scale modulation space (using the modulation filterbank as described in Speech Stimuli). This transformation provides an estimate of the modulation content of the spectrogram reconstruction and allows direct comparison with the modulation reconstruction. The transformed reconstruction was then correlated with the 60 rate-scale components of the original stimulus. Accuracy as a function of rate (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5A</xref>) was calculated by averaging over the scale dimension. Positive and negative rates were also averaged unless otherwise shown. Comparison of reconstruction accuracy for a subset of data in the full rate-scale-frequency modulation space yielded similar results. To impose additivity and approximate a normal sampling distribution of the correlation coefficient statistic, Fisher's <italic>z</italic>-transform was applied to correlation coefficients prior to tests of statistical significance and prior to averaging over stimulus channels and participants. The inverse z-transform was then applied for all reported mean <italic>r</italic> values.</p>
        <p>To visualize the modulation-based reconstruction in the spectrogram domain (<xref ref-type="fig" rid="pbio-1001251-g007">Figure 7B</xref>), the 4-D modulation representation needs to be inverted <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. If both magnitude and phase responses are available, the 2-D spectrogram can be restored by a linear inverse filtering operation <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. Here, only the magnitude response is reconstructed directly from neural activity. In this case, the spectrogram can be recovered approximately from the magnitude-only modulation representation using an iterative projection algorithm and an overcomplete set of modulation filters as described in Chi et al. <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. <xref ref-type="fig" rid="pbio-1001251-g007">Figure 7B</xref> displays the average of 100 random initializations of this algorithm. This approach is subject to non-neural errors due to the phase-retrieval problem (i.e., the algorithm does not perfectly recover the spectrogram, even when applied to the original stimulus) <xref ref-type="bibr" rid="pbio.1001251-Chi1">[18]</xref>. Therefore, quantitative comparisons with the spectrogram-based reconstruction were performed in the modulation space.</p>
        <p>Reconstruction accuracy was cross-validated and the reported correlation is the average over all resamples (see Cross-Validation) <xref ref-type="bibr" rid="pbio.1001251-David2">[53]</xref>. Standard error is computed as the standard deviation of the resampled distribution <xref ref-type="bibr" rid="pbio.1001251-Hastie1">[17]</xref>. The reported correlations are not corrected to account for the noise ceiling on prediction accuracy <xref ref-type="bibr" rid="pbio.1001251-Wu1">[16]</xref>, which limits the amount of potentially explainable variance. An ideal model would not achieve perfect prediction accuracy of <italic>r</italic> = 1.0 due to the presence of random noise that is unrelated to the stimulus. With repeated trials of identical stimuli, it is possible to estimate trial-to-trial variability to correct for the amount of potentially explainable variance <xref ref-type="bibr" rid="pbio.1001251-David3">[56]</xref>. In the experiments reported here, a sufficient number of trial repetitions (&gt;5) was generally unavailable for a robust estimate, and uncorrected values are therefore reported.</p>
      </sec>
      <sec id="s4g">
        <title>STRF Encoding Models</title>
        <p>Encoding models describe the linear mapping between the stimulus representation and the neural response at individual sites. For a stimulus representation <italic>s</italic>(<italic>x</italic>,<italic>t</italic>) and instantaneous neural response <italic>r</italic>(<italic>t</italic>) sampled at times <italic>t</italic> = 1 … <italic>T</italic>, the encoding model is defined as the linear mapping <xref ref-type="bibr" rid="pbio.1001251-Mesgarani1">[14]</xref>,<xref ref-type="bibr" rid="pbio.1001251-David3">[56]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.e005" xlink:type="simple"/><label>(4)</label></disp-formula>Each coefficient of <italic>h</italic> indicates the gain applied to stimulus feature <italic>x</italic> at time lag <italic>u</italic>. Positive values indicate components of the stimulus correlated with increased neural response, and negative values indicate components correlated with decreased response. The residual, <italic>e</italic>(<italic>t</italic>), represents components of the response (nonlinearities and noise) that cannot be predicted by the encoding model.</p>
        <p>Model fitting for the STRF models (<italic>h</italic> in Eqn. 4) proceeded similarly to reconstruction except a standard gradient descent algorithm (with early stopping regularization) was used that does not impose a sparse solution <xref ref-type="bibr" rid="pbio.1001251-Wu1">[16]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Willmore1">[36]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Kay1">[49]</xref>. The linear STRF model included 32 frequency channels×100 time lags (3,200 parameters). The full nonlinear modulation STRF model included 32 frequency×5 scale×12 rate×100 time lags (192,000 parameters) and the reduced rate-time modulation model (<xref ref-type="supplementary-material" rid="pbio.1001251.s004">Figure S4</xref>) included 6 rate×100 time lags (600 parameters). The STRF models were cross-validated using 20 resampled data sets with no overlap between training and test partitions within each resample. Data partitions were identical across STRF model type (linear and nonlinear). We did not enforce identical resampled data sets for estimating STRF and reconstruction models, because the predictive power of these two approaches is not comparable. Tuning curves were estimated from STRFs as follows: Frequency tuning was estimated from the linear STRF models by first setting all inhibitory weights to zero and then summing across the time dimension <xref ref-type="bibr" rid="pbio.1001251-David2">[53]</xref>. Nonlinear rate tuning was estimated from the nonlinear STRF modulation model by the same procedure, using the reduced rate-only representation. Linear rate tuning was estimated from the linear STRF model by filtering the fitted STRF with the modulation filterbank (see Speech Stimuli) and averaging along the irrelevant dimensions. Linear rate tuning computed in this way was similar to that computed from the modulation transfer function (modulus of the 2-D Fourier transform) of the fitted linear STRF <xref ref-type="bibr" rid="pbio.1001251-Miller1">[57]</xref>. For all tuning curves, standard error was computed as the standard deviation of the resampled estimates <xref ref-type="bibr" rid="pbio.1001251-Hastie1">[17]</xref>. Frequency tuning curve peaks were identified as significant parameters (<italic>t</italic>&gt;2.0) separated by more than a half octave. To calculate ensemble tuning curves (<xref ref-type="fig" rid="pbio-1001251-g005">Figure 5B</xref>), the tuning curve for each site was normalized by the maximum value and averaged across sites. STG sites with forward prediction accuracy of <italic>r</italic>&gt;0.1 were analyzed (<italic>n</italic> = 195).</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pbio.1001251.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Anatomical distribution of surface local field potential (LFP) responses and linear STRFs in a low density grid participant (10 mm electrode spacing). (A) Trial averaged spectral LFP responses to English sentences (2–4 s duration) at individual electrode sites. Consistent with previous intracranial language studies <xref ref-type="bibr" rid="pbio.1001251-Young1">[1]</xref>–<xref ref-type="bibr" rid="pbio.1001251-Rauschecker1">[5]</xref>, speech stimuli evoke increased high gamma power (∼70–150 Hz) sometimes accompanied by decreased power at lower frequencies (&lt;40 Hz) throughout sites in the temporal auditory cortex. Black outline indicates temporal cortex sites with high gamma responses (&gt;0.5 SD from baseline). (B) Example linear STRFs across all sites for one participant. All models are fit to power in the high gamma band range (70–150 Hz). (C) Anatomical location of subdural electrode grid (10 mm electrode spacing). Yellow outline indicates sites as in (A) and (B).</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p>Frequency tuning. (A) Left panels: linear STRFs for two example electrode sites. Right panels: pure tone frequency tuning (black curves) matches frequency tuning derived from fitted linear STRF models (red curves). For one participant, pure tones (375–6,000 Hz, logarithmically spaced) were presented for 100 ms at 80 dB. Pure tone tuning curves were calculated as the amplitudes of the induced high gamma response across tone frequencies. STRF-derived tuning curves were calculated by first setting all inhibitory weights to zero and then summing across the time dimension <xref ref-type="bibr" rid="pbio.1001251-Eggermont1">[6]</xref>. At these two sites, frequency tuning is approximately high-pass (top) or low-pass (bottom). (B) Distribution of the number of frequency tuning peaks across significant electrodes (<italic>N</italic> = 15 participants) estimated from linear STRF models (32-channel). The majority of sites exhibit complex frequency tuning patterns of 2–5 peaks. Peaks were identified as significant parameters (<italic>t</italic>&gt;2.0) separated by more than a half octave.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p>Mean reconstruction accuracy for the joint rate-scale space across all participants (<italic>N</italic> = 15). Top: modulation-based (nonlinear) decoding accuracy is significantly higher compared to frequency-based (linear) decoding (bottom) for all spectral scales at temporal rates ≥16 Hz (<italic>p</italic>&lt;0.05, post hoc pair-wise comparisons, Bonferroni correction, following significant two-way repeated measures ANOVA; model type by stimulus component interaction effect, <italic>F</italic>(59,826) = 1.84, <italic>p</italic>&lt;0.0005).</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p>Modulation rate tuning was estimated from both linear and nonlinear STRF models, based on the spectrogram or modulation representation, respectively. Linear STRFs have a 2-D parameter space (frequency×time). Modulation rate tuning for the linear STRF was computed by filtering the fitted STRF model with the modulation filterbank (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>) and averaging along the irrelevant dimensions. Modulation rate tuning computed in this way was similar to that computed from the modulation transfer function (MTF) (modulus of the 2-D Fourier transform of the fitted STRF <xref ref-type="bibr" rid="pbio.1001251-Shamma1">[7]</xref>). Nonlinear STRFs have a 4-D parameter space (rate×scale×frequency×time). Modulation-based rate tuning curves were computed by summing across the three irrelevant dimensions <xref ref-type="bibr" rid="pbio.1001251-Geschwind1">[8]</xref>. Modulation rate tuning was similar whether this procedure was applied to a reduced dimension model (rate×time only) or to the marginalized full model. Reported estimates of modulation rate tuning were computed from the reduced (rate×time) models. (A) Left: example linear STRF. The linear STRF can be transformed into rate-scale space (the MTF, right) by taking the modulus of the 2-D Fourier transform <xref ref-type="bibr" rid="pbio.1001251-Shamma1">[7]</xref> or by filtering the STRF with the modulation filter bank. The linear modulation rate tuning curve (blue curve, top) is obtained after averaging along the scale dimension. (B) Left: example nonlinear STRF from the same site as in (A), fit in the rate-time parameter space. Right: the corresponding modulation-based rate tuning curve (red) is plotted against the spectrogram-based tuning curve (blue) from (A) (only positive rates are shown).</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s005" xlink:type="simple">
        <label>Figure S5</label>
        <caption>
          <p>Confusion matrix for word identification (<xref ref-type="fig" rid="pbio-1001251-g008">Figure 8</xref>). Left: pair-wise similarities (correlation coefficient) between actual auditory spectrograms of each word pair. Right: pair-wise similarities between reconstructed and actual spectrograms of each word pair. Correlations were computed prior to any spectrogram smoothing.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s006" xlink:type="simple">
        <label>Figure S6</label>
        <caption>
          <p>Stimulus correlations in linear and nonlinear stimulus representations. Speech, like other natural sounds, has strong stimulus correlations (illustrated for acoustic frequency, top panels, and temporal modulation rate, bottom panels). Correlations were estimated from 1,000 randomly selected TIMIT sentences at different time lags (<italic>τ</italic> = 0, 50, 250 ms; note the temporal asymmetry due to the use of causal modulation filters). Under an efficient coding hypothesis <xref ref-type="bibr" rid="pbio.1001251-Recanzone1">[9]</xref>, these statistical redundancies may be exploited by the brain during sensory processing. In this study, we used an optimal linear estimator (Wiener filter) <xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>, which is essentially a multivariate linear regression and does not account for correlations among the output variables. Stimulus reconstruction therefore reflects an upper bound on the stimulus features that are encoded by the neural ensemble <xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>. As described in previous work <xref ref-type="bibr" rid="pbio.1001251-Steinschneider1">[10]</xref>,<xref ref-type="bibr" rid="pbio.1001251-Romanski1">[11]</xref>, the effect of stimulus statistics on reconstruction accuracy can be explored systematically using different stimulus priors.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s007" xlink:type="simple">
        <label>Figure S7</label>
        <caption>
          <p>Audio playback of reconstructed speech. The audio file contains a sequence of six isolated words that were reconstructed from single trial neural activity. Single trial reconstructions are generally not intelligible. However, coarse features such as syllable structure may be discerned. In addition, up and down frequency sweeps (corresponding to faster temporal rates) are more evident in the modulation reconstructions compared to the spectrogram reconstructions. Perceptual similarities between original and reconstructed words can be more easily recognized after first listening to the original sound. In the audio file, each word is presented as a sequence of the original sound heard by the participant, followed by the spectrogram (linear) reconstruction, followed by the modulation (nonlinear) reconstruction. The figure shows the spectrograms of the original and reconstructed words. For audio playback, the spectrogram or modulation representations must be converted to an acoustic waveform, a transformation that requires both magnitude and phase information. Because the reconstructed representations are magnitude-only, the phase must be estimated. In general, this is known as the phase retrieval problem <xref ref-type="bibr" rid="pbio.1001251-Geschwind1">[8]</xref>. To recover the acoustic waveform from the spectrogram, we used an iterative projection algorithm to estimate the phase <xref ref-type="bibr" rid="pbio.1001251-Geschwind1">[8]</xref>. This step introduces additional acoustic artifacts that can distort the auditory features reconstructed directly from neural responses. Consequently, the audio file is an accurate but not perfect reflection of the reconstructed speech representation. A similar algorithm can be used to recover the spectrogram from the modulation representation <xref ref-type="bibr" rid="pbio.1001251-Geschwind1">[8]</xref>. For the purposes of this demo, we instead projected the spectrogram reconstruction into the (complex) modulation domain, extracted the phase, and then combined the extracted phase with the reconstructed magnitude of the modulation representation. With both phase and magnitude information, an invertible transformation can then be used to convert the (complex) modulation representation back to the spectrogram <xref ref-type="bibr" rid="pbio.1001251-Geschwind1">[8]</xref>. Finally, to aid perceptual inspection of the reconstructions, the sample rate of the audio file is slightly slower (14 kHz) than that presented to participants (16 kHz).</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s008" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Supporting Information references.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio.1001251.s009" mimetype="audio/x-wav" position="float" xlink:href="info:doi/10.1371/journal.pbio.1001251.s009" xlink:type="simple">
        <label>Audio File S1</label>
        <caption>
          <p>Example audio of reconstructed speech.</p>
          <p>(WAV)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We are grateful to the individuals who participated in this experiment and to the laboratories of Nina Dronkers, Ralph Freeman, Frederic Theunissen, and Jack Gallant for technical assistance and discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pbio.1001251-Young1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Young</surname><given-names>E. D</given-names></name></person-group>             <year>2008</year>             <article-title>Neural representation of spectral and temporal information in speech.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>363</volume>             <fpage>923</fpage>             <lpage>945</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Joris1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Joris</surname><given-names>P. X</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>C. E</given-names></name><name name-style="western"><surname>Rees</surname><given-names>A</given-names></name></person-group>             <year>2004</year>             <article-title>Neural processing of amplitude-modulated sounds.</article-title>             <source>Physiol Rev</source>             <volume>84</volume>             <fpage>541</fpage>             <lpage>577</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Schreiner1">
        <label>3</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schreiner</surname><given-names>C. E</given-names></name><name name-style="western"><surname>Froemke</surname><given-names>R. C</given-names></name><name name-style="western"><surname>Atencio</surname><given-names>C. A</given-names></name></person-group>             <year>2011</year>             <article-title>Spectral processing in auditory cortex.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Winer</surname><given-names>J. A</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>C. E</given-names></name></person-group>             <source>The auditory cortex</source>             <publisher-name>Springer US</publisher-name>             <fpage>275</fpage>             <lpage>308</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Hickok1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hickok</surname><given-names>G</given-names></name><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name></person-group>             <year>2007</year>             <article-title>The cortical organization of speech processing.</article-title>             <source>Nat Rev Neurosci</source>             <volume>8</volume>             <fpage>393</fpage>             <lpage>402</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Rauschecker1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rauschecker</surname><given-names>J. P</given-names></name><name name-style="western"><surname>Scott</surname><given-names>S. K</given-names></name></person-group>             <year>2009</year>             <article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing.</article-title>             <source>Nat Neurosci</source>             <volume>12</volume>             <fpage>718</fpage>             <lpage>724</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Eggermont1">
        <label>6</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Eggermont</surname><given-names>J. J</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name></person-group>             <year>2011</year>             <article-title>Temporal coding in auditory cortex.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Winer</surname><given-names>J. A</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>C. E</given-names></name></person-group>             <source>The auditory cortex</source>             <publisher-name>Springer US</publisher-name>             <fpage>275</fpage>             <lpage>308</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Shamma1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shamma</surname><given-names>S. A</given-names></name></person-group>             <year>2003</year>             <article-title>Physiological foundations of temporal integration in the perception of speech.</article-title>             <source>Journal of Phonetics</source>             <volume>31</volume>             <fpage>279</fpage>             <lpage>287</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Geschwind1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Geschwind</surname><given-names>N</given-names></name></person-group>             <year>1965</year>             <article-title>Disconnexion syndromes in animals and man: Part I.</article-title>             <source>Brain</source>             <volume>88</volume>             <fpage>237</fpage>             <lpage>294</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Recanzone1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Recanzone</surname><given-names>G. H</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>Y. E</given-names></name></person-group>             <year>2010</year>             <article-title>Serial and parallel processing in the primate auditory cortex revisited.</article-title>             <source>Behavioural Brain Research</source>             <volume>206</volume>             <fpage>1</fpage>             <lpage>7</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Steinschneider1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Steinschneider</surname><given-names>M</given-names></name></person-group>             <year>2011</year>             <article-title>Unlocking the role of the superior temporal gyrus for speech sound categorization.</article-title>             <source>Journal of Neurophysiology</source>             <volume>105</volume>             <fpage>2631</fpage>             <lpage>2633</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Romanski1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Romanski</surname><given-names>L. M</given-names></name><name name-style="western"><surname>Averbeck</surname><given-names>B. B</given-names></name></person-group>             <year>2009</year>             <article-title>The primate cortical auditory system and neural representation of conspecific vocalizations.</article-title>             <source>Annu Rev Neurosci</source>             <volume>32</volume>             <fpage>315</fpage>             <lpage>346</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Bialek1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R. R</given-names></name><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name></person-group>             <year>1991</year>             <article-title>Reading a neural code.</article-title>             <source>Science</source>             <volume>252</volume>             <fpage>1854</fpage>             <lpage>1857</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Stanley1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stanley</surname><given-names>G. B</given-names></name><name name-style="western"><surname>Li</surname><given-names>F. F</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>1999</year>             <article-title>Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>8036</fpage>             <lpage>8042</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Mesgarani1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name><name name-style="western"><surname>David</surname><given-names>S. V</given-names></name><name name-style="western"><surname>Fritz</surname><given-names>J. B</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S. A</given-names></name></person-group>             <year>2009</year>             <article-title>Influence of context and behavior on stimulus reconstruction from neural activity in primary auditory cortex.</article-title>             <source>J Neurophysiol</source>             <volume>102</volume>             <fpage>3329</fpage>             <lpage>3339</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Ramirez1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ramirez</surname><given-names>A. D</given-names></name><name name-style="western"><surname>Ahmadian</surname><given-names>Y</given-names></name><name name-style="western"><surname>Schumacher</surname><given-names>J</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>D</given-names></name><name name-style="western"><surname>Woolley</surname><given-names>S. M</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Incorporating naturalistic correlation structure improves spectrogram reconstruction from neuronal activity in the songbird auditory midbrain.</article-title>             <source>J Neurosci</source>             <volume>31</volume>             <fpage>3828</fpage>             <lpage>3842</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Wu1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>M. C</given-names></name><name name-style="western"><surname>David</surname><given-names>S. V</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J. L</given-names></name></person-group>             <year>2006</year>             <article-title>Complete functional characterization of sensory neurons by system identification.</article-title>             <source>Annu Rev Neurosci</source>             <volume>29</volume>             <fpage>477</fpage>             <lpage>505</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Hastie1">
        <label>17</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <source>Elements of statistical learning</source>             <publisher-loc>New York, NY</publisher-loc>             <publisher-name>Springer Science</publisher-name>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Chi1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name><name name-style="western"><surname>Ru</surname><given-names>P</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S. A</given-names></name></person-group>             <year>2005</year>             <article-title>Multiresolution spectrotemporal analysis of complex sounds.</article-title>             <source>J Acoust Soc Am</source>             <volume>118</volume>             <fpage>887</fpage>             <lpage>906</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Chi2">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y</given-names></name><name name-style="western"><surname>Guyton</surname><given-names>M. C</given-names></name><name name-style="western"><surname>Ru</surname><given-names>P</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name></person-group>             <year>1999</year>             <article-title>Spectro-temporal modulation transfer functions and speech intelligibility.</article-title>             <source>J Acoust Soc Am</source>             <volume>106</volume>             <fpage>2719</fpage>             <lpage>2732</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Singh1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>N. C</given-names></name><name name-style="western"><surname>Theunissen</surname><given-names>F. E</given-names></name></person-group>             <year>2003</year>             <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing.</article-title>             <source>J Acoust Soc Am</source>             <volume>114</volume>             <fpage>3394</fpage>             <lpage>3411</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Elliott1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Elliott</surname><given-names>T. M</given-names></name><name name-style="western"><surname>Theunissen</surname><given-names>F. E</given-names></name></person-group>             <year>2009</year>             <article-title>The modulation transfer function for speech intelligibility.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000302</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000302" xlink:type="simple">10.1371/journal.pcbi.1000302</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Dau1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dau</surname><given-names>T</given-names></name><name name-style="western"><surname>Kollmeier</surname><given-names>B</given-names></name><name name-style="western"><surname>Kohlrausch</surname><given-names>A</given-names></name></person-group>             <year>1997</year>             <article-title>Modeling auditory processing of amplitude modulation. I. Detection and masking with narrow-band carriers.</article-title>             <source>J Acoust Soc Am</source>             <volume>102</volume>             <fpage>2892</fpage>             <lpage>2905</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Evans1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Evans</surname><given-names>A. C</given-names></name><name name-style="western"><surname>Collins</surname><given-names>D. L</given-names></name><name name-style="western"><surname>Mills</surname><given-names>S. R</given-names></name><name name-style="western"><surname>Brown</surname><given-names>E. D</given-names></name><name name-style="western"><surname>Kelly</surname><given-names>R. L</given-names></name></person-group>             <year>1993</year>             <fpage>1813</fpage>             <lpage>1817</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Crone1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Crone</surname><given-names>N. E</given-names></name><name name-style="western"><surname>Boatman</surname><given-names>D</given-names></name><name name-style="western"><surname>Gordon</surname><given-names>B</given-names></name><name name-style="western"><surname>Hao</surname><given-names>L</given-names></name></person-group>             <year>2001</year>             <article-title>Induced electrocorticographic gamma activity during auditory perception. Brazier Award-winning article, 2001.</article-title>             <source>Clin Neurophysiol</source>             <volume>112</volume>             <fpage>565</fpage>             <lpage>582</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Canolty1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Canolty</surname><given-names>R. T</given-names></name><name name-style="western"><surname>Soltani</surname><given-names>M</given-names></name><name name-style="western"><surname>Dalal</surname><given-names>S. S</given-names></name><name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name><name name-style="western"><surname>Dronkers</surname><given-names>N. F</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Spatiotemporal dynamics of word processing in the human brain.</article-title>             <source>Front Neurosci</source>             <volume>1</volume>             <fpage>185</fpage>             <lpage>196</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Nourski1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nourski</surname><given-names>K. V</given-names></name><name name-style="western"><surname>Reale</surname><given-names>R. A</given-names></name><name name-style="western"><surname>Oya</surname><given-names>H</given-names></name><name name-style="western"><surname>Kawasaki</surname><given-names>H</given-names></name><name name-style="western"><surname>Kovach</surname><given-names>C. K</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>Temporal envelope of time-compressed speech represented in the human auditory cortex.</article-title>             <source>J Neurosci</source>             <volume>29</volume>             <fpage>15564</fpage>             <lpage>15574</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Flinker1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Flinker</surname><given-names>A</given-names></name><name name-style="western"><surname>Chang</surname><given-names>E. F</given-names></name><name name-style="western"><surname>Kirsch</surname><given-names>H. E</given-names></name><name name-style="western"><surname>Barbaro</surname><given-names>N. M</given-names></name><name name-style="western"><surname>Crone</surname><given-names>N. E</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Single-trial speech suppression of auditory cortex activity in humans.</article-title>             <source>The Journal of Neuroscience</source>             <volume>30</volume>             <fpage>16643</fpage>             <lpage>16650</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Edwards1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name><name name-style="western"><surname>Nagarajan</surname><given-names>S. S</given-names></name><name name-style="western"><surname>Dalal</surname><given-names>S. S</given-names></name><name name-style="western"><surname>Canolty</surname><given-names>R. T</given-names></name><name name-style="western"><surname>Kirsch</surname><given-names>H. E</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Spatiotemporal imaging of cortical activation during verb generation and picture naming.</article-title>             <source>Neuroimage</source>             <volume>50</volume>             <fpage>291</fpage>             <lpage>301</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Pei1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pei</surname><given-names>X</given-names></name><name name-style="western"><surname>Leuthardt</surname><given-names>E. C</given-names></name><name name-style="western"><surname>Gaona</surname><given-names>C. M</given-names></name><name name-style="western"><surname>Brunner</surname><given-names>P</given-names></name><name name-style="western"><surname>Wolpaw</surname><given-names>J. R</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Spatiotemporal dynamics of electrocorticographic high gamma activity during overt and covert word repetition.</article-title>             <source>Neuroimage</source>             <volume>54</volume>             <fpage>2960</fpage>             <lpage>2972</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Logothetis1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name><name name-style="western"><surname>Wandell</surname><given-names>B. A</given-names></name></person-group>             <year>2004</year>             <article-title>Interpreting the BOLD signal.</article-title>             <source>Annu Rev Physiol</source>             <volume>66</volume>             <fpage>735</fpage>             <lpage>769</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-LiegeoisChauvel1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Liegeois-Chauvel</surname><given-names>C</given-names></name><name name-style="western"><surname>Lorenzi</surname><given-names>C</given-names></name><name name-style="western"><surname>Trebuchon</surname><given-names>A</given-names></name><name name-style="western"><surname>Regis</surname><given-names>J</given-names></name><name name-style="western"><surname>Chauvel</surname><given-names>P</given-names></name></person-group>             <year>2004</year>             <article-title>Temporal envelope processing in the human left and right auditory cortices.</article-title>             <source>Cereb Cortex</source>             <volume>14</volume>             <fpage>731</fpage>             <lpage>740</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Adelson1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adelson</surname><given-names>E. H</given-names></name><name name-style="western"><surname>Bergen</surname><given-names>J. R</given-names></name></person-group>             <year>1985</year>             <article-title>Spatiotemporal energy models for the perception of motion.</article-title>             <source>J Opt Soc Am A</source>             <volume>2</volume>             <fpage>284</fpage>             <lpage>299</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Kouh1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kouh</surname><given-names>M</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>             <year>2008</year>             <article-title>A canonical neural circuit for cortical nonlinear operations.</article-title>             <source>Neural Computation</source>             <volume>20</volume>             <fpage>1427</fpage>             <lpage>1451</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Hubel1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hubel</surname><given-names>D. H</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>T. N</given-names></name></person-group>             <year>1962</year>             <article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.</article-title>             <source>J Physiol</source>             <volume>160</volume>             <fpage>106</fpage>             <lpage>154</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-David1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>S. V</given-names></name><name name-style="western"><surname>Vinje</surname><given-names>W. E</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J. L</given-names></name></person-group>             <year>2004</year>             <article-title>Natural stimulus statistics alter the receptive field structure of v1 neurons.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>6991</fpage>             <lpage>7006</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Willmore1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Willmore</surname><given-names>B. D</given-names></name><name name-style="western"><surname>Prenger</surname><given-names>R. J</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J. L</given-names></name></person-group>             <year>2010</year>             <article-title>Neural representation of natural images in visual area V2.</article-title>             <source>J Neurosci</source>             <volume>30</volume>             <fpage>2102</fpage>             <lpage>2114</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Wang1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name><name name-style="western"><surname>Lu</surname><given-names>T</given-names></name><name name-style="western"><surname>Bendor</surname><given-names>D</given-names></name><name name-style="western"><surname>Bartlett</surname><given-names>E</given-names></name></person-group>             <year>2008</year>             <article-title>Neural coding of temporal information in auditory thalamus and cortex.</article-title>             <source>Neuroscience</source>             <volume>154</volume>             <fpage>294</fpage>             <lpage>303</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Lu1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>T</given-names></name><name name-style="western"><surname>Liang</surname><given-names>L</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name></person-group>             <year>2001</year>             <article-title>Temporal and rate representations of time-varying signals in the auditory cortex of awake primates.</article-title>             <source>Nat Neurosci</source>             <volume>4</volume>             <fpage>1131</fpage>             <lpage>1138</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Bendor1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bendor</surname><given-names>D</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name></person-group>             <year>2007</year>             <article-title>Differential neural coding of acoustic flutter within primate auditory cortex.</article-title>             <source>Nat Neurosci</source>             <volume>10</volume>             <fpage>763</fpage>             <lpage>771</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Rabiner1">
        <label>40</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rabiner</surname><given-names>L</given-names></name><name name-style="western"><surname>Juang</surname><given-names>B. H</given-names></name></person-group>             <year>1993</year>             <source>Fundamentals of speech recognition</source>             <publisher-loc>Englewood Cliffs, NJ</publisher-loc>             <publisher-name>Prentice-Hall, Inc</publisher-name>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Rieke1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>Bodnar</surname><given-names>D. A</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1995</year>             <article-title>Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents.</article-title>             <source>Proc Biol Sci</source>             <volume>262</volume>             <fpage>259</fpage>             <lpage>265</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Nourski2">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nourski</surname><given-names>K. V</given-names></name><name name-style="western"><surname>Brugge</surname><given-names>J. F</given-names></name></person-group>             <year>2011</year>             <article-title>Representation of temporal sound features in the human auditory cortex.</article-title>             <source>Reviews in the Neurosciences</source>             <volume>22</volume>             <fpage>187</fpage>             <lpage>203</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Greenberg1">
        <label>43</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name></person-group>             <year>2006</year>             <article-title>A multi-tier theoretical framework for understanding spoken language. In: Listening to speech.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name><name name-style="western"><surname>Ainsworth</surname><given-names>W. A</given-names></name></person-group>             <source>Listening to speech: an auditory perspective</source>             <publisher-loc>Mahwah, NJ</publisher-loc>             <publisher-name>Lawrence Erlbaum Associates</publisher-name>             <fpage>411</fpage>             <lpage>433</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Russ1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Russ</surname><given-names>B. E</given-names></name><name name-style="western"><surname>Ackelson</surname><given-names>A. L</given-names></name><name name-style="western"><surname>Baker</surname><given-names>A. E</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>Y. E</given-names></name></person-group>             <year>2008</year>             <article-title>Coding of auditory-stimulus identity in the auditory non-spatial processing stream.</article-title>             <source>Journal of Neurophysiology</source>             <volume>99</volume>             <fpage>87</fpage>             <lpage>95</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Formisano1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name><name name-style="western"><surname>De Martino</surname><given-names>F</given-names></name><name name-style="western"><surname>Bonte</surname><given-names>M</given-names></name><name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name></person-group>             <year>2008</year>             <article-title>“Who” is saying “what”? Brain-based decoding of human voice and speech.</article-title>             <source>Science</source>             <volume>322</volume>             <fpage>970</fpage>             <lpage>973</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Chang1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>E. F</given-names></name><name name-style="western"><surname>Rieger</surname><given-names>J. W</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>K</given-names></name><name name-style="western"><surname>Berger</surname><given-names>M. S</given-names></name><name name-style="western"><surname>Barbaro</surname><given-names>N. M</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Categorical speech representation in human superior temporal gyrus.</article-title>             <source>Nat Neurosci</source>             <volume>13</volume>             <fpage>1428</fpage>             <lpage>1432</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Tsunada1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tsunada</surname><given-names>J</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J. H</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>Y. E</given-names></name></person-group>             <year>2011</year>             <article-title>Representation of speech categories in the primate auditory cortex.</article-title>             <source>Journal of Neurophysiology</source>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Mitchell1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mitchell</surname><given-names>T. M</given-names></name><name name-style="western"><surname>Shinkareva</surname><given-names>S. V</given-names></name><name name-style="western"><surname>Carlson</surname><given-names>A</given-names></name><name name-style="western"><surname>Chang</surname><given-names>K. M</given-names></name><name name-style="western"><surname>Malave</surname><given-names>V. L</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Predicting human brain activity associated with the meanings of nouns.</article-title>             <source>Science</source>             <volume>320</volume>             <fpage>1191</fpage>             <lpage>1195</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Kay1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kay</surname><given-names>K. N</given-names></name><name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name><name name-style="western"><surname>Prenger</surname><given-names>R. J</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J. L</given-names></name></person-group>             <year>2008</year>             <article-title>Identifying natural images from human brain activity.</article-title>             <source>Nature</source>             <volume>452</volume>             <fpage>352</fpage>             <lpage>355</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Naselaris1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name><name name-style="western"><surname>Prenger</surname><given-names>R. J</given-names></name><name name-style="western"><surname>Kay</surname><given-names>K. N</given-names></name><name name-style="western"><surname>Oliver</surname><given-names>M</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J. L</given-names></name></person-group>             <year>2009</year>             <article-title>Bayesian reconstruction of natural images from human brain activity.</article-title>             <source>Neuron</source>             <volume>63</volume>             <fpage>902</fpage>             <lpage>915</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Dalal1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dalal</surname><given-names>S. S</given-names></name><name name-style="western"><surname>Guggisberg</surname><given-names>A. G</given-names></name><name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name><name name-style="western"><surname>Sekihara</surname><given-names>K</given-names></name><name name-style="western"><surname>Findlay</surname><given-names>A. M</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Spatial localization of cortical time-frequency dynamics.</article-title>             <source>Conf Proc IEEE Eng Med Biol Soc</source>             <volume>2007</volume>             <fpage>4941</fpage>             <lpage>4944</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Garofolo1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Garofolo</surname><given-names>J. S</given-names></name><name name-style="western"><surname>Lamel</surname><given-names>L. F</given-names></name><name name-style="western"><surname>Fisher</surname><given-names>W. M</given-names></name><name name-style="western"><surname>Fiscus</surname><given-names>J. G</given-names></name><name name-style="western"><surname>Pallet</surname><given-names>D. S</given-names></name><etal/></person-group>             <year>1993</year>             <article-title>Acoustic-phonetic continuous speech corpus.</article-title>             <source>Linguistic Data Consortium</source>          </element-citation>
      </ref>
      <ref id="pbio.1001251-David2">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>S. V</given-names></name><name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S. A</given-names></name></person-group>             <year>2007</year>             <article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli.</article-title>             <source>Network</source>             <volume>18</volume>             <fpage>191</fpage>             <lpage>212</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Rokach1">
        <label>54</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rokach</surname><given-names>L</given-names></name></person-group>             <year>2010</year>             <source>Pattern classification using ensemble method</source>             <publisher-loc>London</publisher-loc>             <publisher-name>World Scientific Pub Co Inc</publisher-name>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Boulesteix1">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Boulesteix</surname><given-names>A. L</given-names></name><name name-style="western"><surname>Strobl</surname><given-names>C</given-names></name><name name-style="western"><surname>Augustin</surname><given-names>T</given-names></name><name name-style="western"><surname>Daumer</surname><given-names>M</given-names></name></person-group>             <year>2008</year>             <article-title>Evaluating microarray-based classifiers: an overview.</article-title>             <source>Cancer Inform</source>             <volume>6</volume>             <fpage>77</fpage>             <lpage>97</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-David3">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>David</surname><given-names>S. V</given-names></name><name name-style="western"><surname>Gallant</surname><given-names>J. L</given-names></name></person-group>             <year>2005</year>             <article-title>Predicting neuronal responses during natural vision.</article-title>             <source>Network</source>             <volume>16</volume>             <fpage>239</fpage>             <lpage>260</lpage>          </element-citation>
      </ref>
      <ref id="pbio.1001251-Miller1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>L. M</given-names></name><name name-style="western"><surname>Escabi</surname><given-names>M. A</given-names></name><name name-style="western"><surname>Read</surname><given-names>H. L</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>C. E</given-names></name></person-group>             <year>2002</year>             <article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex.</article-title>             <source>J Neurophysiol</source>             <volume>87</volume>             <fpage>516</fpage>             <lpage>527</lpage>          </element-citation>
      </ref>
    </ref-list>
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>A1</term>
          <def>
            <p>primary auditory cortex</p>
          </def>
        </def-item>
        <def-item>
          <term>STG</term>
          <def>
            <p>superior temporal gyrus</p>
          </def>
        </def-item>
        <def-item>
          <term>STRF</term>
          <def>
            <p>spectro-temporal receptive field</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    
  </back>
</article>