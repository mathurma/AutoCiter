<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00122</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002716</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>A Model of Reward- and Effort-Based Optimal Decision Making and Motor Control</article-title><alt-title alt-title-type="running-head">Optimal Decision and Action</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Rigoux</surname>
            <given-names>Lionel</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Guigon</surname>
            <given-names>Emmanuel</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>UPMC Univ Paris 06, UMR 7222, ISIR, Paris, France</addr-line>
      </aff><aff id="aff2">
        <label>2</label>
        <addr-line>CNRS, UMR 7222, ISIR, Paris, France</addr-line>
      </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Diedrichsen</surname>
            <given-names>Jörn</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">
        <addr-line>University College London, United Kingdom</addr-line>
      </aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">emmanuel.guigon@gmail.com</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: LR EG. Performed the experiments: LR EG. Analyzed the data: LR EG. Contributed reagents/materials/analysis tools: LR EG. Wrote the paper: LR EG.</p>
        </fn>
      </author-notes><pub-date pub-type="collection">
        <month>10</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>4</day>
        <month>10</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>10</issue><elocation-id>e1002716</elocation-id><history>
        <date date-type="received">
          <day>20</day>
          <month>1</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>10</day>
          <month>8</month>
          <year>2012</year>
        </date>
      </history><permissions>
        
        <copyright-holder>Rigoux, Guigon</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions><abstract>
        <p>Costs (e.g. energetic expenditure) and benefits (e.g. food) are central determinants of behavior. In ecology and economics, they are combined to form a utility function which is maximized to guide choices. This principle is widely used in neuroscience as a normative model of decision and action, but current versions of this model fail to consider how decisions are actually converted into actions (i.e. the formation of trajectories). Here, we describe an approach where decision making and motor control are optimal, iterative processes derived from the maximization of the discounted, weighted difference between expected rewards and foreseeable motor efforts. The model accounts for decision making in cost/benefit situations, and detailed characteristics of control and goal tracking in realistic motor tasks. As a normative construction, the model is relevant to address the neural bases and pathological aspects of decision making and motor control.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Behavior is made of decisions and actions. The decisions are based on the costs and benefits of potential actions, and the chosen actions are executed through the proper control of body segments. The corresponding processes are generally considered in separate theories of decision making and motor control, which cannot explain how the actual costs and benefits of a chosen action can be consistent with the expected costs and benefits involved at the decision stage. Here, we propose an overarching optimal model of decision and motor control based on the maximization of a mixed function of costs and benefits. The model provides a unified account of decision in cost/benefit situations (e.g. choice between small reward/low effort and large reward/high effort options), and motor control in realistic motor tasks. The model appears suitable to advance our understanding of the neural bases and pathological aspects of decision making and motor control.</p>
      </abstract><funding-group>
        <funding-statement>We received no funding for this work.</funding-statement>
      </funding-group><counts>
        <page-count count="13"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Consider a simple living creature that needs to move in its environment to collect food for survival (foraging problem; <xref ref-type="bibr" rid="pcbi.1002716-Stephens1">[1]</xref>). For instance, it can have to choose between a small amount of food at a short distance and a larger amount at a longer distance <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref>. These two choices should not in general be equivalent as they differ by the proposed benefit (amount of food), the cost of time (temporal discounting of the benefit), and the cost of movement (energetic expenditure) <xref ref-type="bibr" rid="pcbi.1002716-Rudebeck1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Floresco1">[6]</xref>. To behave appropriately in its environment, our creature should be able to: 1. make decisions based on the estimated costs and benefits of actions; 2. translate selected actions into actual movements in a way which is consistent with the decision process, i.e. the criterion used <italic>a priori</italic> for decision should be backed up <italic>a posteriori</italic> by the measured costs and benefits of the selected action; 3. update its behavior at any time during the course of action as required by changes in the environment (e.g. removal or change in the position of food).</p>
      <p>Most theories of decision making and motor control do not account for these characteristics of behavior. The main reason for this is that decision and control are essentially blind to each other in the proposed frameworks <xref ref-type="bibr" rid="pcbi.1002716-Braun1">[7]</xref>. On the one hand, standard theories of decision making <xref ref-type="bibr" rid="pcbi.1002716-Kahneman1">[8]</xref> rely on value-based processes (e.g. maximization of expected benefit), and fail to integrate the cost of physical actions into decisions <xref ref-type="bibr" rid="pcbi.1002716-Prvost1">[9]</xref>. On the other hand, modern theories of motor control are cast in the framework of optimal control theory, and propose to elaborate motor commands using a cost-based process (e.g. minimization of effort), irrespective of the value of actions <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon1">[11]</xref>. An interesting exception is the model proposed by Trommershäuser et al. <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Trommershuser3">[14]</xref> which casts into a Bayesian framework the observation that at least one aspect of motor control (intrinsic motor variability) is optimally integrated into decision making processes.</p>
      <p>Here, we consider a normative approach to decision making and motor control derived from the theory of <italic>reinforcement learning</italic> (RL; <xref ref-type="bibr" rid="pcbi.1002716-Sutton1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Todorov2">[17]</xref>), i.e. goals are defined by spatially located time-discounted rewards, and decision making and motor control are optimal processes based on the maximization of utility, defined as the discounted difference between benefits (reward) and costs (of motor commands). The proposed mechanism concurrently provides a criterion for choice among multiple actions, and an optimal control policy for execution of the chosen action. We show that: 1. The model accounts for decision making in cost/benefit situations, and characteristics of control in realistic motor tasks; 2. Parameters that govern the model can explain the perviousness of these behaviors to motivational and task-related influences (precision, instructions, urgency). As a normative construction, the model can be considered as a prescription of what the nervous system should do <xref ref-type="bibr" rid="pcbi.1002716-Krding1">[18]</xref>, and is thus relevant to address and discuss the neural bases and pathological aspects of decision making and motor control. In particular, we focus on the role of dopamine (DA) whose implication in decision making, motor control and reward/effort processing has been repeatedly emphasized <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Floresco1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Mazzoni1">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Kurniawan1">[22]</xref>.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>The proposed model is a model for decision and action. It is based on an objective function representing a trade-off between expected benefits and foreseeable costs of potential actions (<xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1A</xref> and <xref ref-type="disp-formula" rid="pcbi.1002716.e005">Eq. 4</xref>; see <xref ref-type="sec" rid="s4"><bold>Materials and Methods</bold></xref>). Maximization of this function attributes a utility to each action, which can be used for a decision process, and generate a control policy to carry out the action (<xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref>). Our goal is two-fold. First, we show that the model accounts for decision making in cost/benefit situations, and control in realistic motor tasks. Second, we show that the model makes sense from a psychological and neural standpoint. As a preliminary, we describe parameters that are central to the functioning of the model.</p>
      <fig id="pcbi-1002716-g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Objective function and model architecture.</title>
          <p><bold>A</bold>. Objective function (<italic>thick</italic>) as a function of movement duration, built from the sum of a discounted reward term (<italic>thin</italic>) and a discounted effort term (<italic>dashed</italic>). Optimal duration is indicated by a vertical <italic>dotted</italic> line. <bold>B</bold>. Architecture of the infinite-horizontal optimal feedback controller. See <bold>Text</bold> for notations.</p>
        </caption>
        <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g001" xlink:type="simple"/>
      </fig>
      <sec id="s2a">
        <title>Nature of the parameters</title>
        <p>The model contains five parameters (<bold>x</bold><sup>*</sup>, <italic>r</italic>, ρ, ε, γ; <xref ref-type="disp-formula" rid="pcbi.1002716.e006">Eqs. 5</xref> and <xref ref-type="disp-formula" rid="pcbi.1002716.e007">6</xref>). Parameter <bold>x</bold><sup>*</sup> specifies the location of the goal to be pursued, and acts as a classic boundary condition for a control policy. Parameter <italic>r</italic> is a value attached to the goal that can correspond to a reward on an objective scale (e.g. amount of food, amount of money), or to any factor that modulates the pursuit and achievement of goals (e.g. interest, attractiveness, difficulty, …). For pure motor tasks in which there is no explicit reward, we will assume that <italic>r</italic> corresponds to one of these factors (see <xref ref-type="sec" rid="s3"><bold>Discussion</bold></xref>). <bold>x</bold><sup>*</sup> and <italic>r</italic> are parameters related to the specification of a task, and will be called <italic>task</italic> parameters.</p>
        <p>For the purpose of decision and action, a reward value needs to be translated into an internal currency which measures “how much a reward is rewarding” (parameter ρ). A subject may not attribute the same value to food if he is hungry or satiated, and the same value to money if he plays Monopoly or trades at the stock exchange. <italic>r</italic> and ρ are redundant in the sense that only their product matters (<xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref>), but we keep both of them because their meaning is different.</p>
        <p>Parameter ε is a scaling factor that expresses “how much an effort is effortful”. A subject may not attribute the same value to effort if he is rested or exhausted. ρ and ε are redundant in the sense that only their ratio matters (<xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref>), but we keep both of them because their meaning is different, and they can be regulated differently (e.g. level of wealth vs level of fatigue). In general, we consider variations in the ratio ρ/ε, that we call <italic>vigor</italic> factor in the following.</p>
        <p>Parameter γ is a discount factor on reward and effort. It is both a computational parameter that is necessary to the formulation of the model, and a factor related to the process by which delayed or far away reinforcers lose value <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Green1">[23]</xref>. Note that a decrease in γ corresponds to faster discount.</p>
        <p>In the following, ρ, ε, and γ are called <italic>internal</italic> parameters, to indicate that they are not directly specified by the external environment, but correspond to a subjective valuation of concrete influences in the body and the environment. These parameters are allowed to vary to explore their role in the model. To provide a neural interpretation of the model, we tentatively relate effects of these variations to identified physiological elements.</p>
        <p>We note that the principle of the model is independent of the values of the parameters, i.e. the decision process and the control policy are generic characteristics of the model.</p>
      </sec>
      <sec id="s2b">
        <title>Decision making in a cost/benefit situation</title>
        <p>The model provides a normative criterion for decision making when choices involve different costs and benefits. To explore this issue, we considered the simple situation depicted in <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2A:</xref> a small reward at a short distance (reference distance) and a larger reward at a variable distance (test distance). Distance is used here as a way to modulate the required effort level. Simulations were run with Object I in the absence of noise. As the test distance increased, the effort to obtain the larger reward increased, and the utility decreased (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2B</xref>). Beyond a given distance (<italic>indifference point</italic>), the utility became smaller than the reference utility. Thus the indifference point separated two regions corresponding to a preference for the large reward/high effort and the small reward/low effort. This result corresponds to a classic observation in cost/benefit choice tasks <xref ref-type="bibr" rid="pcbi.1002716-Rudebeck1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Floresco1">[6]</xref>.</p>
        <fig id="pcbi-1002716-g002" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Simulation of Stevens <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[<bold>3</bold>]</xref><bold>.</bold></title>
            <p><bold>A</bold>. Cost/benefit choice task between a reference option (small reward/short distance) and a test option (large reward/long distance). <bold>B</bold>. Utility vs distance. The dotted line indicates the utility for the reference option (<italic>r</italic> = 1, distance = .35 m). The solid line gives the utility for the test option (<italic>r</italic> = 3) for different distances (range .35–2.45 m). An arrow indicates the distance at which the preference changes. <xref ref-type="sec" rid="s2">Results</xref> obtained with Object I. Parameters: ρ/ε = 1, γ = 2. <bold>C</bold>. Vigor and discount factors for synthetic monkeys (<italic>black</italic>: marmosets; <italic>gray</italic>: tamarins) derived from <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref>. The figure was built in the following way. Mean <italic>m</italic> and standard deviation σ of displacement duration were obtained from <xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref> for each species and each amplitude. For each species, a random sample was drawn from the corresponding Gaussian distribution <italic>N</italic>(<italic>m</italic>,σ) for each amplitude, giving two durations. These two durations were used to identify a unique pair of parameters (vigor, discount). Each point corresponds to one pair. See Text for further explanation. <bold>D</bold>. Indifference points corresponding to the simulated monkeys shown in <bold>C</bold> (T = tamarin, M = marmoset). Bold bar is the median, hinges correspond to the first and third quartile (50% of the population), and whiskers to the first and ninth decile (90% of the population). <bold>E</bold>. Probability of choosing the large reward option according the test distance. Solid lines are the experimental data from Stevens <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref>. <italic>Dashed</italic> lines and <italic>shaded</italic> areas correspond respectively to the mean and the 95% confidence interval of the decision process derived from the simulated utilities and a soft-max rule. The temperature parameter was selected for each monkey to fit empirical data.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g002" xlink:type="simple"/>
        </fig>
        <p>The model further states that the same parameters underlie both decision and movement production. To test this idea, we modeled the experiment reported by Stevens et al. <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref> [referred as Stevens in the following], in which the behavior of two species of monkey (marmoset and tamarin) was assessed in the choice situation of <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2A</xref>. The monkeys had to choose between one reward at 35 cm, and three rewards at 35–245 cm (distances 1 to 7). Stevens reported the choice behavior of the monkeys (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2</xref> in Stevens) as well as the durations of chosen actions (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref> in Stevens). The modeling principle is the following. We consider that the behavior of a monkey is determined by two parameters: a vigor factor (ρ/ε) and a discount factor (γ). The question is: if we infer these parameters from the displacement duration of the monkey, can we explain its choice behavior? An important issue is the underlying determinant of amplitude/duration data (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref> in Stevens). There is strong experimental evidence for the existence of a linear relationship between distance and duration for locomotor displacements (<xref ref-type="bibr" rid="pcbi.1002716-Decety1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Kunz1">[27]</xref>; see also <xref ref-type="bibr" rid="pcbi.1002716-Mhlhoff1">[28]</xref> with fish). This observation suggests that two parameters could be sufficient to capture covariations between displacement amplitudes and durations.</p>
        <fig id="pcbi-1002716-g003" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Basic characteristics of motor control.</title>
            <p><bold>A</bold>. Trajectories for movements of different amplitudes (direction: 45 deg; 5, 10, 15, 20, 25, 30 cm). <bold>B</bold>. Trajectories for movements in different directions (10 cm). <bold>C</bold>. Amplitude/duration scaling law and velocity profiles (inset) for the movements in <bold>A</bold>. <bold>D</bold>. Direction/duration (<italic>plain line</italic>), direction/apparent inertia (<italic>dotted line</italic>; arbitrary unit; <xref ref-type="bibr" rid="pcbi.1002716-Gordon1">[31]</xref>). <xref ref-type="sec" rid="s2">Results</xref> obtained with Object IIIa. Initial arm position (deg): (75,75). Parameters: <italic>r</italic> = 40, ρ/ε = 1/300, γ = .5, σ<sub>SINs</sub> = .001, σ<sub>SDNm</sub> = 1.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g003" xlink:type="simple"/>
        </fig>
        <p>For Object I, we have an analytic formula for optimal movement duration <italic>T</italic><sup>*</sup>(<italic>A</italic>,<italic>r</italic>,ρ/ε,γ) as a function of movement amplitude (<italic>A</italic>), reward (<italic>r</italic>), vigor (ρ/ε) and discount (γ) (see <xref ref-type="sec" rid="s4"><bold>Materials and Methods</bold></xref>). From <xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref> in Stevens, we also obtained the duration of displacement <italic>T</italic> (mean±s.e.m of the individual mean performances across the population) for each species in two conditions: one reward (<italic>r<sub>1</sub></italic> = 1) located at <italic>A<sub>1</sub></italic> = 0.35 m (marmoset: <italic>T<sub>1</sub></italic> = .75±.061 s, tamarin: <italic>T<sub>1</sub></italic> = .66±.047 s), and three rewards (<italic>r<sub>2</sub></italic> = 3) at <italic>A<sub>2</sub></italic> = 2.45 m (marmoset: <italic>T<sub>2</sub></italic> = 1.84±.082 s, tamarin: <italic>T<sub>2</sub></italic> = 1.32±.050 s).</p>
        <p>We randomly drew pairs of movement duration (one for each condition) from a Gaussian distribution specified by the mean and sd ( = s.e.m×sqrt(<italic>N</italic>), <italic>N</italic> = 4) given above, thus generating for each species a set of synthetic monkeys (<italic>n</italic> = 100). For each sample monkey, we obtained a unique value of vigor and discount factors [two unknowns: ρ/ε and γ; two equations: <italic>T</italic><sub>1</sub> = <italic>T</italic><sup>*</sup>(<italic>A</italic><sub>1</sub>,<italic>r<sub>1</sub></italic>,ρ/ε,γ) and <italic>T</italic><sub>2</sub> = <italic>T</italic><sup>*</sup>(<italic>A</italic><sub>2</sub>,<italic>r<sub>2</sub></italic>,ρ/ε,γ)]. The corresponding parameters are shown in <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2C</xref>. The two synthetic species were clearly associated with distinct regions of the parameter space, the marmosets being more sensitive to effort than the tamarins. It should be noted that <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2C</xref> does not mean that there exists a redundancy between the two parameters: in fact, each point of the clouds corresponds to a different displacement behavior, i.e. different distance/duration relationships. The correlation between the parameters suggests a potential lack of specificity of the duration measurements for our method to parsimoniously characterize the populations. However, although it would be possible to tighten our predictions with more structured data (e.g. estimated parameters based on individual behavior), it is unnecessary to reveal a clear cut dissociation between the two species.</p>
        <p>Then we computed for each monkey (i.e. for each set of parameters shown in <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2C</xref>) the utility of the different options (1 reward/35 cm, 3 rewards/35–245 cm). The two sets of parameters produced different indifference points (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2D</xref>). Specifically, the majority of marmosets, in contrast with tamarins, showed an inversion in their preferences within the tested range of distances (&lt;2.45 m).</p>
        <p>To determine the choice behavior of the monkeys from option utilities, we calculated the probability to choose the large reward at the different distances vs the small reward at the shortest distance using a softmax rule<disp-formula id="pcbi.1002716.e001"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e001" xlink:type="simple"/></disp-formula>where <italic>J</italic><sub>∞</sub><sup>large</sup> and <italic>J</italic><sub>∞</sub><sup>small</sup> are the utilities for the large reward and small reward options, respectively, and β a temperature parameter which represents the degree of randomness of the action selection. It should be noted that the softmax transform is not a part of the model, but a way to translate utilities into choice proportions, using the natural principle that different option utilities should lead to a proportion near 1 (or 0), and equal option utilities to a proportion of 0.5. The parameter β, which had no qualitative effect on the predicted preferences, was selected for each monkey to fit the data from Stevens. The model quantitatively reproduced the empirical results in the decision task for the two monkey species (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2E</xref>). Some outliers exhibited a less characteristic behavior (whiskers in <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2D</xref>) due to some imprecisions in our estimation. However, these marginal profiles were very scarce, and did not undermine our general results (see confidence interval; <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2E</xref>).</p>
        <p>To assess more precisely the ability of the model to predict the choices, we performed a detailed analysis over the two sets of simulated utilities (not over choices, to rule out any confound induced by β). We found that distance to the large reward modulated the utility of the large reward for both species, and that: 1. for tamarins, the large reward option had a larger utility than the small reward option for all distances; 2. for marmosets, the large reward option had a larger utility than the small reward option only for test distances strictly smaller than 210 cm. These results exactly parallel the effects found by Stevens, and show that the model can quantitatively predict the inversion of preferences of the different species. This further supports the hypothesis that the same process governs decision making and action in a cost/benefit choice situation.</p>
      </sec>
      <sec id="s2c">
        <title>Control in realistic motor tasks</title>
        <p>The model reproduced basic characteristics of motor behavior, as expected from the close relationship with previous optimal control models <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon2">[30]</xref>. Simulations were run with Object IIIa (two-joint planar arm) in the absence of noise. The internal parameters (ρ/ε and γ) were chosen to obtain a range of velocities compatible with observations on arm movements, and were kept constant for simulations of motor control task (<xref ref-type="fig" rid="pcbi-1002716-g003">Figs. 3</xref>, <xref ref-type="fig" rid="pcbi-1002716-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002716-g005">5</xref>). Their values had no qualitative influence on the reported results. Movements of different amplitudes (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3A</xref>) and in different directions (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3B</xref>) were considered. Simulated trajectories were straight (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3A,B</xref>) with a bell-shaped velocity profile (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3C</xref>, inset). Movement duration emerged implicitly corresponding to the best compromise between discounted rewards and efforts. Accordingly, duration was a function of movement amplitude (amplitude/duration scaling law; <xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3C</xref>), and movement direction (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3D</xref>, <italic>plain line</italic>). In fact, the influence of direction was related the inertial anisotropy of the arm (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3D</xref>, <italic>dotted line</italic>). Scaling was also observed for peak velocity and peak acceleration (not shown). These results are consistent with experimental observations <xref ref-type="bibr" rid="pcbi.1002716-Gordon1">[31]</xref>.</p>
        <fig id="pcbi-1002716-g004" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Simulation of Liu and Todorov <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[<bold>29</bold>]</xref><bold>.</bold></title>
            <p> <bold>A</bold>. Simulated trajectories for reaching movements toward a target which jumps unexpectedly up or down, 100 ms, 200 ms or 300 ms after movement onset. <bold>B</bold>. Corresponding velocity profiles. <bold>C</bold>. Arrival time as a function of the timing of the perturbation. <xref ref-type="sec" rid="s2">Results</xref> obtained with Object IIIa. Initial arm position (deg): (15,120). Same parameters as in <xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref>.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g004" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002716-g005" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Simulation of Shadmehr and Mussa-Ivaldi <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[<bold>32</bold>]</xref>.</title>
            <p><bold>A</bold>. Velocity profiles for unperturbed movements in four directions. <bold>B</bold>. Hand trajectories during exposure to a velocity-dependent force field. <bold>C</bold>. Velocity profiles for perturbed movements in four directions (data from <bold>B</bold>). <xref ref-type="sec" rid="s2">Results</xref> obtained with Object IIIb. Initial arm position (deg): (15,100). Same parameters as in <xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref>.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g005" xlink:type="simple"/>
        </fig>
        <p>Unexpected events can perturb an ongoing action, and prevent a planned movement to reach its goal. Typical examples are sudden changes in target location <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref> or mechanical alteration of limbs dynamics <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>. In these experiments, participants correct their movements and proceed to the goal by smoothly modifying the kinematics of their arm and the duration of the action. In the model, movement duration is not fully specified in advance, but emerges from an online feedback process concerned only by the remaining effort necessary to get a reward. We wanted to test if this property could explain motor control when movement execution requires flexibility to deal with unforeseen perturbations.</p>
        <p>In the experiment of Liu and Todorov <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, the target location jumped unpredictably during the reach. This caused a lengthening of movement duration which increased with the time elapsed between movement onset and perturbation onset (perturbation time; <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1g</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>), and systematic modifications of trajectory (<xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1a</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>) and velocity profile (<xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1b</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>). We simulated this task with Object IIIa by changing the goal position (<bold>x</bold><sup>*</sup>) in the controller at different times (perturbation time+Δ, to account for delayed perception of the change). The parameters of the model were estimated from unperturbed trials. The model quantitatively reproduced trajectory formation (<xref ref-type="fig" rid="pcbi-1002716-g004">Fig. 4A</xref>; <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1a</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>), velocity profiles (<xref ref-type="fig" rid="pcbi-1002716-g004">Fig. 4B</xref>; <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1b</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>), and the effect of perturbation time on movement duration (<xref ref-type="fig" rid="pcbi-1002716-g004">Fig. 4C</xref>; <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1g</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>). Liu and Todorov <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref> have proposed an optimal feedback control model to explain their results. However, in their approach, the duration of perturbed movements was not an emergent property of the model, and they used experimentally measured durations in their simulations. Later in their article, they described a different model, including a cost of time, which was potentially able to predict the duration of perturbed movements, but this model was not used to explain their initial target jump data.</p>
        <p>In the experiment of Shadmehr and Mussa-Ivaldi <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>, participants performed reaching movements using a robotic device that exerted a force on their arm, i.e. altered the dynamic of their limb and continuously deflected the arm from its intended trajectory. Initial exposure to the perturbation induced deviations from straight line trajectories with typical hook-like final corrections (<xref ref-type="fig" rid="pcbi-1002716-g007">Fig. 7</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>), and multiple peak velocity profiles (Fig. 10 in <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>). We simulated this task with Object IIIb in the presence of a velocity-dependent force field. The controller was unaware of the presence of the force field. The parameters were those used in the preceding simulations (<xref ref-type="fig" rid="pcbi-1002716-g003">Figs. 3</xref> and <xref ref-type="fig" rid="pcbi-1002716-g004">4</xref>), and were appropriate to fit unperturbed trials. Unperturbed velocity profiles are shown for 4 directions in <xref ref-type="fig" rid="pcbi-1002716-g005">Fig. 5A</xref>. From the interplay between the naïve controller and the altered arm dynamics emerged curved trajectories with typical hooks (<xref ref-type="fig" rid="pcbi-1002716-g005">Fig. 5B</xref>), and multi-peaked velocity profiles (<xref ref-type="fig" rid="pcbi-1002716-g005">Fig. 5C</xref>), which are qualitatively similar to the experimental data.</p>
        <p>These results illustrate how a unique set of parameters, and thus a unique controller, explains both normal trajectory formation, and complex updating of motor commands and trajectories when participants face unexpected perturbations. The same mechanisms (optimality, feedback control, implicit determination of duration) underlie basic motor characteristics (scaling law), and flexible control and goal tracking in complex situations.</p>
      </sec>
      <sec id="s2d">
        <title>Modulation of decision making and motor control</title>
        <p>The model is governed by the vigor (ρ/ε) and discount (γ) factors that can modulate both the decision process and the control policy (<xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref>).</p>
        <p>Decision making in a cost/benefit situation (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2A</xref>) was characterized by a threshold that delineates choice preference between small reward/low effort and large reward/high effort options (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2B</xref>). We observed a shift of the decision criterion toward the small reward/low effort option for a decreased vigor (lower ρ/ε; <xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6A</xref>), or a steepened discount (lower γ; <xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6B</xref>). Interestingly, the shift was accompanied by a decreased velocity in the former case (<xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6C</xref>), and an increased velocity in the latter (<xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6D</xref>). Note that the parameters were different from those used in <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2</xref>, and were chosen here to obtain a range of velocities compatible with observations on arm movements. This choice had no influence on the results. This result is especially interesting since it reveals a dissociation between the influence of vigor and discount on decision making and motor control. The effects of vigor, but not discount, resemble the shift of decision criterion toward small reward/low effort options <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Floresco1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Salamone1">[20]</xref>, and the decrease in velocity <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref> observed in rat's behavior following systemic injection of dopamine receptor antagonists or DA depletion in the ventral striatum.</p>
        <fig id="pcbi-1002716-g006" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Influence of parameters.</title>
            <p><bold>A</bold>. Change in the distance/utility relationship induced by a decrease in vigor: ρ/ε from 50 (<italic>gray</italic>) to 16 (<italic>black</italic>). Same experiment as in <xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2A</xref>. Parameters: <italic>r</italic> = 1, γ = 2. <bold>B</bold>. Same as <bold>A</bold> for a decrease in the value of discount factor: γ from 4 (<italic>gray</italic>) to 1 (<italic>black</italic>). Parameters: <italic>r</italic> = 1, ρ/ε = 50. <bold>C</bold>. Change in movement duration corresponding to the results in <bold>A</bold>. <bold>D</bold>. Change in movement duration corresponding to the results in <bold>B</bold>. <xref ref-type="sec" rid="s2">Results</xref> obtained with Object I.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g006" xlink:type="simple"/>
        </fig>
        <p>Motor control was characterized by scaling laws (<xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3C</xref>). Each factor, by its variation, defined a family of amplitude/duration scaling laws. For instance, a decrease in vigor induced an upward shift of the scaling law (<xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6C</xref>). Consistent with the influence of vigor described above, this result could correspond to the widely reported preservation and shift of amplitude/duration (and amplitude/velocity) scaling laws across DA manipulations and basal ganglia lesions in animals <xref ref-type="bibr" rid="pcbi.1002716-Hikosaka1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Alamy1">[36]</xref>, and basal ganglia disorders in humans (bradykinesia; <xref ref-type="bibr" rid="pcbi.1002716-Georgiou1">[37]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Negrotti1">[39]</xref>). However, this interpretation is tentative as the shifts induced by vigor and discount were qualitatively similar (<xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6C,D</xref>; see <xref ref-type="sec" rid="s3"><bold>Discussion</bold></xref>).</p>
        <p>Along the scaling laws defined by each factor (<xref ref-type="fig" rid="pcbi-1002716-g006">Fig. 6C,D</xref>), amplitude, duration and variability varied in a concerted way that conformed to Fitts' law <xref ref-type="bibr" rid="pcbi.1002716-Fitts1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Bainbridge1">[41]</xref>, i.e. movement duration is a function of the index of difficulty (i.e. log<sub>2</sub>(2<italic>A</italic>/<italic>W</italic>), where <italic>A</italic> is the amplitude and <italic>W</italic> the endpoint variability; <xref ref-type="fig" rid="pcbi-1002716-g007">Fig. 7A</xref>). We note that the underlying pattern of spatiotemporal variability had two peaks, one around peak velocity and the other near the end of the movement (<xref ref-type="fig" rid="pcbi-1002716-g007">Fig. 7B</xref>), and is consistent with experimental observations (although the temporal profiles are usually cut before variability starts to return toward premovement levels; <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Osu1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Selen1">[43]</xref>). These results show that the vigor and discount factors can induce modulations of movement duration and scaling laws that might correspond to experimentally identified elements (see above and <xref ref-type="sec" rid="s3"><bold>Discussion</bold></xref>) while strictly obeying to a robust and ubiquitous law of motor control. Interestingly, for a given amplitude, any of these factors can act as an internal representation of a target size (<xref ref-type="fig" rid="pcbi-1002716-g007">Fig. 7C</xref>), i.e. it specifies a control policy that can instantaneously elaborate a movement of a given precision. It should be noted that there exist numerous models of Fitts' law in the literature <xref ref-type="bibr" rid="pcbi.1002716-Guigon2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Meyer1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Tanaka1">[46]</xref>. Our purpose here is not to propose a new model, but simply to check that Fitts' law can properly emerge from the proposed framework.</p>
        <fig id="pcbi-1002716-g007" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002716.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Fitts' law and variability.</title>
            <p><bold>A</bold>. Duration as a function of the index of difficulty (ID) for 3 distances (10, 20 and 30 cm) and different values of vigor and discount (see legend). <bold>B</bold>. Typical spatiotemporal variability (s.d. of position). <bold>C</bold>. Endpoint variability for different values of the discount factor. Color is for the level of vigor (legend in <bold>A</bold>). <xref ref-type="sec" rid="s2">Results</xref> obtained with Object II. Parameters: distance = 30 cm, <italic>r</italic> = 1, ρ/ε = 100, γ = 2, σ<sub>SINs</sub> = .001, σ<sub>SDNm</sub> = 1.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002716.g007" xlink:type="simple"/>
        </fig>
        <p>Overall, these results show that the internal parameters modulate decision making and motor control in a way that makes sense from a physiological and psychological point of view.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have presented a computational framework that describes decision making and motor control as an ecological problem. The problem was cast in the framework of reinforcement learning, and the solution formulated as an optimal decision process and an optimal control policy. The resulting model successfully addressed decision making in cost/benefit situations and control in realistic motor tasks.</p>
      <sec id="s3a">
        <title>Disclaimer</title>
        <p>The proposed model is not intended to be a general theory of decision making and motor control, which may not be feasible (e.g. <xref ref-type="bibr" rid="pcbi.1002716-Wu1">[47]</xref>), but a more modest theory for cost/benefit situations, i.e. specific situations in which expected benefits and foreseeable physical costs of potential actions have to be evaluated and balanced. Accordingly, the model is not concerned with classic issues of risk and uncertainty which have been thoroughly addressed in studies of Trommershäuser and colleagues <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Trommershuser3">[14]</xref>.</p>
      </sec>
      <sec id="s3b">
        <title>Previous models</title>
        <p>Our model is closely related to previous works in the field of decision making and motor control. The central idea derives from optimal feedback control theory <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, and continuous time reinforcement learning <xref ref-type="bibr" rid="pcbi.1002716-Doya1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Todorov2">[17]</xref>. Several modeling studies have proposed modified versions of the optimal control approach to explain movement duration and amplitude/duration scaling laws <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Hoff1">[48]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>. The common idea is to consider a compromise between a cost of time (which increases with movement duration), and a cost of action (which decreases with movement duration; <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Hoff1">[48]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>). In a different framework, Niv et al. <xref ref-type="bibr" rid="pcbi.1002716-Niv1">[51]</xref> proposed a compromise between a “cost of acting quickly” and a cost of “getting the reward belatedly”. In these studies, the two costs varied in opposite directions with time, and their sum had a minimum value corresponding to an optimal behavioral timing (movement duration or latency; e.g. <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1B</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Harris2">[49]</xref>). Our model exploits the same formal idea (our <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1A</xref>), but with two differences. First, the cost of time in the previous studies were chosen for specific, task-related purposes (e.g. minimize the loss of vision from image motion during a saccade in <xref ref-type="bibr" rid="pcbi.1002716-Harris2">[49]</xref>; minimize the time it takes to get a target on the fovea with a saccade in <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>; see below for a further discussion on the cost of time). In our model, the cost of time derives from a general normative criterion. Second, optimization in the previous models involved only cost terms. In these approaches (e.g. <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>), a larger reward leads to a larger cost of time, thus producing a faster movements but also a lower utility, which is problematic if one wants to account for rational choices between actions. Indeed, none of these formalisms proposed to formulate motor control as a decision making problem. In our model, the reward modulates a benefit term, i.e. a larger reward leads to a larger benefit. This latter approach may be more appropriate to address cost/benefit situations in behavioral studies <xref ref-type="bibr" rid="pcbi.1002716-Salamone2">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Kurniawan2">[53]</xref>, and the differential sensitivity of costs and benefits to pharmacological manipulations <xref ref-type="bibr" rid="pcbi.1002716-Salamone2">[52]</xref>.</p>
        <p>A series of study by Trommershäuser and colleagues <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Trommershuser3">[14]</xref> has explored the connection between decision making and motor control. These studies showed that human participants make optimal motor decisions (where to point in a spatial reward/penalty landscape) that take into account their intrinsic motor variability. The results suggest that at least one aspect of motor control (variability) is integrated into decision making processes (see also <xref ref-type="bibr" rid="pcbi.1002716-Battaglia1">[54]</xref>). Our study explores a different aspect of the interaction between decision making and motor control: the influence of motor costs. In the early publications of Trommershäuser and colleagues <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser2">[13]</xref>, a biomechanical cost was introduced, but was not actually used as it was assumed to be constant. The model described in <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser2">[13]</xref> is a model of decision making, which solves a spatial gain/loss trade-off at a motor planning level, but not a model of motor control as it does neither explain how movements are actually produced following a decision, nor how motor variability is estimated for a use in the decision process. Our model is primarily a model of motor control, which solves a temporal reward/effort trade-off at a motor control level, but disregards the issue of uncertainty. In this sense, our approach and that developed by Trommershäuser and colleagues <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser4">[55]</xref> are complementary, and both useful to disclose the relationships between decision making and motor control.</p>
        <p>A central and novel aspect of the model is the integration of motor control into the decision process. This idea was not exploited in previous models because movement duration was fixed <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Nagengast1">[56]</xref>. Our model is close to the model proposed by Dean et al. <xref ref-type="bibr" rid="pcbi.1002716-Dean1">[57]</xref> (see below), as both models involve a trade-off between a time-decaying (reward) quantity and a time-increasing (accuracy in <xref ref-type="bibr" rid="pcbi.1002716-Niv1">[51]</xref>, minus effort in our model) quantity. However, the time-increasing quantity in <xref ref-type="bibr" rid="pcbi.1002716-Dean1">[57]</xref> is derived from experimental data, and is not generated by the model, i.e. there is no normative account of the speed/accuracy relationship.</p>
        <p>The model was described here in its simplest form. In particular, decision making was considered as a deterministic process. The scope of the model could easily be extended to address stochastic paradigms as in previous models <xref ref-type="bibr" rid="pcbi.1002716-Trommershuser2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Nagengast1">[56]</xref>. Utility needs to be replaced by mean (expected) utility or possibly mean-variance combinations <xref ref-type="bibr" rid="pcbi.1002716-Braun1">[7]</xref>. Further extensions could involve subjective utilities. In fact, none of these modifications would alter the very principle of the model.</p>
      </sec>
      <sec id="s3c">
        <title>Decision making</title>
        <p>An analysis of behavior in terms of costs and benefits has long since been usual in behavioral ecology <xref ref-type="bibr" rid="pcbi.1002716-Stephens1">[1]</xref>, but has only recently been exploited in the study of choice behavior in the field of neuroscience <xref ref-type="bibr" rid="pcbi.1002716-Walton1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Salamone2">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Phillips1">[58]</xref>. There is now strong evidence that not only payoff but also cost in terms of time and physical effort are integrated in the valuation of actions during a decision process <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Floresco1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Salamone2">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Cos1">[59]</xref>. The model captures this view using an objective function in which a temporal cost is represented by a discount factor on the payoff (reward), and an effort cost by the integrated size of motor commands. The strength of this function is that it is not merely an aggregation of cost and benefit terms <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>, but it has a true normative and sequential dimension <xref ref-type="bibr" rid="pcbi.1002716-Doya1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Todorov2">[17]</xref> which gives a consistent account of decision making and motor control.</p>
        <p>A central observation in behavioral settings is that the calculation of cost involves a detailed knowledge of motor behavior <xref ref-type="bibr" rid="pcbi.1002716-Phillips1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Cos1">[59]</xref>. Experiments using parametric manipulations of costs (e.g. number of level presses) and benefits (e.g. food quantity) have shown that the choices are based on a rational ordering of actions (as measured by percentages of choice and latencies; <xref ref-type="bibr" rid="pcbi.1002716-Gan1">[21]</xref>). The model also accounts for this aspect as decision is based on an exact estimation of the actual effort of tested actions derived from a complete planning process.</p>
        <p>The study of Dean et al. <xref ref-type="bibr" rid="pcbi.1002716-Dean1">[57]</xref> provides indirect evidence for the proposed decision process. In this study, subjects performed rapid arm movements to hit a rewarded target. As the reward value decayed with time (a manipulation imposed by the experimenter) and movement accuracy improved with time (natural speed/accuracy relationship), the subjects had to choose a movement duration corresponding to a trade-off between reward and accuracy (see <xref ref-type="fig" rid="pcbi-1002716-g003">Fig. 3</xref> in <xref ref-type="bibr" rid="pcbi.1002716-Dean1">[57]</xref>). The process described in <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1A</xref> is similar, but exploits the control cost (effort) rather than the movement accuracy. This is not a critical difference since there exists an univocal relationship between effort and variability <xref ref-type="bibr" rid="pcbi.1002716-Guigon2">[30]</xref>. Interestingly, Dean et al. <xref ref-type="bibr" rid="pcbi.1002716-Dean1">[57]</xref> observed that a majority of subjects behaved optimally in this task, i.e. chose movement durations that maximized their expected gains. These results indicate that our hypothesized optimal decision process is a feasible operation for the brain.</p>
      </sec>
      <sec id="s3d">
        <title>Motor control</title>
        <p>A central property of the model is motor control, i.e. the formation of trajectories for redundant biomechanical systems. This property is inherited from a close proximity with previous models based on optimal feedback control <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon2">[30]</xref>. A main novelty of this approach is to define a motor goal as a rewarded state rather than as a spatiotemporal constraint. Accordingly, movement duration is not a parameter, but an emerging characteristic of the interaction between a control policy, a controlled object, and unexpected events (noise, perturbations). The control policy makes no difference between a <italic>normal</italic> and a <italic>perturbed</italic> state, and always elaborates commands according to the same principle. This means that a perturbation requires neither an artificial updating of movement duration <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, nor a dual control process for early (anticipatory feedforward), and late (impedance-based) motor commands <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Bhushan1">[60]</xref>.</p>
      </sec>
      <sec id="s3e">
        <title>Interpreting the role of parameters</title>
        <p>The model is governed by task and internal parameters that specify choices in cost/benefit situations, and kinematics and precision in motor tasks. These parameters have a psychological and neural dimension that we discuss below.</p>
        <p>Parameter <italic>r</italic> reflects the well-documented influence of reward magnitude on decision making and intensity of action <xref ref-type="bibr" rid="pcbi.1002716-Crespi1">[61]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Roesch1">[64]</xref>. Although the observed effects are primarily mediated by physical objects (e.g. food), they can occur in the absence of reward <xref ref-type="bibr" rid="pcbi.1002716-Aarts1">[65]</xref>, and are influenced by numerous elements. Experimental manipulations of DA transmission have been shown to bias decision making in cost/benefit situations <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Floresco1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Kurniawan2">[53]</xref>, and alter movement intensity <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Choi1">[66]</xref>. The model offers two interpretations of these observations and of the role of DA in decision making and action, based on parameters ρ and ε (change in the perceived value of rewards or efforts). As ρ and ε have a symmetrical role, the model cannot help to decide between these interpretations. Recent studies tend to favor a relationship between effort and dopamine <xref ref-type="bibr" rid="pcbi.1002716-Mazzoni1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Salamone1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Kurniawan1">[22]</xref>. A link between ε and DA would provide a normative explanation of the strong sensitivity to response costs with preserved primary motivation for rewards following reduction of DA function <xref ref-type="bibr" rid="pcbi.1002716-Salamone1">[20]</xref>. Yet, the situation is probably more complex since dopamine is also involved in the valuation of reward in the absence of effort <xref ref-type="bibr" rid="pcbi.1002716-Gan1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Nicola1">[67]</xref>. Overall our results suggest that ρ and ε, through the vigor factor ρ/ε, are related to the modulation of motivational influences. Niv et al. <xref ref-type="bibr" rid="pcbi.1002716-Niv1">[51]</xref> proposed the very similar idea that tonic dopamine modulates the effort to invest in a (free operant) behavior. In contrast with our work, they focused on the rate of responding irrespective of the content of the actions, i.e. motor production. The two models are grounded on the same theoretical framework, and could complementarily help to explain the dual role of dopamine in motor behavior (e.g. vigor, time discounting) and foraging behavior (e.g. rate of reward, opportunity costs).</p>
        <p>Parameter γ has two dimensions. On the one hand, it is a <italic>computational</italic> parameter that is central to the infinite-horizon formulation of optimal control <xref ref-type="bibr" rid="pcbi.1002716-Todorov2">[17]</xref>. On the other hand, it is a psychological parameter which is widely used in behavioral ecology and economics to represent the process by which delayed reinforcers lose value <xref ref-type="bibr" rid="pcbi.1002716-Green1">[23]</xref>. What is the status of γ in the model? Two aspects need to be elucidated. First, are three parameters (ρ, ε, γ) necessary to control movement duration? Second, is γ similar to a discount factor in behavioral economics? The first question could amount to show that γ is related to nonmotivational influences. Many elements affect movement duration, such as task instructions (e.g. move accurately; <xref ref-type="bibr" rid="pcbi.1002716-Brown2">[68]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Hefter1">[69]</xref>), task difficulty <xref ref-type="bibr" rid="pcbi.1002716-Montagnini1">[70]</xref>, and task conditions (e.g. externally-triggered movements are faster than internally-triggered movements; <xref ref-type="bibr" rid="pcbi.1002716-Majsak1">[71]</xref>–<xref ref-type="bibr" rid="pcbi.1002716-Welchman1">[73]</xref>). Although it might seem clear that motivational influences are not involved in these cases, it is not easy to prove it explicitly. In this framework, the latter contrast between externally- and internally-triggered movements is especially interesting. On the one hand, this contrast is similar in normal subjects and Parkinsonian patients, both on- and off-medication <xref ref-type="bibr" rid="pcbi.1002716-Majsak1">[71]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Ballanger1">[72]</xref>. On the other hand, Parkinsonian patients fail to properly translate motivation into action <xref ref-type="bibr" rid="pcbi.1002716-Mazzoni1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Schmidt1">[74]</xref>. The extreme case of apathetic patients is particularly revealing as they are insensitive to incentives <xref ref-type="bibr" rid="pcbi.1002716-Schmidt1">[74]</xref> while having “relatively spared externally-driven responses” <xref ref-type="bibr" rid="pcbi.1002716-Lvy1">[75]</xref>. This dichotomy is likely related to the specific implication of DA transmission in internally-generated actions <xref ref-type="bibr" rid="pcbi.1002716-Jahanshahi1">[76]</xref>. Overall these results indicate that action can be modulated by influences which are independent of dopamine and motivation. The discount factor γ could mediate one of these influences.</p>
        <p>The second question is related to the relationship between delay discounting and velocity. The study of Stevens et al. <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref> is relevant to this issue. They compared the behavior of monkeys on an intertemporal choice task (a small food reward available immediately vs a delayed larger reward) and a spatial discounting choice task (a small, close reward vs a larger, more distant reward). They found that marmosets preferred larger delayed rewards in the former task, and closer, smaller rewards in the latter task. Thus their patience to wait to obtain a reward was not predictive of their will to travel farther away and for a longer time to get a larger reward. Furthermore, their travel time to the reward was not determined by their temporal discounting factor. These results indicate that decision for action is not directly governed by a discounting of time. This view is supported by neuroanatomical and neuropharmacological dissociations between effort and delay discounting in rats <xref ref-type="bibr" rid="pcbi.1002716-Denk1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-GhodsSharifi1">[77]</xref>. Accordingly, the cost of time as used in the present model and in previous models <xref ref-type="bibr" rid="pcbi.1002716-Hoff1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Harris2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Niv1">[51]</xref>, seems unlike a classic temporal discounting factor, and could be specific to cost/benefit situations and motor control. This issue questions the uniqueness of time discounting across situations <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr2">[50]</xref>. At odds with classical economics theories, it highlights the potential complexity and pervasiveness of the neural processes underlying computation of the cost of time <xref ref-type="bibr" rid="pcbi.1002716-Schweighofer1">[78]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Peters1">[79]</xref>.</p>
        <p>The model was applied to pure motor tasks in which there was no explicit reward <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>. Yet, although these tasks do not apparently correspond to cost-benefit situations, there is strong experimental evidence that their execution can be modulated by cost- and benefit-related factors, e.g. loads <xref ref-type="bibr" rid="pcbi.1002716-Bock1">[80]</xref>, fatigue <xref ref-type="bibr" rid="pcbi.1002716-Corcos1">[81]</xref>, task difficulty <xref ref-type="bibr" rid="pcbi.1002716-Montagnini1">[70]</xref>, attractiveness <xref ref-type="bibr" rid="pcbi.1002716-XuWilson1">[82]</xref>. These observations suggest that pure motor tasks and reward-related motor tasks could share the same underlying representation.</p>
      </sec>
      <sec id="s3f">
        <title>Neural architecture</title>
        <p>The model is built on a classic control/estimation architecture (<xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1B</xref>), which has been thoroughly discussed in the literature <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr3">[83]</xref>. There is evidence that the control process is subserved by motor cortical regions <xref ref-type="bibr" rid="pcbi.1002716-Scott1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon3">[85]</xref>, and the estimation process by the cerebellum <xref ref-type="bibr" rid="pcbi.1002716-Miall1">[86]</xref>. A central component of the model is the translation of the task parameters into a duration, a process which involves an integration of the internal parameters to calibrate costs and benefits. As discussed above, the basal ganglia and dopamine should play a crucial role in this process. In this framework, the basal ganglia would render decision making and motor control pervious to fundamental behavioral attributes (e.g. motivation, emotion, …; <xref ref-type="bibr" rid="pcbi.1002716-Schmidt1">[74]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Pessiglione1">[87]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Schmidt2">[88]</xref>). This view is supported by studies which show that interruption of basal ganglia outputs leads to basically preserved functions <xref ref-type="bibr" rid="pcbi.1002716-Turner1">[89]</xref>, but deficits in behavioral modulation <xref ref-type="bibr" rid="pcbi.1002716-Schmidt1">[74]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Kao1">[90]</xref>.</p>
      </sec>
      <sec id="s3g">
        <title>Testing the theory</title>
        <p>A central proposal of the model is a common basis for decision and action. The only available data that quantitatively support this proposal are those of Stevens et al. <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref>, which describe both choices and displacement characteristics in a spatial discounting task (<xref ref-type="fig" rid="pcbi-1002716-g002">Fig. 2A</xref>). In fact, any cost/benefit decision task (e.g. T-maze; <xref ref-type="bibr" rid="pcbi.1002716-Salamone2">[52]</xref>) could be used to test the theory if data on displacement duration were available. As in <xref ref-type="bibr" rid="pcbi.1002716-Stevens1">[3]</xref>, there should be a univocal relationship between displacement characteristics and choice behavior. A failure to observe this relationship would falsify the model. This would in fact correspond to a self-contradictory behavior: the costs and benefits that are estimated at the time of the decision would not be equal to those effectively encountered (during and after the movement). It should be noted that this failure would not be of the same nature as that usually reported in the field of decision making (e.g. a deviation from the laws of probabilities).</p>
        <p>The preceding results involved locomotor patterns, but appropriate data for arm movements could be obtained using methods described in <xref ref-type="bibr" rid="pcbi.1002716-Cos1">[59]</xref>. In a different domain, the model suggests that movement intensity can be modulated by nonmotivational elements, represented by the discount factor γ. One element could be urgency <xref ref-type="bibr" rid="pcbi.1002716-Majsak1">[71]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Ballanger1">[72]</xref>. This issue could be tested in apathetic patients, who should show a preserved sensitivity to urgency despite a loss of sensitivity to incentives <xref ref-type="bibr" rid="pcbi.1002716-Schmidt1">[74]</xref>.</p>
      </sec>
    </sec>
    <sec id="s4" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <p>Our objective is to formulate a unified model of decision making and motor control. Classical normative approaches formalize decision making as a maximization process on a <italic>utility function</italic> <xref ref-type="bibr" rid="pcbi.1002716-Pratt1">[91]</xref>, and motor control as a minimization process on a <italic>cost function</italic> <xref ref-type="bibr" rid="pcbi.1002716-Todorov3">[92]</xref>. Our proposal is to build a global <italic>utility minus cost</italic> function (that we call again a utility function) that could govern choices and commands in a unitary way. The central issue is time, because costs in motor control are a function of time (i.e. slower movements are less costly than faster movements), as are rewards due to a discounting effect (i.e. late rewards are less valuable than immediate ones). This means that a rational choice between two actions should involve an evaluation of their durations. However, the duration of the chosen action is only a prospective duration, valid at a given time, based on the assumption that current conditions will not change until the end of movement. The actual duration of the action can differ from this prospective duration if unexpected perturbations are encountered during the course of its execution.</p>
      <p>We have arbitrarily chosen the notations of control theory (<italic>J</italic> for utility/cost function, <italic>u</italic> for control) rather than those of decision theory (<italic>U</italic> for utility function, <italic>a</italic> for action).</p>
      <p>The principles of the model are first explained on a simple, deterministic example. Then the complete, stochastic version is described. The model is cast in the framework of reinforcement learning although we only exploit the optimal planning/decision processes of RL, but not the learning processes. The rationale for this choice is the following. Formally, the model corresponds to an infinite-horizon optimal control problem <xref ref-type="bibr" rid="pcbi.1002716-Bertsekas1">[93]</xref>. This jargon is typically used in economics <xref ref-type="bibr" rid="pcbi.1002716-Kunkel1">[94]</xref>, but is much less familiar in the fields of motor control and decision making, which describe similar problems in the terminology of RL <xref ref-type="bibr" rid="pcbi.1002716-Sutton1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Doya1">[16]</xref>. Furthermore, the RL framework encompasses learning processes which could explain how the proposed operations are learned by the nervous system <xref ref-type="bibr" rid="pcbi.1002716-Simpkins1">[95]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Marin1">[96]</xref>.</p>
      <sec id="s4a">
        <title>A starting example</title>
        <p>We consider an inertial point (controlled object) described by its mass <italic>m</italic> and its state <bold>x</bold> = (<italic>p</italic>,<italic>v</italic>) (where <italic>p</italic> and <italic>v</italic> are the position and velocity of the object; <bold>bold</bold> is for vectors). This object can move along a line, actuated by a force generating system (e.g. a set of muscles). The force generating system is defined by a function <italic>h</italic> which translates a control vector <bold>u</bold> into muscular force (<xref ref-type="bibr" rid="pcbi.1002716-Zajac1">[97]</xref>; <italic>h</italic> needs not be specified for the moment). This is a simplistic case to address e.g. the control of unidimensional saccades or single joint movements <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Harris2">[49]</xref>. The dynamics of the point is given by the general equation<disp-formula id="pcbi.1002716.e002"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e002" xlink:type="simple"/><label>(1)</label></disp-formula>corresponding to<disp-formula id="pcbi.1002716.e003"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e003" xlink:type="simple"/><label>(2)</label></disp-formula>in the case of a single muscle. To control this object means finding a <italic>control policy</italic>, i.e. a function <bold>u</bold>(<italic>t</italic>) (<italic>t</italic>∈[<italic>t</italic><sub>0</sub>;<italic>t</italic><sub>f</sub>]) that can displace the point between given states in the duration <italic>t</italic><sub>f</sub>−<italic>t</italic><sub>0</sub>. In the framework of the optimal control theory, the control policy is derived from the constraint to minimize a cost function<disp-formula id="pcbi.1002716.e004"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e004" xlink:type="simple"/><label>(3)</label></disp-formula>for any time <italic>t</italic>∈[<italic>t</italic><sub>0</sub>;<italic>t</italic><sub>f</sub>], where <italic>L</italic> is a function which generally penalizes large controls (<italic>effort</italic>) and deviations from a goal state (<italic>error</italic>; see <xref ref-type="bibr" rid="pcbi.1002716-Todorov3">[92]</xref> for a review). This formulation is appropriate to solve the problem of motor control, i.e. the mastering of the dynamics of articulated mechanical systems <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, but does not directly apply to a foraging problem (as described in the <xref ref-type="sec" rid="s1"><bold>Introduction</bold></xref>) for at least two reasons. First, function <italic>L</italic> is not concerned with values in the environment, although this difficulty could be relieved by the addition of a value-related term. Second, and more fundamental, the objective function cannot be used to specify the duration of an action, or to attribute a value to an action independent of its duration. Thus <italic>J</italic> cannot be considered as a utility function for decision making among multiple actions.</p>
        <p>An alternative approach has been elaborated as an extension of RL in continuous time and space <xref ref-type="bibr" rid="pcbi.1002716-Doya1">[16]</xref>. In this case, an infinite-horizon formulation is used where the error/effort cost function is replaced by a time-discounted, reward/effort function (to be maximized in this case)<disp-formula id="pcbi.1002716.e005"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e005" xlink:type="simple"/><label>(4)</label></disp-formula>where <italic>R</italic> is a function which weights rewarding states positively and effort negatively, and γ a time constant for discounting reward and effort. As for <xref ref-type="disp-formula" rid="pcbi.1002716.e004">Eq. 3</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e005">Eq. 4</xref> gives a recipe to find an optimal control policy <bold>u</bold>(<italic>s</italic>) for <italic>s</italic>∈[<italic>t</italic>;∞]. For clarity, we use the symbol γ for the discount parameter as usually found in RL studies <xref ref-type="bibr" rid="pcbi.1002716-Sutton1">[15]</xref>. Yet the range for the discount factor is [0;1] for discrete RL, and [0;+∞[ for the continuous-time formulation used here (see <xref ref-type="bibr" rid="pcbi.1002716-Doya1">[16]</xref> for a correspondence between discrete and continuous RL). As in RL, a small value of γ corresponds to a large discounting effect.</p>
        <p>We consider the case of a simple <italic>reward minus effort</italic> function where there is a single reward of value <italic>r</italic> at state <bold>x</bold><sup>*</sup>, i.e.<disp-formula id="pcbi.1002716.e006"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e006" xlink:type="simple"/><label>(5)</label></disp-formula>where δ is the function which is 1 when <bold>x</bold>(<italic>s</italic>) = <bold>x</bold><sup>*</sup>, and 0 everywhere else, and ρ and ε are scaling factors for reward and effort, respectively (see <xref ref-type="sec" rid="s2"><bold>Results</bold></xref> for a complete description of the parameters). If the inertial point starts to move at time <italic>t</italic>, reaches the rewarded state at an unknown time <italic>T</italic>, and the reward is given for a single timestep, we can write from <xref ref-type="disp-formula" rid="pcbi.1002716.e005">Eq. 4</xref> and <xref ref-type="disp-formula" rid="pcbi.1002716.e006">Eq. 5</xref>, using the fact that <bold>u</bold>(<italic>s</italic>) = 0 for <italic>s</italic>&gt;<italic>T</italic> (the point stays indefinitely at the rewarded state)<disp-formula id="pcbi.1002716.e007"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e007" xlink:type="simple"/><label>(6)</label></disp-formula></p>
        <p>where the term ρ<italic>r</italic>e<sup>−<italic>T</italic>/γ</sup> is the discounted reward (this result comes from the fact that ∫ <italic>g</italic>(<italic>s</italic>)δ(<italic>s</italic>)d<italic>s</italic> = <italic>g</italic>(0) for any function <italic>g</italic>), and <italic>J</italic><bold><sub>u</sub></bold>(<bold>x</bold>(<italic>t</italic>)) is the motor cost<disp-formula id="pcbi.1002716.e008"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e008" xlink:type="simple"/><label>(7)</label></disp-formula>We have removed the term exp(<italic>t</italic>/γ) which has no influence on the maximization process. This point highlights the fact that the maximization process does not depend on current time <italic>t</italic>. For clarity, in the following, <italic>J</italic><sub>∞</sub> and <italic>J</italic><bold><sub>u</sub></bold> are considered as functions of the reward time <italic>T</italic>.</p>
        <p>The purpose of <xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref> is, as for <xref ref-type="disp-formula" rid="pcbi.1002716.e004">Eq. 3</xref>, to obtain an optimal control policy. Maximizing <italic>J</italic><sub>∞</sub> requires to find a time <italic>T</italic> and an optimal control policy <bold>u</bold>(<italic>s</italic>) for <italic>s</italic>∈[<italic>t</italic>;<italic>T</italic>] that provide the best compromise between the <italic>discounted reward</italic> (ρ<italic>r</italic>e<sup>−<italic>T</italic>/γ</sup>) and the <italic>effort</italic> (<italic>J</italic><bold><sub>u</sub></bold>). This point is illustrated in <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1A</xref>. Both the discounted reward and the effort (−<italic>J</italic><bold><sub>u</sub></bold> is depicted) decreases with <italic>T</italic> (i.e. a faster movement involves more effort, but leads to a less discounted reward while a slower movement takes less effort, but incurs a larger discount), and their difference takes a maximum value at a time <italic>T</italic><sup>*</sup> (<italic>optimal duration</italic>). For each <italic>T</italic>, the control policy is optimal, and is obtained by solving a classic <italic>finite-horizon</italic> optimal control problem with the boundary condition <bold>x</bold>(<italic>T</italic>) = <bold>x</bold><sup>*</sup> (<xref ref-type="bibr" rid="pcbi.1002716-Todorov4">[98]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon4">[99]</xref>; see below). We note that <italic>T</italic><sup>*</sup> may not exist in general, depending on the shape of the reward and effort terms (<xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1A</xref>). Yet, this situation was never encountered in the simulations. The search of an optimal duration can be viewed both as a decision-making process (decide what is the best movement duration <italic>T</italic><sup>*</sup> if it exists), and a control process (if <italic>T</italic><sup>*</sup> exists, act with the optimal control policy defined by <italic>T</italic><sup>*</sup>). In the following, the maximal value of <italic>J</italic><sub>∞</sub> (for <italic>T</italic> = <italic>T</italic><sup>*</sup>) will be called <italic>utility</italic>.</p>
        <p>This description in terms of duration should not hide the fact that duration is only an intermediate quantity in the maximization of the utility function, and direct computation of choices and commands is possible without explicit calculus of duration <xref ref-type="bibr" rid="pcbi.1002716-Simpkins1">[95]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Marin1">[96]</xref>.</p>
        <p>If there are multiple reward states in the environment, the utility defines a normative priority order among these states. A decision process which selects the action with the highest utility will choose the best possible cost/benefit compromise.</p>
        <p>The proposed objective function involves two elements that are central to a decision making process: the benefit and the cost associated with a choice. A third element is uncertainty on the outcome of a choice. In the case where uncertainty can be represented by a probability (risk), this element could be integrated in the decision process without substantial modification of the model. A solution is to weight the reward value by the probability, in order to obtain an “expected value”. Another solution is to consider that temporal discounting already contains a representation of risk <xref ref-type="bibr" rid="pcbi.1002716-Platt1">[100]</xref>.</p>
        <p>In summary, <xref ref-type="disp-formula" rid="pcbi.1002716.e005">equations (4)</xref> and (<xref ref-type="disp-formula" rid="pcbi.1002716.e006">5</xref>) are interesting for four reasons: 1. Movement duration emerges as a compromise between discounted reward and effort; 2. The objective function is a criterion for decision-making either between different movement durations, or between different courses of action if there are multiple goals in the environment; 3. The objective function subserves both decision and control, which makes them naturally consistent. The utility that governs a decision is exactly the one that is obtained following the execution of the selected action (in the absence of noise and perturbations); 4. The objective function does not depend explicitly on time, which leads to a stationary control policy <xref ref-type="bibr" rid="pcbi.1002716-Doya1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Todorov2">[17]</xref>.</p>
      </sec>
      <sec id="s4b">
        <title>General framework</title>
        <p>For any dynamics (<xref ref-type="disp-formula" rid="pcbi.1002716.e002">Eq. 1</xref>), the problem defined by <xref ref-type="disp-formula" rid="pcbi.1002716.e005">Eqs. 4</xref> and <xref ref-type="disp-formula" rid="pcbi.1002716.e006">5</xref> is a generic infinite-horizon optimal control problem that leads, for each initial state, to an optimal movement duration and an optimal control policy (see above). This policy is also an optimal feedback control policy for each estimated state derived from an optimal state estimator <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon4">[99]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Bryson1">[101]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Stengel1">[102]</xref>. Thus the current framework is appropriate to study online movement control in the presence of noise and uncertainty. The only difference with previous approaches based on optimal feedback control <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon4">[99]</xref> is that movement duration is not given <italic>a priori</italic>, but calculated at each time to maximize an objective function.</p>
        <p>The general control architecture is depicted in <xref ref-type="fig" rid="pcbi-1002716-g001">Fig. 1B</xref>. As it has been thoroughly described previously <xref ref-type="bibr" rid="pcbi.1002716-Guigon2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Todorov4">[98]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon4">[99]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon5">[103]</xref>, we only give here a rapid outline. The architecture contains: 1. A <italic>controlled object</italic> whose dynamics is described by <xref ref-type="disp-formula" rid="pcbi.1002716.e002">Eq. 1</xref>, and is corrupted by noise <bold>n</bold><sub>OBJ</sub>; 2. A <italic>controller</italic> defined as<disp-formula id="pcbi.1002716.e009"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e009" xlink:type="simple"/><label>(8)</label></disp-formula>which is an optimal feedback controller for <xref ref-type="disp-formula" rid="pcbi.1002716.e002">Eqs. 1</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e005">4</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e006">5</xref>, where <bold>x</bold><sup>∧</sup> is the state estimate (described below); 3. An <italic>optimal state estimator</italic> that combines commands and sensory feedback to obtain a state estimate <bold>x</bold><sup>∧</sup> according to<disp-formula id="pcbi.1002716.e010"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e010" xlink:type="simple"/><label>(9)</label></disp-formula>where <bold>K</bold> is the Kalman gain matrix [constructed to provide an optimal weighting between the output of the forward model (first term in the rhs of <xref ref-type="disp-formula" rid="pcbi.1002716.e010">Eq. 9</xref>), and the correction based on delayed sensory feedback (second term in the rhs of <xref ref-type="disp-formula" rid="pcbi.1002716.e010">Eq. 9</xref>)], <bold>H</bold> the observation matrix, <bold>y</bold>(<italic>t</italic>) = <bold>Hx</bold>(<italic>t</italic>−Δ)+<bold>n</bold><sub>OBS</sub> the observation vector corrupted by observation noise, and Δ the time delay in sensory feedback pathways. The observed states were the position and velocity of the controlled object.</p>
        <p>Object noise was a multiplicative (signal-dependent) noise with standard deviation σ<sub>SDNm</sub>, and observation noise was an additive (signal-independent) noise with standard deviation σ<sub>SINs</sub> <xref ref-type="bibr" rid="pcbi.1002716-Todorov4">[98]</xref>. The rationale for this choice is to consider the simplest noisy environment: 1. Signal-dependent noise on object dynamics is necessary for optimal feedback control to implement a minimum intervention principle <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon4">[99]</xref>; 2. Signal-independent noise on observation is the simplest form of noise on sensory feedback. We note that a stochastic formulation was necessary to the specification of the state estimator even though most simulations actually did not involve noise.</p>
      </sec>
      <sec id="s4c">
        <title>Simulations</title>
        <p>A simulation consisted in calculating the utility (maximal value of the objective function), and the timecourse of object state and controls for a given dynamics <italic>f</italic>, initial state, and parameters <bold>x</bold><sup>*</sup>, <italic>r</italic>, ρ, ε, γ, σ<sub>SINs</sub>, σ<sub>SDNm</sub>, Δ. The solution was calculated iteratively at discretized times (timestep η). At each time <italic>t</italic>, a control policy was obtained for the current state estimate <bold>x</bold><sup>∧</sup> (<xref ref-type="disp-formula" rid="pcbi.1002716.e009">Eq. 8</xref>). Two types of method were necessary. First, the integral term in the rhs of <xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref> (<xref ref-type="disp-formula" rid="pcbi.1002716.e008">Eq. 7</xref>) required to solve a finite-horizon optimal control problem. This problem was solved analytically in the linear case, and numerically in the nonlinear case (see below). Second, optimal movement duration was obtained from <xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref> using a golden section search method <xref ref-type="bibr" rid="pcbi.1002716-Press1">[104]</xref>. Then <xref ref-type="disp-formula" rid="pcbi.1002716.e002">Eqs. 1</xref> and <xref ref-type="disp-formula" rid="pcbi.1002716.e010">9</xref> were integrated between <italic>t</italic> and <italic>t</italic>+η for the selected control policy and current noise levels (σ<sub>SINs</sub>, σ<sub>SDNm</sub>) to obtain <bold>x</bold>(<italic>t</italic>+η) and <bold>x</bold><sup>∧</sup>(<italic>t</italic>+η). The duration of the simulation was set empirically to be long enough to guarantee that the movement was completely unfolded. Actual movement duration (and the corresponding endpoint) was determined from the velocity profile using a threshold (3 cm/s).</p>
        <p>Three types of object were considered, corresponding to different purposes. The rationale was to use the simplest object which is deemed sufficient for the intended demonstration. Object I was a unidimensional linear object similar to that described in the starting example. The force generating system was <italic>h</italic>(<italic>u</italic>) = <italic>u</italic>. This object was used for decision making in a cost/benefit situation. Object II was similar to Object I, but the force generating system was a single linear second-order filter force generator (time constant τ), i.e. the dynamics was<disp-formula id="pcbi.1002716.e011"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e011" xlink:type="simple"/><label>(10)</label></disp-formula>where <italic>a</italic> and <italic>e</italic> are muscle activation and excitation, respectively, and g = 1 a conversion factor from activation to force. The filtering process is a minimalist analog of a muscle input/output function <xref ref-type="bibr" rid="pcbi.1002716-vanderHelm1">[105]</xref>. This object was used to study motor control in the presence of noise (relationship between amplitude, duration, and variability) <xref ref-type="bibr" rid="pcbi.1002716-Todorov1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Guigon2">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002716-Harris1">[45]</xref>. In this case, variability was calculated as the 95% confidence interval of endpoint distribution over repeated trials (<italic>N</italic> = 200). Object III (IIIa and IIIb) was a classic two-joint planar arm (shoulder/elbow) actuated by two pairs of antagonist muscles. The muscles were described as nonlinear second-order filter force generators. All the details are found below. This object was used to assess characteristics of motor control in realistic motor tasks.</p>
      </sec>
      <sec id="s4d">
        <title>Parameters</title>
        <p>For Objects I and II, the mass <italic>m</italic> was arbitrarily chosen to be 1 kg (no influence on the reported results). For Object III, the biomechanical parameters are given below. Other fixed parameters were: τ = 0.04 s, Δ = 0.13 s, η = 0.001 s. Noise parameters (σ<sub>SINs</sub>, σ<sub>SDNm</sub>) were chosen to obtain an appropriate functioning of the Kalman filter, and a realistic level of variability. The remaining parameters (<bold>x</bold><sup>*</sup>, <italic>r</italic>, ρ, ε, γ) are “true” parameters that are varied to explore the model (see <xref ref-type="sec" rid="s2"><bold>Results</bold></xref>).</p>
      </sec>
      <sec id="s4e">
        <title>Model of the two-joint planar arm</title>
        <p>Object III is a two-joint (shoulder, elbow) planar arm. Its dynamics is given by<disp-formula id="pcbi.1002716.e012"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e012" xlink:type="simple"/></disp-formula>where θ = (θ<sub>1</sub>,θ<sub>2</sub>) is the vector of joint angles, <bold>M</bold> the inertia matrix, <bold>C</bold> the matrix of velocity-dependent forces, <bold>W</bold> an optional velocity-dependent force field matrix, and <bold>T</bold>(<italic>t</italic>) the vector of muscle torques defined by<disp-formula id="pcbi.1002716.e013"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e013" xlink:type="simple"/></disp-formula>where <bold>A</bold> is the matrix of moment arms, <bold>F</bold><sub>max</sub> the matrix of maximal muscular forces, and <bold>a</bold> the vector of muscular activations resulting from the application of a control signal <bold>u</bold>(<italic>t</italic>) (see <xref ref-type="disp-formula" rid="pcbi.1002716.e011">Eq. 10</xref>).</p>
        <p>For each segment (1: upper arm, 2: forearm), <italic>l</italic> is the length, <italic>I</italic> the inertia, <italic>m</italic> the mass, and <italic>c</italic> the distance to center of mass from the preceding joint. Matrix <bold>M</bold> is<disp-formula id="pcbi.1002716.e014"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e014" xlink:type="simple"/></disp-formula>with<disp-formula id="pcbi.1002716.e015"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e015" xlink:type="simple"/></disp-formula>Matrix <bold>C</bold> is<disp-formula id="pcbi.1002716.e016"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e016" xlink:type="simple"/></disp-formula>with<disp-formula id="pcbi.1002716.e017"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e017" xlink:type="simple"/></disp-formula>Matrix <bold>W</bold> is <bold>JDJ</bold><italic><sup>T</sup></italic>, where <bold>J</bold> is the Jacobian matrix of the arm, and <bold>D</bold> (Ns/m) is<disp-formula id="pcbi.1002716.e018"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e018" xlink:type="simple"/></disp-formula>Matrix <bold>F</bold><sub>max</sub> (N) is diag([700;382;572;449]). Matrix <bold>A</bold> (m) is<disp-formula id="pcbi.1002716.e019"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e019" xlink:type="simple"/></disp-formula></p>
        <p>Two sets of parameter values were used in the simulations. For Object IIIa, we used the values found in <xref ref-type="bibr" rid="pcbi.1002716-Liu1">[29]</xref> (in S.I.): <italic>l</italic><sub>1</sub> = .30, <italic>l</italic><sub>2</sub> = .33, <italic>I</italic><sub>1</sub> = .025, <italic>I</italic><sub>2</sub> = .045, <italic>m</italic><sub>1</sub> = 1.4, <italic>m</italic><sub>2</sub> = 1.1, <italic>c</italic><sub>1</sub> = .11, <italic>c</italic><sub>2</sub> = .16. For Object IIIb, we used the values given in <xref ref-type="bibr" rid="pcbi.1002716-Shadmehr1">[32]</xref>: <italic>l</italic><sub>1</sub> = .33, <italic>l</italic><sub>2</sub> = .34, <italic>I</italic><sub>1</sub> = .0141, <italic>I</italic><sub>2</sub> = .0188, <italic>m</italic><sub>1</sub> = 1.93, <italic>m</italic><sub>2</sub> = 1.52, <italic>c</italic><sub>1</sub> = .165, <italic>c</italic><sub>2</sub> = .19.</p>
      </sec>
      <sec id="s4f">
        <title>Resolution of the optimal control problem</title>
        <p>The problem is to find the sequence of control <bold>u</bold>(<italic>t</italic>) which optimizes the objective function <italic>J<sub>u</sub></italic>(<italic>T</italic>) (<xref ref-type="disp-formula" rid="pcbi.1002716.e008">Eq. 7</xref>), and conforms to the boundary conditions <bold>x</bold>(<italic>t</italic><sub>0</sub>) = <bold>x</bold><sub>0</sub> and <bold>x</bold>(<italic>T</italic>) = <bold>x</bold><sup>*</sup> for a given dynamic <italic>f</italic>. The general approach to solve this problem is based on variational calculus <xref ref-type="bibr" rid="pcbi.1002716-Kirk1">[106]</xref>. The first step is to construct the Hamiltonian function which combines the objective function and the dynamic thanks to the Lagrangian multipliers (or co-state) denoted by λ<disp-formula id="pcbi.1002716.e020"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e020" xlink:type="simple"/></disp-formula>The optimal control minimizes the Hamiltonian, a property known as the Pontryagin's minimum principle given formally by<disp-formula id="pcbi.1002716.e021"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e021" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pcbi.1002716.e022"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e022" xlink:type="simple"/><label>(12)</label></disp-formula><disp-formula id="pcbi.1002716.e023"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e023" xlink:type="simple"/><label>(13)</label></disp-formula><xref ref-type="disp-formula" rid="pcbi.1002716.e022">Equation (12)</xref>, widely used in economics, is slightly different from what is usually used in the motor control literature because of the discounting factor in the objective function. We will thereafter consider two methods to solve this set of differential equations depending on the complexity of the dynamics.</p>
        <sec id="s4f1">
          <title>Linear case</title>
          <p>If the dynamic <italic>f</italic> is linear, as for Objects I and II, the system of differential equations (<xref ref-type="disp-formula" rid="pcbi.1002716.e021">Eqs. 11</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e022">12</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e023">13</xref>) is also linear, and can be solved analytically. We rewrite the dynamics as<disp-formula id="pcbi.1002716.e024"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e024" xlink:type="simple"/></disp-formula>From <xref ref-type="disp-formula" rid="pcbi.1002716.e023">Eq. 13</xref>, we can reformulate the optimal control <bold>u</bold><sup>*</sup>(<italic>t</italic>) as<disp-formula id="pcbi.1002716.e025"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e025" xlink:type="simple"/></disp-formula>In order to find λ(<italic>t</italic>), we then replace <bold>u</bold>(<italic>t</italic>) by <bold>u</bold><sup>*</sup>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="pcbi.1002716.e021">Eqs. 11</xref> and <xref ref-type="disp-formula" rid="pcbi.1002716.e022">12</xref>, and get<disp-formula id="pcbi.1002716.e026"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e026" xlink:type="simple"/></disp-formula>where <bold>I</bold> is the identity matrix. The resolution of this system gives the optimal trajectory of the state and the co-state<disp-formula id="pcbi.1002716.e027"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002716.e027" xlink:type="simple"/></disp-formula>where Γ is the analytic solution to <xref ref-type="disp-formula" rid="pcbi.1002716.e001">Eq. 14</xref>, and <bold>C</bold> can be deduced from the boundary conditions <xref ref-type="bibr" rid="pcbi.1002716-Guigon4">[99]</xref>. Finally, we replace λ by λ<italic><sup>*</sup></italic> in <xref ref-type="disp-formula" rid="pcbi.1002716.e001">Eq. 14</xref> to get the value of the optimal control. From <xref ref-type="disp-formula" rid="pcbi.1002716.e007">Eq. 6</xref>, we obtain an analytic version of the utility, from which we can derive the optimal duration <italic>T</italic><sup>*</sup> analytically. Symbolic calculus was performed with Maxima (Maxima, a Computer Algebra System. Version 5.18.1 (2009) <ext-link ext-link-type="uri" xlink:href="http://maxima.sourceforge.net/" xlink:type="simple">http://maxima.sourceforge.net/</ext-link>).</p>
        </sec>
        <sec id="s4f2">
          <title>Nonlinear case</title>
          <p>When the dynamics is nonlinear (Object III), the set of differential equations (<xref ref-type="disp-formula" rid="pcbi.1002716.e021">Eqs. 11</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e022">12</xref>, <xref ref-type="disp-formula" rid="pcbi.1002716.e023">13</xref>) cannot be solved directly. However, the minimum of the Hamiltonian (and thus the optimal control) can be found through numerical methods using a gradient descent method. The detail of the existing algorithms is outside the scope of this article, and the reader is referred to <xref ref-type="bibr" rid="pcbi.1002716-Bryson1">[101]</xref>, and <xref ref-type="bibr" rid="pcbi.1002716-Kirk1">[106]</xref>.</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank O. Sigaud, A. Terekhov, P. Baraduc, and M. Desmurget for fruitful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002716-Stephens1">
        <label>1</label>
        <mixed-citation publication-type="other" xlink:type="simple">Stephens DW, Krebs JR (1986) Foraging Theory. Princeton, NJ: Princeton University Press. 262 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Denk1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Denk</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Jennings</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Sharp</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Rushworth</surname><given-names>MF</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Differential involvement of serotonin and dopamine systems in cost-benefit decisions about delay or effort</article-title>. <source>Psychopharmacology (Berl)</source> <volume>179</volume>: <fpage>587</fpage>–<lpage>596</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Stevens1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevens</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Rosati</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Ross</surname><given-names>KR</given-names></name>, <name name-style="western"><surname>Hauser</surname><given-names>MD</given-names></name> (<year>2005</year>) <article-title>Will travel for food: Spatial discounting in two new world monkeys</article-title>. <source>Curr Biol</source> <volume>15</volume>: <fpage>1855</fpage>–<lpage>1860</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Rudebeck1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rudebeck</surname><given-names>PH</given-names></name>, <name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Smyth</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Bannerman</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Rushworth</surname><given-names>MF</given-names></name> (<year>2006</year>) <article-title>Separate neural pathways process different decision costs</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>1161</fpage>–<lpage>1168</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Walton1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Kennerley</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Bannerman</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>PEM</given-names></name>, <name name-style="western"><surname>Rushworth</surname><given-names>MF</given-names></name> (<year>2006</year>) <article-title>Weighing up the benefits of work: Behavioral and neural analyses of effort-related decision making</article-title>. <source>Neural Netw</source> <volume>19</volume>: <fpage>1302</fpage>–<lpage>1314</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Floresco1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Floresco</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Tse</surname><given-names>MT</given-names></name>, <name name-style="western"><surname>Ghods-Sharifi</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Dopaminergic and glutamatergic regulation of effort- and delay-based decision making</article-title>. <source>Neuropsychopharmacology</source> <volume>33</volume>: <fpage>1966</fpage>–<lpage>1979</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Braun1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Braun</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Nagengast</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2011</year>) <article-title>Risk-sensitivity in sensorimotor control</article-title>. <source>Front Hum Neurosci</source> <volume>5</volume>: <fpage>1</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kahneman1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kahneman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tversky</surname><given-names>A</given-names></name> (<year>1979</year>) <article-title>Prospect theory: An analysis of decision under risk</article-title>. <source>Econometrica</source> <volume>47</volume>: <fpage>263</fpage>–<lpage>291</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Prvost1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prévost</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Météreau</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Cléry-Melin</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Dreher</surname><given-names>J-C</given-names></name> (<year>2010</year>) <article-title>Separate valuation subsystems for delay and effort decision costs</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>14080</fpage>–<lpage>14090</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Todorov1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorov</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name> (<year>2002</year>) <article-title>Optimal feedback control as a theory of motor coordination</article-title>. <source>Nat Neurosci</source> <volume>5</volume>: <fpage>1226</fpage>–<lpage>1235</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Guigon1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guigon</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Baraduc</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Desmurget</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Computational motor control: Redundancy and invariance</article-title>. <source>J Neurophysiol</source> <volume>97</volume>: <fpage>331</fpage>–<lpage>347</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Trommershuser1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trommershäuser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname><given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name> (<year>2003</year>) <article-title>Statistical decision theory and rapid, goal-directed movements</article-title>. <source>J Opt Soc Am A</source> <volume>20</volume>: <fpage>1419</fpage>–<lpage>1433</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Trommershuser2">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trommershäuser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname><given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name> (<year>2003</year>) <article-title>Statistical decision theory and trade-offs in motor response</article-title>. <source>Spat Vis</source> <volume>16</volume>: <fpage>255</fpage>–<lpage>275</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Trommershuser3">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trommershäuser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gepshtein</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Maloney</surname><given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Banks</surname><given-names>MS</given-names></name> (<year>2005</year>) <article-title>Optimal compensation for changes in task-relevant movement variability</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>7169</fpage>–<lpage>7178</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Sutton1">
        <label>15</label>
        <mixed-citation publication-type="other" xlink:type="simple">Sutton RS, Barto AG (1998) Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press. 322 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Doya1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2000</year>) <article-title>Reinforcement learning in continuous time and space</article-title>. <source>Neural Comput</source> <volume>12</volume>: <fpage>219</fpage>–<lpage>245</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Todorov2">
        <label>17</label>
        <mixed-citation publication-type="other" xlink:type="simple">Todorov E (2007) Optimal control theory. In: Doya K, Ishii S, Pouget A, Rao RPN, editors. Bayesian Brain: Probabilistic Approaches to Neural Coding. Cambridge, MA: MIT Press. pp. 269–298.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Krding1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Decision theory: What “should” the nervous system do?</article-title> <source>Science</source> <volume>318</volume>: <fpage>606</fpage>–<lpage>610</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Mazzoni1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mazzoni</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Hristova</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Krakauer</surname><given-names>JW</given-names></name> (<year>2007</year>) <article-title>Why don't we move faster? Parkinson's disease, movement vigor, and implicit motivation</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>7105</fpage>–<lpage>7116</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Salamone1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Correa</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Farrar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mingote</surname><given-names>SM</given-names></name> (<year>2007</year>) <article-title>Effort-related functions of nucleus accumbens dopamine and associated forebrain circuits</article-title>. <source>Psychopharmacology (Berl)</source> <volume>191</volume>: <fpage>461</fpage>–<lpage>482</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Gan1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gan</surname><given-names>JO</given-names></name>, <name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>PE</given-names></name> (<year>2010</year>) <article-title>Dissociable cost and benefit encoding of future rewards by mesolimbic dopamine</article-title>. <source>Nat Neurosci</source> <volume>13</volume>: <fpage>25</fpage>–<lpage>27</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kurniawan1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurniawan</surname><given-names>IT</given-names></name>, <name name-style="western"><surname>Guitart-Masip</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2011</year>) <article-title>Dopamine and effort-based decision making</article-title>. <source>Front Neurosci</source> <volume>5</volume>: <fpage>81</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Green1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Green</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Myerson</surname><given-names>J</given-names></name> (<year>1996</year>) <article-title>Exponential versus hyperbolic discounting of delayed outcomes: Risk and waiting times</article-title>. <source>Am Zool</source> <volume>36</volume>: <fpage>496</fpage>–<lpage>505</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Decety1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Decety</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Jeannerod</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Prablanc</surname><given-names>C</given-names></name> (<year>1989</year>) <article-title>The timing of mentally represented actions</article-title>. <source>Behav Brain Res</source> <volume>34</volume>: <fpage>35</fpage>–<lpage>42</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Bakker1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bakker</surname><given-names>M</given-names></name>, <name name-style="western"><surname>de Lange</surname><given-names>FP</given-names></name>, <name name-style="western"><surname>Stevens</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Toni</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Bloem</surname><given-names>BR</given-names></name> (<year>2007</year>) <article-title>Motor imagery of gait: A quantitative approach</article-title>. <source>Exp Brain Res</source> <volume>179</volume>: <fpage>497</fpage>–<lpage>504</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Hicheur1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hicheur</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Pham</surname><given-names>QC</given-names></name>, <name name-style="western"><surname>Arechavaleta</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Laumond</surname><given-names>J-P</given-names></name>, <name name-style="western"><surname>Berthoz</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>The formation of trajectories during goal-oriented locomotion in humans. I. A stereotyped behaviour</article-title>. <source>Eur J Neurosci</source> <volume>26</volume>: <fpage>2376</fpage>–<lpage>2390</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kunz1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kunz</surname><given-names>BR</given-names></name>, <name name-style="western"><surname>Creem-Regehr</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>WB</given-names></name> (<year>2009</year>) <article-title>Evidence for motor simulation in imagined locomotion</article-title>. <source>J Exp Psychol: Hum Percept Perform</source> <volume>35</volume>: <fpage>1458</fpage>–<lpage>1471</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Mhlhoff1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mühlhoff</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Stevens</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Reader</surname><given-names>SM</given-names></name> (<year>2011</year>) <article-title>Spatial discounting of food and social rewards in guppies (<italic>Poecilia reticulata</italic>)</article-title>. <source>Front Psychol</source> <volume>2</volume>: <fpage>68</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Liu1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Todorov</surname><given-names>E</given-names></name> (<year>2007</year>) <article-title>Evidence for the flexible sensorimotor strategies predicted by optimal feedback control</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>9354</fpage>–<lpage>9368</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Guigon2">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guigon</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Baraduc</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Desmurget</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Computational motor control: Feedback and accuracy</article-title>. <source>Eur J Neurosci</source> <volume>27</volume>: <fpage>1003</fpage>–<lpage>1016</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Gordon1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gordon</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ghilardi</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Ghez</surname><given-names>C</given-names></name> (<year>1994</year>) <article-title>Accuracy of planar reaching movements. II. Systematic extent errors resulting from inertial anisotropy</article-title>. <source>Exp Brain Res</source> <volume>99</volume>: <fpage>112</fpage>–<lpage>130</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Shadmehr1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name> (<year>1994</year>) <article-title>Adaptive representation of dynamics during learning a motor task</article-title>. <source>J Neurosci</source> <volume>14</volume>: <fpage>3208</fpage>–<lpage>3224</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Hikosaka1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Wurtz</surname><given-names>RH</given-names></name> (<year>1985</year>) <article-title>Modification of saccadic eye movements by GABA-related substances. I. Effect of muscimol and bicuculline in monkey superior colliculus</article-title>. <source>J Neurophysiol</source> <volume>53</volume>: <fpage>266</fpage>–<lpage>291</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Hikosaka2">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Wurtz</surname><given-names>RH</given-names></name> (<year>1985</year>) <article-title>Modification of saccadic eye movements by GABA-related substances. II. Effect of muscimol in monkey substantia nigra pars reticulata</article-title>. <source>J Neurophysiol</source> <volume>53</volume>: <fpage>292</fpage>–<lpage>308</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kato1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kato</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Miyashita</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Matsumura</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Usui</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>1995</year>) <article-title>Eye movements in monkeys with local dopamine depletion in the caudate nucleus. 1. Deficits in spontaneous saccades</article-title>. <source>J Neurosci</source> <volume>15</volume>: <fpage>912</fpage>–<lpage>927</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Alamy1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alamy</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pons</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gambarelli</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Trouche</surname><given-names>E</given-names></name> (<year>1996</year>) <article-title>A defective control of small amplitude movements in monkeys with globus pallidus lesions: An experimental study on one component of pallidal bradykinesia</article-title>. <source>Behav Brain Res</source> <volume>72</volume>: <fpage>57</fpage>–<lpage>62</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Georgiou1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Georgiou</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>JG</given-names></name>, <name name-style="western"><surname>Bradshaw</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Cunnington</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Chiu</surname><given-names>E</given-names></name> (<year>1997</year>) <article-title>Impairments of movement kinematics in patients with Huntington's disease: A comparison with and without a concurrent task</article-title>. <source>Mov Disorders</source> <volume>12</volume>: <fpage>386</fpage>–<lpage>396</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Robichaud1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robichaud</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Pfann</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Comella</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Corcos</surname><given-names>DM</given-names></name> (<year>2002</year>) <article-title>Effect of medication on EMG patterns in individuals with Parkinson's disease</article-title>. <source>Mov Disorders</source> <volume>17</volume>: <fpage>950</fpage>–<lpage>960</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Negrotti1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Negrotti</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Secchi</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gentilucci</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Effects of disease progression and L-dopa therapy on the control of reaching-grasping in Parkinson's disease</article-title>. <source>Neuropsychologia</source> <volume>43</volume>: <fpage>450</fpage>–<lpage>459</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Fitts1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitts</surname><given-names>PM</given-names></name> (<year>1954</year>) <article-title>The information capacity of the human motor system in controlling the amplitude of movement</article-title>. <source>J Exp Psychol</source> <volume>47</volume>: <fpage>381</fpage>–<lpage>391</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Bainbridge1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bainbridge</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sanders</surname><given-names>M</given-names></name> (<year>1972</year>) <article-title>The generality of Fitts's law</article-title>. <source>J Exp Psychol</source> <volume>96</volume>: <fpage>130</fpage>–<lpage>133</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Osu1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Osu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Kamimura</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Iwasaki</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Nakano</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Wada</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Optimal impedance control for task achievement in the presence of signal-dependent noise</article-title>. <source>J Neurophysiol</source> <volume>92</volume>: <fpage>1199</fpage>–<lpage>1215</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Selen1">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Selen</surname><given-names>LP</given-names></name>, <name name-style="western"><surname>Beek</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>van Dieen</surname><given-names>JH</given-names></name> (<year>2006</year>) <article-title>Impedance is modulated to meet accuracy demands during goal-directed arm movements</article-title>. <source>Exp Brain Res</source> <volume>172</volume>: <fpage>129</fpage>–<lpage>138</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Meyer1">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyer</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Abrams</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Kornblum</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wright</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>JEK</given-names></name> (<year>1988</year>) <article-title>Optimality in human motor performance: Ideal control of rapid aimed movement</article-title>. <source>Psychol Rev</source> <volume>95</volume>: <fpage>340</fpage>–<lpage>370</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Harris1">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>1998</year>) <article-title>Signal-dependent noise determines motor planning</article-title>. <source>Nature</source> <volume>394</volume>: <fpage>780</fpage>–<lpage>784</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Tanaka1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Krakauer</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Qian</surname><given-names>N</given-names></name> (<year>2006</year>) <article-title>An optimization principle for determining movement duration</article-title>. <source>J Neurophysiol</source> <volume>95</volume>: <fpage>3875</fpage>–<lpage>3886</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Wu1">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Delgado</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Maloney</surname><given-names>LT</given-names></name> (<year>2009</year>) <article-title>Economic decision-making compared with an equivalent motor task</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>106</volume>: <fpage>6088</fpage>–<lpage>6093</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Hoff1">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoff</surname><given-names>B</given-names></name> (<year>1994</year>) <article-title>A model of duration in normal and perturbed reaching movement</article-title>. <source>Biol Cybern</source> <volume>71</volume>: <fpage>481</fpage>–<lpage>488</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Harris2">
        <label>49</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2006</year>) <article-title>The main sequence of saccades optimizes speed-accuracy trade-off</article-title>. <source>Biol Cybern</source> <volume>95</volume>: <fpage>21</fpage>–<lpage>29</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Shadmehr2">
        <label>50</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Orban de Xivry</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Xu-Wilson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shih</surname><given-names>TY</given-names></name> (<year>2010</year>) <article-title>Temporal discounting of reward and the cost of time in motor control</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>10507</fpage>–<lpage>10516</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Niv1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Joel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2007</year>) <article-title>Tonic dopamine: Opportunity costs and the control of response vigor</article-title>. <source>Psychopharmacology (Berl)</source> <volume>191</volume>: <fpage>507</fpage>–<lpage>520</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Salamone2">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Cousins</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Bucher</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Anhedonia or anergia? Effects of haloperidol and nucleus accumbens dopamine depletion on instrumental response selection in a T-maze cost/benefit procedure</article-title>. <source>Behav Brain Res</source> <volume>65</volume>: <fpage>221</fpage>–<lpage>229</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kurniawan2">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurniawan</surname><given-names>IT</given-names></name>, <name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Talmi</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Yoshida</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Chater</surname><given-names>N</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Choosing to make an effort: The role of striatum in signaling physical effort of a chosen action</article-title>. <source>J Neurophysiol</source> <volume>104</volume>: <fpage>313</fpage>–<lpage>321</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Battaglia1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Battaglia</surname><given-names>PW</given-names></name>, <name name-style="western"><surname>Schrater</surname><given-names>PR</given-names></name> (<year>2007</year>) <article-title>Humans trade off viewing time and movement duration to improve visuomotor accuracy in a fast reaching task</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>6984</fpage>–<lpage>6994</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Trommershuser4">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trommershäuser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname><given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname><given-names>MS</given-names></name> (<year>2008</year>) <article-title>Decision making, movement planning and statistical decision theory</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>: <fpage>291</fpage>–<lpage>297</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Nagengast1">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nagengast</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Braun</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2010</year>) <article-title>Risk-sensitive optimal feedback control accounts for sensorimotor behavior under uncertainty</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000857</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Dean1">
        <label>57</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dean</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Maloney</surname><given-names>LT</given-names></name> (<year>2007</year>) <article-title>Trading off speed and accuracy in rapid, goal-directed movements</article-title>. <source>J Vis</source> <volume>7</volume>: <fpage>10</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Phillips1">
        <label>58</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Phillips</surname><given-names>PEM</given-names></name>, <name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Jhou</surname><given-names>TC</given-names></name> (<year>2007</year>) <article-title>Calculating utility: Preclinical evidence for cost-benefit analysis by mesolimbic dopamine</article-title>. <source>Psychopharmacology (Berl)</source> <volume>191</volume>: <fpage>483</fpage>–<lpage>495</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Cos1">
        <label>59</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cos</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Bélanger</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Cisek</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>The influence of predicted arm biomechanics on decision making</article-title>. <source>J Neurophysiol</source> <volume>105</volume>: <fpage>3022</fpage>–<lpage>3033</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Bhushan1">
        <label>60</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bhushan</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>1999</year>) <article-title>Computational nature of human adaptive control during learning of reaching movements in force fields</article-title>. <source>Biol Cybern</source> <volume>81</volume>: <fpage>39</fpage>–<lpage>60</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Crespi1">
        <label>61</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crespi</surname><given-names>LP</given-names></name> (<year>1942</year>) <article-title>Quantitative variation in incentive and performance in the white rat</article-title>. <source>Am J Psychol</source> <volume>55</volume>: <fpage>467</fpage>–<lpage>517</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Brown1">
        <label>62</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname><given-names>VJ</given-names></name>, <name name-style="western"><surname>Bowman</surname><given-names>EM</given-names></name> (<year>1995</year>) <article-title>Discriminative cues indicating reward magnitude continue to determine reaction time of rats following lesions of the nucleus accumbens</article-title>. <source>Eur J Neurosci</source> <volume>7</volume>: <fpage>2479</fpage>–<lpage>2485</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Watanabe1">
        <label>63</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watanabe</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Lauwereyns</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name> (<year>2003</year>) <article-title>Effects of motivational conflicts on visually elicited saccades in monkeys</article-title>. <source>Exp Brain Res</source> <volume>152</volume>: <fpage>361</fpage>–<lpage>367</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Roesch1">
        <label>64</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roesch</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>PL</given-names></name>, <name name-style="western"><surname>Mullins</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>Ventral striatal neurons encode the value of the chosen action in rats deciding between differently delayed or sized rewards</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>13365</fpage>–<lpage>13376</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Aarts1">
        <label>65</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aarts</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Custers</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Marien</surname><given-names>H</given-names></name> (<year>2008</year>) <article-title>Preparing and motivating behavior outside of awareness</article-title>. <source>Science</source> <volume>319</volume>: <fpage>1639</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Choi1">
        <label>66</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Choi</surname><given-names>WY</given-names></name>, <name name-style="western"><surname>Morvan</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Balsam</surname><given-names>PD</given-names></name>, <name name-style="western"><surname>Horvitz</surname><given-names>JC</given-names></name> (<year>2009</year>) <article-title>Dopamine D1 and D2 antagonist effects on response likelihood and duration</article-title>. <source>Behav Neurosci</source> <volume>123</volume>: <fpage>1279</fpage>–<lpage>1287</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Nicola1">
        <label>67</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nicola</surname><given-names>SM</given-names></name> (<year>2010</year>) <article-title>The flexible approach hypothesis: Unification of effort and cue-responding hypotheses for the role of nucleus accumbens dopamine in the activation of reward-seeking behavior</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>16585</fpage>–<lpage>16600</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Brown2">
        <label>68</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Hefter</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Mertens</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Freund</surname><given-names>HJ</given-names></name> (<year>1990</year>) <article-title>Disturbances in human arm trajectory due to mild cerebellar dysfunction</article-title>. <source>J Neurol Neurosurg Psychiatry</source> <volume>53</volume>: <fpage>306</fpage>–<lpage>313</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Hefter1">
        <label>69</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hefter</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Cooke</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Freund</surname><given-names>HJ</given-names></name> (<year>1996</year>) <article-title>Basal ganglia and cerebellar impairment differentially affect the amplitude and time scaling during the performance of forearm step tracking movements</article-title>. <source>Electromyogr Clin Neurophysiol</source> <volume>36</volume>: <fpage>121</fpage>–<lpage>128</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Montagnini1">
        <label>70</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montagnini</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chelazzi</surname><given-names>L</given-names></name> (<year>2005</year>) <article-title>The urgency to look: Prompt saccades to the benefit of perception</article-title>. <source>Vis Res</source> <volume>45</volume>: <fpage>3391</fpage>–<lpage>3401</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Majsak1">
        <label>71</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Majsak</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Kaminski</surname><given-names>TR</given-names></name>, <name name-style="western"><surname>Gentile</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Flanagan</surname><given-names>JR</given-names></name> (<year>1998</year>) <article-title>The reaching movements of patients with Parkinson's disease under self-determined maximal speed and visually cued conditions</article-title>. <source>Brain</source> <volume>121</volume>: <fpage>755</fpage>–<lpage>766</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Ballanger1">
        <label>72</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ballanger</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Thobois</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Baraduc</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Turner</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Broussolle</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>“Paradoxical kinesis” is not a hallmark of Parkinson's disease but a general property of the motor system</article-title>. <source>Mov Disorders</source> <volume>21</volume>: <fpage>1490</fpage>–<lpage>1495</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Welchman1">
        <label>73</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Welchman</surname><given-names>AE</given-names></name>, <name name-style="western"><surname>Stanley</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Schomers</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Miall</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Bülthoff</surname><given-names>HH</given-names></name> (<year>2010</year>) <article-title>The quick and the dead: When reaction beats intention</article-title>. <source>Proc Biol Sci</source> <volume>277</volume>: <fpage>1667</fpage>–<lpage>1674</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Schmidt1">
        <label>74</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmidt</surname><given-names>L</given-names></name>, <name name-style="western"><surname>d'Arc</surname><given-names>BF</given-names></name>, <name name-style="western"><surname>Lafargue</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Galanaud</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Czernecki</surname><given-names>V</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Disconnecting force from money: Effects of basal ganglia damage on incentive motivation</article-title>. <source>Brain</source> <volume>131</volume>: <fpage>1303</fpage>–<lpage>1310</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Lvy1">
        <label>75</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lévy</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Czernecki</surname><given-names>V</given-names></name> (<year>2006</year>) <article-title>Apathy and the basal ganglia</article-title>. <source>J Neurol</source> <volume>253</volume>: <fpage>VII54</fpage>–<lpage>61</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Jahanshahi1">
        <label>76</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jahanshahi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name> (<year>1998</year>) <article-title>Willed action and its impairment</article-title>. <source>Cogn Neuropsychol</source> <volume>15</volume>: <fpage>483</fpage>–<lpage>533</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-GhodsSharifi1">
        <label>77</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghods-Sharifi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Floresco</surname><given-names>SB</given-names></name> (<year>2010</year>) <article-title>Differential effects on effort discounting induced by inactivations of the nucleus accumbens core or shell</article-title>. <source>Behav Neurosci</source> <volume>124</volume>: <fpage>179</fpage>–<lpage>191</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Schweighofer1">
        <label>78</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schweighofer</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shishida</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Okamoto</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Tanaka</surname><given-names>SC</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Humans can adopt optimal discounting strategy under real-time constraints</article-title>. <source>PLoS Comput Biol</source> <volume>2</volume>: <fpage>e152</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Peters1">
        <label>79</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peters</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Büchel</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>The neural mechanisms of inter-temporal decision-making: Understanding variability</article-title>. <source>Trends Cogn Sci</source> <volume>15</volume>: <fpage>227</fpage>–<lpage>239</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Bock1">
        <label>80</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bock</surname><given-names>O</given-names></name> (<year>1990</year>) <article-title>Load compensation in human goal-directed arm movements</article-title>. <source>Behav Brain Res</source> <volume>41</volume>: <fpage>167</fpage>–<lpage>177</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Corcos1">
        <label>81</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Corcos</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>HY</given-names></name>, <name name-style="western"><surname>Wilding</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gottlieb</surname><given-names>GL</given-names></name> (<year>2002</year>) <article-title>Fatigue induced changes in phasic muscle activation patterns for fast elbow flexion movements</article-title>. <source>Exp Brain Res</source> <volume>142</volume>: <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-XuWilson1">
        <label>82</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu-Wilson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zee</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>The intrinsic value of visual information affects saccade velocities</article-title>. <source>Exp Brain Res</source> <volume>196</volume>: <fpage>475</fpage>–<lpage>481</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Shadmehr3">
        <label>83</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Krakauer</surname><given-names>JW</given-names></name> (<year>2008</year>) <article-title>A computational neuroanatomy for motor control</article-title>. <source>Exp Brain Res</source> <volume>185</volume>: <fpage>359</fpage>–<lpage>381</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Scott1">
        <label>84</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scott</surname><given-names>SH</given-names></name> (<year>2004</year>) <article-title>Optimal feedback control and the neural basis of volitional motor control</article-title>. <source>Nat Rev Neurosci</source> <volume>5</volume>: <fpage>532</fpage>–<lpage>546</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Guigon3">
        <label>85</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guigon</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Baraduc</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Desmurget</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Coding of movement- and force-related information in primate primary motor cortex: A computational approach</article-title>. <source>Eur J Neurosci</source> <volume>26</volume>: <fpage>250</fpage>–<lpage>260</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Miall1">
        <label>86</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miall</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>1996</year>) <article-title>Forward models for physiological motor control</article-title>. <source>Neural Netw</source> <volume>9</volume>: <fpage>1265</fpage>–<lpage>1279</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Pessiglione1">
        <label>87</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Schmidt</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Draganski</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kalisch</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lau</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>How the brain translates money into force: A neuroimaging study of subliminal motivation</article-title>. <source>Science</source> <volume>316</volume>: <fpage>904</fpage>–<lpage>906</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Schmidt2">
        <label>88</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmidt</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Cléry-Melin</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Lafargue</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Valabrègue</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Fossati</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Get aroused and be stronger: Emotional facilitation of physical effort in the human brain</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>9450</fpage>–<lpage>9457</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Turner1">
        <label>89</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turner</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Desmurget</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Basal ganglia contributions to motor control: A vigorous tutor</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>: <fpage>704</fpage>–<lpage>716</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kao1">
        <label>90</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kao</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Brainard</surname><given-names>MS</given-names></name> (<year>2006</year>) <article-title>Lesions of an avian basal ganglia circuit prevent context-dependent changes to song variability</article-title>. <source>J Neurophysiol</source> <volume>96</volume>: <fpage>1441</fpage>–<lpage>1455</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Pratt1">
        <label>91</label>
        <mixed-citation publication-type="other" xlink:type="simple">Pratt JW, Raiffa H, Schlaifer R (1995) Introduction to Statistical Decision Theory. Cambridge: MIT Press. 895 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Todorov3">
        <label>92</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorov</surname><given-names>E</given-names></name> (<year>2004</year>) <article-title>Optimality principles in sensorimotor control</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>907</fpage>–<lpage>915</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Bertsekas1">
        <label>93</label>
        <mixed-citation publication-type="other" xlink:type="simple">Bertsekas DP, Shreve SE (1996) Stochastic Optimal Control: The Discrete Time Case. Belmont: Athena Scientific. 323 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kunkel1">
        <label>94</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kunkel</surname><given-names>P</given-names></name>, <name name-style="western"><surname>von Dem Hagen</surname><given-names>O</given-names></name> (<year>2000</year>) <article-title>Numerical solution of infinite-horizon optimal-control problems</article-title>. <source>Comput Econ</source> <volume>16</volume>: <fpage>189</fpage>–<lpage>205</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Simpkins1">
        <label>95</label>
        <mixed-citation publication-type="other" xlink:type="simple">Simpkins A, Todorov E (2009) Practical numerical methods for stochastic optimal control of biological systems in continuous time and space. In: Proceedings of the IEEE International Symposium on Adaptive Dynamic Programming and Reinforcement Learning; 30 March—2 April 2009; Nashville, Tennessee, United States. ADPRL 2006. Available: <ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4927547" xlink:type="simple">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4927547</ext-link>. Accessed 24 August 2012.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Marin1">
        <label>96</label>
        <mixed-citation publication-type="other" xlink:type="simple">Marin D, Decock J, Rigoux L, Sigaud O (2011) Learning cost-efficient control policies with XCSF: Generalization capabilities and further improvement. In: Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation; 12–16 July 2011; Dublin, Ireland. GECCO 2011.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Zajac1">
        <label>97</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zajac</surname><given-names>FE</given-names></name> (<year>1989</year>) <article-title>Muscle and tendon: Models, scaling, and application to biomechanics and motor control</article-title>. <source>Crit Rev Biomed Eng</source> <volume>17</volume>: <fpage>359</fpage>–<lpage>415</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Todorov4">
        <label>98</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorov</surname><given-names>E</given-names></name> (<year>2005</year>) <article-title>Stochastic optimal control and estimation methods adapted to the noise characteristics of the sensorimotor system</article-title>. <source>Neural Comput</source> <volume>17</volume>: <fpage>1084</fpage>–<lpage>1108</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Guigon4">
        <label>99</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guigon</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Baraduc</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Desmurget</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Optimality, stochasticity, and variability in motor behavior</article-title>. <source>J Comput Neurosci</source> <volume>24</volume>: <fpage>57</fpage>–<lpage>68</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Platt1">
        <label>100</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Platt</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Huettel</surname><given-names>SA</given-names></name> (<year>2008</year>) <article-title>Risky business: The neuroeconomics of decision making under uncertainty</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>398</fpage>–<lpage>403</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Bryson1">
        <label>101</label>
        <mixed-citation publication-type="other" xlink:type="simple">Bryson AE, Ho Y-C (1975) Applied Optimal Control - Optimization, Estimation, and Control. New York: Hemisphere Publ Corp. 481 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Stengel1">
        <label>102</label>
        <mixed-citation publication-type="other" xlink:type="simple">Stengel RF (1986) Stochastic Optimal Control: Theory and Application. New York, NY: Wiley. 638 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Guigon5">
        <label>103</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guigon</surname><given-names>E</given-names></name> (<year>2010</year>) <article-title>Active control of bias for the control of posture and movement</article-title>. <source>J Neurophysiol</source> <volume>104</volume>: <fpage>1090</fpage>–<lpage>1102</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Press1">
        <label>104</label>
        <mixed-citation publication-type="other" xlink:type="simple">Press WH, Teukolsky SA, Vetterling WT, Flannery BP (2002) Numerical Recipes in C. The Art of Scientific Computing. 2<sup>nd</sup> edition. New York: Cambridge University Press. 994 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-vanderHelm1">
        <label>105</label>
        <mixed-citation publication-type="other" xlink:type="simple">van der Helm FCT, Rozendaal LA (2000) Musculoskeletal systems with intrinsic and proprioceptive feedback. In: Winters JM, Crago PE, editors. Biomechanics and Neural Control of Posture and Movement. New York, NY: Springer. pp. 164–174.</mixed-citation>
      </ref>
      <ref id="pcbi.1002716-Kirk1">
        <label>106</label>
        <mixed-citation publication-type="other" xlink:type="simple">Kirk DE (2004) Optimal Control Theory: An Introduction. Mineola, NY: Dover. 452 p.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>