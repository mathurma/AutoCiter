<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="article-commentary" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pbio</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="ppub">1544-9173</issn><issn pub-type="epub">1545-7885</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="doi">10.1371/journal.pbio.0040122</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Primer</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Biophysics</subject>
          <subject>Neuroscience</subject>
          <subject>Physiology</subject>
          <subject>Computational Biology/Systems Biology</subject>
        </subj-group>
        <subj-group subj-group-type="System Taxonomy">
          <subject>Mammals</subject>
        </subj-group>
      </article-categories><title-group><article-title>How Do Neurons Look at the World?</article-title><alt-title alt-title-type="running-head">Primer</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Solla</surname>
            <given-names>Sara A</given-names>
          </name>
        </contrib>
      </contrib-group><author-notes>
        <fn fn-type="current-aff" id="n2">
          <p>Sara A. Solla is at the Department of Physiology and the Department of Physics and Astronomy at Northwestern University, Chicago, Illinois, United States. E-mail:
					<email xlink:type="simple">solla@northwestern.edu</email>
				</p>
        </fn>
      <fn fn-type="conflict">
        <p>
				 The author has declared that no competing interests exist.
			</p>
      </fn></author-notes><pub-date pub-type="ppub">
        <month>4</month>
        <year>2006</year>
      </pub-date><pub-date pub-type="epub">
        <day>11</day>
        <month>4</month>
        <year>2006</year>
      </pub-date><volume>4</volume><issue>4</issue><elocation-id>e122</elocation-id><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2006</copyright-year><copyright-holder>Sara A. Solla</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article page="e92" related-article-type="companion" vol="4" xlink:href="info:doi/10.1371/journal.pbio.0040092" xlink:title="research article" xlink:type="simple">
				<article-title>Tuning Curves, Neuronal Variability, and Sensory Coding</article-title>
			</related-article><abstract abstract-type="toc">
        <p>Sara Solla explores what it means for groups of neurons to most efficiently represent information in the sensory world.</p>
      </abstract><funding-group><funding-statement>
				 The author acknowledges the hospitality of the Kavli Institute for Theoretical Physics at the University of California Santa Barbara, and partial National Science Foundation support under grant PHY99-07949.
			</funding-statement></funding-group></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title/>
      <p>When we consider the extraordinary abilities of our brains, we tend to focus on much-valued capabilities for the manipulation of abstract symbols and for the representation of the self. It is these capabilities that allow us to use language, do mathematics, create music, play chess, and maintain, as we do all this and much more, a unified and continuous sense of who we are. As much as we delight in these features, most likely to characterize the human brain as somewhat unique within an evolutionary scale, a crucial function of our brains, as well as the brains of many other organisms, is to provide an interface with the external world. This interface has two fundamental components: the processing of sensory information and the control of movement.</p>
      <p>Sensory information is typically represented by the collective activity of large populations of neurons. Consider, for instance, neurons that fire action potentials in response to visual inputs. The spiking activity of an individual neuron in this ensemble represents a reduced part of the visual world: the receptive field. Within its receptive field, the neuron is sensitive to the presence of a few specific features, such as an edge separating brightness from darkness, or an illuminated bar against a dark background. However, neurons are seldom all-or-nothing feature detectors; they respond not only to the presence or absence of features but also to their values. Neurons use their ability to produce graded responses by controlling the number of spikes they fire, in order to encode the value of continuous features such as the location of an edge or the orientation of a bright bar. This ability to encode continuous features through graded responses leads to the concept of a tuning curve, which describes the average number of spikes fired by a neuron in response to specific features of a visual stimulus. A population of neurons will therefore implement a distributed code in which each participating neuron responds best to certain feature values and less well to others. Each neuron will contribute to the ensemble information by responding to the visual input according to its own preferred values for the relevant features.</p>
      <p>Consider a visual input such as an illuminated bar: its location, orientation, and brightness are each described by a continuous variable, and
				<italic>n</italic> such variables are needed to describe
				<italic>n</italic> features. But neural responses are noisy: even if the values of these
				<italic>n</italic> features are held constant, the response will still vary from trial to trial. It is useful to describe this variability as fluctuations around a mean value; it is precisely this mean value that is captured by the tuning curve. Much has been investigated about the relation between tuning curves and the ability to process information. For instance, if information is typically represented by a population of neurons that respond quite broadly to a range of feature values, how will the accuracy of this representation depend on the width of the tuning curve [<xref ref-type="bibr" rid="pbio-0040122-b1">1</xref>,
				<xref ref-type="bibr" rid="pbio-0040122-b2">2</xref>]? The answer, it turns out, depends crucially on the dimensionality of the encoded variable: a sharpening of the tuning curve (a decrease in its width) improves the coding accuracy if neurons encode only one feature (
				<italic>n</italic> = 1), has no effect for
				<italic>n</italic> = 2, and actually reduces the amount of encoded information for
				<italic>n</italic> ≥ 3. Thus, only extremely specialized neurons benefit from narrow tuning curves, while neurons that respond to and encode for a multidimensional set of features benefit from broad tuning.
			</p>
      <p>Accuracy of representation is only one aspect of the problem that sensory neurons are engaged in solving. For instance, we may consider what tuning curve properties would result in a pattern of activity that is maximally informative about the various features that characterize a specific stimulus [<xref ref-type="bibr" rid="pbio-0040122-b3">3</xref>]. Or it might be useful to ask what type of tuning curves would result in maximal ability to discriminate between two stimuli [<xref ref-type="bibr" rid="pbio-0040122-b3">3</xref>]. Do these two related but not identical characterizations of optimal performance lead to the same solution as to what type of tuning curves are most desirable? Another important aspect of population codes is their redundancy; it is generally believed that this redundancy serves to compensate for the variability of individual neural responses, as the population average should improve the signal-to-noise ratio. But what happens when this noise is actually correlated among neurons in a population? Does the information capacity of the population still increase as the number of neurons increases [<xref ref-type="bibr" rid="pbio-0040122-b4">4</xref>,
				<xref ref-type="bibr" rid="pbio-0040122-b5">5</xref>]?
			</p>
      <p>In order to investigate these and related questions, theorists have mostly relied on two distinct mathematical tools to characterize performance; these two formal measures are the Fisher information, which characterizes the efficiency of reading out the population code, and the mutual information, which characterizes the average amount of information carried by the neural activity of the population about the features it encodes for. These two measures of information are not the same, and a significant amount of theoretical research has been devoted to clarifying the relationships and differences between them [<xref ref-type="bibr" rid="pbio-0040122-b6">6–8</xref>]. To develop an intuition about these measures, consider the case of a population of neurons that respond to only the orientation of a visual stimulus, as measured by a single angular variable θ that takes values between −π and +π. The activity of a population of
				<italic>N</italic> neurons is described as a multidimensional response. Each neuron is characterized by a tuning curve
				<italic>f<sub>i</sub>
				</italic>(θ), for
				<italic>i</italic> = 1,…,
				<italic>N</italic>. The tuning curves are all assumed to be bell shaped and identical in form, but the peak of the mean firing rate occurs at different values of θfor different neurons. Each neuron has a preferred orientation for which the average firing rate is maximum; this is where the tuning curve peaks. Let us now consider one of these neurons, for which the peak of the tuning curve occurs for a preferred orientation θ<sub>0</sub>, and ask a simple question: if we consider all possible stimulus orientations and the corresponding responses, for which stimuli is the response most informative? A very interesting puzzle arises when this simple problem is considered. If the mutual information is computed, the answer is that the most informative responses occur for stimuli with θ close to θ<sub>0</sub>. The most informative stimuli are those with orientation around the preferred orientation; these are stimuli that elicit, on average, maximal responses. If the Fisher information is computed, the answer turns out to be different. In this approach, the most informative responses occur for stimuli associated not with the peak of the tuning curve but with regions where the tuning curve has maximal slope. These stimuli have orientations quite different from the preferred orientation θ<sub>0</sub>. These orientations are selected because the large slope of the tuning curve facilitates the discrimination between different but close values of θ. This sensitivity to small changes in orientation is lost at the peak of the tuning curve [<xref ref-type="bibr" rid="pbio-0040122-b3">3</xref>,
				<xref ref-type="bibr" rid="pbio-0040122-b7">7</xref>].
			</p>
      <p>We are thus faced with an apparent contradiction: the answer seems to depend on the way in which we look at the problem! This situation clashes with our expectations about the lack of ambiguity of mathematical theories. The way out of this puzzle resides in reflecting upon the nature of mathematical models. A mathematical theory provides a conceptual framework for analyzing a problem. The framework is complemented by analytical and numerical tools that lead to solutions. When confronted with a problem, the theorist needs to choose a conceptual framework, a mathematical probe that will lead to answers. Different frameworks are likely to focus on different aspects of the problem, and thus illuminate different aspects of its underlying structure. In this, alternative mathematical theories are not so different from alternative experimental tools: they are simply a set of probes that in a complementary manner reveal different aspects of the problem. As is often the case in experimental work, the resolution of this paradox has required the design of a new probe.</p>
      <p>This is precisely the path taken by Daniel Butts and Mark Goldman in work reported in this issue of
				<italic>PLoS Biology</italic> [<xref ref-type="bibr" rid="pbio-0040122-b9">9</xref>]. The key was to go back to the principles of information theory and look for a tool better suited to the investigation of this problem. In the idealized problem discussed here, the question is simple and well posed: which are the stimuli that elicit most informative responses? The appropriate tools had already been laid out. A response-specific information (RSI) had been defined to characterize the amount of information about the stimulus gained by the observation of a particular response [<xref ref-type="bibr" rid="pbio-0040122-b10">10</xref>]. The RSI can be computed for every observed response, and responses can be ranked according to their RSI value. This concept led to a novel and quite useful tool for quantifying stimulus-specific information (SSI): for a given stimulus, characterize the responses it elicits, and compute the weighted average of the RSI value of these responses [<xref ref-type="bibr" rid="pbio-0040122-b11">11</xref>]. In this framework, the most informative stimuli are those that elicit the most informative responses.
			</p>
      <p>It is the SSI concept that was used by Butts and Goldman [<xref ref-type="bibr" rid="pbio-0040122-b9">9</xref>] to re-examine the pending paradox about tuning curves. Their investigations revealed a fundamental aspect of the problem that had been overlooked: the role of noise. They found that in low-noise environments, it is indeed advantageous from the information point of view to operate neurons in the maximal slope regime, so as to obtain better discrimination between similar but different orientations. However, fine orientation discrimination cannot be reliable in high-noise environments. In this regime, it is advantageous to operate neurons in the maximal firing regime, close to the peak of their tuning curves. This finding provides an intuitively satisfying solution to the paradox, and suggests the potential existence of an adaptive readout mechanism that would adjust its strategy according to the noise level.
			</p>
      <p>The relevance of the Butts and Goldman paper [<xref ref-type="bibr" rid="pbio-0040122-b9">9</xref>] goes beyond the resolution of a long-standing puzzle. In its elegant formulation, it illustrates the power and limitations of mathematical modeling. This story reminds us of the need to match the probe to the problem—this is as true in designing an experimental setup as it is in formulating the appropriate mathematical formalism.
			</p>
      <boxed-text id="box1" position="float">
        <sec>
          <title>Box 1. Glossary</title>
          <p>
						<bold>Fisher information</bold> provides a useful measure of encoding accuracy because its inverse is the Cramer-Rao bound on the squared error [<xref ref-type="bibr" rid="pbio-0040122-b12">12</xref>]. To understand the relevance of this bound, consider all possible ways of estimating features from neural activity, without systematic error. Among these unbiased estimators, the optimal, most efficient readout method [<xref ref-type="bibr" rid="pbio-0040122-b13">13</xref>] is the one with smallest variance, as specified by the inverse of the Fisher information.
					</p>
          <p>
						<bold>Mutual information</bold> between the response of a population of neurons and the features that the collective activity encodes for provides a measure of the average amount of information about n stimulus features gained through the observation of the activity of N neurons [<xref ref-type="bibr" rid="pbio-0040122-b14">14</xref>,
						<xref ref-type="bibr" rid="pbio-0040122-b15">15</xref>].
					</p>
          <p>
						<bold>Response-specific information (RSI)</bold> focuses on an observed response, and computes the amount of information that this observation provides about stimulus features [<xref ref-type="bibr" rid="pbio-0040122-b10">10</xref>]. The response is held fixed, and the information gained is averaged over all stimuli that could have elicited it.
					</p>
          <p>
						<bold>Stimulus-specific information (SSI)</bold> evaluates the information content of a stimulus. This stimulus can elicit a distribution of responses, each of them characterized by an RSI value. The SSI is simply the average RSI over the distribution of responses associated with this stimulus [<xref ref-type="bibr" rid="pbio-0040122-b11">11</xref>].
					</p>
        </sec>
      </boxed-text>
    </sec>
  </body>
  <back>
    <ack>
      <p>The author dedicates this paper to Predrag Cvitanović.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pbio-0040122-b1">
        <label>1</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Pouget</surname>
              <given-names>A</given-names>
            </name>
            <name name-style="western">
              <surname>Deneve</surname>
              <given-names>S</given-names>
            </name>
            <name name-style="western">
              <surname>Ducom</surname>
              <given-names>JC</given-names>
            </name>
            <name name-style="western">
              <surname>Latham</surname>
              <given-names>PE</given-names>
            </name>
          </person-group>
          <article-title>Narrow versus wide tuning curves: What's better for a population code?</article-title>
          <source>Neural Comput</source>
          <year>1999</year>
          <volume>11</volume>
          <fpage>85</fpage>
          <lpage>90</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b2">
        <label>2</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Zhang</surname>
              <given-names>K</given-names>
            </name>
            <name name-style="western">
              <surname>Sejnowski</surname>
              <given-names>TJ</given-names>
            </name>
          </person-group>
          <article-title>Neuronal tuning: To sharpen or to broaden?</article-title>
          <source>Neural Comput</source>
          <year>1999</year>
          <volume>11</volume>
          <fpage>75</fpage>
          <lpage>84</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b3">
        <label>3</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Seung</surname>
              <given-names>HS</given-names>
            </name>
            <name name-style="western">
              <surname>Sompolinsky</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>Simple models for reading population codes.</article-title>
          <source>Proc Natl Acad Sci USA</source>
          <year>1993</year>
          <volume>90</volume>
          <fpage>10749</fpage>
          <lpage>10753</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b4">
        <label>4</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Abbott</surname>
              <given-names>LF</given-names>
            </name>
            <name name-style="western">
              <surname>Dayan</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>The effect of correlated variability on the accuracy of a population code.</article-title>
          <source>Neural Comput</source>
          <year>1999</year>
          <volume>11</volume>
          <fpage>91</fpage>
          <lpage>101</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b5">
        <label>5</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Sompolinsky</surname>
              <given-names>H</given-names>
            </name>
            <name name-style="western">
              <surname>Yoon</surname>
              <given-names>H</given-names>
            </name>
            <name name-style="western">
              <surname>Kang</surname>
              <given-names>K</given-names>
            </name>
            <name name-style="western">
              <surname>Shamir</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Population coding in neuronal systems with correlated noise.</article-title>
          <source>Phys Rev E Stat Nonlin Soft Matter Phys</source>
          <year>2001</year>
          <volume>64</volume>
          <fpage>51904</fpage>
          <comment>E-pub 17 October 2001</comment>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b6">
        <label>6</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Rissanen</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Fisher information and stochastic complexity.</article-title>
          <source>IEEE Trans Inf Theory</source>
          <year>1996</year>
          <volume>42</volume>
          <fpage>40</fpage>
          <lpage>47</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b7">
        <label>7</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Brunel</surname>
              <given-names>N</given-names>
            </name>
            <name name-style="western">
              <surname>Nadal</surname>
              <given-names>JP</given-names>
            </name>
          </person-group>
          <article-title>Mutual information, Fisher information, and population coding.</article-title>
          <source>Neural Comput</source>
          <year>1998</year>
          <volume>10</volume>
          <fpage>1731</fpage>
          <lpage>1757</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b8">
        <label>8</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Kang</surname>
              <given-names>K</given-names>
            </name>
            <name name-style="western">
              <surname>Sompolinsky</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>Mutual information of population codes and distance measures in probability space.</article-title>
          <source>Phys Rev Lett</source>
          <year>2001</year>
          <volume>86</volume>
          <fpage>4958</fpage>
          <lpage>4961</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b9">
        <label>9</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Butts</surname>
              <given-names>DA</given-names>
            </name>
            <name name-style="western">
              <surname>Goldman</surname>
              <given-names>MS</given-names>
            </name>
          </person-group>
          <article-title>Tuning curves, neuronal variability, and sensory coding.</article-title>
          <source>PLoS Biol</source>
          <year>2006</year>
          <volume>4</volume>
          <fpage>e92</fpage>
          <comment>doi: <ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0040092" xlink:type="simple">10.1371/journal.pbio.0040092</ext-link></comment>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b10">
        <label>10</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>De Weese</surname>
              <given-names>MR</given-names>
            </name>
            <name name-style="western">
              <surname>Meister</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>How to measure the information gained from one symbol.</article-title>
          <source>Network: Comput Neural Syst</source>
          <year>1999</year>
          <volume>10</volume>
          <fpage>325</fpage>
          <lpage>340</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b11">
        <label>11</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Butts</surname>
              <given-names>DA</given-names>
            </name>
          </person-group>
          <article-title>How much information is associated with a particular stimulus?</article-title>
          <source>Network: Comput Neural Syst</source>
          <year>2003</year>
          <volume>14</volume>
          <fpage>177</fpage>
          <lpage>187</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b12">
        <label>12</label>
        <nlm-citation publication-type="book" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Kay</surname>
              <given-names>SM</given-names>
            </name>
          </person-group>
          <source>Fundamentals of statistical signal processing: Estimation theory</source>
          <year>1993</year>
          <publisher-loc>Englewood Cliffs (New Jersey)</publisher-loc>
          <publisher-name>Prentice Hall</publisher-name>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b13">
        <label>13</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Pouget</surname>
              <given-names>A</given-names>
            </name>
            <name name-style="western">
              <surname>Zhang</surname>
              <given-names>K</given-names>
            </name>
            <name name-style="western">
              <surname>Deneve</surname>
              <given-names>S</given-names>
            </name>
            <name name-style="western">
              <surname>Latham</surname>
              <given-names>PE</given-names>
            </name>
          </person-group>
          <article-title>Statistically efficient estimation using population code.</article-title>
          <source>Neural Comput</source>
          <year>1998</year>
          <volume>10</volume>
          <fpage>373</fpage>
          <lpage>401</lpage>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b14">
        <label>14</label>
        <nlm-citation publication-type="book" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Blahut</surname>
              <given-names>RE</given-names>
            </name>
          </person-group>
          <source>Principles and practice of information theory</source>
          <year>1988</year>
          <publisher-loc>Reading (Massachusetts)</publisher-loc>
          <publisher-name>Addison-Wesley</publisher-name>
        </nlm-citation>
      </ref>
      <ref id="pbio-0040122-b15">
        <label>15</label>
        <nlm-citation publication-type="journal" xlink:type="simple">
          <person-group person-group-type="author">
            <name name-style="western">
              <surname>Bialek</surname>
              <given-names>W</given-names>
            </name>
            <name name-style="western">
              <surname>Rieke</surname>
              <given-names>F</given-names>
            </name>
            <name name-style="western">
              <surname>de Ruyter van Steveninck</surname>
              <given-names>R</given-names>
            </name>
            <name name-style="western">
              <surname>Warland</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Reading a neural code.</article-title>
          <source>Science</source>
          <year>1991</year>
          <volume>252</volume>
          <fpage>1854</fpage>
          <lpage>1857</lpage>
        </nlm-citation>
      </ref>
    </ref-list>
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>RSI</term>
          <def>
            <p>response-specific information</p>
          </def>
        </def-item>
        <def-item>
          <term>SSI</term>
          <def>
            <p>stimulus-specific information</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    
  </back>
</article>