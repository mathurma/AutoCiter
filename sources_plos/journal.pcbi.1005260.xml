<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005260</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01286</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Reaction time</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Reaction time</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Human Inferences about Sequences: A Minimal Transition Probability Model</article-title>
<alt-title alt-title-type="running-head">Building Block of Sequence Knowledge</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6992-678X</contrib-id>
<name name-style="western">
<surname>Meyniel</surname>
<given-names>Florent</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6851-4927</contrib-id>
<name name-style="western">
<surname>Maheu</surname>
<given-names>Maxime</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7418-8275</contrib-id>
<name name-style="western">
<surname>Dehaene</surname>
<given-names>Stanislas</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Cognitive Neuroimaging Unit, CEA DRF/I2BM, INSERM, Université Paris‐Sud, Université Paris‐Saclay, NeuroSpin center, Gif-sur-Yvette, France</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Université Paris Descartes, Sorbonne Paris Cité, Paris, France</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Collège de France, Paris, France</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item><p><bold>Conceptualization:</bold> FM SD.</p></list-item> <list-item><p><bold>Formal analysis:</bold> FM MM.</p></list-item> <list-item><p><bold>Funding acquisition:</bold> SD.</p></list-item> <list-item><p><bold>Methodology:</bold> FM MM.</p></list-item> <list-item><p><bold>Project administration:</bold> FM.</p></list-item> <list-item><p><bold>Supervision:</bold> FM SD.</p></list-item> <list-item><p><bold>Writing – original draft:</bold> FM MM SD.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">florent.meyniel@cea.fr</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>28</day>
<month>12</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>12</issue>
<elocation-id>e1005260</elocation-id>
<history>
<date date-type="received">
<day>7</day>
<month>8</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>11</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Meyniel et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005260"/>
<abstract>
<p>The brain constantly infers the causes of the inputs it receives and uses these inferences to generate statistical expectations about future observations. Experimental evidence for these expectations and their violations include explicit reports, sequential effects on reaction times, and mismatch or surprise signals recorded in electrophysiology and functional MRI. Here, we explore the hypothesis that the brain acts as a near-optimal inference device that constantly attempts to infer the time-varying matrix of transition probabilities between the stimuli it receives, even when those stimuli are in fact fully unpredictable. This parsimonious Bayesian model, with a single free parameter, accounts for a broad range of findings on surprise signals, sequential effects and the perception of randomness. Notably, it explains the pervasive asymmetry between repetitions and alternations encountered in those studies. Our analysis suggests that a neural machinery for inferring transition probabilities lies at the core of human sequence knowledge.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>We explore the possibility that the computation of time-varying transition probabilities may be a core building block of sequence knowledge in humans. Humans may then use these estimates to predict future observations. Expectations derived from such a model should conform to several properties. We list six such properties and we test them successfully against various experimental findings reported in distinct fields of the literature over the past century. We focus on five representative studies by other groups. Such findings include the “sequential effects” evidenced in many behavioral tasks, i.e. the pervasive fluctuations in performance induced by the recent history of observations. We also consider the “surprise-like” signals recorded in electrophysiology and even functional MRI, that are elicited by a random stream of observations. These signals are reportedly modulated in a quantitative manner by both the local and global statistics of observations. Last, we consider the notoriously biased subjective perception of randomness, i.e. whether humans think that a given sequence of observations has been generated randomly or not. Our model therefore unifies many previous findings and suggests that a neural machinery for inferring transition probabilities must lie at the core of human sequence knowledge.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>INSERM</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6992-678X</contrib-id>
<name name-style="western">
<surname>Meyniel</surname>
<given-names>Florent</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>CEA</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6992-678X</contrib-id>
<name name-style="western">
<surname>Meyniel</surname>
<given-names>Florent</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Collège de France</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7418-8275</contrib-id>
<name name-style="western">
<surname>Dehaene</surname>
<given-names>Stanislas</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004963</institution-id>
<institution>Seventh Framework Programme</institution>
</institution-wrap>
</funding-source>
<award-id>604102</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7418-8275</contrib-id>
<name name-style="western">
<surname>Dehaene</surname>
<given-names>Stanislas</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded by Institut National de la Santé Et de la Recherche Médicale (SD), Commissariat à l’Energie Atomique (SD, FM), Collège de France (SD), a “Frontières du Vivant” doctoral fellowship involving the Ministère de l’Enseignement Supérieur et de la Recherche and Programme Bettencourt (MM), a grant from the European Union Seventh Framework Programme (FP7/2007 2013, <ext-link ext-link-type="uri" xlink:href="http://ec.europa.eu/research/" xlink:type="simple">http://ec.europa.eu/research/</ext-link>) under grant agreement no. 604102 - “Human Brain Project“ (FM, SD) and by an advanced European Research Council grant “NeuroSyntax” (SD). The funders had no role in the study design, analysis or decision to publish.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="1"/>
<page-count count="26"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Codes are available at <ext-link ext-link-type="uri" xlink:href="http://github.com/florentmeyniel/MinimalTransitionProbsModel" xlink:type="simple">http://github.com/florentmeyniel/MinimalTransitionProbsModel</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>From bird song to music, sea waves, or traffic lights, many processes in real life unfold across time and generate time series of events. Sequences of observations are therefore often underpinned by some regularity that depends on the underlying generative process. The ability to detect such sequential regularities is fundamental to adaptive behavior, and many experiments in psychology and neuroscience have assessed this ability by appealing to tasks involving sequences of events. Various effects suggestive of local sequence learning have been consistently reported, even when experimental sequences are devoid of any regularity (i.e. purely random) and restricted to only two possible items or actions. Studies of “novelty detection” for instance show that the mere exposure to a sequence of stimuli elicits reproducible “novelty” brain responses that vary quantitatively as a function of the item infrequency and divergence from previous observations [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref011">11</xref>]. Behaviorally, studies using two-alternative forced-choices have revealed “sequential effects”, i.e. fluctuations in performance induced by local regularities in the sequence. For instance, subjects become faster and more accurate when they encounter a pattern that repeats the same instructed response, or that alternates between two responses, and they slow down and may even err when this local pattern is discontinued [<xref ref-type="bibr" rid="pcbi.1005260.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>]. Finally, studies asking subjects to produce random sequences or to rate the apparent “randomness” of given sequences, show a notorious underestimation of the likelihood of alternations [<xref ref-type="bibr" rid="pcbi.1005260.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref029">29</xref>].</p>
<p>Here, we propose a model that provides a principled and unifying account for those seemingly unrelated results, reported in various studies and subfields of the literature quoted above.</p>
<p>We adopt a Bayesian-inference approach [<xref ref-type="bibr" rid="pcbi.1005260.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref037">37</xref>] which relies on three pillars. The first one is that information processing in the brain relies on the computation of probabilities [<xref ref-type="bibr" rid="pcbi.1005260.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref043">43</xref>]. A second pillar is that these probabilistic computations closely approximate Bayes’ rule. This means that, in order to infer the hidden regularities of the inputs it receives, the brain combines the likelihood of observations given putative regularities and the prior likelihood of these regularities [<xref ref-type="bibr" rid="pcbi.1005260.ref044">44</xref>]. A third pillar is the predictive and iterative nature of Bayesian computations: once the hidden regularities of the inputs are inferred, the brain uses them to anticipate the likelihood of future observations. Comparison between expectations and actual data allows the brain to constantly update its estimates–a computational mode termed “active inference” [<xref ref-type="bibr" rid="pcbi.1005260.ref045">45</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref046">46</xref>].</p>
<p>To apply this general framework to sequences, one must identify the models that the brain computes when learning from a sequence. One possibility that we explore here is that there are core building blocks of sequence knowledge that the brain uses across many different domains [<xref ref-type="bibr" rid="pcbi.1005260.ref047">47</xref>]. Throughout this paper, our goal is to identify the minimal building block of sequence knowledge. By “minimal”, we mean that a simpler hypothesis would demonstrably fail to account for experimental effects such as surprise signals, sequential effects in reaction times and the biased perception of randomness.</p>
<p>Our proposal can be succinctly formulated: the brain constantly extracts the statistical structure of its inputs by estimating the <italic>non-stationary transition probability matrix</italic> between successive items. “Transition probability matrix” means that the brain attributes a specific probability to each of the possible transitions between successive items. “Non-stationary” means that the brain entertains the hypothesis that these transition probabilities may change abruptly, and constantly revises its estimates based on the most recent observations.</p>
<p>We formalized this proposal into a quantitative model, which we call the “local transition probability model”. As we shall see, this model predicts that expectations arising from a sequence of events should conform to several properties. We list these properties below and unpack them, one at a time, in the Results section.</p>
<p>To test whether the proposed model is general, we simulated the results of five different tasks previously published [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>]. They differ in the type of observable: either reaction times [<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>], judgment of randomness [<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>], functional MRI signals [<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>] or EEG signals [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>]. They also differ in the experimental task: either passive listening [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>], two-alternative forced-choice [<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>] or subjective ratings [<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>]. Last, they also differ in the way stimuli are presented: sequential and auditory [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>], sequential and visual [<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>] or simultaneous and visual [<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>]. Yet, as we shall see, all of these observations fall under the proposed local transition probability model.</p>
<p>We also tested the minimal character of the model, i.e. the necessity of its two main hypotheses, namely, that transition probabilities are learned, and that such learning is local in time. Instead of transition probabilities, simpler models previously proposed that subjects learn the absolute frequency of items, or the frequency of alternations [<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref048">48</xref>]. We evaluated the predictive ability of these statistics, whose relationships are illustrated in <xref ref-type="fig" rid="pcbi.1005260.g001">Fig 1</xref>. We also tested the non-stationarity hypothesis by comparing the local transition probability model with other models that assume no change in the quantity they estimate, or a simple forgetting rule (as illustrated in <xref ref-type="fig" rid="pcbi.1005260.g002">Fig 2</xref>). This model comparison is not exhaustive since many proposals were formulated over the past fifty years; however, it allows to test for the necessity of our assumptions. To anticipate on the results, we found that only a learning of local transition probabilities was compatible with the large repertoire of experimental effects reported here.</p>
<fig id="pcbi.1005260.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005260.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Three different hypothesis spaces</title>
<p>(A) Sequences can be characterized by a hierarchy of statistics. We consider here binary sequences with only two items: X and Y. The simplest statistic considers stimuli in isolation, based on the frequency of items, p(X) and p(Y). A second level considers pairs of items irrespective of their order, distinguishing pairs of identical versus different items (XX and YY vs. XY and YX). The relevant statistic is the frequency of alternations, or conversely, the frequency of repetitions: p(alt.) = 1 – p(rep.). A third level considers ordered pairs, distinguishing X<sub>1</sub>Y<sub>2</sub> from Y<sub>1</sub>X<sub>2</sub>. The relevant statistics are the two transition probabilities between consecutive items: p(Y<sub>2</sub>|X<sub>1</sub>) and p(X<sub>2</sub>|Y<sub>1</sub>). For brevity, we generally omit the subscripts. For binary sequences, the space of transition probabilities is 2-dimensional. In this space, the diagonals are special cases where transition probabilities coincide with the frequency of items and frequency of alternations. Out of the diagonals, there is no linear mapping between transition probabilities and the frequency of items (shown in red/blue and iso-contours) or the frequency of alternations (shown with transparency and iso-contours). (B) Example sequences generated from distinct statistics. From top to bottom: The sequences (1) and (2) differ in their frequency of X but not in their frequency of alternations. To generate such sequences, one can select the next stimulus by flipping a biased coin. The sequences (3) and (4) differ in their frequency of alternations, but not in their frequency of X. To generate such a sequence, one can start the sequence arbitrarily with X or Y, and then decide whether to repeat the same item or not by flipping a biased coin. The sequence (5) is biased both in its frequency of alternations and its frequency of items. It cannot be generated with a single biased coin, but instead two biased coins are required, one to decide which item should follow an X and the other to decide which item should follow a Y. The sequence (6) is a purely random sequence, with no bias in either transition probabilities, and hence, no bias in item nor alternation frequencies. It can be generated by flipping a fair coin.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.g001" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005260.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005260.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Three different inference styles</title>
<p>Panel A shows an example of a sequence in which the statistics change abruptly: the first half, from 1 to 150, was generated with p(X|Y) = 1 – p(Y|X) = 2/3, and the second half with p(X|Y) = 1 – p(Y|X) = 1/3. In this paper, we consider different hypotheses regarding the inference algorithm used by the brain to cope with such abrupt changes (panel B). Some models assume that a single statistic generates all the observations received (“fixed belief”) while other assume volatility, i.e. that the generative statistic may change from one observation to the next with fixed probability <italic>p</italic><sub>c</sub> (“dynamic belief”). Models with fixed belief may estimate the underlying statistic either by weighting all observations equally (“perfect integration”), or by considering all observations within a fixed recent window of N stimuli (“windowed integration”, not shown in the figure), or by forgetting about previous observations with an exponential decay <italic>ω</italic> (“leaky integration”). The heat maps show the posterior distributions of transition probabilities generating the sequence in (A) as estimated by each model. The white dash line indicates the true generative value. The insets show the estimated 2-dimensional space of transition probabilities at distinct moments in the sequence. White circles indicate the true generative values.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Description of the model</title>
<p>The local transition probability model assumes that several brain circuits involved in sequence learning entertain the hypothesis that the sequence of items has been generated by a “Markovian” generative process, i.e. only the previous item <italic>y</italic><sub><italic>t</italic>–1</sub> has a predictive power onto the current item <italic>y</italic><sub><italic>t</italic></sub>. Those circuits therefore attempt to infer the “transition probability matrix” which expresses the probability of observing a given item, given the identity of the preceding one.</p>
<p>Further, the model is local in time in that it assumes that the transition probabilities generating the observations may change over time (some theorists call this a model with a “dynamic belief”). More precisely, it assumes that there is a fixed, non-zero, probability <italic>p</italic><sub>c</sub> that the full matrix of transition probabilities changes suddenly from one observation to the next (see <xref ref-type="fig" rid="pcbi.1005260.g002">Fig 2A</xref>). Therefore, at any given moment, the current and unknown generative transition probabilities must be estimated only from the observations that followed the last change. Note that the occurrence of such changes is itself unknown–the model must infer them. Bayes’ rule and probabilistic inference allow to solve this challenging problem optimally. Intuitively, the optimal solution discounts remote observations and adjusts the strength of this discounting process to the trial-by-trial likelihood of changes. The estimation of transition probabilities is therefore “local” and non-stationary.</p>
<p>In this paper, we contrast the local transition probability model with an alternative model which entertains a “fixed belief”, i.e. which assumes that the generative process never changes (<italic>p</italic><sub>c</sub> is exactly 0). The fixed-belief assumption greatly simplifies the estimation of transition probabilities, which boils down to counting the occurrence of each transition between any two items–but it prevents the model from adapting to the recent history of events. We also consider models which only approximate the Bayes-optimal “dynamic belief” inference. One such model is a forgetful count that discards old observations or weights recent observations more than past ones [<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>]. The count may be forgetful because it is limited to a fixed window of recent observations (the “windowed model”), or because it involves a leaky integration, such that previous observations are progressively forgotten. Importantly, the time scale over which forgetting occurs is fixed at a preset value and therefore cannot be adjusted to the trial-by-trial likelihood of changes, unlike the optimal solution.</p>
<p>The leaky integration and the Bayes-optimal dynamic belief are two algorithms, each with a single free parameter, that result in local estimates of statistics. Both yield similar results in the present context, we therefore refer to both as the “local transition probability model”. We reported the results for the leaky integration in the main text and the results for the Bayes-optimal dynamic belief as supplementary information (see <xref ref-type="supplementary-material" rid="pcbi.1005260.s001">S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005260.s002">S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005260.s003">S3</xref> Figs).</p>
<p>These different inference styles of transition probabilities–fixed belief, dynamic belief, leaky integration–are depicted in <xref ref-type="fig" rid="pcbi.1005260.g002">Fig 2B</xref>. For comparison, we also implemented variants that resort to the same inference styles but estimate a different statistic: either the absolute frequency of items, or the frequency of alternation between successive items. It is important to note that these statistics are simpler than transition probabilities, because the information about the frequency of items and the frequency of alternations is embedded in the larger space of transition probabilities (see <xref ref-type="fig" rid="pcbi.1005260.g001">Fig 1A</xref>). Transition probabilities also dictate the frequency of ordered pairs of items (see <xref ref-type="supplementary-material" rid="pcbi.1005260.s006">S1 Text</xref> for Supplementary Equations). Some of the models included in our comparison were proposed by others, e.g. the fixed-belief model that learns item frequencies was proposed by Mars and colleagues [<xref ref-type="bibr" rid="pcbi.1005260.ref004">4</xref>] and dynamic-belief models were also proposed by Behrens, Nassar and colleagues [<xref ref-type="bibr" rid="pcbi.1005260.ref049">49</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref050">50</xref>] for learning item frequencies, and by Yu and Cohen [<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>] for learning the frequency of alternations.</p>
<p>The local transition probability model makes several predictions:</p>
<list list-type="order">
<list-item><p><bold>In binary sequences, expectations should reflect at least two statistics: the absolute frequency of items, and the frequency of alternations.</bold> This is because these statistics are special cases of transition probabilities (see <xref ref-type="fig" rid="pcbi.1005260.g001">Fig 1A</xref>).</p></list-item>
<list-item><p><bold>Transition probabilities should drive expectations both globally (given the entire sequence) and locally (given the recent history of observations).</bold> Since the inference is local, it captures both local statistics, and their average (the global statistics).</p></list-item>
<list-item><p><bold>The local effects should be observed even in fully unbiased sequences where items and transitions are equiprobable.</bold> Indeed, a local, non-stationary inference constantly captures the local deviations from equiprobability that emerge by chance even in purely random sequences.</p></list-item>
<list-item><p><bold>The same observations received in different orders should produce different expectations.</bold> Since the model progressively discounts previous observations, the impact of a given observation depends on its rank within the recent past.</p></list-item>
<list-item><p><bold>Repeating stimuli should produce stronger expectations than the same number of alternating stimuli.</bold> This is an emergent property of learning transition probabilities: a repeating sequence (e.g. XXXXX) provides twice as much evidence about a single transition type (X→X) than an alternating sequence (e.g. XYXYX) where the evidence is spread among two transition types (X→Y and Y→X).</p></list-item>
<list-item><p><bold>The perception of randomness should be biased: although, the highest degree of randomness is, by definition, achieved when there is no bias, a sequence that contains slightly more alternations than repetitions should be perceived as “more random” than a genuinely unbiased sequence.</bold> This is because stronger expectations arise from repetition than alternation.</p></list-item>
</list>
<p>In the following, we characterize these predictions in greater detail in specific experimental contexts, and we test them against a variety of data sets and by comparing with simpler models.</p>
</sec>
<sec id="sec004">
<title>Effects of sequence statistics in electrophysiological data</title>
<p>The P300 is an event-related potential that can be easily measured on the human scalp and is sensitive to the surprise elicited by novel or unexpected stimuli. Squires et al. (1976) made a seminal contribution: they showed that, even in a random stream of items, the P300 amplitude varies strongly with the local history of recent observations. Even in purely random sequences (like fair coin flips) the amplitude of the P300 elicited by a given stimulus X increases when it is preceded by an increasing number of other stimuli Y, i.e. when it violates a “streak” of recent repetitions of Y (e.g. XXXX<underline>X</underline> vs. YXXX<underline>X</underline> vs. YYXX<underline>X</underline> vs. YYYX<underline>X</underline> vs. YYYY<underline>X</underline>). The P300 amplitude also increases when a stimulus violates a pattern of alternations enforced by the recent history (e.g. XYXY<underline>X</underline> vs. YXYX<underline>X</underline>). Squires et al. (1976) plotted these history effects as “trees” reflecting the entire history of recent stimuli (<xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3A</xref>). When they varied the overall frequency of items in the sequence (from p(X) = 0.5 to 0.7 or 0.3), they also found that the entire tree of local effects was shifted up or down according to p(X). Altogether, their data show that the P300 amplitude reflects, in a quantitative manner, the violation of statistical expectations based on three factors: the global frequency of items, their local frequency and the local frequency of alternations. Importantly, these local effects emerged even in purely random sequences (see the middle tree in <xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3A</xref>).</p>
<fig id="pcbi.1005260.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005260.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The electrophysiological P300 response reflects the tracking of statistical regularities.</title>
<p>A) Data redrawn from Squires et al. (1976). Subjects passively listened to binary streams of auditory stimuli (denoted X and Y). Stimuli were generated randomly with global frequency p(X) = 0.5 (no bias), p(X) = 0.7 or p(X) = 0.3 (biased frequencies) in separate sessions. The P300 amplitude was averaged at the end of all possible patterns of 5 stimuli at most, and plotted as a “tree” whose branches show the possible extensions for each pattern. (B-C) Average theoretical levels of surprise for all possible patterns. For each model (i.e. each set of three trees), the theoretical surprise levels were adjusted for offset and scaling to fit the data. For local models with leaky integration (B), we show the trees corresponding to the best fitting value of the leak parameter <italic>ω</italic>. The insets show a direct comparison between data and best-fitting theoretical surprise levels, with the regression <italic>R</italic><sup>2</sup>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.g003" xlink:type="simple"/>
</fig>
<p>These effects correspond to properties #1, #2 and #3 of the local transition probability model. Because the P300 wave seems to reflect the violation of expectations, rather than the expectations themselves, we quantified whether a given observation fulfills or deviates from expectations with the mathematical notion of surprise [<xref ref-type="bibr" rid="pcbi.1005260.ref051">51</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref052">52</xref>]. We computed theoretical levels of surprise, given the observations received (and no other information), from the local transition probability model and we found that they quantitatively reproduce the data from Squires et al. (<xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3B</xref>).</p>
<p>More precisely, the local transition probability model has a single free parameter, which controls the non-stationarity of the inference. It is crucial to avoid conflating the dimensionality of the estimated quantities (which is two here, for the transition–probability matrix between two items) and the number of free parameters constraining this estimation (which is one for the local transition probability model). In the approximate model that we tested here, the only free parameter is the leak of the integration (<italic>ω</italic>) whose best fitting value was <italic>ω</italic> = 16 stimuli. This exponential decay factor means that the weight of a given observation is divided by two after a half-life of <italic>ω</italic> * ln(2) ≈ 11 new observations. We report in <xref ref-type="supplementary-material" rid="pcbi.1005260.s001">S1 Fig</xref> the results for the exact inference (Bayes-optimal dynamic belief), for which the best fitting value of the a priori probability of change was <italic>p</italic><sub>c</sub> = 0.167.</p>
<p>While the assumptions of the local transition probability model seem sufficient to account for the data, we can also demonstrate that each of them is actually necessary and that they can be distinguished from one anther (see <xref ref-type="supplementary-material" rid="pcbi.1005260.s004">S4 Fig</xref>). Models with constant integration, i.e. without leak or a recent observation window, become increasingly insensitive to the recent history of observations as more observations are received. For such models, further details in the recent history have little impact on their expectations, as seen in the corresponding shriveled trees (see <xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3C</xref>). Models that learn simpler statistics are also not able to fully reproduce the data. The ones that learn the frequency of alternations show little effect of the global item frequency (see the position of the roots of trees in <xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3B</xref>). Those that learn the frequency of items capture the effect of global item frequency, but they fail to reproduce the specific arrangement of branches of the trees. For instance, in purely random sequences (when p(X) = 0.5), such models predict that the surprise elicited by YXY<underline>X</underline> patterns should be like the average response (compare its position relatively to the root of the tree in <xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3B</xref>) whereas it is not the case in the data. A model that learns transition probabilities captures the lower-than-average activity for YXY<underline>X</underline> patterns because it detects the repeated alternation (see <xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3B</xref>).</p>
<p>We quantified the superiority of the local transition probability model using the Bayesian Information Criterion (BIC), which favors goodness-of-fit while penalizing models for their number of free parameters. The local transition probability model was better than the others: all ΔBIC &gt; 9.46 (see <xref ref-type="table" rid="pcbi.1005260.t001">Table 1</xref>). Cross-validation accuracy, another metric for model comparison, yielded the same conclusion (see <xref ref-type="supplementary-material" rid="pcbi.1005260.s005">S5 Fig</xref>). We also included the model proposed by Squires et al. (see <xref ref-type="sec" rid="sec008">Methods</xref>) that achieves a similar goodness-of-fit, but at the expanse of a higher complexity. In addition, this model is descriptive (a linear regression of several effects of interest) and not principled.</p>
<table-wrap id="pcbi.1005260.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005260.t001</object-id>
<label>Table 1</label> <caption><title>Model comparison</title></caption>
<alternatives>
<graphic id="pcbi.1005260.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Statistics</th>
<th align="center">Integration</th>
<th align="center">Number of free parameters</th>
<th align="center">Best-fitting parameter</th>
<th align="center">MSE</th>
<th align="center">ΔBIC</th>
<th align="center"><italic>R</italic><sup>2</sup> (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="4">Item frequency</td>
<td align="center">Perfect</td>
<td align="center">2 + 0</td>
<td align="center">- / -</td>
<td align="center">1.06 / 0.50</td>
<td align="center">34.97 / 25.89</td>
<td align="center">48 / 63</td>
</tr>
<tr>
<td align="center">Windowed</td>
<td align="center">2 + 1</td>
<td align="center">14 / 15</td>
<td align="center">0.75 / 0.28</td>
<td align="center">22.39 / 15.07</td>
<td align="center">63 / 79</td>
</tr>
<tr>
<td align="center">Leaky</td>
<td align="center">2 + 1</td>
<td align="center">11 / 12</td>
<td align="center">0.69 / 0.26</td>
<td align="center">18.51 / 13.72</td>
<td align="center">66 / 81</td>
</tr>
<tr>
<td align="center">Dynamic</td>
<td align="center">2 + 1</td>
<td align="center">0.0190 / 0.013</td>
<td align="center">0.58 / 0.20</td>
<td align="center">9.83 / 7.68</td>
<td align="center">71 / 85</td>
</tr>
<tr>
<td align="center" rowspan="4">Alternation frequency</td>
<td align="center">Perfect</td>
<td align="center">2 + 0</td>
<td align="center">- / -</td>
<td align="center">1.51 / 1.15</td>
<td align="center">52.18 / 45.93</td>
<td align="center">25 / 15</td>
</tr>
<tr>
<td align="center">Windowed</td>
<td align="center">2 + 1</td>
<td align="center">18 / 13</td>
<td align="center">1.31 / 1.02</td>
<td align="center">49.24 / 46.22</td>
<td align="center">35 / 24</td>
</tr>
<tr>
<td align="center">Leaky</td>
<td align="center">2 + 1</td>
<td align="center">16 / 10</td>
<td align="center">1.31 / 1.00</td>
<td align="center">49.26 / 45.92</td>
<td align="center">35 / 25</td>
</tr>
<tr>
<td align="center">Dynamic</td>
<td align="center">2 + 1</td>
<td align="center">0.0083 / 0.019</td>
<td align="center">1.28 / 0.98</td>
<td align="center">47.88 / 45.23</td>
<td align="center">37 / 27</td>
</tr>
<tr>
<td align="center" rowspan="4">Transition probabilities</td>
<td align="center">Perfect</td>
<td align="center">2 + 0</td>
<td align="center">- / -</td>
<td align="center">0.90 / 0.39</td>
<td align="center">27.22 / 20.30</td>
<td align="center">56 / 71</td>
</tr>
<tr>
<td align="center">Windowed</td>
<td align="center">2 + 1</td>
<td align="center">18 / 20</td>
<td align="center">0.57 / 0.18</td>
<td align="center">9.46 / 4.86</td>
<td align="center">72 / 87</td>
</tr>
<tr>
<td align="center">Leaky</td>
<td align="center">2 + 1</td>
<td align="center">16 / 19</td>
<td align="center">0.47 / 0.15</td>
<td align="center">0 / 0</td>
<td align="center">77 / 89</td>
</tr>
<tr>
<td align="center">Dynamic</td>
<td align="center">2 + 1</td>
<td align="center">0.0130 / 0.013</td>
<td align="center">0.51 / 0.17</td>
<td align="center">3.88 / 2.53</td>
<td align="center">75 / 88</td>
</tr>
<tr>
<td align="center" colspan="2">Squires et al. (1976) model</td>
<td align="center">2 + 3</td>
<td align="center">- / -</td>
<td align="center">0.51 / -</td>
<td align="center">11.48 / -</td>
<td align="center">75 / -</td>
</tr>
<tr>
<td align="center" colspan="2">Kolossa et al. (2013) model</td>
<td align="center">2 + 6</td>
<td align="center">- / -</td>
<td align="center">- / 0.24</td>
<td align="center">- / 27.62</td>
<td align="center">- / 82</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>The table compares the fit of several models onto the dataset from Squires et al. (1976) and Kolossa et al. (2013). Values around the slash correspond to Squires / Kolossa data sets. The number of fitted parameters includes 2 linear transformations (scaling and offset) plus model-specific free parameters (from 0 to 6), see <xref ref-type="sec" rid="sec008">Methods</xref> for a description of the models. Best fitting parameter values are reported, excepted for the models corresponding to Squires et al. 1976 and Kolossa et al. (2013) (the values are reported in the original publications) and for perfect models that have no free parameter. Bayesian Information Criterion (BIC) was computed from the mean squared error (MSE) and <italic>n</italic> = 48 data points (one per pattern of 5 stimuli) in Squires and <italic>n</italic> = 24 in Kolossa (one per pattern of 4 stimuli). BIC values are reported relatively to the best model as difference in BIC values: ΔBIC. Smaller values of MSE indicate better goodness-of-fit and smaller ΔBIC values indicate better models.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>We tested the robustness of the local transition probability model with another dataset from Kolossa et al. (2013). The authors introduced noticeable differences in the original design by Squires et al.: stimuli were visual (instead of auditory) and subjects had to make a two-alternative button press for each item of the sequence (instead of listening quietly). Again, we could reproduce all qualitative and quantitative aspects of the data. Notably, we found almost the same best-fitting leak parameter (<italic>ω</italic> = 17 instead of 16). The BIC again favors the local transition probability model (see <xref ref-type="table" rid="pcbi.1005260.t001">Table 1</xref>), even when compared to the model proposed by Kolossa et al. (see <xref ref-type="sec" rid="sec008">Methods</xref>).</p>
</sec>
<sec id="sec005">
<title>Sequential effects in reaction times</title>
<p>Reaction time tasks submit subjects to long and purely random sequences of two items. Subjects are asked to press a dedicated button for each item, and response times typically vary with the recent history of button presses. We compared subjects' reaction times to the theoretical surprise levels computed from different leaky integration models in the same experiment. Huettel et al. (2002) were interested in the effects of streaks on reaction times and brain signals recorded with fMRI. Their data show that reaction times were slower for stimuli violating a streak (see <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4A and 4B</xref>) than for those continuing it. This was true both for repeating (XXXX<underline>Y</underline> vs. XXXX<underline>X</underline>) and alternating streaks (XYXY<underline>X</underline> vs. XYXY<underline>Y</underline>), with a correlation with the streak length: the longer the streak, the larger the difference between violation and continuation. Importantly, the violation vs. continuation difference in reaction times increased more steeply with the length of repeating streaks compared to alternating streaks. This corresponds to property #5 of the local transition probability model.</p>
<fig id="pcbi.1005260.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005260.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Tracking of statistical regularities and reaction times.</title>
<p>(A-B) Experimental data redrawn from Huettel et al. (2002) [<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>]. Subjects were presented with a purely random stream of two items. They had to press a key corresponding to the presented item as fast as possible. Reaction times are sorted depending on whether the local sequence of items followed a local streak of repeated or alternated items, and whether the last item continued or violated the preceding pattern. For instance, in XXXX<underline>Y</underline>, the last item violates a previous streak of four repeated items. (C) Experimental data redrawn from Cho et al. (2002) [<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>]. The task was similar to Huettel et al. (2002) but reaction times are now sorted based on all possible patterns of repetition (R) or alternation (A) across the five past stimuli. For instance, the pattern AAA<underline>R</underline> denotes that the current item is a repetition of the previous item, and that the four preceding stimuli all formed alternations (e.g. XYXY<underline>Y</underline>). (D-L) Theoretical surprise levels estimated in purely random sequences by three different local models. These local models differ only in the statistic they estimate. Their single free parameter is the leak of integration, it was fitted to each dataset. We report the regression <italic>R</italic><sup>2</sup> for these best parameters. Note that regressions include the data for both repetitions and alternations in the case of Huettel et al. Note that only a learning of transition probabilities predicts several aspects of the experimental data.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.g004" xlink:type="simple"/>
</fig>
<p>Importantly, this property is specific to a model that learns transition probabilities. A model that learns the frequency of alternations has identical expectations for repeating and for alternating streaks, because alternations and repetitions play symmetrical roles for this statistic. A model that learns the frequency of items has expectations in repeating streaks but not in alternation streaks. Indeed, as the streak length increases, the frequency of the repeated item increases but in alternating streaks, the frequency of either item remains similar. On the contrary, in a model that learns transition probabilities, expectations build up for both streak types by counting all possible transition types between successive items. In that case, an asymmetry emerges because repeating sequences offer twice the evidence about the current transition than do alternating sequences. For instance, in XXXXXXX, one may predict that the item following the last X should be another X, since six transitions X→X preceded without a single X→Y transition. In XYXYXYX, one can predict that the item following the last X should be a Y, since three transitions X→Y preceded without a single X→X transition. However, the ratio of evidence supporting the transition currently expected is stronger in the repeating sequence (6:0) compared to the alternating sequence (3:0).</p>
<p>One could argue that such an asymmetry is not a property of statistical learning but a simple consequence of motor constraints or motor priming. However, such a conclusion would be inconsistent with the EEG data recorded from passive subjects in Squires et al. study, in which the P300 difference between XXXX<underline>Y</underline> and XXXX<underline>X</underline> was also larger than between XYXY<underline>Y</underline> and XYXY<underline>X</underline>. In addition, Huettel et al. also recorded fMRI signals while participants performed the task in a scanner. Activity levels in several non-motor brain regions such as the insula and the inferior frontal gyrus showed the same sequential effects as the reaction times, again with a larger brain activation for violations of repetition patterns than for violations of alternation patterns.</p>
<p>Cho et al. (2002) were interested not only in the effect of the preceding number of repetitions and alternations on reaction times, but also in their order. To do so, they sorted reaction times based on all patterns of five consecutive stimuli (see <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4C</xref>). Each pattern contains four successive pairs, which can either be an alternation (denoted A) or a repetition (denoted R) of the same item. There are in total 2<sup>4</sup> = 16 possible patterns of repetition and alternation. Their analysis confirmed several effects already mentioned above, such as the effect of local frequency (e.g. RRR<underline>R</underline> vs. RRA<underline>R</underline> vs. RAA<underline>R</underline> vs. AAA<underline>R</underline>).</p>
<p>Their data also show clear evidence for property #4 of the local transition probability model: the same observations, in a different order, produce different expectations. Consider, for instance the sequences ARR<underline>R</underline>, RAR<underline>R</underline> and RRA<underline>R</underline>. The local frequency of R is the same in these three patterns since they each contain a single discrepant observation (A); yet, the order of the observations matters: reaction times are slower when the discrepant A was observed more recently. In the local transition probability model, it is due to the non-stationarity of the estimation, which weights recent observations more than remote ones.</p>
<p>This order effect could also be reproduced by a model that learns the frequency of alternation (see <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4I</xref>). However, this model predicts that surprise levels should be symmetrical for alternations and repetitions. This contradicts property #5, according to which expectations build up more rapidly for alternations than repetitions. The data conform to this property: reaction times for patterns ending with a repetition are lower than those ending with an alternation (see <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4C</xref>), similarly to surprise levels in the local transition probability model (see <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4L</xref>).</p>
<p>Interestingly, the local transition probability model also captures additional aspects of the data that are left unexplained by a model that learns the frequency of alternations. When patterns are ordered as in <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4C</xref>, reaction times show gradual increases over the first eight patterns and gradual decreases for the last eight. There are also local deviations from this global trend: it is particularly salient for patterns RAA<underline>R</underline> and ARA<underline>A</underline>. The local transition probability model reproduces these local deviations. A model learning the frequency of alternations also predicts local deviations, but for other patterns (RRAR and AARA). The observed deviations are thus specific to a learning of transition probabilities. RAA<underline>R</underline> corresponds to XXYX<underline>X</underline> where the last pair XX was already observed once, whereas in ARA<underline>R</underline>, which corresponds to XYYX<underline>X</underline>, the last pair XX was not observed. Surprise is therefore lower (and not higher, as predicted by a model learning alternation frequency) for RAA<underline>R</underline> than ARA<underline>R</underline>. A similar explanation holds for ARA<underline>A</underline> vs. RAA<underline>A</underline>.</p>
<p>Finally, note that a model that learns the frequency of items fails to reproduce many aspects of the data (see <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4F</xref>) since it is completely insensitive to repetition vs. alternation effects.</p>
<p>We obtained the results shown in <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4</xref> by fitting the leak parameter <italic>ω</italic> of each model. The best-fitting value for Huettel / Cho data was: <italic>ω =</italic> 8 / <italic>ω =</italic> 4 with a learning of stimulus frequency, <italic>ω =</italic> 6 / <italic>ω =</italic> 1 with alternation frequency, and <italic>ω =</italic> 6 / <italic>ω =</italic> 3 with transition probabilities. However, simulations using the leak value fitted to the independent dataset by Squires et al. (<xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3</xref>) led to the same qualitative conclusions. Thus, a single set of parameters may capture both data sets.</p>
</sec>
<sec id="sec006">
<title>Asymmetric perception of randomness</title>
<p>The asymmetry in expectation for alternation vs. repetition is probably the least trivial property of the local transition probability model (#5). This property is evidenced above in sequential effects and it entails a prediction in another domain: judgments of randomness should also be asymmetric. This prediction is confirmed: the human perception of randomness is notoriously asymmetric, as shown in particular by Falk &amp; Konold (1997) (see <xref ref-type="fig" rid="pcbi.1005260.g005">Fig 5A</xref>). Sequences with probabilities of alternations p(alt.) that are slightly larger than 0.5 are perceived as more random than they truly are. This is an illusion of randomness: in actuality, the least predictable sequence is when p(alt.) = 0.5, i.e. when the next item has the same probability of being identical or different from the previous one.</p>
<fig id="pcbi.1005260.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005260.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Asymmetric perception of randomness.</title>
<p>(A) Data redrawn from Falk (1975) and reported in [<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>]. Subjects were presented with various binary sequences of 21 stimuli. They were asked to rate the apparent randomness of each sequence. The range of perceived randomness was normalized between 0 and 1. Ratings were sorted based on the alternation frequency in the sequences. (B-D) Theoretical levels of entropy estimated by distinct local models. The entropy characterizes the unpredictability of a sequence. For each model, we generated random sequences differing in their alternation frequencies. For each sequence and model, we computed the estimated probability of the next stimulus of the sequence, given the preceding stimuli. We then converted these predictions into entropy levels and plotted the average for different values of the leak parameter of the model. Note that only a learning of transition probabilities predicts a slight asymmetry of perceived randomness.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.g005" xlink:type="simple"/>
</fig>
<p>This bias in the perception of randomness is actually rational from the viewpoint of the local transition probability model. In order to quantify the perceived randomness of a sequence in the local transition probability model, we estimated the unpredictability of the next outcome. This unpredictability is formalized mathematically by the notion of entropy. The resulting estimated entropy level was maximal for sequences with p(alt.) larger than 0.5 (see <xref ref-type="fig" rid="pcbi.1005260.g005">Fig 5D</xref>). This bias was all the more pronounced that fewer stimuli were taken into account in the estimation: a model with a stronger leak results in a larger bias. This aspect is specific to the local transition probability model. In contrast, a model that learns the frequency of alternation shows no bias because alternations and repetitions play symmetrical roles for such a model (see <xref ref-type="fig" rid="pcbi.1005260.g005">Fig 5C</xref>). On the other hand, a model that learns the frequency of items shows an extreme bias: the maximal entropy level is reached for p(alt.) = 1 (see <xref ref-type="fig" rid="pcbi.1005260.g005">Fig 5B</xref>). This is because when stimuli alternate, their observed frequencies are identical, closest to chance level (50%) from the point of view of an observer that focuses solely on item frequency.</p>
<p>To understand how the asymmetry emerges, one should note that, in the local transition probability model, expectations arise from both repeating transitions (XX and YY) and alternating transitions (XY and YX). High expectations arise when one transition type is much more frequent than the other. The estimated entropy therefore decreases when p(alt.) approaches 1, where alternating transitions dominate, and when p(alt.) approaches 0, where repeating transitions dominate. However, remember that stronger expectations arise from repetitions than alternations in the local transition probability model (property #5). Therefore, expectations are not symmetric with respect to p(alt.), but higher for p(alt.) &lt; 0.5 than p(alt.) &gt; 0.5, so that the ensuing estimated entropy peaks at a value of p(alt.) that is slightly higher than 0.5.</p>
<p>This asymmetry is also dampened, without being abolished, when the leaky integration parameter of the local transition probability model is weaker. Indeed, experimental evidence confirms that the difference in expectations arising from repeating and alternating transitions is more pronounced for shorter sequences (see the results from Huettel et al, <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4A and 4B</xref>).</p>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>We showed that learning non-stationary transition probabilities entails six properties. First, expectations derived from such a learning show effects of both the frequencies of items and their alternations because these statistics are specific aspects of transition probabilities (#1). Second, these effects emerge both globally and locally in the learning process because the inference is non-stationary (#2). Third, this non-stationarity also entails that local effects emerge even in purely random sequences (#3). Fourth, it depends on the exact order of observations within the local history (#4). Fifth, since the space of transition probabilities is more general than the frequencies of items and their alternations, the local transition probability model makes a non-trivial prediction, unaccounted for by simpler statistics: expectations build up more strongly from repetitions than from alternations (#5). Sixth, this asymmetry translates into a subjective illusion of randomness which is biased toward alternations (#6). We identified many signatures of expectations and their violation in human behavior (such as reaction times) and brain signals (measured by electrophysiology and fMRI) which conformed both qualitatively and quantitatively to these predictions. We therefore conclude that transition probabilities constitute a core building block of sequence knowledge in the brain, which applies to a variety of sensory modalities and experimental situations.</p>
<p>Early studies [<xref ref-type="bibr" rid="pcbi.1005260.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref016">16</xref>] proposed that the information provided by stimuli modulates reaction times within sequences [<xref ref-type="bibr" rid="pcbi.1005260.ref012">12</xref>]. According to the information theory framework, an observation is informative inasmuch it cannot be predicted [<xref ref-type="bibr" rid="pcbi.1005260.ref051">51</xref>]. In line with this information-theoretic approach, the local transition probability model quantifies the extent to which an observation deviates from the preceding ones. The central role of expectations in cognitive processes has also been put forward by the predictive coding [<xref ref-type="bibr" rid="pcbi.1005260.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref053">53</xref>] and the active inference [<xref ref-type="bibr" rid="pcbi.1005260.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref054">54</xref>] frameworks, and applied, for instance, to motor control [<xref ref-type="bibr" rid="pcbi.1005260.ref055">55</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref056">56</xref>] or reinforcement learning [<xref ref-type="bibr" rid="pcbi.1005260.ref057">57</xref>].</p>
<p>However, some have claimed that sequential effects in reaction times arise from low-level processes such as motor adaptation. For instance, Bertelson wrote in 1961 “one must thus admit that the shorter reaction times [for repetitions] cannot depend on something which must be learnt about the series of signals–unless one assumes that this learning is fast enough to be completed and give already its full effect on performance in the first 50 responses” [<xref ref-type="bibr" rid="pcbi.1005260.ref013">13</xref>]. In contrast, the local transition probability model shows that, with optimal statistical learning, sequence effects can arise from a very local integration: our fit of Squires et al. (1976) data suggests a leak factor <italic>ω</italic> of 16 stimuli, meaning that the weight of a given observation is reduced by half after 16 * ln(2) ≈ 11 observations. In addition, facilitation of reaction times is observed for both streaks of repetitions and streaks of alternations, which speaks against a pure motor interpretation [<xref ref-type="bibr" rid="pcbi.1005260.ref017">17</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref018">18</xref>]. Moreover, similar sequential effects are also observed in electrophysiological and fMRI measures of brain activity in the absence of any motor task. Therefore, although motor constraints may also contribute to reaction times fluctuations, a parsimonious and general explanation for sequential effects is that they arise from learned statistical expectations.</p>
<p>The non-stationary integration also explains why both local and global effects emerge and why local effects persist in the long run even within purely random sequences [<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>]. From the brain's perspective, the constant attempt to learn the non-stationary structure of the world could be a fundamental consequence of a general belief that the world can change at unpredictable times, as already suggested by others [<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>]. Many studies indeed show that the brain can perform non-stationary estimation and thereby efficiently adapt to changes in the environment [<xref ref-type="bibr" rid="pcbi.1005260.ref049">49</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref050">50</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref058">58</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref060">60</xref>]. Technically, the belief in a changing world can be captured in two different ways: either by the <italic>a priori</italic> likelihood of a sudden change (a.k.a. volatility) <italic>p</italic><sub>c</sub> in the exact dynamic belief model, or by the leaky integration factor <italic>ω</italic> in the approximate model. The present data do not suffice to separate those two possibilities. This is because the latter (leaky integration) is such a good approximation of the former that both are difficult to disentangle in practice. Leaky integration is a popular model in neuroscience because it seems easy to implement in biological systems [<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref058">58</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref061">61</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref062">62</xref>]. However, the dynamic belief model may not be less plausible given that neuronal populations have been proposed to represent and compute with full probability distributions [<xref ref-type="bibr" rid="pcbi.1005260.ref033">33</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref041">41</xref>]. Furthermore, only the full Bayesian model recovers an explicit probabilistic representation of change likelihood and change times. Several recent experimental studies suggest that the brain is indeed capable of estimating a hierarchical model of the environment, and that human subjects can explicitly report sudden changes in sequence statistics [<xref ref-type="bibr" rid="pcbi.1005260.ref060">60</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref063">63</xref>].</p>
<p>Our results suggest that, during sequence learning, the brain considers a hypothesis space that is more general than previously thought. We found that sequential effects in binary sequences are better explained by a learning of transition probabilities (a 2-dimensional hypothesis space) than of the absolute item frequencies or the frequency of their alternations (which are one-dimensional spaces). Importantly, all of these models have the same number of free parameters, so that the local transition probability model is more general without being more complex or less constrained. The critical difference lies in the content of what is learned (e.g. item frequencies vs. transition probabilities). More is learned in the latter case (a 2D space is larger than a 1D space) without resorting to any additional free parameter. The value of the learned statistic is not a free parameter, it is instead dictated by the sequence of observations and the assumptions of the model. In general, a Bayesian learner may consider a vast hypothesis space (see the many grammars used by Kemp and Tenenbaum [<xref ref-type="bibr" rid="pcbi.1005260.ref064">64</xref>]) and yet, as a model that attempts to capture human behavior, it may possess very few or even zero adjustable parameters.</p>
<p>An alternative to the full 2D transition-probability model would be to combine two learning processes: one for the frequency of items and one for the frequency of alternations. However, such a model introduces a new free parameter compared to the local transition probability model: the relative weight between the predictions based on the frequency of items and the predictions based on the frequency of alternations. In addition, the distinction between learning transition probabilities vs. the frequency of items and their alternations is not a simple change of viewpoint: the correspondence between the two is extremely non-linear as shown in <xref ref-type="fig" rid="pcbi.1005260.g001">Fig 1A</xref>. Learning the frequency of items and the frequency of alternations is therefore not only less parsimonious than learning transition probabilities, it is also genuinely different.</p>
<p>The difference between learning transition probabilities vs. the frequency of items and their alternation may have been overlooked in the past. However, the distinction is important since these learning strategies make distinct predictions about the asymmetry of expectations arising from repetitions and alternations. This asymmetry is a classical aspect of data, in particular response times [<xref ref-type="bibr" rid="pcbi.1005260.ref013">13</xref>]. In previous models, this asymmetry was simply assumed and incorporated as a prior [<xref ref-type="bibr" rid="pcbi.1005260.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>]. We show here, to our knowledge for the first time, how this asymmetry follows naturally from first principles (Bayes’ rule) in the local transition probability model. Moreover, our account is also unifying since it addresses not only sequential effects but also judgments of randomness.</p>
<p>We claim that the learning of transition probabilities is a core and general building block of sequence knowledge because we found supportive evidence in five representative datasets. There is also additional evidence from other fields. For instance, word segmentation in language relies on transition probabilities between syllables [<xref ref-type="bibr" rid="pcbi.1005260.ref065">65</xref>]. Moreover, neurons in the monkey inferior temporal cortex reduce their firing in direct proportion to the learned transition probabilities [<xref ref-type="bibr" rid="pcbi.1005260.ref066">66</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref067">67</xref>]. Ramachandran et al. (2016), in particular, present single-cell recordings suggesting that the expectation about the next item does not depend on its absolute frequency or the absolute frequency of the pair it forms with the previous item, but instead on the conditional probabilities of items learned with a covariance-based rule. Additional sources of evidence that human subjects learn transition probabilities is provided by studies of “repetition suppression” [<xref ref-type="bibr" rid="pcbi.1005260.ref068">68</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref069">69</xref>], choices in decision-making problems [<xref ref-type="bibr" rid="pcbi.1005260.ref070">70</xref>] and explicit reports of learned transition probabilities [<xref ref-type="bibr" rid="pcbi.1005260.ref060">60</xref>]. The study by Bornstein and Daw [<xref ref-type="bibr" rid="pcbi.1005260.ref070">70</xref>] in particular shows that humans can learn transition probabilities among 4 items. Our local transition probability model naturally extends from the binary case to a larger number of categories, a situation that is pervasive in every-day life. Learning only the frequency of items and of alternations becomes gradually inadequate when the number of items increases, because most environmental regularities are captured by various item-specific transition probabilities rather than absolute frequencies. For instance, the probability of imminent rain is typically high after a thunderstorm, but very low during a sunny day, and intermediate in case of strong wind. The learning of transition probabilities may even operate without awareness [<xref ref-type="bibr" rid="pcbi.1005260.ref071">71</xref>–<xref ref-type="bibr" rid="pcbi.1005260.ref073">73</xref>].</p>
<p>Our claim that a learning of transition probabilities accounts for a variety of experimental effects does not rule out the possibility that the brain also computes simpler statistics. Many studies report effects of item frequencies or alternation frequency. Electrophysiology in particular shows that these effects unfold across time and across brain circuits, as reflected in signals such as the mismatch negativity and the P300 [<xref ref-type="bibr" rid="pcbi.1005260.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref019">19</xref>]. In particular, Strauss et al. (2015, experiment 2) identified two distinct time windows in magneto-encephalographic recordings, during which the absolute frequency of items and the frequency of alternations, respectively, affected the human brain responses to simple sounds. However, in most studies, it is not clear whether such effects are particular cases of a general learning of transition probabilities or whether they are genuinely limited to item frequency or alternation frequency. Both hypotheses are indistinguishable in most studies because of their experimental design. Therefore, it is not clear for the moment whether different brain circuits are tuned to these different statistics and compute them in parallel [<xref ref-type="bibr" rid="pcbi.1005260.ref048">48</xref>], or whether most brain regions are equipped for the computation of transition probabilities. By contrast, it seems that more sophisticated building blocks of sequence knowledge, such as ordinal knowledge, chunking, algebraic patterns and tree structures are operated by specific brain circuits [<xref ref-type="bibr" rid="pcbi.1005260.ref047">47</xref>]. Future work should aim to incorporate these additional levels of representation to the local transition probability model, which we propose here as a minimal building block, likely to be duplicated in many brain regions and shared by humans and other animals alike.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Methods</title>
<p>We provide the MATLAB code for our ideal observer models to reproduce our simulations and figures: <ext-link ext-link-type="uri" xlink:href="https://github.com/florentmeyniel/MinimalTransitionProbsModel" xlink:type="simple">https://github.com/florentmeyniel/MinimalTransitionProbsModel</ext-link>. In particular the “trees” corresponding to <xref ref-type="fig" rid="pcbi.1005260.g003">Fig 3</xref> and simulated with the dynamic belief model are not shown in the article, but they can be easily generated with our code.</p>
<sec id="sec009">
<title>Formal description of the models</title>
<p>The models are “ideal observers”: they use Bayes’ rule to estimate the posterior distribution of the statistic they estimate, θ<sub>t</sub>, based on a prior on this statistic and the likelihood provided by previous observations, <italic>y</italic><sub><italic>1</italic>:<italic>t</italic></sub> (here, a sequence of Xs and Ys). Subscripts denote the observation number within a sequence.</p>
<disp-formula id="pcbi.1005260.e001">
<alternatives>
<graphic id="pcbi.1005260.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace width="0.15em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 1)</label>
</disp-formula>
<p>Different ideal observer models (M) estimate different statistics. The parameter θ can be the frequency of items, the frequency of alternations, or transition probabilities between items. The estimation of θ depends on the assumption of the ideal observer model: it can either consider that θ is fixed and must generate all the observations (“fixed belief models”) or that θ may change from one observation to the next (“dynamic belief models”). For all models, we use a prior distribution that is non-informative: all possible values of θ are considered with equal probability.</p>
<p>Note that a model estimating the frequency of alternations is equivalent to a model estimating the frequency of items after recoding of the stimuli as repetitions or alternations. Therefore, we only present below the derivation for the item frequency and transitions probabilities, in the case of both fixed belief and dynamic belief models.</p>
<sec id="sec010">
<title>Fixed belief models</title>
<p>For fixed belief, <bold>θ</bold> should not depend on the observation number. Therefore, the likelihood function can be decomposed as follows using the chain rule:
<disp-formula id="pcbi.1005260.e002">
<alternatives>
<graphic id="pcbi.1005260.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4.15em"/><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>…</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mn>1</mml:mn><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 2)</label>
</disp-formula></p>
<p>For models that estimate the frequency of items, the likelihood of a given observation depends only on the estimated frequency:
<disp-formula id="pcbi.1005260.e003">
<alternatives>
<graphic id="pcbi.1005260.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mi>p</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4.15em"/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>X</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 3)</label>
</disp-formula></p>
<p>Where θ<sub>X</sub> is the frequency of X, <italic>N</italic><sub>X</sub> and <italic>N</italic><sub>Y</sub> the numbers of X and Y in the sequence <italic>y</italic><sub>1:t</sub>. This likelihood is a Beta distribution whose parameters are <italic>N</italic><sub>X</sub> + 1 and <italic>N</italic><sub>Y</sub> + 1. In order to derive the posterior, the likelihood must be multiplied by the prior. Here, the prior is a non-informative, flat distribution: a Beta distribution with parameters [1, 1]. Since the product of two Beta distributions is also a Beta distribution in which the parameters are simply added, the posterior distribution has an analytical solution:
<disp-formula id="pcbi.1005260.e004">
<alternatives>
<graphic id="pcbi.1005260.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 4)</label>
</disp-formula></p>
<p>For models that estimate transition probabilities between consecutive stimuli, the likelihood of a given observation depends only on the estimated transition probabilities and the previous stimulus:
<disp-formula id="pcbi.1005260.e005">
<alternatives>
<graphic id="pcbi.1005260.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mi>p</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4.25em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 5)</label>
</disp-formula></p>
<p>Where θ denotes a vector of two transition probabilities θ = [θ<sub>X|Y</sub>, θ<sub>Y|X</sub>], and <italic>N</italic><sub>X|Y</sub> denotes the number of YX pairs in the sequence <italic>y</italic><sub>1:t</sub>. For simplicity, the first observation can be considered as arbitrary, so that p(<italic>y</italic><sub>1</sub>|θ) = 1/2. <xref ref-type="disp-formula" rid="pcbi.1005260.e005">Eq 5</xref> therefore corresponds to the product of two Beta distributions, with parameters corresponding to the transition counts plus one. The product of this likelihood and a uniform prior distribution results in an analytical solution:
<disp-formula id="pcbi.1005260.e006">
<alternatives>
<graphic id="pcbi.1005260.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 6)</label>
</disp-formula></p>
<p>For models that estimate frequency and transition probabilities, we distinguish between different ways of counting. The leaky integration was modeled with a free parameter: an exponential decay <italic>ω</italic> on the previous observations, i.e. a weight <italic>e</italic><sup><italic>–k</italic>/<italic>ω</italic></sup> for the <italic>k</italic>-th past stimulus. The perfect integration was modeled by counting all stimuli equally. For the sake of completeness, we also included a model that counts perfectly in a window of recent observations. The length of this window is a free parameter. Note that “perfect integration” is a special case of the window size being equal or larger than the number of stimuli in the sequence.</p>
<p>The posterior distribution can then be turned into the likelihood of the next stimulus using Bayes’ rule (again):
<disp-formula id="pcbi.1005260.e007">
<alternatives>
<graphic id="pcbi.1005260.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 7)</label>
</disp-formula></p>
<p>Note that for models that learn frequencies, θ is a single number (and not a vector, as for transition probabilities) and the conditional dependence on <italic>y</italic><sub><italic>t</italic></sub> can be ignored.</p>
</sec>
<sec id="sec011">
<title>Dynamic belief models</title>
<p>In dynamic belief models, <bold>θ</bold> may change from one observation to the next with probability <italic>p</italic><sub>c</sub>, which is the only free parameter of the model. The computation is tractable given the so-called Markov property of the generative process. If one knows <bold>θ</bold> at time <italic>t</italic>, then the next observation <italic>y</italic><sub><bold><italic>t</italic>+1</bold></sub> is generated with <bold>θ</bold><sub><bold><italic>t</italic>+1</bold></sub> = <bold>θ</bold><sub><italic>t</italic></sub> if no change occurred and with another value drawn from the prior distribution otherwise. Therefore, if one knows <bold>θ</bold><sub><italic>t</italic></sub>, previous observations are not needed to estimate <bold>θ</bold><sub><bold><italic>t</italic>+1</bold></sub>. Casting the generative process as a Hidden Markov Model (HMM) allows to compute the joint distribution of <bold>θ</bold> and observations iteratively, starting from the prior, and updating this distribution by moving forward in the sequence of observations:
<disp-formula id="pcbi.1005260.e008">
<alternatives>
<graphic id="pcbi.1005260.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 8)</label>
</disp-formula></p>
<p>The first term in the right hand side is the likelihood of the current observation. The first term within the integral is the joint distribution from the previous iteration. The last term captures the a priori changes in θ from one observation to the next: the probability that it stays the same is 1 – <italic>p</italic><sub>c</sub> and the probability of another value is <italic>p</italic><sub>c</sub> times the prior probability of θ for that particular value. This integral can be computed numerically by discretization on a grid. The posterior probability can be obtained by normalizing the joint distribution.</p>
<p>The posterior distribution can then be turned into the likelihood of the next stimulus, using Bayes’ rule (again) and the a priori changes in θ.</p>
<disp-formula id="pcbi.1005260.e009">
<alternatives>
<graphic id="pcbi.1005260.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005260.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="5.5em"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="5.5em"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(Eq 9)</label>
</disp-formula>
<p>Note that for the estimation of transition probabilities, θ has two dimensions. For the estimation of frequencies, the likelihood of <italic>y</italic><sub><italic>t</italic>+1</sub> given θ<sub><italic>t</italic>+1</sub> does not depend on the previous observation <italic>y</italic><sub><italic>t</italic></sub> so that <italic>y</italic><sub><italic>t</italic></sub> should be omitted on the right-hand side.</p>
</sec>
</sec>
<sec id="sec012">
<title>Summary of the experimental procedure in Squires et al.</title>
<p>Squires et al. (1976) presented 7 subjects with sequences of two auditory stimuli (pure tones of 1500 Hz and 1000 Hz, denoted X and Y) during electroencephalogram (EEG) recordings. In separate sessions, the sequences were generated randomly with p(X) equals to 0.5 (no bias) or 0.7 (biased condition). In the biased condition, p(Y) = 1 – p(X) = 0.3. Because X and Y play symmetrical roles, the authors present results for a virtual condition “p(X) = 0.3” which actually corresponds to analyzing the responses to item Y in the biased condition. Subjects were not told about these exact probabilities. The stimulus duration was 60 ms and the stimulus onset asynchrony was 1.3 s. Subjects were asked to count the number of X items silently and report their count after each block of 200 trials. They were presented in total and in each condition, with 800 to 1600 stimuli.</p>
<p>Squires et al. measured a P300 score for each stimulus. This score is a weighted combination of signals measured at central electrodes (Fz, Cz, Pz) and latencies corresponding to the N200, P300 and slow-wave. The weights derive from a discriminant analysis to separate optimally the signals elicited by rare and frequent patterns (XXXX<underline>Y</underline> vs. XXXX<underline>X</underline>). The average scores are reported in each condition, for all patterns of five stimuli terminated by X.</p>
</sec>
<sec id="sec013">
<title>Summary of the experimental procedure in Kolossa et al.</title>
<p>Kolossa et al. collected EEG data from 16 subjects who were presented with a stream of two visual stimuli (red or blue rectangles, denoted X and Y; the mapping was counterbalanced across participants). In separate blocks, stimuli were generated with p(X) = 0.5 (no bias condition) or 0.7 (biased condition). A virtual condition p(X) = 0.3 corresponds, as in Squires et al., to the response to item Y in the biased condition. Subjects were not told about these exact probabilities. Subjects completed 12 blocks of 192 stimuli with 6 blocks in a row for each condition. The order of conditions was counterbalanced across participants. The stimulus duration was 10 ms and the stimulus onset asynchrony was 1.5 s. Subjects were asked to press a dedicated button for each item as quickly and accurately as possible.</p>
<p>Kolossa et al. measured the P300 amplitude at electrode Pz. The exact latency of the measurement varied across trials and participants. The authors first identified subject-specific peak latencies for the difference between rare and frequent items in the biased condition. Then, in each trial they extracted the maximum value of the signal within a window of 120 ms centered on subject-specific peaks. P300 levels are reported for each condition, for all patterns of four stimuli terminated by X.</p>
</sec>
<sec id="sec014">
<title>Fitting procedure for Squires et al. and Kolossa et al. data</title>
<p>We generated three sequences of 200 stimuli with probabilities p(X) equal to 0.3, 0.5 and 0.7. For each sequence, we computed the inference of the hidden statistics for each observer model and different values of their free parameters (if any). We computed surprise, in bit of information, for each model and each stimulus in the sequences, as log<sub>2</sub>(p(<italic>y</italic><sub><italic>t</italic></sub>|<italic>y</italic><sub><bold>1:<italic>t</italic>–1</bold></sub>)), where p(<italic>y</italic><sub><italic>t</italic></sub>|<italic>y</italic><sub><bold>1:<italic>t</italic>–1</bold></sub>) is the likelihood of the actual observation. We sorted surprise levels by patterns of five stimuli terminated with X. We repeated this simulation 200 times and we averaged over simulations to reach stable results.</p>
<p>To compare our simulation with the data from Squires et al, we extracted their values from figure 1 in [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>]. For each model, we adjusted the offset and scaling to minimize the mean squared error (MSE) between simulated and experimental data. We repeated this procedure for different values of the free parameter <italic>ω</italic> in fixed belief models with leaky integration, and different values of <italic>p</italic><sub>c</sub> in dynamic belief models. For all models, we fitted the data only for patterns of 5 stimuli since shorter patterns are not independent from longer ones: they are weighted averages of the data obtained for longer patterns. Including shorter patterns would have thus inflated some aspects of the data. For instance, the effect of item frequency can be seen for all pattern lengths, including length 1, but by definition, the effect of alternations can be seen only in longer patterns. Therefore, including shorter patterns would have over-weighted the effect of global item frequency relatively to local alternations.</p>
<p>Our fitting procedure gives the same weight to all patterns of 5 stimuli, although rare patterns are more likely to be corrupted by noise in the experimental data. However, our results are robust to this choice and are replicated when using a weighted MSE, taking into account the expected frequency of patterns. We also checked that the grids of values used for <italic>p</italic><sub>c</sub> or <italic>ω</italic> were sufficiently dense around the global maxima, as shown in <xref ref-type="supplementary-material" rid="pcbi.1005260.s001">S1 Fig</xref>.</p>
<p>We replicated this procedure with the data from Kolossa et al., taking their values from figure 8 in [<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>]. The only difference was that we used patterns of length 4, as reported by Kolossa et al., instead of length 5 as Squires et al.</p>
<p>For comparison, we implemented the models previously proposed by Squires et al. and Kolossa et al. These models are fully described in the related articles [<xref ref-type="bibr" rid="pcbi.1005260.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005260.ref009">9</xref>]. In short, the model by Squires et al. is a weighted sum of three factors (the variables within brackets correspond to the notations by Squires et al.):</p>
<list list-type="bullet">
<list-item><p>the global frequency of X in the sequence (P),</p></list-item>
<list-item><p>the local number of X within each pattern of 5 stimuli (M),</p></list-item>
<list-item><p>an alternation score for each pattern of 5 stimuli (A).</p></list-item>
</list>
<p>The free parameters of this model are the relative weight of global frequency (P) vs. local frequency (M), the relative weight of global frequency (P) vs. patterns of alternations (A), and a decay factor for counting the local number of stimuli (M).</p>
<p>The model by Kolossa et al. is a sophistication of the model by Squires et al. It is a weighted sum of three factors, thought of as the output of digital filters computing probabilities. These filters correspond to:</p>
<list list-type="bullet">
<list-item><p>a count function for the occurrence of items with a short-term memory,</p></list-item>
<list-item><p>a count function for the occurrence of items with a long-term memory,</p></list-item>
<list-item><p>a count function for alternations.</p></list-item>
</list>
<p>This model includes six free parameters (we use the notations from Kolossa et al.): a decay factor for short-term memory (<italic>β</italic><sub>S</sub>), two normalized time constants for the dynamic long-term memory (<italic>τ</italic><sub>1</sub> and <italic>τ</italic><sub>2</sub>), the relative weight of item probabilities computed from short- and long-term memory (<italic>α</italic><sub>S</sub>), the relative weight of probabilities computed for items and their alternations (<italic>α</italic><sub>Δ</sub>), and a parameter capturing the subjective distortion of probabilities (<italic>γ</italic><sub>Δ,2</sub>).</p>
<p>We use the best-fitting values of the free parameters reported by Squires et al. and Kolossa et al. in their respective article.</p>
<p>To compare the fit provided by our models and by the models by Squires et al. and Kolossa et al., we used the Bayesian Information Criterion (BIC). The BIC favors the goodness-of-fit but penalizes model for their number of free parameters [<xref ref-type="bibr" rid="pcbi.1005260.ref074">74</xref>]. For maximum likelihood estimate of the model parameters and Gaussian residuals, BIC = <italic>n ·</italic> log(MSE) + <italic>k ·</italic> log(<italic>n</italic>), with <italic>n</italic> the number of fitted data points and <italic>k</italic> the number of free parameters in the model. Note that here, <italic>k</italic> counts the scaling and offset parameter to adjust the model's data and the experimental data, and the internal free parameters of the model (from 0 for fixed belief model with perfect integration, to 6 for the model by Kolossa et al.).</p>
</sec>
<sec id="sec015">
<title>Summary of the experimental procedure in Huettel et al.</title>
<p>Huettel et al. presented 14 subjects with a stream of two visual stimuli (a square and a circle) randomly generated with equal probability. The sequence length was 1800. Each stimulus was presented for 250 ms, and the stimulus onset asynchrony was 2 s. Subjects were asked to press a dedicated button for each item as quickly as possible. Subjects performed the experiment in an MRI scanner for functional recordings.</p>
</sec>
<sec id="sec016">
<title>Simulation of the results by Huettel et al.</title>
<p>We extracted the data by Huettel et al. from their figure 2 in [<xref ref-type="bibr" rid="pcbi.1005260.ref002">2</xref>]. We simulated these results using the fixed belief model with leaky integration. We fitted the leak constant to the data using a grid search. We replicated the simulation results with the dynamic belief model, using <italic>p</italic><sub>c</sub> = 0.019 for the estimation of frequencies and <italic>p</italic><sub>c</sub> = 0.167 for the estimation of transition probabilities, which are the best fitting values for Squires et al. data.</p>
<p>We generated a sequence of 10<sup>5</sup> stimuli with p(X) = 0.5. This large number of observations ensured stable simulation results. Another solution is to generate many short sequences. Both options actually yield similar results because of the limited horizon of the non-stationary estimations used here.</p>
<p>Similar to our fit of Squires et al., we computed posterior inferences and surprise levels. The difference was that surprise levels were sorted based on whether local patterns of stimuli were alternating or repeating, whether the last item violated or continued the pattern, and the length of the pattern (up to 8).</p>
</sec>
<sec id="sec017">
<title>Summary of the experimental procedure in Cho et al.</title>
<p>Cho et al. presented 6 subjects with a stream of two visual stimuli (a small and a large circle), generated randomly with equal probabilities. Subjects were asked to press a dedicated button for each item as quickly and accurately as possible. Each stimulus was presented until a response was made within a limit of 2 s. The next stimulus appeared after a delay of 0.8 s. Subjects performed 13 series of 120 trials (1560 stimuli in total) with a short break between series.</p>
</sec>
<sec id="sec018">
<title>Simulation of the results by Cho et al.</title>
<p>We extracted the data by Cho et al. from their figure 1B in [<xref ref-type="bibr" rid="pcbi.1005260.ref020">20</xref>]. We used the same simulation as for Huettel et al. The only difference being that surprise levels were sorted based on all patterns of alternations and repetitions formed by 5 stimuli.</p>
</sec>
<sec id="sec019">
<title>Summary of the experimental procedure in Falk et al.</title>
<p>Ass described in [<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>], Falk presented 219 subjects with sequences of 21 binary visual stimuli (“X” and “O”). The sequences had ratios of alternations ranging from 0.1 to 1 with 0.1 steps. The order of the sequences varied randomly across participants. Each sequence was printed as a row on a paper sheet: stimuli were therefore presented simultaneously. Subjects were asked to rate the apparent randomness of each sequence from 0 to 20, with the indication that this judgment should reflect the likelihood of the sequence having been generated by flipping a fair coin. Ratings were later rescaled between 0 and 1.</p>
</sec>
<sec id="sec020">
<title>Simulation of the results by Falk</title>
<p>We extracted the data by Falk from figure 1, condition “AR<sub>I</sub>” in [<xref ref-type="bibr" rid="pcbi.1005260.ref025">25</xref>]. Following the original experiment, we generated sequences of 21 binary stimuli with various probabilities of alternations. For each sequence, we computed the posterior inference of the hidden statistics and the prediction about the next stimulus (the 22<sup>th</sup>) given the previous ones, for each observer model. We used different values for their leak parameter. To quantify the “randomness” of the sequence, we computed the entropy of the prediction: H(p) = – p * log<sub>2</sub>(p) – (1 – p) * log<sub>2</sub>(1 – p). For each leak parameter and alternation frequency, we averaged over 10<sup>4</sup> sequences to reach stable results. Note that instead of focusing on the last prediction, one could average across successive predictions in each sequence. This alternative yields the same qualitative results as shown in <xref ref-type="fig" rid="pcbi.1005260.g005">Fig 5</xref>.</p>
</sec>
</sec>
<sec id="sec021">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005260.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Goodness-of-fit of electrophysiological data for different parameter values and models.</title>
<p>The plot shows the mean squared error of model fit, for different models, different values of their free parameter and different datasets. The inset shows a zoom around the best-fitting parameter. Note that different models have different inference styles: fixed belief with perfect integration within a window of observation, fixed belief with leaky integration and dynamic belief (presented in different columns) and they estimate different statistics: item frequency, alternation frequency and transition probabilities (presented as colored lines within each plot).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005260.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Sequential effects in reaction times predicted by the dynamic belief model.</title>
<p>This figure is similar to <xref ref-type="fig" rid="pcbi.1005260.g004">Fig 4</xref>. The only difference is that theoretical surprise levels are computed from the dynamic belief model (see <xref ref-type="sec" rid="sec008">Methods</xref>). The free parameter of the models, the <italic>a priori</italic> change probability for the estimated statistics, was selected independently, as the best fitting value for Squires et al. data.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005260.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Judgement of randomness predicted by the dynamic belief model.</title>
<p>This figure is similar to <xref ref-type="fig" rid="pcbi.1005260.g005">Fig 5</xref>. The only difference is that theoretical entropy levels are computed from the dynamic belief model (see <xref ref-type="sec" rid="sec008">Methods</xref>) using different a priori change probabilities for the estimated statistics.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005260.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>The models make qualitatively different predictions.</title>
<p>We estimated whether the models we consider make predictions that can be distinguished quantitatively from one another. We started by simulating the results of the Squires et al. experiment, using the best-fitting parameters of each model. As in Squires et al., each simulated data set contained 48 data points, corresponding to 16 patterns times 3 block types. We then estimated the ability of a given model to recover the predictions of another model with a leave-one out procedure. More precisely, we took the 48 simulated values of a given model and we adjusted the free parameters of another model to 47 of these simulated values, leaving one out. Given these fitted parameters, we then computed the prediction of the second model about the left-out point and we measured the error (the unsigned difference) compared with the original simulation. The bars show, for all pairs of models, the mean error and SEM across left-out points. The comparison of a model against itself yields no error (see arrows). Different models that make similar predictions should yield an error close to 0. This occurs in only one case (see dashed arrows): the predictions of a model learning perfectly a given statistic can be recovered almost exactly by a model learning the same statistic with a leaky integration. This is because the leak can be adjusted so as to approach a perfect integration. Critically, models that learn different statistics all produced quantitatively different predictions.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005260.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Comparison of the predictive accuracy of different models (Squires et al. dataset).</title>
<p>We estimated the predictive accuracy of each model using a leave-one out procedure. The parameters of each model (the offset and slope of the linear transformation from theoretical surprise to P300 data, and the leak of leaky integration models) were fitted to all data points but one. We then measured the error of a given model, as the distance between the actual left-out point and the value predicted by the model and its fitted parameters. Since Squires et al. report data for 16 patterns in 3 different block types, we repeated the leave-one out procedure 16 * 3 = 48 times. Bars show the mean error with SEM, and the numbers indicate the mean (and SEM) of the best-fitting leak parameter <italic>ω</italic> across left-out points. The model learning local estimates of transition probabilities achieved the best predictive accuracy, i.e. the smallest error.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005260.s006" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005260.s006" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supplementary Equations</title>
<p>(DOC)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Tobias Donner, Christophe Pallier and Angela Yu for useful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005260.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Squires</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Wickens</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Squires</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Donchin</surname> <given-names>E</given-names></name>. <article-title>The effect of stimulus sequence on the waveform of the cortical event-related potential</article-title>. <source>Science</source>. <year>1976</year>;<volume>193</volume>: <fpage>1142</fpage>–<lpage>1146</lpage>. <object-id pub-id-type="pmid">959831</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huettel</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Mack</surname> <given-names>PB</given-names></name>, <name name-style="western"><surname>McCarthy</surname> <given-names>G</given-names></name>. <article-title>Perceiving patterns in random series: dynamic processing of sequence in prefrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2002</year>;<volume>5</volume>: <fpage>485</fpage>–<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn841" xlink:type="simple">10.1038/nn841</ext-link></comment> <object-id pub-id-type="pmid">11941373</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bendixen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Roeber</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Schröger</surname> <given-names>E</given-names></name>. <article-title>Regularity Extraction and Application in Dynamic Auditory Stimulus Sequences</article-title>. <source>J Cogn Neurosci</source>. <year>2007</year>;<volume>19</volume>: <fpage>1664</fpage>–<lpage>1677</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocn.2007.19.10.1664" xlink:type="simple">10.1162/jocn.2007.19.10.1664</ext-link></comment> <object-id pub-id-type="pmid">18271740</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mars</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Debener</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gladwin</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Haggard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rothwell</surname> <given-names>JC</given-names></name>, <etal>et al</etal>. <article-title>Trial-by-Trial Fluctuations in the Event-Related Electroencephalogram Reflect Dynamic Changes in the Degree of Surprise</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>12539</fpage>–<lpage>12545</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2925-08.2008" xlink:type="simple">10.1523/JNEUROSCI.2925-08.2008</ext-link></comment> <object-id pub-id-type="pmid">19020046</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bekinschtein</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rohaut</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tadel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Naccache</surname> <given-names>L</given-names></name>. <article-title>Neural signature of the conscious processing of auditory regularities</article-title>. <source>Proc Natl Acad Sci</source>. <year>2009</year>;<volume>106</volume>: <fpage>1672</fpage>–<lpage>1677</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0809667106" xlink:type="simple">10.1073/pnas.0809667106</ext-link></comment> <object-id pub-id-type="pmid">19164526</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schröger</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Czigler</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ohira</surname> <given-names>H</given-names></name>. <article-title>Human visual system automatically encodes sequential regularities of discrete events</article-title>. <source>J Cogn Neurosci</source>. <year>2010</year>;<volume>22</volume>: <fpage>1124</fpage>–<lpage>1139</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocn.2009.21299" xlink:type="simple">10.1162/jocn.2009.21299</ext-link></comment> <object-id pub-id-type="pmid">19583466</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wacongne</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Changeux</surname> <given-names>J-P</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>A Neuronal Model of Predictive Coding Accounting for the Mismatch Negativity</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>3665</fpage>–<lpage>3678</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5003-11.2012" xlink:type="simple">10.1523/JNEUROSCI.5003-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22423089</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yaron</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hershenhoren</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Sensitivity to Complex Statistical Regularities in Rat Auditory Cortex</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>: <fpage>603</fpage>–<lpage>615</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.08.025" xlink:type="simple">10.1016/j.neuron.2012.08.025</ext-link></comment> <object-id pub-id-type="pmid">23141071</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kolossa</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fingscheidt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Wessel</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kopp</surname> <given-names>B</given-names></name>. <source>A</source> <article-title>Model-Based Approach to Trial-By-Trial P300 Amplitude Fluctuations</article-title>. <source>Front Hum Neurosci</source>. <year>2013</year>;<volume>6</volume>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lieder</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Garrido</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>. <article-title>Modelling Trial-by-Trial Changes in the Mismatch Negativity</article-title>. <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>, editor. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1002911</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002911" xlink:type="simple">10.1371/journal.pcbi.1002911</ext-link></comment> <object-id pub-id-type="pmid">23436989</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strauss</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sitt</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>J-R</given-names></name>, <name name-style="western"><surname>Elbaz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Azizi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Buiatti</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Disruption of hierarchical predictive coding during sleep</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2015</year>;<volume>112</volume>: <fpage>E1353</fpage>–<lpage>1362</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1501026112" xlink:type="simple">10.1073/pnas.1501026112</ext-link></comment> <object-id pub-id-type="pmid">25737555</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyman</surname> <given-names>R</given-names></name>. <article-title>Stimulus information as a determinant of reaction time</article-title>. <source>J Exp Psychol</source>. <year>1953</year>;<volume>45</volume>: <fpage>188</fpage>–<lpage>196</lpage>. <object-id pub-id-type="pmid">13052851</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bertelson</surname> <given-names>P</given-names></name>. <article-title>Sequential redundancy and speed in a serial two-choice responding task</article-title>. <source>Q J Exp Psychol</source>. <year>1961</year>;<volume>13</volume>: <fpage>90</fpage>–<lpage>102</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tune</surname> <given-names>SG</given-names></name>. <article-title>Response preferences: A review of some relevant literature</article-title>. <source>Psychol Bull</source>. <year>1964</year>;<volume>61</volume>: <fpage>286</fpage>–<lpage>302</lpage>. <object-id pub-id-type="pmid">14140335</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref015"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Rouanet H. Les modèles stochastiques d’apprentissage, Recherches et perspectives. Paris: Mouton; 1967</mixed-citation></ref>
<ref id="pcbi.1005260.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schvaneveldt</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Chase</surname> <given-names>WG</given-names></name>. <article-title>Sequential effects in choice reaction time</article-title>. <source>J Exp Psychol</source>. <year>1969</year>;<volume>80</volume>: <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kirby</surname> <given-names>NH</given-names></name>. <article-title>Sequential effects in two-choice reaction time: automatic facilitation or subjective expectancy?</article-title> <source>J Exp Psychol Hum Percept Perform</source>. <year>1976</year>;<volume>2</volume>: <fpage>567</fpage>–<lpage>577</lpage>. <object-id pub-id-type="pmid">1011006</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soetens</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>C</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>E</surname> <given-names>J</given-names></name>. <article-title>Expectancy or automatic facilitation? Separating sequential effects in two-choice reaction time</article-title>. <source>J Exp Psychol Hum Percept Perform</source>. <year>1985</year>;<volume>11</volume>: <fpage>598</fpage>–<lpage>616</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sommer</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Leuthold</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Soetens</surname> <given-names>E</given-names></name>. <article-title>Covert signs of expectancy in serial reaction time tasks revealed by event-related potentials</article-title>. <source>Percept Psychophys</source>. <year>1999</year>;<volume>61</volume>: <fpage>342</fpage>–<lpage>353</lpage>. <object-id pub-id-type="pmid">10089765</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cho</surname> <given-names>RY</given-names></name>, <name name-style="western"><surname>Nystrom</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Braver</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>PJ</given-names></name>, <etal>et al</etal>. <article-title>Mechanisms underlying dependencies of performance on stimulus history in a two-alternative forced-choice task</article-title>. <source>Cogn Affect Behav Neurosci</source>. <year>2002</year>;<volume>2</volume>: <fpage>283</fpage>–<lpage>299</lpage>. <object-id pub-id-type="pmid">12641174</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lungu</surname> <given-names>OV</given-names></name>, <name name-style="western"><surname>Wächter</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Willingham</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Ashe</surname> <given-names>J</given-names></name>. <article-title>Probability detection mechanisms and motor learning</article-title>. <source>Exp Brain Res</source>. <year>2004</year>;<volume>159</volume>: <fpage>135</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-004-1945-7" xlink:type="simple">10.1007/s00221-004-1945-7</ext-link></comment> <object-id pub-id-type="pmid">15258712</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perruchet</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cleeremans</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Destrebecqz</surname> <given-names>A</given-names></name>. <article-title>Dissociating the effects of automatic activation and explicit expectancy on reaction times in a simple associative learning task</article-title>. <source>J Exp Psychol Learn Mem Cogn</source>. <year>2006</year>;<volume>32</volume>: <fpage>955</fpage>–<lpage>965</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0278-7393.32.5.955" xlink:type="simple">10.1037/0278-7393.32.5.955</ext-link></comment> <object-id pub-id-type="pmid">16938039</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>Sequential effects: Superstition or rational behavior?</article-title> <source>Adv Neural Inf Process Syst</source>. <year>2008</year>;<volume>21</volume>: <fpage>1873</fpage>–<lpage>1880</lpage>. <object-id pub-id-type="pmid">26412953</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kareev</surname> <given-names>Y</given-names></name>. <article-title>Positive bias in the perception of covariation</article-title>. <source>Psychol Rev</source>. <year>1995</year>;<volume>102</volume>: <fpage>490</fpage>–<lpage>502</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Falk</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Konold</surname> <given-names>C</given-names></name>. <article-title>Making sense of randomness: Implicit encoding as a basis for judgment</article-title>. <source>Psychol Rev</source>. <year>1997</year>;<volume>104</volume>: <fpage>301</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hahn</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>PA</given-names></name>. <article-title>Perceptions of randomness: Why three heads are better than four</article-title>. <source>Psychol Rev</source>. <year>2009</year>;<volume>116</volume>: <fpage>454</fpage>–<lpage>461</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0015241" xlink:type="simple">10.1037/a0015241</ext-link></comment> <object-id pub-id-type="pmid">19348550</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>H</given-names></name>. <article-title>Perception of randomness: On the time of streaks</article-title>. <source>Cognit Psychol</source>. <year>2010</year>;<volume>61</volume>: <fpage>333</fpage>–<lpage>342</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogpsych.2010.07.001" xlink:type="simple">10.1016/j.cogpsych.2010.07.001</ext-link></comment> <object-id pub-id-type="pmid">20728080</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fawcett</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Fallenstein</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Higginson</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Houston</surname> <given-names>AI</given-names></name>, <name name-style="western"><surname>Mallpress</surname> <given-names>DEW</given-names></name>, <name name-style="western"><surname>Trimmer</surname> <given-names>PC</given-names></name>, <etal>et al</etal>. <article-title>The evolution of decision rules in complex environments</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year>;<volume>18</volume>: <fpage>153</fpage>–<lpage>161</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2013.12.012" xlink:type="simple">10.1016/j.tics.2013.12.012</ext-link></comment> <object-id pub-id-type="pmid">24467913</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Bhattacharyya</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>H</given-names></name>. <article-title>Latent structure in random sequences drives neural learning toward a rational bias</article-title>. <source>Proc Natl Acad Sci</source>. <year>2015</year>;<volume>112</volume>: <fpage>3788</fpage>–<lpage>3792</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1422036112" xlink:type="simple">10.1073/pnas.1422036112</ext-link></comment> <object-id pub-id-type="pmid">25775565</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deneve</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Reading population codes: a neural implementation of ideal observers</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>: <fpage>740</fpage>–<lpage>745</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/11205" xlink:type="simple">10.1038/11205</ext-link></comment> <object-id pub-id-type="pmid">10412064</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>. <article-title>An optimal estimation approach to visual perception and learning</article-title>. <source>Vision Res</source>. <year>1999</year>;<volume>39</volume>: <fpage>1963</fpage>–<lpage>1989</lpage>. <object-id pub-id-type="pmid">10343783</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>: <fpage>429</fpage>–<lpage>433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/415429a" xlink:type="simple">10.1038/415429a</ext-link></comment> <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hanks</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Churchland</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Roitman</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Probabilistic Population Codes for Bayesian Decision Making</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>60</volume>: <fpage>1142</fpage>–<lpage>1152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.09.021" xlink:type="simple">10.1016/j.neuron.2008.09.021</ext-link></comment> <object-id pub-id-type="pmid">19109917</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>. <article-title>Decision-theoretic models of visual perception and action</article-title>. <source>Vision Res</source>. <year>2010</year>;<volume>50</volume>: <fpage>2362</fpage>–<lpage>2374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2010.09.031" xlink:type="simple">10.1016/j.visres.2010.09.031</ext-link></comment> <object-id pub-id-type="pmid">20932856</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>. <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science</source>. <year>2011</year>;<volume>331</volume>: <fpage>83</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1195870" xlink:type="simple">10.1126/science.1195870</ext-link></comment> <object-id pub-id-type="pmid">21212356</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Girshick</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>: <fpage>926</fpage>–<lpage>932</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2831" xlink:type="simple">10.1038/nn.2831</ext-link></comment> <object-id pub-id-type="pmid">21642976</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diaconescu</surname> <given-names>AO</given-names></name>, <name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Weber</surname> <given-names>LAE</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kasper</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lomakina</surname> <given-names>EI</given-names></name>, <etal>et al</etal>. <article-title>Inferring on the intentions of others by hierarchical Bayesian learning</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>: <fpage>e1003810</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003810" xlink:type="simple">10.1371/journal.pcbi.1003810</ext-link></comment> <object-id pub-id-type="pmid">25187943</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref038"><label>38</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hoyer</surname> <given-names>PO</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <chapter-title>Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</chapter-title>. <source>Advances in Neural Information Processing Systems</source>. <publisher-name>MIT Press</publisher-name>; <year>2002</year></mixed-citation></ref>
<ref id="pcbi.1005260.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Mumford</surname> <given-names>D</given-names></name>. <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>. <source>J Opt Soc Am A Opt Image Sci Vis</source>. <year>2003</year>;<volume>20</volume>: <fpage>1434</fpage>–<lpage>1448</lpage>. <object-id pub-id-type="pmid">12868647</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title>. <source>Trends Neurosci</source>. <year>2004</year>;<volume>27</volume>: <fpage>712</fpage>–<lpage>719</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2004.10.007" xlink:type="simple">10.1016/j.tins.2004.10.007</ext-link></comment> <object-id pub-id-type="pmid">15541511</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>: <fpage>1432</fpage>–<lpage>1438</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1790" xlink:type="simple">10.1038/nn1790</ext-link></comment> <object-id pub-id-type="pmid">17057707</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>. <article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title>. <source>Trends Cogn Sci</source>. <year>2010</year>;<volume>14</volume>: <fpage>119</fpage>–<lpage>130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.01.003" xlink:type="simple">10.1016/j.tics.2010.01.003</ext-link></comment> <object-id pub-id-type="pmid">20153683</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>. <article-title>Neural coding of uncertainty and probability</article-title>. <source>Annu Rev Neurosci</source>. <year>2014</year>;<volume>37</volume>: <fpage>205</fpage>–<lpage>220</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-071013-014017" xlink:type="simple">10.1146/annurev-neuro-071013-014017</ext-link></comment> <object-id pub-id-type="pmid">25032495</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname> <given-names>ET</given-names></name>. <chapter-title>Probability Theory: The Logic of Science</chapter-title>. <publisher-name>Cambridge University Press</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Learning and inference in the brain</article-title>. <source>Neural Netw Off J Int Neural Netw Soc</source>. <year>2003</year>;<volume>16</volume>: <fpage>1325</fpage>–<lpage>1352</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nat Rev Neurosci</source>. <year>2010</year>;<volume>11</volume>: <fpage>127</fpage>–<lpage>138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2787" xlink:type="simple">10.1038/nrn2787</ext-link></comment> <object-id pub-id-type="pmid">20068583</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Meyniel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wacongne</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pallier</surname> <given-names>C</given-names></name>. <article-title>The Neural Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>88</volume>: <fpage>2</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2015.09.019" xlink:type="simple">10.1016/j.neuron.2015.09.019</ext-link></comment> <object-id pub-id-type="pmid">26447569</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref048"><label>48</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Wilder</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mozer</surname> <given-names>MC</given-names></name>. <chapter-title>Sequential effects reflect parallel learning of multiple environmental regularities</chapter-title>. In: <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schuurmans</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lafferty</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>CKI</given-names></name>, <name name-style="western"><surname>Culotta</surname> <given-names>A</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 22</source>. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2009</year>. pp. <fpage>2053</fpage>–<lpage>2061</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shannon</surname> <given-names>CE</given-names></name>. <article-title>A mathematical theory of communication</article-title>. <source>Bell Syst Tech J</source>. <year>1948</year>;<volume>27</volume>: <fpage>379</fpage>–<lpage>423</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Stern</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Dunson</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Vehtari</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <chapter-title>Bayesian Data Analysis</chapter-title>, <edition>Third Edition</edition>. <publisher-name>CRC Press</publisher-name>; <year>2013</year>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref051"><label>51</label><mixed-citation publication-type="other" xlink:type="simple">Rao RP, Sejnowski TJ. Predictive coding, cortical feedback, and spike-timing dependent plasticity. Statistical Theories of the Brain. 2000.</mixed-citation></ref>
<ref id="pcbi.1005260.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>A theory of cortical responses</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2005</year>;<volume>360</volume>: <fpage>815</fpage>–<lpage>836</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2005.1622" xlink:type="simple">10.1098/rstb.2005.1622</ext-link></comment> <object-id pub-id-type="pmid">15937014</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>. <article-title>Computational principles of movement neuroscience</article-title>. <source>Nat Neurosci</source>. <year>2000</year>;<volume>3</volume>: <fpage>1212</fpage>–<lpage>1217</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/81497" xlink:type="simple">10.1038/81497</ext-link></comment> <object-id pub-id-type="pmid">11127840</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>. <article-title>Optimality principles in sensorimotor control</article-title>. <source>Nat Neurosci</source>. <year>2004</year>;<volume>7</volume>: <fpage>907</fpage>–<lpage>915</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1309" xlink:type="simple">10.1038/nn1309</ext-link></comment> <object-id pub-id-type="pmid">15332089</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref055"><label>55</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Introduction to Reinforcement Learning</source>. <edition>1st ed.</edition> <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name>. <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>1214</fpage>–<lpage>1221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1954" xlink:type="simple">10.1038/nn1954</ext-link></comment> <object-id pub-id-type="pmid">17676057</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Rumsey</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Parikh</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Heasly</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>1040</fpage>–<lpage>1046</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3130" xlink:type="simple">10.1038/nn.3130</ext-link></comment> <object-id pub-id-type="pmid">22660479</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ossmy</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pfeffer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tsetsos</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Usher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Donner</surname> <given-names>TH</given-names></name>. <article-title>The Timescale of Perceptual Evidence Integration Can Be Adapted to the Environment</article-title>. <source>Curr Biol</source>. <year>2013</year>;<volume>23</volume>: <fpage>981</fpage>–<lpage>986</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2013.04.039" xlink:type="simple">10.1016/j.cub.2013.04.039</ext-link></comment> <object-id pub-id-type="pmid">23684972</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGuire</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Kable</surname> <given-names>JW</given-names></name>. <article-title>Functionally Dissociable Influences on Learning Rate in a Dynamic Environment</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>84</volume>: <fpage>870</fpage>–<lpage>881</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.10.013" xlink:type="simple">10.1016/j.neuron.2014.10.013</ext-link></comment> <object-id pub-id-type="pmid">25459409</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyniel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schlunegger</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>The Sense of Confidence during Probabilistic Learning: A Normative Account</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>e1004305</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004305" xlink:type="simple">10.1371/journal.pcbi.1004305</ext-link></comment> <object-id pub-id-type="pmid">26076466</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name>. <article-title>Probabilistic Decision Making by Slow Reverberation in Cortical Circuits</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>: <fpage>955</fpage>–<lpage>968</lpage>. <object-id pub-id-type="pmid">12467598</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Honey</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Thesen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Donner</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Silbert</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Carlson</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Devinsky</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Slow Cortical Dynamics and the Accumulation of Information over Long Timescales</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>: <fpage>423</fpage>–<lpage>434</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.08.011" xlink:type="simple">10.1016/j.neuron.2012.08.011</ext-link></comment> <object-id pub-id-type="pmid">23083743</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gallistel</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Krishan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>The perception of probability</article-title>. <source>Psychol Rev</source>. <year>2014</year>;<volume>121</volume>: <fpage>96</fpage>–<lpage>123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0035232" xlink:type="simple">10.1037/a0035232</ext-link></comment> <object-id pub-id-type="pmid">24490790</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kemp</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>The discovery of structural form</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2008</year>;<volume>105</volume>: <fpage>E10687</fpage>–<lpage>10692</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005260.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saffran</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Aslin</surname> <given-names>RN</given-names></name>, <name name-style="western"><surname>Newport</surname> <given-names>EL</given-names></name>. <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>. <year>1996</year>;<volume>274</volume>: <fpage>1926</fpage>–<lpage>1928</lpage>. <object-id pub-id-type="pmid">8943209</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ramachandran</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>CR</given-names></name>. <article-title>Statistical Learning of Serial Visual Transitions by Neurons in Monkey Inferotemporal Cortex</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>9332</fpage>–<lpage>9337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1215-14.2014" xlink:type="simple">10.1523/JNEUROSCI.1215-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25009266</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramachandran</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>CR</given-names></name>. <article-title>Prediction suppression in monkey inferotemporal cortex depends on the conditional probability between images</article-title>. <source>J Neurophysiol</source>. <year>2016</year>;<volume>115</volume>: <fpage>355</fpage>–<lpage>362</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00091.2015" xlink:type="simple">10.1152/jn.00091.2015</ext-link></comment> <object-id pub-id-type="pmid">26581864</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Summerfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Trittschuh</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Monti</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Mesulam</surname> <given-names>M-M</given-names></name>, <name name-style="western"><surname>Egner</surname> <given-names>T</given-names></name>. <article-title>Neural repetition suppression reflects fulfilled perceptual expectations</article-title>. <source>Nat Neurosci</source>. <year>2008</year>;<volume>11</volume>: <fpage>1004</fpage>–<lpage>1006</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2163" xlink:type="simple">10.1038/nn.2163</ext-link></comment> <object-id pub-id-type="pmid">19160497</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorovic</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>van Ede</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Maris</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>de Lange</surname> <given-names>FP</given-names></name>. <article-title>Prior Expectation Mediates Neural Adaptation to Repeated Sounds in the Auditory Cortex: An MEG Study</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>9118</fpage>–<lpage>9123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1425-11.2011" xlink:type="simple">10.1523/JNEUROSCI.1425-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21697363</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornstein</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Cortical and Hippocampal Correlates of Deliberation during Model-Based Decisions for Rewards in Humans</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003387</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003387" xlink:type="simple">10.1371/journal.pcbi.1003387</ext-link></comment> <object-id pub-id-type="pmid">24339770</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rose</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Haider</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Büchel</surname> <given-names>C</given-names></name>. <article-title>Unconscious detection of implicit expectancies</article-title>. <source>J Cogn Neurosci</source>. <year>2005</year>;<volume>17</volume>: <fpage>918</fpage>–<lpage>927</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0898929054021193" xlink:type="simple">10.1162/0898929054021193</ext-link></comment> <object-id pub-id-type="pmid">15969909</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Zuijen</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Simoens</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Paavilainen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Näätänen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tervaniemi</surname> <given-names>M</given-names></name>. <article-title>Implicit, Intuitive, and Explicit Knowledge of Abstract Regularities in a Sound Sequence: An Event-related Brain Potential Study</article-title>. <source>J Cogn Neurosci</source>. <year>2006</year>;<volume>18</volume>: <fpage>1292</fpage>–<lpage>1303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocn.2006.18.8.1292" xlink:type="simple">10.1162/jocn.2006.18.8.1292</ext-link></comment> <object-id pub-id-type="pmid">16859415</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Atas</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Faivre</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Timmermans</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Cleeremans</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kouider</surname> <given-names>S</given-names></name>. <article-title>Nonconscious Learning From Crowded Sequences</article-title>. <source>Psychol Sci</source>. <year>2014</year>;<volume>25</volume>: <fpage>113</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0956797613499591" xlink:type="simple">10.1177/0956797613499591</ext-link></comment> <object-id pub-id-type="pmid">24186918</object-id></mixed-citation></ref>
<ref id="pcbi.1005260.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name>. <article-title>Estimating the Dimension of a Model</article-title>. <source>Ann Stat</source>. <year>1978</year>;<volume>6</volume>: <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>