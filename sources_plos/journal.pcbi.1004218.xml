<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004218</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-02026</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Measuring Fisher Information Accurately in Correlated Neural Populations</article-title>
<alt-title alt-title-type="running-head">Measuring Fisher Information in Correlated Neural Populations</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Kanitscheider</surname>
<given-names>Ingmar</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Coen-Cagli</surname>
<given-names>Ruben</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kohn</surname>
<given-names>Adam</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
<xref rid="aff004" ref-type="aff"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Pouget</surname>
<given-names>Alexandre</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff005" ref-type="aff"><sup>5</sup></xref>
<xref rid="aff006" ref-type="aff"><sup>6</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Basic Neuroscience, University of Geneva, Geneva, Switzerland</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Center of Learning and Memory and Department of Neuroscience, The University of Texas at Austin, Austin, Texas, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Dominick Purpura Department of Neuroscience Albert Einstein College of Medicine, Bronx, New York, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Ophthalmology and Visual Sciences, Albert Einstein College of Medicine, Bronx, New York, United States of America</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, United States of America</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Gatsby Computational Neuroscience Unit, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname>
<given-names>Matthias</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: IK RCC AK AP. Performed the experiments: IK RCC. Analyzed the data: IK RCC. Wrote the paper: IK RCC AK AP. Designed the software used in analysis: IK RCC.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ruben.coencagli@unige.ch</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>1</day>
<month>6</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>6</issue>
<elocation-id>e1004218</elocation-id>
<history>
<date date-type="received">
<day>7</day>
<month>11</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>2</day>
<month>3</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Kanitscheider et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004218" xlink:type="simple"/>
<abstract>
<p>Neural responses are known to be variable. In order to understand how this neural variability constrains behavioral performance, we need to be able to measure the reliability with which a sensory stimulus is encoded in a given population. However, such measures are challenging for two reasons: First, they must take into account noise correlations which can have a large influence on reliability. Second, they need to be as efficient as possible, since the number of trials available in a set of neural recording is usually limited by experimental constraints. Traditionally, cross-validated decoding has been used as a reliability measure, but it only provides a lower bound on reliability and underestimates reliability substantially in small datasets. We show that, if the number of trials per condition is larger than the number of neurons, there is an alternative, direct estimate of reliability which consistently leads to smaller errors and is much faster to compute. The superior performance of the direct estimator is evident both for simulated data and for neuronal population recordings from macaque primary visual cortex. Furthermore we propose generalizations of the direct estimator which measure changes in stimulus encoding across conditions and the impact of correlations on encoding and decoding, typically denoted by <italic>I<sub>shuffle</sub></italic> and <italic>I<sub>diag</sub></italic> respectively.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>A central problem in systems neuroscience is to understand how the activity of neural populations is mapped onto behavior. Neural responses in sensory areas vary substantially upon repeated presentations of the same stimulus, and this limits the reliability with which two similar stimuli can be discriminated by any read-out of neural activity. Fisher information provides a quantitative measure of the reliability of the sensory representation, and it has been used extensively to analyze neural data. Traditional methods for quantifying Fisher information rely on decoding neural activity; however, optimizing a decoder requires larger amounts of data than available in typical experiments, and as a result decoding-based estimators systematically underestimate information. Here we introduce a novel estimator that can accurately determine information with far less data, and that runs orders of magnitude faster. The estimator is based on analytical calculation, and corrects the bias that arises when estimating information directly from limited data. The analytical guarantee of an unbiased estimator and its computational simplicity will allow experimentalists to compare coding reliability across behavioral conditions and monitor it over time.</p>
</abstract>
<funding-group>
<funding-statement>AP was supported by a grant from the Swiss National Science Foundation (31003A_143707) and a grant from the Simons Foundation. RCC was supported by a fellowship from the Swiss National Science Foundation (PAIBA3-145045). AK was supported by a grant from the NIH (EY016774). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="2"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data and matlab code to reproduce the results of the paper are available through the CRCNS public repository: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6080/K0PK0D3B" xlink:type="simple">http://dx.doi.org/10.6080/K0PK0D3B</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The advent of technical advances like multi-electrode recordings and calcium imaging allows the simultaneous recording of an ever increasing number of neurons. The availability of this data allows us to explore not only the qualitative properties of the neural code but also the reliability of coding.</p>
<p>One challenge of assessing coding reliability from neural recordings is that the number of available trials is typically quite limited. In contrast to the downstream circuitry, which potentially had a lifetime of experience to learn the statistics of neural responses, an experimenter recording neural activity only has finite data both to fit the response statistics and to assess the coding reliability. This requires efficient methods to achieve an accurate estimate of reliability with as few trials as possible.</p>
<p>In the frequently considered case of population coding for a continuous stimulus variable, a common approach to quantifying coding reliability is to assess how well a decoder of population patterns of activity can discriminate between two slightly different stimulus values (e.g. [<xref rid="pcbi.1004218.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1004218.ref003" ref-type="bibr">3</xref>]). The discrimination threshold (i.e. the smallest difference between two stimuli that can be correctly classified say 80% of the time) is determined by the variance of the decoder’s estimate for a fixed stimulus value. A well-known result of information theory, the Cramer-Rao bound, specifies that the optimal decoder variance is larger than or equal to the inverse of the Fisher information [<xref rid="pcbi.1004218.ref004" ref-type="bibr">4</xref>,<xref rid="pcbi.1004218.ref005" ref-type="bibr">5</xref>]. Therefore Fisher information quantifies the amount of information that can be extracted by the ideal observer (or, equivalently, an optimal decoder).</p>
<p>In this paper, we focus on estimating linear Fisher information—the information that can be extracted by the locally optimal linear estimator, i.e. a linear estimator optimized to the response statistics around a specific stimulus value [<xref rid="pcbi.1004218.ref001" ref-type="bibr">1</xref>]. Linear Fisher information is a lower bound on Fisher information, and captures the fraction of the total information contained in the trial-averaged responses which can be extracted without further non-linear processing. For example, a population of V1 neurons typically has substantial linear information about orientation, since its trial-averaged responses, the tuning curves, depend on orientation. V1 neurons also encode information about faces. This information, however, requires sophisticated non-linear processing to be extracted, and hence linear Fisher information will be low. For such a stimulus one would expect to measure higher linear Fisher information in higher visual areas than in V1 [<xref rid="pcbi.1004218.ref006" ref-type="bibr">6</xref>].</p>
<p>The linear Fisher information of a given population is determined by its tuning curves and covariance matrix (Materials and Methods, Section 1):
<disp-formula id="pcbi.1004218.e001">
<alternatives>
<graphic id="pcbi.1004218.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <bold>f</bold>(<italic>θ</italic>) is the vector of tuning curves with entries <italic>f</italic><sub><italic>i</italic></sub>(<italic>θ</italic>), <italic>i</italic> = 1.<italic>N</italic>, the prime denotes derivation with respect to the stimulus value <italic>θ</italic>, and ∑<sub><italic>ij</italic></sub> is the noise covariance matrix [<xref rid="pcbi.1004218.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004218.ref008" ref-type="bibr">8</xref>]. However, accurately determining tuning curves and covariance matrices from finite data is difficult. As shown in [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>], only the part of the covariance matrix proportional to <bold>f</bold>′(<italic>θ</italic>)<bold>f</bold>′(<italic>θ</italic>)<sup><italic>T</italic></sup>—termed differential correlations—limits Fisher information in large populations. Differential correlations can be extremely small and therefore small errors in estimating correlations can have a huge impact on information. As a result, measurement errors in <bold>f</bold>′ and Σ lead to large biases in the estimate of <italic>I</italic>.</p>
<p>There is at least one method for dealing with these biases. Moreno et al. [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>] showed recently that a lower bound on linear Fisher information can be obtained with a cross-validated decoder even when differential correlations are present. In this approach (inspired by [<xref rid="pcbi.1004218.ref001" ref-type="bibr">1</xref>]), the data are split into a training set which is used to train a decoder, and a validation set which measures information by assessing the reliability of the decoder. As we show here, this method generally underestimates the true information, and will have a small bias only when the number of trials is much larger than the number of neurons. This is because a cross-validated decoder trained on finite data is typically suboptimal, resulting in a higher variance on the test set than the optimal decoder. This is a serious problem for experimental data, as the number of trials is rarely large enough to prevent biases with this approach.</p>
<p>In this paper, we show that for small number of trials, there is a better alternative to estimating Fisher information than decoding. The information can be estimated directly from Eq (<xref rid="pcbi.1004218.e001" ref-type="disp-formula">1</xref>), based on the empirically measured tuning curves and covariance matrix. The key is to correct Fisher information for biases that are introduced by computing nonlinear functions of the tuning curves and covariance matrix estimated from limited data. We provide analytical expressions, and corrections, for the biases and show that the resulting bias-corrected estimator is much more accurate than the decoding method for a fixed number of trials while being much faster to compute. Furthermore, we provide a closed-form expression for the variance of the estimator. We illustrate the results on both synthetic data and data recorded in primate visual cortex.</p>
<p>Decoding methods are also often used to measure changes in the reliability of neural codes between conditions [<xref rid="pcbi.1004218.ref010" ref-type="bibr">10</xref>] and to assess information loss due to suboptimal readout [<xref rid="pcbi.1004218.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004218.ref011" ref-type="bibr">11</xref>,<xref rid="pcbi.1004218.ref012" ref-type="bibr">12</xref>]. We show that in these cases too, a bias-corrected direct estimate of information is often better than the ones obtained with cross-validated decoding. Likewise, this approach works well for estimating <italic>I</italic><sub><italic>shuffle</italic></sub>, the information in a data set in which responses have been shuffled across trials, and <italic>I</italic><sub><italic>diag</italic></sub>, the information recovered by using a factorized decoder, i.e., a decoder that ignores correlations.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>In a typical discrimination task the subject is asked to distinguish two similar stimuli, <italic>θ</italic><sup>+</sup> = <italic>θ</italic> + <italic>dθ</italic> and <italic>θ</italic><sup>−</sup> = <italic>θ</italic>−<italic>dθ</italic>. To measure Fisher information, a measure of discriminability, we consider neural responses to the two stimuli and estimate the performance of an optimal unbiased linear decoder to classify the stimulus. Fisher information is the inverse of the variance of the estimate of the stimulus based on this optimal linear decoder (see <xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1A</xref>; details are provided in Materials and Methods Section 1).</p>
<fig id="pcbi.1004218.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Stimulus decoding and Fisher information.</title>
<p>(<bold>a</bold>) Left: In a typical experiment, the responses of a population of neurons are recorded simultaneously while a visual stimulus is presented. Middle: Population responses (<italic>N</italic> = 2 in the cartoon) to two different stimulus values (orange vs. green symbols) are collected over many trials. Right: The optimal decoding weights,<italic>w</italic><sub><italic>i</italic></sub>, are applied to the population response, <bold>r</bold>, to obtain an estimate of the stimulus, <inline-formula id="pcbi.1004218.e002"><alternatives><graphic id="pcbi.1004218.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e002" xlink:type="simple"/><mml:math display="inline" id="M2" overflow="scroll"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. Linear Fisher information corresponds to the inverse of the variance of such estimates, across trials where the same stimulus was presented. (<bold>b</bold>) Ordinate: Fisher information in a population of <italic>N</italic> = 50 model neurons, estimated by a linear decoder as illustrated in (<bold>a</bold>), using cross-validation with early stopping. Abscissa: number of trials per stimulus condition. We ran 200 experiments for each trial. The top line is the information estimated from the training set; the bottom line is the information estimated from the validation set; the two will converge asymptotically. The dashed line is the true information value in the simulated population. The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>c</bold>) Fisher information obtained by directly estimating the tuning curves and covariance, and then applying Eq (<xref rid="pcbi.1004218.e030" ref-type="disp-formula">11</xref>). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>d</bold>) Similar to (<bold>c</bold>), but after correcting for the estimation bias, according to Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed analytically using Eq (<xref rid="pcbi.1004218.e045" ref-type="disp-formula">19</xref>).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g001" position="float" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Summary of the model used for simulations</title>
<p>We first consider simulated responses from <italic>N</italic> neurons to the two discrimination stimuli, which are each repeated for <italic>T</italic> trials. Responses were generated using a model of orientation-selective units, such as those found in primary visual cortex. Inputs to the model were Gabor images corrupted by white pixel noise. Model neurons were represented by Gabor linear filters, whose output was half-rectified, and further corrupted by independent Poisson noise to produce realistic response variability. This is a doubly stochastic model, with part of the variability induced by image pixel noise, and part due to the Poisson step. Due to image noise and filter overlap, the network contains noise correlations which decay with tuning similarity, consistent with a wealth of experimental data [<xref rid="pcbi.1004218.ref013" ref-type="bibr">13</xref>]. We generated synthetic population responses from a network with <italic>N</italic> = 50 neurons (except where noted). Each simulated experiment comprised <italic>T</italic> trials per orientation. The number of trials varied between 50 and 4000, and for each <italic>T</italic> we ran 200 experiments. We computed the ground truth information for the model using the analytical expressions (Eq (<xref rid="pcbi.1004218.e001" ref-type="disp-formula">1</xref>)) for the tuning curves and noise covariances. Further details about the model and simulations are provided in Materials and Methods Section 8.</p>
</sec>
<sec id="sec004">
<title>Measuring Fisher information with cross-validated decoding</title>
<p>Measuring Fisher information with cross-validated decoding requires splitting the data in a training and validation set. The training set is used to find a good estimate for the optimal decoder, while the validation set estimates its performance. Unless the number of trials available is much larger than the number of neurons, this method tends to overfit the training set, which in turn leads to severe underestimation of Fisher information for the validation set. A straightforward method to address this is early stopping. Early stopping splits the data in three sets, the training, test and validation set. It then proceeds by performing gradient descent on the training set while monitoring the error on the test set. As soon as the error in the test set starts increasing, the training is stopped. The Fisher information is then taken to be the inverse variance of the decoder’s estimates in the validation set (see <xref rid="sec011" ref-type="sec">Materials and Methods</xref> section 1, for details).</p>
<p>In <xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1B</xref> we evaluate the performance of the decoder on artificial data obtained from the model described above. We see that cross-validated decoding using early stopping underestimates information (dashed line) substantially unless the number of trials is much larger than the number of neurons (e.g., by a factor of 25 to reach 90% of the true information; see <xref rid="sec011" ref-type="sec">Materials and Methods</xref>, Section 8). The reason is residual overfitting: for a small number of trials, the best-performing decoder in the training set will still be suboptimal and therefore underestimate information in the validation set.</p>
</sec>
<sec id="sec005">
<title>Measuring Fisher information using direct estimation</title>
<p>An estimate of Fisher information can be obtained directly from Eq (<xref rid="pcbi.1004218.e001" ref-type="disp-formula">1</xref>), using the empirical covariance matrix and an estimate of the tuning curve derivative obtained from the difference in mean responses of the two presented stimuli. However, this naïve estimate of Fisher information substantially overestimates the true Fisher information (<xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1C</xref>).</p>
<p>The reason for this overestimation is that the expression for Fisher information (Eq (<xref rid="pcbi.1004218.e001" ref-type="disp-formula">1</xref>)) has a non-linear dependence both on the covariance matrix and on the tuning curve derivative, since the former is inverted and the latter is squared. Even though the empirical covariance matrix and difference in mean responses are unbiased estimators, the bias is reintroduced by the non-linear transformations. Consider for instance the simple case of a Gaussian variable <italic>X</italic> with true mean <italic>μ</italic> and variance <italic>σ</italic><sup>2</sup>. If we collect <italic>T</italic> measurements, <italic>x</italic><sub>1…<italic>T</italic></sub>, then the estimate of the mean <inline-formula id="pcbi.1004218.e003"><alternatives><graphic id="pcbi.1004218.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e003" xlink:type="simple"/><mml:math display="inline" id="M3" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is unbiased, and has a variance <inline-formula id="pcbi.1004218.e004"><alternatives><graphic id="pcbi.1004218.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e004" xlink:type="simple"/><mml:math display="inline" id="M4" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.10em"/><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (across several experiments, each with <italic>T</italic> measurements). Suppose now we are interested in estimating the square, <italic>μ</italic><sup>2</sup>. Then the naïve estimator, <inline-formula id="pcbi.1004218.e005"><alternatives><graphic id="pcbi.1004218.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e005" xlink:type="simple"/><mml:math display="inline" id="M5" overflow="scroll"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, is biased by the basic fact that <inline-formula id="pcbi.1004218.e006"><alternatives><graphic id="pcbi.1004218.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e006" xlink:type="simple"/><mml:math display="inline" id="M6" overflow="scroll"><mml:mrow><mml:mo>〈</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.10em"/><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. The bias vanishes if <italic>T</italic> is large enough, but can be substantial when based on limited measurements.</p>
<p>Fortunately it is possible to calculate the bias analytically and correct for it, if one assumes Gaussian response variability (but the results are robust to realistic violations of this assumption, as we illustrate below) and has more trials than neurons (more exactly, the empirical covariance matrix is invertible). In this case, the sampling distribution of the empirical covariance matrix is given by the Wishart distribution and its inverse by the inverse Wishart distribution. The bias of the inverse Wishart distribution is well-known [<xref rid="pcbi.1004218.ref014" ref-type="bibr">14</xref>]. The quadratic appearance of the tuning curve derivative introduces an additional bias. Once an analytical expression is derived for such biases, they can also be easily corrected (see <xref rid="sec011" ref-type="sec">Materials and Methods</xref>, Section 3 for full derivation), yielding an unbiased estimate of Fisher information:
<disp-formula id="pcbi.1004218.e007">
<alternatives>
<graphic id="pcbi.1004218.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e007" xlink:type="simple"/>
<mml:math display="block" id="M7" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <bold>μ</bold> and <italic>S</italic> represent the empirical mean and covariance, respectively.</p>
<p>The variance of the bias-corrected estimator can also be calculated analytically (Materials and Methods, Section 4, Eq (<xref rid="pcbi.1004218.e045" ref-type="disp-formula">19</xref>); <xref rid="pcbi.1004218.s002" ref-type="supplementary-material">S1 Fig</xref>), and can be used to obtain error bars. The expression for the variance shows that it diverges if <italic>T</italic> = (<italic>N</italic> + 5) / 2; furthermore the empirical covariance matrix is invertible with probability 1 only if <italic>T</italic> &gt; (<italic>N</italic> + 2) / 2, and therefore direct estimation cannot be used for smaller values of <italic>T</italic>.</p>
<p>The performance of the bias-corrected estimator can be seen in <xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1D</xref>. It closely approximates the true information, even for small numbers of trials, and is on average unbiased. As expected, the variance of the estimate increases as the number of trials decreases. In <xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2A, we</xref> compare the expected squared error (and the corresponding relative error, i.e. the ratio between estimation error and true value) of the cross-validated decoding method with the bias-corrected direct estimator. For all simulations illustrated (<italic>T</italic> &gt; <italic>N</italic>), the direct estimator is more reliable than the decoding estimate. For instance, with 250 trials, the bias-corrected estimate is within 11% of the true information while the decoder estimate is within 24%. To achieve the same level of accuracy as the bias-corrected method for 250 trials, the decoder estimate requires 1000 trials. <xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2B</xref> shows that the improvement of the bias-corrected estimator over decoding is roughly independent of the population size, and increases with the number of trials between <italic>T</italic> = <italic>N</italic> and <italic>T</italic> = 5 <italic>N</italic>. For a smaller number of trials, in the range (<italic>N</italic> + 5) / 2 &lt; <italic>T</italic> &lt; <italic>N</italic>, the bias-corrected estimator can be calculated but it may be less accurate than the decoding estimate (<xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2C and 2D</xref>).</p>
<fig id="pcbi.1004218.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Comparison of the estimation errors of the decoder and the direct bias-corrected method.</title>
<p>(<bold>a</bold>) Mean squared error (MSE) of the cross-validated decoder-based estimate (CV decoder, red) and the bias-corrected direct estimator (BC estimator, blue) calculated from the estimates of <xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1</xref> (<italic>N</italic> = 50 neurons). The ordinate axis on the right indicates the corresponding relative error, namely <inline-formula id="pcbi.1004218.e008"><alternatives><graphic id="pcbi.1004218.e008g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e008" xlink:type="simple"/><mml:math display="inline" id="M8" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow></mml:msqrt></mml:mrow><mml:mo>/</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. (<bold>b</bold>) Relative error for CV decoder (red) and BC estimator (blue), for different population sizes (abscissa). The number of trials was set to 1, 2, or 5 times the number of neurons (indicated at the top of each panel). (<bold>c</bold>) Relative error for CV decoder (right) and BC estimator (left), for different population sizes (abscissa) and numbers of trials expressed as proportion of the number of neurons (ordinate). (<bold>d</bold>) Log-ratio of relative errors for BC vs. CV from panel (<bold>c</bold>). The black contour separates cases in which the CV estimator is more accurate than BC (warm colors) from cases in which BC is more accurate (cold colors).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g002" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Assessing context-dependent encoding and decoding</title>
<p>A broad class of experimental questions involves estimating the information that a decoder which is trained in one experimental condition can extract in a different experimental condition. An important example is whether changes in the sensory representation (i.e. the encoding stage) cause behavioral changes following experimental manipulations such as the allocation of attention [<xref rid="pcbi.1004218.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1004218.ref015" ref-type="bibr">15</xref>], perceptual learning [<xref rid="pcbi.1004218.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1004218.ref017" ref-type="bibr">17</xref>] or adaptation [<xref rid="pcbi.1004218.ref018" ref-type="bibr">18</xref>,<xref rid="pcbi.1004218.ref019" ref-type="bibr">19</xref>]. Another example is whether the representation of orientation in primary visual cortex is invariant to image contrast; that is whether a decoder specialized for one contrast level can extract all the information from population responses to another contrast level [<xref rid="pcbi.1004218.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004218.ref003" ref-type="bibr">3</xref>].</p>
<p>One way to test for context-dependent encoding and decoding is to train a decoder on data collected before the manipulation and then compare its performance on validation data collected before vs. after the manipulation. However, this approach leads to an underestimation of information, for the reasons discussed above. We propose instead a direct, unbiased estimator for the general case of a decoder trained on dataset A and tested on dataset B (analogous to the ‘unfaithful model’ discussed in [<xref rid="pcbi.1004218.ref011" ref-type="bibr">11</xref>]). The optimal decoding weights for dataset A are [<xref rid="pcbi.1004218.ref020" ref-type="bibr">20</xref>]</p>
<disp-formula id="pcbi.1004218.e009">
<alternatives>
<graphic id="pcbi.1004218.e009g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e009" xlink:type="simple"/>
<mml:math display="block" id="M9" overflow="scroll">
<mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub><mml:mo>∝</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
<p>The information that can be extracted by such decoder from dataset B is</p>
<disp-formula id="pcbi.1004218.e010">
<alternatives>
<graphic id="pcbi.1004218.e010g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e010" xlink:type="simple"/>
<mml:math display="block" id="M10" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
<p>A bias-corrected estimator of the expression (4) is given in Materials and Methods, Section 6.</p>
<p>We compared the cross-validated decoding method with the bias-corrected direct estimators on synthetic data. We considered two separate sets of covariance matrices and tuning curves, such that the optimal decoder was different for the two cases, and therefore the true crossed information <italic>I</italic><sub><italic>AB</italic></sub> was smaller than the true information in the second population, <italic>I</italic><sub><italic>B</italic></sub> (Materials and Methods, Section 7). We then generated data from those sets of tuning and covariances to evaluate the estimators. In <xref rid="pcbi.1004218.g003" ref-type="fig">Fig 3</xref> we show that, for <italic>N</italic> = 50, as soon as <italic>T</italic> ≥ 5 <italic>N</italic>, the direct estimator is more reliable than the decoding estimate. Thus, in this case, similar to <xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2D</xref>, there is still a cross-over in performance between the direct estimator and the decoder, but it happens at a larger number of trials than <italic>N</italic>. However, since we could not find a closed-form expression for the variance of <italic>I</italic><sub><italic>AB</italic></sub>, it is hard to make a general statement about the precise point of cross-over, which will depend on the statistics of the datasets A and B.</p>
<fig id="pcbi.1004218.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Fisher information estimates for the general case of a decoder trained and tested on different response statistics.</title>
<p>Left: Illustration of the scenario. Decoding weights are optimized for data recorded from population A, then tested on data from population B. Note that the decision boundary derived from A is not optimal for B. (<bold>a</bold>) Estimate of the Fisher information obtained by decoding (red) or direct estimation with bias correction (blue). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>b</bold>) MSE of the decoder-based estimate (red) and the direct estimator (blue).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g003" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Directly measuring the impact of correlations on encoding and decoding</title>
<p>Sometimes experimental studies seek not only to estimate the information in a given population but also how that information is affected by correlations. Two widely used measures are <italic>I</italic><sub><italic>shuffle</italic></sub>, which quantifies how information <italic>encoding</italic> is affected by correlations, i.e. how much information is present in a population with the same marginal statistics but no correlations; and <italic>I</italic><sub><italic>diag</italic></sub>, which quantifies how information <italic>decoding</italic> is affected by correlations, i.e. how much information can be retrieved from the correlated population if the decoder does not model correlations [<xref rid="pcbi.1004218.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004218.ref022" ref-type="bibr">22</xref>]. <italic>I</italic><sub><italic>shuffle</italic></sub> is typically measured using cross-validated decoding after the neural responses are shuffled across trials. This shuffling destroys correlations and by comparing <italic>I</italic><sub><italic>shuffle</italic></sub> to the information in the original data, one can thus assess the impact of correlations on information encoding. To calculate <italic>I</italic><sub><italic>diag</italic></sub>, a decoder is trained on shuffled data and tested on the original data, ensuring that the decoder cannot model correlations.</p>
<p>We propose to measure <italic>I</italic><sub><italic>shuffle</italic></sub> and <italic>I</italic><sub><italic>diag</italic></sub> directly using bias-corrected direct estimators. <italic>I</italic><sub><italic>shuffle</italic></sub> is defined as:
<disp-formula id="pcbi.1004218.e011">
<alternatives>
<graphic id="pcbi.1004218.e011g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e011" xlink:type="simple"/>
<mml:math display="block" id="M11" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msup><mml:mo>′</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo>′</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where <inline-formula id="pcbi.1004218.e012"><alternatives><graphic id="pcbi.1004218.e012g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e012" xlink:type="simple"/><mml:math display="inline" id="M12" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are the marginal variances of neural responses. An unbiased estimator of <italic>I</italic><sub><italic>shuffle</italic></sub> is given by
<disp-formula id="pcbi.1004218.e013">
<alternatives>
<graphic id="pcbi.1004218.e013g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e013" xlink:type="simple"/>
<mml:math display="block" id="M13" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="0.15em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where <inline-formula id="pcbi.1004218.e014"><alternatives><graphic id="pcbi.1004218.e014g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e014" xlink:type="simple"/><mml:math display="inline" id="M14" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the unbiased estimator of the sample variance of neuron <italic>i</italic> (see <xref rid="sec011" ref-type="sec">Materials and Methods</xref>, Section 5). Note that Eq (<xref rid="pcbi.1004218.e013" ref-type="disp-formula">6</xref>) does not require actually shuffling the data. This is yet another advantage of this technique over decoding approaches: shuffling cannot remove correlations entirely due to the finite number of trials, thus introducing an additional source of estimation error for decoder-based methods.</p>
<p>In <xref rid="pcbi.1004218.g004" ref-type="fig">Fig 4</xref>, we compare the cross-validated decoding method with the bias-corrected direct estimator for <italic>I</italic><sub><italic>shuffle</italic></sub>. With <italic>N</italic> = 50 neurons, for <italic>T</italic> &gt; <italic>N</italic>, the direct estimator is more reliable than the decoding estimate, and for small numbers of trials (<italic>T</italic> &lt; 1000) the gap between the decoder and the direct estimator is even larger than in <xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2A</xref>. This is because the decoder-based estimate of <italic>I</italic><sub><italic>shuffle</italic></sub> involves training on shuffled data, which can lead to large errors. Indeed, the decoding method works better when the code is highly redundant, which is to say, when the information saturates as the number of decoded neurons increases (or equivalently, when the code contains differential correlations [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>]). Thanks to this redundancy, decoders that deviate slightly from the optimal (e.g. by placing too little weight on the most informative neurons) can still recover a large proportion of the information. Once the data are shuffled, the redundancies are gone. As a result, a small error in the decoder will lead to poor performance on the shuffled data [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>]. Therefore, it is expected that a slightly suboptimal decoder (e.g. due to finite training set size) will miss a larger proportion of the information in the shuffled case than in the original case. Furthermore, this weakness of decoding methods becomes even more apparent in large populations, leading to larger estimation errors for <italic>I</italic><sub><italic>shuffle</italic></sub> in large than small populations. Conversely, the error of the direct estimator of <italic>I</italic><sub><italic>shuffle</italic></sub> decreases in larger populations (<xref rid="pcbi.1004218.s003" ref-type="supplementary-material">S2 Fig</xref>).</p>
<fig id="pcbi.1004218.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Fisher information estimates for an independent population.</title>
<p>Left: Illustration of the scenario. The decoder is trained and tested on shuffled data, i.e. data where correlations have been destroyed by randomly permuting across trials the responses of each neuron independently. The faint lines represent the covariance ellipses of the original data, before shuffling. (<bold>a</bold>) Estimate of the Fisher information obtained by decoding (red) or direct estimation with bias correction (blue). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>b</bold>) MSE of the decoder-based estimate (red), and the direct estimator (blue).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g004" position="float" xlink:type="simple"/>
</fig>
<p>We were not able to find an exact analytical expression for an unbiased estimator of <italic>I</italic><sub><italic>diag</italic></sub> but this scenario is a subcase of the general problem we considered earlier: how to compute information in a dataset B, obtained with an optimal decoder of a dataset A. Here, dataset A is the shuffled data while dataset B corresponds to the original data. Accordingly, we use the same correction as before (Eq (<xref rid="pcbi.1004218.e010" ref-type="disp-formula">4</xref>)), discussed in Materials and Methods, Section 7.</p>
<p>In <xref rid="pcbi.1004218.g005" ref-type="fig">Fig 5</xref>, we show the comparison for <italic>I</italic><sub><italic>diag</italic></sub> and find that the direct estimator is more reliable than the cross-validated decoder for <italic>T</italic> ≥ 2 <italic>N</italic>. For this set of simulations, the cross-over between direct estimator and decoder occurs at a smaller <italic>T</italic> than in the simulations for <italic>I</italic><sub><italic>AB</italic></sub> (<xref rid="pcbi.1004218.g003" ref-type="fig">Fig 3B</xref>), although this needs not be true in general. Note that the accuracy of the estimator for <italic>I</italic><sub><italic>diag</italic></sub> (relative error of 6% compared to ground truth, for <italic>T</italic> = 1000) is comparable to those for <italic>I</italic> (namely the information in the original data; relative error of 6% for <italic>T</italic> = 1000) and <italic>I</italic><sub><italic>shuffle</italic></sub> (relative error of 5% for <italic>T</italic> = 1000), despite the lack of an analytical expression for <italic>I</italic><sub><italic>diag</italic></sub>. In these simulations, the actual values of <italic>I</italic> and <italic>I</italic><sub><italic>diag</italic></sub> were similar (compare the dashed line in Figs <xref rid="pcbi.1004218.g001" ref-type="fig">1</xref> and <xref rid="pcbi.1004218.g005" ref-type="fig">5</xref>). We therefore repeated the analysis using a different model of noise correlations [<xref rid="pcbi.1004218.ref012" ref-type="bibr">12</xref>](<xref rid="pcbi.1004218.s001" ref-type="supplementary-material">S1 Text</xref>), one which produces a larger gap between <italic>I</italic> and <italic>I</italic><sub><italic>diag</italic></sub>, and found similar results (<xref rid="pcbi.1004218.s004" ref-type="supplementary-material">S3 Fig</xref>).</p>
<fig id="pcbi.1004218.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Fisher information estimates when ignoring correlations.</title>
<p>Left: Illustration of the scenario. Decoding weights are optimized for the shuffled data, then tested on the original data. The faint lines on the top plot represent the covariance ellipses of the original data, before shuffling. Note that the decision boundary derived from shuffled data is not necessarily optimal for the original data. (<bold>a</bold>) Estimate of the Fisher information obtained by decoding (red) or direct estimation with bias correction (blue). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>b</bold>) MSE of the decoder-based estimate (red) and the direct estimator (blue).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g005" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Evaluations on neural data</title>
<p>So far we have tested the bias-corrected estimators on simulated data, which was generated using a rectified multivariate Gaussian distribution followed by a Poisson step. Even though the analytical form of the bias correction was derived assuming Gaussian variability, our results show that the bias-correction works well for the Gaussian-Poisson model.</p>
<p>To provide a stronger test of the estimation methods, we applied them to data recorded from populations of neurons in the primary visual cortex of an anesthetized macaque monkey. In this experiment, spike count responses were recorded from 52 units to gratings of two different orientations, each presented for 900 trials (Materials and Methods Section 9).</p>
<p>We first used a cross-validated decoder to estimate information. <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref> (top-left) shows that the decoder’s upper and lower information bounds (obtained from the training and validating sets) diverge for small numbers of trials as was found previously for simulated data (<xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1B</xref>). While the gap between the two bounds is much reduced when using all 900 trials, it remains sizeable (18% of the asymptotic value). In contrast, the information obtained from a bias-corrected estimator is within 10% of its asymptotic value with a little as 100 trials, Moreover, the asymptotic value lies in between the two bounds from the cross-validated decoder indicating that the bias corrected estimate must be close to the true information value. Using the mean between the two bounds for the decoder as ground truth, we also verified that in most conditions the bias-corrected estimator leads to a lower mean squared error (<xref rid="pcbi.1004218.s005" ref-type="supplementary-material">S4 Fig</xref>). This outcome is not guaranteed: in principle, were the bias correction not properly calculated, the direct estimator could reach an asymptotic value that differs largely from the asymptotic value of the cross-validated decoder. Thus, this outcome indicates that the bias correction derived under the assumption of Gaussian variability is also accurate for non-Gaussian cortical variability.</p>
<fig id="pcbi.1004218.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Fisher information estimated from cortical data.</title>
<p>Fisher information in a population of <italic>N</italic> = 52 macaque V1 neurons, estimated by decoding (red) or direct estimation (blue). Trials were subsampled 100 times without replacement, except for the last point on the abscissa which included all (<italic>T</italic> = 900) trials. The continuous lines represent the mean, the shaded area represents ±1 std across samples, computed by bootstrap. Stimulus orientations were spaced by 7 deg (top row) and 21 deg (bottom row); in the right column, images were masked by white noise on the pixels (for stimulus details, see <xref rid="sec011" ref-type="sec">Materials and Methods</xref> Section 9). Population-average firing rates were R = 0.7 spikes/trial (top-left); R = 0.7 spikes/trial (bottom-left); R = 2.2 spikes/trial (top-right); and R = 2.1 spikes/trial (bottom-right). Note that for large orientation differences, the stimuli can be more easily discriminated: Using the bias-corrected estimate at <italic>T</italic> = 900 and the known conversion between Fisher information and percent correct, percent correct with 7 deg separation is 77% (top-left) and 61% (top-right), whereas the corresponding values with 21 deg separation are 92% (bottom-left) and 70% (bottom-right).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g006" position="float" xlink:type="simple"/>
</fig>
<p>We then asked whether the performance improvement for direct estimation over decoding is robust. We expanded the stimulus set, by manipulating two parameters that are known to affect the estimator accuracy (explained in Materials and Methods Section 4, Eq (<xref rid="pcbi.1004218.e045" ref-type="disp-formula">19</xref>)). First, we varied the total amount of information in the population, by manipulating the noise added to the image pixels (different columns in <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>). Second, we varied the difference between the two stimulus values, using <italic>d θ</italic> = {7, 21} degrees (different rows in <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>). <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref> shows that the gap between decoder estimates on training and validation sets remained sizeable (between 9% and 95% of the asymptotic value) at 900 trials in all conditions. In contrast, the direct estimator reached within 10% of its asymptotic value, with 50 to 150 trials. We compared also the direct and decoding-based estimators of <italic>I</italic><sub><italic>shuffle</italic></sub> and <italic>I</italic><sub><italic>diag</italic></sub>, and found for the cortical data (<xref rid="pcbi.1004218.s006" ref-type="supplementary-material">S5 Fig</xref>) results analogous to the simulations of Figs <xref rid="pcbi.1004218.g004" ref-type="fig">4</xref> and <xref rid="pcbi.1004218.g005" ref-type="fig">5</xref>, thus confirming robustness to realistic deviations from the assumptions of the bias-corrected estimators.</p>
</sec>
<sec id="sec009">
<title>Robustness to deviations from Gaussianity at low spike counts</title>
<p>The performance of the direct estimator for data simulated using the Gaussian-Poisson model and for non-Gaussian cortical data indicates that the direct estimator is robust to realistic deviations from the Gaussianity assumption. The response variability is expected to deviate further from Gaussian at low spike counts per trial, which can be manipulated both in the simulated data as well as in the cortical data by reducing the observation window.</p>
<p>Note that there is an important difference between linear and full Fisher information: For low spike counts or short time windows there is no guarantee that there exists an efficient (non-linear) estimator reaching the Cramer-Rao bound [<xref rid="pcbi.1004218.ref023" ref-type="bibr">23</xref>–<xref rid="pcbi.1004218.ref025" ref-type="bibr">25</xref>]. In contrast, linear Fisher information for discrimination is <italic>defined</italic> to be the inverse minimum stimulus-conditioned variance of a linear estimator which is unbiased for the two presented stimuli. This linear estimator can always be constructed given the tuning curves and noise covariance matrix and by definition there is never a discrepancy between optimal linear estimator variance and inverse linear Fisher information as in the non-linear case. Also, linear Fisher information is defined for general response distributions with existing first and second moments and does not require the assumption of Gaussian response variability.</p>
<p>In <xref rid="pcbi.1004218.g007" ref-type="fig">Fig 7A and 7B</xref>, we compare the performance of the direct estimator with the cross-validated decoding methods at low spike counts for simulated data. The parameters of the plot are the same as in <xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2A</xref>, except for the average tuning amplitude, which we set to <italic>g</italic> = 1 instead of <italic>g</italic> = 30, corresponding to 0.8 spikes per neuron per trial (compared to 11.3 spikes per neuron per trial in <xref rid="pcbi.1004218.g002" ref-type="fig">Fig 2A</xref>). At low spike counts, we cannot compute analytically the ground truth information accurately in our model, due to the approximation used to account for the rectification. We have used instead the estimate obtained with a large number of trials (<italic>T</italic> = 100,000). We find that as soon as <italic>T</italic> ≥ 3<italic>N</italic>, the direct estimator is more reliable than the decoding estimate. Similar results are obtained for a model of Von Mises tuning curves and independent Poisson variability, in which the ground truth can be calculated (<xref rid="pcbi.1004218.s007" ref-type="supplementary-material">S6 Fig</xref>).</p>
<fig id="pcbi.1004218.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparison of estimators at low spike counts.</title>
<p>(<bold>a,b</bold>) Simulated data with model parameters identical to <xref rid="pcbi.1004218.t001" ref-type="table">Table 1</xref> and <xref rid="pcbi.1004218.t002" ref-type="table">Table 2</xref> except for <italic>g</italic> = 1 (corresponding to 0.8 spikes per neuron per trial on average). (<bold>a</bold>) Estimate of the Fisher information obtained by decoding (red) or direct estimation with bias correction (blue). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>b</bold>) MSE of the decoder-based estimate (red) and the direct estimator (blue). (<bold>c</bold>) Evaluation on the neural data of <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>, in the condition of stimulus orientation spaced by 7 deg, without pixel noise in the image. Each panel corresponds to a different spike count window, reported at the top of the panel, starting 30 ms after stimulus onset. The top-left panel (250ms window) is identical to the top-left panel of <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>. The color code is identical to <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.g007" position="float" xlink:type="simple"/>
</fig>
<p>In <xref rid="pcbi.1004218.g007" ref-type="fig">Fig 7C</xref> we test the robustness for cortical data at low spike counts by successively shortening the observation window. For the original observation widow (250ms, top-left panel of <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>) the average count is 0.7 spikes per neuron per trial, which is reduced to 0.07 spikes per neuron per trial for the smallest observation window of 50ms in <xref rid="pcbi.1004218.g007" ref-type="fig">Fig 7C</xref>. For such a small observation windows we observe that the direct estimator does get biased at small numbers of trials. This is due to the large deviation from Gaussianity: In particular, the abundance of trials where not a single spike was fired by any neuron (19% of the trials for a 50ms window, compared to 0.07% of trials for 250ms window) implies that more trials are required to properly estimate the covariance matrix. For extremely low spike counts and scarce data, cross-validated decoding might lead to better results than the direct estimator. However, the direct estimator remains asymptotically unbiased regardless of window size, and even for a 100ms window (average spike count of 0.24, with 4% silent trials) it is unbiased with as little as <italic>T</italic> ≥ 2<italic>N</italic>.</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>We have presented a fast and accurate method to estimate the amount of information about an encoded stimulus in a correlated neural population. As recently pointed out [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>], estimating the tuning curve derivatives and covariance matrix from the data, and then applying the equation that defines Fisher information, Eq (<xref rid="pcbi.1004218.e001" ref-type="disp-formula">1</xref>), leads to large biases. We have shown that the bias of the direct estimator can be predicted exactly, and we have demonstrated that correcting for this bias leads to accurate estimates of the true information in the population. Using both realistic simulations as well as experimental data, we have shown that our bias-corrected estimator largely outperforms the current state of the art methods, based on decoding. Furthermore, while training a decoder requires a typically lengthy numerical minimization, the method we proposed only requires a matrix inversion, and is therefore orders of magnitude faster. We have also derived an analytical expression for the variance of the estimator, and extended our bias-correction method to the widely studied cases of independent populations obtained by shuffling the data, and of a factorized decoder (i.e. decoding correlated data under the assumption that there are no correlations). As datasets of increasing size become available, our method will provide an invaluable tool to explore quantitatively the relation between the neural code and behavioral variability.</p>
<p>Our bias correction method assumes Gaussian variability of neuronal responses, an assumption that is often violated by experimental data. Therefore, we compared our estimator and decoding on neural data recorded from macaque primary visual cortex. In this case we do not have access to the true information; however, by subsampling trials, we showed that the two methods give consistent results in the limit of large numbers of trials, but our estimator is systematically more accurate, except for very low spike counts when only a few trials are available.</p>
<p>Why is our direct estimator more accurate than decoding? Both training a decoder, and assessing its performance, require large amount of data. Therefore, to use decoding, the available trials need to be split in similar proportions between the training set and validation set; in addition, early stopping requires further splitting of the training set, in order to monitor and prevent overfitting. As a consequence, decoding-based methods have reduced statistical power, compared to direct estimation. One way to mitigate this issue is to use model-based regularization (e.g. variational Bayes logistic regression [<xref rid="pcbi.1004218.ref026" ref-type="bibr">26</xref>] or L<sub>2</sub> regularization), to avoid splitting the training set, and leave-one-out cross validation to maximize the size of the training set. However, these approaches also rely on assumptions about the data that are not always met, and are subject to overfitting for finite datasets. We found that the performance of these alternative decoding methods was not systematically better than early stopping (<xref rid="pcbi.1004218.s008" ref-type="supplementary-material">S7 Fig</xref>). All three decoding methods performed significantly worse than the bias-corrected estimator and had run times between 1 and 4 orders of magnitude larger than the bias-corrected estimator (<xref rid="pcbi.1004218.s008" ref-type="supplementary-material">S7 Fig</xref>).</p>
<p>Our derivation of an analytical expression for the variance of the direct, bias-corrected estimator allows one to draw exact error bars without relying on bootstrapping methods. It also allowed us to understand limiting conditions under which the estimation error explodes. First we found that the variance of the bias-corrected estimator diverges for <italic>T</italic> = (<italic>N</italic> + 5) / 2. This is the lower bound on the number of trials which need to be present for the estimator to be useable. Currently, if fewer trials than this lower bound are available, decoding remains the only available method, although the estimates it provides may be highly inaccurate. Extending our direct estimator to this case is an important direction for future work.</p>
<p>Second, we found that the estimation error becomes large if the difference of the presented stimuli, <italic>d θ</italic>, is small relative to the inverse square root of the true Fisher information <inline-formula id="pcbi.1004218.e015"><alternatives><graphic id="pcbi.1004218.e015g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e015" xlink:type="simple"/><mml:math display="inline" id="M15" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>I</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> (see Eq (<xref rid="pcbi.1004218.e045" ref-type="disp-formula">19</xref>)). The reason is that the smaller <italic>d θ</italic> is, the noisier the estimation of the tuning curve derivative <bold>f</bold>′ will be. Conversely, if the aim is to measure the information available in a fine discrimination task, <italic>d θ</italic> cannot be too large either: This will lead to a bias in the estimation for non-linear tuning curves, because <inline-formula id="pcbi.1004218.e016"><alternatives><graphic id="pcbi.1004218.e016g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e016" xlink:type="simple"/><mml:math display="inline" id="M16" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>≠</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. If the aim is to estimate information in a fine discrimination tasks, the experimental choice of <italic>d θ</italic> must strike a balance between these two constraints. Note however that, for any fixed <italic>d θ</italic> our estimator will still have lower variance and less underestimation than a linear decoder, when <italic>T</italic> &gt; <italic>N</italic>. Hence, the choice of <italic>d θ</italic> should not influence the decision whether to use the decoder or the direct estimator. The direct estimator is also a better option for coarse discrimination tasks, as long as one is interested in linear Fisher information. The only difference in the case of coarse discrimination compared to fine discrimination is that the derivative of the tuning curves should be replaced with <inline-formula id="pcbi.1004218.e017"><alternatives><graphic id="pcbi.1004218.e017g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e017" xlink:type="simple"/><mml:math display="inline" id="M17" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>For investigators interested in estimating Fisher information, as opposed to linear Fisher information, other techniques must be used such as cross-validated nonlinear decoders. However, training a non-linear decoder from limited data will be even more difficult than training a linear decoder and will most likely lead to biased estimates of Fisher information. We do not know yet whether it is possible to obtain direct unbiased estimates of Fisher information, as we have described here for linear Fisher information.</p>
<p>Our results apply to the case of linear Fisher information about a continuous stimulus in a fine or coarse discrimination task. However, when studying the neural code in higher cortical areas, typically higher-level tasks are used such as object recognition, which involve a classification between multiple discrete classes [<xref rid="pcbi.1004218.ref027" ref-type="bibr">27</xref>]. Extending our approach to such multiclass classification is another important direction for future work.</p>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec012">
<title>Ethics statement</title>
<p>All procedures were approved by the Albert Einstein College of Medicine at Yeshiva University and followed the guidelines in the United States Public Health Service Guide for the Care and Use of Laboratory Animals.</p>
<sec id="sec013">
<title>1. Derivation of linear Fisher information</title>
<p>Linear Fisher information is defined to be the inverse variance of the locally optimal unbiased linear decoder [<xref rid="pcbi.1004218.ref004" ref-type="bibr">4</xref>]. Given two presented stimuli <italic>θ</italic><sup>+</sup> = <italic>θ</italic> + <italic>dθ</italic> and <italic>θ</italic><sup>−</sup> = <italic>θ</italic>−<italic>dθ</italic> in a fine discrimination task one constructs a locally linear estimator by the relation</p>
<disp-formula id="pcbi.1004218.e018">
<alternatives>
<graphic id="pcbi.1004218.e018g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e018" xlink:type="simple"/>
<mml:math display="block" id="M18" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>We would like to minimize the variance of this estimator while ensuring it to be unbiased for the two presented stimuli. This yields
<disp-formula id="pcbi.1004218.e019">
<alternatives>
<graphic id="pcbi.1004218.e019g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e019" xlink:type="simple"/>
<mml:math display="block" id="M19" overflow="scroll">
<mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msup><mml:mo>′</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
and</p>
<disp-formula id="pcbi.1004218.e020">
<alternatives>
<graphic id="pcbi.1004218.e020g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e020" xlink:type="simple"/>
<mml:math display="block" id="M20" overflow="scroll">
<mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msup><mml:mo>′</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>In analogy with the Cramer-Rao bound, linear Fisher information is defined to be the inverse of this variance:</p>
<disp-formula id="pcbi.1004218.e021">
<alternatives>
<graphic id="pcbi.1004218.e021g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e021" xlink:type="simple"/>
<mml:math display="block" id="M21" overflow="scroll">
<mml:mrow><mml:mi>I</mml:mi><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msup><mml:mo>′</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Given the tuning curves and noise covariance matrix one can always find the optimal discrimination weights given by Eq (<xref rid="pcbi.1004218.e018" ref-type="disp-formula">7</xref>). As a consequence, linear Fisher information is always attainable, unlike the full, non-linear Fisher information. Note also that this derivation does not include any assumptions about Gaussian response variability.</p>
<p>Furthermore, the above derivation of linear Fisher information can be straightforwardly extended to coarse discrimination and to the case of different noise covariance matrices at the two presented stimuli. In this case linear Fisher information is given by
<disp-formula id="pcbi.1004218.e022">
<alternatives>
<graphic id="pcbi.1004218.e022g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e022" xlink:type="simple"/>
<mml:math display="block" id="M22" overflow="scroll">
<mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.20em"/><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="0.10em"/><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="0.10em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>d</italic><bold>f</bold> = <bold>f</bold>(<italic>θ</italic><sup>+</sup>)−<bold>f</bold>(<italic>θ</italic><sup>−</sup>) and <inline-formula id="pcbi.1004218.e023"><alternatives><graphic id="pcbi.1004218.e023g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e023" xlink:type="simple"/><mml:math display="inline" id="M23" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. For ease of presentation we focus on fine discrimination and equal covariances in the remainder of the Material &amp; Methods.</p>
</sec>
<sec id="sec014">
<title>2. Estimating Fisher information by decoding</title>
<p>The method used most frequently to estimate Fisher information in neural population data is based on decoding. Given population responses to <italic>T</italic> trials of stimulus <italic>θ</italic><sup>+</sup> = <italic>θ</italic> + <italic>dθ</italic> and <italic>T</italic> trials of stimulus <italic>θ</italic><sup>−</sup> = <italic>θ</italic>−<italic>dθ</italic>, one uses a locally optimal unbiased linear decoder to estimate the stimulus value, and takes the inverse of the variance of the estimate of the stimulus to be the Fisher information [<xref rid="pcbi.1004218.ref004" ref-type="bibr">4</xref>].</p>
<p>Finding the optimal decoder can be formalized as a regression problem: Find the decoding weights vector <bold>w</bold> that minimizes the following squared error
<disp-formula id="pcbi.1004218.e024">
<alternatives>
<graphic id="pcbi.1004218.e024g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e024" xlink:type="simple"/>
<mml:math display="block" id="M24" overflow="scroll">
<mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mi>θ</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>θ</italic><sub><italic>t</italic></sub> and <bold>r</bold><sub><italic>t</italic></sub> denote, respectively, the true stimulus value and the population response on trial <italic>t</italic>, and 〈·〉 denotes the average across all trials. Since all estimates are based on finite number of trials, this method leads to overfitting, which in turn leads to overestimation of the true Fisher information [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>] (see <xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1A</xref>). Therefore, cross-validation must be used to assess decoder performance and some form of regularization is required to mitigate overfitting. Here, following Moreno et al. [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>], we use early stopping (we also considered alternative approaches to regularization and found similar results, <xref rid="pcbi.1004218.s008" ref-type="supplementary-material">S7 Fig</xref>). We split the data in three sets (training, test, and validation) of approximately equal size, and update the decoding weights by gradient descent on the training set
<disp-formula id="pcbi.1004218.e025">
<alternatives>
<graphic id="pcbi.1004218.e025g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e025" xlink:type="simple"/>
<mml:math display="block" id="M25" overflow="scroll">
<mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>E</italic><sub><italic>TR</italic></sub> is the squared error on the training set. We initialize the weights randomly, and update them until the test set error, denoted <italic>E</italic><sub><italic>TE</italic></sub>, starts to increase. This is when its derivative ∂<italic>E</italic><sub><italic>TE</italic></sub> / ∂<italic>τ</italic> changes sign from negative to positive, where</p>
<disp-formula id="pcbi.1004218.e026">
<alternatives>
<graphic id="pcbi.1004218.e026g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e026" xlink:type="simple"/>
<mml:math display="block" id="M26" overflow="scroll">
<mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Once the optimization terminates, the Fisher information is estimated on the validation set
<disp-formula id="pcbi.1004218.e027">
<alternatives>
<graphic id="pcbi.1004218.e027g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e027" xlink:type="simple"/>
<mml:math display="block" id="M27" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>Early Stopping</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
where 〈<bold>r</bold><sub><italic>VAL</italic></sub>(<italic>θ</italic><sup>±</sup>)〉 denotes the average population responses, and <italic>S</italic><sub><italic>VAL</italic></sub>(<italic>θ</italic><sup>±</sup>) the sample covariance matrices, computed from all trials in the validation set corresponding to either stimulus <italic>θ</italic><sup>+</sup> or <italic>θ</italic><sup>−</sup>. Note that the first term on the r.h.s. corrects for biases of the decoder, e.g. due to a wrong scaling of <bold>w</bold>.</p>
</sec>
<sec id="sec015">
<title>3. Bias-corrected estimator of full Fisher information</title>
<p>Here we show that the estimator in Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>) for Fisher information is unbiased assuming Gaussian variability. In this case the response distribution in trials <italic>t</italic> = 1…<italic>T</italic> to stimuli <italic>θ</italic> ± <italic>d θ</italic> is given by the multivariate Gaussian
<disp-formula id="pcbi.1004218.e028">
<alternatives>
<graphic id="pcbi.1004218.e028g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e028" xlink:type="simple"/>
<mml:math display="block" id="M28" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mo>±</mml:mo></mml:msubsup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>±</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>±</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where we assume the difference between the two presented stimuli <italic>d θ</italic> to be small enough that we can linearly expand the tuning curve and neglect the change in covariance as a function of <italic>θ</italic>. The empirical mean and covariance for each presented stimulus is given by</p>
<disp-formula id="pcbi.1004218.e029">
<alternatives>
<graphic id="pcbi.1004218.e029g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e029" xlink:type="simple"/>
<mml:math display="block" id="M29" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mo>±</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mo>±</mml:mo></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>±</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mo>±</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mo>±</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mo>±</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mo>±</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>This allows us to construct unbiased estimators of both <bold>f</bold>′(<italic>θ</italic>) and Σ:</p>
<disp-formula id="pcbi.1004218.e030">
<alternatives>
<graphic id="pcbi.1004218.e030g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e030" xlink:type="simple"/>
<mml:math display="block" id="M30" overflow="scroll">
<mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mo>+</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>;</mml:mo><mml:mspace width="1em"/><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mrow><mml:mo>〈</mml:mo><mml:mi>S</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
<p>Since the linear Fisher information is a non-linear function of <bold>f</bold>′(<italic>θ</italic>) and Σ the naive estimator
<disp-formula id="pcbi.1004218.e031">
<alternatives>
<graphic id="pcbi.1004218.e031g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e031" xlink:type="simple"/>
<mml:math display="block" id="M31" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
will have a bias which we will now calculate. For that we make use of the fact that the sampling distributions of the empirical mean and covariance given <italic>T</italic> trials are given by
<disp-formula id="pcbi.1004218.e032">
<alternatives>
<graphic id="pcbi.1004218.e032g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e032" xlink:type="simple"/>
<mml:math display="block" id="M32" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mo>±</mml:mo></mml:msup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>±</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mfrac><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>S</mml:mi><mml:mo>±</mml:mo></mml:msup><mml:mo>∼</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where <italic>W</italic><sub><italic>p</italic></sub>(<italic>V</italic>, <italic>n</italic>) is the <italic>p</italic>-dimensional Wishart distribution with <italic>n</italic> degrees of freedom [<xref rid="pcbi.1004218.ref014" ref-type="bibr">14</xref>]. Consequently, the unbiased estimators in Eq (<xref rid="pcbi.1004218.e029" ref-type="disp-formula">10</xref>) are sampled from
<disp-formula id="pcbi.1004218.e033">
<alternatives>
<graphic id="pcbi.1004218.e033g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e033" xlink:type="simple"/>
<mml:math display="block" id="M33" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.75em"/><mml:mi>S</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
If Σ is invertible and <italic>N</italic> &lt; 2(<italic>T</italic>−1), <italic>S</italic> will be invertible with probability 1. The expectation value of its inverse is given by [<xref rid="pcbi.1004218.ref014" ref-type="bibr">14</xref>]</p>
<disp-formula id="pcbi.1004218.e034">
<alternatives>
<graphic id="pcbi.1004218.e034g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e034" xlink:type="simple"/>
<mml:math display="block" id="M34" overflow="scroll">
<mml:mrow><mml:mo>〈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>The second result we need is that the sampling distributions of mean and covariance of a Gaussian are independent (see e.g. [<xref rid="pcbi.1004218.ref028" ref-type="bibr">28</xref>]). It follows that the expectation value of the naive estimator Eq (<xref rid="pcbi.1004218.e030" ref-type="disp-formula">11</xref>) is given by</p>
<disp-formula id="pcbi.1004218.e035">
<alternatives>
<graphic id="pcbi.1004218.e035g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e035" xlink:type="simple"/>
<mml:math display="block" id="M35" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo>〈</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>〉</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>〉</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.25em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>′</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msup><mml:mo>′</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Correcting for this bias yields the expression for the bias-corrected estimator Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>). Note that the sampling distributions of mean and covariance are independent if and only if the underlying distribution is Gaussian. Measuring the independence of sample mean and covariance in real data with a relevant number of trials is very difficult since it would require repeating the same experiment many times. It is easier to determine whether the response distribution is close to Gaussian, which is the case only at high spike count. The fact that the bias-corrected estimator is robust to deviations from Gaussianity at low spike count implies that it is also robust to the dependence of the sampling distribution of mean and covariance.</p>
<p>Note that the correction in Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>) can lead occasionally to negative estimates of information. This will only happen when the estimate is very noisy, in which case the error bars are expected to be of similar magnitude as the actual estimate. As we show next, we can also derive an analytical prediction for the variance of the estimator, which allows us to draw error bars even for a single experiment.</p>
</sec>
<sec id="sec016">
<title>4. Variance of the bias-corrected estimator</title>
<p>Due to finite-sampling variability, the estimator in Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>) evaluated in a single experiment with <italic>T</italic> trials can be interpreted as a random draw from its sampling distribution. In the following we compute analytically the variance of this sampling distribution which allows calculating error bars on the measurements. The estimator in Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>) can be rewritten as
<disp-formula id="pcbi.1004218.e036">
<alternatives>
<graphic id="pcbi.1004218.e036g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e036" xlink:type="simple"/>
<mml:math display="block" id="M36" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where we have defined</p>
<disp-formula id="pcbi.1004218.e037">
<alternatives>
<graphic id="pcbi.1004218.e037g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e037" xlink:type="simple"/>
<mml:math display="block" id="M37" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
<p>Since the sampling distributions of <italic>X</italic><sub><italic>ij</italic></sub> and <inline-formula id="pcbi.1004218.e038"><alternatives><graphic id="pcbi.1004218.e038g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e038" xlink:type="simple"/><mml:math display="inline" id="M38" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are independent the variance of <inline-formula id="pcbi.1004218.e039"><alternatives><graphic id="pcbi.1004218.e039g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e039" xlink:type="simple"/><mml:math display="inline" id="M39" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is given by</p>
<disp-formula id="pcbi.1004218.e040">
<alternatives>
<graphic id="pcbi.1004218.e040g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e040" xlink:type="simple"/>
<mml:math display="block" id="M40" overflow="scroll">
<mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.10em"/><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow></mml:mstyle><mml:mo>〈</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>〉</mml:mo><mml:mo>〈</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>〉</mml:mo><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
<p>The expectation values are given by
<disp-formula id="pcbi.1004218.e041">
<alternatives>
<graphic id="pcbi.1004218.e041g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e041" xlink:type="simple"/>
<mml:math display="block" id="M41" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo>〈</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>〈</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
where we have defined <italic>γ</italic> = 2 / (<italic>Tdθ</italic><sup>2</sup>). The covariance of <italic>X</italic><sub><italic>ij</italic></sub> can be calculated using the higher moments of the Gaussian distribution in Eq (<xref rid="pcbi.1004218.e032" ref-type="disp-formula">13</xref>),</p>
<disp-formula id="pcbi.1004218.e042">
<alternatives>
<graphic id="pcbi.1004218.e042g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e042" xlink:type="simple"/>
<mml:math display="block" id="M42" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>′</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula>
<p>The covariance of <inline-formula id="pcbi.1004218.e043"><alternatives><graphic id="pcbi.1004218.e043g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e043" xlink:type="simple"/><mml:math display="inline" id="M43" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> can be found using well-known expressions for the inverse Wishart distribution [<xref rid="pcbi.1004218.ref014" ref-type="bibr">14</xref>]
<disp-formula id="pcbi.1004218.e044">
<alternatives>
<graphic id="pcbi.1004218.e044g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e044" xlink:type="simple"/>
<mml:math display="block" id="M44" overflow="scroll">
<mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mspace width="0.20em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
where</p>
<disp-formula id="pcbi.1004218.e045">
<alternatives>
<graphic id="pcbi.1004218.e045g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e045" xlink:type="simple"/>
<mml:math display="block" id="M45" overflow="scroll">
<mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mspace width="0.10em"/><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Combining Eq (<xref rid="pcbi.1004218.e040" ref-type="disp-formula">16</xref>), Eq (<xref rid="pcbi.1004218.e041" ref-type="disp-formula">17</xref>) and Eq (<xref rid="pcbi.1004218.e043" ref-type="disp-formula">18</xref>) we can calculate Eq (<xref rid="pcbi.1004218.e039" ref-type="disp-formula">15</xref>) in a tedious, but straightforward calculation. The result is</p>
<disp-formula id="pcbi.1004218.e046">
<alternatives>
<graphic id="pcbi.1004218.e046g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e046" xlink:type="simple"/>
<mml:math display="block" id="M46" overflow="scroll">
<mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.10em"/><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>I</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula>
<p>This formula provides several insights. First, one can see that the variance of the bias-corrected estimator diverges for <italic>T</italic> = (<italic>N</italic> + 5) / 2. This is the lower bound on the number of trials which need to be present for the estimator to be useable. Second, one can see that the second and third term in Eq (<xref rid="pcbi.1004218.e045" ref-type="disp-formula">19</xref>) are large if the difference of the presented stimuli, <italic>d θ</italic>, is small relative to the inverse square root of the true Fisher information <inline-formula id="pcbi.1004218.e047"><alternatives><graphic id="pcbi.1004218.e047g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e047" xlink:type="simple"/><mml:math display="inline" id="M47" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>I</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. The reason is that for small <italic>d θ</italic> the estimate of the tuning curve derivative <bold>f</bold>′ will be noisy. Conversely, if <italic>d θ</italic> is too large, one will introduce a bias in the estimation if the true tuning curve is non-linear. The right choice of <italic>d θ</italic> involves finding a tradeoff between these two constraints.</p>
</sec>
<sec id="sec017">
<title>5. Bias-corrected estimator of <italic>I</italic><sub><italic>shuffle</italic></sub></title>
<p>An unbiased estimator for <italic>I</italic><sub><italic>shuffle</italic></sub> can be derived in a similar way. Here we assume that the covariance is diagonal, <inline-formula id="pcbi.1004218.e048"><alternatives><graphic id="pcbi.1004218.e048g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e048" xlink:type="simple"/><mml:math display="inline" id="M48" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. The unbiased estimator for the variances on the diagonal is given by</p>
<disp-formula id="pcbi.1004218.e049">
<alternatives>
<graphic id="pcbi.1004218.e049g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e049" xlink:type="simple"/>
<mml:math display="block" id="M49" overflow="scroll">
<mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mo>±</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>±</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mo>±</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Similarly we can construct the naive estimator
<disp-formula id="pcbi.1004218.e050">
<alternatives>
<graphic id="pcbi.1004218.e050g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e050" xlink:type="simple"/>
<mml:math display="block" id="M50" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(20)</label>
</disp-formula>
whose bias we will again calculate. The sampling distribution of the diagonal variance is given by a Gamma (or chi-square) distribution:</p>
<disp-formula id="pcbi.1004218.e051">
<alternatives>
<graphic id="pcbi.1004218.e051g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e051" xlink:type="simple"/>
<mml:math display="block" id="M51" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>The distribution of the empirical mean is identical to the one in Eq (<xref rid="pcbi.1004218.e031" ref-type="disp-formula">12</xref>). The expectation value of the inverse variance is given by</p>
<disp-formula id="pcbi.1004218.e052">
<alternatives>
<graphic id="pcbi.1004218.e052g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e052" xlink:type="simple"/>
<mml:math display="block" id="M52" overflow="scroll">
<mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Using again the independence of sample mean and variance, the expectation value of the naive estimator Eq (<xref rid="pcbi.1004218.e049" ref-type="disp-formula">20</xref>) is given by</p>
<disp-formula id="pcbi.1004218.e053">
<alternatives>
<graphic id="pcbi.1004218.e053g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e053" xlink:type="simple"/>
<mml:math display="block" id="M53" overflow="scroll">
<mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo>′</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Correcting for the bias yields the estimator Eq (<xref rid="pcbi.1004218.e013" ref-type="disp-formula">6</xref>) for <italic>I</italic><sub><italic>shuffle</italic></sub>.</p>
</sec>
<sec id="sec018">
<title>6. Bias-corrected estimator for a suboptimal decoder</title>
<p>Here we set out to find an unbiased estimator for the general case of a decoder optimized to dataset A and tested on dataset B. We assume that A and B are generated by neural populations with independent covariance matrices and tuning curves. The optimal decoding weights for dataset A are
<disp-formula id="pcbi.1004218.e054">
<alternatives>
<graphic id="pcbi.1004218.e054g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e054" xlink:type="simple"/>
<mml:math display="block" id="M54" overflow="scroll">
<mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub><mml:mo>∝</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(21)</label>
</disp-formula>
and the information that can be extracted by such decoder from dataset B is</p>
<disp-formula id="pcbi.1004218.e055">
<alternatives>
<graphic id="pcbi.1004218.e055g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e055" xlink:type="simple"/>
<mml:math display="block" id="M55" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(22)</label>
</disp-formula>
<p>We focus separately on the biases in the naïve estimates of the numerator and denominator, and correct for them. This will not guarantee that the full expression is unbiased since the division by the denominator is a non-linear transformation. Furthermore, all three terms in the numerator are independent (since the tuning curves for populations A and B are assumed independent) hence we correct for their biases individually (more specifically, for the covariance inversion) but we neglect the bias due to the squaring in the numerator. However both the numerator and denominator are one-dimensional quantities, whose variances are of order O(1/T). Therefore we expect that the size of the biases due to squaring and division is of order O(1/T) rather than O(N/T) for the naive estimator.</p>
<p>Focusing now on the denominator, a first step towards removing the bias is to use the bias-corrected estimator <inline-formula id="pcbi.1004218.e056"><alternatives><graphic id="pcbi.1004218.e056g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e056" xlink:type="simple"/><mml:math display="inline" id="M56" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> defined in Eq (<xref rid="pcbi.1004218.e036" ref-type="disp-formula">14</xref>) rather than the naive estimator. This will however only remove part of the bias, since <inline-formula id="pcbi.1004218.e057"><alternatives><graphic id="pcbi.1004218.e057g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e057" xlink:type="simple"/><mml:math display="inline" id="M57" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> appears twice in the denominator. The bias is given by</p>
<disp-formula id="pcbi.1004218.e058">
<alternatives>
<graphic id="pcbi.1004218.e058g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e058" xlink:type="simple"/>
<mml:math display="block" id="M58" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="12.5em"/><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(23)</label>
</disp-formula>
<p>In order to evaluate <inline-formula id="pcbi.1004218.e059"><alternatives><graphic id="pcbi.1004218.e059g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e059" xlink:type="simple"/><mml:math display="inline" id="M59" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we make use of the basic fact that
<disp-formula id="pcbi.1004218.e060">
<alternatives>
<graphic id="pcbi.1004218.e060g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e060" xlink:type="simple"/>
<mml:math display="block" id="M60" overflow="scroll">
<mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(24)</label>
</disp-formula>
where the covariance of <inline-formula id="pcbi.1004218.e061"><alternatives><graphic id="pcbi.1004218.e061g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e061" xlink:type="simple"/><mml:math display="inline" id="M61" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> can be found as in Eq (<xref rid="pcbi.1004218.e043" ref-type="disp-formula">18</xref>). After a lengthy but straightforward calculation, we find that</p>
<disp-formula id="pcbi.1004218.e062">
<alternatives>
<graphic id="pcbi.1004218.e062g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e062" xlink:type="simple"/>
<mml:math display="block" id="M62" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(25)</label>
</disp-formula>
<p>It follows that
<disp-formula id="pcbi.1004218.e063">
<alternatives>
<graphic id="pcbi.1004218.e063g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e063" xlink:type="simple"/>
<mml:math display="block" id="M63" overflow="scroll">
<mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msubsup><mml:mo>'</mml:mo><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:msub><mml:mo>'</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(26)</label>
</disp-formula>
Where
<disp-formula id="pcbi.1004218.e064">
<alternatives>
<graphic id="pcbi.1004218.e064g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e064" xlink:type="simple"/>
<mml:math display="block" id="M64" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(27)</label>
</disp-formula>
and <italic>I</italic><sub><italic>A</italic></sub> is the true information in population A.</p>
<p>Eq (<xref rid="pcbi.1004218.e062" ref-type="disp-formula">26</xref>) provides an expression for the expected bias. In order to remove such bias from the estimator of <italic>I</italic><sub><italic>AB</italic></sub>, we first need an unbiased estimate the bias itself, which we obtain by substituting, in Eqs (<xref rid="pcbi.1004218.e062" ref-type="disp-formula">26</xref>) and (<xref rid="pcbi.1004218.e063" ref-type="disp-formula">27</xref>), <inline-formula id="pcbi.1004218.e065"><alternatives><graphic id="pcbi.1004218.e065g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e065" xlink:type="simple"/><mml:math display="inline" id="M65" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> for <italic>I</italic><sub><italic>A</italic></sub>; and <italic>Ŝ</italic><sup>−1</sup> for the inverse covariance Σ<sup>−1</sup>.</p>
<p>As a result, the bias-corrected estimator is given by
<disp-formula id="pcbi.1004218.e066">
<alternatives>
<graphic id="pcbi.1004218.e066g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e066" xlink:type="simple"/>
<mml:math display="block" id="M66" overflow="scroll">
<mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(28)</label>
</disp-formula>
Where</p>
<disp-formula id="pcbi.1004218.e067">
<alternatives>
<graphic id="pcbi.1004218.e067g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e067" xlink:type="simple"/>
<mml:math display="block" id="M67" overflow="scroll">
<mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
</sec>
<sec id="sec019">
<title>7. Bias-corrected estimator of <italic>I</italic><sub><italic>diag</italic></sub></title>
<p>The unbiased estimator in Eq (<xref rid="pcbi.1004218.e065" ref-type="disp-formula">28</xref>) was derived under the assumption that datasets A and B are generated by populations with independent covariance matrices and tuning curve derivatives. However, estimating <italic>I</italic><sub><italic>diag</italic></sub> corresponds to the case that datasets A and B are generated by populations with the same tuning curves derivatives, hence an additional correction is required for the numerator of Eq (<xref rid="pcbi.1004218.e054" ref-type="disp-formula">22</xref>), analogous to the one in Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>), leading to</p>
<disp-formula id="pcbi.1004218.e068">
<alternatives>
<graphic id="pcbi.1004218.e068g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e068" xlink:type="simple"/>
<mml:math display="block" id="M68" overflow="scroll">
<mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(29)</label>
</disp-formula>
<p>Note that in this case, <italic>S</italic><sub><italic>B</italic></sub> represents the sample covariance of the original data, whereas <inline-formula id="pcbi.1004218.e069"><alternatives><graphic id="pcbi.1004218.e069g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e069" xlink:type="simple"/><mml:math display="inline" id="M69" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> represents the bias-corrected inverse covariance of the shuffled data (i.e. it includes residual correlations due to shuffling a finite number of trials). Note also that, since <italic>S</italic><sub><italic>A</italic></sub> and <italic>S</italic><sub><italic>B</italic></sub> are derived from the same data, before and after shuffling, they are not exactly independent. Hence, <inline-formula id="pcbi.1004218.e070"><alternatives><graphic id="pcbi.1004218.e070g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e070" xlink:type="simple"/><mml:math display="inline" id="M70" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">〉</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> holds only approximately (i.e. neglecting 3<sup>rd</sup> order terms). In practice, as shown in the Results, this approximation provides good results.</p>
</sec>
<sec id="sec020">
<title>8. Models of correlated neural populations</title>
<p>The simulations are based on a bank of orientation-selective filters. The inputs to the network are 32×32 pixel Gabor patches corrupted by additive white noise with variance <inline-formula id="pcbi.1004218.e071"><alternatives><graphic id="pcbi.1004218.e071g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e071" xlink:type="simple"/><mml:math display="inline" id="M71" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. The Gabor patches are defined by:
<disp-formula id="pcbi.1004218.e072">
<alternatives>
<graphic id="pcbi.1004218.e072g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e072" xlink:type="simple"/>
<mml:math display="block" id="M72" overflow="scroll">
<mml:mrow><mml:mtext>J</mml:mtext><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mspace width="0.10em"/><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mspace width="0.10em"/><mml:mtext>cos</mml:mtext><mml:mspace width="0.10em"/><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mspace width="0.10em"/><mml:mtext>sin</mml:mtext><mml:mspace width="0.10em"/><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(30)</label>
</disp-formula>
where (<italic>x</italic>, <italic>y</italic>) are the coordinates of the image, <italic>c</italic> the carrier contrast, <italic>σ</italic> is the size of the Gaussian envelope, <italic>λ</italic> the preferred spatial wavelength, <italic>θ</italic> the preferred orientation, and <italic>ϕ</italic> the phase offset of the Gabor filter (parameters values are specified in <xref rid="pcbi.1004218.t001" ref-type="table">Table 1</xref>). The inputs to the network, after the addition of noise, are:
<disp-formula id="pcbi.1004218.e073">
<alternatives>
<graphic id="pcbi.1004218.e073g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e073" xlink:type="simple"/>
<mml:math display="block" id="M73" overflow="scroll">
<mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>J</mml:mi></mml:mstyle><mml:mo stretchy="true">˜</mml:mo></mml:mover><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>J</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(31)</label>
</disp-formula>
where <italic>P</italic> is the image length in pixels and <bold>1</bold><sub><italic>P</italic></sub> is the identity matrix of size <italic>P</italic> × <italic>P</italic>.</p>
<table-wrap id="pcbi.1004218.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.t001</object-id>
<label>Table 1</label> <caption><title>Parameters of the input images.</title></caption>
<alternatives>
<graphic id="pcbi.1004218.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"><underline>Symbol</underline></th>
<th align="left" rowspan="1" colspan="1"><underline>Meaning</underline></th>
<th align="left" rowspan="1" colspan="1"><underline>Value</underline></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>P</italic></td>
<td align="left" rowspan="1" colspan="1">Side length (pixels)</td>
<td align="left" rowspan="1" colspan="1">32</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>θ</italic></td>
<td align="left" rowspan="1" colspan="1">Orientation (degrees)</td>
<td align="left" rowspan="1" colspan="1">{−7, 0}</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>σ</italic></td>
<td align="left" rowspan="1" colspan="1">Gaussian envelope std (degrees)</td>
<td align="left" rowspan="1" colspan="1"><italic>P</italic>/5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>λ</italic></td>
<td align="left" rowspan="1" colspan="1">Preferred spatial wavelength (pixel/cycle)</td>
<td align="left" rowspan="1" colspan="1"><italic>P</italic>/1.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>ϕ</italic></td>
<td align="left" rowspan="1" colspan="1">Preferred spatial phase</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>c</italic></td>
<td align="left" rowspan="1" colspan="1">Michelson contrast</td>
<td align="left" rowspan="1" colspan="1">0.75</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>σ</italic><sub>0</sub></td>
<td align="left" rowspan="1" colspan="1">Input noise std</td>
<td align="left" rowspan="1" colspan="1">0.2</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Model neurons are represented by linear filters whose outputs are half-rectified, and further corrupted by independent Poisson noise. This is a doubly stochastic model, with part of the variability induced by input fluctuations, and part due to the Poisson step. The linear filters are also Gabor patches rescaled to have zero mean and norm 1, and with the same size, wavelength and phase as the image patches (parameter values are provided in <xref rid="pcbi.1004218.t002" ref-type="table">Table 2</xref>). We denote by <bold>F</bold><sub><italic>k</italic></sub> the filter representing the <italic>k</italic>-th neuron. Fisher information in a population of such neurons is determined by their tuning curves and covariance matrix, which, ignoring half-rectification, are given by:
<disp-formula id="pcbi.1004218.e074">
<alternatives>
<graphic id="pcbi.1004218.e074g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e074" xlink:type="simple"/>
<mml:math display="block" id="M74" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>F</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>J</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(32)</label>
</disp-formula>
<disp-formula id="pcbi.1004218.e075">
<alternatives>
<graphic id="pcbi.1004218.e075g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e075" xlink:type="simple"/>
<mml:math display="block" id="M75" overflow="scroll">
<mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>F</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>F</mml:mi></mml:mstyle><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(33)</label>
</disp-formula>
with random amplitudes <italic>a</italic><sub><italic>k</italic></sub> drawn from a log-normal distribution. We fixed the phase offset of the images and neurons to 0, such that the output of the filters is never negative.</p>
<table-wrap id="pcbi.1004218.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004218.t002</object-id>
<label>Table 2</label> <caption><title>Parameters of the model filters.</title></caption>
<alternatives>
<graphic id="pcbi.1004218.t002g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.t002" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"><underline>Symbol</underline></th>
<th align="left" rowspan="1" colspan="1"><underline>Meaning</underline></th>
<th align="left" rowspan="1" colspan="1"><underline>Value</underline></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>N</italic></td>
<td align="left" rowspan="1" colspan="1">Number of neurons</td>
<td align="left" rowspan="1" colspan="1">50</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>P</italic></td>
<td align="left" rowspan="1" colspan="1">RF side length (pixels)</td>
<td align="left" rowspan="1" colspan="1">32</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>θ</italic></td>
<td align="left" rowspan="1" colspan="1">Preferred orientation (degrees)</td>
<td align="left" rowspan="1" colspan="1"><inline-formula id="pcbi.1004218.e076"><alternatives><graphic id="pcbi.1004218.e076g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e076" xlink:type="simple"/><mml:math display="inline" id="M76" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>180</mml:mn><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:mn>360</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:mn>180</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>σ</italic></td>
<td align="left" rowspan="1" colspan="1">Gaussian envelope std (pixels)</td>
<td align="left" rowspan="1" colspan="1"><italic>P</italic>/5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>λ</italic></td>
<td align="left" rowspan="1" colspan="1">Preferred spatial wavelength (pixel/cycle)</td>
<td align="left" rowspan="1" colspan="1"><italic>P</italic>/1.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>ϕ</italic></td>
<td align="left" rowspan="1" colspan="1">Preferred spatial phase</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>g</italic><sub><italic>k</italic></sub></td>
<td align="left" rowspan="1" colspan="1">Tuning amplitude</td>
<td align="left" rowspan="1" colspan="1"><italic>g</italic><sub><italic>k</italic></sub> = <italic>ga</italic><sub><italic>k</italic></sub>; <italic>g</italic> = 30; <italic>a</italic><sub><italic>k</italic></sub> ∼ <italic>LogNormal</italic> (0.25)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We considered a fine discrimination between stimuli <italic>θ</italic><sub>+</sub> and <italic>θ</italic><sub>−</sub>, with <italic>d θ</italic> = 7 degrees. To compute the ground-truth information, we first evaluated the local tuning curve derivatives and covariance:
<disp-formula id="pcbi.1004218.e077">
<alternatives>
<graphic id="pcbi.1004218.e077g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e077" xlink:type="simple"/>
<mml:math display="block" id="M77" overflow="scroll">
<mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>'</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
</disp-formula>
and then used Eq (<xref rid="pcbi.1004218.e001" ref-type="disp-formula">1</xref>).</p>
<p>To evaluate information with the bias-corrected estimator and the decoder, we generated synthetic population responses from the network. Each simulated experiment comprised T trials per stimulus condition. The number of trials varied between 50 and 4000 (see figures), and for each T we ran 200 experiments. An experiment started by sampling images from Eq (<xref rid="pcbi.1004218.e072" ref-type="disp-formula">31</xref>), then taking the dot product between the images and the neural filters, Eq (<xref rid="pcbi.1004218.e073" ref-type="disp-formula">32</xref>). The filters’ outputs were then half-rectified, and used to define the mean of the Poisson process from which we sampled the spike counts, to produce realistic response variability. Therefore, the spike count of neuron <italic>k</italic> during trial <italic>t</italic> was:</p>
<disp-formula id="pcbi.1004218.e078">
<alternatives>
<graphic id="pcbi.1004218.e078g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e078" xlink:type="simple"/>
<mml:math display="block" id="M78" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>F</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>J</mml:mi></mml:mstyle><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(34)</label>
</disp-formula>
<p>Note that in this model, the information in the cortical population cannot exceed the information in the input image, namely <inline-formula id="pcbi.1004218.e079"><alternatives><graphic id="pcbi.1004218.e079g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004218.e079" xlink:type="simple"/><mml:math display="inline" id="M79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>J</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, correlations in this model limit information (i.e. the responses contain differential correlations [<xref rid="pcbi.1004218.ref009" ref-type="bibr">9</xref>]).</p>
<p>In <xref rid="pcbi.1004218.g003" ref-type="fig">Fig 3</xref>, we considered the responses of two populations with different parameters, called population A and B. For population A we used the parameter values of <xref rid="pcbi.1004218.t002" ref-type="table">Table 2</xref>. For population B, we used filters with smaller Gaussian envelope (<italic>σ</italic> = <italic>P</italic> / 8) and shorter preferred spatial wavelength (<italic>λ</italic> = P / 3).</p>
<p>Note that due to the half-rectification, Eq (<xref rid="pcbi.1004218.e073" ref-type="disp-formula">32</xref>) and Eq (<xref rid="pcbi.1004218.e074" ref-type="disp-formula">33</xref>) are only approximations to the true tuning and covariance. In another set of simulations (described in <xref rid="pcbi.1004218.s001" ref-type="supplementary-material">S1 Text</xref>) we considered a different model where the true tuning curves and covariance (and therefore the true information) are known exactly, and verified that the results were unchanged. Specifically, the direct estimator bias and variance were exactly predicted by Eq (<xref rid="pcbi.1004218.e007" ref-type="disp-formula">2</xref>) and Eq (<xref rid="pcbi.1004218.e045" ref-type="disp-formula">19</xref>), respectively (<xref rid="pcbi.1004218.s002" ref-type="supplementary-material">S1 Fig</xref> and <xref rid="pcbi.1004218.s004" ref-type="supplementary-material">S3 Fig</xref>).</p>
</sec>
<sec id="sec021">
<title>9. Experimental procedures</title>
<p>Data were collected from 1 adult male monkey (<italic>macaca fascicularis</italic>). Animal preparation and general methods were described previously [<xref rid="pcbi.1004218.ref029" ref-type="bibr">29</xref>]. In brief, anesthesia was induced with ketamine (10 mg/kg) and maintained during surgery with isoflurane (1.0–2.5% in 95% O2). During recordings, anesthesia was maintained by sufentanil citrate (6–18 μg/kg/hr, adjusted as needed). Vecuronium bromide (0.15 mg/kg/hr) was used to suppress eye movements. The use of anesthesia allowed us to present a large number of trials, while ensuring precise and reproducible retinal positioning across trials. All procedures were approved by the Albert Einstein College of Medicine at Yeshiva University and followed the guidelines in the United States Public Health Service Guide for the Care and Use of Laboratory Animals.</p>
<p>We recorded neuronal activity using arrays of 10 × 10 microelectrodes (400 μm spacing, 1 mm length) inserted in the opercular region of V1. Waveform segments that exceeded a threshold (a multiple of the RMS noise on each channel) were digitized (30 kHz) and sorted off-line. For all analysis we included signals from well-isolated single units as well as small multi-unit clusters, and refer to both as neurons.</p>
<p>We first measured the spatial RF of each neuron, using small gratings (0.5 degrees in diameter; 4 orientations; 250 ms presentation) presented at a range of positions. The receptive field center of each neuron was defined as the location of the peak of a 2-dimensional Gaussian fit to the spatial activity map (across the population, median <italic>R</italic><sup><italic>2</italic></sup> = 0.79). We then measured the preferred orientation and spatial frequency of each neuron. Orientation tuning was measured with gratings drifting in 16 different directions, in 22.5 deg steps. Spatial frequency was measured at 4 orientations (0, 45, 90, and 135), with gratings whose spatial frequency varied between 0.1 and 8 cycles per degree. We used this information to align our stimuli with the center of the aggregate receptive field, and determine the orientation and spatial frequency of the stimuli closest to the preference of the sampled population.</p>
<p>The stimuli for the main experiment were static sinusoidal grating patches with a diameter of 2 degrees (100 pixels). The gratings’ Michelson contrast was 0.25 and the orientation was offset by {−7, 0, 14} degrees from the population preference. The pixel noise was drawn from a Gaussian distribution with standard deviation of either 0% (i.e., no noise) or 24% of the range of pixel values [0, 255]. To ensure that the spatial frequency content of the noise did not exceed the typical high frequency cutoff for parafoveal V1 (approximately 6 to 8 cycles/degree [<xref rid="pcbi.1004218.ref030" ref-type="bibr">30</xref>]), we first downsampled the gratings by a factor of 4 (corresponding to 12.5 pixels/degree), then added the pixel noise, and then upsampled by a factor of 4 by copying each pixel value in blocks of 4×4 pixels. After adding noise, pixels values outside the range [0, 255] were clipped.</p>
<p>We displayed stimuli on a calibrated CRT monitor (1024 × 768 pixels; 100 Hz frame rate; ~40 cd/m<sup>2</sup> mean luminance) placed 110 cm from the animal, using custom software. All stimuli were displayed in pseudo-random order for 250 ms each, followed by a 250 ms uniform gray screen. Each stimulus was presented 900 times. Stimuli were presented monocularly in a circular aperture surrounded by a gray field of average luminance.</p>
</sec>
</sec>
</sec>
<sec id="sec022">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004218.s001" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s001" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Synthetic tuning curve model.</title>
<p>Details of the model of synthetic tuning curves and covariances used for supporting <xref rid="pcbi.1004218.s002" ref-type="supplementary-material">S1 Fig</xref> and <xref rid="pcbi.1004218.s004" ref-type="supplementary-material">S3</xref> Fig</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s002" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s002" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>(a) Histogram of differences between the predicted and empirical variance of the bias-corrected estimator, relative to the empirical variance.</title>
<p>Simulations are based on the model described in <xref rid="pcbi.1004218.s001" ref-type="supplementary-material">S1 Text</xref>, with <italic>N</italic> = 100 neurons, 1000 simulated experiments and 200 trials per experiment per stimulus condition. The blue triangle at the top represents the mean relative difference. (<bold>b</bold>) Predicted variance (blue line) and empirical variance (dashed black line), as a function of the number of trials. The shaded area represents the standard deviation of the predicted variance across experiments.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s003" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s003" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Relative error for the decoder (a,b) and the bias-corrected estimator (c,d), for different population sizes: Continuous lines, <italic>N</italic> = 250, dashed lines, <italic>N</italic> = 50.</title>
<p>Simulations are based on the model used in the main text, with the same parameters except the number of filters. (<bold>a,c</bold>) Fisher information in the original data. (<bold>b,d</bold>) Fisher information in the shuffled data. Note that in the shuffled data differential correlations are destroyed and the code is not robust, hence a slightly suboptimal decoder (e.g. one trained on finite data) is expected to miss much of the information, and to perform worse for larger than smaller populations. This is illustrated in (<bold>b</bold>), where the estimation error for the decoder increases with population size, as opposed to (<bold>a</bold>) where the error is relatively insensitive to population size. The direct estimator does not suffer from this issue: the estimation error for the shuffled information decreases, rather than increase, with population size (<bold>d</bold>).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s004" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s004" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>(a) Each bar represents the relative error when using the direct estimator for the original data, the shuffled data, and the factorized decoder.</title>
<p>Simulations are based on the model described in <xref rid="pcbi.1004218.s001" ref-type="supplementary-material">S1 Text</xref>, with <italic>N</italic> = 100 neurons, 1000 simulated experiments and 200 trials per experiment per stimulus condition. (<bold>b-d</bold>) Histograms of differences between the predicted and empirical Fisher information, for the original data (<bold>b</bold>), the shuffled data (<bold>c</bold>), and the factorized decoder (<bold>d</bold>). All histograms are centered at 0, hence the estimators are unbiased.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s005" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s005" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>MSE for the data of <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref>, for bias-corrected estimator (blue) and cross-validated decoder (red).</title>
<p>The ground truth information value is not available for cortical data, therefore we used the arithmetic mean between the training set and validation set estimates obtained with the decoder at <italic>T</italic> = 900. Data are recorded from a population of <italic>N</italic> = 52 macaque V1 neurons. Conventions are as in <xref rid="pcbi.1004218.g006" ref-type="fig">Fig 6</xref> in the main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s006" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s006" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Fisher information when removing correlations entirely by shuffling the data (top, denoted I<sub>shuf</sub>), and when decoding under the assumption that the data are independent (bottom, denoted I<sub>diag</sub>).</title>
<p>Data are recorded from a population of <italic>N</italic> = 52 macaque V1 neurons. All conventions are as in <xref rid="pcbi.1004218.g005" ref-type="fig">Fig 5</xref> in the main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s007" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s007" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Comparison of estimators at low spike counts.</title>
<p>Data were generated using a model with Von Mises tuning curves and independent Poisson variability, with N = 50 neurons, and population-averaged spike count per trial matched to main Fig <xref rid="pcbi.1004218.g007" ref-type="fig">7A</xref> and <xref rid="pcbi.1004218.g007" ref-type="fig">7B</xref>. (<bold>a</bold>) Estimate of the Fisher information obtained by decoding (red) or direct estimation with bias correction (blue). The continuous lines represent the mean, the shaded area represents ±1 std across experiments, computed by bootstrap. (<bold>b</bold>) MSE of the decoder-based estimate (red) and the direct estimator (blue).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004218.s008" xlink:href="info:doi/10.1371/journal.pcbi.1004218.s008" mimetype="application/pdf" position="float" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>(a) Comparison between estimates obtained by the bias-corrected estimator (blue, BC) and different decoding methods.</title>
<p>Left: early-stopping (CV), same as <xref rid="pcbi.1004218.g001" ref-type="fig">Fig 1B</xref> in main text; center: variational Bayes decoder (VB); leave-one-out cross validation with L2 regularization (LOOCV), with regularization parameter set to 0.1. Upper and lower bounds for the decoders correspond to information estimated from training set and validation set, respectively. Dashed black line denotes ground truth information. (<bold>b</bold>) Mean squared errors for all estimation methods. (<bold>c</bold>) Run time (in seconds) per experiment, for different estimators: Bias-corrected estimator (blue, BC); decoding with early-stopping (red, CV); variational Bayes decoder (brown, VB); leave-one-out cross validation with L2 regularization (green, LOOCV) with regularization parameter set to 0.1. Data were generated using the model of the main text, with <italic>N</italic> = 50 neurons. The code was run in Mathworks Matlab 7 (R2012a) on a workstation with Windows 7, processor Intel Core i7 2.70 GHz, 32 GB RAM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank J. Beck for providing the early-stopping code; J. Drugowitsch for making available the Variational Bayes regularization code; and members of the Kohn lab for help with data collection.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004218.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Series</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name> (<year>2004</year>) <article-title>Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>1129</fpage>–<lpage>1135</lpage>. <object-id pub-id-type="pmid">15452579</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graf</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jazayeri</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Movshon</surname> <given-names>J</given-names></name> (<year>2011</year>) <article-title>Decoding the activity of neuronal populations in macaque primary visual cortex</article-title>. <source>Nat Neurosci</source> <volume>14</volume>: <fpage>239</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2733" xlink:type="simple">10.1038/nn.2733</ext-link></comment> <object-id pub-id-type="pmid">21217762</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cotton</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>A.</given-names></name>. (<year>2012</year>) <article-title>A Fast and Simple Population Code for Orientation in Primate V1</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>10618</fpage>–<lpage>10626</lpage>. <object-id pub-id-type="pmid">22855811</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paradiso</surname> <given-names>M</given-names></name> (<year>1988</year>) <article-title>A theory of the use of visual orientation information which exploits the columnar structure of striate cortex</article-title>. <source>Biological Cybernetics</source> <volume>58</volume>: <fpage>35</fpage>–<lpage>49</lpage>. <object-id pub-id-type="pmid">3345319</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>1999</year>) <article-title>The effect of correlated variability on the accuracy of a population code</article-title>. <source>Neural Comput</source> <volume>11</volume>: <fpage>91</fpage>–<lpage>101</lpage>. <object-id pub-id-type="pmid">9950724</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zoccolan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>N</given-names></name> (<year>2012</year>) <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source> <volume>73</volume>: <fpage>415</fpage>–<lpage>434</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.01.010" xlink:type="simple">10.1016/j.neuron.2012.01.010</ext-link></comment> <object-id pub-id-type="pmid">22325196</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name> (<year>2001</year>) <chapter-title>Theoretical Neuroscience</chapter-title>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004218.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bejjanki</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name> (<year>2011</year>) <article-title>Insights from a simple expression for linear fisher information in a recurrently connected population of spiking neurons</article-title>. <source>Neural Comput</source> <volume>23</volume>: <fpage>1484</fpage>–<lpage>1502</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00125" xlink:type="simple">10.1162/NECO_a_00125</ext-link></comment> <object-id pub-id-type="pmid">21395435</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moreno-Bote</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kanitscheider</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pitkow</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. (<year>2014</year>) <article-title>Information-limiting correlations</article-title>. <source>Nat Neurosci</source> <volume>17</volume>: <fpage>1410</fpage>–<lpage>1417</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3807" xlink:type="simple">10.1038/nn.3807</ext-link></comment> <object-id pub-id-type="pmid">25195105</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maunsell</surname> <given-names>J</given-names></name> (<year>2009</year>) <article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>1594</fpage>–<lpage>1600</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2439" xlink:type="simple">10.1038/nn.2439</ext-link></comment> <object-id pub-id-type="pmid">19915566</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nakahara</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name> (<year>2001</year>) <article-title>Population Coding with Correlation and an Unfaithful Model</article-title>. <source>Neural Comput</source> <volume>13</volume>: <fpage>775</fpage>–<lpage>797</lpage>. <object-id pub-id-type="pmid">11255569</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ecker</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>The effect of noise correlations in populations of diversely tuned neurons</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>14272</fpage>–<lpage>14283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2539-11.2011" xlink:type="simple">10.1523/JNEUROSCI.2539-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21976512</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name> (<year>2011</year>) <article-title>Measuring and interpreting neuronal correlations</article-title>. <source>Nat Neurosci</source> <volume>14</volume>: <fpage>811</fpage>–<lpage>819</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2842" xlink:type="simple">10.1038/nn.2842</ext-link></comment> <object-id pub-id-type="pmid">21709677</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref014"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">von Rosen D (1988) Moments for the inverted Wishart distribution. Scandinavian Journal of Statistics: 97–109.</mixed-citation></ref>
<ref id="pcbi.1004218.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sundberg</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>J</given-names></name> (<year>2009</year>) <article-title>Spatial Attention Decorrelates Intrinsic Activity Fluctuations in Macaque Area V4</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>879</fpage>–<lpage>888</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.09.013" xlink:type="simple">10.1016/j.neuron.2009.09.013</ext-link></comment> <object-id pub-id-type="pmid">19778515</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dosher</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Z</given-names></name> (<year>1999</year>) <article-title>Mechanisms of perceptual learning</article-title>. <source>Vision Res</source> <volume>39</volume>: <fpage>3197</fpage>–<lpage>3221</lpage>. <object-id pub-id-type="pmid">10615491</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fetsch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Fok</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sunkara</surname> <given-names>A</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Perceptual learning reduces interneuronal correlations in macaque visual cortex</article-title>. <source>Neuron</source> <volume>71</volume>: <fpage>750</fpage>–<lpage>761</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.06.015" xlink:type="simple">10.1016/j.neuron.2011.06.015</ext-link></comment> <object-id pub-id-type="pmid">21867889</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gutnisky</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dragoi</surname> <given-names>V</given-names></name> (<year>2008</year>) <article-title>Adaptive coding of visual information in neural populations</article-title>. <source>Nature</source> <volume>452</volume>: <fpage>220</fpage>–<lpage>224</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature06563" xlink:type="simple">10.1038/nature06563</ext-link></comment> <object-id pub-id-type="pmid">18337822</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adibi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McDonald</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Clifford</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Arabzadeh</surname> <given-names>E</given-names></name> (<year>2013</year>) <article-title>Adaptation Improves Neural Coding Efficiency Despite Increasing Correlations in Variability</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>2108</fpage>–<lpage>2120</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3449-12.2013" xlink:type="simple">10.1523/JNEUROSCI.3449-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23365247</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name> (<year>2006</year>) <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>1432</fpage>–<lpage>1438</lpage>. <object-id pub-id-type="pmid">17057707</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Averbeck</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name> (<year>2006</year>) <article-title>Effects of noise correlations on information encoding and decoding</article-title>. <source>J Neurophysiol</source> <volume>95</volume>: <fpage>3633</fpage>–<lpage>3644</lpage>. <object-id pub-id-type="pmid">16554512</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Averbeck</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name> (<year>2006</year>) <article-title>Neural correlations, population coding and computation</article-title>. <source>Nat Rev Neurosci</source> <volume>7</volume>: <fpage>358</fpage>–<lpage>366</lpage>. <object-id pub-id-type="pmid">16760916</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xie</surname> <given-names>X</given-names></name> (<year>2002</year>) <article-title>Threshold behaviour of the maximum likelihood method in population decoding</article-title>. <source>Network: Computation in Neural Systems</source> <volume>13</volume>: <fpage>447</fpage>–<lpage>456</lpage>. <object-id pub-id-type="pmid">12463339</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rotermund</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pawelzik</surname> <given-names>K</given-names></name> (<year>2002</year>) <article-title>Optimal short-term population coding: when Fisher information fails</article-title>. <source>Neural Computation</source> <volume>14</volume>: <fpage>2317</fpage>–<lpage>2351</lpage>. <object-id pub-id-type="pmid">12396565</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gerwinn</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>Reassessing optimal neural population codes with neurometric functions</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>108</volume>: <fpage>4423</fpage>–<lpage>4428</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1015904108" xlink:type="simple">10.1073/pnas.1015904108</ext-link></comment> <object-id pub-id-type="pmid">21368193</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref026"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Drugowitsch J (2014) Variational Bayesian inference for linear and logistic regression. arXiv 1310.5438 [stat.ML].</mixed-citation></ref>
<ref id="pcbi.1004218.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quian</surname> <given-names>Quiroga R</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name> (<year>2009</year>) <article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>173</fpage>–<lpage>185</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2578" xlink:type="simple">10.1038/nrn2578</ext-link></comment> <object-id pub-id-type="pmid">19229240</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref028"><label>28</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Johnson</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wichern</surname> <given-names>D</given-names></name> (<year>1988</year>) <chapter-title>Applied multivariate statistical analysis</chapter-title>: <publisher-name>Prentice-Hall, Inc</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004218.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jia</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name> (<year>2011</year>) <article-title>Stimulus Selectivity and Spatial Coherence of Gamma Components of the Local Field Potential</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>9390</fpage>–<lpage>9403</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0645-11.2011" xlink:type="simple">10.1523/JNEUROSCI.0645-11.2011</ext-link></comment> <object-id pub-id-type="pmid">21697389</object-id></mixed-citation></ref>
<ref id="pcbi.1004218.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foster</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gaska</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nagler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pollen</surname> <given-names>D</given-names></name> (<year>1985</year>) <article-title>Spatial and temporal frequency selectivity of neurones in visual cortical areas V1 and V2 of the macaque monkey</article-title>. <source>J Physiol</source> <volume>365</volume>: <fpage>331</fpage>–<lpage>363</lpage>. <object-id pub-id-type="pmid">4032318</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>