<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01039</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003412</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Encoding of Natural Sounds at Multiple Spectral and Temporal Resolutions in the Human Auditory Cortex</article-title>
<alt-title alt-title-type="running-head">Multi-resolution Sound Analysis in Auditory Cortex</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Santoro</surname><given-names>Roberta</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Moerel</surname><given-names>Michelle</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>De Martino</surname><given-names>Federico</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Goebel</surname><given-names>Rainer</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Ugurbil</surname><given-names>Kamil</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Yacoub</surname><given-names>Essa</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Formisano</surname><given-names>Elia</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Maastricht Brain Imaging Center (MBIC), Maastricht, The Netherlands</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Center for Magnetic Resonance Research, Department of Radiology, University of Minnesota, Minneapolis, Minnesota, United States of America</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Department of Neuroimaging and Neuromodeling, Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and Sciences (KNAW), Amsterdam, The Netherlands</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">e.formisano@maastrichtuniversity.nl</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: MM FDM EF RS. Performed the experiments: MM FDM. Analyzed the data: RS MM FDM EF. Contributed reagents/materials/analysis tools: RS FDM RG KU EY EF. Wrote the paper: RS EF.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>1</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>2</day><month>1</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>1</issue>
<elocation-id>e1003412</elocation-id>
<history>
<date date-type="received"><day>11</day><month>6</month><year>2013</year></date>
<date date-type="accepted"><day>12</day><month>11</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Santoro et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Functional neuroimaging research provides detailed observations of the response patterns that natural sounds (e.g. human voices and speech, animal cries, environmental sounds) evoke in the human brain. The computational and representational mechanisms underlying these observations, however, remain largely unknown. Here we combine high spatial resolution (3 and 7 Tesla) functional magnetic resonance imaging (fMRI) with computational modeling to reveal <italic>how</italic> natural sounds are represented in the human brain. We compare competing models of sound representations and select the model that most accurately predicts fMRI response patterns to natural sounds. Our results show that the cortical encoding of natural sounds entails the formation of multiple representations of sound spectrograms with different degrees of spectral and temporal resolution. The cortex derives these multi-resolution representations through frequency-specific neural processing channels and through the combined analysis of the spectral and temporal modulations in the spectrogram. Furthermore, our findings suggest that a spectral-temporal resolution trade-off may govern the modulation tuning of neuronal populations throughout the auditory cortex. Specifically, our fMRI results suggest that neuronal populations in posterior/dorsal auditory regions preferably encode coarse spectral information with high temporal precision. Vice-versa, neuronal populations in anterior/ventral auditory regions preferably encode fine-grained spectral information with low temporal precision. We propose that such a multi-resolution analysis may be crucially relevant for flexible and behaviorally-relevant sound processing and may constitute one of the computational underpinnings of functional specialization in auditory cortex.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>How does the human brain analyze natural sounds? Previous functional neuroimaging research could only describe the response patterns that sounds evoke in the human brain at the level of preferential regional activations. A comprehensive account of the neural basis of human hearing, however, requires deriving computational models that are able to provide quantitative predictions of brain responses to natural sounds. Here, we make a significant step in this direction by combining functional magnetic resonance imaging (fMRI) with computational modeling. We compare competing computational models of sound representations and select the model that most accurately predicts the measured fMRI response patterns. The computational models describe the processing of three relevant properties of natural sounds: frequency, temporal modulations and spectral modulations. We find that a model that represents spectral and temporal modulations jointly and in a frequency-dependent fashion provides the best account of fMRI responses and that the functional specialization of auditory cortical fields can be partially accounted for by their modulation tuning. Our results provide insights on how natural sounds are encoded in human auditory cortex and our methodological approach constitutes an advance in the way this question can be addressed in future studies.</p>
</abstract>
<funding-group><funding-statement>This work was supported by Maastricht University and the Netherlands Organization for Scientific Research (NWO grants 22-001-036, 453-12-002, 021-002-102), the National Institutes of Health (NIH grants P41 EB015894, P30 NS076408, and S10 RR26783), and the WM KECK Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Understanding how natural sounds and scenes are processed in the human auditory cortex remains a major challenge in auditory neuroscience. Current models of auditory cortical processing describe the sound-evoked neural response patterns at the level of preferential regional activations for certain behavioral tasks (e.g. localization vs recognition <xref ref-type="bibr" rid="pcbi.1003412-Alain1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Ahveninen1">[2]</xref>), sound categories (e.g. voices, speech <xref ref-type="bibr" rid="pcbi.1003412-Belin1">[3]</xref>) and (complex) acoustic features <xref ref-type="bibr" rid="pcbi.1003412-Lewis1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Leaver1">[5]</xref>. However, the computational and representational mechanisms underlying these responses remain largely unknown. The overall aim of the present study is to derive a computational model of <italic>how</italic> natural sounds are encoded in the human brain by combining high-resolution fMRI (3 and 7 Tesla) with computational modelling.</p>
<p>Most natural sounds are characterized by modulations of acoustic energy in both the spectral and temporal dimensions (<xref ref-type="fig" rid="pcbi-1003412-g001">Figure 1A</xref>). These modulations occur at multiple scales <xref ref-type="bibr" rid="pcbi.1003412-Singh1">[6]</xref> and are crucial for behaviorally relevant auditory processing such as speech intelligibility <xref ref-type="bibr" rid="pcbi.1003412-Shannon1">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Elliott1">[10]</xref>. Psychophysical investigations indicate that humans are able to detect and discriminate modulations that occur in one dimension alone (temporal: <xref ref-type="bibr" rid="pcbi.1003412-Viemeister1">[11]</xref>; spectral: <xref ref-type="bibr" rid="pcbi.1003412-Green1">[12]</xref>) as well as combined spectro-temporal modulations <xref ref-type="bibr" rid="pcbi.1003412-Chi1">[9]</xref>. Similarly, neurophysiological studies in animals and humans have revealed neuronal tuning for temporal modulations <xref ref-type="bibr" rid="pcbi.1003412-Joris1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Barton1">[15]</xref> and spectral modulations <xref ref-type="bibr" rid="pcbi.1003412-Shamma1">[16]</xref> alone, and the combination of the two <xref ref-type="bibr" rid="pcbi.1003412-Kowalski1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Schnwiesner1">[21]</xref>. This evidence suggests that spectral and temporal modulations are critical stimulus dimensions for the processing of sounds in the auditory cortex. Just as the cochlea generates multiple “views” of the sound pressure wave at different frequencies, an explicit encoding of spectral and temporal modulations would allow the cortex generating multiple “views” of the sound spectrogram with different degrees of spectral and temporal resolution <xref ref-type="bibr" rid="pcbi.1003412-Chi2">[22]</xref> (<xref ref-type="fig" rid="pcbi-1003412-g001">Figure 1B</xref>). Multiple simultaneous representations of the same incoming sounds may be crucially relevant for enabling flexible behavior, as different goal-oriented sound processing (e.g. sound localization or identification) may benefit from different types of representations. Furthermore, the representations of sounds at multiple resolutions may provide the computational basis for binding acoustic elements in sound mixtures and solve complex auditory scenes <xref ref-type="bibr" rid="pcbi.1003412-Elhilali1">[23]</xref>.</p>
<fig id="pcbi-1003412-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g001</object-id><label>Figure 1</label><caption>
<title>Sound examples and multi-resolution decomposition.</title>
<p>(A) Spectrogram of four exemplary natural sounds used in this study as extracted by the computational model mimicking early auditory processing. Natural sounds exhibit modulations of acoustic energy along both frequency and time. (B) Multi-resolution representation of the leftmost spectrogram of panel A. Different “views” are obtained as output of modulation channels tuned to specific spectral modulation (Ω) and temporal modulation (ω) frequencies. Each channel represents the spectrogram with a different combination of spectral and temporal detail.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g001" position="float" xlink:type="simple"/></fig>
<p>Despite extensive investigations in a variety of experimental settings, the specific computational mechanisms used by the human auditory cortex to represent energy modulations in the spectrogram of natural sounds are still a matter of speculation. Here, we use an fMRI “encoding” approach <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref> to compare competing computational models of sound representations and select the <italic>best</italic> model as the one that can predict most accurately fMRI response patterns to natural sounds. We focus on three well-defined aspects of the representation of spectral and temporal modulations: (1) <italic>dependency</italic>, (2) <italic>frequency specificity</italic>, and (3) <italic>spatial organization</italic>.</p>
<p>Dependency refers to the relation between spectral and temporal processing. The spectrogram of natural sounds is characterized by concurrent spectral and temporal modulations and these sound qualities might be represented jointly or independently of each other. An <italic>independent</italic> representation implies separate processing mechanisms for spectral and temporal modulations, such that the response to one dimension is invariant to a change in the other dimension. By contrast, a <italic>joint</italic> representation relies on combined selectivity for the conjunction of spectral and temporal modulations. The joint representation can be modeled as an array of spectro-temporal filters that are selective for combinations of spectral and temporal modulations (<xref ref-type="supplementary-material" rid="pcbi.1003412.s001">Figure S1A</xref>), whereas the independent representation can be seen as a bank of filters that are selective for either temporal or spectral modulations (<xref ref-type="supplementary-material" rid="pcbi.1003412.s001">Figure S1B</xref>). In other words, the two models differ with respect to the dimensions employed by the auditory cortex to encode natural sounds (combined spectro-temporal modulations, and spectral and temporal modulations alone, respectively). Testing for the interdependency of spectral and temporal modulation processing has relevant implications, as the superiority of such a model would indicate that results obtained using sounds that only vary along one dimension (e.g. amplitude modulated tones or stationary ripples) cannot be generalized to mechanisms of representation and processing of natural sounds.</p>
<p>The analysis of the spectro-temporal modulation content of the sound spectrogram can be global (2D Fourier transform) or localized (e.g wavelet transform). A global representation indicates integration along the frequency axis, while in a local analysis spectral and temporal modulations are encoded in a <italic>frequency-specific</italic> fashion. Frequency specific responses are ubiquitous in the auditory cortex; yet it is not clear how this dimension is exploited for the representation of natural sounds. Understanding the nature of the modulation analysis performed by the human auditory cortex can provide insights about the functional role of this representational mechanism.</p>
<p>Finally, the third aspect that we consider is the existence and layout of a large-scale spatial organization of spectro-temporal modulation tuning. Topographic maps of stimulus dimensions are a well-established organizational principle of the auditory cortex <xref ref-type="bibr" rid="pcbi.1003412-Schreiner1">[25]</xref>. In humans, the primary <xref ref-type="bibr" rid="pcbi.1003412-Formisano1">[26]</xref> as well as the non-primary <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref> auditory cortex contain multiple topographic representations of sound frequency (tonotopic maps). Beyond tonotopy, however, the spatial organization of other sound features remains elusive <xref ref-type="bibr" rid="pcbi.1003412-Schreiner1">[25]</xref>. Our methodological approach provides the possibility to obtain maps of multiple sound features and feature-combinations from the same set of fMRI responses and within the ecologically and behaviorally-relevant context of natural sounds processing. Here, we exploit this possibility to study the regional specificity and the spatial organization of spectro-temporal modulation tuning. Such knowledge can reveal the representational and computational basis underlying the functional specialization of auditory cortical subdivisions.</p>
<p>Our results show that the human brain forms multiple representations of incoming natural sounds at distinct spectral and temporal resolutions. The encoding of spectral and temporal modulations is <italic>joint</italic> and <italic>frequency-specific</italic> and is governed by a trade-off between spectral and temporal resolution. Regional variations of voxels modulation preference put forward the hypothesis that the functional specialization of auditory cortical fields can be partially accounted for by their modulation tuning.</p>
</sec><sec id="s2">
<title>Results</title>
<p>We modeled the data from two fMRI experiments in humans (3 <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref> and 7T <xref ref-type="bibr" rid="pcbi.1003412-DeMartino1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Moerel2">[29]</xref>). In both experiments, fMRI responses were recorded from the auditory cortex while subjects (n = 5, different for the two experiments) listened to a large set of natural sounds, including speech samples, music pieces, animal cries, scenes from nature, and tool sounds (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003412.s009">Text S1</xref>).</p>
<sec id="s2a">
<title>Prediction accuracy of the joint frequency-specific MTF-based model</title>
<p>We applied an “encoding” approach (see <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003412.s002">Figure S2</xref>) and compared several computational models of auditory processing. A first model we tested describes auditory cortical neurons as a bank of frequency-localized filters with joint selectivity for spectral and temporal modulations (see <xref ref-type="bibr" rid="pcbi.1003412-Chi2">[22]</xref> and <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Considering that one voxel reflects the mass activity of a great number of neurons, we modelled each voxel's receptive field as a combination of modulation selective filters, each tuned to a different spectral modulation, temporal modulation and frequency (<xref ref-type="fig" rid="pcbi-1003412-g002">Figure 2</xref>, panel A). Using a subset of fMRI data (training), we estimated a modulation transfer function (MTF, <xref ref-type="fig" rid="pcbi-1003412-g002">Figure 2</xref>, panel A1) for each voxel (see <xref ref-type="fig" rid="pcbi-1003412-g003">Figure 3</xref> for two MTF examples). We then assessed the ability of this MTF-based model to accurately predict the fMRI responses in new, independent data sets (testing). In the 3T experiment, training and testing data involved a single set of natural sounds, whereas two completely distinct sound sets were used for the 7T training and testing datasets. We quantified model's prediction accuracy by performing a sound identification analysis <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref>. Namely, we used the fMRI activity patterns predicted by the estimated models to identify which sound had been heard among all sounds in the test set. Each testing sound was assigned with a score ranging between 0 and 1 and indicating the rank of the correlation between sound's predicted and measured activity patterns (0 indicates that the predicted activity pattern for a given stimulus was least similar to the measured one among all test stimuli; 1 indicates correct identification). The overall model's accuracy was obtained as the average score across all test sounds (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
<fig id="pcbi-1003412-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g002</object-id><label>Figure 2</label><caption>
<title>Overview of candidate encoding models.</title>
<p>(A) Joint modulation selective filters. (A1) Joint frequency specific: the spectrogram is filtered with a bank of modulation selective filters at different spectral modulations (Ω), temporal modulations (ω), and direction (upwards/downwards). The output of the filter bank is averaged across time and direction to yield a reduced representation of modulation energy as a function of Ω, ω, and frequency. The joint frequency-specific MTF-based model predicts that fMRI responses vary linearly with this representation, i.e. sounds that differ with respect to any of the three dimensions will elicit different responses. (A2) Joint frequency non-specific: the 3D modulation representation is averaged across frequency to yield a global measure of modulation energy. By concatenating modulation and frequency content (not shown here, see tonotopy model), the joint frequency non-specific model predicts separate processing for global, joint modulations and frequency. (B) Independent modulation selective filters. (B1) Independent frequency-specific: the spectrogram is filtered with purely spectral and purely temporal modulation selective filters and the output is averaged over time. This yields separate representations of spectral and temporal modulation energy as a function of frequency. The independent frequency-specific model predicts that the response of a voxel dedicated to spectral (temporal) processing will not be affected by a change in temporal (spectral) modulation content. (B2) Independent frequency non-specific: the two separate representations of spectral and temporal modulation energy are averaged across frequency to yield the global spectral and temporal modulation content. This representation is concatenated with the frequency content (not shown here, see tonotopy model) to simulate separate processing for frequency, spectral and temporal modulations. (C) Tonotopy model: the spectrogram is averaged over time and voxels are modeled as frequency selective units, whose response varies linearly with the frequency content of the input stimuli.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g002" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003412-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g003</object-id><label>Figure 3</label><caption>
<title>Sample MTFs, data and model prediction for two exemplary voxels (subject S10).</title>
<p>(A–B) Left: MTFs as estimated by the joint frequency-specific MTF-based model. The color code indicates the voxel's sensitivity to a given combination of frequency, spectral and temporal modulation. MTFs have been interpolated for display purposes. Middle: Marginal response profiles for temporal modulation (top), spectral modulation (middle) and frequency (bottom). Red circles and dashed lines indicate voxels' characteristic spectral modulation, temporal modulation and frequency, computed as the point of maximum of the marginal profiles (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Right: Measured and predicted response to the 24 stimuli in the test set. Responses are shown in z-score units. r indicates Pearson's correlation coefficient.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g003" position="float" xlink:type="simple"/></fig>
<p>For both the 3T and 7T datasets, the accuracy of the joint frequency-specific MTF-based model was significantly higher than chance (0.5) both at group level (3T: mean [SE] = 0.66 [0.02], p = 0.003; 7T: mean [SE] = 0.78 [0.03], p = 0.002; two-tailed paired t-test; <xref ref-type="fig" rid="pcbi-1003412-g004">Figure 4</xref>) and for each individual subject (p = 0.01 for subject S4, p = 0.005 for all other subjects, permutation test; <xref ref-type="fig" rid="pcbi-1003412-g005">Figure 5</xref>). Remarkably, for the 7T dataset the joint frequency-specific MTF-based model was able to generalize to stimuli not used for parameter estimation.</p>
<fig id="pcbi-1003412-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g004</object-id><label>Figure 4</label><caption>
<title>Comparison between models.</title>
<p>Bars indicate the prediction accuracy (mean ± SEM, N = 5) for the five models in both the 3T and 7T experiments. The joint frequency-specific MTF-based model showed significantly better prediction accuracy than all other models (see main text). Accuracies are normalized between 0 and 1. Chance level is 0.5.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g004" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003412-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g005</object-id><label>Figure 5</label><caption>
<title>Prediction accuracies for individual participants.</title>
<p>Accuracies of the joint frequency-specific MTF-based (left) and tonotopy (right) models are reported for the 3T (A) and 7T (B) datasets. Each panel shows the accuracy obtained with correct labels and the accuracy derived by permuting the sound labels before training the model.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Comparison between joint frequency-specific MTF-based model and tonotopy model</title>
<p>FMRI activity from voxels in primary and non-primary auditory regions reflects the tonotopic organization of neural responses. Therefore, as a control analysis we compared the prediction accuracy of the MTF-based model against the prediction accuracy of a tonotopy model, which incorporates the hypothesis that voxels simply reflect information about the frequency content of the stimuli (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> and <xref ref-type="fig" rid="pcbi-1003412-g002">Figure 2</xref>, panel C). The tonotopy model performed above chance both at group level (3T: mean [SE] = 0.62 [0.02], p = 0.002; 7T: mean [SE] = 0.69 [0.03], p = 0.004; two-tailed paired t-test; <xref ref-type="fig" rid="pcbi-1003412-g004">Figure 4</xref>) and for each individual subject (p = 0.015 for subject S4, p = 0.005 for all other subjects, permutation test; <xref ref-type="fig" rid="pcbi-1003412-g005">Figure 5</xref>). However, the tonotopy model performed significantly worse than the joint frequency-specific MTF-based model (3T: p = 0.009; 7T: p = 0.007; two-tailed paired t-test). The significant improvement of the MTF-based over the tonotopy model indicates that a model accounting for the joint, frequency-specific modulation content of the spectrogram is a better representation of fMRI responses to natural sounds.</p>
</sec><sec id="s2c">
<title>Comparison between frequency-specific and non-specific joint MTF-based models</title>
<p>To assess the relevance of frequency-localization in the encoding of joint spectro-temporal modulations, we trained a model that represents frequency and joint modulation content independently of each other (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> and <xref ref-type="fig" rid="pcbi-1003412-g002">Figure 2</xref>, panel A2). The joint frequency non-specific MTF-based model performed above chance both at group level (3T: mean [SE] = 0.63 [0.02], p = 0.004; 7T: mean [SE] = 0.71 [0.02], p = 0.0003; two-tailed paired t-test; <xref ref-type="fig" rid="pcbi-1003412-g004">Figure 4</xref>) and for each individual subject (p = 0.02 for subject S4, p = 0.01 for subject S6, p = 0.005 for all other subjects, permutation test). However, the frequency non-specific model performed significantly worse than the frequency-specific MTF-based model (3T: p = 0.002; 7T: p = 0.021; two-tailed paired t-test).</p>
</sec><sec id="s2d">
<title>Comparison between joint and independent frequency-specific MTF-based models</title>
<p>In order to quantify the contribution of joint selectivity to identification performance, we trained an independent frequency-specific MTF-based encoding model. We modelled each voxel's receptive field as a combination of purely temporal and purely spectral modulation selective filters, operating in a frequency-specific fashion (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> and <xref ref-type="fig" rid="pcbi-1003412-g002">Figure 2</xref>, panels B and B1). The independent model performed above chance both at group level (3T: mean [SE] = 0.63 [0.01], p = 0.001; 7T: mean [SE] = 0.72 [0.02], p = 0.0007; two-tailed paired t-test; <xref ref-type="fig" rid="pcbi-1003412-g004">Figure 4</xref>) and for each individual subject (p = 0.015 for subject S4, p = 0.01 for subject S7, p = 0.005 for all other subjects, permutation test). However, the independent model performed significantly worse than the joint MTF-based model (3T: p = 0.012; 7T: p = 0.011; two-tailed paired t-test).</p>
</sec><sec id="s2e">
<title>Comparison between joint frequency-specific and independent frequency non-specific MTF-based models</title>
<p>As an additional control, we tested a model that simulates independent selectivity for spectral modulations, temporal modulations and frequency (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> and <xref ref-type="fig" rid="pcbi-1003412-g002">Figure 2</xref>, panel B2). The independent frequency non-specific model performed above chance both at group level (3T: mean [SE] = 0.63 [0.02], p = 0.002; 7T: mean [SE] = 0.71 [0.02], p = 0.0008; two-tailed paired t-test; <xref ref-type="fig" rid="pcbi-1003412-g004">Figure 4</xref>) and for each individual subject (p = 0.01 for subject S1, S4 and S9, p = 0.005 for all other subjects, permutation test). However, the independent frequency non-specific model performed significantly worse than the joint frequency-specific MTF-based model (3T: p = 0.011; 7T: p = 0.016; two-tailed paired t-test).</p>
</sec><sec id="s2f">
<title>Spatial distribution of voxels' tuning properties</title>
<p>To investigate the cortical topography of voxels tuning properties, we computed maps of voxels characteristic spectral modulation (CSM), temporal modulation (CTM) and frequency (CF). For each feature, the estimated MTF was marginalized across irrelevant dimensions (i.e. spectral and temporal modulations for CF) and the point of maximum of the marginal sum was assigned as the voxel's preferred feature value (see example in <xref ref-type="fig" rid="pcbi-1003412-g003">Figure 3</xref>). We obtained maps of CSM, CTM and CF by color-coding the voxels' preferred values and projecting them onto an inflated representation of the subject's cortex (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Maps of CF confirmed the presence of multiple tonotopic gradients in primary auditory regions (Heschl's gyrus - HG) and surrounding superior temporal cortex <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref> (<xref ref-type="supplementary-material" rid="pcbi.1003412.s003">Figure S3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003412.s004">S4</xref>). The spatial distribution of voxels CSM and CTM appeared to be more complex and variable across subjects (<xref ref-type="fig" rid="pcbi-1003412-g006">Figure 6</xref> for the group and <xref ref-type="supplementary-material" rid="pcbi.1003412.s005">Figure S5</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003412.s006">S6</xref> for all individual subjects). However, the group data and the majority of the individual subjects suggested distinct regional sensitivities to modulation frequencies (see schematic summary in <xref ref-type="fig" rid="pcbi-1003412-g007">Figure 7</xref>). In both hemispheres, clusters with a preference for fine spectral modulations (high CSM, purple colors) were primarily and consistently localized along the HG and anterior superior temporal gyrus (STG) (see circles on group maps - <xref ref-type="fig" rid="pcbi-1003412-g006">Figure 6</xref>), while clusters with a preference for coarse spectral modulations (low CSM, orange color) were mostly located posterior-laterally to HG, on the planum temporal (PT) and on STG (see squares on group maps – <xref ref-type="fig" rid="pcbi-1003412-g006">Figure 6</xref>). Bilaterally, a preference for slow temporal modulations (low CTM, orange color) was found along HG and STG, whereas clusters with a preference for fast temporal modulations (high CTM, purple) were observed on the PT, posteriorly to HG and in a region medially adjacent to HG. Supporting the spatial dissociation between spectral and temporal modulation at map level, we found a significant negative correlation between voxels characteristic spectral and temporal modulation (3T: mean [SE] = −0.19 [0.01], p = 0.02; 7T: mean [SE] = −0.11 [0.01], p = 0.01; group level random effects two-tailed t test, see <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
<fig id="pcbi-1003412-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g006</object-id><label>Figure 6</label><caption>
<title>Group maps of CSM and CTM.</title>
<p>Maps are displayed for the 3T (A) and 7T (B) datasets. Top: inflated representation of the group cortex. Maps are shown in the cortical region highlighted by the black square. Middle, Bottom: purple denotes tuning for fine (fast) spectral (temporal) structures; orange denotes tuning for coarse (slow) spectral (temporal) features. The white circle and square outline anterior/ventral and posterior/dorsal auditory regions, respectively. The black line indicates HG.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g006" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003412-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003412.g007</object-id><label>Figure 7</label><caption>
<title>Summary of spatial distribution of voxels' tuning properties.</title>
<p>The cartoon is a schematic representation of regional preferences for spectral modulation (left), temporal modulation (middle), and frequency (right). The schematic of spectral and temporal modulation preference summarizes the most evident characteristics emerging from the complex spatial pattern of CSM and CTM. The cartoon of frequency preference shows the main tonotopic gradient in regions along and surrounding HG. Cartoon clusters are superimposed over the left hemisphere of the group cortex as derived from the experiment at 3T. The black line indicates HG.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003412.g007" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Mechanisms of spectral and temporal modulation processing</title>
<p>Our results show that the representation of natural sounds in the human auditory cortex relies on a frequency-specific analysis of combined spectro-temporal modulations. By showing superior performance of the joint MTF-based model over the independent model, we have demonstrated that the hypothesis of independent tuning for spectral <xref ref-type="bibr" rid="pcbi.1003412-Shamma1">[16]</xref> and temporal modulations <xref ref-type="bibr" rid="pcbi.1003412-Jepsen1">[30]</xref> is insufficient to account for the representation of natural sounds in the human auditory cortex. Furthermore, the frequency-specificity that we revealed indicates that the organization of the auditory cortex according to frequency extends beyond the representation of the spectral content of incoming sounds. We show that, at least for spectro-temporal modulations, the integration along the whole range of frequencies occurs at a later stage than the extraction of the feature itself.</p>
<p>The encoding mechanism that our results support is consistent with a recent study showing that a frequency-specific representation of combined spectro-temporal modulations allows the accurate reconstruction of speech in the human posterior superior temporal gyrus <xref ref-type="bibr" rid="pcbi.1003412-Pasley1">[31]</xref>. The present study generalizes these observations to sounds from natural categories other than speech. Furthermore, our results are in line with psychophysics studies showing that tuning for combined spectro-temporal modulations provides a better account of human behavior during the performance of auditory tasks <xref ref-type="bibr" rid="pcbi.1003412-Sabin1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Patil1">[33]</xref>.</p>
<p>Previous neuroimaging studies had examined the processing of spectral and temporal modulations by measuring the tuning to synthetic stimuli with varying spectral modulation frequency, temporal modulation frequency or the combination of the two. This approach suffers from two main limitations. First, natural sounds are complex stimuli with characteristic statistical regularities <xref ref-type="bibr" rid="pcbi.1003412-Singh1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Attias1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Escab1">[36]</xref> and it has been suggested that the auditory system is adapted to such regularities in order to efficiently encode sounds in natural settings <xref ref-type="bibr" rid="pcbi.1003412-Barlow1">[37]</xref>. Even the most complex synthetic stimuli lack both the statistical structure and the behavioral relevance of natural sounds; therefore there is not guarantee that they engage the auditory cortex in processing that is actually used during the analysis of natural sounds. Second, tuning per se only allows indirect inference on cortical encoding mechanisms: proofing a general computational strategy requires building a model that is able to predict brain responses to a broad range of natural stimuli <xref ref-type="bibr" rid="pcbi.1003412-Wu1">[38]</xref>. The approach that we followed in the present study allowed overcoming these limitations, therefore providing direct evidence for a specific encoding mechanism. However, two important caveats should be mentioned. First, by estimating a linear mapping between modulation acoustic space and fMRI responses, we only modeled the linear response properties of voxels. One might argue that because of the linear approximation, the use of natural sounds provides no advantage over synthetic stimuli (e.g. dynamic ripples). However, it has been shown that tuning properties of both auditory <xref ref-type="bibr" rid="pcbi.1003412-Theunissen1">[39]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Laudanski1">[41]</xref> and visual <xref ref-type="bibr" rid="pcbi.1003412-DavidS1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Talebi1">[43]</xref> neurons differ significantly under natural and synthetic stimulus condition and that linear models obtained from natural stimuli predict neurons responses significantly better. This shows that natural and synthetic stimuli activate neurons in a different manner and that, despite being an incomplete description, linear models estimated from responses to natural stimuli may be more accurate. We suggest that this is true also for models of voxels receptive fields. Second, it might be possible that some auditory cortical locations are selective to higher-level sound attributes (i.e. sound categories) that co-occur with specific spectro-temporal modulations. As a consequence of this co-occurrence, these locations would then be assigned with a preferred temporal and spectral modulation frequency, only in virtue of their category selectivity. To examine the role of category selectivity on our results, we performed additional analyses on the 7T dataset and tested a model that included categorical predictors together with the original MTF-based model (<xref ref-type="supplementary-material" rid="pcbi.1003412.s009">Text S1</xref>). The results showed that predictions of new sounds do not improve with the inclusion of categorical information (mean [SE] = 0.76 [0.03]) and that estimated CTM and CSM maps do not change (<xref ref-type="supplementary-material" rid="pcbi.1003412.s008">Figure S8</xref>). This analysis suggests that category tuning may result from preference to specific lower level features or combination of features. However, it would be important to further investigate this issue and compare responses and voxels receptive fields obtained with both natural and synthetic sounds (see <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref> for a similar comparison for frequency responses). Such an investigation is experimentally challenging, as it would require as many stimuli (dynamic ripples) as model parameters used in the present study. However, it could be crucial for understanding the relation between acoustic and perceptual levels of sound representation in the auditory cortex.</p>
</sec><sec id="s3b">
<title>Spatial topographies and relation to current functional models</title>
<p>On the basis of positron emission tomography responses to tone sequences that differed either in the temporal or spectral dimension, Zatorre and Belin <xref ref-type="bibr" rid="pcbi.1003412-Zatorre1">[44]</xref> reported a left-hemispheric preference for rapid temporal processing and complementary preference in the right hemisphere for fine-grained spectral analysis. While the analyses we conducted cannot exclude that hemispheric differences exist at regional level, our maps - obtained at a much higher spatial resolution and with natural sounds - suggest a more complex spatial pattern of spectral and temporal modulation preference within each hemisphere. The most evident characteristic is that – in both the hemispheres - regions located posterior-laterally to HG (see squares in <xref ref-type="fig" rid="pcbi-1003412-g006">Figure 6</xref> and the schematic summary in <xref ref-type="fig" rid="pcbi-1003412-g007">Figure 7</xref>) preferably encode coarse spectral information with high temporal precision while regions located along HG or antero-ventrally (see circles in <xref ref-type="fig" rid="pcbi-1003412-g006">Figure 6</xref> and the schematic summary in <xref ref-type="fig" rid="pcbi-1003412-g007">Figure 7</xref>) preferably encode fine-grained spectral information with low temporal precision. Both the two previous human neuroimaging studies that investigated tuning for combined spectro-temporal modulations with dynamic ripples (<xref ref-type="bibr" rid="pcbi.1003412-Langers1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Schnwiesner1">[21]</xref>) reported a role of anterior auditory regions in the analysis of fine spectral details, which is consistent with our observations, whereas results are less coherent for temporal modulation maps. Again, a direct comparison between maps obtained with dynamic ripples and natural sounds would be required to address this issue.</p>
<p>Our results of spatial topographies for CTM and CTF support the view that the auditory cortex forms multiple (parallel) representations of the incoming sounds at different spectro-temporal resolutions (<xref ref-type="bibr" rid="pcbi.1003412-Samson1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Bendor1">[46]</xref>). We suggest that this may be relevant for enabling flexible behavior, as different goal-oriented sound processing may benefit from different types of auditory representations. Importantly, this suggestion can be tested empirically in future experiments and studies where (natural) sounds are presented in the context of multiple behavioral tasks.</p>
<p>A spectral-temporal resolution “trade-off” analogous to the one reported here has previously been described for neurons in the inferior colliculus of the cat <xref ref-type="bibr" rid="pcbi.1003412-Rodrguez1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Rodrguez2">[48]</xref> and is in agreement with the low-pass behavior of the MTF of the human auditory cortex <xref ref-type="bibr" rid="pcbi.1003412-Schnwiesner1">[21]</xref> and the psychophysically derived detection thresholds for spectro-temporal modulations <xref ref-type="bibr" rid="pcbi.1003412-Chi1">[9]</xref>. Furthermore, modulation spectra of natural sounds exhibit a similar trade-off, i.e. natural sounds rarely present both high spectral and high temporal modulation frequencies <xref ref-type="bibr" rid="pcbi.1003412-Singh1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Elliott1">[10]</xref>. A match between stimulus statistics and neuronal response properties is generally interpreted as an evidence for the theory of efficient coding <xref ref-type="bibr" rid="pcbi.1003412-Woolley1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Escab1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Barlow1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Rodrguez2">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Simoncelli1">[49]</xref>. Thus, our data provide further support to the idea that the auditory system has adapted in order to efficiently encode the statistical regularities of natural sounds.</p>
</sec><sec id="s3c">
<title>Comparing computational models of auditory processing with fMRI</title>
<p>Besides providing insights into the representation of natural sounds in the human auditory cortex, our results pave the way to future research aiming at testing increasingly complex encoding models of auditory processing. The combination of fMRI and “encoding” techniques has proven to be a successful tool to investigate the representation of natural images in the human visual cortex <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Naselaris1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Nishimoto1">[51]</xref>, as well as to predict the brain activity associated with the meaning of words <xref ref-type="bibr" rid="pcbi.1003412-Mitchell1">[52]</xref>. In the auditory domain, the application of such powerful method has lagged behind. We have recently demonstrated that “encoding” makes it possible to detect the spectral tuning of voxels in the human auditory cortex from fMRI responses to natural sounds <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Moerel2">[29]</xref>. In the present study, we show that models embedding more complex representations than frequency selectivity can be learned from fMRI activity. The challenge for future studies is to explore more sophisticated voxels receptive field models. Here we only considered voxels tuning along three stimulus dimensions (frequency, spectral modulations and temporal modulations). However, natural sounds vary in a higher dimensional acoustic space and interactions with parameters not considered here might occur.</p>
<p>Interestingly, we consistently observed higher prediction accuracy for the 7T compared to the 3T dataset (<xref ref-type="fig" rid="pcbi-1003412-g004">Figure 4</xref>), despite the fact that at 7T the model was trained and tested on independent sound ensembles (while different presentations of the same sounds were used for the 3T data set). We interpret this difference as a result of the interplay between two important factors, namely the number of stimuli and the functional contrast to noise ratio (CNR). The larger amount of different sounds employed in the 7T experiment has probably increased the variance along the dimensions represented by the model; this, together with the higher CNR and the higher spatial specificity achieved at 7T, has likely led to a more accurate model estimation, which in turn has resulted in higher prediction accuracy. These observations provide important guidelines for the design of future experiments in this framework.</p>
<p>It should be mentioned that in our study, accuracy based on percent correct was significantly above chance ([12.5%, 12.5%, 16.7%, 20.8%, 25%] for subjects S6–S10 for the best performing model at 7T; chance = 4.2%), but still quite small compared to the outstanding results reported in similar encoding studies in the visual domain (e.g. <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref>). However, the distribution of ranks was skewed towards 1 (correct identification), indicating that for most sounds the correlation between predicted and measured response was ranked very high (e.g. second or third). The lower percent correct performance for sound identification can be ascribed to a variety of reasons. It might be due to the lower functional CNR, as BOLD responses observed in the auditory cortex are substantially lower than those in the visual cortex, probably because of the effects of the scanner noise <xref ref-type="bibr" rid="pcbi.1003412-Gaab1">[53]</xref>. Furthermore, our clustered fMRI acquisition with a silent gap between scans limits the number of sounds used for training/testing the model (compared e.g. to the number of images in <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref>). Finally, the model of receptive field based on spectro-temporal modulations might be too simple for allowing distinguishing two acoustically similar sounds (e.g. two speech sounds).</p>
<p>Although the proposed combination of high field fMRI with the encoding approach is valuable for testing well-defined hypotheses on sound processing in the human brain, there are intrinsic limitations. A voxel - even at the high spatial resolution achievable with 7T fMRI - samples a large number of neurons and the relation between the measured BOLD signal and the neural activation is only partly understood. Results based on BOLD fMRI (and thus fMRI encoding) reflect a complex mixture of neuronal (spiking and synaptic activity, excitation, inhibition) as well as neurovascular phenomena. In particular, neural inhibition may be associated with both positive and negative BOLD, depending on the specific neural network configuration <xref ref-type="bibr" rid="pcbi.1003412-Logothetis1">[54]</xref>. Understanding the neuronal dynamics underlying our fMRI observations would thus require combining electrophysiological (at single-cell and neuronal population level) and fMRI investigations in animal models <xref ref-type="bibr" rid="pcbi.1003412-Logothetis2">[55]</xref> and/or humans <xref ref-type="bibr" rid="pcbi.1003412-Bitterman1">[40]</xref>.</p>
<p>In summary, our study represents a first demonstration of how fMRI data and “encoding” techniques can be successfully combined to test competing computational models of auditory processing and to concurrently estimate response properties of cortical locations along multiple dimensions within an ecologically valid framework. Also, by using a biologically inspired computational model, we pave the way for linking electrophysiology in animals and non-invasive research in humans.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Ethics statement</title>
<p>The Ethical Committee of the Faculty of Psychology and Neuroscience at Maastricht University and the Institutional Review Board for human subject research at the University of Minnesota granted approval for the study at 3T and 7T respectively.</p>
</sec><sec id="s4b">
<title>Experimental procedure</title>
<p>Subjects, stimuli, experimental design, MRI parameters, and data preprocessing have been reported in previous publications from our group <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003412-Moerel2">[29]</xref> (see <xref ref-type="supplementary-material" rid="pcbi.1003412.s009">Text S1</xref>). In the following, the most relevant details of the experimental design will be briefly described.</p>
<p>We used 60 (168) recordings of natural sounds for the 3T (7T) experiment. Stimuli included human vocal sounds (both speech and non-speech, e.g., baby cry, laughter, coughing), animal cries (e.g., dog, cat, horse), musical instruments (e.g., piano, flute, drums), scenes from nature (e.g., rain, wind, thunder), and tool sounds (e.g., keys, scissors, vacuum cleaner). Sounds were sampled at 16 kHz and their duration was cut at 1000 ms. Sound onset and offset were ramped with a 10 ms linear slope, and their energy (RMS) levels were equalized.</p>
<p>The 3T and 7T experiments consisted of 3 and 8 runs, respectively; in the 3T (7T) experiment, each run lasted approximately 25 (10) minutes. In the 7T experiment, data were subdivided into six train runs and two test runs. In the train runs, 144 of the 168 stimuli were presented with 3 repetitions overall (i.e. each sound was presented in 3 of the 6 train runs). The remaining 24 sounds were presented in the test runs and repeated 3 times per run.</p>
<p>Sounds were presented in the silent gap between acquisitions with a randomly assigned inter-stimulus interval of 2, 3, or 4 TRs - plus an additional random jitter. Zero trials (trials where no sound was presented; 10% of the trials in the 3T experiment; 6% (5%) of the trials in train (test) runs in the 7T experiment), and catch trials (trials in which the sound which was just heard was presented; 6% of the trials in the 3T experiment; 6% (3%) of the trials in train (test) runs in the 7T experiment) were included. Subjects responded with a button press when a sound was repeated. Catch trials were excluded from the analysis.</p>
</sec><sec id="s4c">
<title>Joint frequency-specific MTF-based model</title>
<p>The stimulus representation in the modulation space was obtained as the output of a biologically inspired model of auditory processing <xref ref-type="bibr" rid="pcbi.1003412-Chi2">[22]</xref>, that explicitly encodes the modulation content of a sound spectrogram. The auditory model consists of two main components: an <italic>early</italic> stage that accounts for the transformations that acoustic signals undergo in the early auditory system, from the cochlea to the midbrain; and a <italic>cortical</italic> stage that simulates the processing of the acoustic input at the level of the (primary) auditory cortex. The spectral analysis performed by the cochlea is mimicked by a bank of 128 overlapping bandpass filters with constant-Q (Q<sub>10 <italic>dB</italic></sub> = 3), equally spaced along a logarithmic frequency axis over a range of 5.3 oct (<italic>f</italic> = 180–7040 Hz). The output of each filter enters a hair cell stage, where it undergoes high-pass filtering, optional non-linear compression and low-pass filtering. A midbrain stage models the enhancement of frequency selectivity as a first-order derivative with respect to the frequency axis, followed by a half-wave rectification. Finally, a short-term temporal integration (time constant <italic>τ</italic> = 8 ms) accounts for the loss of phase locking observed in the midbrain. The auditory spectrogram generated by the early stage is further analyzed by the cortical stage, where neurons are modeled as 2-dimensional (2D) modulation selective filters that are tuned to a specific combination of spectral and temporal modulations, and operate over a limited range of frequencies along the tonotopic axis. These filters have constant Q and are directional, i.e. they respond either to upward or downward frequency sweeps. Computationally, the cortical filter bank performs a complex wavelet decomposition of the auditory spectrogram. The magnitude of such decomposition yields a phase-invariant measure of modulation content. Ultimately, the model's output is a multi-resolution representation of the spectrogram envelope as a function of time, frequency, spectral and temporal modulations, and directionality.</p>
<p>We derived the auditory spectrogram and its modulation content using the “NSL Tools” package (available at <ext-link ext-link-type="uri" xlink:href="http://www.isr.umd.edu/Labs/NSL/Software.htm" xlink:type="simple">http://www.isr.umd.edu/Labs/NSL/Software.htm</ext-link>) and customized Matlab code (The MathWorks Inc.). Pilot analyses showed that model performance was not significantly affected by changes in the parameters of the early stage. Accordingly, parameters for the spectrogram estimation were fixed (i.e. not estimated in the fitting procedure) and set as described above and in <xref ref-type="bibr" rid="pcbi.1003412-Chi2">[22]</xref>. The modulation content of the auditory spectrogram was computed through a bank of 2D modulation selective filters tuned to spectral modulation frequencies of <italic>Ω</italic> = [0.5, 1, 2, 4] cyc/oct and temporal modulation frequencies of <italic>ω</italic> = [1, 3, 9, 27] Hz. The filter bank output was computed at each frequency along the tonotopic axis and then averaged over time. In order to avoid overfitting, a reduced modulation representation was obtained as follows (3T: 3 tonotopic frequencies×4 spectral modulations×4 temporal modulations = 48 parameters to learn; 7T: 8 tonotopic frequencies×4 spectral modulations×4 temporal modulations = 128 parameters to learn; note that we chose a different number of parameters for the 3T and 7T datasets due to the different number of stimuli used for model's estimation - 60 and 144 stimuli, respectively). First, the time-averaged output of the filter bank was averaged across the upward and downward filter directions (note that this corresponds to assuming that sweep direction does not affect voxels activation levels). Then, we divided the tonotopic axis in ranges with constant bandwidth in octaves and averaged the modulation energy within each of these regions. We defined three frequency ranges in the 3T experiment and eight in the 7T experiment. The above processing steps were applied to all stimuli, resulting into an [<italic>S×N</italic>] feature matrix <bold>F</bold> of average modulation energy, where <italic>S</italic> is the number of sounds, and <italic>N</italic> is the number of features in the reduced modulation representation.</p>
</sec><sec id="s4d">
<title>Tonotopy model</title>
<p>The stimuli representation in the frequency space was obtained using only the input stage of the auditory model. The spectrogram was computed at 128 logarithmically spaced frequency values (<italic>f</italic> = 180–7040 Hz) and averaged over time. In the 3T experiment, we generated a reduced frequency representation in order to restrain the effects of overfitting (note that in the 7T experiment the number of observations in the train set was already higher than the number of parameters to estimate). We divided the tonotopic axis in 48 bins with constant bandwidth in octaves and averaged the frequency content within each of these regions. We chose 48 bins in order to have the same number of parameters for both the MTF-based and the tonotopy model. The above processing steps were applied to all stimuli, resulting into an [<italic>S×N</italic>] feature matrix <bold>F</bold> of time-averaged frequency content, where <italic>S</italic> is the number of sounds, and <italic>N</italic> is the number of frequency bins.</p>
</sec><sec id="s4e">
<title>Joint frequency non-specific MTF-based model</title>
<p>We generated the non-localized modulation representation by averaging the frequency-specific joint representation along both time and frequency (this is similar to performing a 2D Fourier transform of the spectrogram). This resulted in a representation with 16 features (4 temporal modulations×4 spectral modulations). However, frequency specific information is indeed reflected in voxels' activity <xref ref-type="bibr" rid="pcbi.1003412-Formisano1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003412-Moerel1">[27]</xref>; therefore, we concatenated the modulation representation with a tonotopic representation obtained as described above for the tonotopy model. We employed 32 frequency bins for the 3T dataset and 112 for the 7T dataset, resulting in a final representation with 48 and 128 features, respectively.</p>
</sec><sec id="s4f">
<title>Independent frequency-specific MTF-based model</title>
<p>We generated the independent modulation representation by filtering the auditory spectrogram with one-dimensional purely spectral and purely temporal modulation filters. Filters were tuned to spectral modulation frequencies of <italic>Ω</italic> = [0.5, 1, 2, 4] cyc/oct and temporal modulation frequencies of <italic>ω</italic> = [1, 3, 9, 27] Hz. The output of each filter bank was averaged over time and within frequency ranges with constant bandwidth in octaves. In order to have a representation with the same number of features as for the joint model, we defined 6 frequency ranges in the 3T experiment and 16 in the 7T experiment. Finally, the outputs of the purely spectral and purely temporal filter banks were concatenated, resulting in a representation with 48 features for the 3T dataset (6 tonotopic frequencies×4 temporal modulations+6 tonotopic frequencies×4 spectral modulations) and 128 for the 7T dataset (16 tonotopic frequencies×4 temporal modulations+16 tonotopic frequencies×4 spectral modulations). The above processing steps were applied to all stimuli, producing an [<italic>S×N</italic>] feature matrix <bold>F</bold> of average modulation energy, where <italic>S</italic> is the number of sounds, and <italic>N</italic> is the number of features.</p>
</sec><sec id="s4g">
<title>Independent frequency non-specific MTF-based model</title>
<p>We generated the non-localized independent representation by averaging across frequency the frequency-specific independent representation. This resulted in a representation with 8 features (4 temporal modulations+4 spectral modulations). The final model was obtained by concatenating the modulation representation with a tonotopic representation obtained as described above for the tonotopy model. We employed 40 frequency bins for the 3T dataset and 120 for the 7T dataset, resulting in a final representation with 48 and 128 features, respectively.</p>
</sec><sec id="s4h">
<title>Model estimation and evaluation</title>
<p>In the 7T experiment, independent train and test runs involving two completely distinct sound sets were used to train and assess the model, whereas leave run out cross-validation was performed for the 3T dataset (the final model parameters and the overall prediction accuracy were computed as the average across cross validations).</p>
</sec><sec id="s4i">
<title>Estimation of fMRI responses to natural sounds</title>
<p>For each voxel <italic>i</italic>, the response vector <italic>Y<sub>i</sub></italic> [(<italic>S</italic>×<italic>1</italic>), <italic>S</italic> = number of sounds] was obtained in two steps. First, a deconvolution analysis with all stimuli treated as a single condition was used to estimate the hemodynamic response function (HRF) common to all stimuli. Then, using this HRF and one predictor per sound, we computed the beta weight of each sound <xref ref-type="bibr" rid="pcbi.1003412-Kay2">[56]</xref>. Further analyses were performed on voxels with a significant response to the sounds (p&lt;.05, uncorrected in order not to be too stringent at this stage of the process) within an anatomically defined mask, which included HG, HS, PT, PP, and STG.</p>
</sec><sec id="s4j">
<title>Estimation of model parameters</title>
<p>The fMRI activity <italic>Y<sub>i</sub></italic> [<italic>S<sub>train</sub>×1</italic>] at voxel <italic>i</italic> was modeled as a linear transformation of the feature matrix <bold>F</bold><italic><sub>train</sub></italic> [<italic>S<sub>train</sub>×N</italic>] plus a noise term <italic>n</italic> [<italic>S<sub>train</sub>×1</italic>] as follows:<disp-formula id="pcbi.1003412.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <italic>S<sub>train</sub></italic> is the number of sounds in the training set, and <italic>C<sub>i</sub></italic> is an [<italic>N×1</italic>] vector of model parameters, whose elements <italic>c<sub>ij</sub></italic> quantify the contribution of feature <italic>j</italic> to the overall response of voxel <italic>i</italic>. Note that <xref ref-type="disp-formula" rid="pcbi.1003412.e001">Equation 1</xref> does not include a constant term as columns of matrices <bold>F</bold><italic><sub>train</sub></italic> and Y<sub>i</sub> were converted to standardized z-scores. Z-scoring of the features and responses does not affect the expressive capacity of the linear regression model. However, in a regularized regression framework like ridge regression (see below), z-scoring does affect the estimated model parameters (weights). In the present study, z-score was performed because the energy content of natural sounds varies on different scales across frequencies and modulations. As a consequence, the estimated model parameters would not be comparable without performing the z-score normalization.</p>
<p>The solution to <xref ref-type="disp-formula" rid="pcbi.1003412.e001">Equation 1</xref> was computed using <italic>ridge regression</italic> <xref ref-type="bibr" rid="pcbi.1003412-Hoerl1">[57]</xref>. The regularization parameter λ was determined independently for each voxel by automatically inspecting the stability of the ridge trace, that is changes in the parameter estimates as a function of λ <xref ref-type="bibr" rid="pcbi.1003412-Hoerl2">[58]</xref>. Namely, parameter estimates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e002" xlink:type="simple"/></inline-formula> were obtained for a range of increasing λ values [λ<sub>1</sub>, λ<sub>2</sub>, …, λ<sub>p</sub>], and the regularization parameter was set at the value λ<sup>*</sup> where all parameter estimates consistently changed less than 20% of their initial value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e003" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003412.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e004" xlink:type="simple"/><label>(2)</label></disp-formula>The inspection of the ridge trace represented an advantage in terms of trade-off between accurate model estimation and computational load. Namely, we observed that the selection of the regularization parameter via cross validation was computationally slower, while not yielding any significant improvement on models performance.</p>
</sec><sec id="s4k">
<title>Model evaluation</title>
<p>We quantified model's prediction accuracy by performing a sound identification analysis <xref ref-type="bibr" rid="pcbi.1003412-Kay1">[24]</xref>. Namely, we used the fMRI activity patterns predicted by the estimated models to identify which sound had been heard among all sounds in the test set.</p>
<p>Because model parameters were estimated in z-score units, we converted to standardized z-score the columns of the feature and response matrices for the stimuli in the test set. Given the trained model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e005" xlink:type="simple"/></inline-formula> [<italic>N×V</italic>] (where V is the number of voxels), and the feature matrix <bold>F</bold><italic><sub>test</sub></italic> [<italic>S<sub>test</sub>×N</italic>] for the test set, the predicted fMRI activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e006" xlink:type="simple"/></inline-formula> [<italic>S<sub>test</sub>×V</italic>] for the test sounds was obtained as follows:<disp-formula id="pcbi.1003412.e007"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e007" xlink:type="simple"/><label>(3)</label></disp-formula>Then, for each stimulus <italic>s<sub>i</sub></italic> we computed the correlation between its predicted fMRI activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e008" xlink:type="simple"/></inline-formula> [<italic>1×V</italic>] and all measured fMRI responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e009" xlink:type="simple"/></inline-formula> [<italic>1×V</italic>], j = 1, 2, …, S. The rank of the correlation between predicted and observed activity for stimulus <italic>s<sub>i</sub></italic> was selected as a measure of the model's ability to correctly match <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e010" xlink:type="simple"/></inline-formula> with its prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003412.e011" xlink:type="simple"/></inline-formula>. The matching score <italic>m</italic> for stimulus <italic>s<sub>i</sub></italic> was obtained by normalizing the computed rank between 0 and 1 as follows (<italic>m</italic> = 1 indicates correct match; <italic>m</italic> = 0 indicates predicted activity pattern for stimulus <italic>s<sub>i</sub></italic> was least similar to the measured one among all stimuli):<disp-formula id="pcbi.1003412.e012"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e012" xlink:type="simple"/><label>(4)</label></disp-formula>Normalized ranks were computed for all stimuli in the test set, and the overall model's accuracy was obtained as the mean of the matching scores across stimuli. Note that the metric we used (<xref ref-type="disp-formula" rid="pcbi.1003412.e012">Eq. 4</xref>) generalizes the more straightforward percent correct, a rank-based metric that considers only stimuli that are ranked first, i.e. stimuli that are correctly identified. Percent correct is a comprehensive metric when models identify new stimuli with high accuracies (close to 100%). As this was not the case in our data (see <xref ref-type="sec" rid="s3">Discussion</xref>), it is informative to look at the whole distribution to assess the degree of incorrect identification.</p>
<p>Statistical significance of the observed accuracy was assessed with permutation testing. Specifically, the empirical null-distribution of accuracies was obtained by randomly permuting (<italic>P</italic> = 200 permutations) the stimulus labels (i.e. <italic>S</italic> in matrix <bold>Y</bold>) and repeating the training and testing procedures. In order to preserve the spatial correlations among cortical locations, the same permutations were applied to all voxels. The regularization parameter was constant across permutations and was set to the value derived when the model was estimated on the unpermuted set of responses. When compared by means of t-test, accuracies were converted to z-scores via Fisher's transformation in order to reduce deviations from normality.</p>
</sec><sec id="s4l">
<title>Topographic maps of temporal modulation, spectral modulation, and frequency preference</title>
<p>For all voxels, response profiles for temporal modulation, spectral modulation and frequency were computed as marginal sums of the estimated stimulus-activity mapping function <bold>C</bold> of the joint frequency-specific MTF-based model, as follows:<disp-formula id="pcbi.1003412.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e013" xlink:type="simple"/><label>(5)</label></disp-formula><disp-formula id="pcbi.1003412.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e014" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula id="pcbi.1003412.e015"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003412.e015" xlink:type="simple"/><label>(7)</label></disp-formula>where <italic>tMTF</italic> and <italic>sMTF</italic> are the temporal and spectral modulation transfer functions, respectively, and <italic>fTF</italic> is the frequency transfer function. Voxels characteristic values (CTM, CSM, CF) were defined as the point of maximum of the <italic>tMTF</italic>, <italic>sMTF</italic> and <italic>fTF</italic>, respectively. A continuous representation of preferred values was obtained by spatial smoothing using a 2-neighbor (3-neighbor) voxels filter for the 3T (7T) dataset. Cortical maps were generated by color-coding the voxels' preferred values and projecting them onto an inflated representation of the subject's cortex. Individual maps were subsequently transformed to functional cortex based aligned (fCBA) space (see below) where group maps were obtained as the mean across subjects. Only voxels that had been included in the analysis of at least 3 out of the 5 subjects were considered when computing group maps.</p>
<p>To assess the reliability of the estimated voxels tuning preference, we computed the signal-to-noise ratio (SNR) of the MTFs estimates via a bootstrap resampling procedure applied to all individual subjects (see <xref ref-type="supplementary-material" rid="pcbi.1003412.s009">Text S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003412.s007">Figure S7</xref>).</p>
</sec><sec id="s4m">
<title>Relation between voxels characteristic spectral and temporal modulation</title>
<p>For each subject, we computed the Spearman's rank correlation coefficient between voxels characteristics CSM and CTM (prior to spatial smoothing). In order to take into account any possible bias introduced by the model's estimation procedure, we derived the empirical expected value of no correlation by computing the correlation coefficient between voxels CSM and CTM as obtained after permuting the stimulus labels (see above). Statistical significance of the Fisher-transformed correlation coefficients was assessed via a group level random effect two-tailed t test.</p>
</sec><sec id="s4n">
<title>Functional cortex based alignment</title>
<p>Additionally to the main experiments, localizer data were collected as responses to amplitude modulated tones (see <xref ref-type="supplementary-material" rid="pcbi.1003412.s009">Text S1</xref>). Tonotopy maps were computed with best-frequency mapping <xref ref-type="bibr" rid="pcbi.1003412-Formisano1">[26]</xref>, and resulting maps were used for fCBA <xref ref-type="bibr" rid="pcbi.1003412-Goebel1">[59]</xref> as follows. In each subject and hemisphere, we delineated the low frequency region consistently present in the vicinity of Heschl's gyrus as region of interest. FCBA was partially driven by this functional region (weighting decreased over iterations), and partially by anatomical information (weighting increased over iterations; <xref ref-type="bibr" rid="pcbi.1003412-Frost1">[60]</xref>). The resulting alignment information was used for calculating and displaying group cortical maps.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003412.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><italic>Joint</italic> and <italic>independent</italic> modulation representations. Spectrograms illustrate a schematic of channels in a modulation filter bank. Vertical and horizontal spacing between bars indicate channels preferred spectral (Ω) and temporal modulation frequencies (ω), respectively. (A) In the <italic>joint</italic> representation, the conjunction of spectral and temporal modulations is analyzed by spectro-temporal channels tuned to specific combinations of spectral and temporal modulation frequencies. Direction of bar tilt indicates tuning for upward or downward modulations. (B) In the <italic>independent</italic> representation, spectral and temporal modulations are independently encoded by separate spectral (top) and temporal (bottom) channels.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p>Schematic of model estimation and evaluation. (A) FMRI responses to a wide variety of natural sounds are used to estimate an encoding model for each voxel. The model projects the stimuli into an N-dimensional feature space and voxels are described as linear combinations of these features. By applying regularized regression, a vector of model's weights is estimated for each voxel. The feature yielding the highest weight is assigned as voxel's characteristic value. (B) Model performance is evaluated by assessing its ability to accurately predict fMRI responses to natural sounds in a new dataset. (S = number of sounds; N = number of features).</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p>Group tonotopic maps. Group maps for the 3T (A) and 7T (B) datasets are displayed on an inflated representation of the group cortex. Maps are shown in the cortical region highlighted by the black square. Group maps are computed as the mean across participants for those voxels that are included in at least 3 individual maps. The black line indicates HG.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s004" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p>Individual tonotopic maps. Individual maps of tonotopy are shown for the 3T (A) and 7T (B) datasets. The black line indicates HG.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s005" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p>Individual topographic maps. Maps of CSM (left) and CTM (right) for all participants in the 3T experiments. Left: purple and orange denote tuning for fine and coarse spectral structures respectively. Right: purple and orange denote tuning for fast and slow temporal variations respectively. The black line indicates HG.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s006" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s006" position="float" xlink:type="simple"><label>Figure S6</label><caption>
<p>Individual topographic maps. Maps of CSM (left) and CTM (right) for all participants in the 7T experiments. Left: purple and orange denote tuning for fine and coarse spectral structures respectively. Right: purple and orange denote tuning for fast and slow temporal variations respectively. The black line indicates HG.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s007" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s007" position="float" xlink:type="simple"><label>Figure S7</label><caption>
<p>Stability of MTFs estimates across bootstraps. Single subjects maps of SNR of voxels MTFs as estimated by the joint frequency-specific MTF-based model at 3T (A) and 7T (B). High values of SNR (bright colors) indicate that the estimated MTF is consistent across bootstraps. The black line outlines HG.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s008" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s008" position="float" xlink:type="simple"><label>Figure S8</label><caption>
<p>Unbiased topographic maps for the 7T dataset. Group maps of CSM, CTM and CF as derived from the joint frequency-specific MTF-based model while explicitly accounting for the effect of sound categories. The black line indicates HG.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003412.s009" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1003412.s009" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>Supplementary methods and references.</p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank G. Valente and L. Hausfeld for valuable discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003412-Alain1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alain</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Arnott</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Hevenor</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Graham</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Grady</surname><given-names>CL</given-names></name> (<year>2001</year>) <article-title>“What” and “where” in the human auditory system</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>98</volume>: <fpage>12301</fpage>–<lpage>12306</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=59809&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=59809&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Ahveninen1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahveninen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Jääskeläinen</surname><given-names>IP</given-names></name>, <name name-style="western"><surname>Raij</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bonmassar</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Devore</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Task-modulated “what” and “where” pathways in human auditory cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>: <fpage>14608</fpage>–<lpage>14613</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1600007&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1600007&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Belin1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belin</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Lafaille</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ahad</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Pike</surname><given-names>B</given-names></name> (<year>2000</year>) <article-title>Voice-selective areas in human auditory cortex</article-title>. <source>Nature</source> <volume>403</volume>: <fpage>309</fpage>–<lpage>312</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10659849" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/10659849</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Lewis1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Talkington</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>Walker</surname><given-names>NA</given-names></name>, <name name-style="western"><surname>Spirou</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Jajosky</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Human cortical organization for processing vocalizations indicates representation of harmonic structure as a signal attribute</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>2283</fpage>–<lpage>2296</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2774090&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2774090&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 2 November 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Leaver1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leaver</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Rauschecker</surname><given-names>JP</given-names></name> (<year>2010</year>) <article-title>Cortical representation of natural complex sounds: effects of acoustic features and auditory object category</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>7604</fpage>–<lpage>7612</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930617&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930617&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 9 August 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Singh1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Singh</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2003</year>) <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title>. <source>J Acoust Soc Am</source> <volume>114</volume>: <fpage>3394</fpage>–<lpage>3411</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://link.aip.org/link/JASMAN/v114/i6/p3394/s1&amp;Agg=doi" xlink:type="simple">http://link.aip.org/link/JASMAN/v114/i6/p3394/s1&amp;Agg=doi</ext-link>. Accessed 16 August 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Shannon1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shannon</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Zeng</surname><given-names>F-G</given-names></name>, <name name-style="western"><surname>Wygonski</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kamath</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Ekelid</surname><given-names>M</given-names></name> (<year>1995</year>) <article-title>Speech recognition with primarily temporal cues</article-title>. <source>Science</source> <volume>(80-) 270</volume>: <fpage>303</fpage>–<lpage>304</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://cbt.beckman.uiuc.edu/papers_spring06/shannon95speechrecognition.pdf" xlink:type="simple">http://cbt.beckman.uiuc.edu/papers_spring06/shannon95speechrecognition.pdf</ext-link>. Accessed 7 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Drullman1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Drullman</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Festen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Plomp</surname><given-names>R</given-names></name> (<year>1994</year>) <article-title>Effect of temporal envelope smearing on speech reception</article-title>. <source>J Acoust Soc Am</source> <volume>95</volume>: <fpage>1053</fpage>–<lpage>1064</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://link.aip.org/link/?JASMAN/95/1053/1" xlink:type="simple">http://link.aip.org/link/?JASMAN/95/1053/1</ext-link>. Accessed 7 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Chi1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Guyton</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Ru</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name> (<year>1999</year>) <article-title>Spectro-temporal modulation transfer functions and speech intelligibility</article-title>. <source>J Acoust Soc Am</source> <volume>106</volume>: <fpage>2719</fpage>–<lpage>2732</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10573888" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/10573888</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Elliott1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elliott</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2009</year>) <article-title>The modulation transfer function for speech intelligibility</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000302</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2639724&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2639724&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 4 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Viemeister1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Viemeister</surname><given-names>NF</given-names></name> (<year>1979</year>) <article-title>Temporal modulation transfer functions based upon modulation thresholds</article-title>. <source>J Acoust Soc Am</source> <volume>66</volume>: <fpage>1364</fpage>–<lpage>1380</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/500975" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/500975</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Green1"><label>12</label>
<mixed-citation publication-type="book" xlink:type="simple">Green D (1986) “Frequency” and the Detection of Spectral Shape Change. In: Moore BJ, Patterson R, editors. Auditory Frequency Selectivity. Springer US, Vol. 119. pp. 351–359. Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-1-4613-2247-4_38" xlink:type="simple">http://dx.doi.org/10.1007/978-1-4613-2247-4_38</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Joris1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joris</surname><given-names>PX</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Rees</surname><given-names>A</given-names></name> (<year>2004</year>) <article-title>Neural processing of amplitude-modulated sounds</article-title>. <source>Physiol Rev</source> <volume>84</volume>: <fpage>541</fpage>–<lpage>577</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15044682" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/15044682</ext-link>. Accessed 18 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Giraud1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Lorenzi</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wable</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Johnsrude</surname><given-names>I</given-names></name>, <etal>et al</etal>. (<year>2000</year>) <article-title>Representation of the temporal envelope of sounds in the human brain</article-title>. <source>J Neurophysiol</source> <volume>84</volume>: <fpage>1588</fpage>–<lpage>1598</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10980029" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/10980029</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Barton1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barton</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Venezia</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Orthogonal acoustic dimensions define auditory field maps in human cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>: <fpage>20738</fpage>–<lpage>20743</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/109/50/20738.short" xlink:type="simple">http://www.pnas.org/content/109/50/20738.short</ext-link>. Accessed 11 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Shamma1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Versnel</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Kowalski</surname><given-names>N</given-names></name> (<year>1995</year>) <article-title>Ripple Analysis in Ferret Primary Auditory Cortex. 1. Response Characteristics of Single Units to Sinusoidally Rippled Spectra</article-title>. <source>Aud Neurosci</source> <volume>1</volume>: <fpage>233</fpage>–<lpage>254</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA452778" xlink:type="simple">http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA452778</ext-link>. Accessed 11 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Kowalski1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kowalski</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Depireux</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>1996</year>) <article-title>Analysis of dynamic spectra in ferret primary auditory cortex. I. Characteristics of single-unit responses to moving ripple spectra</article-title>. <source>J Neurophysiol</source> <volume>76</volume>: <fpage>3503</fpage>–<lpage>3523</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Depireux1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Depireux</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name>, <name name-style="western"><surname>Klein</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Ding</surname><given-names>N</given-names></name> (<year>2001</year>) <article-title>Spectro-Temporal Response Field Characterization With Dynamic Ripples in Ferret Primary Auditory Cortex</article-title>. <source>J Neurophysiol</source> <volume>85</volume>: <fpage>1220</fpage>–<lpage>1234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Woolley1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Woolley</surname><given-names>SMN</given-names></name>, <name name-style="western"><surname>Fremouw</surname><given-names>TE</given-names></name>, <name name-style="western"><surname>Hsu</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2005</year>) <article-title>Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>1371</fpage>–<lpage>1379</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/16136039" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/16136039</ext-link>. Accessed 7 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Langers1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langers</surname><given-names>DRM</given-names></name>, <name name-style="western"><surname>Backes</surname><given-names>WH</given-names></name>, <name name-style="western"><surname>Dijk</surname><given-names>P Van</given-names></name> (<year>2003</year>) <article-title>Spectrotemporal features of the auditory cortex: the activation in response to dynamic ripples</article-title>. <source>Neuroimage</source> <volume>20</volume>: <fpage>265</fpage>–<lpage>275</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S1053811903002581" xlink:type="simple">http://linkinghub.elsevier.com/retrieve/pii/S1053811903002581</ext-link>. Accessed 10 December 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Schnwiesner1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schönwiesner</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name> (<year>2009</year>) <article-title>Spectro-temporal modulation transfer function of single voxels in the human auditory cortex measured with high-resolution fMRI</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>106</volume>: <fpage>14611</fpage>–<lpage>14616</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2732853&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2732853&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Chi2"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ru</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name> (<year>2005</year>) <article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title>. <source>J Acoust Soc Am</source> <volume>118</volume>: <fpage>887</fpage>–<lpage>906</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://link.aip.org/link/JASMAN/v118/i2/p887/s1&amp;Agg=doi" xlink:type="simple">http://link.aip.org/link/JASMAN/v118/i2/p887/s1&amp;Agg=doi</ext-link>. Accessed 13 December 2010.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Elhilali1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>A cocktail party with a cortical twist: how cortical mechanisms contribute to sound segregation</article-title>. <source>J Acoust Soc Am</source> <volume>124</volume>: <fpage>3751</fpage>–<lpage>3771</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2676630&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2676630&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Kay1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kay</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2008</year>) <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source> <volume>452</volume>: <fpage>352</fpage>–<lpage>355</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18322462" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/18322462</ext-link>. Accessed 19 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Schreiner1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Winer</surname><given-names>JA</given-names></name> (<year>2007</year>) <article-title>Auditory cortex mapmaking: principles, projections, and plasticity</article-title>. <source>Neuron</source> <volume>56</volume>: <fpage>356</fpage>–<lpage>365</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/17964251" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/17964251</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Formisano1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Di Salle</surname><given-names>F</given-names></name>, <name name-style="western"><surname>van de Moortele</surname><given-names>PF</given-names></name>, <name name-style="western"><surname>Ugurbil</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Mirror-symmetric tonotopic maps in human primary auditory cortex</article-title>. <source>Neuron</source> <volume>40</volume>: <fpage>859</fpage>–<lpage>869</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/14622588" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/14622588</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Moerel1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moerel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>De Martino</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name> (<year>2012</year>) <article-title>Processing of natural sounds in human auditory cortex: tonotopy, spectral tuning, and relation to voice sensitivity</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>14205</fpage>–<lpage>14216</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/23055490" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/23055490</ext-link>. Accessed 11 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-DeMartino1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Martino</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Moerel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>van de Moortele</surname><given-names>P-F</given-names></name>, <name name-style="western"><surname>Ugurbil</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Spatial organization of frequency preference and selectivity in the human inferior colliculus</article-title>. <source>Nat Commun</source> <volume>4</volume>: <fpage>1386</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ncomms2379" xlink:type="simple">http://dx.doi.org/10.1038/ncomms2379</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Moerel2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moerel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>De Martino</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Santoro</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ugurbil</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Processing of natural sounds: characterization of multipeak spectral tuning in human auditory cortex</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>11888</fpage>–<lpage>11898</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/23864678" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/23864678</ext-link>. Accessed 19 September 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Jepsen1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jepsen</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Ewert</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Dau</surname><given-names>T</given-names></name> (<year>2008</year>) <article-title>A computational model of human auditory signal processing and perception</article-title>. <source>J Acoust Soc Am</source> <volume>124</volume>: <fpage>422</fpage>–<lpage>438</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18646987" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/18646987</ext-link>. Accessed 10 September 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Pasley1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasley</surname><given-names>BN</given-names></name>, <name name-style="western"><surname>David S</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Flinker</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Reconstructing Speech from Human Auditory Cortex</article-title>. <source>PLoS Biol</source> <volume>10</volume>: <fpage>e1001251</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.plos.org/10.1371/journal.pbio.1001251" xlink:type="simple">http://dx.plos.org/10.1371/journal.pbio.1001251</ext-link>. Accessed 31 January 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Sabin1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sabin</surname><given-names>AT</given-names></name>, <name name-style="western"><surname>Eddins</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Wright</surname><given-names>BA</given-names></name> (<year>2012</year>) <article-title>Perceptual Learning Evidence for Tuning to Spectrotemporal Modulation in the Human Auditory System</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>6542</fpage>–<lpage>6549</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5732-11.2012" xlink:type="simple">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5732-11.2012</ext-link>. Accessed 9 May 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Patil1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patil</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Music in our ears: the biological bases of musical timbre perception</article-title>. <source>PLoS Comput Biol</source> <volume>8</volume>: <fpage>e1002759</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3486808&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3486808&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 11 March 2013.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Attias1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attias</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name> (<year>1997</year>) <article-title>Temporal Low-Order Statistics of Natural Sounds</article-title>. <source>Adv Neural Inf Process Syst</source> <volume>9</volume>: <fpage>27</fpage>–<lpage>33</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.201&amp;rep=rep1&amp;type=pdf" xlink:type="simple">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.201&amp;rep=rep1&amp;type=pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Voss1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Voss</surname><given-names>RF</given-names></name>, <name name-style="western"><surname>Clarke</surname><given-names>J</given-names></name> (<year>1975</year>) <article-title>‘1/f noise’ in music and speech</article-title>. <source>Nature</source> <volume>258</volume>: <fpage>317</fpage>–<lpage>318</lpage> Available: citeulike-article-id:4035033.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Escab1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Escabí</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Read</surname><given-names>HL</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name> (<year>2003</year>) <article-title>Naturalistic auditory contrast improves spectrotemporal coding in the cat inferior colliculus</article-title>. <source>J Neurosci</source> <volume>23</volume>: <fpage>11489</fpage>–<lpage>11504</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/14684853" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/14684853</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Barlow1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name> (<year>1961</year>) <article-title>Possible principles underlying the transformation of sensory messages</article-title>. <source>Sens Commun</source> <fpage>217</fpage>–<lpage>234</lpage> Available: citeulike-article-id:6200793.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Wu1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>MC-K</given-names></name>, <name name-style="western"><surname>David S</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2006</year>) <article-title>Complete functional characterization of sensory neurons by system identification</article-title>. <source>Annu Rev Neurosci</source> <volume>29</volume>: <fpage>477</fpage>–<lpage>505</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/16776594" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/16776594</ext-link>. Accessed 4 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Theunissen1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Sen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name> (<year>2000</year>) <article-title>Spectral-Temporal Receptive Fields of Nonlinear Auditory Neurons</article-title>. <source>J Neurosci</source> <volume>20</volume>: <fpage>2315</fpage>–<lpage>2331</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Bitterman1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bitterman</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Mukamel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Malach</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Fried</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name> (<year>2008</year>) <article-title>Ultra-fine frequency tuning revealed in single neurons of human auditory cortex</article-title>. <source>Nature</source> <volume>451</volume>: <fpage>197</fpage>–<lpage>201</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2676858&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2676858&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 13 July 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Laudanski1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laudanski</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Edeline</surname><given-names>J-M</given-names></name>, <name name-style="western"><surname>Huetz</surname><given-names>C</given-names></name> (<year>2012</year>) <article-title>Differences between Spectro-Temporal Receptive Fields Derived from Artificial and Natural Stimuli in the Auditory Cortex</article-title>. <source>PLoS One</source> <volume>7</volume>: <fpage>e50539</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0050539" xlink:type="simple">http://dx.doi.org/10.1371/journal.pone.0050539</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-DavidS1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David S</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Vinje</surname><given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2004</year>) <article-title>Natural stimulus statistics alter the receptive field structure of v1 neurons</article-title>. <source>J Neurosci</source> <volume>24</volume>: <fpage>6991</fpage>–<lpage>7006</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15295035" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/15295035</ext-link>. Accessed 22 June 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Talebi1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Talebi</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>CL</given-names></name> (<year>2012</year>) <article-title>Natural versus Synthetic Stimuli for Estimating Receptive Field Models: A Comparison of Predictive Robustness</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>1560</fpage>–<lpage>1576</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4661-12.2012" xlink:type="simple">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4661-12.2012</ext-link>. Accessed 1 February 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Zatorre1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Belin</surname><given-names>P</given-names></name> (<year>2001</year>) <article-title>Spectral and Temporal Processing in Human Auditory Cortex</article-title>. <source>Cereb Cortex</source> <volume>11</volume> <fpage>946</fpage>–<lpage>953</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://cercor.oxfordjournals.org/content/11/10/946.abstract" xlink:type="simple">http://cercor.oxfordjournals.org/content/11/10/946.abstract</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Samson1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samson</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Zeffiro</surname><given-names>TA</given-names></name>, <name name-style="western"><surname>Toussaint</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Belin</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Stimulus complexity and categorical effects in human auditory cortex: an activation likelihood estimation meta-analysis</article-title>. <source>Front Psychol</source> <volume>1</volume>: <fpage>241</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.frontiersin.org/Journal/Abstract.aspx?s=86&amp;name=auditory_cognitive_neuroscience&amp;ART_DOI=10.3389/fpsyg.2010.00241" xlink:type="simple">http://www.frontiersin.org/Journal/Abstract.aspx?s=86&amp;name=auditory_cognitive_neuroscience&amp;ART_DOI=10.3389/fpsyg.2010.00241</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Bendor1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bendor</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X</given-names></name> (<year>2008</year>) <article-title>Neural response properties of primary, rostral, and rostrotemporal core fields in the auditory cortex of marmoset monkeys</article-title>. <source>J Neurophysiol</source> <volume>100</volume>: <fpage>888</fpage>–<lpage>906</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2525707&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2525707&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 13 July 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Rodrguez1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodríguez</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Read</surname><given-names>HL</given-names></name>, <name name-style="western"><surname>Escabí</surname><given-names>MA</given-names></name> (<year>2010</year>) <article-title>Spectral and temporal modulation tradeoff in the inferior colliculus</article-title>. <source>J Neurophysiol</source> <volume>103</volume>: <fpage>887</fpage>–<lpage>903</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2822687&amp;tool=pmcentrez&amp;rendertype=abstract" xlink:type="simple">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2822687&amp;tool=pmcentrez&amp;rendertype=abstract</ext-link>. Accessed 31 August 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Rodrguez2"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodríguez</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Read</surname><given-names>HL</given-names></name>, <name name-style="western"><surname>Escabí</surname><given-names>MA</given-names></name> (<year>2010</year>) <article-title>Neural modulation tuning characteristics scale to efficiently encode natural sound statistics</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>15969</fpage>–<lpage>15980</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/21106835" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/21106835</ext-link>. Accessed 19 June 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Simoncelli1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name> (<year>2001</year>) <article-title>Natural Image Statistics and Neural Representation</article-title>. <source>Annu Rev Neurosci</source> <volume>24</volume>: <fpage>1193</fpage>–<lpage>1216</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.24.1.1193" xlink:type="simple">http://dx.doi.org/10.1146/annurev.neuro.24.1.1193</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Naselaris1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Kay</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>Oliver</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2009</year>) <article-title>Bayesian reconstruction of natural images from human brain activity</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>902</fpage>–<lpage>915</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19778517" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/19778517</ext-link>. Accessed 31 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Nishimoto1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nishimoto</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vu</surname><given-names>AT</given-names></name>, <name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Benjamini</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title>. <source>Curr Biol</source> <volume>21</volume>: <fpage>1641</fpage>–<lpage>1646</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0960982211009377" xlink:type="simple">http://linkinghub.elsevier.com/retrieve/pii/S0960982211009377</ext-link>. Accessed 23 September 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Mitchell1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>Shinkareva</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Carlson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>K-M</given-names></name>, <name name-style="western"><surname>Malave</surname><given-names>VL</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Predicting human brain activity associated with the meanings of nouns</article-title>. <source>Science</source> <volume>320</volume>: <fpage>1191</fpage>–<lpage>1195</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18511683" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/18511683</ext-link>. Accessed 4 July 2011.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Gaab1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaab</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Gabrieli</surname><given-names>JDE</given-names></name>, <name name-style="western"><surname>Glover</surname><given-names>GH</given-names></name> (<year>2007</year>) <article-title>Assessing the influence of scanner background noise on auditory processing. I. An fMRI study comparing three experimental designs with varying degrees of scanner noise</article-title>. <source>Hum Brain Mapp</source> <volume>28</volume>: <fpage>703</fpage>–<lpage>720</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hbm.20298" xlink:type="simple">http://dx.doi.org/10.1002/hbm.20298</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Logothetis1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>2008</year>) <article-title>What we can do and what we cannot do with fMRI</article-title>. <source>Nature</source> <volume>453</volume>: <fpage>869</fpage>–<lpage>878</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature06976" xlink:type="simple">http://dx.doi.org/10.1038/nature06976</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Logothetis2"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Pauls</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Augath</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Trinath</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Oeltermann</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title>. <source>Nature</source> <volume>412</volume>: <fpage>150</fpage>–<lpage>157</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35084005" xlink:type="simple">http://dx.doi.org/10.1038/35084005</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Kay2"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kay</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>David S</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Hansen</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2008</year>) <article-title>Modeling low-frequency fluctuation and hemodynamic response timecourse in event-related fMRI</article-title>. <source>Hum Brain Mapp</source> <volume>29</volume>: <fpage>142</fpage>–<lpage>156</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hbm.20379" xlink:type="simple">http://dx.doi.org/10.1002/hbm.20379</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Hoerl1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoerl</surname><given-names>AE</given-names></name>, <name name-style="western"><surname>Kennard</surname><given-names>RW</given-names></name> (<year>1970</year>) <article-title>Ridge Regression: Biased Estimation for Nonorthogonal Problems</article-title>. <source>Technometrics</source> <volume>12</volume>: <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Hoerl2"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoerl</surname><given-names>AE</given-names></name>, <name name-style="western"><surname>Kennard</surname><given-names>RW</given-names></name> (<year>1970</year>) <article-title>Ridge Regression: Applications to Nonorthogonal Problems</article-title>. <source>Technometrics</source> <volume>12</volume>: <fpage>69</fpage>–<lpage>82</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Goebel1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Esposito</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name> (<year>2006</year>) <article-title>Analysis of functional image analysis contest (FIAC) data with brainvoyager QX: From single-subject to cortically aligned group general linear model analysis and self-organizing group independent component analysis</article-title>. <source>Hum Brain Mapp</source> <volume>27</volume>: <fpage>392</fpage>–<lpage>401</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hbm.20249" xlink:type="simple">http://dx.doi.org/10.1002/hbm.20249</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003412-Frost1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frost</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name> (<year>2013</year>) <article-title>Functionally informed cortex based alignment: An integrated approach for whole-cortex macro-anatomical and ROI-based functional alignment</article-title>. <source>Neuroimage</source> <volume>83</volume>: <fpage>1002</fpage>–<lpage>1010</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1053811913008252" xlink:type="simple">http://www.sciencedirect.com/science/article/pii/S1053811913008252</ext-link>.</mixed-citation>
</ref>
</ref-list></back>
</article>