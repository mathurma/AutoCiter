<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id><journal-title-group>
<journal-title>PLoS Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-13-02359</article-id>
<article-id pub-id-type="doi">10.1371/journal.pbio.1001752</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Natural language processing</subject></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group><subj-group><subject>Cognitive neuroscience</subject><subject>Computational neuroscience</subject><subject>Neuroimaging</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Speech Rhythms and Multiplexed Oscillatory Sensory Coding in the Human Brain</article-title>
<alt-title alt-title-type="running-head">Speech Rhythms and Multiplexed Oscillatory Coding</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Gross</surname><given-names>Joachim</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hoogenboom</surname><given-names>Nienke</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Thut</surname><given-names>Gregor</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schyns</surname><given-names>Philippe</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Panzeri</surname><given-names>Stefano</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Belin</surname><given-names>Pascal</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Garrod</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Institute for Neuroscience and Psychology, University of Glasgow, Glasgow, United Kingdom</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Institute for Clinical Neuroscience and Medical Psychology, University of Düsseldorf, Düsseldorf, Germany</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Center for Neuroscience and Cognitive Systems, Istituto Italiano di Tecnologia @UniTn, Rovereto, Italy</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname><given-names>David</given-names></name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>New York University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">Joachim.Gross@glasgow.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>The author(s) have made the following declarations about their contributions: Conceived and designed the experiments: JG NH PB SG. Performed the experiments: NH. Analyzed the data: JG NH PS SP. Contributed reagents/materials/analysis tools: SP PS GT. Wrote the paper: JG NH GT PS SP PB SG.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>12</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>31</day><month>12</month><year>2013</year></pub-date>
<volume>11</volume>
<issue>12</issue>
<elocation-id>e1001752</elocation-id>
<history>
<date date-type="received"><day>14</day><month>6</month><year>2013</year></date>
<date date-type="accepted"><day>18</day><month>11</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Gross et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="RA1" related-article-type="companion" ext-link-type="uri" vol="" page="e1001753" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pbio.1001753"> <article-title>How Brain Waves Help Us Make Sense of Speech</article-title></related-article>
<abstract abstract-type="toc"><sec>
<title/>
<p>A neuroimaging study reveals how coupled brain oscillations at different frequencies align with quasi-rhythmic features of continuous speech such as prosody, syllables, and phonemes.</p>
</sec></abstract>
<abstract>
<p>Cortical oscillations are likely candidates for segmentation and coding of continuous speech. Here, we monitored continuous speech processing with magnetoencephalography (MEG) to unravel the principles of speech segmentation and coding. We demonstrate that speech entrains the phase of low-frequency (delta, theta) and the amplitude of high-frequency (gamma) oscillations in the auditory cortex. Phase entrainment is stronger in the right and amplitude entrainment is stronger in the left auditory cortex. Furthermore, edges in the speech envelope phase reset auditory cortex oscillations thereby enhancing their entrainment to speech. This mechanism adapts to the changing physical features of the speech envelope and enables efficient, stimulus-specific speech sampling. Finally, we show that within the auditory cortex, coupling between delta, theta, and gamma oscillations increases following speech edges. Importantly, all couplings (i.e., brain-speech and also within the cortex) attenuate for backward-presented speech, suggesting top-down control. We conclude that segmentation and coding of speech relies on a nested hierarchy of entrained cortical oscillations.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Continuous speech is organized into a nested hierarchy of quasi-rhythmic components (prosody, syllables, phonemes) with different time scales. Interestingly, neural activity in the human auditory cortex shows rhythmic modulations with frequencies that match these speech rhythms. Here, we use magnetoencephalography and information theory to study brain oscillations in participants as they process continuous speech. We show that auditory brain oscillations at different frequencies align with the rhythmic structure of speech. This alignment is more precise when participants listen to intelligible rather than unintelligible speech. The onset of speech resets brain oscillations and improves their alignment to speech rhythms; it also improves the alignment between the different frequencies of nested brain oscillations in the auditory cortex. Since these brain oscillations reflect rhythmic changes in neural excitability, they are strong candidates for mediating the segmentation of continuous speech at different time scales corresponding to key speech components such as syllables and phonemes.</p>
</abstract>
<funding-group><funding-statement>This study was supported by the Wellcome Trust (091928, 098433, 098434) and by the ESRC and MRC (RES-060-25-0010). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>A large number of invasive and non-invasive neurophysiological studies provide converging evidence that cortical oscillations play an important role in gating information flow in the human brain, thereby supporting a variety of cognitive processes including attention, working memory, and decision-making <xref ref-type="bibr" rid="pbio.1001752-Wang1">[1]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Schnitzler1">[3]</xref>. These oscillations can be hierarchically organised. For example, the phase of (4–8) Hz theta oscillations can modulate the amplitude of (30–90 Hz) gamma oscillations; the phase of (1–2 Hz) delta oscillations can modulate the amplitude of theta oscillations <xref ref-type="bibr" rid="pbio.1001752-Lakatos1">[4]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Jensen1">[8]</xref>.</p>
<p>Interestingly, speech comprises a remarkably similar hierarchy of rhythmic components representing prosody (delta band), syllables (theta band), and phonemes (gamma band) <xref ref-type="bibr" rid="pbio.1001752-Ghitza1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>. The similarity in the hierarchical organisation of cortical oscillations and the rhythmic components of speech suggests that cortical oscillations at different frequencies might sample auditory speech input at different rates. Cortical oscillations could therefore represent an ideal medium for multiplexed segmentation and coding of speech <xref ref-type="bibr" rid="pbio.1001752-Ghitza1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Schroeder1">[17]</xref>. The hierarchical coupling of oscillations (with fast oscillations nested in slow oscillations) could be used to multiplex complementary information over multiple time scales <xref ref-type="bibr" rid="pbio.1001752-Panzeri1">[18]</xref> (see also <xref ref-type="bibr" rid="pbio.1001752-Schyns1">[19]</xref>) for example by separately encoding fast (e.g., phonemic) and slower (e.g., syllabic) information and their temporal relationships.</p>
<p>Previous studies have demonstrated amplitude and phase modulation in response to speech stimuli in the delta, theta, and gamma bands using electroencephalography (EEG)/magnetoencephalography (MEG) <xref ref-type="bibr" rid="pbio.1001752-Morillon1">[13]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Lehongre1">[15]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Luo1">[20]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Cogan1">[25]</xref> and electrocorticography (ECOG) <xref ref-type="bibr" rid="pbio.1001752-ZionGolumbic2">[26]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Ding1">[29]</xref>. These findings support an emerging view that speech stimuli induce low-frequency phase patterns in auditory areas that code input information. Interestingly, these phase patterns seem to be under attentional control. For example, in the well known cocktail party situation, they code mainly for the attended stimulus <xref ref-type="bibr" rid="pbio.1001752-ZionGolumbic2">[26]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Lakatos2">[30]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Schroeder2">[31]</xref>. Thus, brain oscillations have become obvious candidates for segmenting and parsing continuous speech because they reflect rhythmic changes in excitability <xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>.</p>
<p>This attractive model leaves three important points largely unresolved: First, a comprehensive account of how rhythmic components in speech interact with brain oscillations is still missing and it is uncertain if the previously reported hemispheric asymmetry during speech perception is also evident in a lateralized alignment of brain oscillations to continuous speech. Behavioural, electrophysiological, and neuroimaging studies <xref ref-type="bibr" rid="pbio.1001752-Morillon1">[13]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Lehongre1">[15]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Luo1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Abrams1">[23]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Boemio1">[32]</xref> suggest that there is a relatively long integration window (100–300 ms, corresponding to the theta band) in the right auditory cortex and a relatively short integration window (20–40 ms, corresponding to the gamma band) in the left auditory cortex <xref ref-type="bibr" rid="pbio.1001752-Poeppel2">[14]</xref>. But it is unclear whether this differentiation is relevant for oscillatory tracking of speech. Second, it is unknown whether cortical brain oscillations are hierarchically coupled during perception of continuous speech. This is of particular interest because hierarchically coupled brain oscillations could represent hierarchically organised speech components (prosody, syllables, phonemes) at different temporal scales. Third, it is unclear how oscillatory speech tracking dynamically adapts to arrhythmic components in speech. If brain oscillations implement a universal mechanism for speech processing they should also account for variations or breaks in speech rhythmicity, so that the phase of low-frequency oscillations aligns to (quasi-periodic) salient speech events for optimal processing.</p>
<p>Here, we addressed these three points using continuous speech and analysis based on information theory. Importantly, all three points were investigated for intelligible and unintelligible (backward played) speech. We analysed the frequency-specific dependencies between the speech envelope and brain activity. We also analysed the dependencies between cortical oscillations across different frequencies. We first hypothesised that a multi-scale hierarchy of oscillations in the listener's brain tracks the dynamics of the speaker's speech envelope—specifically, preferential theta band tracking in the right auditory cortex and gamma band tracking in the left auditory cortex. Second, we asked whether speech-entrained brain oscillations are hierarchically coupled and if so how that coupling is modulated by the stimulus. Third, we asked whether phase of low-frequency brain oscillations (likely indicating rhythmic variations in neural excitability) in the auditory cortex coincide with and adapt to salient events in speech stimuli.</p>
<p>We presented a 7-min long continuous story binaurally to 22 participants while recording neural activity with MEG (“story” condition). As a control condition the same story was played backwards (“back” condition). We used mutual information (MI) to measure all dependencies (linear and nonlinear) between the speech signal and its encoding in brain oscillations <xref ref-type="bibr" rid="pbio.1001752-Magri1">[33]</xref>,<xref ref-type="bibr" rid="pbio.1001752-QuianQuiroga1">[34]</xref>. We did so in all brain voxels for frequencies from 1 to 60 Hz and for important interactions (phase-phase, amplitude-amplitude, cross-frequency phase-amplitude, and cross-frequency amplitude-phase, see <xref ref-type="fig" rid="pbio-1001752-g001">Figure 1</xref> and <xref ref-type="sec" rid="s4">Materials and Methods</xref>). This resulted in frequency specific functional brain maps of dependencies between the speech envelope and brain activity. Similar analysis was performed to study dependencies between brain oscillations within cortical areas but across different frequency bands.</p>
<fig id="pbio-1001752-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g001</object-id><label>Figure 1</label><caption>
<title>Mutual information analysis.</title>
<p>The broadband amplitude envelope is computed for the speech signal. For each frequency band speech envelope and MEG signals are bandpass filtered and activation time series are computed for each voxel in the brain. Phase and amplitude time series are computed from the Hilbert transform for speech and voxel time series and subjected to MI analysis. MI is computed between speech signal and time series for each voxel leading to a tomographic map of MI. Group statistical analysis is performed on these maps across all 22 participants.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g001" position="float" xlink:type="simple"/></fig>
<p>Our results reveal hierarchically coupled oscillations in speech-related brain areas and their alignment to quasi-rhythmic components in continuous speech (prosody, syllables, phonemes), with pronounced asymmetries between left and right hemispheres. Edges in the speech envelope reset oscillatory low-frequency phase in left and right auditory cortices. Phase resets in cortical oscillations code features of the speech edges and help to align temporal windows of high neural excitability to optimise processing of important speech events. Importantly, we demonstrate that oscillatory speech tracking and hierarchical couplings significantly reduce for backward-presented speech and so are not only stimulus driven.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Oscillatory Speech Tracking Relies on Two Mechanisms</title>
<p>We first asked whether there is phase-locking between rhythmic changes in the speech envelope and corresponding oscillatory brain activity. Whereas most previous studies quantify phase-locking to stimulus onset across repeated presentations of the same stimulus, here we studied phase-locking over time directly between speech envelope and brain oscillations. To do this, we compared the phase coupling between the speech and oscillatory brain activity (in 1 Hz steps between 1 and 60 Hz) in two conditions: story and back. <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2</xref> summarizes the results. First, MI revealed a significantly stronger phase coupling between the speech envelope and brain oscillations in the story compared to back conditions in the left and right auditory cortex in delta (1–3 Hz) and theta (3–7 Hz) frequency bands (group statistics, <italic>p</italic>&lt;0.05, false discovery rate [FDR] corrected, see <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2A and 2B</xref>). These results confirm that low-frequency rhythmic modulations in the speech envelope align with low-frequency cortical oscillations in auditory areas (using phase-locking value (PLV) instead of MI and contrasting story with surrogate data lead to virtually identical results, see <xref ref-type="supplementary-material" rid="pbio.1001752.s001">Figure S1</xref>).</p>
<fig id="pbio-1001752-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g002</object-id><label>Figure 2</label><caption>
<title>Mutual information group statistics.</title>
<p>All statistical maps are thresholded at <italic>p</italic> = 0.05 (FDR corrected) and colourbars show <italic>t</italic>-values. (A) Group statistical map of MI between speech phase and phase of brain activity in the delta frequency band (1–3 Hz) for the statistical contrast story versus back (see <xref ref-type="supplementary-material" rid="pbio.1001752.s001">Figure S1</xref> for corresponding map using PLV). (B) Group statistical map of MI between speech phase and phase of brain activity in the theta frequency band (3–7 Hz) for the statistical contrast story versus back (see <xref ref-type="supplementary-material" rid="pbio.1001752.s001">Figure S1</xref> for corresponding map using surrogate data). (C) Group statistical map of MI between 3–7 Hz theta phase in speech signal and 35–45 Hz gamma amplitude in brain activity for the contrast story versus back. (D) Complementarity between theta phase and gamma amplitude. Mutual information between theta phase in speech and theta phase in brain activity was computed with and without corresponding gamma amplitude signal. The statistical map shows significantly increased MI when gamma amplitude is used in addition to theta phase.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g002" position="float" xlink:type="simple"/></fig>
<p>To test for other couplings between the speech and cortical oscillations, we also computed MI between the amplitude of the speech and the amplitude of cortical oscillations and between the amplitude of the speech and the phase of cortical oscillations for each frequency between 1 and 60 Hz. These computations revealed no significant dependencies. Finally, we flipped the computations around, to test whether the phase of the speech envelope modulated the amplitude of cortical oscillations. Again, we carried out this computation across frequencies, for all combinations between 1 and 60 Hz and found one significant phase-amplitude coupling. <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref> illustrates that low-frequency changes in the speech envelope (at 3–7 Hz) modulate the amplitude of 35–45 Hz gamma activity in both auditory cortices significantly more strongly in the story compared to the back condition.</p>
<p>In sum, this comprehensive analysis revealed two distinct speech tracking mechanisms in the brain. First, low-frequency speech modulations entrain (that is, align the phase of) delta and theta oscillations in the auditory cortex. Second, low-frequency speech modulations also entrain the amplitude dynamics of gamma oscillations. Both tracking mechanisms are especially sensitive to intelligible speech because the effects are stronger for the story than the back condition. Since the theta phase of the speech envelope is coupled to both, the theta phase (<xref ref-type="fig" rid="pbio-1001752-g002">Figure 2B</xref>) and gamma amplitude (<xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref>) of auditory brain oscillations, we investigated if both these signals represent the same or different information about the speech stimulus. Again, we performed the analysis within an information-theoretic framework based on that of Ince et al. <xref ref-type="bibr" rid="pbio.1001752-Ince1">[35]</xref>. Specifically, we investigated whether the information about speech in the theta phase of auditory oscillations is similar or complementary to that carried by gamma power. We computed whether gamma amplitude adds significant mutual information about the speech envelope over and above the information carried by the theta phase of brain activity (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> section for details). The analysis revealed that gamma amplitude does add significant complementary information to theta phase. Gamma amplitude adds on average 23% (±7 standard error of the mean [SEM]) to theta phase information. <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2D</xref> illustrates this complementarity and shows how it is particularly pronounced for the left auditory cortex. This suggests that each mechanism is partly independent of the other and thus can capture complementary information about the stimulus.</p>
</sec><sec id="s2b">
<title>Oscillatory Speech Tracking Is Lateralised</title>
<p>Next we statistically tested for possible lateralisation of these different tracking mechanisms. The analysis was based on FDR-corrected dependent samples' <italic>t</italic>-tests of MI values for corresponding voxels in the left and the right hemisphere for the story condition. Interestingly, although present in both left and right hemisphere (<xref ref-type="fig" rid="pbio-1001752-g002">Figure 2A and 2B</xref>), delta and theta phase-locking to speech was significantly stronger in the right (<xref ref-type="fig" rid="pbio-1001752-g003">Figure 3A and 3B</xref>). Lateralisation maps also revealed a spatial dissociation whereby delta MI was right-lateralised in frontal and parietal areas whereas theta MI was only right-lateralised in superior temporal areas. In contrast, gamma amplitude tracking showed the opposite lateralisation with stronger coupling to speech in the left as compared to the right auditory cortex (<xref ref-type="fig" rid="pbio-1001752-g003">Figure 3C</xref>). Finally, we compared lateralisation of theta phase tracking to lateralisation of gamma-amplitude tracking for the story condition. The statistical map shows significantly higher lateralisation for theta phase tracking in the right auditory cortex but significantly higher lateralisation for gamma amplitude tracking in the left auditory cortex (<xref ref-type="fig" rid="pbio-1001752-g003">Figure 3D</xref>). We further confirmed these group results for single participants. A similar lateralisation pattern was seen in 17 out of 22 participants corroborating the group statistics (<xref ref-type="supplementary-material" rid="pbio.1001752.s002">Figure S2</xref>). Mutual information values (mean and SEM) for the left and right auditory cortex are displayed as bar plots in <xref ref-type="supplementary-material" rid="pbio.1001752.s003">Figure S3</xref> for all conditions illustrating the lateralisation patterns.</p>
<fig id="pbio-1001752-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g003</object-id><label>Figure 3</label><caption>
<title>Mutual information group statistics of lateralisation in the story condition.</title>
<p>All maps show <italic>t</italic>-statistics of lateralisation index (left−right)/(left+right) of mutual information. Red colours indicate lateralisation to the left cortical areas. Only the left hemisphere is shown because results are redundant in the right hemisphere. (A) Group statistical map of lateralisation of delta band MI (corresponding to <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2A</xref>). (B) Group statistical map of lateralisation of theta band MI (corresponding to <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2B</xref>). (C) Group statistical map of lateralisation of theta phase to gamma amplitude coupling (corresponding to <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref>). (D) Group statistical map comparing theta phase to gamma-amplitude lateralisation versus theta phase lateralisation. Maps are thresholded at <italic>p</italic> = 0.05 (FDR corrected).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g003" position="float" xlink:type="simple"/></fig>
<p>This analysis revealed differential hemispheric preference for the two coupling mechanisms. Whereas right hemisphere areas showed stronger low-frequency phase coupling to the speech envelope, left hemisphere areas showed stronger high-frequency amplitude coupling to the speech envelope.</p>
</sec><sec id="s2c">
<title>Oscillatory Speech Tracking Mechanisms Depend on a Nested Hierarchy of Brain Oscillations</title>
<p>This delta and theta phase coupling together with gamma amplitude coupling suggests that the brain oscillations might be nested <xref ref-type="bibr" rid="pbio.1001752-Lakatos1">[4]</xref>. To test for this cross-frequency coupling we computed the mutual information between the theta phase and gamma amplitude of each voxel across the 7-min dataset. By contrast to the analysis shown in <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref>, both the theta phase and the gamma amplitude were derived from the same voxel. The resulting mutual information map for each participant quantifies cross-frequency coupling of theta phase and gamma amplitude in each voxel. As before, we performed group statistics on the individual mutual information maps to identify significant differences between the story and back condition. <xref ref-type="fig" rid="pbio-1001752-g004">Figure 4A</xref> shows significantly increased cross-frequency coupling (theta phase and gamma amplitude) for the story condition compared to the back condition both in bilateral auditory areas and in language areas of the left hemisphere.</p>
<fig id="pbio-1001752-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g004</object-id><label>Figure 4</label><caption>
<title>Group statistics of cross-frequency coupling.</title>
<p>(A) Statistical map of difference between story and back condition for mutual information between theta phase and gamma amplitude. (B) Statistical map of lateralisation of mutual information between theta phase and gamma amplitude for the story condition.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g004" position="float" xlink:type="simple"/></fig>
<p>Lateralisation analysis revealed that the modulation of gamma amplitude by theta phase is stronger in the left compared to right hemisphere (<xref ref-type="fig" rid="pbio-1001752-g004">Figure 4B</xref>).</p>
<p>We performed the same analysis for cross-frequency coupling between delta phase and theta amplitude. The statistical difference map between the story and the back condition showed significant effects in bilateral temporal areas (<xref ref-type="supplementary-material" rid="pbio.1001752.s004">Figure S4A</xref>) with lateralisation to left hemisphere (<xref ref-type="supplementary-material" rid="pbio.1001752.s004">Figure S4B</xref>) but these effects were not as strong as those for the theta-gamma coupling.</p>
<p>In summary, these results indicate that oscillatory speech tracking is supported by a nested hierarchy of oscillations at delta, theta, and gamma frequencies and that these cross-frequency interactions are stronger for intelligible than for unintelligible speech.</p>
</sec><sec id="s2d">
<title>Phase Resets of Auditory Brain Oscillations by Speech Edges Improve Speech Tracking</title>
<p>At this juncture, it is important to note that speech, though rhythmic, is not strictly periodic: it comprises discontinuities and changes in syllable rate and duration. Any cortical speech tracking mechanism must be able to track these irregularities. We predicted that temporal edges in the speech envelope <xref ref-type="bibr" rid="pbio.1001752-Chait1">[36]</xref> should induce phase resets in the cortical oscillations tracking the speech thereby enhancing tracking. Here, we focussed on the theta band phase-locking because of its relation to the syllable rate.</p>
<p>We used a thresholding algorithm to identify 254 separate temporal edges in the continuous stimulus (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). We then computed theta-band phase-locking between auditory theta activity and the theta phase of speech envelope time-locked to these edges. This quantifies the alignment between both signals as in <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2B</xref> but now time-locked to temporal edges. <xref ref-type="fig" rid="pbio-1001752-g005">Figure 5</xref> shows increased alignment between brain oscillations and speech envelope in the left (blue solid line) and the right (red solid line) auditory cortex following edges. <italic>t</italic>-Tests revealed significant (<italic>p</italic>&lt;0.05) increase of phase-locking in an early (100–300 ms) and late (400–600 ms) time window compared to baseline (−200 to 0 ms).</p>
<fig id="pbio-1001752-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g005</object-id><label>Figure 5</label><caption>
<title>Phase-locking value in the auditory cortex time-locked to temporal speech edges.</title>
<p>Phase-locking in theta frequency band between low-frequency speech envelope and the left (PLV speech L, blue solid line) and right (PLV speech R, red solid line) auditory cortex is shown following edge onset at 0 ms. Dashed lines show phase-locking across trials (regardless of speech signal) timelocked to edge onset for left (PLV L, blue dashed line) and right (PLV R, red dashed line). The black line represents phase-locking between the left and right auditory cortex.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g005" position="float" xlink:type="simple"/></fig>
<p>To measure the extent to which this increase can be explained by a stereotypical edge-evoked response we computed phase-locking of auditory theta activity across trials time-locked to edge onset (dashed lines). This measure captures the evoked response to edge onset. As expected, this evoked response (dashed lines) increased following edge onset with a similar dynamics as the phase-locking to speech (solid lines). But importantly, phase-locking to speech (solid lines) is significantly stronger in the late time window than phase-locking to edge onset (dashed lines) (<italic>t</italic>-test, <italic>p</italic>&lt;0.05). This demonstrates that speech continuously entrains brain rhythms beyond a stereotypical short-lived phase reset evoked by edges.</p>
<p>Finally, we computed the phase-locking between left and right auditory theta activity (<xref ref-type="fig" rid="pbio-1001752-g005">Figure 5</xref>, black line). This measure quantifies the temporal coordination between both auditory cortices in the theta band. Interestingly, the increased phase alignment to speech coincided with a significant reduction of phase-locking between both auditory cortices in the early window. One interesting possibility is that this reduction in phase-locking reflects the more sensitive tracking of speech theta rhythms in the right auditory cortex compared to the left. Indeed, phase-locking to speech is significantly stronger in right than in the left auditory cortex from 50–100 ms (<italic>t</italic>-test, <italic>p</italic>&lt;0.05). This could indicate that phase resetting in the left hemisphere is partly driven by the right auditory cortex.</p>
<p>Overall, the results confirmed our prediction. Edges in speech increased the alignment of auditory theta oscillations to the speech envelope and this increase outlasted the standard evoked response to edge onset. In addition, speech edges caused a significant transient decoupling of both auditory cortices.</p>
</sec><sec id="s2e">
<title>Oscillatory Speech Tracking Optimises Sampling of The Speech Signal</title>
<p>Since oscillations represent rhythmic fluctuations in the excitability of neural populations we hypothesised that phase-locking (assisted by phase resetting) between the speech envelope and low-frequency oscillations in the auditory cortex implements a mechanism for efficient sampling and segmentation of speech <xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Schroeder2">[31]</xref>. To directly test this sampling hypothesis, we measured the correlation between each cortical oscillatory band between 1 and 60 Hz and the speech envelope for the 254 trials identified in the previous analysis. <xref ref-type="fig" rid="pbio-1001752-g006">Figure 6A</xref> illustrates this analysis for a sample taken from one individual. The black line shows the speech envelope for a given trial and the dashed line shows the cosine of theta phase in the right auditory cortex for this participant. In the full analysis we computed the cross-correlation for each brain voxel and for each of the 254 trials (defined as the 500 ms following an onset) and then averaged the absolute correlation across trials, for each oscillatory band independently. To account for the different tracking mechanisms identified above (phase tracking and amplitude tracking), we computed two correlations. First, we correlated the cosine of the phase of cortical oscillations with the speech envelope. Second, we correlated the amplitude of cortical oscillations with the speech envelope. For comparison, we also computed these correlations after randomly shuffling the trial order of the speech envelope.</p>
<fig id="pbio-1001752-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g006</object-id><label>Figure 6</label><caption>
<title>Oscillatory speech sampling.</title>
<p>(A) Speech envelope (black line) and cosine of theta phase of the right auditory cortex of one participant for one trial. (B) The spatial distribution of significant correlation between low-frequency (3–7 Hz) phase and speech envelope (<italic>p</italic>&lt;0.05, FDR corrected). The statistical map shows <italic>t</italic>-values of the statistical contrast between correlations for the story condition and trial-shuffled surrogate data. (C) Spectrum of cross-correlation between oscillations in the left and right auditory cortex and speech envelope. Black lines correspond to correlations based on the cosine of phase and blue lines to correlations based on amplitude. Solid lines represent the right auditory cortex and dashed lines represent the left auditory cortex. Horizontal dotted lines show 95th percentile of chance distribution of the maximum across frequencies obtained from shuffled data for phase (black) and amplitude (blue).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g006" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pbio-1001752-g006">Figure 6B</xref> shows significantly higher correlations in left and right auditory areas for low-frequency phase oscillations compared with the shuffled condition. <xref ref-type="fig" rid="pbio-1001752-g006">Figure 6C</xref> presents the spectral profile of correlation for the left and right auditory cortex. At frequencies below 10 Hz the phase of auditory oscillations shows higher correlations with the speech envelope than does amplitude. Above 10 Hz this pattern is reversed. Interestingly the correlation based on amplitude (blue lines) shows a peak at 40–50 Hz in agreement with <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref>. An additional peak is evident at about 20 Hz. Speech sampling by phase in the delta and theta band in the left and right auditory cortex is significantly higher for the story compared to the back condition (and also compared to trial-shuffled data, paired <italic>t</italic>-tests, all <italic>p</italic>&lt;0.05). Speech sampling by amplitude in the gamma band is significantly higher for the story compared to the back condition in the left auditory cortex (and compared to trial-shuffled data in both auditory cortices). Although the pattern of lateralisation was overall consistent with <xref ref-type="fig" rid="pbio-1001752-g003">Figure 3</xref>, the difference in lateralisation did not reach significance. This is probably because this correlation measure is less sensitive than the mutual information analysis on the band-pass filtered speech envelope reported in <xref ref-type="fig" rid="pbio-1001752-g003">Figure 3</xref>.</p>
<p>These results indicate that temporal edges in speech amplitude induce modulations in low-frequency phase and high-frequency amplitude dynamics of brain oscillations that align windows of high neural excitability to salient speech events. Importantly, this alignment is not caused by an identical phase resetting for all edges because shuffling the speech trials reduces the correlation. We predicted that edge-specific phase resets coding stimulus features (e.g., edge amplitude) cause this trial-specific alignment. We tested this hypothesis by sorting our previously identified 254 trials by maximum amplitude of speech envelope in the 200 ms window after onset. For each participant we computed in the left and right auditory cortex the theta phase at 100 ms after onset and correlated both quantities using circular correlation <xref ref-type="bibr" rid="pbio.1001752-Berens1">[37]</xref>. Significant correlation was observed in the left and right auditory cortex (<xref ref-type="supplementary-material" rid="pbio.1001752.s005">Figure S5</xref>).</p>
<p>Together, these results demonstrate that the phase of low-frequency cortical oscillations and the amplitude of high-frequency oscillations align to trial-specific speech dynamics, adapting to variations of speech over time. This trial-specific alignment suggests that oscillatory windows of high excitability sample salient speech components. Our analysis on the continuous data (<xref ref-type="fig" rid="pbio-1001752-g004">Figures 4</xref> and <xref ref-type="supplementary-material" rid="pbio.1001752.s004">S4</xref>) has demonstrated a nested hierarchy of oscillations in the auditory cortex with stronger cross-frequency coupling for intelligible speech compared to unintelligible speech. Since edges enhance oscillatory speech tracking we hypothesised that edges also increase this cross-frequency coupling. We tested this hypothesis in our final analysis.</p>
</sec><sec id="s2f">
<title>Speech Edges Increase Cross-Frequency Coupling</title>
<p>We first characterised the spatial distribution of edge-induced changes in cross-frequency coupling by computing coupling of gamma amplitude to theta phase in all brain voxels. We then computed the full cross-frequency coupling matrix separately for the left and the right auditory cortex.</p>
<p>As before, we used MI to analyze cross-frequency oscillatory coupling (as in <xref ref-type="fig" rid="pbio-1001752-g004">Figure 4A</xref>) but now time-locked to edges. For each brain voxel, across all 254 trials we computed a <italic>t</italic>-statistic of MI between theta phase and gamma amplitude for the two 500 ms windows preceding and following speech onset. Since this computation is based on the difference between post-stimulus and pre-stimulus data it captures the edge-induced changes of cross-frequency coupling. We performed the computation for both the story and back condition. As in <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2</xref> we submitted individual maps to dependent samples <italic>t</italic>-test (story versus back condition) with randomisation-based FDR correction. Group <italic>t</italic>-maps are displayed with thresholds corresponding to <italic>p</italic>&lt;0.05 (FDR-corrected). <xref ref-type="fig" rid="pbio-1001752-g007">Figure 7A</xref> shows the spatial distribution of theta phase to gamma-amplitude coupling. Left and right auditory areas show a significant difference of edge-induced changes in cross-frequency coupling between the story and back condition.</p>
<fig id="pbio-1001752-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1001752.g007</object-id><label>Figure 7</label><caption>
<title>Cross-frequency phase-amplitude coupling.</title>
<p>(A) Spatial distribution of theta phase to gamma amplitude coupling. Group statistical map of difference between story and back condition thresholded at <italic>p</italic> = 0.05 (FDR corrected). Colour code represents <italic>t</italic>-values. (B) Spectral distribution of phase-amplitude coupling in the auditory cortex. Cross-frequency phase-amplitude coupling quantified with MI is shown for the left and right auditory cortex. Pixels with significant difference between story and surrogate condition are displayed as opaque. (C) Lateralisation of cross-frequency phase-amplitude coupling. Pixels with significant lateralisation are displayed as opaque. Positive <italic>t</italic>-values indicate left-lateralized effects.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1001752.g007" position="float" xlink:type="simple"/></fig>
<p>The second analysis used the left and right auditory cortex as regions of interest to compute the full cross-frequency coupling matrix. Here, we computed MI as before but now for all combinations of phase (1–10 Hz) and amplitude (4–80 Hz). We computed group <italic>t</italic>-statistics for the difference between the story condition and surrogate data (significant pixels are opaque, see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Both left and right auditory cortices show a frequency-specific coupling of theta phase to gamma amplitude and in addition a frequency-specific coupling of delta phase and theta amplitude (<xref ref-type="fig" rid="pbio-1001752-g007">Figure 7B</xref>). Both effects are significantly stronger (<italic>t</italic>-test, <italic>p</italic>&lt;0.05) in the story condition compared to the back condition, demonstrating a more precise hierarchical nesting of cortical oscillations for intelligible than unintelligible speech.</p>
<p>Finally, we studied lateralisation of the cross-frequency coupling shown in <xref ref-type="fig" rid="pbio-1001752-g007">Figure 7B</xref>. The results in <xref ref-type="fig" rid="pbio-1001752-g007">Figure 7C</xref> demonstrate a significant lateralisation of theta-gamma coupling to the left auditory cortex.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>Our results provide direct evidence for the hypothesis that a listener's brain oscillations segment and encode continuous speech in a frequency-specific manner. This suggests that these oscillations play a functional role in efficient sensory sampling. MI analysis reveals alignment of low-frequency phase and high-frequency amplitude to the speech envelope that is frequency specific, shows hemispheric asymmetry, and is modulated by intelligibility (i.e., enhanced for story compared to back condition). The low-frequency phase alignment is preserved over time by transient events in the stimulus (edges) that lead to phase adjustments. These phase adjustments are stimulus specific and depend on the amplitude of transient events (and likely other features of the stimulus). Interestingly, brain activity in the three observed frequency bands is hierarchically coupled. This cross-frequency coupling is increased following edge onset and the increase is stronger for speech than for reverse speech.</p>
<sec id="s3a">
<title>Spatio-Spectral Characteristics of Speech Entrainment</title>
<p>We observed phase alignment between low-frequency components of the speech envelope and brain activity in the delta and theta band. No consistent phase-phase coupling was observed for frequencies higher than 10 Hz. Previous studies have shown that speech envelope frequencies below 10 Hz are important for intelligibility <xref ref-type="bibr" rid="pbio.1001752-Elliott1">[38]</xref>. Indeed, delta and theta frequencies match the rhythmicity of important temporal structures in continuous speech. Slow speech envelope variations (0.3–1 s, delta band) represent prosody whereas syllables tend to occur at a rate of about 3–7 Hz in normal speech <xref ref-type="bibr" rid="pbio.1001752-Ghitza1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Poeppel1">[10]</xref>. These components are known to modulate oscillatory phase and amplitude dynamics in the auditory cortex <xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>. Our study investigated the underlying mechanisms by using information theory to comprehensively quantify how the phase and amplitude of different frequency components of the speech envelope affect the phase and amplitude of different cortical brain oscillations.</p>
<p>We reported two different mechanisms. First, the low-frequency phase in the speech envelope entrains the low-frequency phase of brain oscillations in delta and theta frequency bands. The specific entrainment patterns support the idea that delta and theta bands are qualitatively different <xref ref-type="bibr" rid="pbio.1001752-Cogan1">[25]</xref>. Phase coupling in the delta band extends more towards right frontal areas compared to theta phase coupling and both frequencies show different spatial lateralisation patterns (<xref ref-type="fig" rid="pbio-1001752-g003">Figure 3</xref>). This indicates selective engagement of different areas for processing the different quasi-rhythmic components of the stimulus. Interestingly, significant right-lateralisation was evident in the delta band in frontal, posterior temporal, and parietal areas but not in primary auditory areas (in contrast to the theta band). These results are consistent with previous findings that right temporal and frontal brain areas are involved in prosodic processing <xref ref-type="bibr" rid="pbio.1001752-Bourguignon1">[24]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Friederici1">[39]</xref>. Bilateral auditory areas show significant theta phase entrainment to the speech envelope. This effect is significantly lateralised to the right hemisphere and confirms previous findings <xref ref-type="bibr" rid="pbio.1001752-Luo1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Abrams1">[23]</xref>.</p>
<p>The second mechanism revealed in our analysis is the alignment of gamma-amplitude modulations to the theta phase of the speech envelope in bilateral temporal, frontal, and parietal areas with lateralisation to the left hemisphere. Taken together, the auditory cortex showed right-lateralisation for theta phase entrainment and left-lateralisation for gamma amplitude entrainment. These results support the asymmetric sampling in time (AST) model <xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Poeppel2">[14]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Hickok1">[40]</xref> (but see <xref ref-type="bibr" rid="pbio.1001752-McGettigan1">[41]</xref>) that suggests a right-hemispheric preference for long temporal integration windows of 100–300 ms (corresponding to theta band) and a left-hemispheric preference for short temporal integration windows of about 20–40 ms (corresponding to gamma frequencies). Indeed, this view is supported by studies of phase consistency in the theta band <xref ref-type="bibr" rid="pbio.1001752-Luo1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Abrams1">[23]</xref> and of oscillatory power in the gamma band <xref ref-type="bibr" rid="pbio.1001752-Morillon1">[13]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Morillon2">[42]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Giraud2">[43]</xref>. Our results demonstrate a direct effect of specific speech components (low-frequency phase of speech envelope) on oscillatory brain activity and show significant lateralisation consistent with the AST-model. Interestingly, this coupling of brain oscillations to speech rhythms is supported by a hierarchical coupling of brain oscillations across frequencies. Delta phase modulates theta amplitude and theta phase modulates gamma amplitude and this modulation is stronger for intelligible compared to unintelligible speech. The hierarchically coupled oscillations could represent speech components (prosody, syllables, phonemes) in parallel at different timescales while preserving their mutual relationships.</p>
<p>All entrainment effects were identified in a statistical contrast between the story and the back condition. This is important because it demonstrates that these entrainments are not just unspecific stimulus-driven effects but that they are modulated by intelligibility of the stimulus. A previous study <xref ref-type="bibr" rid="pbio.1001752-Howard1">[44]</xref> did not find entrainment differences between the two conditions. This might be explained by the fact that their stimulus material consisted only of three sentences across the whole study leading to learning effects even for the reversed speech. Also, the specific task used in that paper did not require comprehension and therefore might have masked differences between the speech and reversed speech condition. Reverse speech is often used as a control condition in speech experiments <xref ref-type="bibr" rid="pbio.1001752-Howard1">[44]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Sato1">[46]</xref> since the physical properties of the stimulus are preserved. Especially, rhythmic components in the speech stimuli are still present in reversed speech (although the quasi-periodicity of rhythmic components in speech will lead to some changes in the oscillatory dynamics of reversed speech). The enhanced entrainment observed in the story condition is therefore likely due to top-down mechanisms that have been previously shown to modulate activity in the auditory cortex during processing of degraded speech <xref ref-type="bibr" rid="pbio.1001752-Sohoglu1">[47]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Wild1">[48]</xref> or speech in noise <xref ref-type="bibr" rid="pbio.1001752-Ding2">[49]</xref>. These mechanisms could lead to changes in oscillatory phase dynamics <xref ref-type="bibr" rid="pbio.1001752-ZionGolumbic2">[26]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Ding3">[50]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Kerlin1">[51]</xref>. We expect that within sentences, paragraphs, and over the entire course of the story participants will predict upcoming words and salient auditory events. This content-based prediction in the story condition seems to affect phase entrainment in early sensory areas <xref ref-type="bibr" rid="pbio.1001752-Peelle1">[22]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Lakatos3">[52]</xref>–<xref ref-type="bibr" rid="pbio.1001752-Peelle2">[54]</xref>.</p>
</sec><sec id="s3b">
<title>Phase Resetting and Oscillatory Speech Sampling</title>
<p>Our study supports emerging models of speech perception that emphasise the role of brain oscillations <xref ref-type="bibr" rid="pbio.1001752-Ghitza1">[9]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Giraud1">[12]</xref>. Hierarchically organised brain oscillations may sample continuous speech input at rates of prominent speech rhythms (prosody, syllables, phonemes) and represent a first step in converting a continuous auditory stream to meaningful internal representations. Our data suggest that this step of sparsening the sensory representation occurs in parallel computations both in frequency (as multiplexed oscillations) and in the left and right hemisphere <xref ref-type="bibr" rid="pbio.1001752-Hickok1">[40]</xref> albeit with lateralised preference for different time scales.</p>
<p>Our results indicate that sharp large-amplitude transients (edges) in speech reset oscillations in the auditory cortex with important consequences. First, these resets increase the alignment between auditory oscillations and the speech envelope (<xref ref-type="fig" rid="pbio-1001752-g006">Figure 6</xref>). This is important to re-align brain oscillations to speech after breaks. Second, this increase in alignment accounts for variations in continuous speech because randomly shuffling the speech signal across trials reduces the alignment. Since each trial represented a different segment of the continuous story this finding shows that brain oscillations are dynamically aligned to the time-varying dynamics of speech. Third, cross-frequency coupling between auditory oscillations increases following edges thereby enhancing precision of multi-scale nested dependencies. Fourth, temporal edges lead to a transient decoupling of the left and right auditory cortex that could be caused by a differential phase reset in both cortices and could indicate sensitivity to different acoustic properties of the stimulus.</p>
<p>In the rat auditory cortex, increases in sound power in the frequency band matching the tonotopy of the considered location lead to large depolarizing currents in the input layers that reset intrinsic oscillations to an “excitable” phase <xref ref-type="bibr" rid="pbio.1001752-Szymanski1">[55]</xref> (see also <xref ref-type="bibr" rid="pbio.1001752-Lakatos4">[56]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Shamir1">[57]</xref>). It is therefore conceivable that our observed phase resets to edges realigns the internal temporal reference frame to the sensory input to optimally sample relevant information at oscillatory phases of high excitability. This phase reset is stimulus dependent because correlation with speech is reduced for trial-shuffled data (<xref ref-type="fig" rid="pbio-1001752-g006">Figure 6</xref>) and because phase after edge-onset codes the amplitude of this edge (<xref ref-type="supplementary-material" rid="pbio.1001752.s005">Figure S5</xref>). This coding of peak stimulus amplitude (and possible other features) in low-frequency phase could explain the previously reported classification of stimulus identity from low-frequency phase dynamics <xref ref-type="bibr" rid="pbio.1001752-Kayser1">[58]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Ng1">[59]</xref>. The stimulus-specific phase resetting could be an important mechanism for aligning time windows of high neural excitability to salient stimulus events because of similar time constants in speech and brain dynamics. The importance of edges for speech entrainment was very recently shown by Doelling et al. <xref ref-type="bibr" rid="pbio.1001752-Doelling1">[60]</xref>. By manipulating the speech envelope they demonstrated that edges enhance speech entrainment and intelligibility.</p>
<p>In summary, we report a nested hierarchy of auditory oscillations at multiple frequencies that match the frequency of relevant linguistic components in continuous speech. These oscillations entrain to speech with differential hemispheric preference for high (left) and low (right) frequencies. Our results indicate that temporal edges in speech increase first the coupling between auditory oscillations across frequency bands and, second, their coupling to the speech envelope.</p>
<p>We can only speculate about the nature of the observed phase/amplitude alignments. Most likely the alignments are caused by a combination of modulatory and evoked effects <xref ref-type="bibr" rid="pbio.1001752-Szymanski1">[55]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Lakatos4">[56]</xref> where stimulus-driven activity is top-down modulated via ongoing oscillatory activity <xref ref-type="bibr" rid="pbio.1001752-Lakatos2">[30]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Luczak1">[61]</xref>. In this framework oscillatory activity is a mechanism for attentional selection and flexible gating of information from primary sensory areas.</p>
<p>Finally, going beyond speech perception, the entrainment of hierarchically organized oscillations between speaker and listener may well have a more general role in interpersonal communication <xref ref-type="bibr" rid="pbio.1001752-Hasson1">[62]</xref>,<xref ref-type="bibr" rid="pbio.1001752-Pickering1">[63]</xref>.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Participants and Recording</title>
<p>22 healthy, right-handed volunteers participated in the study (11 males; age range 19–44 years, mean 27 years). All participants provided informed written consent and received monetary compensation for their participation. The study was approved by the local ethics committee (University of Glasgow Faculty of Information and Mathematical Sciences) and conducted in conformity with the Declaration of Helsinki.</p>
<p>MEG recordings were obtained with a 248-magnetometers whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging) at 1,017 Hz sampling rate.</p>
<p>The analysis of the MEG signal was performed using the FieldTrip toolbox <xref ref-type="bibr" rid="pbio.1001752-Oostenveld1">[64]</xref>, the Information-Theory Toolbox <xref ref-type="bibr" rid="pbio.1001752-Magri1">[33]</xref>, and in-house MATLAB code according to recently published guidelines <xref ref-type="bibr" rid="pbio.1001752-Gross1">[65]</xref>.</p>
<p>Stimuli have been previously used in an fMRI study <xref ref-type="bibr" rid="pbio.1001752-Lerner1">[66]</xref>. The main stimulus consisted of a recording of a 7-min real-life story (“Pie-man,” told by Jim O'Grady at “The Moth” storytelling event, New York). The story was presented binaurally via a sound pressure transducer through two 5 m long plastic tubes terminating in plastic insert earpieces. Presentation was controlled with Psychtoolbox <xref ref-type="bibr" rid="pbio.1001752-Brainard1">[67]</xref> under MATLAB. In addition to one standard presentation of the story (story), individuals also listened to the backward played story (back). Eye fixation was maintained throughout the experiment. Experimental conditions were recorded in randomised order.</p>
</sec><sec id="s4b">
<title>Analysis</title>
<sec id="s4b1">
<title>Speech preprocessing</title>
<p>We computed the amplitude envelope of auditory signals following Chandrasekaran et al. <xref ref-type="bibr" rid="pbio.1001752-Chandrasekaran1">[11]</xref>. Using the Chimera toolbox we constructed nine frequency bands in the range 100–10,000 Hz to be equidistant on the cochlear map <xref ref-type="bibr" rid="pbio.1001752-Smith1">[68]</xref>. Auditory stimuli were band-pass filtered in these bands using a fourth-order Butterworth filter (forward and reverse). Amplitude envelopes for each band were computed as absolute values of the Hilbert transform and averaged across bands to obtain a wide-band amplitude envelope that was used for all further analysis.</p>
<p>MEG signals were denoised with information from the reference sensors using the denoise_pca function in FieldTrip. Bad channels were excluded by visual inspection.</p>
</sec><sec id="s4b2">
<title>MEG-MRI co-registration</title>
<p>T1-weighted structural magnetic resonance images (MRIs) of each participant were co-registered to the MEG coordinate system using a semi-automatic procedure. Anatomical landmarks (nasion, left and right pre-auricular points) were manually identified in the individual's MRI. Initial alignment of both coordinate systems was based on these three points. Subsequently, numerical optimisation was achieved by using the ICP algorithm <xref ref-type="bibr" rid="pbio.1001752-Besl1">[69]</xref>. All region-of-interest analysis for the auditory cortex is based on the mean effect of all voxels in BA 41.</p>
</sec><sec id="s4b3">
<title>Source localisation</title>
<p>Individual head models were created from anatomical MRIs using segmentation routines in FieldTrip/SPM5. Leadfield computation was based on a single shell volume conductor model <xref ref-type="bibr" rid="pbio.1001752-Nolte1">[70]</xref> using a 10 mm grid defined on the template (MNI) brain. The template grid was transformed into individual head space by linear spatial transformation.</p>
<p>Cross-spectral density was computed using Fast Fourier Transform on 1-s segments of data after applying Hanning window. For frequencies above 40 Hz spectral analysis was performed using multitaper (±5 Hz frequency smoothing <xref ref-type="bibr" rid="pbio.1001752-Percival1">[71]</xref>). Source localisation was performed using DICS <xref ref-type="bibr" rid="pbio.1001752-Gross2">[72]</xref>. Beamformer coefficients were computed sequentially for all frequencies from 1 to 60 Hz for the dominant source direction in all voxels with a regularisation of 7% of the mean across eigenvalues of the cross-spectral density matrix.</p>
</sec><sec id="s4b4">
<title>Mutual information</title>
<p>Dependencies between phase and amplitude of speech and MEG signal were all analysed in the common framework of information theory <xref ref-type="bibr" rid="pbio.1001752-Cover1">[73]</xref>. Specifically, MI between two signals was computed using the Information-Theory Toolbox <xref ref-type="bibr" rid="pbio.1001752-Magri1">[33]</xref>. MI measures how much knowing one signal reduces the uncertainty about the other signal. MI analysis was used because it captures both linear and non-linear dependencies (in contrast to coherence or correlation) and it affords the quantification of encoding by a range of sound and brain activity features (e.g., phase-phase, amplitude-amplitude, phase-amplitude, or cross-frequency encoding) within the same theoretic framework and on a common principled scale in units of bits.</p>
<p>First, frequency-specific brain activation time series were computed by applying the (frequency-specific) beamformer coefficients to the MEG data filtered in the same frequency band (fourth order Butterworth filter, forward and reverse, centre frequency ±1 Hz (or ±5 Hz for frequencies above 40 Hz). The broadband speech envelope was processed identically. Second, Hilbert transform was applied to the bandpass filtered data to compute phase or amplitude dynamics. Finally, MI was computed between the speech signal and brain signal for each voxel, frequency band, and for all combinations of signals (phase-phase, phase-amplitude, amplitude-phase, amplitude-amplitude). MI computation was performed using the direct method with quadratic extrapolation for bias correction in the Information-Theory Toolbox <xref ref-type="bibr" rid="pbio.1001752-Magri1">[33]</xref>. We quantised data into ten equi-populated bins but results were robust to changes in the number of bins. The result of this computation was a volumetric MI map (describing dependencies between speech and brain activity) for each frequency and individual. This computation was performed for the story condition and the back condition. In addition, surrogate MI maps were created by computing MI between the brain activity from the story condition and the reversed speech signal. This provides an estimate of MI values that can be expected by chance.</p>
</sec><sec id="s4b5">
<title>Statistics</title>
<p>Group statistical analysis was performed on the data of all 22 participants using non-parametric randomisation statistics in FieldTrip (Monte Carlo randomisation). Specifically, individual volumetric maps were smoothed with a 10 mm Gaussian kernel and subjected to dependent-samples T-test. The null distribution was estimated using 500 randomisations and multiple comparison correction was performed using FDR <xref ref-type="bibr" rid="pbio.1001752-Benjamini1">[74]</xref>. Only significant results (<italic>p</italic>&lt;0.05 corrected) are reported. Group statistics were computed to compare the story condition to back condition and surrogate analysis. Final statistical maps (thresholded at <italic>p</italic>&lt;0.05 corrected) are rendered on the MNI template brain. To confirm that MI for phase-phase interaction is due to phase-locking of speech and brain signals we computed PLV <xref ref-type="bibr" rid="pbio.1001752-Lachaux1">[75]</xref> and performed the same group statistics as for MI maps (<xref ref-type="supplementary-material" rid="pbio.1001752.s001">Figure S1</xref>).</p>
</sec><sec id="s4b6">
<title>Lateralisation</title>
<p>Statistical analysis of lateralisation was performed in three steps. First, corresponding voxels in both hemispheres were identified on the basis of their coordinates. Second, the lateralisation index (LI = [right−left]/[right+left]) was computed for each voxel. Third, significance of lateralisation index was tested (<italic>t</italic>-test against 0) following the approach described in the previous paragraph with FDR correction for multiple comparisons. For <xref ref-type="fig" rid="pbio-1001752-g003">Figure 3D</xref> we performed statistical comparison of theta lateralisation index against theta-gamma lateralisation index.</p>
</sec><sec id="s4b7">
<title>Complementarity of speech tracking mechanisms</title>
<p>To address the question whether MI I of theta speech phase (S<sub>theta</sub>) and theta brain phase (B<sub>theta</sub>) is significantly increased by including gamma amplitude in the computation (<xref ref-type="fig" rid="pbio-1001752-g002">Figure 2D</xref>) we used the approach by Ince et al. <xref ref-type="bibr" rid="pbio.1001752-Ince1">[35]</xref>. The amount of information in gamma amplitude that is complementary to that of theta phase is computed as the difference of I (S<sub>theta</sub>, B<sub>theta</sub> &amp; B<sub>gamma</sub>) and I (S<sub>theta</sub>, B<sub>theta</sub>) using bias-corrected mutual information estimates (values are then expressed as percentage increase with respect to I [S<sub>theta</sub>, B<sub>theta</sub>]). The significance of the difference is tested by computing a null distribution without bias correction for I (S<sub>theta</sub>, B<sub>theta</sub> &amp; B<sub>gamma</sub>) where B<sub>gamma</sub> is shuffled for fixed values of the binned signal B<sub>theta</sub>. The null distribution is then compared to I (S<sub>theta</sub>, B<sub>theta</sub>) computed without bias correction. These separate computations are motivated by the fact that bias correction decreases statistical power but increases accuracy of magnitude estimation <xref ref-type="bibr" rid="pbio.1001752-Ince1">[35]</xref>.</p>
</sec><sec id="s4b8">
<title>Analysis of temporal speech edges</title>
<p>A thresholding algorithm was used to identify temporal edges in speech. The speech envelope was normalised to a maximum amplitude of 1. Speech edges were defined using the following criteria: (1) Mean amplitude in 400 ms before onset is less than 0.05. (2) Mean amplitude in 1 s after onset is larger than 0.05. (3) The difference between the mean amplitude 20 ms before and 20 ms after onset is larger than 0.05. For our particular speech stimulus this resulted in 254 time points characterised by a short period of low speech envelope amplitude followed by a sharp increase in amplitude. Onsets were confirmed by visual inspection of the speech envelope. Speech onset results were robust against small changes of these criteria. The same algorithm was applied to identify speech edges in the back condition. Mean and maximum amplitude and mean and maximum slope in the 100 ms following edge onsets were compared for the story and back condition and showed no significant difference (<italic>t</italic>-test, all <italic>p</italic>&gt;0.05). Time-locked to these onsets we have extracted trials from −500 ms to 1,000 ms.</p>
</sec><sec id="s4b9">
<title>PLV analysis</title>
<p>PLVs <xref ref-type="bibr" rid="pbio.1001752-Benjamini1">[74]</xref> were computed in three ways. First, as phase-locking of auditory theta activity across trials (PLV = 1/<italic>n</italic>|∑ exp(i * ph)| where <italic>n</italic> is the number of trials and ph the phase of auditory theta signal). Second, the phase-locking of the phase difference between auditory theta signal and the theta speech envelope was computed (PLVsp = 1/<italic>n</italic> |∑ (exp(i * (ph−phs))| where <italic>n</italic> is the number of trials and ph the phase of auditory theta signal and phs the theta phase of speech envelope). Third, the phase-locking between left and right auditory theta activity (PLVsp = 1/<italic>n</italic> |∑ (exp(i * (phl−phr))| where <italic>n</italic> is the number of trials and phl and phr the phase of left and right auditory theta signal, respectively). Time-resolved PLV data were averaged in three time windows (−200 ms to 0 ms, 100–300 ms, 400–600 ms) and subjected to Anova analysis with factors time window and PLV measure. Both factors and their interactions were highly significant (time window: F = 39.77, <italic>p</italic>&lt;0.001; PLV measure: F = 50.11, <italic>p</italic>&lt;0.001; interaction: F = 14.86, <italic>p</italic>&lt;0.001).</p>
</sec><sec id="s4b10">
<title>Speech sampling</title>
<p>For each voxel the instantaneous amplitude A and phase ph for each speech trial was computed (<xref ref-type="fig" rid="pbio-1001752-g006">Figure 6</xref>). For each trial the cross-correlation of either cos(ph) or A with the speech envelope was computed over the time range 0–500 ms following onset with a maximum lag of 150 ms. The maximum correlation across lags was averaged across trials. As control the same computation was repeated with a random shuffling of trial order for the speech data (to destroy the correspondence between trials for speech and brain data).</p>
</sec><sec id="s4b11">
<title>Cross-frequency analysis</title>
<p>We performed two separate analyses to investigate the spatio-spectral distribution of cross-frequency coupling (<xref ref-type="fig" rid="pbio-1001752-g007">Figure 7</xref>). First, we computed cross-frequency coupling between theta phase and 40 Hz gamma amplitude in all brain voxels. Second, we computed the full cross-frequency coupling matrix separately for the left and right auditory cortex.</p>
<p>The first analysis was motivated by <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref> that demonstrates coupling between speech theta phase and auditory 40 Hz amplitude dynamics and by <xref ref-type="fig" rid="pbio-1001752-g004">Figure 4</xref> that shows theta phase to gamma amplitude coupling in the auditory cortex. Analysis of cross-frequency coupling was performed by computing MI as in <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2C</xref> (but without using the speech signal). For each brain voxel MI between theta phase and gamma amplitude was computed for the two 500 ms windows preceding and following speech onset across all 254 trials. <italic>t</italic>-values of contrast post-onset versus pre-onset were computed across trials. The computation was performed for the story and back condition. As in <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2</xref> individual maps were subjected to dependent samples <italic>t</italic>-test with randomisation-based FDR correction. Group <italic>t</italic>-maps are displayed with thresholds corresponding to <italic>p</italic>&lt;0.05 (FDR corrected). The second analysis was performed only in the left and right auditory cortex. Here, we computed MI as before but now for all combinations of phase (range 1–10 Hz) and amplitude (range 4–80 Hz). Group <italic>t</italic>-statistic was computed for the difference between story condition and surrogate data (surrogate data were the same as story condition but each amplitude signal was matched with phase signal from a random trial).</p>
<p>For each frequency-frequency pair we computed a bootstrap confidence level by randomly drawing 22 participants with replacement in each of 500 bootstrap iterations and computing the 95th percentile.</p>
<p>The lateralisation analysis in <xref ref-type="fig" rid="pbio-1001752-g007">Figure 7C</xref> follows the same approach as for <xref ref-type="fig" rid="pbio-1001752-g007">Figure 7B</xref> and compares cross-frequency coupling for the story condition between the left and right auditory cortex.</p>
</sec></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pbio.1001752.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pbio.1001752.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p>(A) Mutual information group statistics for surrogate data. Group statistical map of phase-phase MI dependencies in the theta frequency band. This figure corresponds to <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2B</xref> but here the back condition has been replaced with a surrogate condition consisting of the MEG data from the story condition and the reversed speech envelope from the story condition to estimate dependencies that could be expected by chance. (B) Phase-locking group statistics. This figure corresponds to <xref ref-type="fig" rid="pbio-1001752-g002">Figure 2</xref> but instead of MI PLV has been used to quantify the dependence between phase of low-frequency speech envelope and brain activity in the delta band. (C) Same as (B) but for theta frequency band.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pbio.1001752.s002" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pbio.1001752.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Bar plot of individual lateralisation indices.</bold> For each participant the lateralisation index for theta-phase lateralisation (red) and theta-gamma lateralisation (blue) in Heschl's gyrus (left panel) and superior temporal gyrus (STG, right panel) is shown. Each pair of red/blue bars corresponds to an individual.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pbio.1001752.s003" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pbio.1001752.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Bar plot of mutual information in the auditory cortex.</bold> For each panel mean and SEM is shown for the left and right auditory cortex for all conditions. An asterisk indicates relevant significant differences (<italic>t</italic>-test with <italic>p</italic>&lt;0.05). Control condition is computed from surrogate data where brain activity from story condition is used together with speech envelope from back condition. (A) Bar plot for delta phase. (B) Bar plot for theta phase. (C) Bar plot for mutual information between theta phase in speech and gamma amplitude in the auditory cortex. (D) Bar plot for mutual information between theta phase and gamma amplitude in the auditory cortex. Here, control condition was obtained from mutual information with gamma time series reversed.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pbio.1001752.s004" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pbio.1001752.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Group statistics of cross-frequency coupling.</bold> (A) Statistical map of difference between story and back condition for mutual information between delta phase and theta amplitude. (B) Statistical map of lateralisation of mutual information between delta phase and theta amplitude for the story condition. (C) Statistical map of difference between story and back condition for mutual information between theta phase and gamma amplitude. This map corresponds to <xref ref-type="fig" rid="pbio-1001752-g004">Figure 4A</xref> but is computed using a different method for quantifying cross-frequency coupling <xref ref-type="bibr" rid="pbio.1001752-Tort1">[76]</xref>.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pbio.1001752.s005" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pbio.1001752.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>Phase coding of speech amplitude.</bold> The phase of theta oscillations at 100 ms after speech onset in the left (black) and right (red) auditory cortex codes the maximum amplitude of speech envelope in the first 200 ms following onset. The area signifies the 95% confidence interval around the median obtained from bootstrap analysis.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Yulia Lerner and Uri Hasson for sharing the stimulus material and Laura Menenti for helpful discussion.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pbio.1001752-Wang1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Ding</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ahmar</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Xiang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name> (<year>2012</year>) <article-title>Sensitivity to temporal modulation rate and spectral bandwidth in the human auditory system: MEG evidence</article-title>. <source>J Neurophysiol</source> <volume>107</volume>: <fpage>2033</fpage>–<lpage>2041</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Siegel1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siegel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Donner</surname><given-names>TH</given-names></name>, <name name-style="western"><surname>Engel</surname><given-names>AK</given-names></name> (<year>2012</year>) <article-title>Spectral fingerprints of large-scale neuronal interactions</article-title>. <source>Nat Rev Neurosci</source> <volume>13</volume>: <fpage>121</fpage>–<lpage>134</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Schnitzler1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schnitzler</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Gross</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>Normal and pathological oscillatory communication in the brain</article-title>. <source>Nat Rev Neurosci</source> <volume>6</volume>: <fpage>285</fpage>–<lpage>296</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lakatos1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shah</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Knuth</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Ulbert</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Karmos</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2005</year>) <article-title>An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex</article-title>. <source>J Neurophysiol</source> <volume>94</volume>: <fpage>1904</fpage>–<lpage>1911</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Canolty1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Canolty</surname><given-names>RT</given-names></name>, <name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Dalal</surname><given-names>SS</given-names></name>, <name name-style="western"><surname>Soltani</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nagarajan</surname><given-names>SS</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>High gamma power is phase-locked to theta oscillations in human neocortex</article-title>. <source>Science</source> <volume>313</volume>: <fpage>1626</fpage>–<lpage>1628</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Axmacher1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Axmacher</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Henseler</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Jensen</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Weinreich</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Elger</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Fell</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Cross-frequency coupling supports multi-item working memory in the human hippocampus</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>107</volume>: <fpage>3228</fpage>–<lpage>3233</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Canolty2"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Canolty</surname><given-names>RT</given-names></name>, <name name-style="western"><surname>Knight</surname><given-names>RT</given-names></name> (<year>2010</year>) <article-title>The functional role of cross-frequency coupling</article-title>. <source>Trends Cogn Sci</source> <volume>14</volume>: <fpage>506</fpage>–<lpage>515</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Jensen1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jensen</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Colgin</surname><given-names>LL</given-names></name> (<year>2007</year>) <article-title>Cross-frequency coupling between neuronal oscillations</article-title>. <source>Trends Cogn Sci</source> <volume>11</volume>: <fpage>267</fpage>–<lpage>269</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ghitza1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghitza</surname><given-names>O</given-names></name> (<year>2011</year>) <article-title>Linking speech perception and neurophysiology: speech decoding guided by cascaded oscillators locked to the input rhythm</article-title>. <source>Front Psychol</source> <volume>2</volume>: <fpage>130</fpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Poeppel1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Idsardi</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>van Wassenhove</surname><given-names>V</given-names></name> (<year>2008</year>) <article-title>Speech perception at the interface of neurobiology and linguistics</article-title>. <source>Philos T Roy Soc B</source> <volume>363</volume>: <fpage>1071</fpage>–<lpage>1086</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Chandrasekaran1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Trubanova</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Stillittano</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Caplier</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name> (<year>2009</year>) <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000436</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000436" xlink:type="simple">10.1371/journal.pcbi.1000436</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001752-Giraud1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nat Neurosci</source> <volume>15</volume>: <fpage>511</fpage>–<lpage>517</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Morillon1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morillon</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Lehongre</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Frackowiak</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Ducorps</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kleinschmidt</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Neurophysiological origin of human brain asymmetry for speech and language</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>107</volume>: <fpage>18688</fpage>–<lpage>18693</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Poeppel2"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as asymmetric sampling in time’</article-title>. <source>Speech Commun</source> <volume>41</volume>: <fpage>245</fpage>–<lpage>255</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lehongre1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lehongre</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ramus</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Villiermet</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name> (<year>2011</year>) <article-title>Altered low-gamma sampling in auditory cortex accounts for the three main facets of dyslexia</article-title>. <source>Neuron</source> <volume>72</volume>: <fpage>1080</fpage>–<lpage>1090</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-ZionGolumbic1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zion Golumbic</surname><given-names>EM</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2012</year>) <article-title>Temporal context in speech processing and attentional stream selection: a behavioral and neural perspective</article-title>. <source>Brain Lang</source> <volume>122</volume>: <fpage>151</fpage>–<lpage>161</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Schroeder1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kajikawa</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Partan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Puce</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>Neuronal oscillations and visual amplification of speech</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>: <fpage>106</fpage>–<lpage>113</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Panzeri1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name> (<year>2010</year>) <article-title>Sensory neural codes using multiplexed temporal scales</article-title>. <source>Trends Neurosci</source> <volume>33</volume>: <fpage>111</fpage>–<lpage>120</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Schyns1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>, <name name-style="western"><surname>Thut</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Cracking the code of oscillatory activity</article-title>. <source>PLoS Biol</source> <volume>9</volume>: <fpage>e1001064</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1001064" xlink:type="simple">10.1371/journal.pbio.1001064</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001752-Luo1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title>. <source>Neuron</source> <volume>54</volume>: <fpage>1001</fpage>–<lpage>1010</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ahissar1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahissar</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Nagarajan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Protopapas</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mahncke</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name> (<year>2001</year>) <article-title>Speech comprehension is correlated with temporal response patterns recorded from auditory cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>98</volume>: <fpage>13367</fpage>–<lpage>13372</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Peelle1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelle</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Gross</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name> (<year>2013</year>) <article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title>. <source>Cereb Cortex</source> <volume>23</volume>: <fpage>1378</fpage>–<lpage>1387</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Abrams1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abrams</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Nicol</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Zecker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Kraus</surname><given-names>N</given-names></name> (<year>2008</year>) <article-title>Right-hemisphere auditory cortex is dominant for coding syllable patterns in speech</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>3958</fpage>–<lpage>3965</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Bourguignon1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bourguignon</surname><given-names>M</given-names></name>, <name name-style="western"><surname>De Tiège</surname><given-names>X</given-names></name>, <name name-style="western"><surname>de Beeck</surname><given-names>MO</given-names></name>, <name name-style="western"><surname>Ligot</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Paquier</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>The pace of prosodic phrasing couples the listener's cortex to the reader's voice</article-title>. <source>Hum Brain Mapp</source> <volume>34</volume>: <fpage>314</fpage>–<lpage>326</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Cogan1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cogan</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>A mutual information analysis of neural coding of speech by low-frequency MEG phase information</article-title>. <source>J Neurophysiol</source> <volume>106</volume>: <fpage>554</fpage>–<lpage>563</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-ZionGolumbic2"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zion Golumbic</surname><given-names>EM</given-names></name>, <name name-style="western"><surname>Ding</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Bickel</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Schevon</surname><given-names>CA</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party”</article-title>. <source>Neuron</source> <volume>77</volume>: <fpage>980</fpage>–<lpage>991</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Pasley1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasley</surname><given-names>BN</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Flinker</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Reconstructing speech from human auditory cortex</article-title>. <source>PLoS Biol</source> <volume>10</volume>: <fpage>e1001251</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1001251" xlink:type="simple">10.1371/journal.pbio.1001251</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001752-Mesgarani1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>EF</given-names></name> (<year>2012</year>) <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source> <volume>485</volume>: <fpage>233</fpage>–<lpage>236</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ding1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name> (<year>2012</year>) <article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>: <fpage>11854</fpage>–<lpage>11859</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lakatos2"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Musacchia</surname><given-names>G</given-names></name>, <name name-style="western"><surname>O'Connel</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Falchier</surname><given-names>AY</given-names></name>, <name name-style="western"><surname>Javitt</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2013</year>) <article-title>The spectrotemporal filter mechanism of auditory selective attention</article-title>. <source>Neuron</source> <volume>77</volume>: <fpage>750</fpage>–<lpage>761</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Schroeder2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name> (<year>2009</year>) <article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title>. <source>Trends Neurosci</source> <volume>32</volume>: <fpage>9</fpage>–<lpage>18</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Boemio1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boemio</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Fromm</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Braun</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2005</year>) <article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>389</fpage>–<lpage>395</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Magri1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Magri</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Whittingstall</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>A toolbox for the fast information analysis of multiple-site LFP, EEG and spike train recordings</article-title>. <source>BMC Neuroscience</source> <volume>10</volume>: <fpage>81</fpage>–<lpage>81</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-QuianQuiroga1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quian Quiroga</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>173</fpage>–<lpage>185</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ince1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ince</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Mazzoni</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bartels</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name> (<year>2012</year>) <article-title>A novel test to determine the significance of neural selectivity to single and multiple potentially correlated stimulus features</article-title>. <source>J Neurosci Methods</source> <volume>210</volume>: <fpage>49</fpage>–<lpage>65</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Chait1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chait</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name> (<year>2008</year>) <article-title>Auditory temporal edge detection in human auditory cortex</article-title>. <source>Brain Research</source> <volume>1213</volume>: <fpage>78</fpage>–<lpage>90</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Berens1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berens</surname><given-names>P</given-names></name> (<year>2009</year>) <article-title>CircStat: A MATLAB toolbox for circular statistics</article-title>. <source>Journal of Statistical Software</source> <volume>31</volume>: <fpage>1</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Elliott1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elliott</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2009</year>) <article-title>The modulation transfer function for speech intelligibility</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000302</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000302" xlink:type="simple">10.1371/journal.pcbi.1000302</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001752-Friederici1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2011</year>) <article-title>The brain basis of language processing: from structure to function</article-title>. <source>Physiol Rev</source> <volume>91</volume>: <fpage>1357</fpage>–<lpage>1392</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Hickok1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickok</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>The cortical organization of speech processing</article-title>. <source>Nat Rev Neurosci</source> <volume>8</volume>: <fpage>393</fpage>–<lpage>402</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-McGettigan1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGettigan</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Scott</surname><given-names>SK</given-names></name> (<year>2012</year>) <article-title>Cortical asymmetries in speech perception: what's wrong, what's right and what's left?</article-title> <source>Trends Cogn Sci</source> <volume>16</volume>: <fpage>269</fpage>–<lpage>276</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Morillon2"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morillon</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Arnal</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Bénar</surname><given-names>CG</given-names></name>, <name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name> (<year>2012</year>) <article-title>Asymmetric function of theta and gamma activity in syllable processing: an intra-cortical study</article-title>. <source>Front Psychol</source> <volume>3</volume>: <fpage>248</fpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Giraud2"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Kleinschmidt</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lund</surname><given-names>TE</given-names></name>, <name name-style="western"><surname>Frackowiak</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Laufs</surname><given-names>H</given-names></name> (<year>2007</year>) <article-title>Endogenous cortical rhythms determine cerebral specialization for speech perception and production</article-title>. <source>Neuron</source> <volume>56</volume>: <fpage>1127</fpage>–<lpage>1134</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Howard1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Howard</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension</article-title>. <source>J Neurophysiol</source> <volume>104</volume>: <fpage>2500</fpage>–<lpage>2511</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Gherri1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gherri</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Eimer</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Active listening impairs visual perception and selectivity: an ERP study of auditory dual-task costs on visual attention</article-title>. <source>J Cogn Neurosci</source> <volume>23</volume>: <fpage>832</fpage>–<lpage>844</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Sato1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hirabayashi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Tsubokura</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Kanai</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ashida</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Cerebral hemodynamics in newborn infants exposed to speech sounds: a whole-head optical topography study</article-title>. <source>Hum Brain Mapp</source> <volume>33</volume>: <fpage>2092</fpage>–<lpage>103</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Sohoglu1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sohoglu</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Peelle</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Carlyon</surname><given-names>RP</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name> (<year>2012</year>) <article-title>Predictive top-down integration of prior knowledge during speech perception</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>8443</fpage>–<lpage>8453</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Wild1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wild</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Yusuf</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Peelle</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name> (<year>2012</year>) <article-title>Effortful listening: the processing of degraded speech depends critically on attention</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>14010</fpage>–<lpage>14021</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ding2"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name> (<year>2013</year>) <article-title>Robust cortical encoding of slow temporal modulations of speech</article-title>. <source>Adv Exp Med Biol</source> <volume>787</volume>: <fpage>373</fpage>–<lpage>381</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ding3"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name> (<year>2012</year>) <article-title>Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title>. <source>J Neurophysiol</source> <volume>107</volume>: <fpage>78</fpage>–<lpage>89</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Kerlin1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerlin</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Shahin</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>LM</given-names></name> (<year>2010</year>) <article-title>Attentional gain control of ongoing cortical speech representations in a “cocktail party”</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>620</fpage>–<lpage>628</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lakatos3"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Karmos</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Mehta</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Ulbert</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name> (<year>2008</year>) <article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection</article-title>. <source>Science</source> <volume>320</volume>: <fpage>110</fpage>–<lpage>113</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Schroeder3"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Radman</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Scharfman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name> (<year>2010</year>) <article-title>Dynamics of Active Sensing and perceptual selection</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>: <fpage>172</fpage>–<lpage>176</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Peelle2"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelle</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name> (<year>2012</year>) <article-title>Neural oscillations carry speech rhythm through to comprehension</article-title>. <source>Front Psychol</source> <volume>3</volume>: <fpage>320</fpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Szymanski1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Szymanski</surname><given-names>FD</given-names></name>, <name name-style="western"><surname>Rabinowitz</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Magri</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schnupp</surname><given-names>JW</given-names></name> (<year>2011</year>) <article-title>The laminar and temporal structure of stimulus information in the phase of field potentials of auditory cortex</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>15787</fpage>–<lpage>15801</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lakatos4"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>, <name name-style="western"><surname>O'Connell</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Barczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mills</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Javitt</surname><given-names>DC</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>The leading sense: supramodal control of neurophysiological context by attention</article-title>. <source>Neuron</source> <volume>64</volume>: <fpage>419</fpage>–<lpage>430</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Shamir1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamir</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ghitza</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Epstein</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Kopell</surname><given-names>N</given-names></name> (<year>2009</year>) <article-title>Representation of time-varying stimuli by a network exhibiting oscillations on a faster time scale</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000370</fpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000370" xlink:type="simple">10.1371/journal.pcbi.1000370</ext-link></comment></mixed-citation>
</ref>
<ref id="pbio.1001752-Kayser1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Montemurro</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Spike-phase coding boosts and stabilizes information carried by spatial and temporal spike patterns</article-title>. <source>Neuron</source> <volume>61</volume>: <fpage>597</fpage>–<lpage>608</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Ng1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ng</surname><given-names>BSW</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name> (<year>2012</year>) <article-title>EEG Phase Patterns Reflect the Selectivity of Neural Firing</article-title>. <source>Cereb Cortex</source> <volume>23</volume>: <fpage>389</fpage>–<lpage>398</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Doelling1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doelling</surname><given-names>KB</given-names></name>, <name name-style="western"><surname>Arnal</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Ghitza</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name> (<year>2013</year>) <article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title>. <source>Neuroimage</source> In press.</mixed-citation>
</ref>
<ref id="pbio.1001752-Luczak1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bartho</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2013</year>) <article-title>Gating of sensory input by spontaneous cortical activity</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>1684</fpage>–<lpage>1695</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Hasson1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Galantucci</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Garrod</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Keysers</surname><given-names>C</given-names></name> (<year>2012</year>) <article-title>Brain-to-brain coupling: a mechanism for creating and sharing a social world</article-title>. <source>Trends Cogn Sci</source> <volume>16</volume>: <fpage>114</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Pickering1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pickering</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Garrod</surname><given-names>S</given-names></name> (<year>2007</year>) <article-title>Do people use language production to make predictions during comprehension?</article-title> <source>Trends Cogn Sci</source> <volume>11</volume>: <fpage>105</fpage>–<lpage>110</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Oostenveld1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oostenveld</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Maris</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Schoffelen</surname><given-names>JM</given-names></name> (<year>2011</year>) <article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Comput Intell Neurosci</source> <volume>2011</volume>: <fpage>156869</fpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Gross1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Baillet</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>GR</given-names></name>, <name name-style="western"><surname>Henson</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Hillebrand</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Good practice for conducting and reporting MEG research</article-title>. <source>Neuroimage</source> <volume>65</volume>: <fpage>349</fpage>–<lpage>363</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lerner1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lerner</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Honey</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Silbert</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name> (<year>2011</year>) <article-title>Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>2906</fpage>–<lpage>2915</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Brainard1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname><given-names>DH</given-names></name> (<year>1997</year>) <article-title>The psychophysics toolbox</article-title>. <source>Spatial Vision</source> <volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Smith1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>ZM</given-names></name>, <name name-style="western"><surname>Delgutte</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Oxenham</surname><given-names>AJ</given-names></name> (<year>2002</year>) <article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>. <source>Nature</source> <volume>416</volume>: <fpage>87</fpage>–<lpage>90</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Besl1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Besl</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>McKay</surname><given-names>ND</given-names></name> (<year>1992</year>) <article-title>A method for registration of 3-D shapes</article-title>. <source>IEEE T Pattern Anal</source> <fpage>239</fpage>–<lpage>256</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Nolte1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nolte</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title>. <source>Phys Med Biol</source> <volume>48</volume>: <fpage>3637</fpage>–<lpage>3652</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Percival1"><label>71</label>
<mixed-citation publication-type="book" xlink:type="simple">Percival DB, Walden AT (1993) Spectral analysis for physical applications. Cambridge: Cambridge University Press.</mixed-citation>
</ref>
<ref id="pbio.1001752-Gross2"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kujala</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hämäläinen</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Timmermann</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Schnitzler</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Dynamic imaging of coherent sources: Studying neural interactions in the human brain</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>98</volume>: <fpage>694</fpage>–<lpage>699</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Cover1"><label>73</label>
<mixed-citation publication-type="book" xlink:type="simple">Cover TM, Thomas JA (2006) Elements of information theory. Hoboken (New Jersey): Wiley-Blackwell.</mixed-citation>
</ref>
<ref id="pbio.1001752-Benjamini1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benjamini</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname><given-names>Y</given-names></name> (<year>1995</year>) <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>J Roy Stat Soc B Met</source> <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Lachaux1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lachaux</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Rodriguez</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Martinerie</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Varela</surname><given-names>FJ</given-names></name> (<year>1999</year>) <article-title>Measuring phase synchrony in brain signals</article-title>. <source>Hum Brain Mapp</source> <volume>8</volume>: <fpage>194</fpage>–<lpage>208</lpage>.</mixed-citation>
</ref>
<ref id="pbio.1001752-Tort1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tort</surname><given-names>ABL</given-names></name>, <name name-style="western"><surname>Komorowski</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Eichenbaum</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Kopell</surname><given-names>NJ</given-names></name> (<year>2010</year>) <article-title>Measuring phase-amplitude coupling between neuronal oscillations of different frequencies</article-title>. <source>J Neurophysiol</source> <volume>104</volume>: <fpage>1195</fpage>–<lpage>1210</lpage>.</mixed-citation>
</ref>
</ref-list><glossary><title>Abbreviations</title><def-list><def-item>
<term>FDR</term>
<def>
<p>false discovery rate</p>
</def>
</def-item><def-item>
<term>MEG</term>
<def>
<p>magnetoencephalography</p>
</def>
</def-item><def-item>
<term>MI</term>
<def>
<p>mutual information</p>
</def>
</def-item><def-item>
<term>PLV</term>
<def>
<p>phase-locking value</p>
</def>
</def-item></def-list></glossary></back>
</article>