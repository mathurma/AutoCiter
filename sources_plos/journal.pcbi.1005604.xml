<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005604</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01851</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Adjudicating between face-coding models with individual-face fMRI responses</article-title>
<alt-title alt-title-type="running-head">Adjudicating between face-coding models with fMRI</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0933-1239</contrib-id>
<name name-style="western">
<surname>Carlin</surname>
<given-names>Johan D.</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kriegeskorte</surname>
<given-names>Nikolaus</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>MRC Cognition and Brain Sciences Unit, University of Cambridge, Cambridge, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Daunizeau</surname>
<given-names>Jean</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Brain and Spine Institute (ICM), FRANCE</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"><list-item>
<p><bold>Conceptualization:</bold> JDC NK.</p></list-item> <list-item>
<p><bold>Data curation:</bold> JDC.</p></list-item> <list-item>
<p><bold>Formal analysis:</bold> JDC.</p></list-item> <list-item>
<p><bold>Funding acquisition:</bold> JDC NK.</p></list-item> <list-item>
<p><bold>Investigation:</bold> JDC.</p></list-item> <list-item>
<p><bold>Methodology:</bold> JDC NK.</p></list-item> <list-item>
<p><bold>Project administration:</bold> JDC.</p></list-item> <list-item>
<p><bold>Resources:</bold> JDC NK.</p></list-item> <list-item>
<p><bold>Software:</bold> JDC.</p></list-item> <list-item>
<p><bold>Supervision:</bold> JDC NK.</p></list-item> <list-item>
<p><bold>Validation:</bold> JDC.</p></list-item> <list-item>
<p><bold>Visualization:</bold> JDC.</p></list-item> <list-item>
<p><bold>Writing – original draft:</bold> JDC.</p></list-item> <list-item>
<p><bold>Writing – review &amp; editing:</bold> JDC NK.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">johan.carlin@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>26</day>
<month>7</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>7</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>7</issue>
<elocation-id>e1005604</elocation-id>
<history>
<date date-type="received">
<day>3</day>
<month>11</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Carlin, Kriegeskorte</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005604"/>
<abstract>
<p>The perceptual representation of individual faces is often explained with reference to a norm-based face space. In such spaces, individuals are encoded as vectors where identity is primarily conveyed by direction and distinctiveness by eccentricity. Here we measured human fMRI responses and psychophysical similarity judgments of individual face exemplars, which were generated as realistic 3D animations using a computer-graphics model. We developed and evaluated multiple neurobiologically plausible computational models, each of which predicts a representational distance matrix and a regional-mean activation profile for 24 face stimuli. In the fusiform face area, a face-space coding model with sigmoidal ramp tuning provided a better account of the data than one based on exemplar tuning. However, an image-processing model with weighted banks of Gabor filters performed similarly. Accounting for the data required the inclusion of a measurement-level population averaging mechanism that approximates how fMRI voxels locally average distinct neuronal tunings. Our study demonstrates the importance of comparing multiple models and of modeling the measurement process in computational neuroimaging.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Humans recognize conspecifics by their faces. Understanding how faces are recognized is an open computational problem with relevance to theories of perception, social cognition, and the engineering of computer vision systems. Here we measured brain activity with functional MRI while human participants viewed individual faces. We developed multiple computational models inspired by known response preferences of single neurons in the primate visual cortex. We then compared these neuronal models to patterns of brain activity corresponding to individual faces. The data were consistent with a model where neurons respond to directions in a high-dimensional space of faces. It also proved essential to model how functional MRI voxels locally average the responses of tens of thousands of neurons. The study highlights the challenges in adjudicating between alternative computational theories of visual information processing.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>261352</award-id>
<principal-award-recipient>Nikolaus Kriegeskorte</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000286</institution-id>
<institution>British Academy</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0933-1239</contrib-id>
<name name-style="western">
<surname>Carlin</surname>
<given-names>Johan D.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by the European Research Council (261352 awarded to NK), the UK Medical Research Council (MC_A060_5PR2 awarded to NK), and a British Academy Postdoctoral Fellowship (JDC). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-08-09</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The distance matrices we estimated for cortical and perceptual face spaces are available, along with software to re-generate all computational model fits (separate copies deposited on <ext-link ext-link-type="uri" xlink:href="https://osf.io/5g9rv" xlink:type="simple">https://osf.io/5g9rv</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.242666" xlink:type="simple">https://doi.org/10.5281/zenodo.242666</ext-link>). Raw MRI data is publicly available on openfMRI (<ext-link ext-link-type="uri" xlink:href="https://openfmri.org/dataset/ds000232/" xlink:type="simple">https://openfmri.org/dataset/ds000232/</ext-link>), and can also be obtained by request from the MRC Cognition and Brain Sciences Publication Data Repository.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Humans are expert at recognizing individual faces, but the mechanisms that support this ability are poorly understood. Multiple areas in human occipital and temporal cortex exhibit representations that distinguish individual faces, as indicated by successful decoding of face identity from functional magnetic resonance imaging (fMRI) response patterns [<xref ref-type="bibr" rid="pcbi.1005604.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref010">10</xref>]. Decoding can reveal the presence of face-identity information as well as invariances. However, the nature of these representations remains obscure because individual faces differ along many stimulus dimensions, each of which could plausibly support decoding. To understand the representational space, we need to formulate models of how individual faces might be encoded and test these models with responses to sufficiently large sets of face exemplars. Here we use representational similarity analysis (RSA) [<xref ref-type="bibr" rid="pcbi.1005604.ref011">11</xref>] to test face-coding models at the level of the representational distance matrices they predict. Comparing models to data in the common currency of the distance matrix enables us to pool the evidence over many voxels within a region, obviating the need to fit models separately to noisy individual fMRI voxels.</p>
<p>Many cognitive and neuroscientific models of face processing do not make quantitative predictions about the representation of particular faces [<xref ref-type="bibr" rid="pcbi.1005604.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref013">13</xref>]. However, such predictions can be obtained from models based on the notion that faces are encoded as vectors in a space [<xref ref-type="bibr" rid="pcbi.1005604.ref014">14</xref>]. Most face-space implementations apply principal components analysis (PCA) to face images or laser scans in order to obtain a space, where each component is a dimension and the average face for the training sample is located at the origin [<xref ref-type="bibr" rid="pcbi.1005604.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref016">16</xref>]. In such PCA face spaces, eccentricity is associated with judgments of distinctiveness, while vector direction is associated with perceived identity [<xref ref-type="bibr" rid="pcbi.1005604.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref019">19</xref>]. Initial evidence from macaque single-unit recordings and human fMRI suggests that brain responses to faces are strongly modulated by face-space eccentricity, with most studies finding increasing responses with distinctiveness [<xref ref-type="bibr" rid="pcbi.1005604.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref023">23</xref>]. However, there has been no attempt to develop a unified account for how a single underlying face-space representation can support both sensitivity to face-space direction at the level of multivariate response patterns and sensitivity to eccentricity at the level of regional-mean fMRI activations. Here we develop and evaluate several face-space coding models, which differ with respect to the proposed shape of the neuronal tuning functions across face space and with respect to the distribution of preferred face-space locations over the simulated neuronal population.</p>
<p>Face-space coding models define high-level representational spaces, which are assumed to arise through unspecified low-level featural processing. An alternative possibility is that some cortical face-space representations can be explained directly by low-level visual features, which typically covary with position in PCA-derived face spaces. To explore this possibility, we evaluated a Gabor-filter model, which receives stimulus images rather than face-space coordinates as input, and has previously been used to model response preferences of individual voxels in early visual cortex [<xref ref-type="bibr" rid="pcbi.1005604.ref024">24</xref>].</p>
<p>We found that cortical face responses measured with fMRI strongly reflect face-space eccentricity: a step along the radial axis in face space results in a much larger pattern change than an equal step along the tangential axis. These effects were consistent with either a sigmoidal-ramp-tuning or a Gabor-filter model. The performance of these winning models depended on the inclusion of a measurement-level population-averaging mechanism, which accounted for local averaging of neuronal tunings in fMRI voxel measurements.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Sampling face space with photorealistic but physically-controlled animations</title>
<p>In order to elicit strong percepts of the 3D shape of each individual face, we generated a set of photorealistic animations of face exemplars. Each 2s animation in the main experiment featured a face exemplar in left or right half profile, which rotated outward continuously (<xref ref-type="supplementary-material" rid="pcbi.1005604.s022">S2 Movie</xref>, <xref ref-type="sec" rid="sec017">Materials and methods</xref>). The animations were based on a PCA model of 3D face shape and texture [<xref ref-type="bibr" rid="pcbi.1005604.ref025">25</xref>]. Each frame of each animation was cropped with a feathered aperture and processed to equate low-level image properties across the stimulus set (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). We generated 12 faces from a slice through the PCA face space (<xref ref-type="fig" rid="pcbi.1005604.g001">Fig 1b</xref>). Euclidean distances between the Cartesian coordinates for each face were summarized in a distance matrix (<xref ref-type="fig" rid="pcbi.1005604.g001">Fig 1a</xref>), which served as the reference for comparisons against distances in the perceptual and cortical face spaces (<xref ref-type="fig" rid="pcbi.1005604.g001">Fig 1c–1h</xref>). We generated a physically distinct stimulus set with the same underlying similarity structure for each participant by randomizing the orientation of the slice through the high-dimensional PCA space (for examples, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s001">S1 Fig</xref>). This served to improve generalizability by ensuring that group-level effects were not strongly influenced by idiosyncrasies of face exemplars drawn from a particular PCA space slice. In formal terms, group-level inference therefore treats stimulus as a random effect [<xref ref-type="bibr" rid="pcbi.1005604.ref026">26</xref>].</p>
<fig id="pcbi.1005604.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005604.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Face spaces obtained from a reference PCA model, perceptual similarity judgments and fMRI response patterns in human visual cortex.</title>
<p>(<bold>a-b</bold>) We generated 12 faces in a polar grid arrangement on a 2D slice through the reference PCA space (3 eccentricity levels and 4 directions). The grid was centered on the stimulus space origin. Euclidean distances between the 12 faces are illustrated in a distance matrix (a) and in a 2D visualization of these distances (b, multidimensional scaling, metric-stress criterion). (<bold>c-d</bold>) Group-average perceptual face space (N = 10) estimated from psychophysical similarity judgments. The distance matrix depicts the percentage of trials on which each pair of faces was rated as relatively more dissimilar. (<bold>e-h</bold>) Group-average cortical face-space (N = 10) estimated from fMRI response patterns in human visual cortex. The distance matrices depict a cross-validated estimate of multivariate discriminability where 0 corresponds to chance-level performance (leave-one-run-out cross-validation, <xref ref-type="sec" rid="sec017">Materials and methods</xref>). Each cortical region was defined in individual participants using independent data. The distance matrices use separate color scales since we expect variable effect sizes across cortical regions. For data from other regions of interest, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s002">S2 Fig</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Cortical face spaces are warped relative to the reference PCA space</title>
<p>In order to sample human cortical and perceptual face representations, human participants (N = 10) participated in a perceptual judgment task followed by fMRI scans. The perceptual judgment task (<xref ref-type="supplementary-material" rid="pcbi.1005604.s021">S1 Movie</xref>, 2145 trials over 4 recording days) involved force-choice judgments of the relative similarity between pairs of faces, which were used to estimate a behavioral distance matrix (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). The same faces were then presented individually in a subsequent rapid event-related fMRI experiment (<xref ref-type="supplementary-material" rid="pcbi.1005604.s022">S2 Movie</xref>, 2496 trials over 4 recording days). Brain responses were analyzed separately in multiple independently localized visual regions of interest. We compared representational distance matrices estimated from these data sources to distances predicted according to different models using the Pearson correlation coefficient. These distance-matrix similarities were estimated in single participants and the resulting coefficients were Z-transformed and entered into a summary-statistic group analysis for random-effects inference generalizing across participants and stimuli (<xref ref-type="sec" rid="sec017">Materials and methods</xref>).</p>
<p>We observed a strong group-average correlation between distance matrices estimated from perceptual dissimilarity judgments and Euclidean distances in the reference PCA space (mean(r) = 0.83, mean(Z(r)) = 1.20, standard error = 0.05, p&lt;0.001, Figs <xref ref-type="fig" rid="pcbi.1005604.g001">1c</xref> and <xref ref-type="fig" rid="pcbi.1005604.g005">5a</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s012">S1 Table</xref>). Correlations between the reference PCA space and cortical face spaces were generally statistically significant, but smaller in magnitude (<xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5b and 5c</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s005">S5 Fig</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s012">S1 Table</xref>). Distances estimated from the fusiform face area were weakly, but highly significantly correlated with the reference PCA space (mean(r) = 0.17, mean(Z(r)) = 0.17, standard error = 0.04, p&lt;0.001, <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5c</xref>). Distances estimated from the early visual cortex were even less, though still significantly, correlated with the reference PCA space (mean(r) = 0.07, mean(Z(r)) = 0.07, standard error = 0.04, p = 0.044, <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5b</xref>). These smaller correlations in cortical compared to perceptual face spaces could not be attributed solely to lower functional contrast-to-noise ratios in fMRI data, because the effects generally did not approach the noise-ceiling estimate for the sample (shaded region in <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref>). The noise ceiling was based on the reproducibility of distance matrices between participants (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). Instead, these findings indicate that the reference PCA space could not capture all the explainable dissimilarity variance in cortical face spaces.</p>
</sec>
<sec id="sec005">
<title>Cortical face spaces over-represent eccentricity</title>
<p>We quantified the apparent warps in the cortical face spaces by constructing a multiple-regression RSA model, with separate distance-matrix predictors for eccentricity and direction, and for within and across face viewpoints (<xref ref-type="fig" rid="pcbi.1005604.g002">Fig 2a</xref>, <xref ref-type="sec" rid="sec017">Materials and methods</xref>). These predictors were scaled such that differences between the eccentricity and direction parameters could be interpreted as warping relative to a veridical encoding of distances in the reference PCA face space. We also observed strong viewpoint effects in multiple regions, including the early visual cortex (<xref ref-type="fig" rid="pcbi.1005604.g001">Fig 1e and 1f</xref>). Such effects were modeled by separate constant terms for distances within and across viewpoint.</p>
<fig id="pcbi.1005604.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005604.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Face-space warping reflects an over-representation of eccentricity over direction information.</title>
<p>(<bold>a</bold>) Squared distances in the reference PCA space were parameterized into predictors coding all 4 combinations of face-space metric (direction, eccentricity) and viewpoint (within, across). This multiple regression RSA model also included separate constant terms for each viewpoint, and was fitted separately to each face space using ordinary least squares (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). Importantly, the scaling of the eccentricity and direction predictors ensures that equal parameter estimates corresponds to a preserved reference PCA space. (<bold>b</bold>) Multiple regression fit to the perceptual face space with group-average parameter estimates, distance matrix, fitted distances and residuals. Gray lines reflect single participant parameter estimates (<bold>c-d</bold>) Multiple regression fit to the cortical face spaces, plotted as in b. See <xref ref-type="supplementary-material" rid="pcbi.1005604.s013">S2 Table</xref> for inferential statistics.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.g002" xlink:type="simple"/>
</fig>
<p>Eccentricity changes had a consistently greater effect on representational patterns than direction changes, suggesting that a step change along the radial axis resulted in a larger pattern change than an equivalent step along the tangential axis (<xref ref-type="fig" rid="pcbi.1005604.g002">Fig 2c and 2d</xref>). The overrepresentation of eccentricity relative to direction was observed in each participant, and in both cortical and perceptual face spaces, although the effect was considerably larger in cortical face spaces. A repeated-measures two-factor ANOVA (eccentricity versus direction, within versus across viewpoint) on the single-participant parameter estimates from the multiple-regression RSA model was consistent with these apparent differences, with a statistically significant main effect of eccentricity versus direction for perceptual and cortical face spaces (all p&lt;0.012, <xref ref-type="supplementary-material" rid="pcbi.1005604.s013">S2 Table</xref>). Thus, compared to the encoding in the reference PCA space, cortical face spaces over-represented the radial, distinctiveness-related axis compared to the tangential, identity-related axis.</p>
<p>Although these findings suggest a larger contribution of face-space eccentricity than direction in visual cortex, we also observed clear evidence for greater-than-chance discrimination performance among faces that differed only in face-space direction. Group-average cross-validated discriminant distances for faces that differed in direction but not eccentricity exceeded chance-level performance (p&lt;0.05) for typical and caricatured faces in both the early visual cortex and the fusiform face area (<xref ref-type="supplementary-material" rid="pcbi.1005604.s003">S3 Fig</xref>). Sub-caricatured faces were less consistently discriminable. Indeed, direction discrimination increased with eccentricity both within (mean = 0.013, standard error = 0.007, p = 0.036) and across (mean = 0.012, standard error = 0.005, p = 0.016) viewpoint in the fusiform face area (linear effect of sub&gt;typical&gt;caricature), suggesting that direction discrimination increased with face-space eccentricity in a dose-dependent manner (<xref ref-type="supplementary-material" rid="pcbi.1005604.s014">S3 Table</xref>). By contrast, within viewpoint discrimination performance in the early visual cortex scaled with eccentricity (mean = 0.039, standard error = 0.008, p&lt;0.001), but distances that spanned a viewpoint change did not vary with eccentricity (mean = 0.009, standard error = 0.019, p = 0.297). This is consistent with a view-dependent representation in early visual cortex. Thus, cortical regions discriminate identity-related direction information even in the absence of a difference in distinctiveness-related eccentricity information, suggesting that cortical face representations cannot be reduced to a one-dimensional code based on distinctiveness alone. In summary, cortical coding of face-space position is systematically warped relative to the reference PCA space, with a substantial overrepresentation of eccentricity and a smaller, but reliable contribution of face-space direction.</p>
</sec>
<sec id="sec006">
<title>Regional-mean fMRI activation increases with face-space eccentricity, but removing such effects does not substantially alter cortical face-space warping</title>
<p>Cortical face-space warping could not be explained by regional-mean activation preferences for caricatures. We performed a regional-mean analysis of responses in each cortical area, which confirmed previous reports that fMRI responses increase with distinctiveness across much of visual cortex (<xref ref-type="fig" rid="pcbi.1005604.g006">Fig 6</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s006">S6 Fig</xref>) [<xref ref-type="bibr" rid="pcbi.1005604.ref023">23</xref>]. In order to test the influence of such regional-mean activation effects on representational distances, we adapted our discriminant distance metric to remove additive and multiplicative overall activation effects (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). Distance matrices estimated using this alternative method were highly similar to ones estimated without removal of overall activation effects (all r = 0.9 or greater for the Pearson correlation between group-average distance matrices with and without mean removal, <xref ref-type="supplementary-material" rid="pcbi.1005604.s004">S4 Fig</xref>). Thus, although eccentricity affected the overall activation in all visual areas, the warping of the cortical face spaces could not be attributed to overall activation effects alone.</p>
</sec>
<sec id="sec007">
<title>Accounting for cortical and perceptual face spaces with PCA face-space-tuning models and image-computable models</title>
<p>We developed multiple computational models, each of which predicts a representational distance matrix and a regional-mean activation profile. These models can be divided into three classes: the sigmoidal-ramp tuning and exemplar tuning models receive face-space coordinates as input, while the Gabor-filter model receives gray-scale pixel intensities from the stimulus images as input. We evaluated each of these three model classes with and without a measurement-level population-averaging mechanism, which approximates how fMRI voxels locally average underlying neural activity.</p>
<p>The sigmoidal-ramp-tuning model proposes that the representational space is covered with randomly oriented ramps, each of which exhibits a monotonically increasing response along its preferred direction in face space (<xref ref-type="fig" rid="pcbi.1005604.g003">Fig 3a</xref>). This model is inspired by known preferences for extreme feature values in single units recorded from area V4 and from face-selective patches in the macaque visual cortex [<xref ref-type="bibr" rid="pcbi.1005604.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref027">27</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref028">28</xref>]. We modeled the response along each model neuron’s preferred direction using a sigmoidal function with two free parameters, which control the horizontal offset and the saturation of the response function (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). A third parameter controlled the strength of measurement-level population averaging by translating each individual model unit’s response toward the population-mean response. The way this accounts for local averaging by voxels is illustrated for the fusiform face area in <xref ref-type="fig" rid="pcbi.1005604.g003">Fig 3a and 3b</xref>. It can be seen that measurement-level population averaging introduces a substantial U-shape in the individual response functions, with only a minor deflection in favor of a preferred face-space direction. At the level of Euclidean distances between population response vectors evoked by each face, this leads to exaggerated distances for radial relative to tangential face differences (<xref ref-type="fig" rid="pcbi.1005604.g003">Fig 3c</xref>). In summary, measurement-level population averaging provides a simple means to interpolate between two extreme cases: A value of 0 corresponds to the case where the model’s response is perfectly preserved in the fMRI voxels, whereas a value of 1 corresponds to the case where the model’s response to a given stimulus is reduced to the arithmetic mean over the model units.</p>
<fig id="pcbi.1005604.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005604.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Computational models for face-space coding based on sigmoidal ramp or exemplar tuning coupled with measurement-level population averaging.</title>
<p>The visualization uses the model parameters that were optimal for predicting the face space in the fusiform face area. (<bold>a</bold>) Response function from the sigmoidal ramp model’s internal representation (left panel) and its measurement (right panel) following translation toward the population-average response function (middle panel). (<bold>b</bold>) Two-dimensional generalization of the sigmoidal ramp tuning function to encode a direction in the PCA-space slice. An example unit is plotted in the left and right panels with the population-average response in the middle panel. (<bold>c</bold>) The model’s representational dissimilarity structure was estimated as the Euclidean distance between the population response vectors elicited by a coordinate in the PCA-space slice (white circle) and every other coordinate on the face space slice. Two example coordinates are plotted in the left and right panels. It can be seen that dissimilarity increases more rapidly with radial (eccentricity) than with tangential (direction) face-space distance. (<bold>d</bold>) Two-dimensional response function for an example unit from the Gaussian exemplar model (white marker). A sub-set of other units is overlaid in black markers to illustrate the width of the Gaussian distribution of tuning centers. (<bold>e</bold>) Two-dimensional response function for an example unit from the negative Gaussian exemplar model, plotted as in panel d.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.g003" xlink:type="simple"/>
</fig>
<p>In the exemplar model, each unit prefers a location in face space, rather than a direction, and its tuning is described by a Gaussian centered on the preferred location. The representational space is covered by a population of units whose preferred locations are sampled from a Gaussian centered on the norm face (<xref ref-type="fig" rid="pcbi.1005604.g003">Fig 3d</xref>, <xref ref-type="sec" rid="sec017">Materials and methods</xref>). We fitted the Gaussian exemplar-tuning model similarly to the sigmoidal ramp-tuning model, using two parameters that controlled the width of the Gaussian tuning function and the width of the Gaussian distribution from which preferred faces were sampled. We also evaluated a variant of the exemplar model where the distribution of preferred faces followed an inverted-Gaussian distribution (<xref ref-type="fig" rid="pcbi.1005604.g003">Fig 3e</xref>). Population averaging was modeled in the same way as for the sigmoidal-ramp-tuning model using a third parameter.</p>
<p>The Gabor-filter model differs from the previous model classes in that it receives gray-scale image intensities as input, rather than PCA space position (<xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4a</xref>). Such models have previously been used to account for response preferences of individual voxels in early visual cortex [<xref ref-type="bibr" rid="pcbi.1005604.ref024">24</xref>]. The model comprises Gabor filters varying in orientation, spatial frequency and phase, and spatial position. The filters are organized into banks, each corresponding to a spatial frequency and comprising a different number of spatial positions (coarser for lower spatial frequencies; <xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4b</xref>, <xref ref-type="sec" rid="sec017">Materials and methods</xref>). We assumed that all orientations and spatial positions are equally represented. For the spatial frequencies, however, we let the data determine the weighting. We fitted a weighted representational model with one weight for each spatial-frequency bank (5 free parameters, <xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4c</xref>). Local averaging in fMRI voxels was modeled using two stages of measurement-level population averaging: First, filters with tuning centers on either side of the vertical meridian were translated separately toward their respective hemifield-specific population averages. Second, a global-pool averaging was performed similarly to the other models. The contribution of these two population-average signals to the measured responses was modeled by 2 additional parameters (<xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4d</xref>). The additional hemifield-specific averaging stage was necessary to account for strong view-specific effects in early visual cortex, but did not materially contribute to the fit in ventral temporal regions.</p>
<fig id="pcbi.1005604.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005604.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Schematic illustration of processing stages for the Gabor filter model.</title>
<p>(<bold>a</bold>) The model receives gray-scale image intensities for each face exemplar. Three examples input images are illustrated in rows. (<bold>b</bold>) Image intensities are passed through banks of Gabor filters. The banks vary in spatial scale (filter standard deviation and grid spacing). The rows illustrate example filters from each bank. (<bold>c</bold>) The output of each filter bank is weighted and measurement effects are modeled using subsequent hemifield-specific and global-pool population averaging stages (<bold>d</bold>). The final output of the model is a Euclidean distance matrix estimated from the activation vectors in response to each face.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Multiple models can explain cortical and perceptual face spaces</title>
<p>We fitted each of the computational models so as to best predict the representational distance matrices from cortical regions and perceptual judgments using a grid search over all free parameters. We used a leave-one-participant-out cross-validation approach, in which model performance was evaluated on participants and face identities not used in fitting the parameters (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). Model performance was summarized as the Fisher-Z-transformed Pearson correlation coefficient between the model distances and the data distances. We performed statistical inference on the average Fisher-Z-transformed correlations over all train-test splits of the data using T tests. Our cross-validation scheme tests for generalization across participants and face identities and ensures that models that differ in complexity (number of free parameters) can be compared. In order to investigate the effect of local averaging in fMRI voxels on representational similarity, we fitted two variants of each model: the full model and a variant that excluded measurement-level population averaging.</p>
<p>We found that all evaluated models explained almost all the explainable variance for the perceptual face space (<xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5a</xref>), with only negligible differences in cross-validated generalization performance (for all pairwise model comparisons, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s015">S4 Table</xref>). The inclusion of measurement-level population averaging had little effect on performance. This is expected because perceptual judgments, unlike fMRI voxels, are not affected by local averaging across representational units. Thus, the behavioral data was ambiguous with regard to the proposed models, which motivates model selection by comparison to the functional imaging data.</p>
<fig id="pcbi.1005604.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005604.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Generalization performance (leave-one-participant-out cross-validation) for fitted computational PCA-space (green bars) and image-based (orange bars) models, as well as mean performance for two fixed predictors (final bars in each panel).</title>
<p>The bars provide group-averaged Pearson correlation coefficients, while individual participants are overlaid in gray markers. Filled bars indicate full model fits, while outlined bars indicate model fits excluding measurement-level population averaging. Statistically significant differences between model variants with and without population averaging are illustrated with black connection lines (p&lt;0.05, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s015">S4 Table</xref> for all pairwise comparisons). An estimate of the maximal performance expected given signal-to-noise levels in the sample is illustrated as a shaded noise ceiling (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). Performance is plotted in separate panels for perceptual judgments (<bold>a</bold>), early visual cortex (<bold>b</bold>), and the fusiform face area (<bold>c</bold>). For other regions of interest, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s005">S5 Fig</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.g005" xlink:type="simple"/>
</fig>
<p>Unlike the perceptual judgments data, the cortical face spaces exhibited substantial differences between the model fits, with a robust advantage for measurement-level population averaging in most cases. In the following, we focus on generalization performance for fits to the early visual cortex and the fusiform face area (for fits to other regions, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s005">S5 Fig</xref>).</p>
<p>The early visual cortex was best explained by the Gabor-filter model, which beat the alternative computational models (p&lt;0.001 for all pairwise model comparisons, <xref ref-type="supplementary-material" rid="pcbi.1005604.s015">S4 Table</xref>) and came close to explaining all explainable variance given noise levels in the data. Generalization performance for this model was slightly, but significantly better (p = 0.009, <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5b</xref>) when population averaging was enabled. As a control, we also tested raw pixel intensities as the representational units. We found no significant difference in performance between the 0-parameter pixel-intensity model and the fitted Gabor-filter model with population averaging (p = 0.380).</p>
<p>The fusiform face area was also well explained by the Gabor-filter model, but in this region we observed similar performance for the sigmoidal-ramp-tuning model. Both models, with population averaging, reached the lower bound of the noise ceiling (<xref ref-type="sec" rid="sec017">Materials and methods</xref>; <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5c</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s015">S4 Table</xref>), suggesting that these models were able to explain the variance in the dataset that was consistent between participants. We also observed comparable generalization performance for the Gaussian exemplar model. Measurement-level population averaging improved generalization performance in the fusiform face area for both the Gabor-filter (p&lt;0.001) and sigmoidal-ramp-tuning models (p = 0.032), but did not improve either of the exemplar models (p = 0.263 for Gaussian exemplar, p = 0.096 for negative Gaussian exemplar). In summary, the representation in the fusiform face area could be explained by multiple models, and in most cases measurement-level population averaging improved the quality of the fit.</p>
</sec>
<sec id="sec009">
<title>Regional-mean activation profiles are consistent with sigmoidal-ramp and Gabor-filter, but not Gaussian-exemplar models</title>
<p>Multiple computational models provided qualitatively similar fits to our cortical data at the distance-matrix level. However, we might still be able to adjudicate between them at the level of regional-mean activation profiles. To this end, we obtained activation-profile predictions from each model by averaging over all model units. We then correlated the predicted population-mean activation profile with the regional-mean fMRI activation profile for each participant and performed an activation profile similarity analysis analogously to the distance matrix similarity analysis above. We found that the activation profiles from the computational models were predictive of cortical activation profiles, even though these models were fitted to distance matrices rather than to regional-mean fMRI responses (<xref ref-type="fig" rid="pcbi.1005604.g006">Fig 6</xref>, for other regions see <xref ref-type="supplementary-material" rid="pcbi.1005604.s006">S6 Fig</xref>). In particular, the sigmoidal-ramp and Gabor-filter models both predicted increasing population-mean responses with face-space eccentricity, while the Gaussian-exemplar model predicted decreasing responses with eccentricity (<xref ref-type="supplementary-material" rid="pcbi.1005604.s016">S5</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005604.s017">S6</xref> Tables for pairwise comparisons). This constitutes evidence against the Gaussian-exemplar model, under the assumption that neuronal activity is positively associated with regional-mean fMRI response in visual regions [<xref ref-type="bibr" rid="pcbi.1005604.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref033">33</xref>]. The preference for faces closer to the PCA-space origin (sub-caricatures) in the Gaussian-exemplar model arises as a necessary consequence of the Gaussian distribution of preferred faces, which is centered on the average face. We also tested a Gaussian exemplar-tuning model with an inverted Gaussian distribution of preferred faces. In this model, more units prefer faces far from the norm (caricatures) than faces close to the norm (sub-caricatures). However, the inverted-Gaussian exemplar model’s generalization performance was considerably worse than the standard-Gaussian exemplar model’s (<xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref>).</p>
<fig id="pcbi.1005604.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005604.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Activation-profile similarity analysis for computational PCA-space and image processing models.</title>
<p>Bars (panels <bold>a</bold>, <bold>e</bold>) are plotted similarly as in <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref>, with group-average effects in bars and single-participant estimates in markers. Markers in the line graphs (panels <bold>b-d, f-h</bold>) indicate group-average fMRI responses evoked by each face exemplar, while lines indicate the computational model predictions (with measurement-level population averaging in filled lines, without in outlined lines). Data and model predictions have been Z-scored to better illustrate the correlation-based fit metric we use here. The top panels illustrate effects in the early visual cortex, while the bottom panels illustrate effects in the fusiform face area. For other regions of interest, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s006">S6 Fig</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.g006" xlink:type="simple"/>
</fig>
<p>In summary, exemplar models accurately predicted representational distances for cortical face spaces when the preferred-face distribution was Gaussian, but such distributions led to inaccurate predictions of regional-mean fMRI activation profiles. Thus, analysis of regional-mean fMRI responses enabled us to adjudicate between models that made similar predictions at the distance-matrix level, and specifically indicated that the Gaussian exemplar model is unlikely to be the correct model for cortical face-space representation, despite a good fit at the distance-matrix level.</p>
</sec>
<sec id="sec010">
<title>Measurement-level population averaging is necessary to account for symmetric-view tolerance and over-representation of eccentricity</title>
<p>We found that models that included measurement-level population averaging generally outperformed models that did not. This advantage appeared to originate in how models with population averaging captured two effects in the cortical face spaces: symmetric view-tolerance and over-representation of face-space eccentricity relative to direction.</p>
<p>First, population averaging enabled the image-based Gabor-filter model to exhibit symmetric-view-tolerant responses to the face exemplars. Two mirror-symmetric views will drive a mirror symmetric set of Gabor features. Thus, while the pattern of activity differs, the population-mean activity is similar. Measurement-level pooling over filters centered on distinct visual field locations therefore renders symmetric views more similar in the representation. For instance, the face space in the fusiform face area exhibited little sensitivity to viewpoint, and the Gabor-filter model fit to this region was greatly improved by the inclusion of measurement-level population averaging (<xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5c</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s002">S2 Fig</xref>). Indeed, with population averaging, generalization performance for the image-based Gabor-filter model was similar to the sigmoidal-ramp-tuning and exemplar models, for which view tolerance is assumed at the input stage. General view tolerance, beyond the symmetric views we used here, is computationally more challenging. However, for our stimulus set, it was not necessary to posit any intrinsic view-invariant computations in the fusiform face area to explain how its face spaces come to exhibit symmetric view tolerance.</p>
<p>Second, measurement-level population averaging increased the degree to which both the sigmoidal-ramp and the Gabor-filter model over-represented face-space eccentricity relative to direction, which improved the fit for multiple cortical regions. To isolate this smaller effect from the larger symmetric-view-tolerance effect, we collapsed viewpoint in the first-level single-participant fMRI linear model and re-estimated the cortical face spaces and all model fits for the resulting simplified 12-condition design matrix, where each predictor coded appearances of a given face identity regardless of its viewpoint (<xref ref-type="supplementary-material" rid="pcbi.1005604.s007">S7 Fig</xref>). Even after collapsing across viewpoints at the first level in this way, measurement-level population averaging still improved the generalization performance of the sigmoidal-ramp and the Gabor-filter model in nearly all cases (p&lt;0.05, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s018">S7 Table</xref> for descriptive statistics and <xref ref-type="supplementary-material" rid="pcbi.1005604.s019">S8 Table</xref> for all pairwise comparisons), including the early visual cortex (p&lt;0.001 for sigmoidal ramp tuning, p = 0.004 for Gabor filter) and the fusiform face area (p = 0.001 for sigmoidal ramp tuning, p = 0.007 for Gabor filter). Thus, the advantage for measurement-level population averaging could not be accounted for by the fact that it helps explain symmetric view tolerance. In sum, the addition of population averaging to the model improved model generalization performance, and this advantage appeared to originate in accounts for two distinct observed phenomena.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>This study investigated human face processing by measuring how a face space of individual exemplars was encoded in visual cortical responses measured with fMRI and in perceptual judgments. Relative to a reference PCA model of the 3D shape and texture of faces, cortical face spaces from all targeted regions systematically over-represented eccentricity relative to direction (i.e., the radial relative to the tangential axis). Cortical regions varied in their sensitivity to face viewpoint. We fitted multiple computational models to the data. Considered collectively, the cortical face spaces in the fusiform face area were most consistent with a PCA-space-based sigmoidal-ramp-tuning model and an image-based Gabor-filter model, and less consistent with models based on exemplar coding. As expected, effects in the early visual cortex were consistent primarily with the Gabor-filter model. In all cases, the winning models’ performance depended on the inclusion of a measurement-level population-averaging mechanism, which approximates how individual model units are locally averaged in functional imaging measurements.</p>
<sec id="sec012">
<title>Functional MRI responses in the fusiform face area are best explained by sigmoidal-ramp-tuning and Gabor-filter models</title>
<p>Out of the models we considered, the best accounts for the fusiform face area were a PCA-space-based sigmoidal-ramp-tuning model and an image-based Gabor-filter model. Exemplar-coding models exhibited relatively lower generalization performance, or made inaccurate predictions for regional-mean fMRI activation profiles. Importantly, the advantage for both the sigmoidal-ramp and Gabor-filter model depended on the measurement-level population averaging mechanism. The key contribution of our modeling effort is to narrow the set of plausible representational models for the fusiform face area to two models that can explain both representational distances and the regional-mean activation profile.</p>
<p>It may appear surprising that a PCA-space coding model based on sigmoidal-ramp tuning and an image-based model based on Gabor filters should perform so similarly when fitted to face spaces in the fusiform face area. However, the sigmoidal-ramp-tuning model captures continuous variation in face shape and texture, which covaries with low-level image similarity. For instance, local curvature likely increases with face space eccentricity and is encoded in a ramp-like manner at intermediate stages of visual processing in macaque V4 [<xref ref-type="bibr" rid="pcbi.1005604.ref027">27</xref>]. Conversely, the Gabor-filter model likely possesses sensitivity to face-space direction because the contrast of local orientation content varies with major face features such as eyebrow or lip thickness. Similarly, the Gabor-filter model’s ability to account for regional-mean activation profiles likely arises because local contrast increases with face-space eccentricity, which results in an overall greater activation over the filter banks. There are multiple ways to parameterize face space, not all of which require domain-specific face features. This might also clarify why scene-selective areas such as the parahippocampal place area exhibited somewhat similar representational spaces as face-selective regions in the current study. Such widely-distributed face-exemplar effects are consistent with previous decoding studies [<xref ref-type="bibr" rid="pcbi.1005604.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref004">4</xref>]. The Gabor-filter model provides one simple account for how such widely distributed face-exemplar effects can arise. It is likely that the face-space effects we report are driven at least in part by a general mechanism for object individuation in visual cortex rather than the engagement of specialized processing for face recognition.</p>
</sec>
<sec id="sec013">
<title>Symmetric-view tolerance and over-representation of eccentricity in representational distances can be modeled as an fMRI measurement effect</title>
<p>The models we evaluate here raise the provocative possibility that in some cases, fMRI effects that might conventionally be attributed to high-level featural coding could instead arise from the neuroimaging measurement process. Such an explanation appears possible for two effects in our data. First, even though the Gabor-filter model is a single-layer network with limited representational flexibility, this model nevertheless exhibited near-complete tolerance to mirror-symmetric viewpoint changes, when coupled with measurement-level population averaging. Second, both this model and the sigmoidal-ramp-tuning model showed greater over-representation of eccentricity when measurement-level population averaging was enabled, suggesting that this over-representation in the fMRI data might also plausibly arise through local averaging in voxels.</p>
<p>Previous studies have tended to interpret view-tolerant fMRI effects in terms of cortical processing to support invariant object recognition [<xref ref-type="bibr" rid="pcbi.1005604.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref035">35</xref>]. The Gabor-filter model suggests a mechanism by which functional imaging measures can exaggerate apparent view-tolerance through spatial pooling over neuronal responses. This result does not contradict previous reports of view-tolerant coding for faces in neuronal population codes measured with single-unit recording [<xref ref-type="bibr" rid="pcbi.1005604.ref036">36</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref039">39</xref>], but rather demonstrates that the type of tolerance to symmetric viewpoint changes that we observed in the current study can be explained without resorting to such intrinsic view-tolerant mechanisms (see also Ramirez et al. [<xref ref-type="bibr" rid="pcbi.1005604.ref040">40</xref>]). Such findings may go some way toward reconciling apparent discrepancies between single-unit and functional imaging data. For instance, tolerance to symmetrical viewpoints is widespread in human visual cortex when measured with fMRI [<xref ref-type="bibr" rid="pcbi.1005604.ref034">34</xref>], but appears specific to a subset of regions in the macaque face-patch system when measured with single-unit recordings [<xref ref-type="bibr" rid="pcbi.1005604.ref036">36</xref>]. These results are only contradictory if the measurement process is not considered. In summary, we demonstrate that measurement effects can produce apparent view tolerance in fMRI data. This finding does not suggest that fMRI cannot detect view-tolerant coding (see also [<xref ref-type="bibr" rid="pcbi.1005604.ref041">41</xref>]). For example, population averaging may not account for all cases of non-symmetric view tolerance. However, our results do suggest that modeling of the measurement process is important to correctly infer the presence of such mechanisms from neuroimaging measurements.</p>
</sec>
<sec id="sec014">
<title>Greater regional-mean activation for distinctive faces can arise from local averaging of neuronal responses</title>
<p>The winning models in this study exemplify how sensitivity to face-space eccentricity at the regional-mean activation level can arise as an artifact of averaging, with no individual neuron encoding distinctiveness or an associated psychological construct. Previous functional imaging studies often interpreted response modulations with face eccentricity as evidence for coding of distinctiveness or related social perception attributes [<xref ref-type="bibr" rid="pcbi.1005604.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref042">42</xref>]. However, both the PCA-space-based sigmoidal-ramp-tuning model and the image-based Gabor-filter model exhibited increasing population-average responses with eccentricity, even though neither model encodes eccentricity at the level of its units. Although one could, of course, construct a competing model that explicitly codes eccentricity, the models used here are more consistent with single-unit recording studies, where cells generally are tuned to particular features, with a preference for extreme values, rather than responding to eccentricity regardless of direction [<xref ref-type="bibr" rid="pcbi.1005604.ref020">20</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref027">27</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref028">28</xref>]. Here we demonstrate that when the local averaging of such biologically plausible neuronal tunings is modeled, eccentricity sensitivity emerges without specialized encoding of this particular variable. Related effects have been reported in attention research, where response-gain and contrast-modulation effects at the single-neuron level may sum to similar additive-offset effects at the fMRI-response level [<xref ref-type="bibr" rid="pcbi.1005604.ref043">43</xref>]. In summary, direct interpretation of regional-mean fMRI activations in terms of neuronal tuning can be misleading when the underlying neuronal populations are heterogeneous.</p>
</sec>
<sec id="sec015">
<title>Modeling of measurement-level population averaging is important for computational studies of cortical representation</title>
<p>A simple model of measurement-level population averaging was sufficient here to substantially improve the generalization performance of multiple computational models for multiple cortical regions. The precise way that fMRI voxels sample neuronal activity patterns remains a topic of debate [<xref ref-type="bibr" rid="pcbi.1005604.ref030">30</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref031">31</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref033">33</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref044">44</xref>]. However, under the simple assumption that voxels sample random subsets of neurons by non-negatively weighted averaging, the effect on the measured fMRI distance matrix will be a uniform stretching along the all-one vector (representing the neuronal population average). To appreciate this point, consider the case of measurement channels that are unlike fMRI voxels in that they sample with random positive and negative weights. Under such conditions, we expect neuronal distances to be approximately preserved in the measurement channels according to the Johnson-Lindenstrauss lemma (for further discussion of this point, see [<xref ref-type="bibr" rid="pcbi.1005604.ref045">45</xref>]). Intuitively, the measurement channels re-describe the space with randomly oriented axes (without any directional bias). However, fMRI voxels are better understood as taking local non-negative weighted averages of neuronal activity, since the association between neural responses and fMRI response is generally thought to be positive. In such representational spaces the axes have orientations that are biased to fall along the all-1 vector. In practice, this measurement model assumes that the fMRI distance matrix for a given region of interest will over-represent distances to the extent that those distances modulate the neuronal population-average response.</p>
<p>Here we approximated measurement effects for models with nonlinear parameters by mixing the population average into the predicted representational feature space. Despite the simplicity of this method, our noise-ceiling estimates indicate that the winning models captured nearly all the explainable variance in the current dataset. For model representations without nonlinear parameters, this measurement model can be implemented more easily by linearly combining the model’s original distance matrix and the distance matrix obtained for the population average dimension of the space (using squared Euclidean distances estimates, see also <xref ref-type="bibr" rid="pcbi.1005604.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1005604.ref048">48</xref>). Thus, the measurement-level population averaging mechanism we propose here is widely and easily applicable to any case where a computational model is compared to neuroimaging data at the distance-matrix level.</p>
<p>The distance-matrix effects of local pooling of neuronal responses in fMRI voxels is correctly accounted for by our measurement model under the assumption that neurons are randomly intermixed in cortex (i.e. voxels sample random subsets of neurons). This simplifying assumption is problematic for early visual areas, where there is a well-established retinotopic organization with a strong contralateral response preference. For the retinotopic Gabor filter model, we therefore added a hemifield-specific pooling stage, which helped account for strong view-sensitivity in occipital regions of interest. Accounting for measurement effects in the presence of topographic organization is likely to prove more challenging for naturalistic stimulus sets. One solution is to account for local averaging in fMRI voxels by local averaging of the model’s internal representational map [<xref ref-type="bibr" rid="pcbi.1005604.ref045">45</xref>]. This local-pooling approach can be thought of as providing a further constraint on the comparison between model and data, because smoothing the model representation is only expected to improve the fit if the model response topography resembles the cortical topography. This may provide a means of adjudicating between topographically organized models, even when the models predict similar distance matrices in the absence of measurement-effect modeling. In summary, the global population average is a special dimension of the representational space, which is overrepresented in voxels that pool random subsets of neurons. This effect accounts for much variance in the representational distances in the current study, and is easy to model. Modeling the overrepresentation of the global average, as we did here, is suitable for models that do not predict a spatial organization (e.g. PCA-space coding models). For models that do predict a spatial organization, it may be more appropriate to simulate fMRI voxels by local averaging of the model’s representational map [<xref ref-type="bibr" rid="pcbi.1005604.ref045">45</xref>].</p>
</sec>
<sec id="sec016">
<title>Model comparison is essential for computational neuroimaging</title>
<p>This study demonstrates the importance of considering multiple alternative models to guide progress in computational neuroimaging. In particular, the finding that practically every model we evaluated exhibited significantly greater-than-zero generalization performance strongly suggests how studies that only evaluate a limited set of candidate models can arrive at misleading conclusions (see e.g. [<xref ref-type="bibr" rid="pcbi.1005604.ref049">49</xref>]).</p>
<p>Representational similarity analysis has two key advantages for model comparison relative to alternative approaches. First, competing model predictions can be easily visualized at the level of the best-fitting distance matrix for a given cortical region (e.g., <xref ref-type="supplementary-material" rid="pcbi.1005604.s002">S2 Fig</xref>). By contrast, models that are fitted to individual voxels are harder to visualize because the number of voxels per region typically exceeds what can be practically plotted. Furthermore, individual time-points in a rapid event-related fMRI experiment cannot easily be labeled according to experimental stimuli or conditions, which complicates interpretation of fitted time-courses. Second, RSA makes it possible to compare model fits and estimated parameters across data modalities, for instance, between fMRI responses and psychophysical similarity judgments. Such comparisons are challenging when the data is modeled at a lower level, because modality-specific parameters must be added to each model (e.g., parameters controlling the hemodynamic response function for fMRI, decision-threshold parameters for behavioral judgments). The presence of these non-shared parameters makes it difficult to attribute any apparent modality differences to the data rather than to the model specification. In summary, RSA is a particularly attractive analysis approach for studies that emphasize model comparison.</p>
<p>Although the central goal of model comparison is to select the best account of the data, the finding that some models are not dissociable under the current experimental context also has important implications for the design of future studies. Here we demonstrated that Gaussian-distributed exemplar-coding models are less likely to account for human face coding, while accounts based on sigmoidal ramp tuning and Gabor filter outputs perform very similarly. This suggests the need to design stimulus sets that generate distinct predictions from these winning models. For example, presenting face stimuli on naturalistic textured backgrounds may be sufficient to adjudicate between the two models, because the Gabor-filter model lacks a mechanism for figure-ground separation. In conclusion, our study exemplifies the need to test and compare multiple models and suggests routes by which the sigmoidal-ramp-tuning model of face-space coding could be further evaluated.</p>
</sec>
</sec>
<sec id="sec017" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec018">
<title>Ethics statement</title>
<p>All procedures were performed under a protocol approved by the Cambridge Psychology Research Ethics Committee (CPREC). Human participants provided written informed consent at the beginning of each data recording day.</p>
</sec>
<sec id="sec019">
<title>Participants</title>
<p>10 healthy human participants participated in a similarity judgment task and fMRI scans. The psychophysical task comprised 4 separate days of data collection which were completed prior to 4 separate days of fMRI scans. Participants were recruited from the local area (Cambridge, UK) and were naïve with regard to the purposes of the study. Five additional participants participated in data collection up to the first MRI data recording day, but were not invited to complete the study due to difficulties with vigilance, fixation stability, claustrophobia and/or head movements inside the scanner. The analyses reported here include all complete datasets that were collected for the study.</p>
</sec>
<sec id="sec020">
<title>Sampling the reference PCA face space</title>
<p>We generated faces using a norm-based model of 3D face shape and texture, which has been described in detail previously [<xref ref-type="bibr" rid="pcbi.1005604.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref025">25</xref>]. Briefly, the model comprises two PCA solutions (each trained on 200 faces), one based on 3D shape estimated from laser scans and another based on texture estimated from digital photographs. The components of each PCA solution are considered dimensions in a space that describes natural variation in facial appearance. All stimulus generation was performed using the PCA solution offered by previous investigators, and no further fitting was performed for this study. We yoked the shape and texture solutions in all subsequent analyses since we did not have distinct hypotheses for these.</p>
<p>We developed a method for sampling faces from the reference PCA space in a manner that would maximize dissimilarity variance. This is related to the concept of design efficiency in univariate general linear modeling [<xref ref-type="bibr" rid="pcbi.1005604.ref050">50</xref>], and involves maximizing the variance of hypothesized distances over the stimulus set. Because randomly sampled distances in high dimensional spaces tend to fall in a narrow range of distances relative to the norm [<xref ref-type="bibr" rid="pcbi.1005604.ref051">51</xref>], we reduced each participant’s effective PCA space to 2D by specifying a plane which was centered on the norm of the space and extended at a random orientation. The face exemplars constituted a polar grid on this plane, with 4 directions at 60 degrees separation and 3 eccentricity levels (scaled at 30%, 100% and 170% of the mean eccentricity in the training face set). The resulting half-circle grid on a plane through the high-dimensional space is adequate for addressing our hypotheses concerning the relative role of direction and eccentricity coding under the assumption that the high-dimensional space is isotropic. The use of a half-circle also serves to address a potential concern that apparent eccentricity sensitivity might arise as a consequence of adaptation to the experimental stimuli [<xref ref-type="bibr" rid="pcbi.1005604.ref052">52</xref>]. Such adaptation effects are only collinear with eccentricity (ie, prototype) coding if the prototype is located at the average position over the experienced stimulus images. For our stimulus space, this average position would fall approximately between sub- and typical faces and between the second and third direction in the PCA space. Contrary to an adaptation account, there was no suggestion in our data that cortical distances were exaggerated as a function of distance along this axis. The orientation of the PCA-space slice was randomized between participants and model fits were based on cross-validation over participants. Under these conditions, any non-isotropicity is only expected to impair generalization performance. In preliminary tests we observed that this method yielded substantially greater dissimilarity variance estimates than methods based on Gaussian or uniform sampling of the space.</p>
</sec>
<sec id="sec021">
<title>Face animation preparation</title>
<p>We used Matlab software to generate a 3D face mesh for each exemplar. This mesh was rendered at each of the orientations of interest in the study in a manner that centered the axis of rotation on the bridge of the nose for each face. This procedure ensured that the eye region remained centered on the fixation point throughout each animation in order to discourage eye movements. Renders were performed at sufficient increments to enable 24 frames per second temporal resolution in the resulting animations. Frames were converted to gray-scale and cropped with a feathered oval aperture to standardize the outline of each face and to remove high-contrast mesh edges from the stimulus set. Finally, we performed a frame-by-frame histogram equalization procedure where the average histogram for each frame was imposed on each individual face. Thus, the histogram was allowed to vary across time but not across faces. Note that histogram matching implies that the animations also have identical mean gray-scale intensity and root-mean-square contrast.</p>
<p>A potential concern with these matching procedures is that they could affect the validity of the comparison to the reference PCA space. However, we found that the opposite appeared to be true: distances in the reference PCA space were more predictive of pixelwise correlation distances in the matched images than in the original images. Thus, the matching procedure did not remove features that were encoded in the PCA space and may in fact have acted to emphasize such features.</p>
</sec>
<sec id="sec022">
<title>Perceptual similarity judgment experiment</title>
<p>We used a pair-of-pairs task to characterize perceptual similarity (<xref ref-type="supplementary-material" rid="pcbi.1005604.s021">S1 Movie</xref>). Participants were presented with two vertically offset pairs of faces on a standard LCD monitor under free viewing conditions, and judged which pair was relatively more dissimilar with a button press on a USB keyboard (two-alternative force choice). Each face rotated continuously between a leftward and a rightward orientation (45 degrees left to 45 degrees right of a frontal view over 3 seconds). Ratings across all possible pairings of face pairs (2145 trials: all pairings of the 66 possible pairs of the 12 faces) were combined into a distance matrix for each participant, where each entry reflects the percentage of trials on which that face pair was rated as relatively more dissimilar. The behavioral data was collected over 16 runs (135 trials for the first 15 runs, 120 in the final run). Each participant completed 4 runs in each of 4 data recording days.</p>
</sec>
<sec id="sec023">
<title>Functional MRI experiment</title>
<p>We measured brain response patterns evoked by faces in a rapid event-related fMRI experiment (<xref ref-type="supplementary-material" rid="pcbi.1005604.s022">S2 Movie</xref>). Participants fixated on a central point of the screen where a pseudo-random sequence of face animations appeared (7 degrees visual angle in height, 2s on, 1s fixation interval). We verified fixation accuracy online and offline using an infrared eye tracking system (Sensomotoric Instruments, 50Hz monocular acquisition). The faces rotated outward in leftward and rightward directions on separate trials (18 to 45 degrees rotation left or right of a frontal view), and participants responded with a button press to occasional face repetitions regardless of rotation (one-back task). This served to encourage attention to facial identity rather than to incidental low-level physical features. Consistent with a task strategy based on identity recognition rather than image matching, participants were sensitive to exemplar repetitions within viewpoint (mean d’+-1 standard deviation 2.68+-0.62) and to exemplar repetitions where the viewpoint changed (2.39+-0.52).</p>
<p>The experiment was divided into 16 runs where each run comprised 156 trials bookended by 10s fixation intervals. Each scanner run comprised two experimental runs, which were modeled independently in all subsequent analyses. The data was collected on 4 separate MRI data recording days (2 scanner runs per recording day). The trial order in each run was first-order counterbalanced over the 12 faces using a De Bruijn sequence [<xref ref-type="bibr" rid="pcbi.1005604.ref053">53</xref>] with 1 additional repetition (diagonal entries in transfer matrix) added to each face in order to make the one-back repetition task more engaging and to increase design efficiency [<xref ref-type="bibr" rid="pcbi.1005604.ref050">50</xref>]. The rotation direction in which each face appeared was randomized separately, since a full 24-stimulus De Bruijn sequence would have been over-long (576 trials). Although the resulting 24-stimulus sequences were not fully counter-balanced, we used an iterative procedure to minimize any inhomogeneity by rejecting rotation direction randomizations that generated off-diagonal values other than 0 and 1 in the 24-condition transfer matrix (that is, each possible stimulus-to-stimulus transfer in the sequence could appear once or not at all). These homogeneous trial sequences served to enhance leave-one-run-out cross-validation performance by minimizing over-fitting to idiosyncratic trial sequence biases in particular runs. We modeled the data from each run with one predictor per face exemplar and viewpoint.</p>
</sec>
<sec id="sec024">
<title>Magnetic resonance imaging acquisition</title>
<p>Functional and structural images were collected at the MRC Cognition and Brain Sciences Unit (Cambridge, UK) using a 3T Siemens Tim Trio system and a 32-channel head coil. There were 4 separate MRI data recording days for each participant. Each recording day comprised 2 runs of the main experiment followed by 2 runs of the functional localizer experiment. All functional runs used a 3D echoplanar imaging sequence (2mm isotropic voxels, 30 axial slices, 192 x 192mm field of view, 128 x 128 matrix, TR = 53ms, TE = 30ms, 15° flip angle, effective acquisition time 1.06s per volume) with GRAPPA acceleration (acceleration factor 2 x 2, 40 x 40 PE lines). Each participant’s functional dataset (7376 volumes over 8 scanner runs for the main experiment) was converted to NIFTI format and realigned to the mean of the first recording day’s first experimental run using standard functionality in SPM8 (fil.ion.ucl.ac.uk/spm/software/spm8/). A structural T1-weighted volume was collected in the first recording day using a multi-echo MPRAGE sequence (1mm isotropic voxels)[<xref ref-type="bibr" rid="pcbi.1005604.ref054">54</xref>]. The structural image was de-noised using previously described methods [<xref ref-type="bibr" rid="pcbi.1005604.ref055">55</xref>], and the realigned functional dataset’s header was co-registered with the header of the structural volume using SPM8 functionality. The structural image was then skull-stripped using the FSL brain extraction tool (fmrib.ox.ac.uk/fsl), and a re-sliced version of the resulting brain mask was applied to the fMRI dataset to remove artifacts from non-brain tissue. We constructed design matrices for each experimental run by convolving the onsets of experimental events with the SPM8 canonical hemodynamic response function. Slow temporal drifts in MR signal were removed by projecting out the contribution of a set of nuisance trend regressors (polynomials of degrees 0–4) from the design matrix and the fMRI data in each run.</p>
</sec>
<sec id="sec025">
<title>Cross-validated discriminant analysis</title>
<p>We estimated the neural discriminability of each face pair for each region of interest using a cross-validated version of the Mahalanobis distance [<xref ref-type="bibr" rid="pcbi.1005604.ref056">56</xref>]. This analysis improves on the related Fisher’s linear discriminant classifier by providing a continuous metric of discriminability without ceiling effects. Similarly to the linear discriminant, classifier weights were estimated as the contrast between each condition pair multiplied by the inverse of the covariance matrix of the residual time courses, which was estimated using a sparse prior [<xref ref-type="bibr" rid="pcbi.1005604.ref057">57</xref>]. This discriminant was estimated separately for the concatenated design matrix and fMRI data in each possible leave-one-out split of the experimental runs, and the resulting weights were transformed to unit length and projected onto the contrast estimates from each training split’s corresponding test run (16 estimates per contrast). The 16 run-specific distance estimates were averaged to obtain the final neural discriminability estimate for that participant and region. When the same data is used to estimate the discriminant and evaluate its performance, this algorithm returns the Mahalanobis distance, provided that a full rather than sparse covariance estimator is used [<xref ref-type="bibr" rid="pcbi.1005604.ref056">56</xref>]. However, unlike a true distance measure, the cross-validated version that we use here is centered on 0 under the null hypothesis. This motivates summary-statistic random-effects inference for above-chance performance using conventional T tests.</p>
<p>We developed a variant of this discriminant analysis where effects that might be broadly described as region-mean-related are removed (<xref ref-type="supplementary-material" rid="pcbi.1005604.s004">S4 Fig</xref>). This control analysis involved two modifications to how contrasts were calculated at the level of forming the discriminant and at the level of evaluating the discriminant on independent data. First, each parameter estimate was set to a mean of zero in order to remove any additive offsets in response levels between the conditions. Second, for each pair of mean-subtracted parameter estimates, the linear contribution of the mean estimate over the pair was removed from each estimate before calculating the contrast. This corrects for the case where a single response pattern is multiplicatively scaled by the conditions. The resulting control analysis is insensitive to effects driven by additive or multiplicative scaling offsets between the conditions.</p>
</sec>
<sec id="sec026">
<title>Multiple regression RSA</title>
<p>We used a multiple regression model to estimate the relative contribution of eccentricity and direction to cortical and perceptual face-space representations. Multiple regression fits to distance estimates can be performed after a square transform, since squared distances sum according to the Pythagorean theorem. We partitioned the squared distances in the reference PCA space into variance associated with eccentricity changes by creating a distance matrix where each entry reflected the minimum distance for its eccentricity group in the squared reference PCA matrix (that is, cases along the group’s diagonal where there was no direction change). The direction matrix was then constructed as the difference between the squared reference PCA matrix and the eccentricity matrix (<xref ref-type="fig" rid="pcbi.1005604.g002">Fig 2a</xref>). These predictors were vectorized and entered into a multiple regression model together with a constant term. This partitioning of the dissimilarity variance in the reference PCA matrix is complete in the sense that a multiple regression RSA model where the reference PCA matrix is used as the dependent variable yields parameter estimates of [1,1,0] for eccentricity, direction and constant, respectively, with no residual error. These three predictors were then split according to viewpoint, with separate sets of predictors for distances within and across viewpoint. The absolute values of the cortical and perceptual distance matrices were squared and then transformed back to their original sign before being regressed on the predictor matrix using ordinary least squares. Finally, the absolute values of the resulting parameter estimates were square-root transformed and returned to their original signs.</p>
</sec>
<sec id="sec027">
<title>Functional regions of interest</title>
<p>We used a conventional block-based functional localizer experiment to identify category-selective and visually-responsive regions of interest in human visual cortex. Participants fixated a central cross on the screen while blocks of full-color images were presented (36 images per block presented with 222ms on, 222ms off, 16 s fixation). Participants were instructed to respond to exact image repetitions within the block. Each run comprised 3 blocks each of faces, scenes, objects and phase-scrambled versions of the scene images. Each participant’s data (8 runs of 380 volumes, 2 runs collected for each of 4 MRI data recording days) was smoothed with a Gaussian kernel (6mm full width at half maximum) and responses to each condition were estimated using a standard SPM8 first-level model. Regions of interest were identified using a region-growing approach, where a peak coordinate for each region was identified in individual participants, and a region of interest was grown as a contiguous set of the most selective 100 voxels extending from this coordinate. We defined the face-selective occipital and fusiform face areas with the minimum-statistic conjunction contrast of faces over objects and faces over baseline, and the scene-selective parahippocampal place area and transverse occipital sulcus as the minimum-statistic conjunction contrast of scenes over objects and scenes over baseline, and the early visual cortex as the contrast of scrambled stimuli over the fixation baseline. We also attempted to localize a face-selective region in the posterior superior temporal sulcus, a face-selective region in anterior inferotemporal cortex and a scene-selective region in retrosplenial cortex, but do not report results for these regions here since they could only be identified in a minority of the participants. All regions of interest were combined into bilateral versions before further analysis since we did not have distinct predictions concerning functional lateralization. Example regions of interest can be viewed in <xref ref-type="supplementary-material" rid="pcbi.1005604.s008">S8 Fig</xref>. Typical MNI coordinates for each region are provided in <xref ref-type="supplementary-material" rid="pcbi.1005604.s020">S9 Table</xref>.</p>
</sec>
<sec id="sec028">
<title>Sigmoidal ramp tuning model</title>
<p>The sigmoidal ramp model comprises 1000 model units, each of which exhibits a monotonically increasing response in a random direction extending from the origin of the face space (<xref ref-type="fig" rid="pcbi.1005604.g003">Fig 3</xref>). The response <italic>y</italic> at position <italic>x</italic> along the preferred direction is described by the sigmoid
<disp-formula id="pcbi.1005604.e001">
<alternatives>
<graphic id="pcbi.1005604.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005604.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mtext>y</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>raw</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mtext>o</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where the free parameters are <italic>o</italic>, which specifies the horizontal offset of the response function (zero places the midpoint of the response function at the norm of the space, values greater than zero corresponds to responses shifted away from the norm), and <italic>s</italic>, which defines response function saturation (4 corresponds to a near-linear response in the domain of the face exemplars used here, while values near zero correspond to a step-like increase in response). The raw output of each model unit is then translated toward the population-mean response
<disp-formula id="pcbi.1005604.e002">
<alternatives>
<graphic id="pcbi.1005604.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005604.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mtext>y</mml:mtext><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtext>final</mml:mtext></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>y</mml:mtext><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtext>raw</mml:mtext></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext>y</mml:mtext><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtext>mean</mml:mtext></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>1</mml:mtext><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>y</mml:mtext><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtext>mean</mml:mtext></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math>
</alternatives>
</disp-formula>
where <italic>p</italic> is a free parameter that defines the strength of measurement-level population averaging (0 corresponds to no averaging, 1 corresponds to each model unit returning the population-mean response).</p>
</sec>
<sec id="sec029">
<title>Exemplar model</title>
<p>The exemplar model comprises 1000 model units, each of which prefers a Cartesian coordinate in the face space with response fall-off captured by an isotropic Gaussian. The free parameters are <italic>w</italic>, which controls the full width at half-maximum tuning width of the Gaussian response function, and <italic>d</italic>, which controls the width of the Gaussian distribution of tuning centers (0.1 places Z = 2.32 at 10% of the eccentricity of the caricatures while 3 places this tail at 300% of the eccentricity of the caricatures).</p>
<p>We also constructed an inverted-Gaussian variant of this model where the distribution of distances was inverted at Z = 2.32 and negative distances truncated to zero (1% of exemplars). This model was fitted with similar parameters as the original Gaussian exemplar model.</p>
</sec>
<sec id="sec030">
<title>Gabor filter model</title>
<p>The Gabor filter model is an adaptation of a neuroscientifically-inspired model that has previously been used to successfully predict single-voxel responses in the early visual cortex ([<xref ref-type="bibr" rid="pcbi.1005604.ref024">24</xref>], <ext-link ext-link-type="uri" xlink:href="https://github.com/kendrickkay/knkutils/tree/master/imageprocessing" xlink:type="simple">github.com/kendrickkay/knkutils/tree/master/imageprocessing</ext-link>). The model is composed of 5 banks of Gabor filters varying in spatial position, phase (2 values) and orientation (8 directions, <xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4</xref>). We measured each filter’s response to the last frame of each animation, and corrected for phase shifts by collapsing the two signed phase value filter outputs into a single non-negative estimate of contrast energy (specifically, the square root of the sum over the two squared phase values). This is a standard processing stage in the Gabor filter model [<xref ref-type="bibr" rid="pcbi.1005604.ref024">24</xref>]. The resulting rectified response vectors were weighted according to filter bank membership (5 free parameters). We estimated measurement-level population averaging using two pooling stages: a hemifield-specific pool, where filters were pooled according to whether their centers fell left or right of the vertical meridian, followed by a global pool.</p>
</sec>
<sec id="sec031">
<title>Pixelwise correlation predictor</title>
<p>We used a fixed control predictor to estimate whether coding based on pixelwise features would produce the same face-space warping we observed in our data (<xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4</xref>). The pixelwise correlation predictor was generated by stacking all the pixels in each of the face animations into vectors and estimating the correlation distance between these intensity values.</p>
</sec>
<sec id="sec032">
<title>Estimating the noise ceiling</title>
<p>We estimated the noise ceiling for Z-transformed Pearson correlation coefficients based on methods described previously [<xref ref-type="bibr" rid="pcbi.1005604.ref056">56</xref>]. This method estimates the explained variance that is expected for the true model given noise levels in the data. Although the true noise level of the data cannot be estimated, it is possible to approximate its upper and lower bounds in order to produce a range within which the true noise ceiling is expected to reside. The lower bound estimate is obtained by a leave-one-participant-out cross-validation procedure where the mean distance estimates of the training split are correlated against the left-out-participant’s distances, while the upper bound is obtained by performing the same procedure without splitting the data. These estimates were visualized as a shaded region in figures after reversing the Z-transform (<xref ref-type="fig" rid="pcbi.1005604.g004">Fig 4</xref>).</p>
</sec>
<sec id="sec033">
<title>Statistical inference</title>
<p>All statistical inference was performed using T tests at the group-average level (N = 10 in all cases except the occipital face area and transverse occipital sulcus, N = 9). Correlation coefficients were Z-transformed prior to statistical testing. Average Z statistics were reverse-transformed before visualization for illustrative purposes.</p>
<p>Fold-wise generalization performance estimates are partially dependent, which can lead to sample variance underestimates [<xref ref-type="bibr" rid="pcbi.1005604.ref058">58</xref>,<xref ref-type="bibr" rid="pcbi.1005604.ref059">59</xref>] and greater than intended false positive rates when conventional parametric statistics are used. However, we simulated the effects of this potential bias and found no consistent inflation in false-positive rates for simulations of the parameters used in the current study (<xref ref-type="supplementary-material" rid="pcbi.1005604.s011">S1 Code</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s009">S9</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005604.s010">S10</xref> Figs). Thus, the inferential statistics reported in the current study appear to be robust to this slight dependence and maintain their intended frequentist properties.</p>
</sec>
</sec>
<sec id="sec034">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005604.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Example stimulus sets for 4 participants.</title>
<p>Each stimulus set shares the same underlying distance matrix in the reference PCA space, while the randomization of the orientation of the plane on which the faces are sampled ensures that each set is visually distinct.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Group-average distance matrices from additional regions of interest and best-fitting model predictions from each model considered in the main manuscript (<xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref>).</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Cortical direction discriminability as a function of eccentricity level.</title>
<p>Each point reflects the mean performance for all directions at a given eccentricity level (4x4 block diagonals in <xref ref-type="fig" rid="pcbi.1005604.g001">Fig 1</xref>) for a single participant. Small random offsets have been added to each x coordinate for illustrative purposes, and a line shows the least-squares fit. Performance is plotted separately for distances within viewpoint (round markers, solid line, left offset) and across viewpoint (square markers, dashed line, right offset). All plotted p values are obtained through group analysis of single-participant estimates. Within viewpoint, cortical discrimination performance increases with eccentricity level in all regions except the parahippocampal place area (e). Across viewpoint, statistically significant effects are observed in the ventral temporal fusiform face area in the lateral temporal transverse occipital sulcus, but not in occipital areas (early visual cortex, occipital face area).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Effect of region-mean removal on cortical face spaces.</title>
<p>The top row shows original distance matrices, while the bottom row shows distance matrices after removing additive and multiplicative mean pattern effects (<xref ref-type="sec" rid="sec017">Materials and methods</xref>). The cited Pearson correlation coefficients are calculated at the group-average level.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Cross-validated distance-matrix generalization performance for additional cortical regions of interest.</title>
<p>Plotted as <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref> in main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Cross-validated activation-profile generalization performance for additional cortical regions of interest.</title>
<p>Plotted as in <xref ref-type="fig" rid="pcbi.1005604.g006">Fig 6</xref> in main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Cross-validated distance-matrix similarity, data distance matrices and best-fitting predicted matrices for an analysis where the two viewpoints have been collapsed into a single set of 12 conditions.</title>
<p>Plotted as in Figs <xref ref-type="fig" rid="pcbi.1005604.g001">1</xref> and <xref ref-type="fig" rid="pcbi.1005604.g005">5</xref> in main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Example cortical regions for 3 participants (rows) overlaid on the mean fMRI volume from the first scanner run of the experiment.</title>
<p>Regions are color-coded and labels are provided in the bottom row. Abbreviations and colors: EVC—early visual cortex, green; FFA—fusiform face area, red; OFA—occipital face area, yellow; PPA—parahippocampal place area, purple; TOS—transverse occipital sulcus, blue.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Rejection probability simulation.</title>
<p>We simulated out-of-sample generalization performance for two arbitrary models, using methods that closely matched the ones described in this manuscript (for details, see <xref ref-type="supplementary-material" rid="pcbi.1005604.s011">S1 Code</xref>). We estimated potential bias by comparing cross-validated generalization performance (panels in leftmost column) with generalization to a withheld validation set (left column, subtraction in right column). The probability of rejecting the null hypothesis (p&lt;0.05, T test) over 100000 simulations is plotted for tests of either model against zero (one-tailed test, first two rows of panels), and of zero difference between the models’ generalization performance (two-tailed test, bottom row). Each color-mapped image shows the rejection probability as a function of signal level for model 1 (vertical axis) and model 2 (horizontal axis). The null hypothesis case is highlighted with white circles. The bars in the rightmost panel summarize the mean rejection probabilities (ie, false positives) for each of these null cases.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Summary plot of the data in <xref ref-type="supplementary-material" rid="pcbi.1005604.s009">S9 Fig</xref>.</title>
<p>Each point represents the mean rejection probability for a unique set of simulation parameters (values in color-mapped images in <xref ref-type="supplementary-material" rid="pcbi.1005604.s009">S9 Fig</xref>). It can be seen that there is a close to unit relationship between rejection probability in the dependent, cross-validated case (vertical axis) and the independent, validation case (horizontal axis).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s011" mimetype="text/plain" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s011" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>Code to reproduce <xref ref-type="supplementary-material" rid="pcbi.1005604.s009">S9</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005604.s010">S10</xref> Figs.</title>
<p>Requires Matlab R2013a.</p>
<p>(M)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s012" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s012" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Descriptive and inferential statistics for the distance-matrix correlation between the face-space models and the perceptual and cortical face spaces.</title>
<p>We report the group-average correlation coefficient (mean_r), the group-average Z-transformed correlation (mean_zr), standard error for the Z-transformed correlation (sterr_zr), one-tailed p values for the Z-transformed correlation (ppara_zr) and sample sizes (n). Related to <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s013" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s013" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Analysis of variance on parameter estimates from multiple regression RSA model, with the factors metric (eccentricity, direction), viewpoint (within, across), and a two-way interaction term.</title>
<p>For details, see main text. Related to <xref ref-type="fig" rid="pcbi.1005604.g002">Fig 2</xref>.</p>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s014" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s014" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Descriptive and inferential statistics for the analysis of direction discriminability as a function of eccentricity level.</title>
<p>Mean, standard error (sterr), one-tailed p values (ppara) and sample sizes (n) are included on separate rows. See also <xref ref-type="supplementary-material" rid="pcbi.1005604.s003">S3 Fig</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s015" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s015" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Two-tailed parametric p values for all pairwise comparisons between model distance-matrix generalization performances (<xref ref-type="supplementary-material" rid="pcbi.1005604.s012">S1 Table</xref>).</title>
<p>Related to <xref ref-type="fig" rid="pcbi.1005604.g005">Fig 5</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s016" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s016" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Descriptive and inferential statistics for activation-profile similarity analysis.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1005604.s012">S1 Table</xref> for an account of what the row labels represent. Related to <xref ref-type="fig" rid="pcbi.1005604.g006">Fig 6</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s017" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s017" xlink:type="simple">
<label>S6 Table</label>
<caption>
<title>Two-tailed parametric p values for all pairwise comparisons between activation-profile model fits (<xref ref-type="supplementary-material" rid="pcbi.1005604.s016">S5 Table</xref>).</title>
<p>Related to <xref ref-type="fig" rid="pcbi.1005604.g006">Fig 6</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s018" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s018" xlink:type="simple">
<label>S7 Table</label>
<caption>
<title>Descriptive and inferential statistics for collapsed-view distance-matrix generalization performances.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1005604.s012">S1 Table</xref> for an account of what the row labels represent. Related to <xref ref-type="supplementary-material" rid="pcbi.1005604.s007">S7 Fig</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s019" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s019" xlink:type="simple">
<label>S8 Table</label>
<caption>
<title>Two-tailed parametric p values for all pairwise comparisons between collapsed-view distance-matrix generalization performances.</title>
<p>See <xref ref-type="supplementary-material" rid="pcbi.1005604.s018">S7 Table</xref>, <xref ref-type="supplementary-material" rid="pcbi.1005604.s007">S7 Fig</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s020" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s020" xlink:type="simple">
<label>S9 Table</label>
<caption>
<title>Typical coordinates for the cortical regions of interest.</title>
<p>We identified the peak voxel in each region of interest using the region-defining contrast in the localizer experiment, and converted these native-space voxel indices to standard mm coordinates in the MNI template brain using transformations obtained from SPM8 structural T1 normalization routines. We report means and standard deviations across participants for each region. See also <xref ref-type="supplementary-material" rid="pcbi.1005604.s008">S8 Fig</xref>.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s021" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s021" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>Cropped screen capture of the perceptual judgment task as it appeared to participants during data collection.</title>
<p>(MOV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005604.s022" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005604.s022" xlink:type="simple">
<label>S2 Movie</label>
<caption>
<title>Cropped screen capture of the main experiment as it appeared to participants during data collection.</title>
<p>(MOV)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We are grateful to Jenna Parker for assistance with data collection and to Marta Correia for assistance with 3D EPI protocols.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005604.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anzellotti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Caramazza</surname> <given-names>A</given-names></name>. <article-title>From parts to identity: Invariance and sensitivity of face representations to different face halves</article-title>. <source>Cereb Cortex</source>.: <fpage>1</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhu337" xlink:type="simple">10.1093/cercor/bhu337</ext-link></comment> <object-id pub-id-type="pmid">25628344</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anzellotti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Caramazza</surname> <given-names>A</given-names></name>. <article-title>Decoding representations of face identity that are tolerant to rotation</article-title>. <source>Cereb Cortex</source>. <year>2014</year>;<volume>24</volume>: <fpage>1988</fpage>–<lpage>1995</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bht046" xlink:type="simple">10.1093/cercor/bht046</ext-link></comment> <object-id pub-id-type="pmid">23463339</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Axelrod</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>G</given-names></name>. <article-title>Successful decoding of famous faces in the fusiform face area</article-title>. <source>PLoS One</source>. <year>2015</year>;<volume>10</volume>: <fpage>e0117126</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0117126" xlink:type="simple">10.1371/journal.pone.0117126</ext-link></comment> <object-id pub-id-type="pmid">25714434</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goesaert</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Op de Beeck</surname> <given-names>HP</given-names></name>. <article-title>Representations of facial identity information in the ventral visual stream investigated with multivoxel pattern analyses</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>8549</fpage>–<lpage>8558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1829-12.2013" xlink:type="simple">10.1523/JNEUROSCI.1829-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23658192</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Formisano</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sorger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Goebel</surname> <given-names>R</given-names></name>. <article-title>Individual faces elicit distinct response patterns in human anterior temporal cortex</article-title>. <source>Proc Natl Acad Sci</source>. <year>2007</year>;<volume>104</volume>: <fpage>20600</fpage>–<lpage>20605</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0705654104" xlink:type="simple">10.1073/pnas.0705654104</ext-link></comment> <object-id pub-id-type="pmid">18077383</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Natu</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Narvekar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Keshvari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Blanz</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>O’Toole</surname> <given-names>AJ</given-names></name>. <article-title>Dissociable neural patterns of facial identity across changes in viewpoint</article-title>. <source>J Cogn Neurosci</source>. <year>2010</year>;<volume>22</volume>: <fpage>1570</fpage>–<lpage>1582</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2009.21312" xlink:type="simple">10.1162/jocn.2009.21312</ext-link></comment> <object-id pub-id-type="pmid">19642884</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nestor</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Behrmann</surname> <given-names>M</given-names></name>. <article-title>Unraveling the distributed neural code of facial identity through spatiotemporal pattern analysis</article-title>. <source>Proc Natl Acad Sci</source>. <year>2011</year>;<volume>108</volume>: <fpage>9998</fpage>–<lpage>10003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1102433108" xlink:type="simple">10.1073/pnas.1102433108</ext-link></comment> <object-id pub-id-type="pmid">21628569</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nestor</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Behrmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>. <article-title>The neural basis of visual word form processing: A multivariate investigation</article-title>. <source>Cereb Cortex</source>. <year>2013</year>;<volume>23</volume>: <fpage>1673</fpage>–<lpage>1684</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhs158" xlink:type="simple">10.1093/cercor/bhs158</ext-link></comment> <object-id pub-id-type="pmid">22693338</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gao</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>. <article-title>The neural representation of face space dimensions</article-title>. <source>Neuropsychologia. Elsevier</source>; <year>2013</year>;<volume>51</volume>: <fpage>1787</fpage>–<lpage>1793</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2013.07.001" xlink:type="simple">10.1016/j.neuropsychologia.2013.07.001</ext-link></comment> <object-id pub-id-type="pmid">23850598</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verosky</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Todorov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Turk-Browne</surname> <given-names>NB</given-names></name>. <article-title>Representations of individuals in ventral temporal cortex defined by faces and biographies</article-title>. <source>Neuropsychologia. Elsevier</source>; <year>2013</year>;<volume>51</volume>: <fpage>2100</fpage>–<lpage>2108</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2013.07.006" xlink:type="simple">10.1016/j.neuropsychologia.2013.07.006</ext-link></comment> <object-id pub-id-type="pmid">23871881</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>PA</given-names></name>. <article-title>Representational similarity analysis—connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source>. <year>2008</year>;<volume>2</volume>: <fpage>1</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005604.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haxby</surname> <given-names>J V</given-names></name>, <name name-style="western"><surname>Hoffman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gobbini</surname> <given-names>M</given-names></name>. <article-title>The distributed human neural system for face perception</article-title>. <source>Trends Cogn Sci</source>. <year>2000</year>;<volume>4</volume>: <fpage>223</fpage>–<lpage>233</lpage>. <object-id pub-id-type="pmid">10827445</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruce</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>AW</given-names></name>. <article-title>Understanding face recognition</article-title>. <source>Br J Psychol</source>. <year>1986</year>;<volume>77</volume> (<issue>Pt 3</issue>): <fpage>305</fpage>–<lpage>27</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/3756376" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/3756376</ext-link></mixed-citation></ref>
<ref id="pcbi.1005604.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Valentine</surname> <given-names>T</given-names></name>. <article-title>A unified account of the effects of distinctiveness, inversion, and race in face recognition</article-title>. <source>Q J Exp Psychol</source>. <year>1991</year>;<volume>43A</volume>: <fpage>161</fpage>–<lpage>204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/14640749108400966" xlink:type="simple">10.1080/14640749108400966</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005604.ref015"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Blanz V, Vetter T. A morphable model for the synthesis of 3D faces. Proc 26th Annu Conf Comput Graph Interact Tech—SIGGRAPH ‘99. New York, New York, USA: ACM Press; 1999; 187–194.</mixed-citation></ref>
<ref id="pcbi.1005604.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Toole</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Abdi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Deffenbacher</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Valentin</surname> <given-names>D</given-names></name>. <article-title>Low-dimensional representation of faces in higher dimensions of the face space</article-title>. <source>J Opt Soc Am A</source>. <year>1993</year>;<volume>10</volume>: <fpage>405</fpage>–<lpage>415</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.opticsinfobase.org/abstract.cfm?id=4552" xlink:type="simple">http://www.opticsinfobase.org/abstract.cfm?id=4552</ext-link></mixed-citation></ref>
<ref id="pcbi.1005604.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ross</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Hancock</surname> <given-names>PJB</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>MB</given-names></name>. <article-title>Changing faces: Direction is important</article-title>. <source>Vis cogn</source>. <year>2010</year>;<volume>18</volume>: <fpage>67</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/13506280802536656" xlink:type="simple">10.1080/13506280802536656</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005604.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schulz</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kaufmann</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schweinberger</surname> <given-names>SR</given-names></name>. <article-title>Effects of anticaricaturing vs. caricaturing and their neural correlates elucidate a role of shape for face learning</article-title>. <source>Neuropsychologia</source>. <year>2012</year>;<volume>50</volume>: <fpage>2426</fpage>–<lpage>2434</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2012.06.013" xlink:type="simple">10.1016/j.neuropsychologia.2012.06.013</ext-link></comment> <object-id pub-id-type="pmid">22750120</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Loffler</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wilkinson</surname> <given-names>F</given-names></name>. <article-title>Synthetic faces, face cubes, and the geometry of face space</article-title>. <source>Vision Res</source>. <year>2002</year>;<volume>42</volume>: <fpage>2909</fpage>–<lpage>23</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/12450502" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/12450502</ext-link> <object-id pub-id-type="pmid">12450502</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leopold</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Bondar</surname> <given-names>I V</given-names></name>, <name name-style="western"><surname>Giese</surname> <given-names>MA</given-names></name>. <article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title>. <source>Nature</source>. <year>2006</year>;<volume>442</volume>: <fpage>572</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04951" xlink:type="simple">10.1038/nature04951</ext-link></comment> <object-id pub-id-type="pmid">16862123</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loffler</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Yourganov</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wilkinson</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>. <article-title>fMRI evidence for the neural representation of faces</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>10</volume>: <fpage>1386</fpage>–<lpage>1390</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005604.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davidenko</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Remus</surname> <given-names>D a.</given-names></name>, <name name-style="western"><surname>Grill-Spector</surname> <given-names>K</given-names></name>. <article-title>Face-likeness and image variability drive responses in human face-selective ventral regions</article-title>. <source>Hum Brain Mapp</source>. <year>2012</year>;<volume>33</volume>: <fpage>2334</fpage>–<lpage>2349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hbm.21367" xlink:type="simple">10.1002/hbm.21367</ext-link></comment> <object-id pub-id-type="pmid">21823208</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Said</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Dotsch</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Todorov</surname> <given-names>A</given-names></name>. <article-title>The amygdala and FFA track both social and non-social face dimensions</article-title>. <source>Neuropsychologia. Elsevier Ltd</source>; <year>2010</year>;<volume>48</volume>: <fpage>3596</fpage>–<lpage>3605</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2010.08.009" xlink:type="simple">10.1016/j.neuropsychologia.2010.08.009</ext-link></comment> <object-id pub-id-type="pmid">20727365</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kay</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source>. <year>2008</year>;<volume>452</volume>: <fpage>352</fpage>–<lpage>355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06713" xlink:type="simple">10.1038/nature06713</ext-link></comment> <object-id pub-id-type="pmid">18322462</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref025"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Paysan P, Knothe R, Amberg B, Romdhani S, Vetter T. A 3D Face Model for Pose and Illumination Invariant Face Recognition. 2009 Sixth IEEE Int Conf Adv Video Signal Based Surveill. Ieee; 2009; 296–301.</mixed-citation></ref>
<ref id="pcbi.1005604.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Westfall</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nichols</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Yarkoni</surname> <given-names>T</given-names></name>. <article-title>Fixing the stimulus-as-fixed-effect fallacy in task fMRI</article-title>. <source>BioRxiv</source>. <year>2016</year>;</mixed-citation></ref>
<ref id="pcbi.1005604.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname> <given-names>CE</given-names></name>. <article-title>Shape Representation in Area V4: Position-Specific Tuning for Boundary Conformation</article-title>. <source>J Neurophysiol</source>. <year>2001</year>;<volume>86</volume>: <fpage>2505</fpage>–<lpage>2519</lpage>. <object-id pub-id-type="pmid">11698538</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>M</given-names></name>. <article-title>A face feature space in the macaque temporal lobe</article-title>. <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>: <fpage>1187</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2363" xlink:type="simple">10.1038/nn.2363</ext-link></comment> <object-id pub-id-type="pmid">19668199</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boynton</surname> <given-names>GM</given-names></name>. <article-title>Spikes, BOLD, attention, and awareness: a comparison of electrophysiological and fMRI signals in V1</article-title>. <source>J Vis</source>. <year>2011</year>;<volume>11</volume>: <fpage>12</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.5.12" xlink:type="simple">10.1167/11.5.12</ext-link></comment> <object-id pub-id-type="pmid">22199162</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goense</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>N</given-names></name>. <article-title>Neurophysiology of the BOLD fMRI signal in awake monkeys</article-title>. <source>Curr Biol</source>. <year>2008</year>;<volume>18</volume>: <fpage>631</fpage>–<lpage>640</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2008.03.054" xlink:type="simple">10.1016/j.cub.2008.03.054</ext-link></comment> <object-id pub-id-type="pmid">18439825</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Pauls</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Augath</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Trinath</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oeltermann</surname> <given-names>a</given-names></name>. <article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title>. <source>Nature</source>. <year>2001</year>;<volume>412</volume>: <fpage>150</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35084005" xlink:type="simple">10.1038/35084005</ext-link></comment> <object-id pub-id-type="pmid">11449264</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sirotin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Das</surname> <given-names>A</given-names></name>. <article-title>Anticipatory haemodynamic signals in sensory cortex not predicted by local neuronal activity</article-title>. <source>Nature</source>. <year>2009</year>;<volume>457</volume>: <fpage>475</fpage>–<lpage>479</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07664" xlink:type="simple">10.1038/nature07664</ext-link></comment> <object-id pub-id-type="pmid">19158795</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cardoso</surname> <given-names>MMB</given-names></name>, <name name-style="western"><surname>Sirotin</surname> <given-names>YB</given-names></name>, <name name-style="western"><surname>Lima</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glushenkova</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Das</surname> <given-names>A</given-names></name>. <article-title>The neuroimaging signal is a linear sum of neurally distinct stimulus- and task-related components</article-title>. <source>Nat Neurosci. Nature Publishing Group</source>; <year>2012</year>;<volume>15</volume>: <fpage>1298</fpage>–<lpage>1306</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3170" xlink:type="simple">10.1038/nn.3170</ext-link></comment> <object-id pub-id-type="pmid">22842146</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kietzmann</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Swisher</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Konig</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tong</surname> <given-names>F</given-names></name>. <article-title>Prevalence of Selectivity for Mirror-Symmetric Views of Faces in the Ventral and Dorsal Visual Pathways</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>11763</fpage>–<lpage>11772</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0126-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0126-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22915118</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Axelrod</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>G</given-names></name>. <article-title>Hierarchical Processing of Face Viewpoint in Human Visual Cortex</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>2442</fpage>–<lpage>2452</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4770-11.2012" xlink:type="simple">10.1523/JNEUROSCI.4770-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22396418</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Functional compartmentalization and viewpo—int generalization within the macaque face-processing system</article-title>. <source>Science (80-)</source>. <year>2010</year>;<volume>330</volume>: <fpage>845</fpage>–<lpage>851</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1194908" xlink:type="simple">10.1126/science.1194908</ext-link></comment> <object-id pub-id-type="pmid">21051642</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasselmo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Baylis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Nalwa</surname> <given-names>V</given-names></name>. <article-title>Object-centered encoding by face-selective neurons in the cortex in the superior temporal sulcus of the monkey</article-title>. <source>Exp Brain Res</source>. <year>1989</year>;<volume>75</volume>: <fpage>417</fpage>–<lpage>429</lpage>. <object-id pub-id-type="pmid">2721619</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wachsmuth</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Oram</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perrett</surname> <given-names>DI</given-names></name>. <article-title>Recognition of objects and their component parts: Responses of single units in the temporal cortex of the macaque</article-title>. <source>Cereb Cortex</source>. <year>1994</year>;<volume>4</volume>: <fpage>509</fpage>–<lpage>522</lpage>. <object-id pub-id-type="pmid">7833652</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrett</surname> <given-names>DI</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Caan</surname> <given-names>W</given-names></name>. <article-title>Visual neurones responsive to faces in the monkey temporal cortex</article-title>. <source>Exp Brain Res</source>. <year>1982</year>;<volume>47</volume>: <fpage>329</fpage>–<lpage>342</lpage>. <object-id pub-id-type="pmid">7128705</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramirez</surname> <given-names>FM</given-names></name>, <name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Allefeld</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Haynes</surname> <given-names>J-D</given-names></name>. <article-title>The neural code for face orientation in the human fusiform face area</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>: <fpage>12155</fpage>–<lpage>12167</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3156-13.2014" xlink:type="simple">10.1523/JNEUROSCI.3156-13.2014</ext-link></comment> <object-id pub-id-type="pmid">25186759</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlin</surname> <given-names>JD</given-names></name>. <article-title>Decoding Face Exemplars from fMRI Responses: What Works, What Doesn’t?</article-title> <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>9252</fpage>–<lpage>9254</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1385-15.2015" xlink:type="simple">10.1523/JNEUROSCI.1385-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26109650</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mattavelli</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Andrews</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Asghar</surname> <given-names>AUR</given-names></name>, <name name-style="western"><surname>Towler</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>AW</given-names></name>. <article-title>Response of face-selective brain regions to trustworthiness and gender of faces</article-title>. <source>Neuropsychologia. Elsevier</source>; <year>2012</year>;<volume>50</volume>: <fpage>2205</fpage>–<lpage>2211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2012.05.024" xlink:type="simple">10.1016/j.neuropsychologia.2012.05.024</ext-link></comment> <object-id pub-id-type="pmid">22659107</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hara</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Pestilli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>. <article-title>Differing effects of attention in single-units and populations are well predicted by heterogeneous tuning and the normalization model of attention</article-title>. <source>Front Comput …</source>. <year>2014</year>;<volume>8</volume>: <fpage>1</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00012" xlink:type="simple">10.3389/fncom.2014.00012</ext-link></comment> <object-id pub-id-type="pmid">24600380</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Cusack</surname> <given-names>R</given-names></name>. <article-title>How does an fMRI voxel sample the neuronal activity pattern: Compact-kernel or complex-spatiotemporal filter?</article-title> <source>Neuroimage</source>. <year>2009</year>;<volume>49</volume>: <fpage>1965</fpage>–<lpage>1976</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.09.059" xlink:type="simple">10.1016/j.neuroimage.2009.09.059</ext-link></comment> <object-id pub-id-type="pmid">19800408</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Diedrichsen</surname> <given-names>J</given-names></name>. <article-title>Inferring brain-computational mechanisms with models of activity measurements</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2016</year>; <fpage>1</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2016.0278" xlink:type="simple">10.1098/rstb.2016.0278</ext-link></comment> <object-id pub-id-type="pmid">27574316</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Henriksson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Fixed versus mixed RSA: Explaining visual representations by fixed and mixed feature sets from shallow and deep computational models</article-title>. <source>BiorXiv</source>. <year>2016</year>; <fpage>1</fpage>–<lpage>32</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005604.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>S-M</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>: <fpage>1</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jozwik</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>. <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>. <source>Neuropsychologia. Elsevier</source>; <year>2016</year>;<volume>83</volume>: <fpage>201</fpage>–<lpage>226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2015.10.023" xlink:type="simple">10.1016/j.neuropsychologia.2015.10.023</ext-link></comment> <object-id pub-id-type="pmid">26493748</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlin</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Ramp coding with population averaging predicts human cortical face-space representations and perception</article-title> [Internet]. <source>BiorXiv</source>. <year>2015</year> <month>Oct</month>.</mixed-citation></ref>
<ref id="pcbi.1005604.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Henson</surname> <given-names>RNA</given-names></name>. <chapter-title>Analysis of fMRI timeseries: Linear time-invariant models, event-related fMRI and optimal experimental design</chapter-title>. In: <name name-style="western"><surname>Frackowiak</surname> <given-names>RSJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Price</surname> <given-names>CJ</given-names></name>, editors. <source>Human Brain Function</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <year>2003</year>. pp. <fpage>793</fpage>–<lpage>822</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005604.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burton</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Vokey</surname> <given-names>JR</given-names></name>. <article-title>The face-space typicality paradox: Understanding the face-space metaphor</article-title>. <source>Q J Exp Psychol</source>. <year>1998</year>;<volume>3</volume>: <fpage>475</fpage>–<lpage>483</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.tandfonline.com/doi/abs/10.1080/713755768" xlink:type="simple">http://www.tandfonline.com/doi/abs/10.1080/713755768</ext-link></mixed-citation></ref>
<ref id="pcbi.1005604.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kahn</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Aguirre</surname> <given-names>GK</given-names></name>. <article-title>Confounding of norm-based and adaptation effects in brain responses</article-title>. <source>Neuroimage. Elsevier Inc.</source>; <year>2012</year>;<volume>60</volume>: <fpage>2294</fpage>–<lpage>2299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2012.02.051" xlink:type="simple">10.1016/j.neuroimage.2012.02.051</ext-link></comment> <object-id pub-id-type="pmid">22394673</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aguirre</surname> <given-names>GK</given-names></name>, <name name-style="western"><surname>Mattar</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Magis-Weinberg</surname> <given-names>L</given-names></name>. <article-title>de Bruijn cycles for neural decoding</article-title>. <source>Neuroimage. Elsevier Inc.</source>; <year>2011</year>;<volume>56</volume>: <fpage>1293</fpage>–<lpage>1300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.02.005" xlink:type="simple">10.1016/j.neuroimage.2011.02.005</ext-link></comment> <object-id pub-id-type="pmid">21315160</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Kouwe</surname> <given-names>a. JW</given-names></name>, <name name-style="western"><surname>Benner</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Salat</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Fischl</surname> <given-names>B</given-names></name>. <article-title>Brain morphometry with multiecho MPRAGE</article-title>. <source>Neuroimage</source>. <year>2008</year>;<volume>40</volume>: <fpage>559</fpage>–<lpage>569</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2007.12.025" xlink:type="simple">10.1016/j.neuroimage.2007.12.025</ext-link></comment> <object-id pub-id-type="pmid">18242102</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manjón</surname> <given-names>J V.</given-names></name>, <name name-style="western"><surname>Coupé</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Martí-Bonmatí</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Robles</surname> <given-names>M</given-names></name>. <article-title>Adaptive non-local means denoising of MR images with spatially varying noise levels</article-title>. <source>J Magn Reson Imaging</source>. <year>2010</year>;<volume>31</volume>: <fpage>192</fpage>–<lpage>203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/jmri.22003" xlink:type="simple">10.1002/jmri.22003</ext-link></comment> <object-id pub-id-type="pmid">20027588</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nili</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>A toolbox for representational similarity analysis</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>: <fpage>e1003553</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment> <object-id pub-id-type="pmid">24743308</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Misaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>Neuroimage</source>. <year>2010</year>;<volume>53</volume>: <fpage>103</fpage>–<lpage>118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2010.05.051" xlink:type="simple">10.1016/j.neuroimage.2010.05.051</ext-link></comment> <object-id pub-id-type="pmid">20580933</object-id></mixed-citation></ref>
<ref id="pcbi.1005604.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadeau</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <article-title>Inference for the generalization error</article-title>. <source>Mach Learn</source>. <year>2003</year>;<volume>52</volume>: <fpage>239</fpage>–<lpage>281</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1024068626366" xlink:type="simple">10.1023/A:1024068626366</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005604.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Grandvalet</surname> <given-names>Y</given-names></name>. <article-title>No Unbiased Estimator of the Variance of K-Fold Cross-Validation</article-title>. <source>J Mach Learn Res</source>. <year>2004</year>;<volume>5</volume>: <fpage>1089</fpage>–<lpage>1105</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>