<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="allenpress-id">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">07-PLCB-RA-0724R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000025</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Mathematics/Statistics</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural Coding of Natural Stimuli: Information at Sub-Millisecond Resolution</article-title><alt-title alt-title-type="running-head">Neural Coding at Sub-Millisecond Precision</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Nemenman</surname><given-names>Ilya</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lewen</surname><given-names>Geoffrey D.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Bialek</surname><given-names>William</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>Rob R.</given-names></name><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>Computer, Computational, and Statistical Sciences Division and Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos, New Mexico, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>The Hun School of Princeton, Princeton, New Jersey, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Joseph Henry Laboratories of Physics, Princeton University, Princeton, New Jersey, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Lewis–Sigler Institute for Integrative Genomics, Princeton University, Princeton, New Jersey, United States of America</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Department of Physics, Indiana University, Bloomington, Indiana, United States of America</addr-line>       </aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes><corresp id="cor1">* E-mail: <email xlink:type="simple">nemenman@lanl.gov</email></corresp><fn fn-type="con"><p>The theoretical ideas and experimental methods presented in this paper were developed in close collaboration. IN and WB focused on developing the conceptual framework, implementing statistical tools, and analyzing the data. GL and RR designed the setup and performed the experiments.</p></fn><fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>3</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>7</day><month>3</month><year>2008</year></pub-date><volume>4</volume><issue>3</issue><elocation-id>e1000025</elocation-id><history><date date-type="received"><day>20</day><month>11</month><year>2007</year></date><date date-type="accepted"><day>10</day><month>1</month><year>2008</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Nemenman et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>Sensory information about the outside world is encoded by neurons in sequences of discrete, identical pulses termed action potentials or spikes. There is persistent controversy about the extent to which the precise timing of these spikes is relevant to the function of the brain. We revisit this issue, using the motion-sensitive neurons of the fly visual system as a test case. Our experimental methods allow us to deliver more nearly natural visual stimuli, comparable to those which flies encounter in free, acrobatic flight. New mathematical methods allow us to draw more reliable conclusions about the information content of neural responses even when the set of possible responses is very large. We find that significant amounts of visual information are represented by details of the spike train at millisecond and sub-millisecond precision, even though the sensory input has a correlation time of ∼55 ms; different patterns of spike timing represent distinct motion trajectories, and the absolute timing of spikes points to particular features of these trajectories with high precision. Finally, the efficiency of our entropy estimator makes it possible to uncover features of neural coding relevant for natural visual stimuli: first, the system's information transmission rate varies with natural fluctuations in light intensity, resulting from varying cloud cover, such that marginal increases in information rate thus occur even when the individual photoreceptors are counting on the order of one million photons per second. Secondly, we see that the system exploits the relatively slow dynamics of the stimulus to remove coding redundancy and so generate a more efficient neural code.</p></abstract><abstract abstract-type="summary"><title>Author Summary</title><p>Neurons communicate by means of stereotyped pulses, called action potentials or spikes, and a central issue in systems neuroscience is to understand this neural coding. Here we study how sensory information is encoded in sequences of spikes, using a combination of novel theoretical and experimental techniques. With motion detection in the blowfly as a model system, we perform experiments in an environment maximally similar to the natural one. We report a number of unexpected, striking observations about the structure of the neural code in this system: First, the timing of spikes is important with a precision roughly two orders of magnitude greater than the temporal dynamics of the stimulus. Second, the fly goes a long way to utilize the redundancy in the stimulus in order to optimize the neural code and encode more refined features than would be possible otherwise. This implies that the neural code, even in low-level vision, may be significantly context dependent.</p></abstract><funding-group><funding-statement>This work was supported in part by grants from the National Science Foundation (PHY99-07949, ECS-0425850, IIS-0423039), the Department of Energy under contract DE-AC52-06NA25396, and the Swartz Foundation. Early stages of this work were done when all the authors were at the NEC Research Institute. IN thanks the Kavli Institute for Theoretical Physics at UCSB and the Joint Centers for Systems Biology at Columbia University for their support during this work, WB thanks the Center for Theoretical Neuroscience at Columbia University for its hospitality, and RdR thanks the Linda and Jack Gill Center for Biomolecular Science at Indiana University for its generous support.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Throughout the brain, information is represented by discrete electrical pulses termed action potentials or ‘spikes’ <xref ref-type="bibr" rid="pcbi.1000025-Rieke1">[1]</xref>. For decades there has been controversy about the extent to which the precise timing of these spikes is significant: Should we think of each spike arrival time as having meaning down to millisecond precision <xref ref-type="bibr" rid="pcbi.1000025-MacKay1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1000025-Liu1">[5]</xref>, or does the brain only keep track of the number of spikes occurring in much larger windows of time? Is precise timing relevant only in response to rapidly varying sensory stimuli, as in the auditory system <xref ref-type="bibr" rid="pcbi.1000025-Carr1">[6]</xref>, or can the brain construct specific patterns of spikes with a time resolution much smaller than the time scales of the sensory and motor signals that these patterns represent <xref ref-type="bibr" rid="pcbi.1000025-Abeles1">[3]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Hopfield1">[7]</xref>? Here we address these issues using the motion-sensitive neurons of the fly visual system as a model <xref ref-type="bibr" rid="pcbi.1000025-Hausen1">[8]</xref>.</p><p>We bring together new experimental methods for delivering truly naturalistic visual inputs <xref ref-type="bibr" rid="pcbi.1000025-Lewen1">[9]</xref> and new mathematical methods that allow us to draw more reliable inferences about the information content of spike trains <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref>–<xref ref-type="bibr" rid="pcbi.1000025-Nemenman3">[12]</xref>. We find that as we improve our time resolution for the analysis of spike trains from 2 ms down to a fraction of a millisecond we reveal nearly 30% more information about the trajectory of visual motion. The natural stimuli used in our experiments have essentially no power above 30 Hz, so that the precision of spike timing is not a necessary correlate of the stimulus bandwidth; instead the different patterns of precise spike timing represent subtly different trajectories chosen out of the stimulus ensemble. Further, despite the long correlation times of the sensory stimulus, segments of the neural response separated by ∼30 ms provide essentially independent information, suggesting that the neural code in this system achieves decorrelation <xref ref-type="bibr" rid="pcbi.1000025-Barlow1">[13]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Barlow2">[14]</xref> in the time domain, thereby enhancing the efficiency of the code on time scales relevant to behavior <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>.</p></sec><sec id="s2"><title>Results</title><sec id="s2a"><title>Posing the problem</title><p>Flies exhibit a wide variety of visually guided behaviors, of which perhaps the best known is the optomotor response, in which visual motion drives a compensating torque, stabilizing straight flight <xref ref-type="bibr" rid="pcbi.1000025-Reichardt1">[16]</xref>. This system offers many advantages for the exploration of neural coding and computation: There is a small group of identified, wide-field motion-sensitive neurons <xref ref-type="bibr" rid="pcbi.1000025-Hausen1">[8]</xref> that provide an obligatory link in the process <xref ref-type="bibr" rid="pcbi.1000025-Hausen2">[17]</xref>, and it is possible to make very long, stable recordings from these neurons as well as to characterize in detail the signal and noise properties of the photoreceptors that provide the input data for the computation. In free flight, the trajectory of visual motion is determined largely by the fly's own motion through the world, and there is a large body of data on flight behavior under natural conditions <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1000025-Wagner1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1000025-vanHateren1">[20]</xref>, offering us the opportunity to generate stimuli that approximate those experienced in nature. But the natural visual world of flies involves not only the enormous angular velocities associated with acrobatic flight; natural light intensities and the dynamic range of their variations are very large as well, and both of the fly's compound eyes are stimulated over more than 2π steradians. All of these features are difficult to replicate in the laboratory <xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck1">[21]</xref>. As an alternative, we have moved our experiments outside <xref ref-type="bibr" rid="pcbi.1000025-Lewen1">[9]</xref>, so that flies experience the scenes from the region in which they were caught. We recorded from a single motion-sensitive cell, H1, while rotating the fly along trajectories modeled on published natural flight trajectories (see <xref ref-type="sec" rid="s4">Methods</xref> for details). We should note that for technical reasons, these stimuli do not contain natural translation, pitch, and roll components, which may have an effect on the H1 responses; for other approaches to the delivery of naturalistic stimuli in this system see <xref ref-type="bibr" rid="pcbi.1000025-vanHateren2">[22]</xref>.</p><p>A schematic of our experiment, and an example of the data we obtained, are shown in <xref ref-type="fig" rid="pcbi-1000025-g001">Figure 1</xref>. We see qualitatively that the responses to natural stimuli are very reproducible, and we can point to specific features of the stimulus—such as reversals of motion direction—that generate individual spikes and interspike intervals with better than millisecond precision. The challenge is to quantify these observations: Do precise and reproducible patterns of spikes occur just at some isolated moments, or does looking at the spike train with higher time resolution generally provide more information about the visual input?</p><fig id="pcbi-1000025-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000025.g001</object-id><label>Figure 1</label><caption><title>Neural responses to a natural stimulus ensemble.</title><p>At left is a schematic of the experimental setup (see <xref ref-type="sec" rid="s4">Methods</xref> for details). A fly was immobilized with wax, its body in a plastic tube, with its head protruding. Through a small hole in the back of the head an electrode was inserted to record extracellular potentials from H1, a wide field neuron sensitive to horizontal motion. This signal was amplified, fed through a slip ring system to a second stage amplifier and filter, and recorded by a data acquisition card. In synchrony with its master timer clock, the DAQ card generated a 500 Hz frame clock signal. Every 2 ms, through a bidirectional parallel port, this clock triggered a successive read of a divisor value from a file stored in the stimulus laptop computer. The Intel 8254 Counter/Timer chip used this divisor value to divide down the pulse frequency of a free running 8 MHz clock. In this way, in each successive 2 ms interval, and in strict synchrony with the data taking clock, a defined and evenly spaced burst of pulses was produced. These pulses drove the stepper motor, generating the angular velocity signal. A brief segment of this motion stimulus is shown in the top right panel, below which we plot a raster of action potentials from H1 in response to 100 repetitions of this stimulus. At bottom we expand the scale to illustrate (at left) that individual spikes following a transition from negative to positive velocity jitter from trial to trial by ∼1 ms: The standard deviations of spike times shown here are 0.72 ms for the first spike (•), 0.81 ms for the second spike (°), and 1.22 ms for the third spike (×). When we align the first spikes in this window, we see (at right) that the jitter of interspike intervals is even smaller, 0.21 ms for the first interval and 0.69 ms for the second interval. Our challenge is to quantify the information content of such precise responses.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.g001" xlink:type="simple"/></fig><p>Precise spike timing endows each neuron with a huge “vocabulary” of responses <xref ref-type="bibr" rid="pcbi.1000025-Rieke1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-MacKay1">[2]</xref>, but this potential advantage in coding capacity creates challenges for experimental investigation. If we look with a time resolution of τ = 1 ms, then in each bin of size <italic>τ</italic> we can see either zero or one spike; across the behaviorally relevant time scale of 30 ms <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref> the neural response thus can be described as a 30-bit binary word, and there are 2<sup>30</sup>, or roughly one billion such words. Although some of these responses never occur (because of refractoriness), and others are expected to occur only with low probability, it is clear that if precise timing is important then neurons can generate many more meaningfully distinguishable responses than the number that we can sample in realistic experiments.</p></sec><sec id="s2b"><title>Progress in information estimation</title><p>Can we make progress on assessing the information content and meaning of neural responses even when we can't sample all of them? Recall that the information content is measured by the mutual information between the response and the stimulus that caused it <xref ref-type="bibr" rid="pcbi.1000025-Shannon1">[23]</xref>. This quantity measures (in bits) the reduction in the length of the description of the response spike train caused by knowing the associated velocity stimulus. Thus this mutual information is a difference of entropies <xref ref-type="bibr" rid="pcbi.1000025-Shannon1">[23]</xref> of the ensembles of all possible responses and the responses conditional on particular stimuli. Therefore, the problem of estimation of the information content of spike trains is essentially a problem of estimating the entropy of a probability distribution. This is known to be very hard when sampling is scarce, as in our problem <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>.</p><p>Some hope is provided by the classical problem of how many people need to be present in a room before there is a reasonable chance (about 50%) that at least two of them share a birthday. This number, which turns out to be <italic>N</italic>∼23, is vastly less than the number of possible birthdays, <italic>K</italic> = 365. Turning this argument around, if we didn't know the number of possible birthdays we could estimate it by polling <italic>N</italic> people and checking the frequency of birthday coincidences. Once <italic>N</italic> is large enough to generate several coincidences we can get a pretty good estimate of <italic>K</italic>, and, for <italic>K</italic>→∞, this happens when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e001" xlink:type="simple"/></inline-formula>. Some years ago Ma proposed that this coincidence counting method be used to estimate the entropy of physical systems from molecular dynamics or Monte Carlo simulations <xref ref-type="bibr" rid="pcbi.1000025-Ma1">[25]</xref> (see also <xref ref-type="bibr" rid="pcbi.1000025-Seber1">[26]</xref>). If these arguments could be generalized, it would become feasible to estimate the entropy and information content of neural responses even when experiments provide only a sparse sampling of these responses. The results of <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Nemenman2">[11]</xref> provide such a generalization.</p><p>To understand how the methods of <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref> generate more accurate entropy estimates from small samples, it is useful to think about the simpler problem of flipping a coin under conditions where we don't know the probability <italic>p</italic> that it will come up heads. One strategy is to count the number of heads <italic>n<sub>H</sub></italic> that we see after <italic>N</italic> flips, and identify <italic>p</italic> = <italic>n<sub>H</sub></italic>/<italic>N</italic>; if we then use this “frequentist” or maximum likelihood estimate to compute the entropy of the underlying distribution, it is well known that we will underestimate the entropy systematically <xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Miller1">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Treves1">[28]</xref>. Alternatively, we could take a Bayesian approach and say that a priori all values of 0&lt;<italic>p</italic>&lt;1 are equally likely; the standard methods of Bayesian estimation then will generate a mean and an error bar for our estimate of the entropy given <italic>N</italic> observations. As shown in <xref ref-type="fig" rid="pcbi-1000025-g002">Figure 2</xref>, this procedure actually leads to a systematic <italic>overestimate</italic> of the entropy in cases where the real entropy is not near its maximal value. More seriously, this systematic error is larger than the error bars that emerge from the Bayesian analysis, so we would be falsely confident in the wrong answer.</p><fig id="pcbi-1000025-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000025.g002</object-id><label>Figure 2</label><caption><title>Systematic errors in entropy estimation.</title><p>We consider a coin with unknown probability <italic>p</italic> of coming up heads; from <italic>N</italic> coin flips we try to estimate the entropy <italic>S</italic> = −<italic>p</italic>log<sub>2</sub> <italic>p</italic>−(1−<italic>p</italic>)log<sub>2</sub> (1−<italic>p</italic>); see <xref ref-type="sec" rid="s4">Methods</xref> for details of the calculations. At left, we make Bayesian estimates starting from the prior hypothesis that all values of <italic>p</italic> are equally likely, P(<italic>p</italic>) = 1. We show how the best estimate <italic>S</italic>' differs from the true value <italic>S</italic><sub>0</sub> when this deviation is measured in units of the estimated error bar <italic>δS</italic> (posterior standard deviation); the color bar indicates the value of this scaled deviation. For small numbers of samples, the best estimate is systematically in error by more than two times the size of the error bar, so we would have false confidence in a wrong answer, even at intermediate values of the entropy, which are most relevant for real data. At right, we repeat the same procedure but with a prior hypothesis that all possible value of the entropy are equally likely, P(<italic>S</italic>) = 1. Systematic errors still appear, but they are more nearly compatible with the error bars, even at small <italic>N</italic>, and especially in the range of entropies, which is relevant to our experiments. Notice that here the distinction between the estimators extends to <italic>N</italic>∼<italic>K</italic> = 1; similarly, we expect the uniformization of P(<italic>S</italic>) to be advantageous when <italic>N</italic>&lt;<italic>K</italic> even if <italic>K</italic>&gt;&gt;1.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.g002" xlink:type="simple"/></fig><p><xref ref-type="fig" rid="pcbi-1000025-g002">Figure 2</xref> also shows us that if we use a Bayesian approach with the a priori hypothesis that all values of the <italic>entropy</italic>, rather than <italic>p</italic>, are equally likely, then (and as far as we know, only then) we find estimates such that the systematic errors are comparable to or smaller than the error bars, even when we have seen only one sample. Thus the problem of systematic errors in entropy estimation is not, as one might have thought, the problem of not having seen all the possibilities; the problem rather is that seemingly natural and unbiased prior hypotheses about the nature of the underlying probabilities correspond to highly biased hypotheses about the entropy itself, and this problem gets much worse when we consider distributions over many alternatives. The strategy of <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref> thus is to construct, at least approximately, a ‘flat prior’ on the entropy (see <xref ref-type="sec" rid="s4">Methods</xref> for details). The results of <xref ref-type="bibr" rid="pcbi.1000025-Nemenman3">[12]</xref> demonstrate that this procedure actually works for both simulated and real spike trains, where ‘works’ means that we generate estimates that agree with the true entropy within error bars even when the number of samples is much smaller than the number of possible responses. As expected from the discussion of the birthday problem, what is required for reliable estimation is that the number of coincidences be significantly larger than one <xref ref-type="bibr" rid="pcbi.1000025-Nemenman2">[11]</xref>.</p><p>We note that this estimation method is substantially different from other recent approaches, such as <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Victor1">[29]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Kennel1">[30]</xref>, and we discuss the differences in some detail in the Discussion.</p></sec><sec id="s2c"><title>Words, entropy and information</title><p>The tools described above allow us to estimate the entropy of neural responses. We first analyze a long experiment in which the fly experiences a continuous trajectory of motion with statistics modeled on those of natural flight trajectories (<xref ref-type="fig" rid="pcbi-1000025-g003">Figure 3</xref>; see <xref ref-type="sec" rid="s4">Methods</xref> for details). As shown in <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4A</xref>, we examine segments of the response of duration <italic>T</italic>, and we break these segments into discrete bins with time resolution <italic>τ</italic>. For sufficiently small <italic>τ</italic>, each bin either has one or zero spikes, and hence the response becomes a binary word with <italic>T</italic>/<italic>τ</italic> bits, while in the opposite limit we let <italic>τ</italic> = <italic>T</italic>, and then the response is the total number of spikes in a window of size <italic>T</italic>; for intermediate values of <italic>τ</italic>, the responses are multi-letter words, but with larger than binary alphabet when more than one spike can occur within a single bin. An interesting feature of these words is that they occur with a probability distribution similar to the distribution of words in English (Zipf's law; <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4B</xref>). This Zipf-like behavior emerges only for <italic>T&gt;</italic>20 ms, and was not observed in experiments with less natural, white noise stimuli <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>.</p><fig id="pcbi-1000025-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000025.g003</object-id><label>Figure 3</label><caption><title>Constructing a naturalistic stimulus.</title><p>(A) Digitized version of original video tracking data by Land and Collett <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>. The panel shows traces of a leading fly (blue) and a chasing fly (green). Successive points along the trajectories were recorded at 20 ms intervals. Every tenth point along each trajectory is indicated by a number. From these traces we estimated rotational velocities of the body axis by calculating the angular change in orientation of the trajectory from one point in the sequence to the next, and dividing by 20 ms. The result of this calculation for the leading fly is shown in panel (B). (C) &gt;From these data (on both flies) we constructed a joint distribution, <italic>P</italic>(<italic>V<sub>k</sub></italic>,<italic>V<sub>k</sub></italic><sub>+1</sub>), of successive velocities taken 20 ms apart. (D) Short sample of a trajectory constructed using the distribution in (C) as a Markov process, and then interpolating the velocity trace to 2 ms resolution. (E) Probability densities of angular velocity generated from this Markov process (black dashed line) and scaled down by a factor of two (black line) to avoid destabilizing the experiment; distributions are symmetric and we show only positive velocities. For comparison we show (red line) the distribution of angular velocities recorded for head motion of <italic>Calliphora</italic> during episodes of saccadic turning <xref ref-type="bibr" rid="pcbi.1000025-vanHateren1">[20]</xref>. (F) Power spectrum of synthesized velocity signal, demonstrating the absence of power above 30 Hz. (G) As in (E) but for the accelerations. Note that the distribution of our synthesized and scaled signal was surprisingly close to that for saccadic head motions, as reported in <xref ref-type="bibr" rid="pcbi.1000025-vanHateren1">[20]</xref>.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.g003" xlink:type="simple"/></fig><fig id="pcbi-1000025-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000025.g004</object-id><label>Figure 4</label><caption><title>Words, entropy and information in the neural response to natural signals.</title><p>(A) Schematic showing how we convert the sequence of action potentials into discrete ‘words’, that is, sequences of zeros and ones <xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck2">[31]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>. As an example, at the top we show the stimulus and spike arrival times (red dots) in a 64 ms segment of the experiment. We may treat this as two successive segments of duration <italic>T</italic> = 32 ms, and divide these segments into bins of duration <italic>τ</italic> = 2, 8, or 32 ms. For sufficiently small <italic>τ</italic> (here, <italic>τ</italic> = 2 ms), each bin contains either zero or one spike, and so each neural response becomes a binary word with <italic>T</italic>/<italic>τ</italic> bits; larger values of τ generate larger alphabets, until at <italic>τ</italic> = <italic>T</italic> the response of the neuron is just the spike count in the window of duration <italic>T</italic>. Note that the words are shown here as non-overlapping; this is just for graphical convenience. (B) The distribution of words with <italic>τ</italic> = 1 ms, for various values of <italic>T</italic>; words are plotted in rank order. We see that, for large <italic>T</italic> (<italic>T</italic> = 40 or 50 ms) but not for small <italic>T</italic> (<italic>T</italic> = 20 ms), the distribution of words had a large segment in which the probability of a word is <italic>P</italic> ∝ 1/rank<italic><sup>∝</sup></italic>, corresponding to a straight line on this double logarithmic plot. Similar behavior is commonly observed for words in English, with <italic>α</italic> = 1, which we show for comparison (solid line); this is sometimes referred to as Zipf's law <xref ref-type="bibr" rid="pcbi.1000025-Zipf1">[48]</xref>. (C) The entropy of a <italic>T</italic> = 25 ms segment of the spike train, as a function of the time resolution <italic>τ</italic> with which we record the spikes. We plot this as an entropy rate, <italic>S</italic>(<italic>T</italic>,<italic>τ</italic>)/<italic>T</italic>, in bits/s; this value of <italic>T</italic> was chosen because this is the time scale on which visual motion drives motor behavior <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>. For comparison we show the theoretical results (valid at small <italic>τ</italic>) for a Poisson process <xref ref-type="bibr" rid="pcbi.1000025-Rieke1">[1]</xref>, and a Poisson process with a refractory period <xref ref-type="bibr" rid="pcbi.1000025-Nemenman3">[12]</xref>, with spike rates and refractory periods matched to the data. Note that the real spike train has significantly less entropy than do these simple models. In <xref ref-type="bibr" rid="pcbi.1000025-Nemenman3">[12]</xref> we showed that our estimation methods can recover the correct results for the refractory Poisson model using data sets comparable in size to the one analyzed here; thus our conclusion that real entropies are smaller cannot be the result of undersampling. Error bars are smaller than the data points. (d) The information content of <italic>T</italic> = 25 ms words, as a function of time resolution <italic>τ</italic>; again we plot this as a rate <italic>R</italic><sub>info</sub>(<italic>T</italic>,<italic>τ</italic>) = <italic>I</italic>(<italic>T</italic>, <italic>τ</italic>)/<italic>T</italic>, in bits/s.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.g004" xlink:type="simple"/></fig><p>With a fixed value of <italic>T</italic>, improving our time resolution (smaller <italic>τ</italic>) means that we distinguish more alternatives, increasing the “vocabulary” of the neuron. Mathematically this means that the entropy <italic>S</italic>(<italic>T</italic>,<italic>τ</italic>) of the neural responses is larger, corresponding to a potentially larger capacity for carrying information. This is shown quantitatively in <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4C</xref>, where we plot the entropy rate, <italic>S</italic>(<italic>T</italic>,<italic>τ</italic>)/<italic>T</italic>. The question of whether precise spike timing is important in the neural code is precisely the question of whether this capacity is used by the system to carry information <xref ref-type="bibr" rid="pcbi.1000025-MacKay1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>.</p><p>To estimate the information content of the neural responses, we followed the strategy of <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck2">[31]</xref>. The information content of the ‘words’ generated by the neuron is always less than the total size of the neural vocabulary because there is some randomness or noise in the association of words with sensory stimuli. To quantify this noise we choose a five second segment of the stimulus, and then repeat this stimulus 100 times. At each moment 0&lt;<italic>t</italic>&lt;5 s in the cycle of the repeated stimulus, we look across the one hundred trials to sample the different possible responses to the same input, and with the same mathematical methods as before, we use these samples to estimate the ‘noise entropy’ <italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>) in this ‘slice’ of responses. The information which the responses carry about the stimulus then is given by <italic>I</italic>(<italic>T</italic>,<italic>τ</italic>)<italic> = S</italic>(<italic>T</italic>,<italic>τ</italic>)−〈<italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>T</italic>)〉<italic><sub>t</sub></italic>, where 〈…〉<italic><sub>t</sub></italic> denotes an average over time <italic>t</italic>, which implicitly is an average over stimuli. It is convenient to express this as an information rate <italic>R</italic><sub>info</sub> (<italic>T</italic>,<italic>τ</italic>) = <italic>I</italic>(<italic>T</italic>,<italic>τ</italic>)/<italic>T</italic>, and this is what we show in <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4D</xref>, with <italic>T</italic> = 25 ms, chosen to reflect the time scale of behavioral decisions <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>.</p><p>The striking feature of <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4D</xref> is the growth of information rate with time resolution. We emphasize that this measurement is made under conditions comparable to those which the fly encounters in nature—outdoors, in natural light, moving along trajectories with statistics similar to those observed in free flight. Thus under these conditions, we conclude that the fly's visual system carries information about motion in the timing of spikes down to sub-millisecond resolution. Quantitatively, information rates double as we increase our time resolution from <italic>τ</italic> = 25 ms to below a millisecond, and the final ∼30% of this increase occurs between <italic>τ</italic> = 2 ms and <italic>τ</italic>≤0.5 ms. In the behaviorally relevant time windows <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>, this 30% extra information corresponds to almost a full bit from this one cell, which would provide the fly with the ability to distinguish reliably among twice as many different motion trajectories.</p></sec><sec id="s2d"><title>What do the words mean?</title><p>The information rate tells us <italic>how much</italic> we can learn about the sensory inputs by examining the neural response, but it doesn't tell us <italic>what</italic> we learn. In particular, we would like to make explicit the nature of the extra information that emerges as we increase our time resolution from <italic>τ</italic> = 2 ms to <italic>τ</italic>&lt;1 ms. In other words, we should look at what additional features of the stimulus are encoded by finer spike timing. In the following we will present examples to highlight some of these features. We look at particular “words” in a segment of the neural response, as shown in <xref ref-type="fig" rid="pcbi-1000025-g005">Figure 5</xref>, and then examine the motion trajectories that corresponded to these words <xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck3">[32]</xref>. For simplicity, we consider all responses that had two spikes in successive 2 ms bins, that is the binary pattern 11 when seen at <italic>τ</italic> = 2 ms resolution. When we improve our time resolution to <italic>τ</italic> = 0.2 ms, some of these responses turn out to be of the form 10000000000000000001, while at the other extreme some of the responses have the two spikes essentially as close as possible given the refractory period, 00000100000000100000. Remarkably, as we sweep through these subtly different patterns—which all have the same average spike arrival time but different interspike intervals—the average velocity trajectory changes form qualitatively, from a smooth “on” (negative to positive velocity) transition, to a prolonged period of positive velocity, to a more complex waveform with off and on transitions in succession. Examining more closely the distribution of waveforms conditional on the different responses, we conclude that these differences among mean waveforms are in fact discriminable. Thus, variations in interspike interval on the millisecond or sub-millisecond scale represent significantly different stimulus trajectories.</p><fig id="pcbi-1000025-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000025.g005</object-id><label>Figure 5</label><caption><title>Fine spike timing differences and response conditional ensembles <xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck3">[32]</xref>.</title><p>We consider five different neural responses, all of which are identical when viewed at <italic>τ</italic> = 2 ms resolution, corresponding to the binary pattern 11, spikes in two successive bins. At left, we consider responses which, at higher time resolution, correspond to different interspike intervals. At right, the interspike interval is fixed but higher time resolution revealed that the absolute spike arrival times differ. In each case, we compute the median motion trajectory conditional on the high time resolution response (lines) and we indicate the width of the distribution with bars that range plus and minus one quartile around the median. It is clear that changes in interspike interval encode changes in the distribution of stimulus waveform that are discriminable, since the mid-quartiles do not overlap. Changes in absolute timing are more subtle, and so we estimate the conditional distributions of velocity at each moment in time using the methods of <xref ref-type="bibr" rid="pcbi.1000025-Nemenman4">[49]</xref>, compute the overlap of these distributions, and convert the result into the equivalent signal-to-noise ratio <italic>d'</italic> for discrimination against Gaussian noise <xref ref-type="bibr" rid="pcbi.1000025-Green1">[50]</xref>; that is <italic>d'</italic> is a distance between the means of two unit variance Gaussians that have the same overlap as the distributions in question. Note that we compute this discriminability using single points in time; <italic>d'</italic> values based on extended segments of the waveforms would be even higher.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.g005" xlink:type="simple"/></fig><p>A second axis along which we can study the nature of the extra information at high time resolution concerns the absolute timing of spikes. As an example, responses which at <italic>τ</italic> = 2 ms resolution are of the form 11 can be unpacked at <italic>τ</italic> = 0.2 ms resolution to give patterns ranging from 01000000001000000000 to 00000000010000000010, all with the same interspike interval but with different absolute arrival times. As shown in <xref ref-type="fig" rid="pcbi-1000025-g005">Figure 5</xref>, all of these responses code for motion trajectories with two zero crossings, but the times of these zero crossings shift as the spike arrival times shift. Thus, whereas the times between spikes represent the shape of the waveform, the absolute arrival time of the spikes marks, with some latency, the time at which a specific feature of the waveform occurs, in this case a zero crossing. Again we find that millisecond and sub-millisecond scale shifts generate discriminable differences.</p><p>The idea that sub-millisecond timing of action potentials can carry significant information is not new, but the clearest evidence comes from systems in which the dynamics of the stimulus itself has significant sub-millisecond structure, as in hearing and electroreception <xref ref-type="bibr" rid="pcbi.1000025-Carr1">[6]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Carr2">[33]</xref>. For slow stimuli, the best recorded temporal precision is generally a few milliseconds, and is observed very early in the sensory processing <xref ref-type="bibr" rid="pcbi.1000025-Reich1">[34]</xref>. Even for H1, experiments demonstrating the importance of spike timing at the ∼2 ms level <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Brenner1">[35]</xref> could be criticized on the grounds that the stimuli had unnaturally rapid variations. It is thus important to emphasize that, in the experiments described here, H1 did not achieve millisecond precision simply because the input had a bandwidth of about a kiloHertz; in fact, the stimulus had a correlation time of ∼55 ms (<xref ref-type="fig" rid="pcbi-1000025-g006">Figure 6</xref>), and 99.9% of the stimulus power was contained below 30 Hz (<xref ref-type="fig" rid="pcbi-1000025-g003">Figure 3F</xref>). We are not aware of previous results where sub-millisecond temporal precision has been explicitly shown to encode such slow stimuli.</p><fig id="pcbi-1000025-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000025.g006</object-id><label>Figure 6</label><caption><title>Redundancy reduction in the time domain.</title><p>We measure the redundancy <italic>Y<sub>I</sub></italic>(<italic>T</italic>,<italic>τ</italic>) (points with error bars) between words of length <italic>T</italic> in the neural response, as explained in the text. To allow exploration of large <italic>T</italic> we work at a time resolution <italic>τ</italic> = 3 ms. The redundancy is compared to correlations in the stimulus <italic>Y<sub>v</sub></italic> = 〈<italic>v</italic>(<italic>t</italic>+<italic>T</italic>)<italic>v</italic>(<italic>t</italic>)〉/〈<italic>v</italic><sup>2</sup>〉 (dotted line) or correlations in the spike rate <italic>Y<sub>γ</sub></italic> = 〈<italic>δr</italic>(<italic>t</italic>+<italic>T</italic>)<italic>δr</italic>(<italic>t</italic>)〉/〈<italic>δr</italic><sup>2</sup>〉 (dashed line). Note that the redundancy decays rapidly—we show an exponential fit with a time constant of 17.3 ms. In contrast, the correlations both in the stimulus and the firing rate decay much more slowly—the solid line, for comparison, shows an exponential decay with a time constant of 53.4 ms. Correlations in spike rate are calculated from a separate experiment on the same cell, with 200 repetitions of a 10 s stimulus drawn from the same distribution, that generated more accurate estimates of <italic>r</italic>(<italic>t</italic>).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.g006" xlink:type="simple"/></fig></sec><sec id="s2e"><title>Redundancy reduction</title><p>The long correlation time of these naturalistic stimuli also raises questions about redundancy—while each spike pattern considered in isolation may be highly informative, the long correlation time of the stimulus could very well mean that successive patterns carry information about essentially the same value of the instantaneous velocity. If so, that would mean that successive symbols are significantly redundant. Certainly on very short time scales this is true: Although <italic>R</italic><sub>info</sub>(<italic>T</italic>,<italic>τ</italic>) actually increases at small <italic>T</italic> since larger segments of the response reveal more informative patterns of several spikes <xref ref-type="bibr" rid="pcbi.1000025-Brenner1">[35]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Reinagel1">[36]</xref>, it does decrease at larger <italic>T</italic>, a clear sign of redundancy. However, this approach to a constant information rate is very fast: We measure the redundancy on time scale <italic>T</italic> by computing <italic>Y<sub>I</sub></italic>(<italic>T</italic>,<italic>τ</italic>) = 2<italic>I</italic>(<italic>T</italic>,<italic>τ</italic>)/(2<italic>T</italic>,<italic>τ</italic>)−1, where <italic>Y<sub>I</sub></italic> = 0 signifies that successive windows of size <italic>T</italic> provide completely independent information, and <italic>Y<sub>I</sub></italic> = 1 that they are completely redundant. As shown in <xref ref-type="fig" rid="pcbi-1000025-g006">Figure 6</xref>, <italic>Y<sub>I</sub></italic>(<italic>T</italic>,<italic>τ</italic>) decays rapidly, on a time scale of less than 20 ms. In contrast, correlations in the stimulus itself decay much more slowly, on the ∼55 ms time scale, and we find that the time dependent spike rate <italic>r</italic>(<italic>t</italic>) essentially has the same correlation time as the stimulus. The fact that coding redundancy decays three times more rapidly than the correlations of the time dependent firing rate indicates that the decorrelation of information is a process more intricate than simply filtering the stimulus. It suggests that there may be an adaptational mechanism at play that increases the overall efficiency of coding by exploiting the difference in time scales between stimulus changes and spike timing precision. If correct, this would imply that we should interpret neural firing patterns in context: The same pattern could signify slightly different stimulus depending on what went on before. This point merits further study, and may lead to further refinements in how we should interpret neural firing patterns, such as those shown in <xref ref-type="fig" rid="pcbi-1000025-g005">Figure 5</xref>. As far as we know this is the first direct information theoretic demonstration of temporal redundancy reduction in the context of neural coding.</p></sec><sec id="s2f"><title>Bit rates and photon counting rates</title><p>The ability of the fly's visual system to mark features of the stimulus with millisecond precision, even at a ∼55 ms stimulus correlation time, was demonstrated in conditions where the visual input had very high signal-to-noise ratio. Previous work has suggested that this system can estimate motion with a precision close to the limits set by noise in the photoreceptors <xref ref-type="bibr" rid="pcbi.1000025-Bialek1">[37]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck4">[38]</xref>, which is dominated by photon shot noise <xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck5">[39]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-deRuytervanSteveninck6">[40]</xref>. The present experiments, however, were done under very different conditions: Velocities of motion were much larger, the fly's eye was stimulated over a much larger area, and light intensities outdoors were much larger than generated by laboratory displays. Light intensities in our experiment were estimated to correspond to up to about 1.1·10<sup>6</sup> transduced photon/s per photoreceptor (see <xref ref-type="sec" rid="s4">Methods</xref>). Is it possible that photon counting statistics are limiting the precision of H1, even at these high rates?</p><p>Because the experiments were done outdoors, there were small fluctuations in light intensity from trial to trial as clouds drifted by and obscured the sun. Although the range of these fluctuations was less than a factor two, the arrival times of individual spikes (e.g., the “first spike” after <italic>t</italic> = 1.75 s in <xref ref-type="fig" rid="pcbi-1000025-g001">Figure 1</xref>) had correlation coefficients of up to ρ = −0.42 with the light intensity, with the negative sign indicating that higher light intensities led to earlier spikes. One might see this effect as a failure of the system to adapt to the overall light intensity, but it also suggests that some of what we have called noise really represents a response to trial-by-trial variations in stimulus conditions. Indeed, a correlation between light intensity and spike time implies that the noise entropy <italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>) in windows which contain these spikes has a significant contribution from stimulus variation, and should thus be smaller when this source of variation is absent.</p><p>More subtly, if photon shot noise is relevant, we expect that, on trials with higher light intensity, the neuron will actually convey more information about the trajectory of motion. We emphasize that this is a delicate question. To begin, the differences in light intensity were small, and we expect (at most) proportionately small effects. Further, as the light intensity increased, the total spike rate increased. Interestingly, this increased both the total entropy and the noise entropy. To see if the system used the more reliable signal at higher light intensities to convey more information, we have to determine which of these increases is larger.</p><p>To test the effects of light intensity on information transmission (see <xref ref-type="sec" rid="s4">Methods</xref> for details), we divide the trials into halves based on the average light intensity over the trial, and we try to estimate the information rates in both halves; the two groups of trials differ by just 3% in their median light intensities. Since cutting the number of trials in half makes our sampling problems much worse, we focus on short segments of the response (<italic>T</italic> = 6 ms) at high time resolution (τ = 0.2 ms); note that these are still “words” with 30 letters. For this case we find that for the trials with higher light intensities the information about the motion stimulus is larger by Δ = 0.0204±0.0108 bits, which is small but significant at the 94% confidence level. We find differences with the same sign for all accessible combinations of <italic>T</italic> and <italic>τ</italic>, and the overall statistical significance of the difference thus is much larger. Note that since we were analyzing <italic>T</italic> = 6 ms windows, this difference correspond to Δ<italic>R</italic>∼3 bits/s, 1–2% of the total (cf. <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4</xref>). Thus even at rates of more than one million photons per second per receptor cell, small increases in photon flux produce proportionally small, yet measurable increases in the transmission of information about the motion stimulus.</p></sec></sec><sec id="s3"><title>Discussion</title><p>We have found that under natural stimulus conditions the fly visual system generates spikes and interspike intervals with extraordinary temporal precision. As a consequence, the neural response carries a substantial amount of information that is available only at sub-millisecond time resolution. At this high resolution, absolute spike timing is informative about the time at which particular stimulus features occur, while different interspike intervals provide a rich representation of distinguishable stimulus features. These results clearly demonstrate that the visual system uses sub-millisecond timing to paint a more accurate picture of the natural sensory world, at least in this corner of the fly's brain. We emphasize again that here the sub-millisecond precision is not a result of an equally fast stimulus dynamics since the stimulus, in fact, has essentially no power at these frequencies. This is an important distinction, discussed in detail in <xref ref-type="bibr" rid="pcbi.1000025-Theunissen1">[41]</xref>. In addition, an equally important observation is that the system performs efficiently both in the tasks of estimation and of coding, making use of the extra signal-to-noise provided by increased photon flux, even at daylight levels of light intensity. Perhaps of most interest, the analysis has made it possible to demonstrate a qualitative feature of the neural code in this system, namely the encoding of a temporally redundant stimulus in a neural signal of much shorter correlation time. At this point we can only speculate about the functional implications of this phenomenon, but at the very least it should give us pause in interpreting the code. Further study may reveal it to be an important feature of sensory coding and computation more generally, in particular under natural conditions where signals have high dynamic range, and show dramatic variations in reliability. We hope to be able to develop these ideas in more detail in the near future.</p><p>Finally, we note that our ability to reach these conclusions depends not just on new experimental methods that allow us to generate truly naturalistic stimuli <xref ref-type="bibr" rid="pcbi.1000025-Lewen1">[9]</xref>, but critically on new mathematical methods that allow us to analyze neural responses quantitatively even when it was impossible for us to sample the distribution of responses exhaustively <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Nemenman3">[12]</xref>. The theoretical tools presented here were developed with the explicit aim of being efficient in estimating entropies in the severely undersampled regime. This is crucial in neurophysiological experiments, where large stable datasets are very difficult to obtain. Most previously described entropy estimation methods, such as <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1000025-Miller1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1000025-Kennel1">[30]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Victor2">[42]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Batu1">[43]</xref>, and others reviewed in <xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>, have relied on one of three different ways to overcome the undersampling problem. Some, for example <xref ref-type="bibr" rid="pcbi.1000025-Victor1">[29]</xref>, have chosen to define a metric on the space of responses, which makes it possible to “regularize” the problem by imposing similarity among probabilities of similar outcomes. Others, like <xref ref-type="bibr" rid="pcbi.1000025-Kennel1">[30]</xref>, explore generative models for the data, which serves a similar regularizing function. Both approaches work well if and only if the underlying choices match the properties of the real data. The majority of recent approaches, such as <xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>, follow the third route and rely essentially on applying 1/<italic>N</italic> asymptotic corrections to the maximum likelihood estimator which means that they require mean bin occupancies <italic>O</italic>(1) to work. That leads to severe, and often impractical, demands on the size of the datasets as the cost of guaranteeing an estimator's performance. In contrast, the estimator presented here is based on counting coincidences, which still will occur even if the mean occupancy is much less than one. While we know that, in the worst case, even coincidence-based approaches may still require <italic>O</italic>(1) samples per possible outcome to produce low-bias and low-variance entropy estimates <xref ref-type="bibr" rid="pcbi.1000025-Wyner1">[44]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Paninski1">[24]</xref>, they may require substantially less data in simpler cases (in the best case scenario, to reach equal levels of resolution, the number of independent samples in the data set scales as the square-root of the number required by the other estimation methods. Or alternatively, with the same size dataset, the timing resolution is better by a factor of two.) For the data studied here, Nature cooperated: for example, to estimate noise entropies we use 100 samples for repeated stimuli for binary words of length 30 or more, so that the mean occupancy is &lt;10<sup>−7</sup>. However, the success of the method could not have been predicted a priori, and the majority of our computational effort was spent not on calculation of information rates per se, but on answering the very delicate question of whether the NSB method can be trusted to have small bias for our data. This is why we caution the reader from using NSB as a simple black-box estimation tool, without checking if it really works first. Finally, we notice that our method for estimating entropies bears some resemblance to the work of Wolpert and Wolf <xref ref-type="bibr" rid="pcbi.1000025-Wolpert1">[45]</xref>, who used a single-beta Dirichlet prior to estimate functions of sparsely sampled probability distributions. A crucial distinction, however, is that instead of a single prior we use a <italic>family</italic> of Dirichlet priors to construct a prior <italic>distribution</italic> of entropies that is approximately flat (see <xref ref-type="sec" rid="s4">Methods</xref>). We believe that, without a similar flattening of the distribution of entropies, any Bayesian method is bound to have large biases below bin occupancies of <italic>O</italic>(1).</p><p>Information theoretic approaches force us to formulate questions and quantify observations in unbiased ways. Thus, success in solving a problem in an information theoretic context leads to results of great generality. But success in an experimental context hinges on the solution of practical problems. We hope that the methods presented here contribute to solving an important practical problem, and will be a step toward wider application of information theoretic methods in neuroscience.</p></sec><sec id="s4"><title>Methods</title><sec id="s4a"><title>Neural recording and stimulus generation</title><p>H1 was recorded extracellularly by a short (12 mm shank length) tungsten electrode (FHC). The signal was preamplified by a differential bandpass instrumentation amplifier based on the INA111 integrated circuit (Burr-Brown). After amplification by a second stage samples were digitized at 10 kHz by an AD converter (National Instruments DAQCard-AI-16E-4, mounted in a Fieldworks FW5066P ruggedized laptop). In off line analysis, the analog signal was digitally filtered by a template derived from the average spike waveform. Spikes were then time stamped by interpolating threshold crossing times. The ultimate precision of this procedure was limited by the signal to noise ratio in the recording; for typical conditions this error was estimated to be 50–100 µs. Note that we analyzed spike trains down to a precision of τ = 200 µs, so that some saturation of information at this high time resolution may have actually resulted from instrumental limitations. The experiments were performed outside in a wooded environment, with the fly mounted on a stepper motor with vertical axis. The speed of the stepper motor was under computer control, and could be set at 2 ms intervals. The DAQ card generated a 500 Hz clock signal divided down from the same master clock that governs the AD sample rate. The stepper motor (SIG-Positec RDM566/50, 10,000 pulses per revolution, or 0.036°/pulse) was driven by a controller (SIG-Positec Divistep D331.1), which received pulses at a frequency divided down from a free running 8 MHz clock. Over the short time interval (<italic>t</italic>,<italic>t</italic>+2 ms) the stimulus velocity <italic>v</italic>(<italic>t</italic>) was determined by the pulse frequency, <italic>f</italic>(<italic>t</italic>), that the controller received. This in turn was set by the numerical value, <italic>N<sub>div</sub></italic>(<italic>t</italic>), of a divisor: <italic>f</italic>(<italic>t</italic>) = 8MHZ/<italic>N<sub>div</sub></italic>(<italic>t</italic>), and <italic>v</italic>(<italic>t</italic>) = (0.036) · <italic>f</italic>(<italic>t</italic>) °/s. Successive values of <italic>N<sub>div</sub></italic>(<italic>t</italic>) were read every 2 ms from a stimulus file stored on a dedicated laptop computer. In this way, each 2 ms period the stepper motor speed was set to a value read from computer, keeping long-term synchrony with the data acquisition clock, with a maximum jitter of 1/(8 MHz) = 125 ns. The method for delivering pulses to the motor controller minimized the jerkiness of the motion by spacing the controller pulses evenly over each 2 ms interval. This proved to be crucial for maintaining stability of the electrophysiological recording.</p></sec><sec id="s4b"><title>Controlling temperature</title><p>To stabilize temperature the setup was enclosed by a transparent plexiglass cylinder (radius 15 cm, height 28 cm), with a transparent plexiglass lid. The air temperature in the experimental enclosure was regulated by a Peltier element fitted with heat vanes and fans on the inside and outside for efficient heat dispersal, and driven by a custom built feedback controller. The temperature was measured by a standard J-type thermocouple, and could be regulated over a range from some five degrees below to fifteen degrees above ambient temperature. The controller stabilized temperature over this range to within about a degree. In the experiments described here, temperature was 23±1°C.</p></sec><sec id="s4c"><title>Monitoring light intensity</title><p>A running overall measure of light intensity was obtained by monitoring the current of a photodiode (Hamamatsu S2386-44K) enclosed in a diffusing ping pong ball. After a current to voltage conversion stage, the photodiode signal was amplified by a logarithmic amplifier (Burr-Brown LOG100) operating over five decades. The probe was located ∼50 cm from the fly, and in the experiments the setup was always placed in the shade. The photodiode measurement was intended primarily to get a rough impression of relative light intensity fluctuations. To relate these measurements to outside light levels, at the start of each experiment a separate calibration measurement of zenith radiance was taken with a calibrated radiometer (International Light IL1400A using silicon detector SEL033/F/R, with radiance barrel). The radiance measurement was done over a limited spectral band defined by a transmission filter (International Light, WBS480) and an infrared absorption filter. In this way the radiometer's spectral sensitivity peaks close to the fly photoreceptor's 490 nm long wavelength maximum. However, it is about 20% broader than the fly's spectral sensitivity peak in the 350–600 nm range, and the photoreceptor's UV peak <xref ref-type="bibr" rid="pcbi.1000025-Minke1">[46]</xref> was not included in this measurement. To relate this radiance measurement to fly physiology, the radiance reading was converted to an estimated effective fly photoreceptor photon rate, computed from the spectral sensitivity of the blowfly R1-6 type photoreceptor <xref ref-type="bibr" rid="pcbi.1000025-Minke1">[46]</xref>, the radiometer's spectral sensitivity and the spectral distribution of sky radiance <xref ref-type="bibr" rid="pcbi.1000025-Menzel1">[47]</xref>. The reading of the photodiode was roughly proportional to the zenith intensity reading, with a proportionality factor determined by the placement of the setup and the time of day. In the experiments, light intensities within the visual field of the fly ranged from about 2% to 100% of zenith intensity. To obtain a practical rule of thumb, the photodiode readings were converted to equivalent zenith photon flux values, using the current to zenith radiance conversion factor established at the beginning of the experiment. During the experiments the photodiode signal was sampled at 1 s intervals.</p></sec><sec id="s4d"><title>Repeated stimuli</title><p>In their now classical experiments, Land and Collett measured the trajectories of flies in free flight <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>; in particular they reported the angular position (orientation) of the fly vs. time, from which we can compute the angular velocity <italic>v</italic>(<italic>t</italic>). The short segments of individual trajectories shown in the published data have a net drift in angle, so we include both the measured <italic>v</italic>(<italic>t</italic>) and −<italic>v</italic>(<italic>t</italic>) as parts of the stimulus. We used the trajectories for the two different flies in <xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4</xref> of <xref ref-type="bibr" rid="pcbi.1000025-Land1">[15]</xref>, and grafted all four segments together, with some zero padding to avoid dramatic jumps in velocity, generating a 5 second long stimulus with zero drift, so that repetition of the angular velocity vs. time also repeated the angular position vs. time. Since Land and Collett reported data every 20 ms, we interpolated to generate a signal that drives the stepper motor at 2 ms resolution; interpolation was done using the MATLAB routine interp, which preserved the bandlimited nature of the original signal and hence did not distort the power spectrum.</p></sec><sec id="s4e"><title>Nonrepeated stimulus</title><p>To analyze the full entropy of neural responses, it is useful to have a stimulus that is not repeated. We would like such a stimulus to match the statistical properties of natural stimulus segments described above. To do this, we estimated the probability distribution <italic>P</italic>[<italic>v</italic>(<italic>t</italic>+Δ<italic>t</italic>)|<italic>v</italic>(<italic>t</italic>)] from the published trajectories, where Δ<italic>t</italic> = 20 ms was the time resolution, and then used this as the transition matrix of a Markov process from which we could generate arbitrarily long samples; our nonrepeated experiment was based on a 990 s trajectory drawn in this way. The resulting velocity trajectories, in particular, had exactly the same distributions of velocity and acceleration as in the observed free flight trajectories. Although the real trajectories are not exactly Markovian, our Markovian approximation also captures other features of the natural signals, for example generating a similar number of velocity reversals per second. Again we interpolated these trajectories to obtain a stimulus at 2 ms resolution.</p></sec><sec id="s4f"><title>Entropy estimation in a model problem</title><p>The problem in <xref ref-type="fig" rid="pcbi-1000025-g002">Figure 2</xref> is that of a potentially biased coin. Heads appear with probability <italic>p</italic>, and the probability of observing <italic>n</italic> heads out of <italic>N</italic> flips is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e002" xlink:type="simple"/><label>(1)</label></disp-formula></p><p>If we observe <italic>n</italic> and try to infer <italic>p</italic>, we use Bayes' rule <xref ref-type="bibr" rid="pcbi.1000025-Rieke1">[1]</xref> to construct<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e003" xlink:type="simple"/><label>(2)</label></disp-formula>where <bold>P</bold>(<italic>p</italic>) is our prior and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e004" xlink:type="simple"/></inline-formula> is a normalization constant, which can be ignored. Given this posterior distribution of <italic>p</italic> we can calculate the distribution of the entropy,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e005" xlink:type="simple"/><label>(3)</label></disp-formula></p><p>We proceed as usual to define a function <italic>g</italic>(<italic>S</italic>) that is the inverse of <italic>S</italic>(<italic>p</italic>), that is <italic>g</italic>(<italic>S</italic>(<italic>p</italic>)) = <italic>p</italic>; since <italic>p</italic> and 1-<italic>p</italic> give the same value of <italic>S</italic>, we choose 0&lt;<italic>g</italic>≤0.5 and let <italic>g</italic> ˜ (<italic>S</italic>) = 1-<italic>g</italic>(<italic>S</italic>). Then we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e006" xlink:type="simple"/><label>(4)</label></disp-formula></p><p>From this distribution, we can estimate a mean <italic>S</italic> ˜<italic><sub>N</sub></italic>(<italic>n</italic>) and a variance <italic>σ</italic>2(<italic>n</italic>,<italic>N</italic>) in the usual way. What interests us is the difference between <italic>S</italic> ˜<italic><sub>N</sub></italic>(<italic>n</italic>) and the true entropy <italic>S</italic>(<italic>p</italic>) associated with the actual value of <italic>p</italic> characterizing the coin; it makes sense to measure this difference in units of the standard deviation δ<italic>S</italic>(<italic>n</italic>,<italic>N</italic>). Thus we compute<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e007" xlink:type="simple"/><label>(5)</label></disp-formula>and this is what is shown in <xref ref-type="fig" rid="pcbi-1000025-g002">Figure 2</xref>. We consider two cases. First, a flat prior on <italic>p</italic> itself, so that <bold>P</bold>(<italic>p</italic>) = 1. Second, a flat prior on the entropy, which corresponds to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e008" xlink:type="simple"/><label>(6)</label></disp-formula></p><p>Here, 1/2 in front of the derivative accounts for two values of <italic>p</italic> being mapped into the same <italic>S</italic>. Note that this prior is (gently) diverging near the limits <italic>p</italic> = 0 and <italic>p</italic> = 1, but all the expectation values that we are interested in are finite.</p></sec><sec id="s4g"><title>Entropy estimation: General features</title><p>Our discussion here follows <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Nemenman3">[12]</xref> very closely. Consider a set of possible neural responses labeled by <italic>i</italic> = 1,2,…,<italic>K</italic>. The probability distribution of these responses, which we don't know, is given by <bold>p</bold> ≡ {<italic>p<sub>i</sub></italic>}. A well studied family of priors on this distribution is the Dirichlet prior, parameterized by β,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e009" xlink:type="simple"/><label>(7)</label></disp-formula></p><p>Maximum likelihood estimation, which identifies probabilities with frequencies of occurrence, is obtained in the limit <italic>β</italic> → 0, while <italic>β</italic> = 1 is the natural “uniform” prior. When <italic>K</italic> becomes large, almost any <bold>p</bold> chosen out of this distribution has an entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e010" xlink:type="simple"/></inline-formula> very close to the mean value,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e011" xlink:type="simple"/><label>(8)</label></disp-formula>where ψ<sub>0</sub>(x) = <italic>d</italic>log<sub>2</sub><italic>Γ</italic>(<italic>x</italic>)/<italic>dx</italic>, and <italic>Γ</italic>(<italic>x</italic>) is the gamma function. We therefore construct a prior that is approximately flat on the entropy itself by a continuous superposition of Dirichlet priors,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e012" xlink:type="simple"/><label>(9)</label></disp-formula>and we then use this prior to perform standard Bayesian inference. In particular, if we observe each alternative <italic>i</italic> to occur <italic>n<sub>i</sub></italic> times in our experiment, then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e013" xlink:type="simple"/><label>(10)</label></disp-formula>and hence by Bayes' rule<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e014" xlink:type="simple"/><label>(11)</label></disp-formula></p><p>Once we normalize this distribution we can integrate over all <bold>p</bold> to give the mean and the variance of the entropy given our data {<italic>n<sub>i</sub></italic>}. In fact, all the integrals can be done analytically except for the integral over β <xref ref-type="bibr" rid="pcbi.1000025-Nemenman1">[10]</xref>,<xref ref-type="bibr" rid="pcbi.1000025-Wolpert1">[45]</xref>. Software implementation of this approach is available from <ext-link ext-link-type="uri" xlink:href="http://nsb-entropy.sourceforge.net/" xlink:type="simple">http://nsb-entropy.sourceforge.net/</ext-link>. This basic strategy can be supplemented in cases where we have prior knowledge about the entropies. In particular, when we are trying to estimate entropy in “words” of increasing duration <italic>T</italic>, we know that <italic>S</italic>(<italic>T</italic>*,<italic>τ</italic>)≤<italic>S</italic>(<italic>T</italic>,<italic>τ</italic>)≤<italic>S</italic>(<italic>T</italic>*,<italic>τ</italic>)+<italic>S</italic>(<italic>T</italic>-<italic>T</italic>*,<italic>τ</italic>) for any <italic>T</italic>*&lt;<italic>T</italic>, and thus it makes sense to constrain the priors at <italic>T</italic> using the results from smaller windows <italic>T</italic>', although this is not critical to our results. We obtain results at all integer values of <italic>T</italic>/<italic>τ</italic> for which our estimation procedure is stable (see below) and use cubic splines to interpolate to non-integer values as needed.</p></sec><sec id="s4h"><title>Entropy estimation: Details for total entropy</title><p>There are two critical challenges to estimating the entropy of neural responses to natural signals. First, the overall distribution of (long) words has a Zipf-like structure (<xref ref-type="fig" rid="pcbi-1000025-g004">Figure 4B</xref>), which is troublesome for most estimation strategies and leads to biases dependent on sample size. Second, the long correlation times in the stimulus mean that successive words ‘spoken’ by the neuron are strongly correlated, and hence it is impossible to guarantee that we have independent samples, as assumed implicitly in Eq. (10). We tamed the long tails in the probability distribution by partitioning the space of responses, estimating entropies within each partition, and then using the additivity of the entropy to estimate the total. We investigated a variety of different partitions, including (a) no spikes vs. all other words, (b) no spikes, all words with one spike, all words with two spikes, etc., (c) no spikes, all words with frequencies of over 1000, and all other words. Further, for each partitioning, we followed <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref> and evaluated <italic>S</italic>(<italic>T</italic>,<italic>τ</italic>) for data sets of different sizes <italic>αN</italic>, 0&lt;<italic>α</italic>≤1. By choosing fractions of the data in different ways we separated the problems of correlation and sample size. That is, to check that our estimates were stable as a function of sample size, we chose contiguous segments of experiment, while to check for the impact of correlations we ‘diluted’ our sampling so that there were longer and longer intervals between words. Obviously there are limits to this exploration (one cannot access large, very dilute samples), but as far as we could explore the impact of correlations on our estimates was negligible once the samples sizes were sufficiently large. For the effects of sample size we looked for behavior of the form <italic>S</italic>(<italic>α</italic>) = <italic>S</italic><sub>∞</sub>+<italic>S</italic><sub>1</sub>/<italic>α</italic>+<italic>S</italic><sub>2</sub>/<italic>α</italic><sup>2</sup> and took <italic>S</italic><sub>∞</sub> as our estimate of <italic>S</italic>(<italic>T,τ</italic>), as in <xref ref-type="bibr" rid="pcbi.1000025-Strong1">[4]</xref>. For all partitions in which the most common word (silence) was separated from the rest, these extrapolated estimates agreed and indicated negligible biases at all combinations of <italic>τ</italic> and <italic>T</italic> for which the 1/<italic>α</italic><sup>2</sup> term was negligible (that is, did not change the extrapolation results by more than the extrapolation error) compared to the 1/<italic>α</italic>; this happened for all <italic>τ</italic>≥0.5 ms at <italic>T</italic>≤25 ms. For smaller <italic>τ</italic>, estimation failed at progressively smaller <italic>T</italic>, and to obtain an entropy rate for large <italic>T</italic> we extrapolated to <italic>τ</italic>/<italic>T</italic>→0 using<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e015" xlink:type="simple"/><label>(12)</label></disp-formula>where <italic>s</italic>(<italic>τ</italic>) was our best estimate of the entropy rate at resolution <italic>τ</italic>. All fits were of high quality, and the resulting error bars on the total entropy were negligible compared to those for the noise entropy. In principle, we could be missing features of the code which would appear <italic>only</italic> at high resolution for very long words, but this unlikely scenario is almost impossible to exclude by any means.</p></sec><sec id="s4i"><title>Entropy estimation: Details for noise entropy</title><p>Putting error bars on the noise entropy averaged over time is more difficult because these should include a contribution from the fact that our finite sample over time is only an approximation to the true average over the underlying distribution of stimuli. Specifically, the entropies were very different in epochs that have net positive or negative velocities. We constructed the repeated stimulus, <italic>v</italic>(<italic>t</italic>) = −<italic>v</italic>(<italic>t</italic>+<italic>T</italic><sub>0</sub>), with <italic>T</italic><sub>0</sub> = 2.5 s. As a result, the sum <italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>)+<italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>+<italic>T</italic><sub>1</sub>) with <italic>T</italic><sub>1</sub>≈<italic>T</italic><sub>0</sub> fluctuated much less as a function of <italic>t</italic> than the entropy in an individual slice. Because our stimulus had zero mean, every slice had a partner under this shift, and the small difference between <italic>T</italic><sub>0</sub> and <italic>T</italic><sub>1</sub> took account of the difference in latency between responses to positive and negative inputs. A plot of <italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>)+<italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>+<italic>T</italic><sub>1</sub>) vs. time <italic>t</italic> had clear dips at times corresponding to zero crossings of the stimulus, and we partitioned the data at these points. We derived error bars on the mean noise entropy 〈<italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>)<italic><sub>t</sub></italic>〉 by a bootstrap-like method, in which we constructed samples by randomly sampling with replacements from among these blocks, jittering the individual entropies <italic>S<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>|<italic>t</italic>) by the errors that emerge from the Bayesian analysis of individual slices. These blocks are long enough to preserve temporal correlations within them, but correlations across the block boundaries are negligible in the original signal, validating the procedure. As with the total entropy, we extrapolated to otherwise inaccessible combinations of <italic>T</italic> and <italic>τ</italic>, now writing<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e016" xlink:type="simple"/><label>(13)</label></disp-formula>and fitting by weighted regression. Note that results at different <italic>T</italic> but the same value of <italic>τ</italic> were strongly correlated, and so the computation of <italic>χ</italic><sup>2</sup> was done using the full (non-diagonal) covariance matrix. The periodic term was important at small <italic>τ</italic>, where we could see structure as the window size <italic>T</italic> crossed integer multiples of the average interspike interval, <italic>τ</italic><sub>0</sub> = 2.53 ms. Error estimates emerged from the regression in the standard way, and all fits had <italic>χ</italic><sup>2</sup>∼1 per degree of freedom.</p><p>The procedures followed to get the total and noise entropy estimates in combination with the checks described above result in bias errors that are believed to be smaller than the random errors over the parameter range that we consider in all the analyses presented in this paper.</p></sec><sec id="s4j"><title>Impact of photon flux on information rates</title><p>Since there were no responses to repeated and unrepeated stimuli recorded at exactly the same illuminations, we used the data from the repeated experiment to evaluate both the noise entropy and the total entropy. We were looking for minute effects, so we tightened our analysis by discarding the first two trials, which were significantly different from all the rest (presumably because adaptation was not complete), as well as excluding the epochs in which the stimulus was padded with zeroes. The remaining 98 trials were split into two groups of 49 trials each with the highest and the lowest ambient light levels. We then estimated the total entropy <italic>S</italic><sup>(<italic>h</italic>,<italic>l</italic>)</sup>(<italic>T</italic>,<italic>τ</italic>) for the high (<italic>h</italic>) and low (<italic>l</italic>) intensity groups of trials, and similarly for the noise entropy in each slice at time <italic>t</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e017" xlink:type="simple"/></inline-formula>. As above, assigning error bars was clearer once we formed quantities that were balanced across positive and negative velocities, and we did this directly for the difference in noise entropies,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e018" xlink:type="simple"/><label>(14)</label></disp-formula>where we allowed for a small difference in latencies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000025.e019" xlink:type="simple"/></inline-formula> between the groups of trials at different intensities. We found that <italic>ΔS<sub>n</sub></italic>(<italic>T</italic>,<italic>τ</italic>;<italic>t</italic>) had a unimodal distribution and a correlation time of ∼1.4 ms, which allowed for an easy evaluation of the estimation error.</p></sec></sec></body><back><ref-list><title>References</title><ref id="pcbi.1000025-Rieke1"><label>1</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name><name name-style="western"><surname>de Ruyter van Steven­inck</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1997</year>             <article-title>Spikes: Exploring the Neural Code.</article-title>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref><ref id="pcbi.1000025-MacKay1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>MacKay</surname><given-names>D</given-names></name><name name-style="western"><surname>McCulloch</surname><given-names>WS</given-names></name></person-group>             <year>1952</year>             <article-title>The limiting information capacity of a neuronal link.</article-title>             <source>Bull Math Biophys</source>             <volume>14</volume>             <fpage>127</fpage>             <lpage>135</lpage>          </element-citation></ref><ref id="pcbi.1000025-Abeles1"><label>3</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name></person-group>             <year>1982</year>             <article-title>Local Cortical Circuits: An Electrophysiological Study.</article-title>             <publisher-loc>Berlin</publisher-loc>             <publisher-name>Springer–Verlag</publisher-name>          </element-citation></ref><ref id="pcbi.1000025-Strong1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Strong</surname><given-names>SP</given-names></name><name name-style="western"><surname>Koberle</surname><given-names>R</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1998</year>             <article-title>Entropy and information in neural spike trains.</article-title>             <source>Phys Rev Lett</source>             <volume>80</volume>             <fpage>197</fpage>             <lpage>200</lpage>          </element-citation></ref><ref id="pcbi.1000025-Liu1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>R</given-names></name><name name-style="western"><surname>Tzonev</surname><given-names>S</given-names></name><name name-style="western"><surname>Rebrik</surname><given-names>S</given-names></name><name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name></person-group>             <year>2001</year>             <article-title>Variability and information in a neural code of the cat lateral geniculate nucleus.</article-title>             <source>J Neurophysiol</source>             <volume>86</volume>             <fpage>2789</fpage>             <lpage>2806</lpage>          </element-citation></ref><ref id="pcbi.1000025-Carr1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Carr</surname><given-names>CE</given-names></name></person-group>             <year>1993</year>             <article-title>Processing of temporal information in the brain.</article-title>             <source>Ann Rev Neurosci</source>             <volume>16</volume>             <fpage>223</fpage>             <lpage>243</lpage>          </element-citation></ref><ref id="pcbi.1000025-Hopfield1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1995</year>             <article-title>Pattern recognition computation using action potential timing for stimulus representation.</article-title>             <source>Nature</source>             <volume>376</volume>             <fpage>33</fpage>             <lpage>36</lpage>          </element-citation></ref><ref id="pcbi.1000025-Hausen1"><label>8</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hausen</surname><given-names>K</given-names></name></person-group>             <year>1984</year>             <article-title>The lobular complex of the fly: Structure, function and significance in behavior.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Ali</surname><given-names>M</given-names></name></person-group>             <source>Photoreception and Vision in Invertebrates</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Plenum</publisher-name>             <fpage>523</fpage>             <lpage>559</lpage>          </element-citation></ref><ref id="pcbi.1000025-Lewen1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lewen</surname><given-names>GD</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name></person-group>             <year>2001</year>             <article-title>Neural coding of naturalistic motion stimuli.</article-title>             <source>Network</source>             <volume>12</volume>             <fpage>317</fpage>             <lpage>329</lpage>          </element-citation></ref><ref id="pcbi.1000025-Nemenman1"><label>10</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name><name name-style="western"><surname>Shafee</surname><given-names>F</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2000</year>             <article-title>Entropy and inference, revisited.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Dietterich</surname><given-names>T</given-names></name><name name-style="western"><surname>Becker</surname><given-names>S</given-names></name><name name-style="western"><surname>Gharamani</surname><given-names>Z</given-names></name></person-group>             <source>Advances in Neural Information Processing Systems</source>             <volume>14</volume>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>471</fpage>             <lpage>478</lpage>          </element-citation></ref><ref id="pcbi.1000025-Nemenman2"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name></person-group>             <year>2002</year>             <article-title>Inference of entropies of discrete random variables with unknown cardinalities.</article-title>             <source>Physics 0207009</source>          </element-citation></ref><ref id="pcbi.1000025-Nemenman3"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name></person-group>             <year>2004</year>             <article-title>Entropy and information in neural spike trains: Progress on the sampling problem.</article-title>             <source>Phys Rev E</source>             <volume>69</volume>             <fpage>056111</fpage>          </element-citation></ref><ref id="pcbi.1000025-Barlow1"><label>13</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>             <year>1959</year>             <article-title>Sensory mechanisms, the reduction of redundancy and intelligence.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Blake</surname><given-names>DV</given-names></name><name name-style="western"><surname>Uttley</surname><given-names>AM</given-names></name></person-group>             <source>Proceedings of the Symposium on the Mechanization of Thought Processes, Vol 2</source>             <publisher-loc>London</publisher-loc>             <publisher-name>HM Stationery Office</publisher-name>             <fpage>537</fpage>             <lpage>574</lpage>          </element-citation></ref><ref id="pcbi.1000025-Barlow2"><label>14</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>             <year>1961</year>             <article-title>Possible principles underlying the transformation of sensory messages.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Rosenblith</surname><given-names>W</given-names></name></person-group>             <source>Sensory Communication</source>             <publisher-loc>Cambridge (Massachsuetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>217</fpage>             <lpage>234</lpage>          </element-citation></ref><ref id="pcbi.1000025-Land1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Land</surname><given-names>MF</given-names></name><name name-style="western"><surname>Collett</surname><given-names>TS</given-names></name></person-group>             <year>1974</year>             <article-title>Chasing behavior of houseflies (Fannia canicularis). A description and analysis.</article-title>             <source>J Comp Physiol</source>             <volume>89</volume>             <fpage>331</fpage>             <lpage>357</lpage>          </element-citation></ref><ref id="pcbi.1000025-Reichardt1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reichardt</surname><given-names>W</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>             <year>1976</year>             <article-title>Visual control of orientation behavior in the fly. Part I: A quantitative analysis.</article-title>             <source>Q Rev Biophys</source>             <volume>9</volume>             <fpage>311</fpage>             <lpage>375</lpage>          </element-citation></ref><ref id="pcbi.1000025-Hausen2"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hausen</surname><given-names>K</given-names></name><name name-style="western"><surname>Wehrhahn</surname><given-names>C</given-names></name></person-group>             <year>1983</year>             <article-title>Microsurgical lesions of horizontal cells changes optomotor yaw responses in the blowfly Calliphora erythrocephela.</article-title>             <source>Proc R Soc Lond Ser B</source>             <volume>219</volume>             <fpage>211</fpage>             <lpage>216</lpage>          </element-citation></ref><ref id="pcbi.1000025-Wagner1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wagner</surname><given-names>H</given-names></name></person-group>             <year>1986</year>             <article-title>Flight performance and visual control of flight in the free–flying house fly (Musca domestica L.). I–III.</article-title>             <source>Phil Trans R Soc Ser B</source>             <volume>312</volume>             <fpage>527</fpage>             <lpage>595</lpage>          </element-citation></ref><ref id="pcbi.1000025-Schilstra1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schilstra</surname><given-names>C</given-names></name><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name></person-group>             <year>1999</year>             <article-title>Blowfly flight and optic flow. I. Thorax kinematics and flight dynamics.</article-title>             <source>J Exp Biol</source>             <volume>202</volume>             <fpage>1481</fpage>             <lpage>1490</lpage>          </element-citation></ref><ref id="pcbi.1000025-vanHateren1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name><name name-style="western"><surname>Schilstra</surname><given-names>C</given-names></name></person-group>             <year>1999</year>             <article-title>Blowfly flight and optic flow. II. Head movements during flight.</article-title>             <source>J Exp Biol</source>             <volume>202</volume>             <fpage>1491</fpage>             <lpage>1500</lpage>          </element-citation></ref><ref id="pcbi.1000025-deRuytervanSteveninck1"><label>21</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Borst</surname><given-names>A</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2001</year>             <article-title>Real time encoding of motion: Answerable questions and questionable answers from the fly's visual system.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Zanker</surname><given-names>JM</given-names></name><name name-style="western"><surname>Zeil</surname><given-names>J</given-names></name></person-group>             <source>Processing Visual Motion in the Real World: A Survey of Computational, Neural and Ecological Constraints</source>             <publisher-loc>Berlin</publisher-loc>             <publisher-name>Springer–Verlag</publisher-name>             <fpage>279</fpage>             <lpage>306</lpage>          </element-citation></ref><ref id="pcbi.1000025-vanHateren2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name><name name-style="western"><surname>Kern</surname><given-names>R</given-names></name><name name-style="western"><surname>Schwerdtfeger</surname><given-names>G</given-names></name><name name-style="western"><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group>             <year>2005</year>             <article-title>Function and coding in the blowfly H1 neuron during naturalistic optic flow.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>4343</fpage>             <lpage>4352</lpage>          </element-citation></ref><ref id="pcbi.1000025-Shannon1"><label>23</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>CE</given-names></name><name name-style="western"><surname>Weaver</surname><given-names>W</given-names></name></person-group>             <year>1949</year>             <article-title>The mathematical theory of communication.</article-title>             <publisher-loc>Urbana (Illinois)</publisher-loc>             <publisher-name>The University of Illinois Press</publisher-name>          </element-citation></ref><ref id="pcbi.1000025-Paninski1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2003</year>             <article-title>Estimation of entropy and mutual information.</article-title>             <source>Neural Comp</source>             <volume>15</volume>             <fpage>1191</fpage>             <lpage>1253</lpage>          </element-citation></ref><ref id="pcbi.1000025-Ma1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>S</given-names></name></person-group>             <year>1981</year>             <article-title>Calculation of entropy from data of motion.</article-title>             <source>J Stat Phys</source>             <volume>26</volume>             <fpage>221</fpage>             <lpage>240</lpage>          </element-citation></ref><ref id="pcbi.1000025-Seber1"><label>26</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Seber</surname><given-names>GAF</given-names></name></person-group>             <year>1973</year>             <article-title>Estimation of Animal Abundance and Related Parameters.</article-title>             <publisher-loc>London</publisher-loc>             <publisher-name>Griffin</publisher-name>          </element-citation></ref><ref id="pcbi.1000025-Miller1"><label>27</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>GA</given-names></name></person-group>             <year>1955</year>             <article-title>Note on the bias of information estimates.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Quastler</surname><given-names>H</given-names></name></person-group>             <source>Information Theory in Psychology: Problems and Methods II–B</source>             <publisher-loc>Glencoe (Illinois)</publisher-loc>             <publisher-name>Free Press</publisher-name>             <fpage>95</fpage>             <lpage>100</lpage>          </element-citation></ref><ref id="pcbi.1000025-Treves1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name></person-group>             <year>1995</year>             <article-title>The upward bias in measures of information derived from limited data samples.</article-title>             <source>Neural Comp</source>             <volume>7</volume>             <fpage>399</fpage>             <lpage>407</lpage>          </element-citation></ref><ref id="pcbi.1000025-Victor1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Victor</surname><given-names>J</given-names></name></person-group>             <year>2002</year>             <article-title>Binless strategies for estimation of information from neural data.</article-title>             <source>Phys. Rev. E</source>             <volume>66</volume>             <fpage>051903</fpage>          </element-citation></ref><ref id="pcbi.1000025-Kennel1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kennel</surname><given-names>M</given-names></name><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name><name name-style="western"><surname>Abarbanel</surname><given-names>H</given-names></name><name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group>             <year>2005</year>             <article-title>Estimating entropy rates with Bayesian confidence intervals.</article-title>             <source>Neural Comp.</source>             <volume>17</volume>             <fpage>1531</fpage>             <lpage>1576</lpage>          </element-citation></ref><ref id="pcbi.1000025-deRuytervanSteveninck2"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Lewen</surname><given-names>GD</given-names></name><name name-style="western"><surname>Strong</surname><given-names>SP</given-names></name><name name-style="western"><surname>Koberle</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1997</year>             <article-title>Reproducibility and variability in neural spike trains.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>1805</fpage>             <lpage>1808</lpage>          </element-citation></ref><ref id="pcbi.1000025-deRuytervanSteveninck3"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1988</year>             <article-title>Real–time performance of a movement sensitive neuron in the blowfly visual system: Coding and information transfer in short spike sequences.</article-title>             <source>Proc R Soc London Ser B</source>             <volume>234</volume>             <fpage>379</fpage>             <lpage>414</lpage>          </element-citation></ref><ref id="pcbi.1000025-Carr2"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Carr</surname><given-names>CE</given-names></name><name name-style="western"><surname>Heiligenberg</surname><given-names>W</given-names></name><name name-style="western"><surname>Rose</surname><given-names>GJ</given-names></name></person-group>             <year>1986</year>             <article-title>A time–comparison circuit in the electric fish midbrain. I. Behavior and physiology.</article-title>             <source>J Neurosci</source>             <volume>10</volume>             <fpage>3227</fpage>             <lpage>3246</lpage>          </element-citation></ref><ref id="pcbi.1000025-Reich1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reich</surname><given-names>DS</given-names></name><name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name><name name-style="western"><surname>Knight</surname><given-names>BW</given-names></name><name name-style="western"><surname>Ozaki</surname><given-names>T</given-names></name><name name-style="western"><surname>Kaplan</surname><given-names>E</given-names></name></person-group>             <year>1997</year>             <article-title>Response variability and timing precision of neuronal spike trains in vivo.</article-title>             <source>J. Neurophysiol.</source>             <volume>77</volume>             <fpage>2836</fpage>             <lpage>2841</lpage>          </element-citation></ref><ref id="pcbi.1000025-Brenner1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brenner</surname><given-names>N</given-names></name><name name-style="western"><surname>Strong</surname><given-names>SP</given-names></name><name name-style="western"><surname>Koberle</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name></person-group>             <year>2000</year>             <article-title>Synergy in a neural code.</article-title>             <source>Neural Comp</source>             <volume>12</volume>             <fpage>1531</fpage>             <lpage>1552</lpage>          </element-citation></ref><ref id="pcbi.1000025-Reinagel1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name><name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name></person-group>             <year>2000</year>             <article-title>Temporal coding of visual information in the thalamus.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>5392</fpage>             <lpage>5400</lpage>          </element-citation></ref><ref id="pcbi.1000025-Bialek1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name><name name-style="western"><surname>de Ruyter van Ste­ven­inck</surname><given-names>RR</given-names></name><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name></person-group>             <year>1991</year>             <article-title>Reading a neural code.</article-title>             <source>Science</source>             <volume>252</volume>             <fpage>1854</fpage>             <lpage>1857</lpage>          </element-citation></ref><ref id="pcbi.1000025-deRuytervanSteveninck4"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>1995</year>             <article-title>Reliability and statistical efficiency of a blowfly movement–sensitive neuron.</article-title>             <source>Phil Trans R Soc Lond Ser B</source>             <volume>348</volume>             <fpage>321</fpage>             <lpage>340</lpage>          </element-citation></ref><ref id="pcbi.1000025-deRuytervanSteveninck5"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name></person-group>             <year>1996</year>             <article-title>The rate of information transfer at graded–potential synapses.</article-title>             <source>Nature</source>             <volume>379</volume>             <fpage>642</fpage>             <lpage>645</lpage>          </element-citation></ref><ref id="pcbi.1000025-deRuytervanSteveninck6"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name></person-group>             <year>1996</year>             <article-title>Light adaptation and reliability in blowfly photoreceptors.</article-title>             <source>Int J Neural Syst</source>             <volume>7</volume>             <fpage>437</fpage>             <lpage>444</lpage>          </element-citation></ref><ref id="pcbi.1000025-Theunissen1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Theunissen</surname><given-names>F</given-names></name><name name-style="western"><surname>Miller</surname><given-names>JP</given-names></name></person-group>             <year>1995</year>             <article-title>Temporal encoding in nervous systems: A rigorous definition.</article-title>             <source>J Comput. Neurosci.</source>             <volume>2</volume>             <fpage>149</fpage>             <lpage>162</lpage>          </element-citation></ref><ref id="pcbi.1000025-Victor2"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name><name name-style="western"><surname>Purpura</surname><given-names>K</given-names></name></person-group>             <year>1996</year>             <article-title>Nature and precision of temporal coding in visual cortex: a metric-space analysis.</article-title>             <source>J. Neurophysiol.</source>             <volume>76</volume>             <fpage>1310</fpage>             <lpage>1326</lpage>          </element-citation></ref><ref id="pcbi.1000025-Batu1"><label>43</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Batu</surname><given-names>T</given-names></name><name name-style="western"><surname>Dasgupta</surname><given-names>S</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>R</given-names></name><name name-style="western"><surname>Rubinfeld</surname><given-names>R</given-names></name></person-group>             <year>2002</year>             <article-title>The complexity of approximating the entropy.</article-title>             <source>Proc. 34th Symp</source>             <fpage>678</fpage>             <lpage>687</lpage>             <comment>Theory of Computing (STOC)</comment>          </element-citation></ref><ref id="pcbi.1000025-Wyner1"><label>44</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wyner</surname><given-names>A</given-names></name><name name-style="western"><surname>Foster</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>On the lower limits of entropy estimation. Preprint.</article-title>             <comment><ext-link ext-link-type="uri" xlink:href="http://www-stat.wharton.upenn.edu/ajw/lowlimitsentropy.pdf" xlink:type="simple">http://www-stat.wharton.upenn.edu/ajw/lowlimitsentropy.pdf</ext-link></comment>          </element-citation></ref><ref id="pcbi.1000025-Wolpert1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wolpert</surname><given-names>DH</given-names></name><name name-style="western"><surname>Wolf</surname><given-names>DR</given-names></name></person-group>             <year>1995</year>             <article-title>Estimating functions of probability distributions from a finite set of samples,</article-title>             <source>Phys. Rev. E</source>             <volume>52</volume>             <fpage>6841</fpage>             <lpage>6854</lpage>          </element-citation></ref><ref id="pcbi.1000025-Minke1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Minke</surname><given-names>B</given-names></name><name name-style="western"><surname>Kirschfeld</surname><given-names>K</given-names></name></person-group>             <year>1979</year>             <article-title>The contribution of a sensitizing pigment to the photosensitivity spectra of fly rhodopsin and metarhodopsin.</article-title>             <source>J Gen Physiol</source>             <volume>73</volume>             <fpage>517</fpage>             <lpage>540</lpage>          </element-citation></ref><ref id="pcbi.1000025-Menzel1"><label>47</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Menzel</surname><given-names>R</given-names></name></person-group>             <year>1979</year>             <article-title>Spectral Sensitivity and Color Vision in Invertebrates.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Autrum</surname><given-names>H</given-names></name></person-group>             <source>Handbook of Comparative Physiology</source>             <volume>vol VII/6A</volume>             <publisher-loc>Berlin-Heidelberg-New York</publisher-loc>             <publisher-name>Springer-Verlag</publisher-name>             <fpage>503</fpage>             <lpage>580</lpage>          </element-citation></ref><ref id="pcbi.1000025-Zipf1"><label>48</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zipf</surname><given-names>GK</given-names></name></person-group>             <year>1949</year>             <article-title>Human Behavior and the Principle of Least Effort.</article-title>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>Addison–Wesley</publisher-name>          </element-citation></ref><ref id="pcbi.1000025-Nemenman4"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2002</year>             <article-title>Occam factors and model-independent Bayesian learning of continuous distributions.</article-title>             <source>Phys Rev E</source>             <volume>65</volume>             <fpage>026137</fpage>          </element-citation></ref><ref id="pcbi.1000025-Green1"><label>50</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Green</surname><given-names>DM</given-names></name><name name-style="western"><surname>Swets</surname><given-names>JA</given-names></name></person-group>             <year>1966</year>             <article-title>Signal Detection Theory and Psychophysics.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley</publisher-name>          </element-citation></ref></ref-list></back></article>