<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01601</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003441</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject></subj-group><subj-group><subject>Neuroscience</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group><subj-group><subject>Statistics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and behavioral sciences</subject><subj-group><subject>Information science</subject></subj-group><subj-group><subject>Psychology</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>VBA: A Probabilistic Treatment of Nonlinear Models for Neurobiological and Behavioural Data</article-title>
<alt-title alt-title-type="running-head">VBA-Toolbox</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>Jean</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Adam</surname><given-names>Vincent</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Rigoux</surname><given-names>Lionel</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Brain and Spine Institute, Paris, France</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Wellcome Trust Centre for Neuroimaging, University College London, London, United Kingdom</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Gatsby computational neuroscience Unit, University College London, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Prlic</surname><given-names>Andreas</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>UCSD, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jean.daunizeau@gmail.com</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JD. Performed the experiments: JD LR. Analyzed the data: JD LR. Contributed reagents/materials/analysis tools: JD VA LR. Wrote the paper: JD VA LR.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>1</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>23</day><month>1</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>1</issue>
<elocation-id>e1003441</elocation-id>
<history>
<date date-type="received"><day>3</day><month>9</month><year>2013</year></date>
<date date-type="accepted"><day>26</day><month>11</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Daunizeau et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>This work is in line with an on-going effort tending toward a computational (quantitative and refutable) understanding of human neuro-cognitive processes. Many sophisticated models for behavioural and neurobiological data have flourished during the past decade. Most of these models are partly unspecified (i.e. they have unknown parameters) and nonlinear. This makes them difficult to peer with a formal statistical data analysis framework. In turn, this compromises the reproducibility of model-based empirical studies. This work exposes a software toolbox that provides generic, efficient and robust probabilistic solutions to the three problems of model-based analysis of empirical data: (i) data simulation, (ii) parameter estimation/model selection, and (iii) experimental design optimization.</p>
</abstract>
<funding-group><funding-statement>This work was supported by the European Research Council (JD) and by the Ville de Paris (LR). In addition, authors acknowledge support from the IHU-A-IM. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="16"/></counts></article-meta>
</front>
<body><sec id="s1">
<title/>
<disp-quote>
<p>This is a <italic>PLOS Computational Biology</italic> Software Article</p>
</disp-quote></sec><sec id="s2">
<title>Introduction</title>
<p>Spectrum diseases in psychiatry, such as schizophrenia or depression, display profound heterogeneity with regard to the underlying pathophysiological mechanisms, requiring the development of models that can infer subject-specific mechanisms from neurophysiological and/or behavioural data <xref ref-type="bibr" rid="pcbi.1003441-Stephan1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003441-Schofield1">[3]</xref>. Developing quantitative approaches that can do this will require merging expert knowledge on neurobiology, biophysical generation of neuroimaging signals, cognitive psychology, and statistical data analysis, to mention but a few. Recall that most cerebral mechanisms can be described at two levels of abstraction:</p>
<list list-type="bullet"><list-item>
<p>The cognitive or functional level is concerned with the information processing that is needed to explain behavioural measurements (e.g., choices, reaction times) or subjective reports (e.g., emotions, thoughts).</p>
</list-item><list-item>
<p>The neurobiological or physiological level is related to the neurobiological substrate of the system. Imaging neuroscience or neuroimaging (e.g., electroencephalography/magnetoencephalography or EEG/MEG, functional Magnetic Resonance Imaging or fMRI) is capable of observing (non-invasively) certain biophysical characteristics of this biological substrate.</p>
</list-item></list>
<p>These typically map to two classes of models, i.e. (i) formal models of perception, learning and decision making that predict behavioural responses, and (ii) biophysically realistic models that describe how electrophysiological activity propagate through neural networks. The issue with such models is that they are based upon mechanisms that are usually both hidden (they are not directly accessible from experimental data) and nonlinear (this is the curse of realism). As a consequence, one requires sophisticated statistical approaches that can deal efficiently with parameter estimation and model selection (given experimental data). If only, these are necessary to capture the inter-individual variability of neurophysiological and behavioural responses. More generally, such schemes would embed the models into the data analysis and act as a “mathematical microscope” that is capable of unravelling mechanisms, which are hidden deep within experimental data <xref ref-type="bibr" rid="pcbi.1003441-Moran1">[4]</xref>.</p>
<p>This article describes a (matlab) software toolbox that is designed to perform such model-based analyses of neuroimaging and behavioural data. Essentially, it consists of a probabilistic model inversion scheme that borrows from disciplines such as inverse problems, statistical physics and machine learning. More precisely, the toolbox implements a variational Bayesian approach (VBA) that can deal with a very general class of generative models, which we describe below. In brief, VBA address the following issues:</p>
<list list-type="bullet"><list-item>
<p>performing efficient and robust parameter estimation on nonlinear models;</p>
</list-item><list-item>
<p>providing quantitative diagnostics of model fitting (including summary statistics that can be used for model comparison);</p>
</list-item><list-item>
<p>optimizing the experimental design in the aim of maximizing the statistical power of model-based data analysis;</p>
</list-item><list-item>
<p>assessing the results reproducibility at the group level (e.g., between-groups and between-conditions model comparisons).</p>
</list-item></list>
<p>In addition, the toolbox gathers a growing number of established and novel biophysical and cognitive models, which capture a broad range of phenomena. Among these are deterministic and stochastic variants of dynamic causal models for fMRI data (see, e.g. <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau1">[5]</xref> for a recent review) and bayesian models for human learning and decision making <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau2">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1003441-Mathys1">[7]</xref>. Importantly, VBA includes diagnostic analyses that can be used directly to refine such models, i.e. to account for yet unpredicted features of experimental data (cf. Volterra decompositions). We will demonstrate this below. Our ambition is twofold: (i) to disseminate models and methods that serve experimental purposes, and (ii) to provide a flexible platform, which modellers can easily contribute to.</p>
<p>This paper is organized as follows.</p>
<p>In the next section, we will describe the design and implementation of the toolbox. In particular, we will describe the class of generative models that the VBA toolbox can handle, expose the main aspects of its algorithmic treatment, and summarize the organization of the code. In section “<xref ref-type="sec" rid="s4">Results</xref>”, we will demonstrate its capabilities through analyses of deposited test data. In section “Availability and future directions”, we will discuss limitations and on-going developments of this work.</p>
</sec><sec id="s3">
<title>Design and Implementation</title>
<p>In brief, the toolbox furnishes solutions to the three canonical problems of model-based data analysis, which relate to the experimental cycle (cf. <xref ref-type="fig" rid="pcbi-1003441-g001">Figure 1</xref>). One starts with a set of competing hypotheses, e.g.: is the incentive value discounted (or not) by the amount of cognitive effort that is required to obtain the reward? This question will eventually be framed in terms of a model comparison (e.g., model 1 -with effort discounting- versus model 2 -without effort discounting-). First, one has to be able to predict behavioural and/or biological signals from the candidate models. This simply means simulating, e.g. people's choices under model 1 and 2. Second, one may want to optimize the experimental design, in the aim of best discriminating the candidate models. In our example, this naturally involves manipulating both the reward that is at stake and the amount of effort. However, there might be an optimal manipulation of these two factors, such that models 1 and 2 yield radically different predictions. Third, one proceeds to parameter estimation and/or model selection, given the acquired experimental data. Here, statistical inference follows the variational Bayesian treatment of the candidate models (see below).</p>
<fig id="pcbi-1003441-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g001</object-id><label>Figure 1</label><caption>
<title>The experimental cycle.</title>
<p>The experimental cycle summarizes the interaction between modelling, experimental work and statistical data analysis. One starts with new competing hypotheses about a system of interest. These are then embodied into a set of candidate models that are to be compared with each other given empirical data. One then designs an experiment that is maximally discriminative with respect to the candidate models. Data acquisition and analysis then proceed, the conclusion of which serves to generate a new set of competing hypotheses, etc… Adapted from <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau4">[14]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g001" position="float" xlink:type="simple"/></fig>
<p>Data analysis results can now serve to identify a new set of competing hypotheses, which then triggers a new experimental cycle…</p>
<p>The toolbox is designed so that data simulation, parameter estimation, model selection and design optimization can be handled automatically, given standardized information about the model (see below).</p>
<sec id="s3a">
<title>Stochastic nonlinear state-space models</title>
<p>Any Bayesian data analysis relies upon a generative model, i.e. a probabilistic description of the mechanisms by which observed data are generated. Such descriptions can go from simple static linear models to nonlinear dynamical models in continuous time. This toolbox does not invert any generative model: it has been developed to deal with stochastic nonlinear state-space models. As we will see below, this class of generative models grandfathers most models used in the literature. It is defined by a joint probability distribution over the following set of variables:</p>
<list list-type="bullet"><list-item>
<p><italic>y</italic>: the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e001" xlink:type="simple"/></inline-formula> data time-series</p>
</list-item><list-item>
<p><italic>x</italic>: the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e002" xlink:type="simple"/></inline-formula> hidden states time-series</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e003" xlink:type="simple"/></inline-formula>: the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e004" xlink:type="simple"/></inline-formula> initial conditions of the system</p>
</list-item><list-item>
<p><italic>u</italic>: the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e005" xlink:type="simple"/></inline-formula> inputs time-series</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e006" xlink:type="simple"/></inline-formula>: the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e007" xlink:type="simple"/></inline-formula> evolution parameters</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e008" xlink:type="simple"/></inline-formula>: the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e009" xlink:type="simple"/></inline-formula> observation parameters</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e010" xlink:type="simple"/></inline-formula>: the state noise precision</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e011" xlink:type="simple"/></inline-formula>: the measurement noise precision</p>
</list-item></list>
<p>These variables are assumed to obey the following evolution and observation equations:<disp-formula id="pcbi.1003441.e012"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e012" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e013" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e014" xlink:type="simple"/></inline-formula>) is the evolution (resp. observation) mapping, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e015" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e016" xlink:type="simple"/></inline-formula>) is the state (resp. measurement) noise. As we will see later, continuous dynamical systems can, in principle, be reduced to <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> (cf. <xref ref-type="supplementary-material" rid="pcbi.1003441.s004">Text S3</xref> in the supplementary information). <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> is then augmented with Gaussian prior assumptions regarding the statistical behaviour of initial conditions, evolution/observation parameters and state/measurement noise:<disp-formula id="pcbi.1003441.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e017" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e018" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e019" xlink:type="simple"/></inline-formula>) denotes the mean (resp. covariance matrix) of the Gaussian density. Setting the prior variance of a given variable to zero simply means fixing its value to its prior mean. In addition, we use Gamma priors on precision hyperparameters:<disp-formula id="pcbi.1003441.e020"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e020" xlink:type="simple"/><label>(3)</label></disp-formula>where <italic>a</italic> (resp. <italic>b</italic>) denotes the shape (resp. rate) parameter of the Gamma distribution, which control the first two moments of the Gamma density, as follows: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e021" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e022" xlink:type="simple"/></inline-formula>. Given priors in <xref ref-type="disp-formula" rid="pcbi.1003441.e017">Equations 2</xref>–<xref ref-type="disp-formula" rid="pcbi.1003441.e020">3</xref>, the first line of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> induces a (so-called Markovian) prior density on the trajectory of hidden state <italic>x</italic>. Similarly, the second line of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> now yields a likelihood function, which measures how plausible is any value that experimental measurement <italic>y</italic> can take, under the assumptions (<xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equations 1</xref>–<xref ref-type="disp-formula" rid="pcbi.1003441.e020">3</xref>) of the generative model <italic>m</italic>. The class of generative models that the toolbox handles is in fact slightly more general than can be inferred from <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref>. In particular, the observation mapping can be modified to deal with categorical (e.g. multinomial) data. <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> defines a (potentially nonlinear) state-space model, which grand-fathers many generative models used in the statistics literature. We will come back to this later on. At this point, suffices to say that one's model can be defined simply in terms of the evolution and observation functions. This means that data simulation, parameter estimation, model selection and design optimization only require the specification of these two functions.</p>
</sec><sec id="s3b">
<title>VB approach to parameter estimation and model selection</title>
<p>Inverting the above generative model <italic>m</italic> means (i) approximating the conditional density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e023" xlink:type="simple"/></inline-formula> of unknown variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e024" xlink:type="simple"/></inline-formula> given the set of sampled measurements <italic>y</italic> and (ii) quantifying the model evidence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e025" xlink:type="simple"/></inline-formula>. Nonlinearities in the generative model eschew exact analytical solutions to this problem, which is finessed using variational approaches that rest on optimizing a free-energy lower bound <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e026" xlink:type="simple"/></inline-formula> to the model evidence, with respect to an approximate conditional density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e027" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003441-Beal1">[8]</xref>:<disp-formula id="pcbi.1003441.e028"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e028" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e029" xlink:type="simple"/></inline-formula> is the Kullback-Leibler divergence and the expectation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e030" xlink:type="simple"/></inline-formula> is taken under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e031" xlink:type="simple"/></inline-formula>. From <xref ref-type="disp-formula" rid="pcbi.1003441.e028">Equation 4</xref>, maximizing the functional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e032" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e033" xlink:type="simple"/></inline-formula> minimizes the Kullback-Leibler divergence between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e034" xlink:type="simple"/></inline-formula> and the exact posterior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e035" xlink:type="simple"/></inline-formula>. This decomposition is complete in the sense that if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e036" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e037a" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e037b" xlink:type="simple"/></inline-formula>. This means that the free energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e038" xlink:type="simple"/></inline-formula> can serve as an analytical approximation to the log model evidence.</p>
<p>Here, the iterative maximization of free energy proceeds under the Laplace approximation, where the approximate posterior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e039" xlink:type="simple"/></inline-formula> is assumed to have a Gaussian form (except for the precision hyperparameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e040" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e041" xlink:type="simple"/></inline-formula>, which have Gamma posterior densities; cf. <xref ref-type="fig" rid="pcbi-1003441-g002">Figure 2</xref>). Thus, the variational Bayesian (VB) updates reduce to a regularized Gauss-Newton optimization scheme <xref ref-type="bibr" rid="pcbi.1003441-Friston1">[9]</xref>. This dramatically decreases the computational complexity of the scheme. The second-order moments of the approximate posterior densities are then simply related to the curvature of local cost functions:<disp-formula id="pcbi.1003441.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e042" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e043" xlink:type="simple"/></inline-formula> is the set of all unknown model variables, which is partitioned into subsets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e044" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e045" xlink:type="simple"/></inline-formula> (cf. “mean-field” assumption). Here, the notation “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e046" xlink:type="simple"/></inline-formula>” refers to the complement of the subset indexed by <italic>i</italic>. We refer to <xref ref-type="bibr" rid="pcbi.1003441-Friston1">[9]</xref> for computational details about the VB algorithm. Note that the VB update of the hidden states is very similar in form to a Kalman filter/smoother <xref ref-type="bibr" rid="pcbi.1003441-Kloeden1">[10]</xref>. More precisely, the scheme derives an approximation to the lagged posterior density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e047" xlink:type="simple"/></inline-formula>, where <italic>k</italic> is the lag (see <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau3">[11]</xref> for the derivation of the lagged forward pass). This lag can be chosen arbitrarily (see below), which allows one to infer on hidden states, whose changes impact observed data a few time samples later in time (e.g. due to some form of convolution operation). The main effect of increasing the lag is to average across more data points when deriving the hidden states, hence improving the precision (and the temporal smoothness) of the estimate. The ensuing computational cost scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e048" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003441-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g002</object-id><label>Figure 2</label><caption>
<title>The mean-field/Laplace approximation.</title>
<p>The variational Bayesian approach furnishes an approximation to the marginal posterior densities of subsets of unknown model parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e049" xlink:type="simple"/></inline-formula>. Here, the 2D landscape depicts a (true) joint posterior density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e050" xlink:type="simple"/></inline-formula> and the two black lines are the subsequent marginal posterior densities of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e052" xlink:type="simple"/></inline-formula>, respectively. The mean-field approximation basically describes the joint posterior density as the product of the two marginal densities (black profiles). In turn, stochastic dependencies between parameter subsets are replaced by deterministic dependencies between their posterior sufficient statistics. The Laplace approximation further assumes that the marginal densities can be described by Gaussian densities (red profiles).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g002" position="float" xlink:type="simple"/></fig>
<p>In brief, the core of the toolbox consists of a generic VB treatment of the above class of generative models. Given experimental data <italic>y</italic>, system's input <italic>u</italic> (if any), evolution/observation mappings and priors, it recovers an approximation to both the posterior density on unknown variables and the model evidence (which is used for model comparison). Practical guidance on the software implementation can be found on this wiki page: <ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/mbb-vb-toolbox/" xlink:type="simple">http://code.google.com/p/mbb-vb-toolbox/</ext-link>. In particular, we have implemented a lot of examples and demonstration scripts, in the aim of accelerating users' learning (cf. “getting started” section of the wiki pages: <ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/mbb-vb-toolbox/wiki/getting_started" xlink:type="simple">http://code.google.com/p/mbb-vb-toolbox/wiki/getting_started</ext-link>).</p>
</sec><sec id="s3c">
<title>Optimization of the experimental design</title>
<p>Optimizing the design in the context of, e.g., experimental psychology studies, amounts to identifying the subset of conditions and stimuli (<italic>u</italic>) that yields the highest statistical power, under a variety of practical constraints. From a modelling perspective, this essentially requires predicting experimental data (<italic>y</italic>) under models' assumptions (<italic>m</italic>), including potential confounds that may mask the effects of interest. Design optimization can become a difficult issue whenever the impact of experimental factors onto measurements (through the model) is non-trivial and/or uncertain (cf. unknown model parameters). This motivates the use of automatic design optimization.</p>
<p>The toolbox can handle two classes of problems, namely optimizing the system's input <italic>u</italic> with respect to either parameter estimation or model selection. These two problems correspond to two different objectives, which can be formalized in terms of statistical loss functions <xref ref-type="bibr" rid="pcbi.1003441-Robert1">[12]</xref>. For parameter estimation, one usually minimizes the trace of the expected posterior matrix (cf. so-called “A-optimality” <xref ref-type="bibr" rid="pcbi.1003441-Myung1">[13]</xref>). For model selection, one chooses the input <italic>u</italic> that minimizes the so-called “Laplace-Chernoff risk” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e053" xlink:type="simple"/></inline-formula>, which is an analytical approximation to the model selection error rate <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau4">[14]</xref>. For example, with two models and assuming that (i) both models are a priori equally likely, and (ii) both prior predictive densities have similar variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e054" xlink:type="simple"/></inline-formula>, the Laplace-Chernoff risk is given by:<disp-formula id="pcbi.1003441.e055"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e055" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e056" xlink:type="simple"/></inline-formula> is the difference in the first-order moment of the data predictions under model 1 and model 2, respectively (cf. <xref ref-type="fig" rid="pcbi-1003441-g003">Figure 3</xref>). Optimizing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e057" xlink:type="simple"/></inline-formula> with respect to the design <italic>u</italic> thus reduces to discriminating the predictions under the candidate models, either by increasing the distance between their first-order moments, and/or by decreasing their second-order moments <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e058" xlink:type="simple"/></inline-formula>. The latter derives from the gradient of the observation function with respect to model parameters.</p>
<fig id="pcbi-1003441-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g003</object-id><label>Figure 3</label><caption>
<title>Selection error rate and the Laplace-Chernoff risk.</title>
<p>The (univariate) prior predictive density of two generative models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e059" xlink:type="simple"/></inline-formula> (blue) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e060" xlink:type="simple"/></inline-formula> (green) are plotted as a function of data <italic>y</italic>, given an arbitrary design <italic>u</italic>. The dashed grey line shows the marginal predictive density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e061" xlink:type="simple"/></inline-formula> that captures the probabilistic prediction of the whole comparison set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e062" xlink:type="simple"/></inline-formula>. The area under the curve (red) measures the model selection error rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e063" xlink:type="simple"/></inline-formula>, which depends upon the discriminability between the two prior predictive density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e064" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e065" xlink:type="simple"/></inline-formula>. This is precisely what the Laplace-Chernoff risk <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e066" xlink:type="simple"/></inline-formula> is a measure of. Adapted from <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau4">[14]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g003" position="float" xlink:type="simple"/></fig>
<p>Critically, optimizing the classical efficiency of the design (i.e. statistical power) minimizes the Laplace-Chernoff risk for the equivalent Bayesian model comparison. This is important, since it allows one to generalise established experimental design rules to a data analysis. We refer the interested reader to <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau4">[14]</xref>.</p>
<p>Critically, the optimality of the design relates to the experimental question. In our example, the best design for assessing whether effort devaluates incentive value may not be the same as the best design for identifying the precise way in which this devaluation occurs. In addition, one might want to exploit the real-time accumulation of information to perform on-line design optimization. The toolbox is equipped to address such problems (see, e.g., section “Dynamic causal modelling of fMRI data” below).</p>
</sec><sec id="s3d">
<title>Organization of the toolbox</title>
<p>The toolbox is organized as follows:</p>
<list list-type="bullet"><list-item>
<p>The root folder (/DAVB) contains a core set of (matlab) routines that implement the VB approach to stochastic nonlinear state-space models <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau5">[15]</xref>. The main inversion routine is ‘VBA_NLStateSpaceModel.m’, which is described in more details below. The root folder also contains group-level inference schemes (cf. ‘VBA_groupBMS.m’, <xref ref-type="bibr" rid="pcbi.1003441-Stephan2">[16]</xref>) and post-hoc model selection tools (cf. ‘VBA_SavageDickey.m’, <xref ref-type="bibr" rid="pcbi.1003441-Friston2">[17]</xref>), as well as routines for model simulation (cf. ‘simulateNLSS.m’) and results visualization (cf. ‘VBA_ReDisplay.m’, ‘VBA_PPM.m’). Functions that perform post-hoc inversion diagnostics (such as Volterra kernel estimation, cf. ‘VBA_VolterraKernels.m’) are stored in this folder. Finally, this folder gathers routines that can be used to evaluate and optimize the efficiency of the experimental design, with respect to either model comparison or parameter estimation (cf. ‘VBA_designEfficiency.m’, <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau4">[14]</xref>).</p>
</list-item><list-item>
<p>The subfolder ‘/DAVB/stats&amp;plots’ contains non-specific routines that deal with either statistical and/or display issues, example of which include general linear model and classical contrast inference (‘GLM_contrast.m’), 3D visualization of time-dependant probability density functions (‘plotDensity.m’), or receiver operating characteristic analysis (‘doROC.m’)…</p>
</list-item><list-item>
<p>The subfolder ‘/DAVB/subfunctions’ contains all sorts of example evolution and observation functions, as well as demonstration scripts. A selection of the latter will be described in the next section. This is where demonstration scripts, as well as evolution and observation functions of models for behaviour and neuroimaging data are stored (see examples in section “<xref ref-type="sec" rid="s4">Results</xref>” below).</p>
</list-item></list>
<p>From a practical viewpoint, inserting a model into the toolbox only requires the specification of the observation and evolution functions (see below). In the next section, we will highlight a few example applications, in order to demonstrate the capabilities of the toolbox. Note that a link to an interactive graphical summary of the toolbox can be found on the toolbox's internet wiki pages (<ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/mbb-vb-toolbox/" xlink:type="simple">http://code.google.com/p/mbb-vb-toolbox/</ext-link>).</p>
</sec><sec id="s3e">
<title>Summary of the input/output format of the main functions</title>
<p>Most importantly, evolution and observation functions have to conform to a standardized I/O form: <italic>[fx,dfdx,dfdP] = fname(x,P,u,in)</italic>, where:</p>
<list list-type="bullet"><list-item>
<p><italic>x</italic>: current hidden state value</p>
</list-item><list-item>
<p><italic>P</italic>: evolution/observation parameter value</p>
</list-item><list-item>
<p><italic>u</italic>: current input value</p>
</list-item><list-item>
<p><italic>in</italic>: this is an arbitrary variable that can contain any additional information that has to be passed to evolution/observation functions</p>
</list-item><list-item>
<p><italic>fx</italic>: the current evaluation of the evolution/observation function</p>
</list-item><list-item>
<p><italic>dfdx/dfdP</italic>: these are optional output arguments, which quantify the gradients of the evolution/observation function w.r.t. to hidden states and parameters, respectively. The main inversion routine automatically detects whether these are returned by the evolution/observation function (if not, numerical derivation is used internally).</p>
</list-item></list>
<p>In addition, the main inversion routine (‘VBA_NLStateSpaceModel.m’) has the following I/O form: <italic>[posterior,out] = VBA_NLStateSpaceModel(y,u,f_fname,g_fname,dim,options)</italic>, where:</p>
<list list-type="bullet"><list-item>
<p><italic>y/u</italic>: these are the observed data and controlled input to the system (see above).</p>
</list-item><list-item>
<p><italic>f_fname</italic> (resp. <italic>g_fname</italic>): name/handle of the function that returns the evolution (resp. observation) of the hidden states.</p>
</list-item><list-item>
<p><italic>dim</italic>: a structure variable containing the dimensions of the 3 sets of the model's unknown variables (<italic>n</italic>, <italic>n_theta</italic> and <italic>n_phi</italic>).</p>
</list-item><list-item>
<p><italic>options</italic>: an optional structure containing specific information regarding the model, i.e.: prior sufficient statistics on model parameters, mircrotime resolution (see below), additional information that may have to be passed to evolution/observation functions, lag <italic>k</italic> for the VBA-Kalman forward pass, VB convergence variables (e.g. maximum number of iterations, minimum increment in free energy), delay matrix (see below), flag for continuous/categorical data (see below), etc…</p>
</list-item><list-item>
<p><italic>posterior</italic>: a structure variable whose fields contain the sufficient statistics (typically first and second order moments) of the VB approximation to the posterior densities over the observation/evolution/precision parameters and hidden-states time series.</p>
</list-item><list-item>
<p><italic>out</italic>: a structure variable that recapitulates the optional arguments (in case defaults were used) and provides supplementary information regarding the VB model inversion (e.g., model diagnostics: free energy, percentage of variance explained, Volterra kernels, etc…).</p>
</list-item></list>
<p>We refer the interested reader to the header of ‘VBA_NLStateSpaceModel.m’, as well as to example scripts described in the next section. In addition to matlab workspace variables, the main inversion routine returns a graphical summary of the VB inversion (this can be retrieved using <italic>VBA_ReDisplay(posterior,out)</italic>). This summary is organized in tabs, which are described on the toolbox's wiki page (<ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/mbb-vb-toolbox/wiki/Results" xlink:type="simple">http://code.google.com/p/mbb-vb-toolbox/wiki/Results</ext-link>).</p>
</sec></sec><sec id="s4">
<title>Results</title>
<p>We will first expose a couple of examples of standard models for behavioural and neuroimaging data, respectively. This will serve to demonstrate the capabilities of the toolbox. We will then focus on a few special cases of the broad class of generative models defined above. We believe these examples deserve special attention, given their relevance in the context of models for behavioural and neurobiological data. Finally, we will focus on the more specific issues of (i) performing model selection at the group level (given that the best model may differ across subjects), and (ii) using data-driven stochastic system identification together with inversion diagnostics to constrain and/or improve computational models of neurobiological and/or behavioural data.</p>
<sec id="s4a">
<title>Subject-level analyses: Examples</title>
<sec id="s4a1">
<title>Reinforcement learning models of choice data</title>
<p>In psychological terms, motivation can be defined as the set of processes that generate goals and thus determine behaviour. A goal is nothing else than a “state of affairs”, to which people attribute (subjective) value. Empirically speaking, one can access these values by many means, including subjective verbal report or decision making. Biological signals such as vegetative responses (e.g., skin conductance or pupil dilation) have also been linked to value, through its effect on arousal <xref ref-type="bibr" rid="pcbi.1003441-Bach1">[18]</xref>. These measures have been used to demonstrate how value changes as people learn a new operant response <xref ref-type="bibr" rid="pcbi.1003441-Daw1">[19]</xref>. This is predicted by reinforcement learning theories, which essentially relate behavioural response frequency to reward <xref ref-type="bibr" rid="pcbi.1003441-Thorndike1">[20]</xref>. In this context, value is expected reward, and it changes in proportion to the agent's prediction error, i.e. the difference between actual and expected reward <xref ref-type="bibr" rid="pcbi.1003441-Rescorla1">[21]</xref>. In the form of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> (first line), this learning rule can be captured by the following evolution function (in this case, the system is deterministic, cf. section “What about deterministic systems?”):<disp-formula id="pcbi.1003441.e067"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e067" xlink:type="simple"/><label>(7)</label></disp-formula>where <italic>x</italic> denotes value, <italic>u</italic> is the environmental feedback and <italic>α</italic> is the learning rate. In this notation, experienced reward depends upon the environmental feedback through some arbitrary scaling parameter <italic>β</italic>. For example, one may consider the possibility of some asymmetry in the relative weight of, e.g., financial gains and losses <xref ref-type="bibr" rid="pcbi.1003441-Kahneman1">[22]</xref>. This could be captured by letting <italic>β</italic> depend upon whether the feedback is positive (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e068" xlink:type="simple"/></inline-formula>) or negative (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e069" xlink:type="simple"/></inline-formula>). Alternatively, one may assume that asymmetry in the behavioural response to gains and losses may be due to different learning rates, (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e070" xlink:type="simple"/></inline-formula>).</p>
<p>The script ‘demo_asymRW.m’ simulates an experiment that aims at discriminating between these hypotheses. We consider a go/no-go choice design, whereby the (artificial) subject decides, at each trial, whether to gamble or not. On the one hand, if he gambles (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e071" xlink:type="simple"/></inline-formula>), he receives either a positive, negative or neutral feedback (e.g., 1€, −1€, 0€). On the other hand, if he chooses the ‘no-go’ option (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e072" xlink:type="simple"/></inline-formula>), he receives nothing (neutral feedback). The decision to gamble or not can be thought as an economic choice, which is driven by the learned value of the ‘go’ option. Here, we model the likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e073" xlink:type="simple"/></inline-formula> of choosing the ‘go’ option using the following softmax mapping of the ‘go’ value <italic>x</italic>:<disp-formula id="pcbi.1003441.e074"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e074" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e075" xlink:type="simple"/></inline-formula> captures a potential bias toward the ‘no-go’ option. Mathematical details regarding the definition of such categorical (Bernouilli) likelihood function are given in section “Handling categorical observations” below. Learning occurs after each ‘go’ trial, given the choice outcome <italic>u</italic> (cf. <xref ref-type="disp-formula" rid="pcbi.1003441.e067">equation 7</xref>). In this example, the feedbacks <italic>u</italic> for each ‘go’ trial were randomly sampled following the relative frequencies: 2/5 positive, 2/5 negative, 1/5 neutral.</p>
<p>We then performed a Bayesian model comparison of four models (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e076" xlink:type="simple"/></inline-formula>: asymmetric utility, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e077" xlink:type="simple"/></inline-formula>: asymmetric learning, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e078" xlink:type="simple"/></inline-formula>: both types of asymmetry, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e079" xlink:type="simple"/></inline-formula>: no asymmetry), given either observed choices (categorical data) or vegetative responses (continuous data). Here, the latter type of data is simply simulated by adding random noise to the true value time series (SNR = 1 dB). The ensuing likelihood function has the Gaussian form given in the second line of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref>, where the observation function <italic>g</italic> has been set to the identity mapping. Importantly, both types of data are simulated under model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e080" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1003441-g004">Figure 4</xref> summarizes the results of this simulation.</p>
<fig id="pcbi-1003441-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g004</object-id><label>Figure 4</label><caption>
<title>Comparison of asymmetric utility and asymmetric learning rate.</title>
<p>This figure summarizes the analysis of choice and value data using models that assume asymmetric utility, asymmetric learning rate, both asymmetries or none. <bold>Upper left</bold>: Trial-by-trial feedback history (either negative, neutral or positive). Grey neutral feedbacks correspond to ‘no-go’ choices. <bold>Upper right</bold>: Trial-by-trial dynamics of true value (red), measured value (black) and agent's binary go(1)/no-go(0) choices (black dots). <bold>Middle-left</bold>: posterior probability of the four models given simulated choice data. <bold>Middle-right</bold>: same format, given value data. <bold>Lower left</bold>: family posterior probabilities for both model spaces partitions, given choice data (left: family ‘yes’ = {‘utility’, ‘both’} vs family ‘no’ = {‘learning’, ‘none’}, right: family ‘yes’ = {‘learning’, ‘both’} vs family ‘no’ = {‘utility’, none’}. <bold>Lower left</bold>: same format, given value data.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g004" position="float" xlink:type="simple"/></fig>
<p>Not surprisingly, one can see that the value data is much more informative than the choice data. Overall, there is hardly any statistical evidence in favour of any form of asymmetry in the choice data. In contradistinction, Bayesian model comparison based upon value data correctly identifies the presence of asymmetry in the experienced reward (model ‘asymmetric utility’). We also performed family-inference, which consists in partitioning model space into subsets of models <xref ref-type="bibr" rid="pcbi.1003441-Penny1">[23]</xref>. Here, we chose two orthogonal partitions, which induce two pairs of model families: (i) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e081" xlink:type="simple"/></inline-formula> versus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e082" xlink:type="simple"/></inline-formula>, and (ii) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e083a" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e083b" xlink:type="simple"/></inline-formula> versus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e084" xlink:type="simple"/></inline-formula>. The first (resp. second) family comparison pools evidence for or against utility (resp. learning) asymmetry. Results of the family comparisons confirm the model comparisons, demonstrating that no dimension of the model space is strongly informed by choice data. In contradistinction, family inference given continuous value data (correctly) provides evidence for utility asymmetry, and against learning asymmetry.</p>
</sec><sec id="s4a2">
<title>Dynamic causal modelling of fMRI data</title>
<p>Decomposing the relation existing between cognitive functions and their neurobiological “signature” (the spatio-temporal properties of brain activity) requires an understanding of how information is transmitted through brain networks <xref ref-type="bibr" rid="pcbi.1003441-Sporns1">[24]</xref>. The ambition here is to ask questions such as: “what is the nature of the information that region A passes on to region B”? This stems from the notion of functional integration <xref ref-type="bibr" rid="pcbi.1003441-Tononi1">[25]</xref>, which views function as an emergent property of brain networks. Dynamic causal modelling –DCM- has been specifically developed to address this question (see the seminal DCM work in <xref ref-type="bibr" rid="pcbi.1003441-Friston3">[26]</xref>, and a recent review in <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau1">[5]</xref>). In DCM, hemodynamic (fMRI) signals arise from a network of functionally segregated sources; i.e., brain regions or neuronal sources. First, DCM describes how experimental manipulations (<italic>u</italic>) influence the dynamics of hidden neuronal states of the system (<italic>x</italic>). This is typically written in the form of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> (first line), where the neural evolution function is given by <xref ref-type="bibr" rid="pcbi.1003441-Stephan3">[27]</xref>:<disp-formula id="pcbi.1003441.e085"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e085" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e086" xlink:type="simple"/></inline-formula> is the discretization time step, and the parameters <italic>θ</italic> of this neural evolution function include a between-region coupling (matrix <italic>A</italic>), input-dependent coupling modulation (matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e087" xlink:type="simple"/></inline-formula>), input driving gains (matrix <italic>C</italic>) and gating effects (matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e088" xlink:type="simple"/></inline-formula>). DCM also includes the effect of the hemodynamic response function, which effectively performs a temporal convolution operation on neural states <xref ref-type="bibr" rid="pcbi.1003441-Stephan4">[28]</xref>.</p>
<p>An exhaustive assessment of the properties of DCM simulation and VB model inversion can be found elsewhere <xref ref-type="bibr" rid="pcbi.1003441-Friston4">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1003441-Li1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003441-Robert1">[12]</xref>. In this section, we will focus on the issue of online design optimization. We refer the interested reader to the script ‘demo_dcmonline.m’. This demo simplifies the network identification of <xref ref-type="bibr" rid="pcbi.1003441-Friston3">[26]</xref>. In brief, photic input enters V1, which is reciprocally connected to V5. The goal of the experiment is to assess whether attention modulates the feedforward connection from V1 to V5. This is addressed using Bayesian model comparison given fMRI data (i.e. model 1: attention modulates the V1→V5 connection; model 2: no modulatory effect). Within the session, each block consists of 16 seconds of stimulation and 16 of rest. The on-line design optimization consists in deciding, before each block, whether we modulate subjects' attention. This is done by comparing the design efficiency (i.e. –minus- the Laplace-Chernoff risk that is induced by the comparison of the two models) of two canonical block designs, i.e. photic stimulation with and without attentional modulation. Then, the fMRI response to the chosen stimulation is simulated under the true (but unknown) model. Critically, the evaluation of the design efficiency changes as the experiment unfolds, because both models are inverted given each new block dataset, yielding increasingly precise model predictions. In brief, the on-line design optimization procedure implements the experimental cycle of <xref ref-type="fig" rid="pcbi-1003441-g001">Figure 1</xref>, keeping the set of alternative hypotheses unchanged. <xref ref-type="fig" rid="pcbi-1003441-g005">Figure 5</xref> summarizes the results of this on-line design optimization.</p>
<fig id="pcbi-1003441-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g005</object-id><label>Figure 5</label><caption>
<title>Online design optimization for DCM comparison.</title>
<p>This figure summarizes the simulation of online design optimization, in the aim of best discriminating between two brain network models (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e089" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e090" xlink:type="simple"/></inline-formula>) given fMRI data time series. In this case, the problem reduces to deciding whether or not to introduce the second experimental factor (here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e091" xlink:type="simple"/></inline-formula> = attentional modulation), on top of the first factor (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e092" xlink:type="simple"/></inline-formula> = photic stimulation). <bold>Upper left</bold>: the two network models to be compared given fMRI data (top/bottom: with/without attentional modulation of the V1→V5 connection). <bold>Upper middle</bold>: block-by-block temporal dynamics of design efficiency of both types of blocks. Green (resp. blue) dots correspond to blocks with (resp. without.) attentional modulation. <bold>Upper right</bold>: scan-by-scan temporal dynamics of the optimized (on-line) design. <bold>Lower left</bold>: scan-by-scan temporal dynamics of the simulated fMRI signal (blue: V1, green: V5). <bold>Lower middle</bold>: block-by-block temporal dynamics of 95% posterior confidence intervals on the estimated modulatory effect (under model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e093" xlink:type="simple"/></inline-formula>). The green line depicts the strength of the simulated effect. <bold>Lower right</bold>: block-by-block temporal dynamics of log Bayes factors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e094" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g005" position="float" xlink:type="simple"/></fig>
<p>First, one can see that the design efficiency increases with time, as we expected. Second, the most efficient design alternates across blocks. Eventually, the best design (identified by the on-line procedure) is such that an attentional manipulation is performed each two blocks. It turns out that this is the maximally orthogonal design (correlation between u<sub>photic</sub> and u<sub>attention</sub> = 0.6), under the constraint that photic stimulation is always on. Interestingly, this reproduces the design chosen in the original fMRI experimental study <xref ref-type="bibr" rid="pcbi.1003441-Bchel1">[31]</xref>. In addition, one can see that the posterior credible interval of the attentional modulatory effect converges toward the true (simulated) value. Lastly, the log Bayes factor (as derived from VB free energies) increases with time. This indicates that there is increasing evidence in favour of the true model, as the experiment unfolds.</p>
</sec></sec><sec id="s4b">
<title>Special cases</title>
<sec id="s4b1">
<title>What about deterministic systems?</title>
<p>Deterministic systems can be understood as a particular case of stochastic nonlinear state-space models. They arise at the infinite state noise precision limit (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e095" xlink:type="simple"/></inline-formula>). Such limit follow from setting the Gamma priors accordingly, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e096" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e097" xlink:type="simple"/></inline-formula>. The hidden states trajectory <italic>x</italic> then becomes an implicit function of the evolution parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e098" xlink:type="simple"/></inline-formula>, the initial conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e099" xlink:type="simple"/></inline-formula> and the system's input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e100" xlink:type="simple"/></inline-formula> (if any). This implies that the identification of such deterministic systems can be treated like a static model (see below) of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e101a" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e101b" xlink:type="simple"/></inline-formula>. This simply means that, holding the evolution/observation parameters and initial conditions fixed, one invariably obtains the exact same deterministic predictions of the experimental data (up to measurement errors or residuals).</p>
<p>Note that the VB treatment of stochastic state-space models is systematically initialized by identifying the deterministic variant of the dynamical system. We invite the reader interested in the comparison of stochastic and deterministic model inversions to run demos such as ‘demo_Lorenz.m’, ‘demo_VanDerPol.m’ or ‘demo_doubleWell.m’, which reproduce the exemplar analyses in <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau5">[15]</xref>.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g006">Figure 6</xref> summarizes the comparison of the VB inversion of deterministic and stochastic variants of the Lorenz system (this Figure can be reproduced from the script ‘demo_Lorenz.m’). One can see the profound impact of state noise, both on hidden states trajectories and on the structure of model residuals (estimated measurement noise). More precisely, model inversion under the ‘no state-noise’ assumption leaves a lot of unexplained variance in the data. Critically, model residuals exhibit strong temporal structure (cf. lobes in the temporal autocorrelation). In addition, the magnitude of model residuals seems to depend upon the model predictions. Such structured model residuals typically signal <italic>underfitting</italic>. In contradistinction, the stochastic model inversion produces residuals that have no particular structure (they seem to be equally distributed around model predictions and have no temporal autocorrelation). In this example, Bayesian model comparison based upon the VB free energy correctly favours the stochastic model.</p>
<fig id="pcbi-1003441-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g006</object-id><label>Figure 6</label><caption>
<title>Comparison of deterministic and stochastic dynamical systems.</title>
<p>This figure summarizes the VB comparison of deterministic (upper row) and stochastic (lower row) variants of a Lorenz dynamical system, given data simulated under the stochastic variant of the model. <bold>Upper left</bold>: fitted data (x-axis) is plotted against simulated data (y-axis), for the deterministic case. Perfect model fit would align all points on the red line. <bold>Lower left</bold>: same format, for the stochastic case. <bold>Upper middle</bold>: 95% posterior confidence intervals on hidden-states dynamics. Recall that for deterministic systems, uncertainty in the hidden states arises from evolution parameters' uncertainty. <bold>Lower middle</bold>: same format, stochastic system. <bold>Upper right</bold>: residuals' empirical autocorrelation (y-axis) as a function of temporal lag (x-axis), for the deterministic system. <bold>Lower right</bold>: same format, stochastic system.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g006" position="float" xlink:type="simple"/></fig></sec><sec id="s4b2">
<title>Dealing with delays</title>
<p>Delayed dynamical systems are generally non-Markovian, in their native form. Consider, for example, a system whose evolution is given by: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e102" xlink:type="simple"/></inline-formula> (cf. sequence of Fibonacci numbers). Here, one needs to know both its current and delayed (one step back in time) state to predict the system's next state. VBA can deal with certain forms of delays by embedding the state space <italic>x</italic> into an augmented (Markovian) state-space <italic>X</italic>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e103" xlink:type="simple"/></inline-formula>. Here, <italic>D</italic> is the maximum delay considered. The dimension of the embedded state space is now <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e104" xlink:type="simple"/></inline-formula>, which can be considerably high. The script ‘demo_delays.m’ provides a demonstration of the inversion of a delayed stochastic dynamical system. The demo first sets up priors and optional arguments, which include the delay matrix <italic>D</italic>. In this case, the system is a two-dimensional linear system with (delayed) feedback.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g007">Figure 7</xref> summarizes the comparison of the ensuing VB inversions of (deterministic) delayed and non-delayed variants of the system. One can see the profound impact of delays, both on hidden states trajectories and on the structure of model residuals. In addition, the incorrect (non-delayed) model yields residuals with striking structure (cf. model fit and temporal autocorrelation). In comparison, the delayed (correct) model inversion displays rather weak residual structure. Here again, VB model comparison favours the correct model.</p>
<fig id="pcbi-1003441-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g007</object-id><label>Figure 7</label><caption>
<title>Comparison of delayed and non-delayed dynamical systems.</title>
<p>This figure summarizes the VB comparison of non-delayed (upper row) and delayed (lower row) variants of a linear deterministic dynamical system, given data simulated under the delayed variant of the model. This figure uses the same format as <xref ref-type="fig" rid="pcbi-1003441-g006">Figure 6</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g007" position="float" xlink:type="simple"/></fig>
<p>Alternatively, one may construct a corrected evolution function, using a first-order Taylor expansion around zero-delays. We refer the interested reader to the <xref ref-type="supplementary-material" rid="pcbi.1003441.s002">Text S1</xref> in the supplementary information (see also the appendix in <xref ref-type="bibr" rid="pcbi.1003441-David1">[32]</xref>).</p>
</sec><sec id="s4b3">
<title>Considering serially correlated noise</title>
<p>The toolbox can deal with most forms of auto-regressive or state-dependant noises (both at the level of hidden states and observations). It suffices to construct an augmented state space <italic>X</italic>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e105" xlink:type="simple"/></inline-formula>, and appropriately modify the evolution and observation functions, as well as the priors. For example, the evolution of a stochastic system driven with AR(1) noise could be modelled as follows:<disp-formula id="pcbi.1003441.e106"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e106" xlink:type="simple"/><label>(10)</label></disp-formula>where <italic>f</italic> is the evolution function on the original state space <italic>x</italic>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e107" xlink:type="simple"/></inline-formula> is its modification on the augmented state space <italic>X</italic>. Importantly, under <xref ref-type="disp-formula" rid="pcbi.1003441.e055">Equation 6</xref>, both AR(1) and white noise (respectively <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e108" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e109" xlink:type="simple"/></inline-formula>) can drive the system. To ensure that <italic>z</italic> is the most likely driving force, one can set the augmented state noise (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e110" xlink:type="simple"/></inline-formula>) covariance matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e111" xlink:type="simple"/></inline-formula>, such that its upper-left half block is close to zero. In addition, one may have to increase the lag <italic>k</italic>. This is because the effect of the AR(1) delayed state noise on the hidden states is maximal one sample ahead in time. On thus need to look one step back in time to infer on the delayed state noise.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g008">Figure 8</xref> summarizes the comparison of the VB inversion of a simple linear stochastic system with AR(1) state noise, having assumed AR(1) noise or not (cf. script ‘demo_AR1.m’). One can see the impact of correlation in state noise, both on hidden states trajectories and on the structure of model residuals. More precisely, residuals of the ‘white state-noise’ model exhibit a clear (negative) peak in their autocorrelation function, at lag one. This is due to the model's inability to capture AR(1) state noise. In contradistinction, the AR(1) model inversion show no temporal structure in its residuals. Interestingly, the AR(1) model yields residuals with a higher magnitude than the “white state-noise” model. This is because the AR(1) model imposed (correct) smoothness constraints on hidden states trajectories. This eventually prevented the model from <italic>overfitting</italic> the data, which seems to have occurred for the ‘white state-noise’ model.</p>
<fig id="pcbi-1003441-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g008</object-id><label>Figure 8</label><caption>
<title>Comparison of white and auto-correlated state-noise.</title>
<p>This figure summarizes the VB comparison of stochastic systems driven with either white (upper row) or auto-correlated (lower row) state noise. This figure uses the same format as <xref ref-type="fig" rid="pcbi-1003441-g006">Figure 6</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g008" position="float" xlink:type="simple"/></fig>
<p>Autoregressive processes of any order can be obtained by augmenting the state space with delayed state noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e112" xlink:type="simple"/></inline-formula>, where <italic>D</italic> is the order of the autoregressive process. This is basically using the above delay embedding trick (and requires an appropriate increase of the lag <italic>k</italic>). In fact, almost all forms of state-dependent noise can be modeled using essentially the same strategy. It suffices to replace the last term in the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1003441.e106">equation 10</xref> (second line) with any non-linear function of the augmented states (e.g.:<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e113" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e114" xlink:type="simple"/></inline-formula> plays the role of the state-dependent noise standard deviation).</p>
</sec><sec id="s4b4">
<title>Getting closer to continuous dynamical systems</title>
<p>The evolution equation of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref> is discrete in time. If one directly uses this as an approximation to a continuous dynamical system, then the approximation's accuracy strongly depends on the data sampling frequency (c.f. <xref ref-type="supplementary-material" rid="pcbi.1003441.s004">Text S3</xref> in the supplementary information). However, the toolbox allows one to specify a “microtime” resolution, which is used to recursively apply the evolution function between two time samples. This can be useful to control the time discretization errors introduced when approximating the original continuous dynamical system. For example, let us assume that the system obeys the following ODE: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e115" xlink:type="simple"/></inline-formula>. We wish to derive a prediction from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e116" xlink:type="simple"/></inline-formula> to the next time sample, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e117" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e118" xlink:type="simple"/></inline-formula> is the time lag between two data samples. One can do this by recursively applying the time discretization scheme on a smaller time grid. For example, if one uses an Euler discretization scheme (cf. <xref ref-type="supplementary-material" rid="pcbi.1003441.s004">Text S3</xref> in the supplementary information):<disp-formula id="pcbi.1003441.e119"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e119" xlink:type="simple"/><label>(11)</label></disp-formula>where <italic>δ</italic> is a small integration step (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e120" xlink:type="simple"/></inline-formula>). Then the recursive application of the Euler evolution function <italic>F</italic> yields:<disp-formula id="pcbi.1003441.e121"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e121" xlink:type="simple"/><label>(12)</label></disp-formula>where the function <italic>F</italic> is evaluated recursively (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e122" xlink:type="simple"/></inline-formula> times) between two time samples. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e123" xlink:type="simple"/></inline-formula> induces a “microtime” resolution grid. Note that this procedure does not allow state noise to enter anywhere else than at the times where data are sampled. This means that the impact of state noise will be somehow underestimated. However, in the context of deterministic systems, increasing the microtime resolution eventually yields very accurate approximations to continuous dynamics.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g009">Figure 9</xref> summarizes the impact of reducing the microtime resolution on the VB inversion of a model of the hemodynamic response function (cf. script ‘demo_HRF.m’). In this example, the structure of model residuals does not discriminate between the two variants of the model. However, one can see how different the estimated states' trajectories are. In addition, the impact of unknown model parameters on the model predictions depends upon the microtime resolution. This can be seen in the structure of the posterior correlation matrix. In this particular example, decreasing the microtime resolution eventually compromises parameter identifiability. In other words, the information that can be retrieved on the evolution/observation parameters is degraded when reducing the microtime resolution.</p>
<fig id="pcbi-1003441-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g009</object-id><label>Figure 9</label><caption>
<title>Effect of the micro-time resolution.</title>
<p>This figure summarizes the effect of relying on either a slow (upper row) or fast (lower row) micro-time resolution, when inverting nonlinear dynamical systems. <bold>Left</bold>: same format as <xref ref-type="fig" rid="pcbi-1003441-g006">Figure 6</xref>. <bold>Upper middle</bold>: estimated hidden-states dynamics at low micro-time resolution (data samples are depicted using dots). <bold>Lower middle</bold>: same format, fast micro-time resolution. <bold>Upper right</bold>: parameters' posterior correlation matrix, at low micro-time resolution. <bold>Lower middle</bold>: same format, fast micro-time resolution.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g009" position="float" xlink:type="simple"/></fig></sec><sec id="s4b5">
<title>Handling categorical observations</title>
<p>Strictly speaking, the generative model of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equations 1</xref>–<xref ref-type="disp-formula" rid="pcbi.1003441.e020">3</xref> copes with continuous data, for which there is a natural distance metric. Now if the data is categorical, there is no such natural metric, and one has to resort to probability distributions dealing with discrete events. For example, binary data can be treated as binomial (Bernouilli) samples, whose sufficient statistic (first-order moment) is given by the observation function. This means that the observation equation (second line of <xref ref-type="disp-formula" rid="pcbi.1003441.e012">Equation 1</xref>) is replaced by the following binomial likelihood function:<disp-formula id="pcbi.1003441.e124"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e124" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e125" xlink:type="simple"/></inline-formula>, by definition. Here, the measurement noise precision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e126" xlink:type="simple"/></inline-formula> and covariance components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e127" xlink:type="simple"/></inline-formula> are irrelevant. It turns out that this does not induce any major change in the VB inversion scheme under the Laplace approximation. We refer the interested reader to the <xref ref-type="supplementary-material" rid="pcbi.1003441.s003">Text S2</xref> in the supplementary information. In fact, when the dimension of the data is high enough (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e128" xlink:type="simple"/></inline-formula>), the empirical distribution of the ‘residuals’ <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e129" xlink:type="simple"/></inline-formula> will tend to a Gaussian density. This means that one can then approximate the above likelihood with a Gaussian density with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e130" xlink:type="simple"/></inline-formula> and (unknown) precision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e131" xlink:type="simple"/></inline-formula>.</p>
<p>An interesting application is binary data classification, which can be understood as a special case of <xref ref-type="disp-formula" rid="pcbi.1003441.e124">Equation 13</xref>. In the linear case, the observation mapping reduces to a linear mixture passed through a sigmoid mapping, i.e.: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e132" xlink:type="simple"/></inline-formula>, where <italic>u</italic> is an arbitrary vector of explanatory variables (i.e. features), and <italic>A</italic> (resp. <italic>b</italic>) is an unknown vector (resp. scalar) that encodes the linear mapping (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e133" xlink:type="simple"/></inline-formula>).</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g010">Figure 10</xref> (script ‘demo_bin.m’) summarizes the statistical performance of the VB approach to data classification. In brief, we have simulated categorical data under two different models, namely: H0 (no systematic link between features <italic>u</italic> and data <italic>y</italic>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e134" xlink:type="simple"/></inline-formula>) and H1 (which posits an arbitrary but non-zero mapping <italic>A</italic>). Both H0 and H1 were then inverted given either the first half (to perform classical cross-validation on test-data) or the full dataset (to perform Bayesian model comparison). This procedure was repeated 256 times, in order to derive average performance measures.</p>
<fig id="pcbi-1003441-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g010</object-id><label>Figure 10</label><caption>
<title>Binary data classification.</title>
<p>This figure exemplifies a classification analysis, which is used to infer on the link between a continuous variable <italic>X</italic> and a binary data <italic>y</italic>. The analysis is conducted on data simulated under either a null model (H0: no link) or a sigmoid mapping (H1). <bold>Upper left</bold>: the classification accuracy, in terms of the Monte-Carlo average probability of correct prediction under both types of data (left: H1, right: H0), for the training dataset. The green dots show the expected classification accuracy, using the true values of each model's set of parameters. The dotted red line depicts chance level. <bold>Upper right</bold>: same format, test dataset (no model fitting). <bold>Lower left</bold>: same format, for the log Bayes factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e135" xlink:type="simple"/></inline-formula>, given the training dataset. <bold>Lower right</bold>: same format, given the full (train+test) dataset.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g010" position="float" xlink:type="simple"/></fig>
<p>One can see how Bayesian model comparison replicates the generalizability measure of standard cross-validation procedures, provided the inference is based upon the whole dataset (rather than splat into two train/test halves). More precisely, one can see that classification performance reaches statistical significance when the data are simulated under H1, but not under H0. This falls from the predictive density over ‘test’ data obtained after the VB inversion of the H1 model given ‘train’ data. Equivalently, VB model comparison of H1 and H0 correctly identifies the true model, with greater confidence when the whole dataset is used for deriving the free energy.</p>
</sec><sec id="s4b6">
<title>Inverting static (hierarchical) models</title>
<p>Static models are simplifications of the above class of generative models, where the dimension of the hidden states and the evolution parameters tend to zero (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e136" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e137" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e138" xlink:type="simple"/></inline-formula>). In this case, the generative model reduces to a nonlinear observation equation, i.e.: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e139" xlink:type="simple"/></inline-formula>, with fixed priors on the observation parameters.</p>
<p>Alternatively, a simple (two-levels) hierarchical extension of this static model (whereby the priors are also learned) can easily be derived by actually removing the evolution/observation parameters, but retaining the initial conditions and the first hidden states, with, e.g., identity evolution function, i.e.:<disp-formula id="pcbi.1003441.e140"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e140" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e141" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e142" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e143" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e144" xlink:type="simple"/></inline-formula> are estimated. This is the typical structure for so-called “mixed-effects” models for analysis at the group level. Critically, estimating the “initial conditions” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e145" xlink:type="simple"/></inline-formula> as well as the “state noise” precision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e146" xlink:type="simple"/></inline-formula> enables one to infer on the group mean and variance.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g011">Figure 11</xref> (script ‘demo_RFX.m’) summarizes the statistical performance of the VB treatment of mixed-effect models (cf. <xref ref-type="disp-formula" rid="pcbi.1003441.e140">Equation 14</xref>). One can see that both the estimated group mean and the Bayesian model comparison are coherent, in terms of inferring whether there is a non-zero group-mean (second level effect). More precisely, the posterior estimate of the group mean is different from zero when the data are simulated under H1 (which posits an arbitrary but non-zero group mean), but not under H0 (zero group mean). Equivalently, the VB model comparison correctly identifies H1 from H0, across Monte-Carlo simulations.</p>
<fig id="pcbi-1003441-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g011</object-id><label>Figure 11</label><caption>
<title>Random-effect analysis.</title>
<p>This figure exemplifies a random-effect GLM analysis, which is used to infer on the group mean of an effect of interest. The analysis is conducted on data simulated under either a null model (H0: group mean is zero) or a non-zero RFX model (H1). <bold>Left</bold>: Monte-Carlo average of the VB-estimated group mean under H1, given both types of data (left: H1, right: H0). <bold>Left</bold>: same format, for the log Bayes factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e147" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g011" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s4c">
<title>Group-level Bayesian model selection</title>
<p>The issue of performing random effects Bayesian model selection (BMS) at the group level was originally addressed in <xref ref-type="bibr" rid="pcbi.1003441-Stephan2">[16]</xref>. In this work, models were treated as random effects that could differ between subjects and have a fixed (unknown) distribution in the population. Here, this hierarchical model is inverted using a VB scheme, to provide conditional estimates of the frequency with which any model prevails in the population. This random effects BMS procedure complements fixed effects procedures that assume subjects are sampled from a homogenous population with one (unknown) model (cf. the log group Bayes factor that sums log-evidences over subjects). <xref ref-type="bibr" rid="pcbi.1003441-Stephan2">[16]</xref> also introduced the notion of exceedance probability, which measures how likely it is that any given model is more frequent than all other models in the comparison set. These two summary statistics typically constitute the results of random effects BMS (see, e.g., <xref ref-type="bibr" rid="pcbi.1003441-DenOuden1">[33]</xref>). In addition, the toolbox also returns the model attributions, i.e. the posterior probability, for each subject, of being best described by each model.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g012">Figure 12</xref> (script ‘demo_bmc4glm.m’) demonstrates the random-effect group BMS approach, in the context of a simple static general linear model (GLM). In brief, we simulated two groups of 32 subjects (under arbitrary subject-specific GLMs), one of which only expressing half of the (four) factors. This allows us to derive the Monte-Carlo distribution of within-subjects' data under both a ‘full’ and a ‘reduced’ model. One can see the quality of the fit for a typical simulated dataset (SNR = 0 dB). The log-model evidence of both models was derived for each data (here, at the frequentist limit, cf. ‘lev_GLM.m’). First, observe that the Monte-Carlo histograms of the log-Bayes factors under each model are only partially separated. This is due to the amount of measurement noise. However, despite the relatively weak identifiability of the two models, the model attributions exhibit almost no uncertainty, and the exceedance probabilities clearly identify the underlying model.</p>
<fig id="pcbi-1003441-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g012</object-id><label>Figure 12</label><caption>
<title>Random-effect group-BMS.</title>
<p>This figure exemplifies a random-effect group-BMS analysis, which is used to infer on the best model at the group level. The analysis is conducted on two groups of 32 subjects, whose data were simulated under either a ‘full’ (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e148" xlink:type="simple"/></inline-formula>, group 1) or a ‘reduced’ (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e149" xlink:type="simple"/></inline-formula>, group 2) model. <bold>Upper left</bold>: simulated data (y-axis) plotted against fitted data (x-axis), for a typical simulation. <bold>Lower left</bold>: histograms of log Bayes factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e150" xlink:type="simple"/></inline-formula>, for both groups (red: group 1, blue: group 2). <bold>Upper middle</bold>: model attributions, for group 1. The posterior probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e151" xlink:type="simple"/></inline-formula> for each subject is coded on a black-and-white colour scale (black = 1, white = 0). <bold>Lower middle</bold>: same format, group 2. <bold>Upper right</bold>: exceedance probabilities, for group 1. The red line indicates the usual 95% threshold. <bold>Lower right</bold>: same format, group 2.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g012" position="float" xlink:type="simple"/></fig></sec><sec id="s4d">
<title>Improving computational models using inversion diagnostics</title>
<p>Identifying relevant mechanisms is arguably the most difficult task in modelling complex behavioural and/or biological data. In fact, one may not in a position to suggest an informed model for the data before the experiment. For example, when modelling how subjects update the value of alternative options given the feedback they receive, one may assume that the learning rate may change over trials. However, one may not know what the determinants of learning rate adaptation are. A practical solution to this problem is to first treat the learning rate as a stochastic hidden state, whose random walk dynamics cannot be <italic>a priori</italic> predicted. One would then use the VBA inversion of such a model to estimate the learning rate dynamics from, e.g., observed people's choices. Finally, one could then perform a Volterra decomposition of hidden states dynamics onto a set of appropriately chosen basis functions. This diagnostic analysis allows one to identify the hidden states' impulse response to experimentally controlled inputs to the system. We refer the interested reader to <xref ref-type="supplementary-material" rid="pcbi.1003441.s004">Text S3</xref> in the supplementary information for mathematical details regarding Volterra decompositions.</p>
<p><xref ref-type="fig" rid="pcbi-1003441-g013">Figure 13</xref> (script “demo_dynLearningRate.m”) demonstrates the above procedure, in the context of the two-armed bandit problem <xref ref-type="bibr" rid="pcbi.1003441-Sutton1">[34]</xref>. In brief, an agent has to choose between two alternative actions, each of which may yield positive or negative feedbacks. In our case, we reversed the action-outcome contingency every fifty trials. First, a series of choices are simulated, under an agent model that learns both the evolving action-outcome probabilities and their volatility <xref ref-type="bibr" rid="pcbi.1003441-Mathys1">[7]</xref>. The agent's inferred volatility increases after each contingency reversal, and then decays back to steady-state (cf. upper-left panel in <xref ref-type="fig" rid="pcbi-1003441-g013">Figure 13</xref>). The VBA toolbox is then used to invert a “dynamical” variant of the Q-learning model (cf. section “Reinforcement learning models of choice data”), given the agent's sequence of choices. More precisely, the state-space was augmented with the learning rate, whose state-noise precision was set hundred time smaller than that of action values. This is to ensure that stochastic deviations from deterministic learning dynamics originate from changes in learning rate. One can see (cf. lower-left panel in <xref ref-type="fig" rid="pcbi-1003441-g013">Figure 13</xref>) that the estimated learning rate dynamics strongly correlates with the simulated agent's inferred volatility (classical test: F = 176.1, p&lt;10<sup>−8</sup>). The ensuing Volterra decomposition was performed w.r.t. three input basis functions, namely: the agent's chosen action, the winning action (which might not be the chosen action), and the winning action instability. The latter input is one when the winning action has changed between the previous and the current trial, and is zero otherwise. The Q-learner's choice behaviour is driven by the difference in action values, which mainly responds to the history of winning actions (not shown). In contradistinction, the learning rate exhibits a stereotypical response to winning action instability (cf. middle panels in <xref ref-type="fig" rid="pcbi-1003441-g013">Figure 13</xref>). This diagnosis can then be used to augment Q-learning models with deterministic learning rate dynamics, whose impulse response mimic the estimated Volterra kernel. An example of the evolution function of such an augmented Q-learning model is given by (cf. <xref ref-type="supplementary-material" rid="pcbi.1003441.s004">Text S3</xref> in the supplementary information):<disp-formula id="pcbi.1003441.e152"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003441.e152" xlink:type="simple"/><label>(15)</label></disp-formula>where <italic>s</italic> is the sigmoid mapping, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e153" xlink:type="simple"/></inline-formula> is the agent's previous choice, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e154" xlink:type="simple"/></inline-formula> is the previous feedback, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e155" xlink:type="simple"/></inline-formula> is the winning action instability, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e156" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e157" xlink:type="simple"/></inline-formula>) is the value of the first (resp. second) action, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e158" xlink:type="simple"/></inline-formula> is the (inverse-sigmoid transformed) learning rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e159" xlink:type="simple"/></inline-formula> is its discrete temporal derivative. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e160" xlink:type="simple"/></inline-formula> weighs the impact of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e161" xlink:type="simple"/></inline-formula> onto the learning rate, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e162" xlink:type="simple"/></inline-formula> controls the decay rate of the impulse response. Such augmented Q-learning model predicts a transient acceleration of the learning rate following changes in the winning action whenever <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e163" xlink:type="simple"/></inline-formula>. This is confirmed by the VBA inversion of this model (cf. right panels in <xref ref-type="fig" rid="pcbi-1003441-g013">Figure 13</xref>). Finally, Bayesian model comparison yields overwhelming evidence in favour of the augmented Q-learning model, when compared to the standard Q-learning model (VBA free energies were: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e164" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003441.e165" xlink:type="simple"/></inline-formula>).</p>
<fig id="pcbi-1003441-g013" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003441.g013</object-id><label>Figure 13</label><caption>
<title>Improving Q-learning models with inversion diagnostics.</title>
<p>This figure demonstrates the added-value of Volterra decompositions, when deriving learning models with changing learning rates. <bold>Upper left</bold>: simulated belief (blue/red: outcome probability for the first/second action, green/magenta: volatility of the outcome contingency for the first/second action) of the Bayesian volatile learner (y-axis) plotted against trials (x-axis). <bold>Lower left</bold>: estimated hidden states of the deterministic variant of the dynamic learning rate model (blue/green: first/second action value, red: learning rate). This model corresponds to the standard Q-learning model (the learning rate is constant over time). <bold>Upper middle</bold>: estimated hidden states of the stochastic variant of the dynamic learning rate model (same format). Note the wide posterior uncertainty around the learning rate estimates. <bold>Lower middle</bold>: Volterra decomposition of the stochastic learning rate (blue: agent's chosen action, green: winning action, red: winning action instability). <bold>Upper right</bold>: estimated hidden states of the augmented Q-learning model (same format as before). <bold>Lower right</bold>: Volterra decomposition of the augmented Q-learning model's learning rate (same format as before).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003441.g013" position="float" xlink:type="simple"/></fig>
<p>This concludes the demonstration of the VBA toolbox.</p>
</sec></sec><sec id="s5">
<title>Availability and Future Directions</title>
<p>In this paper, we have exposed the main algorithmic components of the VBA toolbox, which implements a probabilistic treatment of nonlinear models for neurobiological and behavioural data. This toolbox aims at disseminating models and methods that serve experimental purposes, and providing a flexible platform, which modellers can easily contribute to.</p>
<p>VBA is under intense development as we speak. More precisely, the following additions to the current toolbox's version are under test:</p>
<list list-type="bullet"><list-item>
<p>Between-conditions and between-groups second-level Bayesian model selection. This rests on quantifying the evidence for a difference in model labels (resp. frequencies) across conditions (resp. groups). We refer the interested reader to <xref ref-type="bibr" rid="pcbi.1003441-Rigoux1">[35]</xref>.</p>
</list-item><list-item>
<p>Dual categorical/continuous data analysis. In particular, this is necessary for inverting models that aim at explaining concurrent neuroimaging time series and trial-by-trial behavioural observations such as choices.</p>
</list-item><list-item>
<p>VB inversion of mixture models (e.g., mixtures of gaussians and binomials). The objective here is to handle data-driven probabilistic clustering approaches, that can serve as reference points for model-based data analyses.</p>
</list-item><list-item>
<p>Extension of the nonlinear state-space model to arbitrary variance components. This is most useful when dealing with data pooled from qualitatively different sources with potentially very different SNRs (e.g., neuroimaging and skin conductance time series).</p>
</list-item><list-item>
<p>Higher-level functionalities that allow to handle multiple sessions, parameter mappings (e.g., for positivity constraints), factorial family partitioning of model space, etc … The need for such extensions increases as the diversity of VBA users' interests steadily grows.</p>
</list-item><list-item>
<p>Library of observation/evolution functions of models for behavioural and neuroimaging data. These include, but are not limited to: learning rules (e.g., bayesian belief updates with different forms of priors <xref ref-type="bibr" rid="pcbi.1003441-Daunizeau1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003441-Mathys1">7</xref>, deterministic exploration, cognitive dissonance effects <xref ref-type="bibr" rid="pcbi.1003441-Festinger1">[36]</xref>, …), canonical utility functions (e.g., delay discounting <xref ref-type="bibr" rid="pcbi.1003441-Bickel1">[37]</xref>, effort devaluation <xref ref-type="bibr" rid="pcbi.1003441-Meyniel1">[38]</xref>, risk attitude, …), neural spiking dynamics (e.g. Hodgkin-Huxley <xref ref-type="bibr" rid="pcbi.1003441-Hodgkin1">[39]</xref>, Fitz-Hugh-Nagumo <xref ref-type="bibr" rid="pcbi.1003441-FitzHugh1">[40]</xref>, …), neural meso-scale networks (e.g., Jansen-Ritt <xref ref-type="bibr" rid="pcbi.1003441-Jansen1">[41]</xref>, neural fields <xref ref-type="bibr" rid="pcbi.1003441-Amari1">[42]</xref>, …), etc…</p>
</list-item></list>
<p>Some of these extensions are already available from the current code distribution. We encourage the interested reader to look for appropriate key words in the demonstration scripts.</p>
<p>VBA's code is under open-source GNU General Public Licence (v2), and is freely downloadable from the toolbox's internet wiki pages (<ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/mbb-vb-toolbox/wiki/InstallingTheToolbox" xlink:type="simple">http://code.google.com/p/mbb-vb-toolbox/wiki/InstallingTheToolbox</ext-link>). These wiki pages expose a lot of user-oriented information, as well as detailed examples and screen captures. The wiki also serves to gather comments, criticism, suggestions and contribution of VBA users.</p>
</sec><sec id="s6">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003441.s001" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pcbi.1003441.s001" position="float" xlink:type="simple"><label>Software S1</label><caption>
<p>VBA source code. Note that an interactive graphical summary of the toolbox can be found on the toolbox's internet wiki pages (<ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/mbb-vb-toolbox/" xlink:type="simple">http://code.google.com/p/mbb-vb-toolbox/</ext-link>).</p>
<p>(ZIP)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003441.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1003441.s002" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>Dealing with unknown delays in systems' dynamics.</p>
<p>(DOCX)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003441.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1003441.s003" position="float" xlink:type="simple"><label>Text S2</label><caption>
<p>VB-Laplace inversion of models for categorical (binary) data.</p>
<p>(DOCX)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003441.s004" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1003441.s004" position="float" xlink:type="simple"><label>Text S3</label><caption>
<p>Mathematical details regarding the relationship between integral and differential forms, the transition from continuous to discrete time formulations and Volterra decompositions of systems' dynamics.</p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003441-Stephan1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name> (<year>2009</year>) <article-title>Dysconnection in Schizophrenia: From Abnormal Synaptic Plasticity to Failures of Self-monitoring</article-title>. <source>Schizophrenia Bull</source> <volume>35</volume><supplement>(3)</supplement>: <fpage>509</fpage>–<lpage>27</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Schmidt1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmidt</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Smieskova</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Aston</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Allen</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Brain connectivity abnormalities predating the onset of psychosis: correlation with the effect of medication</article-title>. <source>JAMA Psychiatry</source> <volume>70</volume><supplement>(9)</supplement>: <fpage>903</fpage>–<lpage>12</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Schofield1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schofield</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Crinion</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>AJ</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Changes in auditory feedback connections determine the severity of speech processing deficits after stroke</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>4260</fpage>–<lpage>4270</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Moran1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moran</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Symmonds</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>An in vivo assay of synaptic function mediating human cognition</article-title>. <source>Curr Biol</source> <volume>21</volume>: <fpage>1320</fpage>–<lpage>1325</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Daunizeau1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>David</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name> (<year>2011</year>) <article-title>Dynamic causal modeling: a critical review of the biophysical and statistical foundations</article-title>. <source>NeuroImage</source> <volume>58</volume><supplement>(2)</supplement>: <fpage>312</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Daunizeau2"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Den Ouden</surname><given-names>HEM</given-names></name>, <name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Observing the observer (I): meta-Bayesian models of learning and decision-making</article-title>. <source>PLoS ONE</source> <volume>5</volume><supplement>(12)</supplement>: <fpage>e15554</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Mathys1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathys</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>A Bayesian foundation for learning under uncertainty</article-title>. <source>Frontiers Hum Neurosci</source> <volume>5</volume>: <fpage>39</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Beal1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Beal M. (2003), Variational algorithms for approximate Bayesian inference. PhD thesis, Gatsby Computational Unit, University College London, UK.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Friston1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname></name>, <name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Peeny</surname><given-names>W</given-names></name> (<year>2007</year>) <article-title><italic>Variational free energy and the Laplace approximation</italic></article-title>. <source>Neuroimage</source> <volume>34</volume>: <fpage>220</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Kloeden1"><label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">Kloeden P. E., Platen E. (1999), Numerical solution of stochastic differential equations. Springer-Verlag, ISBN 3-540-54062-8.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Daunizeau3"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2012</year>) <article-title>Stochastic Dynamic Causal Modelling of fMRI data: should we care about neural noise?</article-title> <source>Neuroimage</source> <volume>62</volume>: <fpage>464</fpage>–<lpage>481</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Robert1"><label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Robert C. (2007), The Bayesian choice: From Decision-Theoretic Foundations to Computational Implementation. Springer, August 2007.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Myung1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Myung</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Pitt</surname><given-names>MA</given-names></name> (<year>2009</year>) <article-title>Optimal experimental design for model discrimination</article-title>. <source>Psychol Rev</source> <volume>116</volume>: <fpage>499</fpage>–<lpage>518</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Daunizeau4"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Preuschoff</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name> (<year>2011</year>) <article-title>Optimizing experimental design for comparing models of brain function</article-title>. <source>PLoS Comp. Biol</source> <volume>7</volume><supplement>(11)</supplement>: <fpage>e1002280</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Daunizeau5"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name> (<year>2009</year>) <article-title>Variational Bayesian identification and prediction of stochastic nonlinear dynamic causal models</article-title>. <source>Physica D</source> <volume>238</volume>: <fpage>2089</fpage>–<lpage>2118</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Stephan2"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Bayesian model selection for group studies</article-title>. <source>Neuroimage</source> <volume>46</volume>: <fpage>1004</fpage>–<lpage>1017</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Friston2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Post hoc Bayesian model selection</article-title>. <source>Neuroimage</source> <volume>56</volume>: <fpage>2089</fpage>–<lpage>2099</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Bach1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bach</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2010</year>) <article-title>Dynamic causal modelling of anticipatory skin conductance responses</article-title>. <source>Biological Psychology</source><supplement>(2010)</supplement>: <fpage>163</fpage>–<lpage>170</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Daw1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2008</year>) <article-title>The cognitive neuroscience of motivation and learning</article-title>. <source>Social Cogn</source> <volume>26</volume>: <fpage>593</fpage>–<lpage>620</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Thorndike1"><label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">Thorndike EL (1911) Animal intelligence. New York: Macmillan.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Rescorla1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Rescorla R. A., Wagner A. R. (1972) A theory of Pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement. In: Black AH, Prokasy WF (eds) Classical conditioning II: current research and theory. Appleton-Century-Crofts, New York, pp 64–99.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Kahneman1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kahneman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tversky</surname><given-names>A</given-names></name> (<year>1984</year>) <article-title>Choices, Values, and Frames</article-title>. <source>Am Psychol</source> <volume>39</volume><supplement>(4)</supplement>: <fpage>341</fpage>–<lpage>350</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Penny1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Joao</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Flandin</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Comparing Families of Dynamic Causal Models</article-title>. <source>PLoS Comp. Biol</source> <volume>6</volume><supplement>(3)</supplement>: <fpage>e1000709</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Sporns1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Chialvo</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Kaiser</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hilgetag</surname><given-names>CC</given-names></name> (<year>2004</year>) <article-title>Organization, development and function of complex brain networks</article-title>. <source>Trends Cog. Sci</source> <volume>8</volume><supplement>(9)</supplement>: <fpage>418</fpage>–<lpage>425</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Tononi1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name> (<year>1994</year>) <article-title>A measure for brain complexity: relating functional segregation and integration in the nervous system</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>91</volume>: <fpage>5033</fpage>–<lpage>5037</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Friston3"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>WD</given-names></name> (<year>2003</year>) <article-title>Dynamic Causal Modelling</article-title>. <source>Neuroimage</source> <volume>19</volume>: <fpage>1273</fpage>–<lpage>1302</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Stephan3"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Kasper</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Nonlinear dynamic causal models for fMRI</article-title>. <source>Neuroimage</source> <volume>42</volume>: <fpage>649</fpage>–<lpage>662</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Stephan4"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Weiskopf</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Drysdale</surname><given-names>PM</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2007</year>) <article-title>Comparing hemodynamic models with DCM</article-title>. <source>Neuroimage</source> <volume>38</volume>: <fpage>387</fpage>–<lpage>401</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Friston4"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name> (<year>2011</year>) <article-title>Network discovery with DCM</article-title>. <source>Neuroimage</source> <volume>56</volume>: <fpage>1202</fpage>–<lpage>1221</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Li1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2011</year>) <article-title>Generalized filtering and stochastic DCM for fMRI</article-title>. <source>Neuroimage</source> <volume>58</volume><supplement>(2)</supplement>: <fpage>442</fpage>–<lpage>457</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Bchel1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Büchel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>1997</year>) <article-title>Modulation of connectivity in visual pathways by attention: Cortical interactions evaluated with structural equation modelling and fMRI</article-title>. <source>Cerebral Cortex</source> <volume>7</volume>: <fpage>768</fpage>–<lpage>778</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-David1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kilner</surname><given-names>JM</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Dynamic causal modeling of evoked responses in EEG and MEG</article-title>. <source>Neuroimage</source> <volume>30</volume><supplement>(4)</supplement>: <fpage>1255</fpage>–<lpage>1272</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-DenOuden1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Den Ouden</surname><given-names>HEM</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Roiser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name> (<year>2010</year>) <article-title>Striatal prediction error modulates cortical coupling</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>3210</fpage>–<lpage>3219</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Sutton1"><label>34</label>
<mixed-citation publication-type="book" xlink:type="simple">Sutton R., Barto A. (1998), Reinforcement Learning. MIT Press. ISBN 0-585-02445-6.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Rigoux1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigoux</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name> (<year>2013</year>) <article-title>Bayesian model selection for group studies - revisited</article-title>. <source>Neuroimage</source> <volume>84</volume>: <fpage>971</fpage>–<lpage>85</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Festinger1"><label>36</label>
<mixed-citation publication-type="book" xlink:type="simple">Festinger L. (1985), A theory of cognitive dissonance, Stanford, CA: Stanford University Press, ISBN 0-8047-0131-8.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Bickel1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bickel</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Odum</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Madden</surname><given-names>G</given-names></name> (<year>1999</year>) <article-title>Impulsivity and cigarette smoking: delay discounting in current, never, and ex-smokers</article-title>. <source>Psychopharmacology</source> <volume>146</volume><supplement>(4)</supplement>: <fpage>447</fpage>–<lpage>454</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Meyniel1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyniel</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Sergent</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Rigoux</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name> (<year>2013</year>) <article-title>A neuro-computational account of how the human brain decides when to have a break</article-title>. <source>Proc Natl Acad Sci</source> <volume>110</volume><supplement>(7)</supplement>: <fpage>2641</fpage>–<lpage>2646</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Hodgkin1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hodgkin</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Huxley</surname><given-names>AF</given-names></name> (<year>1952</year>) <article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title>. <source>J Physiol</source> <volume>177</volume>: <fpage>500</fpage>–<lpage>544</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-FitzHugh1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>FitzHugh</surname><given-names>R</given-names></name> (<year>1961</year>) <article-title>Impulses and physiological states in theoretical models of nerve membrane</article-title>. <source>Biophysical J</source> <volume>1</volume>: <fpage>445</fpage>–<lpage>466</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Jansen1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jansen</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Rit</surname><given-names>VG</given-names></name> (<year>1995</year>) <article-title>Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns</article-title>. <source>Biol Cybern</source> <volume>73</volume>: <fpage>357</fpage>–<lpage>366</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003441-Amari1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>S</given-names></name> (<year>1977</year>) <article-title>Dynamics of pattern formation in lateral inhibition type neural fields</article-title>. <source>Biol Cybern</source> <volume>27</volume>: <fpage>77</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>