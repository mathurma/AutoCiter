<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00296</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003146</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subject>Learning and memory</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Efficient Partitioning of Memory Systems and Its Importance for Memory Consolidation</article-title>
<alt-title alt-title-type="running-head">Memory Consolidation in Partitioned Neural Systems</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Roxin</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>Stefano</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Centre de Recerca Matemàtica, Campus de Bellaterra, Bellaterra, Barcelona, Spain</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Beck</surname><given-names>Jeff</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Duke University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">sf2237@columbia.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: AR SF. Analyzed the data: AR SF. Wrote the paper: AR SF. Developed the model: AR SF.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>7</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>25</day><month>7</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>7</issue>
<elocation-id>e1003146</elocation-id>
<history>
<date date-type="received"><day>18</day><month>2</month><year>2013</year></date>
<date date-type="accepted"><day>3</day><month>6</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Roxin, Fusi</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Long-term memories are likely stored in the synaptic weights of neuronal networks in the brain. The storage capacity of such networks depends on the degree of plasticity of their synapses. Highly plastic synapses allow for strong memories, but these are quickly overwritten. On the other hand, less labile synapses result in long-lasting but weak memories. Here we show that the trade-off between memory strength and memory lifetime can be overcome by partitioning the memory system into multiple regions characterized by different levels of synaptic plasticity and transferring memory information from the more to less plastic region. The improvement in memory lifetime is proportional to the number of memory regions, and the initial memory strength can be orders of magnitude larger than in a non-partitioned memory system. This model provides a fundamental computational reason for memory consolidation processes at the systems level.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Memory is critical to virtually all aspects of behavior, which may explain why memory is such a complex phenomenon involving numerous interacting mechanisms that operate across multiple brain regions. Many of these mechanisms cooperate to transform initially fragile memories into more permanent ones (memory consolidation). The process of memory consolidation starts at the level of individual synaptic connections, but it ultimately involves circuit reorganization in multiple brain regions. We show that there is a computational advantage in partitioning memory systems into subsystems that operate on different timescales. Individual subsystems cannot both store large amounts of information about new memories, and, at the same time, preserve older memories for long periods of time. Subsystems with highly plastic synapses (fast subsystems) are good at storing new memories but bad at retaining old ones, whereas subsystems with less plastic synapses (slow subsystems) can preserve old memories but cannot store detailed new memories. Here we propose a model of a multi-stage memory system that exhibits the good features of both its fast and its slow subsystems. Our model incorporates some of the important design principles of any memory system and allows us to interpret in a new way what we know about brain memory.</p>
</abstract>
<funding-group><funding-statement>This work was funded by DARPA grant SyNAPSE, the Gatsby Foundation, the Kavli Foundation, the Swartz Foundation and NIH grant NIH-2R01 MH58754. AR acknowledges a Ramon y Cajal grant RYC-2011-08755. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Memories are stored and retained through a series of complex, highly coupled processes that operate on different timescales. In particular, it is widely believed that after the initial encoding of a sensory-motor experience, a series of molecular, cellular, and system-level alterations lead to the stabilization of an initial memory representation (memory consolidation). Some of these alterations occur at the level of local synapses, while others involve the reorganization and consolidation of different types of memories in different brain areas. Studies of patient HM revealed that medial temporal lobe lesions severely impair the ability to consolidate new memories, whereas temporally remote memories remain intact <xref ref-type="bibr" rid="pcbi.1003146-Scoville1">[1]</xref>. These results and more recent work (see e.g. <xref ref-type="bibr" rid="pcbi.1003146-Squire1">[2]</xref>) suggest that there may be distinct memory systems, and that memories, or some of their components, are temporarily stored in the medial temporal lobe and then transferred to other areas of the cortex. Is there any fundamental computational reason for transferring memories from one area to another? Here we consider memory models consisting of several stages, with each stage representing a region of cortex characterized by a particular level of synaptic plasticity. Memories are continuously transferred from regions with more labile synapses to regions with reduced but longer-lasting synaptic modifications. Here we refer to each region as a stage in the memory transfer process. We find that such a multi-stage memory model significantly outperforms single-stage models, both in terms of the memory lifetimes and the strength of the stored memory. In particular, memory lifetimes are extended by a factor that is proportional to the number of memory stages.</p>
<p>In a memory system that is continually receiving and storing new information, synaptic strengths representing old memories must be protected from being overwritten during the storage of new information. Failure to provide such protection results in memory lifetimes that are catastrophically low, scaling only logarithmically with the number of synapses <xref ref-type="bibr" rid="pcbi.1003146-Amit1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1003146-Fusi2">[5]</xref>. On the other hand, protecting old memories too rigidly causes memory traces of new information to be extremely weak, being represented by a small number of synapses. This is one of the aspects of the classic plasticity-rigidity dilemma (see also <xref ref-type="bibr" rid="pcbi.1003146-McCloskey1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1003146-McClelland1">[8]</xref>). Synapses that are highly plastic are good at storing new memories but poor at retaining old ones. Less plastic synapses are good at preserving memories, but poor at storing new ones.</p>
<p>A possible solution to this dilemma is to introduce complexity into synaptic modification in the form of metaplasticity, by which the degree of plasticity at a single synapse changes depending on the history of previous synaptic modifications. Such complex synapses are endowed with mechanisms operating on many timescales, leading to a power-law decay of the memory traces, as is widely observed in experiments on forgetting <xref ref-type="bibr" rid="pcbi.1003146-Wixted1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Wixted2">[10]</xref>. Furthermore, complex synapses can vastly outperform previous models due to an efficient interaction between these mechanisms <xref ref-type="bibr" rid="pcbi.1003146-Fusi3">[11]</xref>. We now show that allowing for a diversity of timescales can also greatly enhance memory performance at the systems level, even if individual synapses themselves are not complex. We do this by considering memory systems that are partitioned into different regions, the stages mentioned above, characterized by different degrees of synaptic plasticity. In other words, we extend the previous idea of considering multiple timescales at single synapses to multiple timescales of plasticity across different cortical areas.</p>
<p>To determine how best to partition such a memory system, we take the point of view of an engineer who is given a large population of synapses, each characterized by a specific degree of plasticity. Because we want to focus on mechanisms of memory consolidation at the systems level, we use a simple binary model in which synaptic efficacies take two possible values, weak and strong. Previous work has shown that binary synapses are representative of a much wider class of more realistic synaptic models <xref ref-type="bibr" rid="pcbi.1003146-Fusi2">[5]</xref>. It seems likely that the mechanisms for storing new memories exploit structural aspects and similarities with previously stored information (see e.g. semantic memories). In our work, we are interested in different mechanisms responsible for storing new information that has already been preprocessed in this way and is thus incompressible. For this reason, we restrict consideration to memories that are unstructured (random) and do not have any correlation with previously stored information (uncorrelated). After constructing multi-state models, we estimate and compare their memory performance both in terms of memory lifetime and the overall strength of their memory traces.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>The importance of synaptic heterogeneity</title>
<p>We first analyzed a homogeneous model (single partition), in which all the synapses have the same learning rate (see <xref ref-type="fig" rid="pcbi-1003146-g001">Fig. 1</xref>). We consider a situation in which new uncorrelated memories are stored at a constant rate. Synapses are assumed to be stable in the absence of any overwriting due to the learning of new memories. Each memory is stored by modifying a randomly selected subset of synapses. As the synapses are assumed to be bistable, we reduce all the complex processes leading to long term modifications to the probability that a synapse makes a transition to a different state. As memories are random and uncorrelated, the synaptic transitions induced by different memories will be stochastic and independent.</p>
<fig id="pcbi-1003146-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003146.g001</object-id><label>Figure 1</label><caption>
<title>Heterogeneity in synaptic learning rates is desirable.</title>
<p><bold>a.</bold> Upper left: Each synapse is updated stochastically in response to a plasticity event, and encodes one bit of information of one specific memory because it has only two states. For this reason, we can assign a color to each synapse which represents the memory that is stored. Lower left: Memories are encoded by subjecting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e001" xlink:type="simple"/></inline-formula> synapses to a pattern of plasticity events, here illustrated by different colors. These patterns, and hence the memories, are random and uncorrelated. The strength of a memory is defined as the correlation between the pattern of synaptic weights and the event being tracked. The degradation of encoded memories is due to the learning of new memories. Only four memories are explicitly tracked in this example: red, green, blue, gray. Those synapses whose state is correlated with previous memories are colored black. Right: Synaptic updating is stochastic in the model leading to variability in the signal for different realizations given the same sequence of events (dotted lines). A mean-field description of the stochastic dynamics captures signal strength averaged over many realizations. We measure the signal-to-noise ratio (SNR) which is the signal relative to fluctuations in the overlap of uncorrelated memories. <bold>b.</bold> There is a trade-off between the initial SNR and the memory lifetime: A large initial SNR can be achieved if the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e002" xlink:type="simple"/></inline-formula> of a synapse changing state is high (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e003" xlink:type="simple"/></inline-formula>), although the decay is rapid, i.e. the memory lifetime scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e004" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e005" xlink:type="simple"/></inline-formula> is the total number of synapses. Long lifetimes can be achieved for small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e006" xlink:type="simple"/></inline-formula> although the initial SNR is weak. Memory lifetime can be as large as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e007" xlink:type="simple"/></inline-formula>, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e008" xlink:type="simple"/></inline-formula>. SNR vs time curves are shown for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e009" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e010" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e011" xlink:type="simple"/></inline-formula>. <bold>c.</bold> In a heterogeneous population of synapses in which many <italic>q</italic>s are present, one can partially overcome the trade-off (black line). The initial SNR scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e012" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e013" xlink:type="simple"/></inline-formula> is the learning rate corresponding to the slowest population. The memory lifetime scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e014" xlink:type="simple"/></inline-formula>. Here there are 50 different <italic>q</italic>s, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e015" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e017" xlink:type="simple"/></inline-formula> synapses of each type. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e018" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003146.g001" position="float" xlink:type="simple"/></fig>
<p>To track a particular memory we take the point of view of an ideal observer who has access to the values of the strengths of all the synapses relevant to a particular memory trace (see also <xref ref-type="bibr" rid="pcbi.1003146-Fusi3">[11]</xref>). Of course in the brain the readout is implemented by complex neural circuitry, and the estimates of the strength of the memory trace based on the ideal observer approach provide us with an upper bound of the memory performance. However, given the remarkable memory capacity of biological systems, it is not unreasonable to assume that specialized circuits exist which can perform a nearly optimal readout, and we will describe later a neural circuit that replicates the performance of an ideal observer.</p>
<p>More quantitatively, to track a memory, we observe the state of an ensemble of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e019" xlink:type="simple"/></inline-formula> synapses and calculate the memory signal, defined as the correlation between the state of the ensemble at a time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e020" xlink:type="simple"/></inline-formula> and the pattern of synaptic modifications induced by the event of interest at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e021" xlink:type="simple"/></inline-formula>. Specifically, we can formalize this model description by assigning the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e022" xlink:type="simple"/></inline-formula> to a potentiated synapse and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e023" xlink:type="simple"/></inline-formula> to a depressed one. Similarly, a plasticity event is assigned a value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e024" xlink:type="simple"/></inline-formula> if it is potentiating and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e025" xlink:type="simple"/></inline-formula> if depressing. We then define a vector of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e026" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e027" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e028" xlink:type="simple"/></inline-formula> is the state of synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e029" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e030" xlink:type="simple"/></inline-formula>. Similarly, the memories are also vectors of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e031" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e032" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e033" xlink:type="simple"/></inline-formula> is the plasticity event to which synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e034" xlink:type="simple"/></inline-formula> is subjected at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e035" xlink:type="simple"/></inline-formula>. If we choose to track the memory presented at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e036" xlink:type="simple"/></inline-formula>, then we define the memory trace as the signal at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e037" xlink:type="simple"/></inline-formula>, which is just the dot product of two vectors, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e038" xlink:type="simple"/></inline-formula>. The signal itself is a stochastic variable, since the updating of the synaptic states is stochastic. This means that if one runs several simulations presenting exactly the same memories, the signal will be different each time, see right hand side of <xref ref-type="fig" rid="pcbi-1003146-g001">Fig. 1a</xref>. The mean signal, understood as the signal averaged over many realizations of the Markov process, can be computed analytically. For the homogeneous model, a continuous-time approximation to the mean signal takes the simple form of an exponential, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e039" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e040" xlink:type="simple"/></inline-formula> is the total number of synapses and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e041" xlink:type="simple"/></inline-formula> is the learning rate, see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref> and <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for details. We must compare this mean signal to the size of fluctuations in the model, i.e the noise.</p>
<p>The memory noise is given by the size of fluctuations in the overlap between uncorrelated patterns, which here is approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e042" xlink:type="simple"/></inline-formula>, see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for details. Therefore, the signal-to-noise ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e043" xlink:type="simple"/></inline-formula>. One can track a particular memory only until it has grown so weak it cannot be discerned from any other random memory. Memory lifetime, which is one measure of the memory performance, is then simply defined as the maximum time over which a memory can be detected. More quantitatively it is the maximum time over which the SNR is larger than 1. The scaling properties of the memory performance that we will derive do not depend on the specific critical SNR value that is chosen. Moreover, it is known that the scaling properties derived from the SNR are conserved in more realistic models of memory storage and memory retrieval with integrate-and-fire neurons and spike driven synaptic dynamics (see e.g. <xref ref-type="bibr" rid="pcbi.1003146-Amit2">[12]</xref>).</p>
<p>As we mentioned, the dynamics of the Markov model we consider are stochastic. Therefore, throughout the paper, we will discuss results from stochastic models for which we have derived corresponding mean-field descriptions. <xref ref-type="fig" rid="pcbi-1003146-g001">Fig. 1b</xref> shows the mean-field result for two extreme cases when all synapses have the same degree of plasticity. If the synapses are fast and the transition probability is high (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e044" xlink:type="simple"/></inline-formula>), then the memory is very vivid immediately after it is stored and the amount of information stored per memory is large, as indicated by the large initial SNR (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e045" xlink:type="simple"/></inline-formula>). However the memory is quickly overwritten as new memories are stored. In particular, the memory lifetime scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e046" xlink:type="simple"/></inline-formula> which is extremely inefficient: doubling the lifetime requires squaring the number of synapses.</p>
<p>It is possible to extend lifetimes by reducing the learning rate, and in particular by letting the learning rate scale with the number of synapses. For the smallest <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e047" xlink:type="simple"/></inline-formula> that still allows one to store sufficient information per memory (i.e. that allows for an initial SNR∼1), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e048" xlink:type="simple"/></inline-formula>, the memory lifetimes are extended by a factor that is proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e049" xlink:type="simple"/></inline-formula>. This trade-off between memory lifetime and initial SNR (i.e. the amount of information stored per memory) cannot be circumvented through the addition of a large number of synaptic states without fine-tuning the balance of potentiation and depression <xref ref-type="bibr" rid="pcbi.1003146-Fusi2">[5]</xref>.</p>
<p>These shortcomings can be partially overcome by allowing for heterogeneity in the transition probabilities within an ensemble of synapses. Specifically, if there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e050" xlink:type="simple"/></inline-formula> equally sized groups of synapses, each with a different transition probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e051" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e052" xlink:type="simple"/></inline-formula>), then the most plastic ones will provide a strong initial SNR while the least plastic ones will ensure long lifetimes. Intermediate time-scales are needed to bridge the gap between the extreme values. In <xref ref-type="fig" rid="pcbi-1003146-g001">Fig. 1c</xref> we plot the SNR as a function of time. Transition probabilities are taken to be of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e053" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e054" xlink:type="simple"/></inline-formula> is the fastest learning rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e055" xlink:type="simple"/></inline-formula> is the slowest learning rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e056" xlink:type="simple"/></inline-formula>. Time is expressed in terms of the number of uncorrelated memories on the lower axis, and we choose an arbitrary rate of new uncorrelated memories (one per hour) to give an idea of the different orders of magnitudes of the timescales that are at play (from hours to years). This model, which we call <italic>the heterogeneous model</italic> is already an interesting compromise in terms of memory performance: as we increase the number of synapses, if the slowest learning rate is scaled as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e057" xlink:type="simple"/></inline-formula>, then both the initial SNR and the memory lifetime scale advantageously with the number of synapses (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e058" xlink:type="simple"/></inline-formula>). Moreover, the model has the desirable property that the memory decay is a power law over a wide range of timescales, as observed in several experiments on forgetting <xref ref-type="bibr" rid="pcbi.1003146-Wixted3">[13]</xref>.</p>
</sec><sec id="s2b">
<title>The importance of memory transfer</title>
<p>In the heterogeneous model, the synapses operate on different timescales independently from each other. We now show that the performance can be significantly improved by introducing a feed-forward structure of interactions from the most plastic group to the least plastic group of synapses. How is this possible? While the least plastic synapses can retain memories for long times, their memory trace is weak. However, this memory trace can be boosted through periodic rewriting of already-stored memories. If a memory is still present in one of the groups of synapses (called hereafter a ‘memory stage’), the stored information can be used to rewrite the memory in the downstream stages, even long after the occurrence of the event that created the memory.</p>
<p>It is important to notice that not all types of rewriting can significantly improve all the aspects of the memory performance. For example, if all memories are simply reactivated the same number of times, then the overall learning rate changes, so that the initial memory trace becomes stronger, but the memory lifetimes are reduced by the same factor. Rather, an alternative strategy is to reactivate and rewrite a combination of multiple memories, one which has a stronger correlation with recent memories and a weaker correlation with the remote ones.</p>
<p>We have built a model, which we will call <italic>the memory transfer model</italic>, that implements this idea. We consider <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e059" xlink:type="simple"/></inline-formula> synapses divided into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e060" xlink:type="simple"/></inline-formula> interacting stages. We assume that all the stages have the same size and that synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e061" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e062" xlink:type="simple"/></inline-formula> can influence a counterpart synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e063" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e064" xlink:type="simple"/></inline-formula>. In particular, synapses in the first stage undergo stochastic event-driven transitions as before (<xref ref-type="fig" rid="pcbi-1003146-g002">Fig. 2a</xref>). They therefore encode each new memory as it is presented. On the other hand, synapses in downstream stages update their state stochastically after each memory is encoded in the first stage.</p>
<fig id="pcbi-1003146-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003146.g002</object-id><label>Figure 2</label><caption>
<title>The memory transfer model.</title>
<p><bold>a.</bold> Upper left: In the model, the state of each synapse in stage one is updated stochastically in response to the occurrence of plasticity events. The synapses of downstream stages update their state according to the state of upstream stages. Lower left: Memories are encoded by subjecting the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e065" xlink:type="simple"/></inline-formula> synapses in stage 1 of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e066" xlink:type="simple"/></inline-formula> stages to a pattern of plasticity events, here illustrated by different colors. The correlation of synaptic states with a memory is initially zero in downstream stages, and builds up over time through feed-forward interactions. Right: The consolidation model always outperforms the heterogeneous model without interactions at sufficiently long times. Here a two-stage model is illustrated. The dashed line is the SNR of the second stage in the heterogeneous model. See text for details. <bold>b.</bold> The memory wave: the memory trace (from <xref ref-type="disp-formula" rid="pcbi.1003146.e320">Eq. 1</xref>) in the consolidation model travels as a localized pulse from stage to stage (starting from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e067" xlink:type="simple"/></inline-formula>, in fast learning stages, presumably localized in the medial temporal lobe, and ending at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e068" xlink:type="simple"/></inline-formula>, in slow learning stages). Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e069" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e070" xlink:type="simple"/></inline-formula>. Stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e071" xlink:type="simple"/></inline-formula> has a learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e072" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e073" xlink:type="simple"/></inline-formula>. New memories are encoded at a rate of one per hour.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003146.g002" position="float" xlink:type="simple"/></fig>
<p>Specifically, at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e074" xlink:type="simple"/></inline-formula>, a memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e075" xlink:type="simple"/></inline-formula> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e076" xlink:type="simple"/></inline-formula> consisting of a random pattern of potentiating (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e077" xlink:type="simple"/></inline-formula>) and depressing (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e078" xlink:type="simple"/></inline-formula>) events is presented to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e079" xlink:type="simple"/></inline-formula> synapses in stage one, which have synaptic state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e080" xlink:type="simple"/></inline-formula>. Synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e081" xlink:type="simple"/></inline-formula> is subjected either to a potentiating (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e082" xlink:type="simple"/></inline-formula>) or to a depressing (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e083" xlink:type="simple"/></inline-formula>) event with probability 1/2, and is updated with a probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e084" xlink:type="simple"/></inline-formula> as in the previous models. Therefore, the updating for synapses in stage 1 is identical to that for ensemble 1 in the synaptic model with heterogeneous transition probabilities which we discussed previously. Now, however, we assume that a synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e085" xlink:type="simple"/></inline-formula> in stage 2 is influenced by the state of synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e086" xlink:type="simple"/></inline-formula> in stage 1 in the following way. If synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e087" xlink:type="simple"/></inline-formula> in stage 1 is in a potentiated (depressed) state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e088" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e089" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e090" xlink:type="simple"/></inline-formula> respectively), then synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e091" xlink:type="simple"/></inline-formula> in stage 2 will potentiate (depress) at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e092" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e093" xlink:type="simple"/></inline-formula>. The update rule for synapses in stage 3 proceeds analogously, but depends now on the state of synapses in stage 2, and so on.</p>
<p>In other words, after each memory is stored, a random portion of the synaptic matrix of each stage is copied to the downstream stages with a probability that progressively decreases. We will show later that this process of “synaptic copying” can actually be mediated by neuronal activity which resembles the observed replay activity <xref ref-type="bibr" rid="pcbi.1003146-Kudrimoti1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1003146-ONeill1">[19]</xref>. Transition probabilities of the different memory stages are the same as in the heterogeneous model: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e094" xlink:type="simple"/></inline-formula>. We will follow the SNR for a particular memory by measuring the correlation of the synaptic states in each stage with the event of interest.</p>
<p>Once again, we can derive a mean-field description of the stochastic dynamics. The upshot is that the mean signal in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e095" xlink:type="simple"/></inline-formula> obeys the differential equation<disp-formula id="pcbi.1003146.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e096" xlink:type="simple"/></disp-formula>which expresses clearly how the signal in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e097" xlink:type="simple"/></inline-formula> is driven by that in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e098" xlink:type="simple"/></inline-formula>. This is precisely the mechanism behind the improvement of memory performance compared to the heterogenous model without interactions. The memory trace in the first stage decays exponentially as new memories are encoded, as in the homogeneous case (see <xref ref-type="fig" rid="pcbi-1003146-g002">Fig. 2a</xref>). Memory traces in downstream stages start from zero, increase as the synaptic states are propagated, and eventually decay once again to zero. Information about all the stored memories is transferred between stages because the synapses that are “copied” are correlated to all the memories that are still represented at the particular memory stage. The most plastic stages retain the memories for a limited time, but during this time they transfer them to less plastic stages. This explains why the memory traces of downstream stages are non-monotonic functions of time: at stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e099" xlink:type="simple"/></inline-formula>, the memory trace keeps increasing as long as the information about the tracked memory is still retained in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e100" xlink:type="simple"/></inline-formula>. The memory trace in the second stage is already greater than that of an equivalent heterogeneous model with independent synaptic groups (<xref ref-type="fig" rid="pcbi-1003146-g002">Fig. 2a</xref>). This effect is enhanced as more stages are added.</p>
<p>The memory trace takes the form of a localized pulse that propagates at an exponentially decreasing rate (<xref ref-type="fig" rid="pcbi-1003146-g002">Fig. 2b</xref>). It begins as a sharply peaked function in the fast learning stages but slowly spreads outward as it propagates toward the slow learning stages. This indicates that although the memory is initially encoded only in the first stage (presumably located in the medial temporal lobe), at later times it is distributed across multiple stages. Nonetheless, it has a well defined peak, meaning that at intermediate times the memory is most strongly represented in the synaptic structure of intermediate networks.</p>
<p>An analytical formula for the pulse can be derived, see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref> and <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic>, which allows us to calculate the SNR and memory lifetimes (<xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3</xref>). Now, when reading out the signal from several stages of the memory transfer model, we must take into account the fact that the noise will be correlated. This was not the case for the heterogeneous model without interactions. In fact, if we consider a naive readout which includes all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e101" xlink:type="simple"/></inline-formula> stages, the noise will increase weakly with the number of stages. On the other hand, if we only read out the combination of stages which maximizes the SNR, one can show that the noise is independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e102" xlink:type="simple"/></inline-formula> and very close to the uncorrelated case. In fact, this readout is equivalent to reading out only those groups whose SNR exceeds a fixed threshold, which could be learned, see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for more details.</p>
<fig id="pcbi-1003146-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003146.g003</object-id><label>Figure 3</label><caption>
<title>The consolidation model yields long lifetimes and large SNR.</title>
<p><bold>a.</bold> The SNR for two values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e103" xlink:type="simple"/></inline-formula> for a fixed number of synapses (solid lines: consolidation model, dotted lines: heterogeneous model without interactions). The initial SNR for both models scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e104" xlink:type="simple"/></inline-formula>. It then decays as power law (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e105" xlink:type="simple"/></inline-formula>) and finally as an exponential for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e106" xlink:type="simple"/></inline-formula> for the heterogeneous model and for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e107" xlink:type="simple"/></inline-formula> for the consolidation model. Three measures of interest are shown in the inset and in the bottom two panels. Inset: crossing time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e108" xlink:type="simple"/></inline-formula> between the SNR of the heterogeneous model and the SNR of the consolidation model as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e109" xlink:type="simple"/></inline-formula>. The heterogeneous model is better than the consolidation model only for very recent memories (stored in the last hours, compared to memory lifetimes of years). <bold>b.</bold> The SNR scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e110" xlink:type="simple"/></inline-formula> in the consolidation model when the SNR decay is approximately a power law (symbols: simulations, line: analytics). The SNR at indicated times is plotted as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e111" xlink:type="simple"/></inline-formula> for three different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e112" xlink:type="simple"/></inline-formula>. <bold>c.</bold> Lifetimes (i.e. time at which SNR = 1) in the consolidation model scale approximately as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e113" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e114" xlink:type="simple"/></inline-formula> is the fastest learning rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e115" xlink:type="simple"/></inline-formula> is the slowest). The memory lifetime is plotted vs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e116" xlink:type="simple"/></inline-formula> for three different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e117" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e118" xlink:type="simple"/></inline-formula> synapses evenly divided into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e119" xlink:type="simple"/></inline-formula> stages. Stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e120" xlink:type="simple"/></inline-formula> has a learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e121" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003146.g003" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3a</xref> shows the SNR for memories in the heterogeneous model (dashed lines) and the memory transfer model (solid lines) for a fixed number of synapses and different numbers of groups <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e122" xlink:type="simple"/></inline-formula>. The curves are computed using the optimal readout described above, for which noise correlations are negligible. Both the SNR for intermediate times and the overall lifetime of memories increase with increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e123" xlink:type="simple"/></inline-formula> in the memory transfer model. The increase in SNR is proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e124" xlink:type="simple"/></inline-formula>, see <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3b</xref>, while the lifetime is approximately linear in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e125" xlink:type="simple"/></inline-formula> for large enough <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e126" xlink:type="simple"/></inline-formula>, see <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3c</xref>. While the initial SNR is reduced compared to the heterogeneous model (by a factor proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e127" xlink:type="simple"/></inline-formula>), it overtakes the SNR of the heterogeneous model already at very short times (inset of <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3a</xref>).</p>
<p>Importantly, the memory transfer model also maintains the propitious scaling seen in the heterogeneous model of the SNR and memory lifetime with the number of synapses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e128" xlink:type="simple"/></inline-formula>. Specifically, if the slowest learning rate is scaled as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e129" xlink:type="simple"/></inline-formula>, then the very initial SNR scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e130" xlink:type="simple"/></inline-formula> (but almost immediately after the memory storage it scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e131" xlink:type="simple"/></inline-formula>) and the lifetime as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e132" xlink:type="simple"/></inline-formula>. Hence the lifetime is extended by a factor that is approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e133" xlink:type="simple"/></inline-formula> with respect to the memory lifetime of both the heterogeneous model and the cascade synaptic model <xref ref-type="bibr" rid="pcbi.1003146-Fusi3">[11]</xref> in which the memory consolidation process occurs entirely at the level of individual complex synapses. The improvement looks modest on a logarithmic scale, as in <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3a</xref>, however it becomes clear that it is a significant amelioration when the actual timescales are considered. In the example of <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3a</xref> the memory lifetime extends from three years for the heterogeneous model, to more than thirty years for the memory transfer model. As the memory lifetime extends, the initial signal to noise ratio decreases compared to the heterogeneous model (but not compared to the cascade model, for which it decreases as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e134" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e135" xlink:type="simple"/></inline-formula> is the number of levels of the cascade, or in other words, the complexity of the synapse). However, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e136" xlink:type="simple"/></inline-formula> reduction is small, and after a few memories the memory transfer model already outperforms the heterogeneous model. In the example of <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3</xref> the heterogeneous model has a larger SNR only for times of the order of hours. This time interval should be compared to the memory lifetime which is of the order of decades.</p>
</sec><sec id="s2c">
<title>Neuronal implementation and the role of replay activity</title>
<p>The consolidation model we have described involves effective interactions between synapses that must be mediated by neuronal activity. We now show that it is possible to build a neuronal model that implements these interactions. We consider a model of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e137" xlink:type="simple"/></inline-formula> identical stages, each one consisting of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e138" xlink:type="simple"/></inline-formula> recurrently connected McCulloch-Pitts neurons (the total number of plastic synapses is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e139" xlink:type="simple"/></inline-formula>). Neurons in each stage are connected by non-plastic synapses to the corresponding neurons in the next stage (feed-forward connections). See <xref ref-type="fig" rid="pcbi-1003146-g004">Fig. 4a</xref> for a scheme of the network architecture. The model operates in two different modes: encoding and transfer. Importantly, we must now be more careful concerning our definition of time. The unit of time we have used up until now was simply that of the encoding of a memory, i.e. one time step equals one memory. Now we have two different time scales: the encoding time scale and the neuronal time scale. The encoding time scale is just the same as before, i.e. it is the time between learning new memories. The neuronal time scale is much faster. Specifically, in the neuronal model we encode a new memory and then stimulate the neurons to drive the transfer of patterns of synaptic weights. The time-step used in the Hebbian learning process when a memory is encoded, as well as the time-step used during this transfer process is a neuronal time scale, perhaps from milliseconds to hundreds of milliseconds. The time between memory encodings, on the other hand, might be on the order of minutes or hours, for example.</p>
<fig id="pcbi-1003146-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003146.g004</object-id><label>Figure 4</label><caption>
<title>The neural network implementing the memory transfer model.</title>
<p><bold>a</bold> A schematic representation of the neural network architecture. Here we show stage 1 and 2, but the other memory stages are wired in the same way. Neurons are represented by triangles and synaptic connections by circles. The axons are red, purple and orange, and the dendritic trees are black. Each neuron connects to all the neurons in the same stage (recurrent connections, orange) and to the corresponding neuron in the downstream stage (feed-forward connections, purple). The recurrent connections are plastic whereas the feed-forward connections are fixed. <bold>b,c</bold> memory encoding: a pattern of activity is imposed in the first stage only and synapses are updated stochastically according to a Hebbian rule. Specifically, if both the pre and the post-synaptic neurons are simultaneously active (b, the activity is schematically represented by trains of spikes), the synapse is potentiated. If the pre-synaptic neuron is active and the post-synaptic neuron is inactive, the synapse is depressed (c).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003146.g004" position="float" xlink:type="simple"/></fig>
<p>During encoding, a subset of neurons in the first stage is activated by the event that creates the memory and the recurrent synapses are updated according to a Hebbian rule, see <xref ref-type="fig" rid="pcbi-1003146-g004">Fig. 4b,c</xref>. Specifically, one half of the neurons are randomly chosen to be activated (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e140" xlink:type="simple"/></inline-formula>), while the remaining neurons are inactive (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e141" xlink:type="simple"/></inline-formula>), where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e142" xlink:type="simple"/></inline-formula> is the state of the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e143" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e144" xlink:type="simple"/></inline-formula>. A synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e145" xlink:type="simple"/></inline-formula> is then potentiated (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e146" xlink:type="simple"/></inline-formula>) with a probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e147" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e148" xlink:type="simple"/></inline-formula> and is depressed (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e149" xlink:type="simple"/></inline-formula>) with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e150" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e151" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e152" xlink:type="simple"/></inline-formula> is a binary synapse from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e153" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e154" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e155" xlink:type="simple"/></inline-formula>. Consistent with the previous analysis, we assume that the neuronal patterns of activity representing the memories are random and uncorrelated. No plasticity occurs in the synapses of neurons in downstream stages during encoding.</p>
<p>During transfer, a random fraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e156" xlink:type="simple"/></inline-formula> of neurons in each stage is activated at one time step, and the network response then occurs on the following time-step due to recurrent excitatory inputs. Specifically, at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e157" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e158" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e159" xlink:type="simple"/></inline-formula> neurons which have been activated in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e160" xlink:type="simple"/></inline-formula>, and otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e161" xlink:type="simple"/></inline-formula>. At time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e162" xlink:type="simple"/></inline-formula> the recurrent input to a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e163" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e164" xlink:type="simple"/></inline-formula> due to this activation is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e165" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e166" xlink:type="simple"/></inline-formula> then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e167" xlink:type="simple"/></inline-formula> and otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e168" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e169" xlink:type="simple"/></inline-formula> is a threshold. At time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e170" xlink:type="simple"/></inline-formula> all neurons are silenced, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e171" xlink:type="simple"/></inline-formula> and then the process is repeated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e172" xlink:type="simple"/></inline-formula> times. The initially activated neurons at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e173" xlink:type="simple"/></inline-formula> are completely random and in general they will not be correlated with the neuronal representations of the stored memories. However, the neuronal response at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e174" xlink:type="simple"/></inline-formula> will be greatly affected by the recurrent synaptic connections. For this reason, the activity during the response will be partially correlated with the memories stored in the upstream stages, similar to what happens in observed replay activity (see e.g. <xref ref-type="bibr" rid="pcbi.1003146-Kudrimoti1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1003146-ONeill1">[19]</xref>).</p>
<p>During transfer, the activated neurons project to counterpart neurons in the downstream stage. Crucially, we assume here that the long-range connections from the upstream stage to the downstream one are up-regulated relative to the recurrent connections in the downstream stage. In this way, the downstream state is “taught” by the upstream one. In the brain this may occur due to various mechanisms which include neuromodulatory effects and other gating mechanisms that modulate the effective couplings between brain regions. Cholinergic tone, in particular, has been shown to selectively modulate hippocampal and some recurrent cortical synapses (see <xref ref-type="bibr" rid="pcbi.1003146-Hasselmo1">[20]</xref>) as well as thalamocortical synapses <xref ref-type="bibr" rid="pcbi.1003146-Blundon1">[21]</xref>. Recent studies have also shown that the interactions between cortical and subcortical networks could be regulated by changing the degree of synchronization between the rhythmic activity of different brain areas (see e.g. <xref ref-type="bibr" rid="pcbi.1003146-Miller1">[22]</xref>).</p>
<p>In our model we assumed that, due to strong feedforward connections, whenever <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e175" xlink:type="simple"/></inline-formula> we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e176" xlink:type="simple"/></inline-formula>. The pattern of activation in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e177" xlink:type="simple"/></inline-formula> therefore follows that of stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e178" xlink:type="simple"/></inline-formula> during the transfer process. Importantly plasticity only occurs in the recurrent synapses of the <italic>downstream</italic> stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e179" xlink:type="simple"/></inline-formula>, i.e. stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e180" xlink:type="simple"/></inline-formula> is ‘teaching’ stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e181" xlink:type="simple"/></inline-formula>. For illustration we first consider a simple learning rule which can perfectly copy synapses from stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e182" xlink:type="simple"/></inline-formula> to stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e183" xlink:type="simple"/></inline-formula>, but only for the special case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e184" xlink:type="simple"/></inline-formula>, i.e. single-neuron stimulation. Following this, we will consider a learning rule which provides for accurate but not perfect copying of synapses but which is valid for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e185" xlink:type="simple"/></inline-formula>.</p>
<p><xref ref-type="fig" rid="pcbi-1003146-g005">Fig. 5</xref> shows a schematic of the transfer process when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e186" xlink:type="simple"/></inline-formula>. In this simplest case, only one presynaptic synapse per neuron is activated. To successfully transfer this synapse to the downstream stage a simple rule can be applied. First, the threshold is set so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e187" xlink:type="simple"/></inline-formula>. If there is a presynaptic spike (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e188" xlink:type="simple"/></inline-formula>) followed by a postsynaptic spike (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e189" xlink:type="simple"/></inline-formula>), then potentiate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e190" xlink:type="simple"/></inline-formula>) with a probability equal to the intrinsic learning rate of the synapses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e191" xlink:type="simple"/></inline-formula>. If there is no postsynaptic spike (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e192" xlink:type="simple"/></inline-formula>) then the corresponding synapse should be depressed (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e193" xlink:type="simple"/></inline-formula>). This leads to perfect transfer.</p>
<fig id="pcbi-1003146-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003146.g005</object-id><label>Figure 5</label><caption>
<title>A schematic example of the transfer process.</title>
<p>Synapses are here transferred from stage 1 to stage 2, the same mechanism applies to any other two consecutive stages. During the transfer process the feed-forward connections are up-regulated, and the recurrent connection of the target stage are down-regulated. The process starts with the stimulation of a fraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e194" xlink:type="simple"/></inline-formula> of randomly chosen neurons in stage 1 (a). The activity of the neuron is schematically represented by a train of spikes. The axon branches of the activated neurons (in this example only one neuron is activated) are highlighted. (b) the spontaneous activation of neuron 1, causes the activation of the corresponding neuron in stage 2 and of the stage 1 neurons that are most strongly connected. The process of relaxation has started. (c) the recurrently connected neurons of stage 1 which are activated, excite and activate the corresponding neurons in stage 2. As a result of the consecutive activation of the two highlighted neurons in stage 2, the synapse pointed by an arrow is potentiated, ending up in the same state as the corresponding synapse in stage 1. The strength of one synapse in stage 1 has been successfully copied to the corresponding synapse in stage 2.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003146.g005" position="float" xlink:type="simple"/></fig>
<p>In general <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e195" xlink:type="simple"/></inline-formula> and therefore it is not possible to perfectly separate inputs with a single threshold. Nevertheless, a learning rule which can accurately copy the synapses in this general case is the following. Consider two thresholds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e196" xlink:type="simple"/></inline-formula>, which are ‘low’ and ‘high’ respectively. On any given transfer (there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e197" xlink:type="simple"/></inline-formula> of them per stage) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e198" xlink:type="simple"/></inline-formula> is set to one of these two thresholds with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e199" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e200" xlink:type="simple"/></inline-formula> then if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e201" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e202" xlink:type="simple"/></inline-formula>, then set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e203" xlink:type="simple"/></inline-formula> with a probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e204" xlink:type="simple"/></inline-formula>. In words, this says that if despite the high threshold, the presynaptic activity succeeded in eliciting postsynaptic activity, then the synapses in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e205" xlink:type="simple"/></inline-formula> must have been strong, therefore one should potentiate the corresponding synapses in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e206" xlink:type="simple"/></inline-formula>. Similarly if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e207" xlink:type="simple"/></inline-formula> then if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e208" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e209" xlink:type="simple"/></inline-formula>, then set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e210" xlink:type="simple"/></inline-formula> with a probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e211" xlink:type="simple"/></inline-formula>. In words, this says that if despite the low threshold, the presynaptic activity did not succeed in eliciting postsynaptic activity, then the synapses in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e212" xlink:type="simple"/></inline-formula> must have been weak, therefore one should depress the corresponding synapses in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e213" xlink:type="simple"/></inline-formula>. For this learning rule to work, both stages <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e214" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e215" xlink:type="simple"/></inline-formula> must be privy to the value of the threshold. Therefore, there must be some global (at least common to these two stages) signal available. This could be achieved via a dynamical brain state with long-range spatial correlations. For example, globally synchronous up-state and down-state transitions <xref ref-type="bibr" rid="pcbi.1003146-Steriade1">[23]</xref>, which are known to occur during so-called slow-wave sleep would be ideally suited to shift neuronal thresholds. Alternatively, theta oscillations have been shown to be coherent between hippocampus and prefrontal cortex in awake behaving rodents during working memory <xref ref-type="bibr" rid="pcbi.1003146-Hyman1">[24]</xref> and learning tasks <xref ref-type="bibr" rid="pcbi.1003146-Benchenane1">[25]</xref> and would also be suited to serve as a global signal for synaptic plasticity.</p>
<p>We have stated that this second learning rule involving two thresholds can lead to accurate learning in the general case. Concretely, we can completely characterize the transfer process between any two stages via two quantities: the transfer rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e216" xlink:type="simple"/></inline-formula>, which is the fraction of synapses transferred after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e217" xlink:type="simple"/></inline-formula> replays of the transfer process, and the accuracy of transfer <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e218" xlink:type="simple"/></inline-formula> which is the fraction of transferred synapses which were correctly transferred. Both of these quantities depend on the stimulation fraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e219" xlink:type="simple"/></inline-formula> and the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e220" xlink:type="simple"/></inline-formula> and can be calculated analytically, see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref>. In short, the stimulation of neurons during the transfer process leads to a unimodal input distribution which is approximately Gaussian for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e221" xlink:type="simple"/></inline-formula>. The transfer rate is proportional to the area in the tails of this distribution above the high threshold and below the low threshold, while the accuracy is the fraction of this area which is due only to strong synapses (above the high threshold) or to weak synapses (below the low threshold). It is easy to see that as the thresholds are moved away from the mean into the tails the transfer rate will decrease while the accuracy will increase. There is therefore a speed-accuracy tradeoff in the transfer process.</p>
<p>Additionally, the transfer process can be implemented even if we relax the assumption of strong one-to-one feedforward connections and allow for random feedforward projections, see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic>. In this case a two-threshold rule is still needed to obtain performance above chance level, although an analytical description is no longer straightforward.</p>
<p>The neuronal implementation of the transfer process reveals an important fact: the probability of correctly updating a synapse does not depend solely on its intrinsic learning rate, but rather on the details of the transfer process itself. In our simple model, the transfer rate is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e222" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e223" xlink:type="simple"/></inline-formula> is a factor which depends on the threshold of the learning process relative to the distribution of inputs and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e224" xlink:type="simple"/></inline-formula> is the intrinsic learning rate of the synapses in the downstream stage. Additionally, since the likelihood of a correct transfer is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e225" xlink:type="simple"/></inline-formula>, the rate of correct transfers is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e226" xlink:type="simple"/></inline-formula>, while there is also a “corruption” rate equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e227" xlink:type="simple"/></inline-formula> which is the probability of an incorrect transfer. Obviously, if a given fraction of synapses is to be transferred correctly, the best strategy is to make <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e228" xlink:type="simple"/></inline-formula> as close to one as possible and increase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e229" xlink:type="simple"/></inline-formula> accordingly. In the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e230" xlink:type="simple"/></inline-formula> the neuronal model is exactly equivalent to the mean-field model we studied earlier with the transfer rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e231" xlink:type="simple"/></inline-formula> playing the role of the learning rate. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e232" xlink:type="simple"/></inline-formula> a modified mean-field model with a “corruption” term can be derived, see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for details. <xref ref-type="fig" rid="pcbi-1003146-g006">Fig. 6</xref> illustrates that the neuronal implementation quantitatively reproduces the behavior of the synaptic mean-field model. Specifically, the transfer rate can be modified by changing the number of transfers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e233" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1003146-g006">Fig. 6a</xref>. In this case, although the intrinsic synaptic properties have not changed at all, learning and forgetting occur twice as fast if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e234" xlink:type="simple"/></inline-formula> is doubled. The combined SNR of ten stages with 1000 all-to-all connected neurons each averaged over ten realizations (symbols) is compared to the mean-field model (line) in <xref ref-type="fig" rid="pcbi-1003146-g006">Fig. 6</xref>. In this case, the parameters of the neuronal model have been chosen such that the transfer rates are equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e235" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e236" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003146-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003146.g006</object-id><label>Figure 6</label><caption>
<title>Memory consolidation in a neuronal model.</title>
<p><bold>a.</bold> The <italic>effective</italic> learning rate of a downstream synapse depends on the transfer process itself. Increasing the number of transfer repetitions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e237" xlink:type="simple"/></inline-formula> increases this rate leading to faster learning and faster forgetting. Shown is SNR of each of the first two stages. Symbols are averages of ten simulations, lines are from the mean-field model, see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref>. Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e238" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e239" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e240" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e241" xlink:type="simple"/></inline-formula> which gives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e242" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e243" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e244" xlink:type="simple"/></inline-formula>. <bold>b.</bold> The neuronal model is well described by the mean-field synaptic model. There are 10 stages, each with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e245" xlink:type="simple"/></inline-formula> all-to-all connected neurons. Parameters are chosen such that transfer rates are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e246" xlink:type="simple"/></inline-formula>. The solid line is for a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e247" xlink:type="simple"/></inline-formula> in the mean-field model. Shown is the combined SNR for all 10 stages.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003146.g006" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>In conclusion, we showed that there is a clear computational advantage in partitioning a memory system into distinct stages, and in transferring memories from fast to slow stages. Memory lifetimes are extended by a factor that is proportional to the number of stages, without sacrificing the amount of information stored per memory. For the same memory lifetimes, the initial memory strength can be orders of magnitude larger than in non-partitioned homogeneous memory systems. In the Results we focused on the differences between the heterogeneous and the memory system model. In Fig. S15 in <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> we show that the SNR of the memory transfer model (multistage model) is always larger than the SNR of homogeneous model for any learning rate. This is true also when one considers that homogeneous models can potentially store more information than the memory transfer model. Indeed, in the homogeneous model all synapses can be modified at the time of memory storage, not only the synapses of the first stage. However, the main limitation of homogeneous models with extended memory lifetimes comes from the tiny initial SNR. If one reduces the amount of information stored per memory to match the information stored in the memory transfer model, it is possible to extend an already long memory lifetime but the initial SNR reduces even further (see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for more details).</p>
<p>Our result complements previous studies (see e.g. <xref ref-type="bibr" rid="pcbi.1003146-McClelland1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Kli1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Battaglia1">[27]</xref>) on memory consolidation that show the importance of partitioning memory systems when new semantic memories are inserted into a body of knowledge. Two-stage memory models were shown to be fundamentally important to avoid catastrophic forgetting. These studies focused mostly on “memory reorganization”, as they presuppose that the memories are highly organized and correlated. We have solved a different class of problems that plague realistic memory models even when all the problems related to memory reorganization were solved. The problems are related to the storage of the memory component that contains only incompressible information, as in the case of random and uncorrelated memories. These problems are not related to the structure of the memories and to their similarity with previously stored information, but rather they arise from the assumption that synaptic efficacies vary in a limited range. We showed here that this problem, discovered two decades ago <xref ref-type="bibr" rid="pcbi.1003146-Amit1">[3]</xref> and partially solved by metaplasticity <xref ref-type="bibr" rid="pcbi.1003146-Fusi3">[11]</xref>, can also be solved efficiently at the systems level by transferring memories from one sub-system to another.</p>
<p>Our neuronal model provides a novel interpretation of replay activity. Indeed, we showed that in order to improve memory performance, synapses should be copied from one stage to another. The copying process occurs via the generation of neuronal activity, that reflects the structure of the recurrent synaptic connections to be copied. The synaptic structure, and hence the neuronal activity, is actually correlated with all past memories, although most strongly with recent ones. Therefore while this activity could be mistaken for passive replay of an individual memory, it actually provides a snapshot of all the information contained in the upstream memory stage. There is already experimental evidence that replay activity is not a mere passive replay <xref ref-type="bibr" rid="pcbi.1003146-Gupta1">[28]</xref>. Our interpretation also implies that the statistics of “replay” activity should change more quickly in fast learning stages like the medial temporal lobe, than in slow learning stages like pre-frontal cortex or some other areas of the cortex <xref ref-type="bibr" rid="pcbi.1003146-Ji1">[18]</xref>.</p>
<p>Our analysis also reveals a speed-accuracy trade off that is likely to be shared by a large class of neuronal models that implement memory transfer: the faster the memories are transferred (i.e. when a large number of synapses are transferred per “replay” and hence a small number of repetitions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e248" xlink:type="simple"/></inline-formula> is needed), the higher the error in the process of synaptic copying (<xref ref-type="fig" rid="pcbi-1003146-g006">Fig. 6a</xref>). Accuracy is achieved only when the number of synapses transferred per “replay” is small and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e249" xlink:type="simple"/></inline-formula> is sufficiently large. This consideration leads to a few requirements that seem to be met by biological systems. In particular, in order to have a large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e250" xlink:type="simple"/></inline-formula>, it is important that the transfer phases are brief, if the animal is performing a task. This implies that the synaptic mechanisms for modifying the synapses in the downstream stages should operate on short timescales, as in the case of Spike Timing Dependent Plasticity (STDP) (see e.g. <xref ref-type="bibr" rid="pcbi.1003146-Bi1">[29]</xref>). Alternatively, the transfer can occur during prolonged intervals in which the memory system is off-line and does not receive new stimuli (e.g. during sleep).</p>
<p>Although we have focused on the transfer of memories in our model, the neuronal model can additionally be used to read out memories. Specifically, the neuronal response of any stage (or several stages) to a previously encoded pattern is larger than to a novel pattern. This is true as long as the SNR, as we have used it in this paper i.e. synaptic overlap, is sufficiently large. This difference in neuronal response can be used by a read-out circuit to distinguish between learned and novel patterns, see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for a detailed implementation.</p>
<p>Our theory led to two important results which generate testable predictions. The results are: 1) the memory performance increases linearly with the number of memory stages, and 2) the memory trace should vary in a non-monotonic fashion in most of the memory stages. The first suggests that long-term memory systems are likely to be more structured than previously thought, although we cannot estimate here what the number of partitions should be, given the simplicity of the model. Some degree of partitioning has already been observed: for example graded retrograde amnesia extends over one or two years in humans with damage to area CA1 of the hippocampus, but can extend to over a decade if the entire hippocampus is damaged <xref ref-type="bibr" rid="pcbi.1003146-Squire2">[30]</xref>. Systematic lesion studies in animals should reveal further partitioning in the hippocampal-cortical pathway for consolidation of episodic memories.</p>
<p>A second prediction is related, since once the partitions have been identified, our work suggests that most stages should exhibit non-monotonic memory traces, although on different time-scales. In fact, a recent imaging study with humans revealed non-monotonic BOLD activation as a function of the age of memories that subjects recalled <xref ref-type="bibr" rid="pcbi.1003146-Smith1">[31]</xref>. Furthermore the non-monotonicity was observed only in cortical areas and not in hippocampus. Here multi-unit electrophysiology in animals would be desirable to obtain high signal-to-noise ratios for measuring the memory traces. An analysis such as the one proposed by <xref ref-type="bibr" rid="pcbi.1003146-Peyrache1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Peyrache2">[33]</xref>, in which spiking activity in rats during sleep was correlated with waking activity, should enable us to estimate the strength of a memory trace. We expect that the memory trace is a non-monotonic function of time in most memory areas. The initial trace is usually small or zero, it then increases because of the information transferred from the upstream memory stages, and it finally decreases as a consequence of the acquisition of new memories. The timescales of the rising phase should reflect the dynamics of the upstream memory stages, whereas the decay is more related to the inherent dynamical properties of the memory stage under consideration. Therefore, the position of the peak of the memory trace and the timescale of the decay give important indications on the position of the neural circuit in the memory stream and on the distribution of parameters for the different memory stages. The statistics of neural activity during memory transfer (replay activity) should reflect the synaptic connections and in particular it should contain a superposition of a few memory traces in the fast systems, and an increasingly larger number of traces in the slower systems. The statistics of the correlations with different memories should change rapidly in the fast systems, and more slowly in the slow systems (e.g. in the hippocampus the changes between two consecutive sleeping sessions should be larger than in cortical areas where longer-term memories are stored).</p>
<p>To obtain experimental evidence for these two sets of predictions, it is important to record neural activity for prolonged times, in general long enough to cover all the timescales of the neural and synaptic processes that characterize a particular brain area. This is important both to determine the time development of the memory traces and to understand the details of the neural dynamics responsible for memory transfer.</p>
<p>To estimate the SNR, one can analyze the recorded spike trains during rest and NREM sleep, when memory transfer is expected to occur. We believe that the strength of memory reactivation is related to our SNR. The analysis proposed in <xref ref-type="bibr" rid="pcbi.1003146-Peyrache1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Peyrache2">[33]</xref> should allow us to estimate the templates of memories that are reactivated during one particular epoch (the templates are the eigenvectors of the covariance matrix that contains the correlations between the firing rates of different neurons). The time development of the memory trace can be then studied by projecting the activity of a different epoch on the eigenvectors. The projections are a measure of the memory reactivation strength and they should be approximately a nonlinear monotonic function of the memory signal. This analysis not only would determine whether the memory trace is a non-monotonic function of time but it would also allow us to estimate the parameters that characterize its shape in different brain areas.</p>
<p>The memory model studied here is a simple abstraction of complex biological systems which illustrates important general principles. Among the numerous simplifications that we made, there are three that deserve additional discussion. The first one is about the representations of the random memories and the second one is about the synaptic dynamics.</p>
<p>The first simplification is that we implicitly assumed that the memory representations are dense, as all synapses are potentially modified every time a new memory is stored. In the brain these representations are likely to be sparse, especially in the early stages of the memory transfer model, which probably correspond to areas in the medial temporal lobe. Sparseness is known to be important for increasing memory capacity <xref ref-type="bibr" rid="pcbi.1003146-Amit1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Willshaw1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Tsodyks1">[35]</xref> and one may legitimately wonder why we did not consider more realistic sparse representations. However, in our simplified model sparser random representations are equivalent to lower learning rates if the average number of potentiations and depressions are kept balanced. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e251" xlink:type="simple"/></inline-formula> is the average fraction of synapses that are modified in the first stage (coding level), then all <italic>q</italic>s of the model should be scaled by the same factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e252" xlink:type="simple"/></inline-formula>. This does not change the scaling properties that we studied, except for a simple rescaling of times (the x-axis of the plots should be transformed as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e253" xlink:type="simple"/></inline-formula>) and SNR (SNR→SNR·<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e254" xlink:type="simple"/></inline-formula>). In conclusion, sparseness is certainly an important factor and we are sure that it plays a role in the memory consolidation processes of the biological brain. However here we focused on mechanisms that are independent from the coding level and hence we did not discuss in detail the effects of sparseness, which have been extensively studied elsewhere <xref ref-type="bibr" rid="pcbi.1003146-Amit1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Willshaw1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003146-Tsodyks1">[35]</xref>.</p>
<p>The second simplification that merits a further discussion is that the model synapses studied here have a single time-scale associated with each of them. Our model can be extended to include synaptic complexity as in <xref ref-type="bibr" rid="pcbi.1003146-Fusi3">[11]</xref>. In fact, allowing for multiple time-scales at the level of the single synapse should lessen the number of stages needed for a given level of performance. Specifically, time-scales spanning the several orders of magnitude needed for high SNR and long memory lifetimes can be achieved through a combination of consolidation processes both at the single synapse, and between spatially distinct brain areas.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<p>Here we include a brief description of the models and formulas used to generate the figures. For a detailed and comprehensive description of the models please refer to <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic>.</p>
<sec id="s4a">
<title>Simple models of synaptic memory storage</title>
<p>The homogeneous and heterogeneous synaptic models are comprised of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e255" xlink:type="simple"/></inline-formula> stochastically updated binary synapses which evolve in discrete time. In the homogeneous case all synapses have the same learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e256" xlink:type="simple"/></inline-formula>, while in the latter case there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e257" xlink:type="simple"/></inline-formula> groups of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e258" xlink:type="simple"/></inline-formula> synapses each. Each group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e259" xlink:type="simple"/></inline-formula> has a learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e260" xlink:type="simple"/></inline-formula>. At each time step all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e261" xlink:type="simple"/></inline-formula> synapses are subjected to a potentiation or depression with equal probability. The N-bit word of potentiations and depressions constitutes the memory to be encoded. The memory signal at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e262" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e263" xlink:type="simple"/></inline-formula> is the correlation of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e264" xlink:type="simple"/></inline-formula> synaptic states with a particular N-bit memory, and we use superscript <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e265" xlink:type="simple"/></inline-formula> to denote evolution in discrete time. The signal-to-noise ratio (SNR) is approximately (and is bounded below by) the signal divided by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e266" xlink:type="simple"/></inline-formula>, see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for more details.</p>
<p>To compare with these Markov models one can derive a mean-field description which captures the memory signal averaged over many realizations of the stochastic dynamics. This is done by considering the probability that a given synapse is in a given state as a function of time. Specifically, the probability of a single synapse with learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e267" xlink:type="simple"/></inline-formula> to be in the potentiated state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e268" xlink:type="simple"/></inline-formula> is just<disp-formula id="pcbi.1003146.e269"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e269" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e270" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e271" xlink:type="simple"/></inline-formula>.</p>
<p>In the case of the homogeneous synaptic model there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e272" xlink:type="simple"/></inline-formula> synapses with the same learning rate. The expected value of the signal averaged over realizations is then<disp-formula id="pcbi.1003146.e273"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e273" xlink:type="simple"/></disp-formula>and so the expected signal-to-noise ratio is<disp-formula id="pcbi.1003146.e274"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e274" xlink:type="simple"/></disp-formula>We can approximate the finite-time equation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e275" xlink:type="simple"/></inline-formula> with a continuous ordinary differential equation which, using the definition of SNR gives<disp-formula id="pcbi.1003146.e276"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e276" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e277"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e277" xlink:type="simple"/></disp-formula>the solution of which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e278" xlink:type="simple"/></inline-formula>. This equation is used to plot the curves in <xref ref-type="fig" rid="pcbi-1003146-g001">Fig. 1b</xref>. The heterogeneous case is analogous with<disp-formula id="pcbi.1003146.e279"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e279" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e280"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e280" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e281" xlink:type="simple"/></inline-formula> is the expected signal at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e282" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e283" xlink:type="simple"/></inline-formula>. This equation is used to plot the solid curve in <xref ref-type="fig" rid="pcbi-1003146-g001">Fig. 1c</xref>. The SNR in the heterogeneous model can be increased by reading out only some of the groups at any one point in time, as opposed to all of them. This optimal readout is used to plot the dashed curves in the top panel of <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3</xref>.</p>
</sec><sec id="s4b">
<title>The memory transfer model</title>
<p>Once again we assume there are a total of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e284" xlink:type="simple"/></inline-formula> synapses divided equally amongst <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e285" xlink:type="simple"/></inline-formula> stages. Synapses in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e286" xlink:type="simple"/></inline-formula> have learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e287" xlink:type="simple"/></inline-formula> and hence the fastest learning rate is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e288" xlink:type="simple"/></inline-formula> and slowest is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e289" xlink:type="simple"/></inline-formula>. Synapses in stage 1 are updated every time step in an identical fashion to those in group 1 of the heterogeneous model above. Synapses in downstream stages however, update according to the state of counterpart synapses in the upstream stage. Specifically, if a synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e290" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e291" xlink:type="simple"/></inline-formula> is potentiated (depressed) at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e292" xlink:type="simple"/></inline-formula>, then synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e293" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e294" xlink:type="simple"/></inline-formula> potentiates (depresses) at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e295" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e296" xlink:type="simple"/></inline-formula>. As before, the signal at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e297" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e298" xlink:type="simple"/></inline-formula> is written <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e299" xlink:type="simple"/></inline-formula>. This fully defines the stochastic model.</p>
<p>As before we can derive a mean-field description of the stochastic dynamics. In this case, the probability of a given synapse in stage 1 to be in a potentiated state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e300" xlink:type="simple"/></inline-formula> is <disp-formula id="pcbi.1003146.e301"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e301" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e302"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e302" xlink:type="simple"/></disp-formula>as in the simple models. The probability of a given synapse in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e303" xlink:type="simple"/></inline-formula> begin in a potentiated state can be written<disp-formula id="pcbi.1003146.e304"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e304" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e305"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e305" xlink:type="simple"/></disp-formula>see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for details. These equations reflect the fact that only synapses in stage 1 are updated due to the presentation of random, uncorrelated memories, while synapses in downstream stages are updated only due to the state of synapses in the preceding stage. The expected signal in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e306" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e307" xlink:type="simple"/></inline-formula>.</p>
<p>The continuous time approximation to the mean-field dynamics is given by the set of equations<disp-formula id="pcbi.1003146.e308"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e308" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e309"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e309" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e310"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e310" xlink:type="simple"/></disp-formula>with initial conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e311" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e312" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e313" xlink:type="simple"/></inline-formula> and we write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e314" xlink:type="simple"/></inline-formula> for the expected signal. These equations are used to plot the curves in <xref ref-type="fig" rid="pcbi-1003146-g002">Fig. 2a</xref> and the solid curves in the top panel of <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3</xref>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e315" xlink:type="simple"/></inline-formula> sufficiently large we can furthermore recast this system of ODEs as a PDE<disp-formula id="pcbi.1003146.e316"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e316" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e317"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e317" xlink:type="simple"/></disp-formula>where the spatial variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e318" xlink:type="simple"/></inline-formula>. An asymptotic solution to this equation valid for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e319" xlink:type="simple"/></inline-formula>, and taking now the SNR, is<disp-formula id="pcbi.1003146.e320"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e320" xlink:type="simple"/><label>(1)</label></disp-formula>see <italic><xref ref-type="supplementary-material" rid="pcbi.1003146.s001">Text S1</xref></italic> for details. This equation is used to plot the pulse solution shown in <xref ref-type="fig" rid="pcbi-1003146-g002">Fig. 2b</xref>. An optimal SNR, in which only some of the stages are read out, can be calculated based on <xref ref-type="disp-formula" rid="pcbi.1003146.e320">Eq. 1</xref> and is<disp-formula id="pcbi.1003146.e321"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e321" xlink:type="simple"/><label>(2)</label></disp-formula>which is valid for intermediate times where the SNR is powerlaw in form. This equation is used to plot the curves in <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3</xref> bottom left. Using <xref ref-type="disp-formula" rid="pcbi.1003146.e320">Eqs. 1</xref> and <xref ref-type="disp-formula" rid="pcbi.1003146.e321">2</xref> one can calculate the lifetime of memories as<disp-formula id="pcbi.1003146.e322"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e322" xlink:type="simple"/><label>(3)</label></disp-formula>if the SNR of the pulse is above one before reaching the last stage or<disp-formula id="pcbi.1003146.e323"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e323" xlink:type="simple"/><label>(4)</label></disp-formula>is the SNR drops below one already before reaching the last stage. <xref ref-type="disp-formula" rid="pcbi.1003146.e322">Eqs. 3</xref> and <xref ref-type="disp-formula" rid="pcbi.1003146.e323">4</xref> are used to plot the solid curves in <xref ref-type="fig" rid="pcbi-1003146-g003">Fig. 3</xref> bottom right.</p>
</sec><sec id="s4c">
<title>Neuronal implementation of the memory transfer model</title>
<p>There are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e324" xlink:type="simple"/></inline-formula> stages. Each stage is made up of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e325" xlink:type="simple"/></inline-formula> all-to-all coupled McCulloch-Pitts neurons. Each one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e326" xlink:type="simple"/></inline-formula> synapses (no self-coupling) can take on one of two non-zero values. Specifically, the synapse from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e327" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e328" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e329" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e330" xlink:type="simple"/></inline-formula>. Furthermore, there are one-to-one connections from a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e331" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e332" xlink:type="simple"/></inline-formula> to a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e333" xlink:type="simple"/></inline-formula> in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e334" xlink:type="simple"/></inline-formula>. The model operates in two distinct modes: Encoding and Transfer.</p>
<sec id="s4c1">
<title>Encoding</title>
<p>All memories are encoded only in stage 1. Specifically, one half of the neurons are randomly chosen to be activated (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e335" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e336" xlink:type="simple"/></inline-formula>), while the remaining neurons are inactive (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e337" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e338" xlink:type="simple"/></inline-formula>). A synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e339" xlink:type="simple"/></inline-formula> is then potentiated to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e340" xlink:type="simple"/></inline-formula> with a probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e341" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e342" xlink:type="simple"/></inline-formula> and is depressed with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e343" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e344" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c2">
<title>Transfer</title>
<p>A fraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e345" xlink:type="simple"/></inline-formula> of randomly chosen neurons in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e346" xlink:type="simple"/></inline-formula> is activated at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e347" xlink:type="simple"/></inline-formula>. Because of the powerful feedforward connections, the same subset of neurons is activated in stage <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e348" xlink:type="simple"/></inline-formula>. The recurrent connectivity may lead to postsynaptic activation in stage 1 neurons. Each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e349" xlink:type="simple"/></inline-formula> receives an input<disp-formula id="pcbi.1003146.e350"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e350" xlink:type="simple"/></disp-formula>at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e351" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e352" xlink:type="simple"/></inline-formula> if neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e353" xlink:type="simple"/></inline-formula> was activated and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e354" xlink:type="simple"/></inline-formula> otherwise. The input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e355" xlink:type="simple"/></inline-formula> is a random variable which for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e356" xlink:type="simple"/></inline-formula> is approximately Gaussian distributed with expected mean and variance<disp-formula id="pcbi.1003146.e357"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e357" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e358"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e358" xlink:type="simple"/></disp-formula>If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e359" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e360" xlink:type="simple"/></inline-formula> is the neuronal threshold, then neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e361" xlink:type="simple"/></inline-formula> is activated at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e362" xlink:type="simple"/></inline-formula>. Again, because of the powerful feedforward connections, the same subset of neurons in stage 2 is activated. We take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e363" xlink:type="simple"/></inline-formula> to be the same for all neurons and assume that it can take one of two values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e364" xlink:type="simple"/></inline-formula> with equal likelihood during each replay.</p>
<p>For a transfer process with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e365" xlink:type="simple"/></inline-formula> stimulations of a fraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e366" xlink:type="simple"/></inline-formula> of neurons, the fraction of synapses updated in the downstream stage, or the transfer rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e367" xlink:type="simple"/></inline-formula>, is a function of the area of the input distribution above (below) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e368" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e369" xlink:type="simple"/></inline-formula>). If the thresholds are placed equidistant from the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e370" xlink:type="simple"/></inline-formula>, then<disp-formula id="pcbi.1003146.e371"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e371" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e372"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e372" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003146.e373"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e373" xlink:type="simple"/></disp-formula>If the fraction of synapses transferred is small then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e374" xlink:type="simple"/></inline-formula>, which is the formula given in the text. Of those synapses which are updated, only some will be updated correctly. This is equal to the fraction of potentiated (depressed) synapses contributing to the total input above (below) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e375" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e376" xlink:type="simple"/></inline-formula>), and is<disp-formula id="pcbi.1003146.e377"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003146.e377" xlink:type="simple"/><label>(5)</label></disp-formula>Finally, the mean-field model describing the memory signal in each stage in the neuronal model is the same as in <xref ref-type="disp-formula" rid="pcbi.1003146.e320">Eqs. 1-1</xref> where the learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e378" xlink:type="simple"/></inline-formula> is now the transfer rate times the fraction of correct transfers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e379" xlink:type="simple"/></inline-formula>, and there is an additional decay term due to incorrect transfers of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e380" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003146.e381" xlink:type="simple"/></inline-formula>. This mean-field model is used to make the solid curves in <xref ref-type="fig" rid="pcbi-1003146-g006">Fig. 6</xref>, whereas the symbols are from the full, Markov model with McCulloch-Pitts neurons.</p>
</sec></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003146.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003146.s001" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>Additional model information.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We are grateful to Larry Abbott, Francesco Battaglia, Randy Bruno, Sandro Romani, Giulio Tononi, and John Wixted for many useful comments on the manuscript and for interesting discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003146-Scoville1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scoville</surname><given-names>WB</given-names></name>, <name name-style="western"><surname>Milner</surname><given-names>B</given-names></name> (<year>1957</year>) <article-title>Loss of recent memory after bilateral hippocampal lesions</article-title>. <source>J Neurol Neurosurg Psychiatr</source> <volume>20</volume>: <fpage>11</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Squire1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Squire</surname><given-names>LR</given-names></name>, <name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name> (<year>2011</year>) <article-title>The cognitive neuroscience of human memory since H.M</article-title>. <source>Annu Rev Neurosci</source> <volume>34</volume>: <fpage>259</fpage>–<lpage>288</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Amit1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Learning in neural networks with material synapses</article-title>. <source>Neural Computation</source> <volume>6</volume>: <fpage>957</fpage>–<lpage>982</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Fusi1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>Hebbian spike-driven synaptic plasticity for learning patterns of mean firing rates</article-title>. <source>Biological Cybernetics</source> <volume>17</volume>: <fpage>305</fpage>–<lpage>317</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Fusi2"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2007</year>) <article-title>Limits on the memory storage capacity of bounded synapses</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>485</fpage>–<lpage>493</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-McCloskey1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mc Closkey</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>NJ</given-names></name> (<year>1989</year>) <article-title>Catastrophic interference in connectionist networks: the sequential learning problem</article-title>. <source>G H Bower (ed) The Psychology of Learning and Motivation</source> <volume>24</volume>: <fpage>109</fpage>–<lpage>164</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Carpenter1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Carpenter G, Grossberg S (1991) Pattern Recognition by Self-Organizing Neural Networks. MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003146-McClelland1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>O'Reilly</surname><given-names>RC</given-names></name> (<year>1995</year>) <article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title>. <source>Psychol Rev</source> <volume>102</volume>: <fpage>419</fpage>–<lpage>457</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Wixted1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Ebbesen</surname><given-names>EB</given-names></name> (<year>1991</year>) <article-title>On the form of forgetting</article-title>. <source>Psychological Science</source> <volume>2</volume>: <fpage>409</fpage>–<lpage>415</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Wixted2"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Ebbesen</surname><given-names>EB</given-names></name> (<year>1997</year>) <article-title>Genuine power curves in forgetting: a quantitative analysis of individual subject forgetting functions</article-title>. <source>Mem Cognit</source> <volume>25</volume>: <fpage>731</fpage>–<lpage>739</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Fusi3"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2005</year>) <article-title>Cascade models of synaptically stored memories</article-title>. <source>Neuron</source> <volume>45</volume>: <fpage>599</fpage>–<lpage>611</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Amit2"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Mongillo</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>Spike-driven synaptic dynamics generating working memory states</article-title>. <source>Neural Comput</source> <volume>15</volume>: <fpage>565</fpage>–<lpage>596</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Wixted3"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name> (<year>2004</year>) <article-title>On Common Ground: Jost's (1897) law of forgetting and Ribot's (1881) law of retrograde amnesia</article-title>. <source>Psychol Rev</source> <volume>111</volume>: <fpage>864</fpage>–<lpage>879</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Kudrimoti1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kudrimoti</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name> (<year>1999</year>) <article-title>Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics</article-title>. <source>J Neurosci</source> <volume>19</volume>: <fpage>4090</fpage>–<lpage>4101</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Lee1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>MA</given-names></name> (<year>2002</year>) <article-title>Memory of sequential experience in the hippocampus during slow wave sleep</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>1183</fpage>–<lpage>1194</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Foster1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foster</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>MA</given-names></name> (<year>2006</year>) <article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title>. <source>Nature</source> <volume>440</volume>: <fpage>680</fpage>–<lpage>683</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Diba1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diba</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>1241</fpage>–<lpage>1242</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Ji1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ji</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>MA</given-names></name> (<year>2007</year>) <article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>100</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-ONeill1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Neill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pleydell-Bouverie</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Dupret</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Csicsvari</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Play it again: reactivation of waking experience and memory</article-title>. <source>Trends Neurosci</source> <volume>33</volume>: <fpage>220</fpage>–<lpage>229</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Hasselmo1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name> (<year>2006</year>) <article-title>The role of acetylcholine in learning and memory</article-title>. <source>Curr Opin Neurobiol</source> <volume>16</volume>: <fpage>710</fpage>–<lpage>715</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Blundon1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blundon</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Bayazitov</surname><given-names>IT</given-names></name>, <name name-style="western"><surname>Zakharenko</surname><given-names>SS</given-names></name> (<year>2011</year>) <article-title>Presynaptic gating of postsynaptically expressed plasticity at mature thalamocortical synapses</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>16012</fpage>–<lpage>16025</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Miller1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name>, <name name-style="western"><surname>Buschman</surname><given-names>TJ</given-names></name> (<year>2013</year>) <article-title>Cortical circuits for the control of attention</article-title>. <source>Curr Opin Neurobiol</source> <volume>23</volume>: <fpage>216</fpage>–<lpage>222</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Steriade1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steriade</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nuez</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Amzica</surname><given-names>F</given-names></name> (<year>1993</year>) <article-title>A novel slow (&lt;1 hz) oscillation of neocortical neurons in vivo: depolarizing and hyperpolarizing components</article-title>. <source>J Neurosci</source> <volume>13</volume>: <fpage>3252</fpage>–<lpage>3265</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Hyman1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyman</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Zilli</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Paley</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name> (<year>2010</year>) <article-title>Working memory performance correlates with prefrontal-hippocampal theta interactions but not with prefrontal neuron firing rates</article-title>. <source>Front Integr Neurosci</source> <volume>4</volume>: <fpage>2</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.07.002.2010" xlink:type="simple">10.3389/neuro.07.002.2010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003146-Benchenane1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benchenane</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Peyrache</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Khamassi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Tierney</surname><given-names>PL</given-names></name>, <name name-style="western"><surname>Gioanni</surname><given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Coherent theta os-cillations and reorganization of spike timing in the hippocampal-prefrontal network upon learning</article-title>. <source>Neuron</source> <volume>66</volume>: <fpage>921</fpage>–<lpage>936</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Kli1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kli</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Off-line replay maintains declarative memories in a model of hippocampal-neocortical interactions</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>286</fpage>–<lpage>294</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Battaglia1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Battaglia</surname><given-names>FP</given-names></name>, <name name-style="western"><surname>Pennartz</surname><given-names>CMA</given-names></name> (<year>2011</year>) <article-title>The construction of semantic memory: grammar-based rep-resentations learned from relational episodic information</article-title>. <source>Front Comput Neurosci</source> <volume>5</volume>: <fpage>36</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Gupta1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>van der Meer</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name> (<year>2010</year>) <article-title>Hippocampal replay is not a simple function of experience</article-title>. <source>Neuron</source> <volume>65</volume>: <fpage>695</fpage>–<lpage>705</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Bi1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name> (<year>1998</year>) <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>J Neurosci</source> <volume>18</volume>: <fpage>10464</fpage>–<lpage>10472</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Squire2"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Squire</surname><given-names>LR</given-names></name> (<year>1992</year>) <article-title>Memory and the Hippocampus: A synthesis from findings with rates, monkeys and humans</article-title>. <source>Psychol Rev</source> <volume>99</volume><supplement>(2)</supplement>: <fpage>195</fpage>–<lpage>231</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Smith1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>CN</given-names></name>, <name name-style="western"><surname>Squire</surname><given-names>LR</given-names></name> (<year>2009</year>) <article-title>Medial temporal lobe activity during retrieval of semantic memory is related to the age of the memory</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>930</fpage>–<lpage>938</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Peyrache1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peyrache</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Khamassi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Benchenane</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wiener</surname><given-names>SI</given-names></name>, <name name-style="western"><surname>Battaglia</surname><given-names>FP</given-names></name> (<year>2009</year>) <article-title>Replay of rule-learning related neural patterns in the prefrontal cortex during sleep</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>919</fpage>–<lpage>926</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Peyrache2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peyrache</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Benchenane</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Khamassi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wiener</surname><given-names>SI</given-names></name>, <name name-style="western"><surname>Battaglia</surname><given-names>FP</given-names></name> (<year>2010</year>) <article-title>Principal component analysis of ensemble recordings reveals cell assemblies at high temporal resolution</article-title>. <source>J Comput Neurosci</source> <volume>29</volume>: <fpage>309</fpage>–<lpage>325</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Willshaw1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Buneman</surname><given-names>OP</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname><given-names>HC</given-names></name> (<year>1969</year>) <article-title>Non-holographic associative memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>–<lpage>962</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003146-Tsodyks1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Feigel'man</surname><given-names>MV</given-names></name> (<year>1988</year>) <article-title>The enhanced storage capacity in neural networks with low activity level</article-title>. <source>Europhysics Letters (EPL)</source> <volume>6</volume>: <fpage>101</fpage>–<lpage>105</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>