<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00531</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003915</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group><subj-group><subject>Neuroscience</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Computer vision</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>
<alt-title alt-title-type="running-head">Deep Supervised Model Explains IT</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname><given-names>Seyed-Mahdi</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Medical Research Council, Cognition and Brain Sciences Unit, Cambridge, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Diedrichsen</surname><given-names>Jörn</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University College London, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">Seyed.KalighRazavi@mrc-cbu.cam.ac.uk</email> (SMKR); <email xlink:type="simple">nikolaus.kriegeskorte@mrc-cbu.cam.ac.uk</email> (NK)</corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: SMKR NK. Performed the experiments: SMKR NK. Analyzed the data: SMKR NK. Wrote the paper: SMKR NK. Implemented the models: SMKR.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>11</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>6</day><month>11</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>11</issue>
<elocation-id>e1003915</elocation-id>
<history>
<date date-type="received"><day>26</day><month>3</month><year>2014</year></date>
<date date-type="accepted"><day>11</day><month>9</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Khaligh-Razavi, Kriegeskorte</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Inferior temporal (IT) cortex in human and nonhuman primates serves visual object recognition. Computational object-vision models, although continually improving, do not yet reach human performance. It is unclear to what extent the internal representations of computational models can explain the IT representation. Here we investigate a wide range of computational model representations (37 in total), testing their categorization performance and their ability to account for the IT representational geometry. The models include well-known neuroscientific object-recognition models (e.g. HMAX, VisNet) along with several models from computer vision (e.g. SIFT, GIST, self-similarity features, and a deep convolutional neural network). We compared the representational dissimilarity matrices (RDMs) of the model representations with the RDMs obtained from human IT (measured with fMRI) and monkey IT (measured with cell recording) for the same set of stimuli (not used in training the models). Better performing models were more similar to IT in that they showed greater clustering of representational patterns by category. In addition, better performing models also more strongly resembled IT in terms of their within-category representational dissimilarities. Representational geometries were significantly correlated between IT and many of the models. However, the categorical clustering observed in IT was largely unexplained by the unsupervised models. The deep convolutional network, which was trained by supervision with over a million category-labeled images, reached the highest categorization performance and also best explained IT, although it did not fully explain the IT data. Combining the features of this model with appropriate weights and adding linear combinations that maximize the margin between animate and inanimate objects and between faces and other objects yielded a representation that fully explained our IT data. Overall, our results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Computers cannot yet recognize objects as well as humans can. Computer vision might learn from biological vision. However, neuroscience has yet to explain how brains recognize objects and must draw from computer vision for initial computational models. To make progress with this chicken-and-egg problem, we compared 37 computational model representations to representations in biological brains. The more similar a model representation was to the high-level visual brain representation, the better the model performed at object categorization. Most models did not come close to explaining the brain representation, because they missed categorical distinctions between animates and inanimates and between faces and other objects, which are prominent in primate brains. A deep neural network model that was trained by supervision with over a million category-labeled images and represents the state of the art in computer vision came closest to explaining the brain representation. Our brains appear to impose upon the visual input certain categorical divisions that are important for successful behavior. Brains might learn these divisions through evolution and individual experience. Computer vision similarly requires learning with many labeled images so as to emphasize the right categorical divisions.</p>
</abstract>
<funding-group><funding-statement>This work was funded by Cambridge Overseas Trust and Yousef Jameel Scholarship to SMKR; and by the Medical Research Council of the UK (programme MC-A060-5PR20) and a European Research Council Starting Grant (ERC-2010-StG 261352) to NK. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="29"/></counts><custom-meta-group><custom-meta id="data-availability" xlink:type="simple"><meta-name>Data Availability</meta-name><meta-value>The authors confirm that all data underlying the findings are fully available without restriction. The data has been used in previous studies, including a recent <italic>PLOS Computational Biology</italic> paper (‘A Toolbox for Representational Similarity Analysis’ Nili et al. 2014), and is already available from here: <ext-link ext-link-type="uri" xlink:href="http://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/" xlink:type="simple">http://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Visual object recognition is thought to rely on a high-level representation in the inferior temporal (IT) cortex, which has been intensively studied in humans and monkeys <xref ref-type="bibr" rid="pcbi.1003915-Desimone1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Cichy1">[12]</xref>. Object images that are less distinct in the IT representation are perceived as more similar by humans <xref ref-type="bibr" rid="pcbi.1003915-Mur1">[10]</xref> and are more frequently confused by humans <xref ref-type="bibr" rid="pcbi.1003915-Majaj1">[13]</xref> and monkeys <xref ref-type="bibr" rid="pcbi.1003915-Kiani1">[6]</xref>. IT cortex represents object images by response patterns that cluster according to conventional categories <xref ref-type="bibr" rid="pcbi.1003915-Kiani1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Bell1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Connolly1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Mur2">[16]</xref>. The strongest categorical division appears to be that between animates and inanimates. Within the animates, faces and bodies form separate sub-clusters <xref ref-type="bibr" rid="pcbi.1003915-Kiani1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Naselaris1">[15]</xref>.</p>
<p>Previous studies have compared the representational dissimilarity matrices (RDMs) of a small number of models (mainly low-level models) with human IT and some other brain areas <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte2">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Leeds1">[19]</xref>. One of the previously tested models was the HMAX model <xref ref-type="bibr" rid="pcbi.1003915-Serre1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Riesenhuber1">[21]</xref>, which was designed as a model of IT taking many of its architectural parameters from the neuroscience literature. The internal representation of one variant of the HMAX model failed to fully explain the IT representational geometry <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>. In particular, the HMAX model did not account for the category clustering observed in the IT representation.</p>
<p>This raises the question if any existing computational vision models, whether motivated by engineering or neuroscientific objectives, can more fully explain the IT representation and account for the IT category clustering. IT clearly represents visual shape. However, the degree to which categorical divisions and semantic dimensions are also represented is a matter of debate <xref ref-type="bibr" rid="pcbi.1003915-Huth1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Baldassi1">[23]</xref>. If visual features constructed without any knowledge of either category boundaries or semantic dimensions reproduced the categorical clusters, then we might think of IT as a <italic>purely</italic> visual representation. To the extent that knowledge of categorical boundaries or semantic dimensions is required to build an IT-like representation, IT is better conceptualized as a visuo-semantic representation.</p>
<p>Here we investigate a wide range of computational models <xref ref-type="bibr" rid="pcbi.1003915-KhalighRazavi1">[24]</xref> and assess their ability to account for the representational geometry of primate IT. Our study addresses the question of how well computational models from computer vision and neuroscience can explain the IT representational geometry. In particular, we investigated whether models not specifically optimized to distinguish categories can explain IT's categorical clusters and whether models trained using supervised learning with category labels better explain the IT representational geometry.</p>
<p>Evaluating a computational model requires a framework for relating brain representations and model representations. One approach is to directly predict the brain responses to a set of stimuli by means of the computational models. Because of its roots in the computational neuroscience of early visual areas, this approach is often referred to as receptive-field modeling. It has been successfully applied to cell recording, e.g. <xref ref-type="bibr" rid="pcbi.1003915-Pillow1">[25]</xref>, and fMRI data, e.g. <xref ref-type="bibr" rid="pcbi.1003915-Mitchell1">[26]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Dumoulin1">[28]</xref>. Here we attempt to test complex network models whose internal representations comprise many units (ranging from 99 to 2,904,000). The brain-activity data consist of hundreds of measured brain responses. In this scenario, the linear correspondency mapping between model units and brain responses is complex (a matrix of number of model units by number of brain responses). Estimating this linear map is statistically costly, requiring a combination of substantial additional data (for a separate set of stimuli) and prior assumptions (for regularizing the fit). Here we avoid these complications by testing the models in the framework of representational similarity analysis (RSA) <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte2">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte3">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte4">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Nili1">[30]</xref>, in which brain and model representations are compared at the level of the dissimilarity structure of the response patterns. The models, thus, predict the dissimilarities among the stimuli in the brain representation. This approach relies on the assumption that the measured responses preserve the geometry of the neuronal representational space. The representational geometry would be conserved to high precision if the measured responses sampled random dimensions of the neuronal representational space <xref ref-type="bibr" rid="pcbi.1003915-Ganguli1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Johnson1">[32]</xref>. The RSA framework enables us to test any pre-trained model directly with data from a single stimulus set.</p>
<p>We tested a total of 37 computational model representations. Some of the models mimic the structure of the ventral visual pathway (e.g. HMAX, VisNet, Stable model, SLF) <xref ref-type="bibr" rid="pcbi.1003915-Serre1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Riesenhuber1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Mutch1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Ghodrati2">[37]</xref>; others are more broadly biologically motivated (e.g. Biotransform, convolutional network) <xref ref-type="bibr" rid="pcbi.1003915-Sountsov1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky1">[41]</xref>; and the others are well-known computer-vision models (e.g. GIST, SIFT, PHOG, PHOW, self-similarity features, geometric blur) <xref ref-type="bibr" rid="pcbi.1003915-Oliva1">[42]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Ojala1">[48]</xref>. Some of the models use features constructed by engineers without training with natural images (e.g. GIST, SIFT, PHOG). Others were trained in an unsupervised fashion (e.g. HMAX and VisNet).</p>
<p>We also tested models that were supervised with category labels. Two of the models (GMAX and supervised HMAX) <xref ref-type="bibr" rid="pcbi.1003915-Ghodrati1">[35]</xref> were trained in a supervised fashion to distinguish animates from inanimates, using 884 training images. In addition, we tested a deep supervised convolutional neural network <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky1">[41]</xref>, trained by supervision with over a million category-labeled images from ImageNet <xref ref-type="bibr" rid="pcbi.1003915-Deng1">[49]</xref>.</p>
<p>We also attempted to recombine model features, so as to construct a representation resembling IT in both its categorical divisions and within-category representational geometry. We linearly recombined the features in two ways: (a) by <italic>reweighting</italic> features (thus stretching and squeezing the representational space along its original axes) and (b) by <italic>remixing</italic> the features, creating new features as linear combinations of the original features (thus performing general affine transformations). All unsupervised and supervised training and all reweighting and remixing was based on sets of images nonoverlapping with the image set used to assess how well models accounted for IT.</p>
<p>We analyzed brain responses in monkey IT (mIT; cell recording data acquired by Kiani and colleagues <xref ref-type="bibr" rid="pcbi.1003915-Kiani1">[6]</xref>) and human IT (hIT; fMRI data from <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>) for a rich set of color images of isolated objects spanning multiple animate and inanimate categories. The human fMRI measurements covered the entire ventral stream, so we also tested the models on fMRI data from the foveal confluence of early visual cortex (EVC), the lateral occipital complex (LOC), the fusiform face area (FFA), and the parahippocampal place area (PPA).</p>
<p>Internal representations of the HMAX model (the C2 stage) and several computer-vision models performed well on EVC. Most of the models captured some component of the representational dissimilarity structure in IT and other visual regions. Several models clustered the human faces, which were mostly frontal and had a high amount of visual similarity. However, all the unsupervised models failed to cluster human and animal faces that were very different in visual appearance in a single face cluster, as seen for human and monkey IT. The unsupervised models also failed to replicate IT's clear animate/inanimate division. The deep supervised convolutional network better captured the categorical divisions, but did not fully replicate the categorical clustering observed in IT. We proceeded to remix the features of the deep supervised model to emphasize the major categorical divisions of IT using maximum-margin linear discriminants. In order to construct a representation resembling IT, we combined these discriminants with the different representational stages of the deep network, weighting each discriminant and layer of the deep network so as to best explain the IT representational geometry. The resulting IT-geometry model, when tested with crossvalidation to avoid overfitting to the image set, explains our IT data. Our results suggest that intensive supervised training with large sets of labeled images might be necessary to model the IT representational space.</p>
</sec><sec id="s2">
<title>Results</title>
<p>The results for the 37 model representations are presented separately for two sets of representations. The first set comprises the not-strongly-supervised representations (<xref ref-type="fig" rid="pcbi-1003915-g001">Figures 1</xref>–<xref ref-type="fig" rid="pcbi-1003915-g005">5</xref>). The second set comprises the layers of a strongly supervised deep convolutional network and an IT-like representation constructed by remixing and reweighting the features of the deep supervised model (<xref ref-type="fig" rid="pcbi-1003915-g006">Figures 6</xref>–<xref ref-type="fig" rid="pcbi-1003915-g010">10</xref>). The not-strongly-supervised set (<xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>) includes two supervised models: GMAX and Supervised HMAX (<xref ref-type="sec" rid="s3">Materials and Methods</xref>). These were supervised much more weakly than the deep convolutional network, using merely hundreds of images. The deep convolutional network (<xref ref-type="table" rid="pcbi-1003915-t002">Table 2</xref>) was supervised with 1.2 million category-labeled images. Note that the first set contains many independent model representations, whereas the second set contains the stages of a single deep strongly supervised object-vision model.</p>
<fig id="pcbi-1003915-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g001</object-id><label>Figure 1</label><caption>
<title>Representational dissimilarity matrices for IT and for the seven best-fitting not-strongly-supervised models.</title>
<p>The IT RDMs (black frames) for human (<bold>A</bold>) and monkey (<bold>B</bold>) and the seven most highly correlated model RDMs (excluding the representations in the strongly supervised deep convolutional network). The model RDMs are ordered from left to right and top to bottom by their correlation with the respective IT RDM. These are the seven most higly correlated RDMs among the 27 models that were not strongly supervised and their combination model (combi27). Biologically motivated models are in black, computer-vision models are in gray. The number below each RDM is the Kendall τ<sub>A</sub> correlation coefficient between the model RDM and the respective IT RDM. All correlations are statistically significant. For statistical inference, see <xref ref-type="fig" rid="pcbi-1003915-g002">Figure 2</xref>. For model abbreviations and RDM-correlation p values, see <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>. For other brain ROIs (i.e. LOC, PPA, FFA, EVC) see <xref ref-type="supplementary-material" rid="pcbi.1003915.s001">Figure S1</xref> and <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>. The RDMs here are 96×96, including the four stimuli we did not have monkey data for. The corresponding rows and columns are shown in blue in the mIT RDM and were ignored in the RDM comparisons.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g001" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g002</object-id><label>Figure 2</label><caption>
<title>The not-strongly-supervised models fail to fully explain the IT data.</title>
<p>The bars show the Kendall-<italic>τ</italic><sub>A</sub> RDM correlations between the not-strongly-supervised models and IT for human (<bold>A</bold>) and monkey (<bold>B</bold>). The error bars are standard errors of the mean estimated by bootstrap resampling of the stimuli. Asterisks indicate significant RDM correlations (random permutation test based on 10,000 randomizations of the stimulus labels; ns: not significant, p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). Most models explain a small, but significant portion of the variance of the IT representational geometry. The noise ceiling (gray bar) indicates the expected correlation of the true model (given the noise in the data). The upper and lower edges of the gray horizontal bar are upper and lower bound estimates of the maximum correlation any model can achieve given the noise. None of the not-strongly-supervised models reaches the noise ceiling. The noise ceiling could not be estimated for mIT, because the available data were from only two animals. Models with the subscript ‘UT’ are <italic>unsupervised trained</italic> models, models with the subscript ‘ST’ are <italic>supervised trained</italic> models, and others without a subscript are untrained models. Note that the supervised models included here were “weakly supervised”, i.e. with small numbers (884) of category-labeled images. Biologically motivated models are set in black font, and computer-vision models are set in gray font.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g002" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g003</object-id><label>Figure 3</label><caption>
<title>IT-like categorical structure is not apparent in any of the not-strongly-supervised models.</title>
<p>Brain and model RDMs are shown in the left columns of each panel. We used a linear combination of category-cluster RDMs (<xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>) to model the categorical structure (least-squares fit). The categories modeled were animate, inanimate, face, human face, non-human face, body, human body, non-human body, natural inanimates, and artificial inanimates. The fitted linear-combination of category-cluster RDMs is shown in the middle columns. This descriptive visualization shows to what extent different categorical divisions are prominent in each RDM. The residual RDMs of the fits are shown in the right column. For statistical inference, see <xref ref-type="fig" rid="pcbi-1003915-g004">Figure 4</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g003" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g004</object-id><label>Figure 4</label><caption>
<title>The not-strongly-supervised models are less categorical than IT.</title>
<p>Categoricality was measured using a categoricality index (vertical axis) for each model and brain RDM. The categoricality index is defined as the proportion of RDM variance explained by the category-cluster model (<xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>), i.e. the squared correlation between the fitted category-cluster model and the RDM it is fitted to. Bars show the categoricality index for each of the not-strongly-supervised models. The blue (gray) line shows the categoricality index for hIT (mIT). Error bars show 95%-confidence intervals of the categoricality index estimates for the models. The 95%-confidence intervals for hIT and mIT are shown by the blue and gray shaded regions, respectively. Significant categoricality indices are marked by stars underneath the bars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). Error bars are based on bootstrapping of the stimulus set, and the p-values are obtained by category label randomization test. Significant differences between the categoricality indices of each model and hIT (inference by bootstrap resampling of the stimuli) are indicated by blue vertical arrows (p&lt;0.05, Bonferroni-adjusted for 28 tests). The corresponding inferential comparisons for mIT are indicated by gray vertical arrows. Categoricality is significantly greater in hIT and mIT than in any of the 28 models. This analysis is based on equating the noise level in the models with that of hIT (<xref ref-type="sec" rid="s3">Materials and Methods</xref>). Similar results obtain for a conservative inferential analysis comparing the categoricality of the noise-less models with that of the noisy estimates for hIT and mIT (<xref ref-type="supplementary-material" rid="pcbi.1003915.s009">Figure S9</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g004" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g005</object-id><label>Figure 5</label><caption>
<title>Remixing and reweighting features of the not-strongly supervised models does not explain IT.</title>
<p>In order to build an IT-like representation, we attempted to remix the features to strengthen relevant categorical divisions. We trained three linear SVM classifiers (for animate/inanimate, face/nonface, and body/nonbody) on the combi27 features using 884 training images (separate from the set we had brain data for). RDMs for the resulting SVM decision values for the 92 images presented to humans and monkeys are shown at the top. The Kendall-<monospace><italic>τ</italic></monospace><sub>A</sub> RDM correlations with hIT and mIT are stated underneath the RDMs. The RDM correlations are low, but all three are statistically significant (p&lt;0.05). We further attempted to create an IT-like representation as a reweighted combination of the models. We fitted one weight for each of the 27 not-strongly-supervised models, the combi27 model, and the three SVM decision values. The weights were fitted by non-negative least squares, so as to minimize the sum of squared deviations between the RDM of the weighted combination of the features and the hIT RDM. The resulting weights are shown in the second row. Error bars indicate 95%-confidence intervals obtained by bootstrap resampling of the stimulus set. The resulting IT-geometry-supervised RDM is shown at the bottom (center) in juxtaposition to hIT (left) and mIT (right). Importantly, the RDM was obtained by cross-validation to avoid overfitting to the image set (<xref ref-type="sec" rid="s3">Materials and Methods</xref>). The RDMs here are 92×92, excluding the four stimuli that we did not have monkey data for.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g005" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g006</object-id><label>Figure 6</label><caption>
<title>RDMs of all layers of the strongly supervised deep convolutional network.</title>
<p>RDMs for all layers of the deep convolutional network (Krizhevsky et al. 2012) ref <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky1">[41]</xref> are shown for the set of the 96 images (L1: layer 1 to L7: layer 7). Kendall-<monospace><italic>τ</italic></monospace><sub>A</sub> RDM correlations of the models with hIT and mIT are stated underneath each RDM. All correlations are statistically significant. For inferential comparisons to IT and other regions, see <xref ref-type="fig" rid="pcbi-1003915-g007">Figure 7</xref> and <xref ref-type="table" rid="pcbi-1003915-t002">Table 2</xref>, respectively.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g006" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g007</object-id><label>Figure 7</label><caption>
<title>The strongly supervised deep network, with features remixed and reweighted, fully explains the IT data.</title>
<p>The bars show the Kendall-τ<sub>A</sub> RDM correlations between the layers of the strongly supervised deep convolutional network and human IT. The error bars are standard errors of the mean estimated by bootstrap resampling of the stimuli. Asterisks indicate significant RDM correlations (random permutation test based on 10,000 randomizations of the stimulus labels; p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). As we ascend the layers of the deep network, model RDMs explain increasing proportions of the variance of the hIT RDM. The noise ceiling (gray bar) indicates the expected correlation of the true model (given the noise in the data). The upper and lower edges of the gray horizontal bar are upper and lower bound estimates of the maximum correlation any model can achieve given the noise. None of the layers of the deep network reaches the noise ceiling. However, the final fully connected layers 6 and 7 come close to the ceiling. Remixing the features of layer 7 (<xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref>) using linear SVMs to strengthen the categorical divisions, provides a representation composed of three discriminants (animate/inanimate, face/nonface, and body/nonbody) that reaches the noise ceiling. Reweighting the model layers and the three discriminants (see <xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref> for details) yields a representation that explains the hIT geometry even better. A horizontal line over two bars indicates that the two models perform significantly differently (inference by bootstrap resampling of the stimulus set). Multiple testing across the many pairwise comparisons is accounted for by controlling the expected FDR at 0.05. The pairwise statistical comparisons show that the IT-geometry-supervised deep model explains IT significantly better than all other candidate representations.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g007" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g008</object-id><label>Figure 8</label><caption>
<title>IT-like categorical structure emerges across the layers of the deep supervised model, culminating in the IT-geometry-supervised layer.</title>
<p>Descriptive category-clustering analysis as in <xref ref-type="fig" rid="pcbi-1003915-g003">Figure 3</xref>, but for the deep supervised network. We used a linear combination of category-cluster RDMs (<xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>) to model the categorical structure. The fitted linear-combination of category-cluster RDMs is shown in the middle columns. This descriptive visualization shows to what extent different categorical divisions are prominent in each layer of the deep supervised model. The layers show some of the categorical divisions emerging. However, remixing of the features (linear SVM readout) is required to emphasize the categorical divisions to a degree that is similar to IT. The final IT-geometry-supervised layer (weighted combination of layers and SVM discriminants) has a categorical structure that is very similar to IT. Overfitting to the image set was avoided by crossvalidation. For statistical inference, see <xref ref-type="fig" rid="pcbi-1003915-g009">Figure 9</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g008" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g009</object-id><label>Figure 9</label><caption>
<title>The layers of the deep supervised model are less categorical than IT, but remixing and reweighting achieves IT-level categoricality.</title>
<p>Bars show the categoricality index for each layer of the deep convolutional network and for the IT-geometry-supervised layer. For conventions and for definition of the categoricality index, see <xref ref-type="fig" rid="pcbi-1003915-g004">Figure 4</xref>. Error bars and shaded regions indicate 95%-confidence intervals. Significant Categoricality indices are indicated by stars underneath the bars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). Significant differences between the categoricality index of each model and the hIT categoricality index are indicated by blue vertical arrows (p&lt;0.05, Bonferroni-adjusted for 9 tests). The corresponding inferential comparisons for mIT are indicated by gray vertical arrows. Categoricality is significantly greater in hIT and mIT than in any of the internal layers of the deep convolutional network. However, the IT-geometry-supervised layer (remixed and reweighted) achieves a categoricality similar to (and not significantly different from) IT. This analysis is based on equating the noise level in the models with that of hIT (<xref ref-type="sec" rid="s3">Materials and Methods</xref>). Similar results obtain for a conservative inferential analysis comparing the categoricality of the noise-less models with that of the noisy estimates for hIT and mIT (<xref ref-type="supplementary-material" rid="pcbi.1003915.s010">Figure S10</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g009" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003915-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g010</object-id><label>Figure 10</label><caption>
<title>Remixing and reweighting features of the deep supervised network achieves an IT-like representational geometry.</title>
<p>All analyses and conventions here are analogous to <xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, but applied to the strongly supervised deep convolutional network, rather than to the not-strongly supervised models. Remixing the features of layer 7 by fitting linear SVMs (separate set of training images) for the major categorical divisions (animate/inanimate, face/nonface, and body/nonbody) helped account for the categorical clusters in IT. The Kendall-<monospace><italic>τ</italic></monospace><sub>A</sub> RDM correlations between the SVM decision values and IT (stated underneath the RDMs in the top row) are statistically significant (p&lt;0.05). For the deep convolutional network used here, feature remixing accounted for the animate/inanimate division of IT. We attempted to create an IT-like representation as a reweighted combination of the layers of the deep network and the SVM decision values. We fitted one weight for each of the layers and one weight for each of the three decision values. The bar graph in the middle row shows the weights, with 95%-confidence intervals obtained by bootstrap resampling of the stimulus set. As before, the weights were fitted using non-negative least squares to minimize the sum of squared deviations between the RDM of the weighted combination and the hIT RDM. The resulting IT-geometry-supervised RDM (bottom row, center) is very similar to the RDMs of hIT (left) and mIT (right). The <monospace><italic>τ</italic></monospace><sub>A</sub> RDM correlation between the fitted model and IT is about equal for monkey IT (0.40) and human IT (0.38). Both of these RDM correlations are higher than the RDM correlation between hIT and mIT, reflecting the effect of noise on the empirical RDM estimates. As in <xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, the fitted model RDM was obtained by cross-validation to avoid overfitting to the image set.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g010" position="float" xlink:type="simple"/></fig><table-wrap id="pcbi-1003915-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.t001</object-id><label>Table 1</label><caption>
<title>RDM correlations between brain regions and not-strongly-supervised models.</title>
</caption><alternatives><graphic id="pcbi-1003915-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="6" align="left" rowspan="1">correlation to</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1">model</td>
<td align="left" rowspan="1" colspan="1">mIT</td>
<td align="left" rowspan="1" colspan="1">hIT [0.26, 0.48]</td>
<td align="left" rowspan="1" colspan="1">LOC [0.20, 0.41]</td>
<td align="left" rowspan="1" colspan="1">FFA [0.10, 0.39]</td>
<td align="left" rowspan="1" colspan="1">PPA [0.08, 0.38]</td>
<td align="left" rowspan="1" colspan="1">EVC [0.13, 0.40]</td>
</tr>
</thead>
<tbody>
<tr>
<td colspan="2" align="left" rowspan="1"><bold>without training</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>1. V1 model</bold></td>
<td align="left" rowspan="1" colspan="1">0.104****</td>
<td align="left" rowspan="1" colspan="1">0.080***</td>
<td align="left" rowspan="1" colspan="1">0.048**</td>
<td align="left" rowspan="1" colspan="1">0.045*</td>
<td align="left" rowspan="1" colspan="1">0.006<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.123****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>2.Convolutional network</bold></td>
<td align="left" rowspan="1" colspan="1">0.132****</td>
<td align="left" rowspan="1" colspan="1">0.111****</td>
<td align="left" rowspan="1" colspan="1">0.058***</td>
<td align="left" rowspan="1" colspan="1">0.083***</td>
<td align="left" rowspan="1" colspan="1">−0.023<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.174****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>3.Bio transform-1<sup>st</sup> stage</bold></td>
<td align="left" rowspan="1" colspan="1">0.117****</td>
<td align="left" rowspan="1" colspan="1">0.059**</td>
<td align="left" rowspan="1" colspan="1">0.031<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.023<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.018<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.075*</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>4.Bio transform-2<sup>nd</sup> stage</bold></td>
<td align="left" rowspan="1" colspan="1">0.096****</td>
<td align="left" rowspan="1" colspan="1">0.015<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.029<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.077**</td>
<td align="left" rowspan="1" colspan="1">−0.008<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.044<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>5. Bio transform-both</bold></td>
<td align="left" rowspan="1" colspan="1">0.105****</td>
<td align="left" rowspan="1" colspan="1">0.056**</td>
<td align="left" rowspan="1" colspan="1">0.066***</td>
<td align="left" rowspan="1" colspan="1">0.067**</td>
<td align="left" rowspan="1" colspan="1">−0.026<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.037<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>6. Radon</bold></td>
<td align="left" rowspan="1" colspan="1">0.013<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.035<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.002<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.031<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.016<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.004<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>7. Dense SIFT</bold></td>
<td align="left" rowspan="1" colspan="1">0.145****</td>
<td align="left" rowspan="1" colspan="1">0.101****</td>
<td align="left" rowspan="1" colspan="1">0.077****</td>
<td align="left" rowspan="1" colspan="1">0.067***</td>
<td align="left" rowspan="1" colspan="1">0.021<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.171****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>8. Stimulus image (Lab)</bold></td>
<td align="left" rowspan="1" colspan="1">0.044**</td>
<td align="left" rowspan="1" colspan="1">0.023<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.044*</td>
<td align="left" rowspan="1" colspan="1">0.083*</td>
<td align="left" rowspan="1" colspan="1">−0.033<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.119<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>9. LBP</bold></td>
<td align="left" rowspan="1" colspan="1">0.067***</td>
<td align="left" rowspan="1" colspan="1">0.078**</td>
<td align="left" rowspan="1" colspan="1">0.084****</td>
<td align="left" rowspan="1" colspan="1">0.051*</td>
<td align="left" rowspan="1" colspan="1">−0.057<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.025<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>10. Lab joint histogram</bold></td>
<td align="left" rowspan="1" colspan="1">0.147****</td>
<td align="left" rowspan="1" colspan="1">0.092****</td>
<td align="left" rowspan="1" colspan="1">0.081****</td>
<td align="left" rowspan="1" colspan="1">0.094***</td>
<td align="left" rowspan="1" colspan="1">−0.055<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.011<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>11. Silhouette image</bold></td>
<td align="left" rowspan="1" colspan="1">0.168****</td>
<td align="left" rowspan="1" colspan="1">0.092****</td>
<td align="left" rowspan="1" colspan="1">0.061***</td>
<td align="left" rowspan="1" colspan="1">0.052*</td>
<td align="left" rowspan="1" colspan="1">−0.003<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.209****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>12. Gist</bold></td>
<td align="left" rowspan="1" colspan="1">0.164****</td>
<td align="left" rowspan="1" colspan="1">0.120****</td>
<td align="left" rowspan="1" colspan="1">0.065****</td>
<td align="left" rowspan="1" colspan="1">0.059**</td>
<td align="left" rowspan="1" colspan="1">−0.014<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.185****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>13. PHOG</bold></td>
<td align="left" rowspan="1" colspan="1">0.120****</td>
<td align="left" rowspan="1" colspan="1">0.094****</td>
<td align="left" rowspan="1" colspan="1">0.059**</td>
<td align="left" rowspan="1" colspan="1">0.061*</td>
<td align="left" rowspan="1" colspan="1">−0.010<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.212****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>14. Geometric blur</bold></td>
<td align="left" rowspan="1" colspan="1">0.123****</td>
<td align="left" rowspan="1" colspan="1">0.061***</td>
<td align="left" rowspan="1" colspan="1">0.028<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.058*</td>
<td align="left" rowspan="1" colspan="1">0.010<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.172****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>15. Ssim</bold></td>
<td align="left" rowspan="1" colspan="1">0.171****</td>
<td align="left" rowspan="1" colspan="1">0.126****</td>
<td align="left" rowspan="1" colspan="1">0.068****</td>
<td align="left" rowspan="1" colspan="1">0.104***</td>
<td align="left" rowspan="1" colspan="1">−0.048<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.169****</td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>16. Gssim</bold></td>
<td align="left" rowspan="1" colspan="1">0.158****</td>
<td align="left" rowspan="1" colspan="1">0.106****</td>
<td align="left" rowspan="1" colspan="1">0.069****</td>
<td align="left" rowspan="1" colspan="1">0.063**</td>
<td align="left" rowspan="1" colspan="1">−0.000<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.191****</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>with training</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>un-supervised</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>17.VisNet<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.012<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.037**</td>
<td align="left" rowspan="1" colspan="1">0.016<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.005<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.001<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.057***</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>18.Stable model<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.092****</td>
<td align="left" rowspan="1" colspan="1">0.081****</td>
<td align="left" rowspan="1" colspan="1">0.056**</td>
<td align="left" rowspan="1" colspan="1">0.088**</td>
<td align="left" rowspan="1" colspan="1">−0.083<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.004<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>19.HMAX-C1<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.127****</td>
<td align="left" rowspan="1" colspan="1">0.085****</td>
<td align="left" rowspan="1" colspan="1">0.051***</td>
<td align="left" rowspan="1" colspan="1">0.036<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.025<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.154****</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>20.HMAX-C2<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.182****</td>
<td align="left" rowspan="1" colspan="1">0.114****</td>
<td align="left" rowspan="1" colspan="1">0.055***</td>
<td align="left" rowspan="1" colspan="1">0.098***</td>
<td align="left" rowspan="1" colspan="1">−0.012<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1"><bold>0.217****</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>21.HMAX-C2b<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.114****</td>
<td align="left" rowspan="1" colspan="1">0.112****</td>
<td align="left" rowspan="1" colspan="1">0.095****</td>
<td align="left" rowspan="1" colspan="1">0.065**</td>
<td align="left" rowspan="1" colspan="1">−0.043<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.011<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>22.HMAX-C3<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.139****</td>
<td align="left" rowspan="1" colspan="1">0.114****</td>
<td align="left" rowspan="1" colspan="1">0.074****</td>
<td align="left" rowspan="1" colspan="1">0.081***</td>
<td align="left" rowspan="1" colspan="1">−0.047<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.078**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>23.HMAX-All<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.165****</td>
<td align="left" rowspan="1" colspan="1">0.139****</td>
<td align="left" rowspan="1" colspan="1">0.108****</td>
<td align="left" rowspan="1" colspan="1"><bold>0.132****</bold></td>
<td align="left" rowspan="1" colspan="1">−0.067<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.081**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>24.SLF<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.061****</td>
<td align="left" rowspan="1" colspan="1">0.054**</td>
<td align="left" rowspan="1" colspan="1">0.015<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.091***</td>
<td align="left" rowspan="1" colspan="1">−0.074<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.027<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>25.PHOW<sub>UT</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.031*</td>
<td align="left" rowspan="1" colspan="1">0.054*</td>
<td align="left" rowspan="1" colspan="1">0.046*</td>
<td align="left" rowspan="1" colspan="1">0.038<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.067<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.021<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>with training</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>supervised</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>26.GMAX<sub>ST</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.078****</td>
<td align="left" rowspan="1" colspan="1">0.060***</td>
<td align="left" rowspan="1" colspan="1">0.023<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.098***</td>
<td align="left" rowspan="1" colspan="1">−0.074<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.038<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>27.Supervised HMAX<sub>ST</sub></bold></td>
<td align="left" rowspan="1" colspan="1">0.085****</td>
<td align="left" rowspan="1" colspan="1">0.079****</td>
<td align="left" rowspan="1" colspan="1">0.041*</td>
<td align="left" rowspan="1" colspan="1">0.109***</td>
<td align="left" rowspan="1" colspan="1">−0.074<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.032<sup>ns</sup></td>
</tr>
<tr>
<td colspan="2" align="left" rowspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>28. Combination of all 27</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.253****</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.169****</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.137****</bold></td>
<td align="left" rowspan="1" colspan="1">0.045****</td>
<td align="left" rowspan="1" colspan="1"><bold>0.034***</bold></td>
<td align="left" rowspan="1" colspan="1">0.096****</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Kendall-<italic>τ</italic><sub>A</sub> RDM correlation coefficients between brain regions and not-strongly-supervised models. Significant correlations are indicated by asterisks (ns: not significant, * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). The brain regions are the lateral occipital complex (LOC), the fusiform face area (FFA), the parahippocampal place area (PPA), and the foveal confluence of early visual areas (EVC). For each brain region, the highest RDM correlation is set in bold. Lower and upper bounds of the noise ceiling are stated in brackets below the labels to the human brain ROIs (top row).</p></fn></table-wrap-foot></table-wrap><table-wrap id="pcbi-1003915-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.t002</object-id><label>Table 2</label><caption>
<title>RDM correlations between brain regions and layers of the deep convolutional network.</title>
</caption><alternatives><graphic id="pcbi-1003915-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="6" align="left" rowspan="1">correlation to</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">model</td>
<td align="left" rowspan="1" colspan="1">mIT</td>
<td align="left" rowspan="1" colspan="1">hIT [0.26, 0.48]</td>
<td align="left" rowspan="1" colspan="1">LOC [0.20, 0.41]</td>
<td align="left" rowspan="1" colspan="1">FFA [0.10, 0.39]</td>
<td align="left" rowspan="1" colspan="1">PPA [0.08, 0.38]</td>
<td align="left" rowspan="1" colspan="1">EVC [0.13, 0.40]</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>deep convolutional network layers</bold> (Krizhevsky et al. 2012)</td>
<td align="left" rowspan="1" colspan="1"><bold>Layer 1</bold> (convolutional)</td>
<td align="left" rowspan="1" colspan="1">0.08*</td>
<td align="left" rowspan="1" colspan="1">0.03*</td>
<td align="left" rowspan="1" colspan="1">0.03**</td>
<td align="left" rowspan="1" colspan="1">0.04*</td>
<td align="left" rowspan="1" colspan="1">−0.03<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.01<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 2</bold> (convolutional)</td>
<td align="left" rowspan="1" colspan="1">0.21****</td>
<td align="left" rowspan="1" colspan="1">0.12****</td>
<td align="left" rowspan="1" colspan="1">0.08****</td>
<td align="left" rowspan="1" colspan="1">0.09****</td>
<td align="left" rowspan="1" colspan="1">−0.01<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1"><bold>0.17****</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 3</bold> (convolutional)</td>
<td align="left" rowspan="1" colspan="1">0.23****</td>
<td align="left" rowspan="1" colspan="1">0.15****</td>
<td align="left" rowspan="1" colspan="1">0.10****</td>
<td align="left" rowspan="1" colspan="1">0.07***</td>
<td align="left" rowspan="1" colspan="1">−0.01<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.16****</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 4</bold> (convolutional)</td>
<td align="left" rowspan="1" colspan="1">0.25****</td>
<td align="left" rowspan="1" colspan="1">0.17****</td>
<td align="left" rowspan="1" colspan="1">0.12****</td>
<td align="left" rowspan="1" colspan="1">0.06***</td>
<td align="left" rowspan="1" colspan="1">0.01<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.11****</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 5</bold> (convolutional)</td>
<td align="left" rowspan="1" colspan="1">0.24****</td>
<td align="left" rowspan="1" colspan="1">0.17****</td>
<td align="left" rowspan="1" colspan="1">0.14****</td>
<td align="left" rowspan="1" colspan="1">0.05**</td>
<td align="left" rowspan="1" colspan="1">0.01<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.09****</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 6</bold> (fully connected)</td>
<td align="left" rowspan="1" colspan="1">0.29****</td>
<td align="left" rowspan="1" colspan="1">0.23****</td>
<td align="left" rowspan="1" colspan="1">0.18****</td>
<td align="left" rowspan="1" colspan="1"><bold>0.12****</bold></td>
<td align="left" rowspan="1" colspan="1">−0.02<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.07****</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 7</bold> (fully connected)</td>
<td align="left" rowspan="1" colspan="1">0.29****</td>
<td align="left" rowspan="1" colspan="1">0.24****</td>
<td align="left" rowspan="1" colspan="1">0.18****</td>
<td align="left" rowspan="1" colspan="1">0.09****</td>
<td align="left" rowspan="1" colspan="1">−0.02<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.06**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>Layer 8</bold> (scores)</td>
<td align="left" rowspan="1" colspan="1">0.18****</td>
<td align="left" rowspan="1" colspan="1">0.13****</td>
<td align="left" rowspan="1" colspan="1">0.12****</td>
<td align="left" rowspan="1" colspan="1">0.02<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.02<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.01<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>remixed features</bold> (linear SVM readout)</td>
<td align="left" rowspan="1" colspan="1"><bold>animate/inanimate</bold></td>
<td align="left" rowspan="1" colspan="1">0.20****</td>
<td align="left" rowspan="1" colspan="1">0.25****</td>
<td align="left" rowspan="1" colspan="1">0.20****</td>
<td align="left" rowspan="1" colspan="1">0.02<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1"><bold>0.07****</bold></td>
<td align="left" rowspan="1" colspan="1">0.03*</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>face/nonface</bold></td>
<td align="left" rowspan="1" colspan="1">0.13****</td>
<td align="left" rowspan="1" colspan="1">0.08***</td>
<td align="left" rowspan="1" colspan="1">0.08***</td>
<td align="left" rowspan="1" colspan="1">0.04***</td>
<td align="left" rowspan="1" colspan="1">0.02**</td>
<td align="left" rowspan="1" colspan="1">0.03**</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><bold>body/nonbody</bold></td>
<td align="left" rowspan="1" colspan="1">0.06**</td>
<td align="left" rowspan="1" colspan="1">0.05***</td>
<td align="left" rowspan="1" colspan="1">0.05***</td>
<td align="left" rowspan="1" colspan="1">0.00<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">0.01<sup>ns</sup></td>
<td align="left" rowspan="1" colspan="1">−0.01<sup>ns</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>reweighted combination of the above</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>IT-geometry supervised Layer</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.40</bold>****</td>
<td align="left" rowspan="1" colspan="1"><bold>0.38</bold>****</td>
<td align="left" rowspan="1" colspan="1"><bold>0.27****</bold></td>
<td align="left" rowspan="1" colspan="1">0.07****</td>
<td align="left" rowspan="1" colspan="1">0.05***</td>
<td align="left" rowspan="1" colspan="1">0.07****</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>Kendall-<italic>τ</italic><sub>A</sub> RDM correlation coefficients between brain regions and layers of the deep supervised network. Conventions as in <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>. Significant correlations are indicated by asterisks (ns: not significant, * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). For each brain region, the highest RDM correlation is set in bold.</p></fn></table-wrap-foot></table-wrap><sec id="s2a">
<title>Most models explain a small component of the IT representational geometry</title>
<p>Among the not-strongly-supervised models, the seven models with the highest RDM correlations with hIT and mIT are shown in <xref ref-type="fig" rid="pcbi-1003915-g001">Figure 1</xref> (for other brain regions, see <xref ref-type="supplementary-material" rid="pcbi.1003915.s001">Figure S1</xref> and <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>). Visual inspection suggests that the models capture the human-face cluster, which is also prevalent in IT. However, the models do not appear to place human and animal faces in a single cluster. In addition, the inanimate objects appear less clustered in the models.</p>
<p>All models shown in <xref ref-type="fig" rid="pcbi-1003915-g001">Figure 1</xref> have small, but highly significant (p&lt;0.0001) RDM correlations with hIT and mIT (<xref ref-type="fig" rid="pcbi-1003915-g001">Figure 1A, 1B</xref>, respectively; for RDM correlation with other brain regions see <xref ref-type="supplementary-material" rid="pcbi.1003915.s002">Figure S2</xref> for the not-strongly-supervised models, and <xref ref-type="supplementary-material" rid="pcbi.1003915.s003">Figure S3</xref> for the deep supervised model representations). Most of the other not-strongly-supervised models also have significant RDM correlations (<xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>, <xref ref-type="fig" rid="pcbi-1003915-g002">Figure 2</xref>; inference by randomization of stimulus labels). Although often significant, all RDM correlations between not-strongly-supervised models and IT were small (Kendall <italic>τ</italic><sub>A</sub>&lt;0.17 for hIT; <italic>τ</italic><sub>A</sub>&lt;0.26 for mIT).</p>
</sec><sec id="s2b">
<title>Combining features from multiple models improves the explanation of IT</title>
<p>Combining features from the not-strongly-supervised models improved the RDM correlations to IT. Model features were combined by summarizing each model representation by its first 95 principal components and then concatenating these sets of principal components. This approach ensured that each model contributed equally to the combination (same number of features and same total variance contributed).</p>
<p>The combination of the 27 not-strongly-supervised models (combi27) has a higher RDM correlation with both hIT and mIT than any of the 27 contributing models. Second to the combi27 model, internal representations of the HMAX model have the highest RDM correlation with hIT and mIT. This might reflect the fact that the architecture and parameters of the HMAX model closely follow the literature on the primate ventral stream.</p>
<p>In addition to the combi27, we also tested the combination of untrained models, the combination of unsupervised trained models, and the combination of weakly supervised trained models (<xref ref-type="supplementary-material" rid="pcbi.1003915.s004">Figure S4</xref>). The combi27 explained IT equally well or better than other combinations of the not-strongly-supervised models. In the remaining analyses, we therefore omit the other combinations and consider the combi27 along with each individual model.</p>
<p>Monkey IT was significantly better explained by the combi27 than by the second best among the not-strongly-supervised models (HMAX-C2<sub>UT</sub>; p = 0.02; inference by bootstrap resampling of the stimulus set <xref ref-type="bibr" rid="pcbi.1003915-Efron1">[50]</xref>, not shown). This suggests that the models are somewhat complementary in explaining the IT features space. For hIT, the second best model was also a version of HMAX (HMAX-all<sub>UT</sub>), but it did not explain hIT significantly worse than combi27 (p = 0.261, not shown).</p>
<p>Model RDM correlations with mIT tended to be higher than model correlations with the hIT RDM. For example, the dissimilarity correlation of the combi27 with mIT was 0.25, whereas for hIT it is 0.17. This difference is statistically significant (p = 0.001), suggesting that the models were able to better explain the mIT RDM compared to the hIT RDM. This could be caused by a lower level of noise in the mIT RDM (estimated from cell-recording data) than in the hIT RDM (from fMRI data).</p>
</sec><sec id="s2c">
<title>None of the not-strongly-supervised models fully explains the IT data</title>
<p>For the human data we were able to estimate a noise ceiling <xref ref-type="bibr" rid="pcbi.1003915-Nili1">[30]</xref> (<xref ref-type="sec" rid="s3">Materials and Methods</xref>), indicating the RDM correlation expected for the true model, given the noise in the data. None of the 28 not-strongly-supervised models reached the noise ceiling (<xref ref-type="fig" rid="pcbi-1003915-g002">Figure 2A</xref>). The combi27 representation came closest, but at <italic>τ</italic><sub>A</sub> = 0.17, it was far from the lower bound of the noise ceiling (<italic>τ</italic><sub>A</sub> = 0.26). This indicates that the fMRI data capture a component of the hIT representation that all the not-strongly-supervised models leave unexplained. For mIT, we could not estimate the noise ceiling because we had data from only two animals.</p>
</sec><sec id="s2d">
<title>IT is more categorical than any of the not-strongly-supervised models</title>
<p>The main categorical divisions observed in IT appear weak or absent in the best fitting models (<xref ref-type="fig" rid="pcbi-1003915-g001">Figure 1</xref>). To measure the strength of categorical clustering in each model and brain representation, we fitted a linear model of category-cluster RDMs to each model and brain RDM (<xref ref-type="sec" rid="s3">Materials and Methods</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>). The fitted models (<xref ref-type="fig" rid="pcbi-1003915-g003">Figure 3</xref>) descriptively visualize the categorical component of each RDM, summarizing sets of within- and between-category dissimilarities by their averages. The fits for several computational models show a strong human-face cluster, and a weak animate cluster. The human-face cluster is expected on the basis of the visual similarity of the human-face images (all frontal aligned human faces of the same approximate size). The animate cluster could reflect the similar colors and more rounded shapes shared by the animate objects. However, IT in both human and monkey exhibits additional categorical clusters that are not easily accounted for in terms of visual similarity. First, the IT representation has a strong face cluster that includes human and animal faces of different species, which differ widely in shape, color, and pose. Second, the IT representation has an inanimate cluster, which includes a wide variety of natural and artificial objects and scenes of totally different visual appearance. These clusters are largely absent from the not-strongly-supervised models (<xref ref-type="fig" rid="pcbi-1003915-g003">Figures 3</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s006">S6</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s007">S7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s008">S8</xref>).</p>
<p>In order to statistically compare the overall strength of categorical divisions between IT and each of the models, we computed a categoricality index for each representation. The categoricality index is the proportion of RDM variance explained by categorical divisions. The categoricality index is calculated as the squared correlation between the fitted category-cluster model (<xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>) and the RDM it is fitted to (<xref ref-type="fig" rid="pcbi-1003915-g004">Figure 4</xref>). The model RDMs are noise-less. However, the brain RDMs are affected by noise, which lowers the categoricality index. To account for the noise and make the categoricality indices comparable between models and IT, we added noise matching the noise level of hIT to the model representations (<xref ref-type="sec" rid="s3">Materials and Methods</xref>). We then compared the categoricality indices of the 28 not-strongly-supervised models to that of hIT (<xref ref-type="fig" rid="pcbi-1003915-g004">Figure 4</xref>). Human IT has a categoricality index of 0.4. All of the not-strongly supervised models have categoricality indices below 0.16; most of them below 0.1.</p>
<p>Inferential comparisons show that the categoricality index is significantly higher for hIT than for any of the models (inference by bootstrap resampling of the image set). We also compared the categoricality indices between models and IT without equating the noise levels. In this analysis, the categoricality index reflects the categoricality of the models without noise. For hIT and mIT, the noise lowers the categoricality estimate. Nevertheless, the hIT categoricality index remains significantly greater than that of any of the models. For mIT, similarly, the categoricality index is significantly greater than for all but three of the models (<xref ref-type="supplementary-material" rid="pcbi.1003915.s009">Figure S9</xref>).</p>
<p>We also analyzed the clustering strength separately for each of the categories (<xref ref-type="supplementary-material" rid="pcbi.1003915.s006">Figure S6</xref>). For animates, clustering strength was significant for a few models (Lab joint color histogram, PHOG, and HMAX-all). For human faces, significant clustering was observed for several computational models (convNet, bioTransform, dense SIFT, LBP, silhouette image, gist, geometric blur, local self-similarity descriptor, global self-similarity descriptor, stable model, HMAX-C1, and combi27). These significant category clusters reflect the visual similarity of the members of these categories.</p>
<p>Inferential comparisons of clustering strength between each of the models and hIT (<xref ref-type="supplementary-material" rid="pcbi.1003915.s008">Figure S8</xref>) and mIT (<xref ref-type="supplementary-material" rid="pcbi.1003915.s008">Figure S8</xref>) for each of the categories revealed that IT clusters animates, inanimates, and faces (including human and animal faces) significantly more strongly in both species than most of the models (blue bars in <xref ref-type="supplementary-material" rid="pcbi.1003915.s007">Figures S7</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003915.s008">S8</xref>). There are only a few cases, in which a model clusters one of the categories more strongly than IT.</p>
</sec><sec id="s2e">
<title>Remixing and reweighting of the features of the not-strongly-supervised models does not improve the explanation of the IT data</title>
<p>The finding that categoricality is stronger in IT than in any of the models raises the question of what the models are missing. One possibility is that the models contain all essential nonlinear features, but in proportions different from IT, thus emphasizing the features differently in the representational geometry. In that case reweighting of the features (i.e. stretching and squeezing the representational space along its original axes) should help approximate the IT representational geometry.</p>
<p>For example, the representation might contain a feature perfectly discriminating animates from inanimates. This single categorical feature would not have been reflected strongly in the overall RDM if none of the other features emphasized this categorical division. The influence of such a feature on the overall representational geometry could be increased either by replicating the feature in the representation or by amplifying the feature values. These two alternatives are equivalent in their effects on the RDM, so we consider only the latter.</p>
<p>Another possibility is that all essential nonlinearities are present, but the features need to be linearly recombined (i.e. performing general affine transformations) to approximate the IT representational geometry. We therefore investigated whether linear remixing and reweighting of the features of the not-strongly-supervised models could provide a better explanation of the IT representational geometry.</p>
<sec id="s2e1">
<title>Remixing of features</title>
<p>We attempted to create new features as linear combinations of the original features. The space of all linear recodings is difficult to search given limited data. We therefore restricted this analysis to the combi27 features (which represent a combination of the not-strongly-supervised models) and attempted to find linear combinations that specifically emphasize the missing categorical divisions. In order to find such linear combinations, we trained three linear support vector machine (SVM) classifiers for body/nonbody, face/non-face, and animate/inanimate categorization. The SVMs were trained on a set of 884 labeled images of isolated objects nonoverlapping with the set of 96 images we had brain data for. We used the decision-value outputs of the classifiers as new features. The resulting single-feature RDMs (<xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, top; one RDM for each SVM) are not highly categorical and have only a low correlation (<italic>τ</italic><sub>A</sub>&lt;0.1) with the IT RDMs for human and monkey. This is consistent with the fact that the combi27 representation does not perform very well on categorization tasks (<xref ref-type="fig" rid="pcbi-1003915-g011">Figures 11</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s011">S11</xref>).</p>
<fig id="pcbi-1003915-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g011</object-id><label>Figure 11</label><caption>
<title>Animate/inanimate categorization accuracy for all models.</title>
<p>Each dark blue bar shows the categorization accuracy of a linear SVM applied to one of the computational model representations. Categorization accuracy for each model was estimated by 12-fold crossvalidation on the 96 stimuli. To assess whether categorization accuracy was above chance level, we performed a permutation test, in which we retrained the SVMs on (category-orthogonalized) 10,000 random dichotomies among the stimuli. Light blue bars show the average model categorization accuracy for random label permutations. Categorization performance was significantly greater than chance for most models (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). The deep convolutional network model (final fully connected layer 7) has the highest animate/inanimate categorization performance (96%). The combi27 has the second highest performance (76%).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g011" position="float" xlink:type="simple"/></fig></sec><sec id="s2e2">
<title>Feature reweighting</title>
<p>Combining the not-strongly-supervised models with equal weight in the combi27 representation improved the explanation of our IT data. We wanted to test whether appropriate weighting of the not-strongly-supervised models could further improve the explanation of the IT geometry. In addition to the 27 not-strongly-supervised models, we included the combi27 model, and the three categorical SVM discriminants in the set of representations to be combined. We fitted one weight for each of these representations (27+1+3 = 31 weights in total), so as to best explain the hIT RDM (<xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, middle row).</p>
<p>Flipping the sign of a feature (weight = −1) has no effect on the representational distances. We can, thus, consider only positive weights, without loss of generality. We therefore used a non-negative-least-squares fitting algorithm <xref ref-type="bibr" rid="pcbi.1003915-Lawson1">[51]</xref> to find the non-negative weights for the models that minimize the sum of squared deviations between the hIT RDM and the RDM of the weighted combination of models. The RDM of the weighted combination of the model features is equivalent to a weighted combination of the RDMs of the models (<xref ref-type="sec" rid="s3">Materials and Methods</xref>) when squared Euclidean distance is used. We used the squared Euclidean distance for normalized representational patterns, which is equivalent to correlation distance, as used throughout this paper. We therefore applied the nonnegative least-squares algorithm at the level of the RDMs.</p>
<p>In order to avoid overestimation of the RDM correlation between the fitted model and hIT due to overfitting to the image set, we fitted the weights to random subsets of 88 of the 96 images in a crossvalidation procedure, holding out 8 images on each fold. We then estimated the representational dissimilarities for the weighted-combination model for the 8 held-out images. We repeated this procedure until the entire RDM of 96 by 96 images was estimated (<xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, bottom row, center).</p>
<p>Feature reweighting and remixing did not reproduce the categorical structure observed in IT (<xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, bottom row). In fact the weighted-combination model did slightly worse than combi27 at explaining hIT and mIT (<italic>τ</italic><sub>A</sub> = 0.13 for hIT, <italic>τ</italic><sub>A</sub> = 0.20 for mIT). The lower performance, despite the inclusion of combi27 as one of the component representations, reflects the cost of overfitting. However, since we fitted only 31 weights in the reweighting step, that cost is small. The failure to improve the explanation of the IT geometry through remixing and reweighting, thus, suggests that the not-strongly-supervised models are missing features important to the IT representational geometry. Different nonlinear features and more powerful supervised learning methods may be needed to fully capture the structure of the IT representation. We therefore next tested a deep supervised convolutional neural network <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky2">[52]</xref>.</p>
</sec></sec><sec id="s2f">
<title>A strongly supervised deep convolutional network better explains the IT data</title>
<p>So far, we showed that none of the not-strongly-supervised models were able to reproduce the categorical structure present in IT. Most of these models were untrained or trained without supervision. A few of them were weakly supervised (i.e. supervised with merely 884 training images). Their failure at explaining our IT data suggests that computational features trained to cluster the categories through supervised learning with many labeled images might be needed to explain the IT representational geometry. We therefore tested a deep convolutional neural network trained with 1.2 million labelled images <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky2">[52]</xref>, nonoverlapping with the set of 96 images used here. The model has eight layers. The RDM for each of the layers and the RDM correlations with hIT and mIT are shown in <xref ref-type="fig" rid="pcbi-1003915-g006">Figure 6</xref>. The deep supervised convolutional network explains the IT geometry better than any of the not-strongly-supervised models. The RDM correlation between hIT and the deep convolutional network's best-performing layer (layer 7) is <italic>τ</italic><sub>A</sub> = 0.24. Layer 7 explains the hIT representation significantly better (p&lt;0.05; obtained by bootstrap resampling of the stimulus set) than combi27 (<italic>τ</italic><sub>A</sub> = 0.17), the best-performing of the not-strongly-supervised models. Monkey IT, as well, is better explained by layer 7 (<italic>τ</italic><sub>A</sub> = 0.29) than by combi27 (<italic>τ</italic><sub>A</sub> = 0.25), although the difference is not significant.</p>
<p>Layer 7 is the deep network's highest continuous representational space, followed only by the readout layer (layer 8, also known as the “scores”). The readout layer is composed of 1000 features, one for each of the 1000 category labels used in training the network. The readout layer has a lower RDM correlation with hIT (<italic>τ</italic><sub>A</sub> = 0.13) and mIT (<italic>τ</italic><sub>A</sub> = 0.18) than layer 7.</p>
<p>From layer 1 to layer 7 the RDM correlation with IT rises roughly monotonically (<xref ref-type="fig" rid="pcbi-1003915-g007">Figure 7</xref>, <xref ref-type="table" rid="pcbi-1003915-t002">Table 2</xref>) and many of the pairwise comparisons between RDM correlations for higher and lower layers are significant (<xref ref-type="fig" rid="pcbi-1003915-g007">Figure 7</xref>, horizontal lines at the top). Nevertheless, even the best-performing layer 7 does not reach the noise ceiling (<xref ref-type="fig" rid="pcbi-1003915-g007">Figure 7</xref>). Although the deep convolutional network outperforms all not-strongly-supervised models, it does not fully explain our IT data.</p>
<p>As for the not-strongly-supervised models, we analyzed the categoricality of the layers of the deep supervised model (<xref ref-type="fig" rid="pcbi-1003915-g008">Figures 8</xref>, <xref ref-type="fig" rid="pcbi-1003915-g009">9</xref>). All layers of the deep supervised model, including layer 7 and layer 8 (the readout layer), have significantly lower categoricality indices than hIT and mIT (<xref ref-type="fig" rid="pcbi-1003915-g009">Figure 9</xref>). This might reflect the fact that the stimulus set was equally divided into animates and inanimates and this division, thus, strongly influences our categoricality index. Importantly, the deep supervised network emphasizes some categorical divisions more strongly and others less strongly than IT (<xref ref-type="fig" rid="pcbi-1003915-g008">Figure 8</xref>). For example, layer 7 emphasizes the division between human and animal faces and the division between artificial and natural inanimate objects more strongly than IT. However, IT emphasizes the animate/inanimate and the face/body division more strongly than layer 7.</p>
</sec><sec id="s2g">
<title>Remixing and reweighting of the deep supervised features fully explains the IT data</title>
<p>We have seen that the deep supervised model provides better separation of the categories than the not-strongly-supervised models and that it also better explains IT. However, it did not reach the noise ceiling. As for the not-strongly-supervised models, we therefore asked whether remixing the features linearly (by adding linear readout features emphasizing the right categorical divisions) and reweighting of the different layers and readout features could provide a better model of the IT representation.</p>
<p>The method for remixing and reweighting was exactly the same as for the not-strongly-supervised models (<xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>). However, the linear SVM features were based on layer 7 (instead of combi27) and the reweighting involved fitting one weight for each of the layers (1–8) and one weight for each of the three linear SVM features.</p>
<p>As before, the linear SVM features were trained for body/nonbody, face/non-face, and animate/inanimate categorization using the nonoverlapping set of 884 training images. The RDMs for the SVM readout features show strong categorical divisions (<xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref>, top row). This is consistent with the fact that the layer-7 representation performs well on categorization tasks (<xref ref-type="fig" rid="pcbi-1003915-g011">Figures 11</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s011">S11</xref>).</p>
<p>As before, we used non-negative least square fitting to find the weighted combination of the representations that best approximates hIT. Again, we avoided overfitting to the image set by fitting the weights to random subsets of 88 of the 96 images in a crossvalidation procedure, holding out 8 images on each fold. This procedure yielded a weight for each of the eight layers of the deep network and for each of the three linear SVM readout features (11 weights in total; <xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref>, middle row; <xref ref-type="sec" rid="s3">Materials and Methods</xref>).</p>
<p>We refer to this weighted combination as the IT-geometry-supervised deep model. Inspecting the RDM reveals the similarity of its representational geometry to hIT and mIT (<xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref>, bottom row). The model emphasizes the major categorical divisions similarly to IT (<xref ref-type="fig" rid="pcbi-1003915-g008">Figure 8</xref>, bottom right). In contrast to all other models, this model has a categoricality index matching mIT and not significantly different from either mIT or hIT (<xref ref-type="fig" rid="pcbi-1003915-g009">Figure 9</xref>). The IT-geometry-supervised deep model explains hIT better than any layer of the deep network (<xref ref-type="fig" rid="pcbi-1003915-g007">Figure 7</xref>, horizontal lines at the top). It has the highest RDM correlation with hIT (<italic>τ</italic><sub>A</sub> = 0.38) and mIT (<italic>τ</italic><sub>A</sub> = 0.4) among all model representations considered in this paper. Importantly, it falls well within the upper and lower bounds of the noise ceiling and, thus, fully explains the non-noise component of our hIT data.</p>
</sec><sec id="s2h">
<title>Model representations more similar to IT categorize better</title>
<p><xref ref-type="fig" rid="pcbi-1003915-g011">Figure 11</xref> shows the animate/inanimate categorization accuracy of linear SVM classifiers taking each of the model representations as their input (for the face/body dichotomy and the artificial/natural dichotomy among inanimates, see <xref ref-type="supplementary-material" rid="pcbi.1003915.s011">Figure S11</xref>). The categorization accuracy for each model was estimated by 12-fold crossvalidation of the 96 stimuli (<xref ref-type="sec" rid="s3">Materials and Methods</xref>). The deep convolutional network model (layer 7) has the highest animate/inanimate categorization performance (96%), and the combi27 has the second highest performance (76%).</p>
<p><xref ref-type="fig" rid="pcbi-1003915-g012">Figure 12</xref> shows that models whose representations were more similar to IT tended to have a higher animate/inanimate categorization performance. The Pearson correlation between the IT-to-model representational similarity (<italic>τ</italic><sub>A</sub> RDM correlation) and categorization accuracy was 0.75 for hIT and 0.68 for mIT across the 28 not-strongly-supervised model representations and the seven layers of the deep supervised model. This finding could simply reflect the fact that the categories correspond to clusters in the IT representation and any representation clustering the categories will be well-suited for categorization. Indeed categorization performance is also predicted by the RDM correlation between a model and an animate-inanimate categorical RDM, albeit with a lower correlation coefficient (r = 0.38, not shown).</p>
<fig id="pcbi-1003915-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003915.g012</object-id><label>Figure 12</label><caption>
<title>Model representations resembling IT afford better categorization accuracy.</title>
<p>A model's IT-resemblance (measured by the RDM correlation between IT and model) predicts its categorization accuracy (animate/inanimate). This holds for both human-IT resemblance (top) and monkey-IT resemblance (bottom). The substantial positive correlation between IT-resemblance and categorization accuracy could reflect the categorical clustering of IT (left panels). However, the within-category RDM correlation between a model and IT also predicts model categorization accuracy (right panels). Each panel shows the least-squares fit (gray line) and the Spearman rank correlation r (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). Each circle shows one of the models. Numbers indicate the model (see <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref> for model numbering). Different layers of the deep supervised convolutional network are indicated by colored labels “L1” (layer 1) to “L7” (layer 7). The deep model's layers are color-coded from light blue to light red (from lower to higher layers). Computer vision models are shown by gray circles; biologically motivated models are shown by black circles. The transparent horizontal and vertical rectangles cover non-significant ranges along each axis.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003915.g012" position="float" xlink:type="simple"/></fig>
<p>In order to further assess whether it was only the category clustering that predicted categorization accuracy or something deeper about the similarity of the model representation to IT, we considered the within-category dissimilarity correlation between each model and IT as a predictor of categorization accuracy. Models that were more similar to IT in terms of their within-category representational geometry (dissimilarities among animates and dissimilarities among inanimates) also tended to have higher categorization performance (Pearson r = 0.45 for hIT, r = 0.67 for mIT; p&lt;0.01, p&lt;0.0001, respectively).</p>
<p>These results may add to the motivation for computer vision to learn from biological vision. If computational feature spaces more similar to the IT representation yield better categorization performance within the present set of models, then it might be a good strategy for computer vision to seek to construct features even more similar to IT.</p>
</sec><sec id="s2i">
<title>Several models using Gabor filters and other low-level features explain human early visual cortex</title>
<p>We could not distinguish early visual areas V1, V2, and V3, because stimuli were presented foveally in the human fMRI experiment (2.9° visual angle in diameter, centered on fixation). Instead we defined an ROI for early visual cortex (EVC), which covered the foveal confluence of these retinotopic representations.</p>
<p>Several models using Gabor filters (SIFT, gist, PHOG, HMAX, ConvNet) and other features (Geometric blur, local self-similarity descriptor, global self-similarity descriptor, silhouette image) explained the early visual RDM estimated from fMRI (<xref ref-type="supplementary-material" rid="pcbi.1003915.s001">Figure S1A</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003915.s002">S2A</xref>). These models not only explained significant dissimilarity variance, but reached the noise ceiling, indicating that they explain the EVC representation to the extent that the noise in our data enables us to assess this. For the HMAX model (as implemented by Serre et al. <xref ref-type="bibr" rid="pcbi.1003915-Serre1">[20]</xref>), we tested several internal representations. The HMAX-C2 layer had the highest RDM correlation with EVC among all models. The HMAX-C2 layer falls within the early stages (above S1, C1, and S2 layers, and below S2b, S3, C2b, C3, and S4 layers) of the HMAX model and its features closely parallel the initial stages of primate visual processing. For the deep supervised model, the RDM correlations of different layers with EVC are shown in <xref ref-type="supplementary-material" rid="pcbi.1003915.s003">Figure S3A</xref>. Layers 2 and 3 of the model have the highest RDM correlation with EVC and reach the noise ceiling. However, their correlation with EVC is lower than that of the HMAX-C2 layer.</p>
</sec><sec id="s2j">
<title>Object-vision models and other brain regions</title>
<p>We also compared the model RDMs with brain areas other than IT and EVC (i.e. FFA, LOC, and PPA). <xref ref-type="supplementary-material" rid="pcbi.1003915.s002">Figure S2</xref> shows how well each of the 28 not-strongly-supervised models explained EVC, FFA, LOC, and PPA. The seven not-strongly-supervised models with the highest RDM correlations to these brain regions are shown in <xref ref-type="supplementary-material" rid="pcbi.1003915.s001">Figure S1</xref>. Among the not-strongly-supervised models, the HMAX model showed the highest RDM correlation with EVC and FFA. Specifically, the HMAX-C2 layer had the highest RDM correlation with EVC (<italic>τ</italic><sub>A</sub> = 0.22) and HMAX-all had the highest RDM correlation with the FFA (<italic>τ</italic><sub>A</sub> = 0.13). The combi27 model had the highest RDM correlation with LOC and PPA (<italic>τ</italic><sub>A</sub> = 0.14 and <italic>τ</italic><sub>A</sub> = 0.03, respectively).</p>
<p>For the deep supervised model, <xref ref-type="supplementary-material" rid="pcbi.1003915.s003">Figure S3</xref> shows how well different layers explain EVC, FFA, LOC, and PPA. Layers 2 and 3 reached the noise ceiling for EVC. Subsequent layers along the deep network's processing stream exhibited decreasing RDM correlations with EVC and increasing RDM correlations with LOC. Layer 7 gets closest to the LOC noise ceiling, but does not reach it. For FFA, however, layer 6 reaches the noise ceiling.</p>
<p>PPA exhibited the lowest RDM correlations with the models, including both the not-strongly-supervised and the deep supervised representations. The only model with a significant RDM correlation with PPA was combi27 (<italic>τ</italic><sub>A</sub> = 0.034, p&lt;0.001; <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>), which was far below the noise ceiling. This somewhat puzzling result might reflect a limitation of our stimulus set for investigating PPA. Konkle and Oliva <xref ref-type="bibr" rid="pcbi.1003915-Konkle1">[53]</xref> have shown that a bilateral parahippocampal region that overlaps with PPA responds more strongly to objects that are big than to objects that are small in the real world. Our stimulus set included a limited set of place and scene images and mostly objects that are small in the real world.</p>
</sec></sec><sec id="s3" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s3a">
<title>Object-vision models</title>
<p>We used a wide range of computational models to explore many different ways for extracting visual features. We selected some of the well-known biologically motivated object recognition models as well as several models and feature extractors from computer vision. Some of the models need a training phase (these are shown by a subscript –either ‘ST’ for supervised trained, or ‘UT’ for unsupervised trained) and some others do not (models without any subscript).</p>
<p>For the models with a training phase, we used a new set of 884 training images. Half of the images were animates and the other half were inanimates. Then, all models were tested using the testing stimuli (the set of 96 images). In the training set –similar to the testing set– animate images had subcategories of human/animal faces and human/animal bodies. Inanimate images had subcategories of artificial and natural inanimates.</p>
<p>Below is a description for all models used in this study (see <xref ref-type="bibr" rid="pcbi.1003915-KhalighRazavi1">[24]</xref> for a more comprehensive explanation of the models). For those models that the code was freely available online, we have provided the link.</p>
<sec id="s3a1">
<title>Stimulus image (Lab)</title>
<p>Lab color space approximates a linear representation of human perceptual color space. Each Lab image was obtained by transferring the color image (175×175) from RGB color space to the Lab color space. Then, the image was converted to a pixel vector with the length of 175×175×3.</p>
</sec><sec id="s3a2">
<title>Color set (Lab joint histogram)</title>
<p>First, images (175×175) were transferred from RGB color space to Lab color space. Then, the three Lab dimensions were divided into 6 bins of equal width. The joint histogram was computed by counting the number of figure pixels falling into each of the 6×6×6 bins. Finally, the obtained lab joint histogram was converted to a vector with the length of 6×6×6.</p>
</sec><sec id="s3a3">
<title>Radon</title>
<p>The Radon transform of an image is a matrix, in which each column corresponds to a set of integrals of the image intensities along parallel lines of a given angle. The Matlab function Radon was used to compute the Radon transform for each luminance image.</p>
</sec><sec id="s3a4">
<title>Silhouette image</title>
<p>All RGB color images were converted to binary silhouette images by setting all background pixels to 0 and all figure pixels to 1. Each image was then converted to a vector with the length of 175×175.</p>
</sec><sec id="s3a5">
<title>Unsupervised convolutional network</title>
<p>A hierarchical architecture of two stages of feature extraction, each of which is formed by random convolutional filters and subsampling layers <xref ref-type="bibr" rid="pcbi.1003915-Jarrett1">[39]</xref>. Convolutional layers scan the input image inside their receptive field. Receptive Fields (RFs) of convolutional layers get their input from various places on the input image, and RFs with identical weights make a unit. The outputs of each unit make a feature map. Convolutional layers are then followed by subsampling layers that perform a local averaging and subsampling, which make the feature maps invariant to small shifts <xref ref-type="bibr" rid="pcbi.1003915-LeCun1">[40]</xref>. The convolutional network which we used had two stages of unsupervised random filters, that is shown by RR in <xref ref-type="table" rid="pcbi-1003915-t001">table 1</xref> in Jarret et al. (2009) <xref ref-type="bibr" rid="pcbi.1003915-Jarrett1">[39]</xref>. The obtained result for each image was then vectorized. The parameters were exactly the same as used in <xref ref-type="bibr" rid="pcbi.1003915-Jarrett1">[39]</xref> (<ext-link ext-link-type="uri" xlink:href="http://koray.kavukcuoglu.org/code.html" xlink:type="simple">http://koray.kavukcuoglu.org/code.html</ext-link>).</p>
</sec><sec id="s3a6">
<title>Deep supervised convolutional network</title>
<p>This is a supervised convolutional neural network, trained with 1.2 million labelled images from ImageNet (1000 category labels) <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky2">[52]</xref>. The network has 8 layers: 5 convolutional layers, followed by 3 fully connected layers. The output of the last layer is a distribution over the 1000 class labels. This is the result of applying a 1000-way softmax on the output of the last fully connected layer <xref ref-type="bibr" rid="pcbi.1003915-Donahue1">[54]</xref> [<ext-link ext-link-type="uri" xlink:href="http://caffe.berkeleyvision.org/" xlink:type="simple">http://caffe.berkeleyvision.org/</ext-link> (Caffe: Convolutional Architecture for Fast Feature Embedding)].</p>
</sec><sec id="s3a7">
<title>Biological Transform (BT)</title>
<p>BT is a hierarchical transform based on local spatial frequency analysis of oriented segments. This transform has two stages, each of which has an edge detector followed by an interval detector <xref ref-type="bibr" rid="pcbi.1003915-Sountsov1">[38]</xref>. The edge detector consists of a bar edge filter and a box filter. For a given interval <italic>I</italic> and angle <italic>θ</italic>, the interval detector finds edges that have angle <italic>θ</italic> and are separated by an interval <italic>I</italic>. In the first stage, for any given <italic>θ</italic> and <italic>I</italic>, all pixels of the filtered image were summed and then normalized by the squared sum of the input. They were then rectified by the Heaviside function. The second stage was the same as the first stage, except that in the first stage <italic>θ</italic> was changing between 0–180° and <italic>I</italic> between 100–700 pixels and the input to the first stage had not a periodic boundary condition on the <italic>θ</italic> axis (repeating the right-hand side of the image to the left of the image and vice versa); but in the second stage the input, which is the output of the first stage, was given a periodic boundary condition on the <italic>θ</italic> axis, and <italic>I</italic> was changing between 15–85 pixels.</p>
</sec><sec id="s3a8">
<title>Gist</title>
<p>Each image was divided into 16 bins, and then oriented Gabor filters (in 8 orientations) were applied over different scales (4 scales) in each bin. Finally, the average filter energy in each bin was calculated <xref ref-type="bibr" rid="pcbi.1003915-Oliva1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Oliva2">[55]</xref>. Then each obtained image was converted to a vector of length (8×8×8). The code is available from here: <ext-link ext-link-type="uri" xlink:href="http://people.csail.mit.edu/torralba/code/spatialenvelope/" xlink:type="simple">http://people.csail.mit.edu/torralba/code/spatialenvelope/</ext-link></p>
</sec><sec id="s3a9">
<title>Geometric Blur (GB)</title>
<p>289 uniformly distributed points were selected on each image, then the Geometric Blur descriptors <xref ref-type="bibr" rid="pcbi.1003915-Belongie1">[56]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Zhang1">[58]</xref> were calculated by applying spatially varying blur around the feature points. We used GB features that were part of multiple kernels for image classification described in <xref ref-type="bibr" rid="pcbi.1003915-Vedaldi1">[59]</xref>(<ext-link ext-link-type="uri" xlink:href="http://www.robots.ox.ac.uk/~vgg/software/MKL/#download" xlink:type="simple">http://www.robots.ox.ac.uk/~vgg/software/MKL/#download</ext-link>). The blur parameters were set to α = 0.5 and β = 1; the number of descriptors was set to 300.</p>
</sec><sec id="s3a10">
<title>Dense SIFT</title>
<p>For each grayscale image, SIFT descriptors <xref ref-type="bibr" rid="pcbi.1003915-Lowe2">[60]</xref> of 16×16 pixel patches were sampled uniformly on a regular grid. Then, all the descriptors were concatenated in a vector as the SIFT representation of that image. We used the dense SIFT descriptors that were used in <xref ref-type="bibr" rid="pcbi.1003915-Lazebnik1">[44]</xref> to extract PHOW features, described below.</p>
</sec><sec id="s3a11">
<title>Pyramid Histogram of Visual Words (PHOW<sub>UT</sub>)</title>
<p>Dense SIFT descriptors were calculated for each image and then quantized using k-means clustering to form a visual vocabulary. A spatial pyramid of three levels was then created and the histogram of SIFT visual words was calculated for each bin. The concatenation of all histograms was used as the PHOW representation of that image <xref ref-type="bibr" rid="pcbi.1003915-Lazebnik1">[44]</xref>. We used the implementation available online(<ext-link ext-link-type="uri" xlink:href="http://www.cs.unc.edu/~lazebnik/research/spatial_pyramid_code.zip" xlink:type="simple">http://www.cs.unc.edu/~lazebnik/research/spatial_pyramid_code.zip</ext-link>). The dictionary size was fixed to 200 and the number of spatial pyramid levels was fixed to three.</p>
</sec><sec id="s3a12">
<title>Pyramid Histogram of Gradients (PHOG)</title>
<p>The canny edge detector was applied on grayscale images, and then a spatial pyramid was created with four levels <xref ref-type="bibr" rid="pcbi.1003915-Bosch1">[45]</xref>. The histogram of orientation gradients was calculated for all bins in each level. All histograms were then concatenated to create PHOG representation of the input image. We used Matlab implementation that was freely available online (<ext-link ext-link-type="uri" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/caltech/phog.html" xlink:type="simple">http://www.robots.ox.ac.uk/~vgg/research/caltech/phog.html</ext-link>). Number of quantization bins was set to forty, number of pyramid levels to four and the angular range to 360°.</p>
</sec><sec id="s3a13">
<title>VisNet <sub>UT</sub></title>
<p>VisNet is a hierarchical model of ventral visual pathway for invariant object recognition that has four successive layers of self-organizing maps. Neurons which are higher in the hierarchy have larger receptive fields. Each layer in the model corresponds to a specific area of the primate ventral visual pathway in terms of the size of its receptive fields <xref ref-type="bibr" rid="pcbi.1003915-Wallis1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Tromans1">[61]</xref>. The model was trained with trace learning rule <xref ref-type="bibr" rid="pcbi.1003915-Stringer1">[62]</xref>. The learning rate was set to 0.1 and number of epochs in each of the four layers was fixed to 100. Finally the representation of the last layer was vectorized and used as VisNet features.</p>
</sec><sec id="s3a14">
<title>Local self-similarity descriptor (ssim)</title>
<p>This is a descriptor that is not directly based on the image appearance; instead, it is based on the correlation surface of local self-similarities. For computing local self-similarity features at a specific point on the image, say <italic>p</italic>, a local internal correlation surface can be created around <italic>p</italic> by correlating the image patch centred at <italic>p</italic> to its immediate neighbours <xref ref-type="bibr" rid="pcbi.1003915-Shechtman1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Chatfield1">[63]</xref>. We used the code available for ssim features that were part of multiple kernels for image classification described in <xref ref-type="bibr" rid="pcbi.1003915-Vedaldi1">[59]</xref>(<ext-link ext-link-type="uri" xlink:href="http://www.robots.ox.ac.uk/~vgg/software/SelfSimilarity/" xlink:type="simple">http://www.robots.ox.ac.uk/~vgg/software/SelfSimilarity/</ext-link>). The ssim descriptors were computed uniformly at every five pixels in both X and Y directions.</p>
</sec><sec id="s3a15">
<title>Global self-similarity descriptor (gssim)</title>
<p>This descriptor is an extension of the local self-similarity descriptor mentioned above. Gssim uses self-similarity globally to capture the spatial arrangements of self-similarity and long range similarities within the entire image <xref ref-type="bibr" rid="pcbi.1003915-Deselaers1">[47]</xref>. We used gssim Matlab implementation available online(<ext-link ext-link-type="uri" xlink:href="http://www.vision.ee.ethz.ch/~calvin/software.html" xlink:type="simple">http://www.vision.ee.ethz.ch/~calvin/software.html</ext-link>). Number of clusters for the patch prototype codebook was set to 400, with 20000 patches to be clustered. D1 and D2 for the self-similarity hypercube were both set to 10.</p>
</sec><sec id="s3a16">
<title>Local Binary Patterns (LBP)</title>
<p>Local binary patterns are usually used in texture categorization. The underlying idea of LBP is that a 2-dimensional surface can be described by two complementary measures: local spatial patterns and gray scale contrast. For a given pixel, LBP descriptor gives binary labels to surrounding pixels by thresholding the difference between the intensity value of the pixel in the center and the surrounding pixels <xref ref-type="bibr" rid="pcbi.1003915-Ojala1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Ojala2">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Pietikinen1">[65]</xref>. We used LBP Matlab implementation freely available online(<ext-link ext-link-type="uri" xlink:href="http://www.cse.oulu.fi/CMV/Downloads/LBPMatlab" xlink:type="simple">http://www.cse.oulu.fi/CMV/Downloads/LBPMatlab</ext-link>). Number of sampling points was fixed to eight.</p>
</sec><sec id="s3a17">
<title>V1 model</title>
<p>A population of simple and complex cells were modelled and were fed by the luminance images as inputs. Gabor filters of 4 different orientations (0°, 90°, −45°, and 45°) and 12 sizes (7–29 pixels) were used as simple cell receptive fields. Then, the receptive field of complex cells were modelled by performing the MAX operation on the neighboring simple cells with similar orientations. The outputs of all simple and complex cells were concatenated in a vector as the V1 representational pattern of each image.</p>
</sec><sec id="s3a18">
<title>HMAX<sub>UT</sub></title>
<p>The HMAX model developed by Serre et al. <xref ref-type="bibr" rid="pcbi.1003915-Serre1">[20]</xref> has a hierarchical architecture inspired by the well-known simple to complex cells model of Huble &amp; Wiesel <xref ref-type="bibr" rid="pcbi.1003915-HUBEL1">[66]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Hubel1">[67]</xref>. There has been several extensions to the HMAX model, improving its feature selection process (e.g. <xref ref-type="bibr" rid="pcbi.1003915-Ghodrati2">[37]</xref>) or adding new processing layers to the model <xref ref-type="bibr" rid="pcbi.1003915-Zabbah1">[68]</xref>. The HMAX model that is used here adds three more layers –ends at S4- on the top of the complex cell outputs of the V1 model described above. The model has alternating S and C layers. S layers perform a Gaussian-like operation on their inputs, and C layers perform a max-like operation, which makes the output invariant to small shifts in scale and position. We used the freely available version of the HMAX model (<ext-link ext-link-type="uri" xlink:href="http://cbcl.mit.edu/software-datasets/pnas07/index.html" xlink:type="simple">http://cbcl.mit.edu/software-datasets/pnas07/index.html</ext-link>). All simple and complex layers were included until the S4 layer.</p>
<p>Note: The HMAX model which has been used in <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte3">[18]</xref> was a pre-trained version of the HMAX model; however, in this study we have trained the HMAX model using a dataset that contains 442 animate and 442 inanimate objects. So, the obtained RDMs are different.</p>
</sec><sec id="s3a19">
<title>Sparse Localized Features (SLF<sub>UT</sub>)</title>
<p>This is a biologically motivated model based on the HMAX C2 features. The model introduces sparsified and localized intermediate-level visual features <xref ref-type="bibr" rid="pcbi.1003915-Mutch1">[33]</xref>. We used the Matlab code available for these feature (<ext-link ext-link-type="uri" xlink:href="http://www.mit.edu/~jmutch/fhlib/" xlink:type="simple">http://www.mit.edu/~jmutch/fhlib/</ext-link>); and the default model parameters were used.</p>
</sec><sec id="s3a20">
<title>GMAX<sub>ST</sub></title>
<p>This model is an extension of the HMAX model C2 features in which authors have used feedback from the classification layer (analogous to PFC) to extract informative visual features. Their method uses an optimization algorithm (i.e. genetic algorithm) to select informative patches from a large pool of patches <xref ref-type="bibr" rid="pcbi.1003915-Ghodrati1">[35]</xref>. Using genetic algorithm a subset of patches that gives the best categorization performance is selected. A linear SVM classifier was used to calculate the categorization performance. In other words, in the training phase of the model the categorization performance is used as the fitness function for the genetic algorithm. To run this model we used the same set of model parameters suggested in <xref ref-type="bibr" rid="pcbi.1003915-Ghodrati1">[35]</xref>. In the process of finding optimal patches in the optimization algorithm, we used a random subset of 884 training images described before.</p>
</sec><sec id="s3a21">
<title>Stable Model <sub>UT</sub></title>
<p>This is another biologically motivated model, which has a hierarchy of simple to complex cells. The model uses the adaptive resonance theory (ART) mechanism <xref ref-type="bibr" rid="pcbi.1003915-Grossberg1">[69]</xref> for extracting informative intermediate level visual features. This has made the model stable against forgetting previously learned patterns <xref ref-type="bibr" rid="pcbi.1003915-Rajaei1">[36]</xref>. Similar to HMAX model it extracts C2-like features, except that in the training phase it only selects the highest active C2 units as prototypes that represent the input image. This is done using top-down connections from C2 layer to C1 layer. The connections match the C1-like features of the input image to the prototypes of the C2 layer. The matching degree is controlled by a vigilance parameter that is fixed separately on a validation set. We set the model parameters the same as was suggested by authors except that instead of using all patch sizes, we used patches of size 12 that made the output RDM more correlated with brain RDMs. It is also shown in <xref ref-type="bibr" rid="pcbi.1003915-Rajaei1">[36]</xref> that patches of size 12 make the model more stable. Furthermore when using patches of size 12, the model performs better in the face/non-face classification task <xref ref-type="bibr" rid="pcbi.1003915-Rajaei1">[36]</xref>.</p>
</sec><sec id="s3a22">
<title>Supervised HMAX<sub>ST</sub></title>
<p>We used this approach to remove non-discriminative patches of the HMAX model. After training the HMAX model with the training images of animates and inanimates, extracted patches were divided into two clusters using k-means clustering. One cluster represented the patches extracted from animate images, and the other cluster represented the patches extracted from inanimate images. Then, in order to remove the non-discriminative patches (i.e. patches that do not distinguish between animates and inanimates), those patches that were extracted from the animate images but fell nearer to the center of the inanimate cluster were removed. Similarly the patches that were extracted from the inanimate images but fell nearer to the center of the animate cluster were removed. The remaining patches were used for the test phase.</p>
</sec><sec id="s3a23">
<title>Combination of all not-strongly-supervised models (combi27)</title>
<p>This is the concatenation of features extracted by all of the above-mentioned models. Given an input stimulus, features from all of the above-mentioned models were extracted. Because the dimension for extracted features differs across models, we used principle component analysis (PCA) to reduce the dimension of all of them to a unique number. We used the first 95 PCs from each of the models and concatenated them along a vector (95 was the largest possible number of PCs that we were able to use, because we had 96 images; so the covariance matrix has only 95 non-zero eigenvalues). Therefore, combi27 features for each image is a vector of length 95×27 = 2565.</p>
<p>For some of the above-mentioned models that had a hierarchical architecture, we made an RDM for each of the stages in the hierarchy, as well as an RDM from the concatenation of the model representation in all stages.</p>
</sec></sec><sec id="s3b">
<title>Fitting of category-cluster RDMs to model and brain RDMs</title>
<p>Ten category-cluster RDMs (<xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>) were created as predictors for a linear model of each RDM. The category clusters were: animate, inanimate, face, human face, non-human face, body, human body, non-human body, natural inanimate, and artificial inanimate. To measure the clustering strength for each of the categories in each brain and computational-model RDM, we fit the category-cluster RDMs to each brain and computational-model RDM minimizing the sum of squared dissimilarity deviations (<xref ref-type="fig" rid="pcbi-1003915-g003">Figure 3</xref>).</p>
<p>The design matrix for the least-squares fitting was created using the ten category RDMs (each RDM was vectorized to form a column in the design matrix) with addition of a constant vector of 1 (confound mean RDM). Then the category model RDMs were fitted to object-vision model RDMs. Bars in <xref ref-type="supplementary-material" rid="pcbi.1003915.s006">Figure S6</xref> show the fitted coefficients (Beta values). Standard errors and p values are based on bootstrapping of the stimulus set. For each bootstrap sample of the stimulus set, a new instance is generated for the reference RDM (e.g. hIT RDM) and for each of the candidate RDMs (e.g. model RDMs). We did stratified resampling, which means that the proportion of categories was the same across all bootstrapped resamples. Because bootstrap resampling is resampling with replacement, the same condition can appear multiple times in a sample. This entails 0 entries (from the diagonal of the original RDM) in off-diagonal positions of the RDM for a bootstrap sample. These zeros are treated as missing values and excluded from the dissimilarities, across which the RDM correlations are computed. The number of bootstrap resamplings used in bootstrap tests was 10,000.</p>
</sec><sec id="s3c">
<title>Weighting model features</title>
<sec id="s3c1">
<title>Remixing of features</title>
<p>For the not-strongly-supervised models as well as the deep supervised model representations, we attempted to create new features as linear combinations of the original features that specifically emphasize the missing categorical divisions. For the not-strongly-supervised models, we used combi27 features to find these linear combinations. Three linear support vector machine (SVM) classifiers for body/nonbody, face/non-face, and animate/inanimate categorization were trained on a set of 884 labeled images of isolated objects nonoverlapping with the set of 96 images. We then used the decision-value outputs of the classifiers as new features. The resulting single-feature RDMs for the not-strongly-supervised models are shown in <xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, top – one RDM for each SVM. For the deep supervised model, we used features from layer 7 to find linear combinations that emphasize the categorical divisions. The resulting single-feature RDMs for the deep supervised model are shown in <xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref>, top.</p>
</sec><sec id="s3c2">
<title>Reweighting of features</title>
<p>We tested whether appropriate weighting of the combination of the original model features and the new features learned by remixing could further improve the explanation of the IT geometry. We did the reweighting for both not-strongly-supervised model features, and deep supervised model features. For the not-strongly-supervised models, in addition to the 27 not-strongly-supervised models, we included the combi27 model and the three categorical SVM discriminants (learned through remixing) in the set of representations to be combined. We fitted one weight for each of these representations (27+1+3 = 31 weights in total), so as to best explain the hIT RDM. <xref ref-type="fig" rid="pcbi-1003915-g005">Figure 5</xref>, middle row, shows the weights obtained for each of the model representations. For the deep supervised model representations, we weighted the combination of all eight layers of the deep convolutional network and the three categorical SVM discriminants obtained by remixing the deep supervised features. Please note that one weight is learned for each layer, and each of the SVM discriminants (8+3 = 11 weights in total). <xref ref-type="fig" rid="pcbi-1003915-g010">Figure 10</xref>, middle row, shows the weights obtained for each of the layers of the deep convolutional network and the SVM discriminants.</p>
<p>We used a non-negative-least-squares fitting algorithm <xref ref-type="bibr" rid="pcbi.1003915-Lawson1">[51]</xref> to find the non-negative weights for the models that minimize the sum of squared deviations between the hIT RDM and the RDM of the weighted combination of models.</p>
<p>The RDM of the weighted combination of the model features is equivalent to a weighted combination of the RDMs of the models when squared Euclidean distance is used. We used the squared Euclidean distance for normalized representational patterns, which is equivalent to correlation distance, as used throughout this paper. We therefore applied the nonnegative least-squares algorithm at the level of the RDMs. This procedure is further explained in the following equations: <xref ref-type="disp-formula" rid="pcbi.1003915.e001">equations (1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003915.e002">(2)</xref>.</p>
<p><xref ref-type="disp-formula" rid="pcbi.1003915.e001">Equation (1)</xref> states that the squared distance between weighted model features, equals the weighted squared distance of the features:<disp-formula id="pcbi.1003915.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003915.e001" xlink:type="simple"/><label>(1)</label></disp-formula>Where <italic>w<sub>k</sub></italic> is the weight given to model <italic>k</italic>. <italic>f<sub>k,l</sub>(i)</italic> is the <italic>l<sup>th</sup></italic> feature extracted by model <italic>k</italic> for stimulus <italic>i</italic>.</p>
<p><xref ref-type="disp-formula" rid="pcbi.1003915.e002">Equation (2)</xref> shows how each of the <italic>n</italic> model representations are weighted by minimizing the sum of squared deviations between the hIT RDM and the RDM of the weighted combination of model representations.<disp-formula id="pcbi.1003915.e002"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003915.e002" xlink:type="simple"/><label>(2)</label></disp-formula>Where <italic>d<sub>i,j</sub></italic> is the distance between stimuli <italic>i,j</italic> in the hIT RDM. <bold>w</bold> is the weight vector that minimizes the sum of squared errors between the pairwise dissimilarities of the stimuli in the hIT representation and the pairwise dissimilarities of the weighted combination of model features. <italic>k</italic> changes from 1 to <italic>n</italic> where <italic>n</italic> is the number of model representations to be weighted. <italic>m<sub>k</sub></italic> indicates the number of features for model <italic>k</italic>.</p>
<p>To avoid overfitting to the image set, we fitted the weights to random subsets of 88 of the 96 images in a crossvalidation procedure, holding out 8 images on each fold. The representational dissimilarities for the weighted-combination model was then estimated for the 8 held-out images. This procedure was repeated until the pairwise dissimilarities for the entire RDM of 96 by 96 images were estimated.</p>
</sec></sec><sec id="s3d">
<title>IT-geometry model</title>
<p>The IT-geometry supervised models (i.e. IT-geometry-supervised combi27, and IT-geometry-supervised deep convolutional network) are made by remixing and reweighting of the model features. For the IT-geometry-supervised combi27, only the not-strongly-supervised models were used for remixing and reweighting; and for the IT-geometry-supervised deep convolutional network, only deep supervised model representations were used for remixing and reweighting.</p>
<p>For both of them, as explained before in the context of remixing and reweighting, we trained three SVM classifiers for animate/inanimate, face/nonface, and body/nonbody classification using 884 training images. The SVM classifiers were then fed with the 96 stimuli and we used the SVM decision values as new features. The non-negative least square fitting was then used for finding the optimal weights for different model representations and the SVM discriminant features so as to minimize the sum of squared errors between the RDM of the weighted combination of the features and the hIT RDM.</p>
<p>For making the IT-geometry supervised RDM, which is a weighted combination of the model representations and the SVM discriminants, we fit the non-negative weights by cross-validating the stimulus set. Each time we randomly left out 8 stimuli (4 animates and 4 inanimates) from the set of 96, and learned the optimal weights over the remaining stimuli (88 images) so as to minimize the sum of squared errors between the RDM of the weighted combination of the features and the hIT RDM. Note that the hIT RDM and the model RDMs become 88×88 (not 96×96) because 8 stimuli are left out. The obtained weights were then applied to weight the model feature for the left-out stimuli. The result is an 8×8 weighted RDM that shows the pairwise dissimilarities for the left-out stimuli. This procedure was repeated for several times until a point that we had the cross-validated pairwise dissimilarities for all the 96 stimuli.</p>
</sec><sec id="s3e">
<title>Categorization performance of models</title>
<p>We calculated the categorization performance of the object-vision models in the following categorization tasks: animates vs. inanimates (<xref ref-type="fig" rid="pcbi-1003915-g011">Figure 11</xref>), faces vs. bodies (<xref ref-type="supplementary-material" rid="pcbi.1003915.s011">Figure S11B</xref>), and artificial inanimates vs. natural inanimates (<xref ref-type="supplementary-material" rid="pcbi.1003915.s011">Figure S11A</xref>). For each of the models, a SVM classifier <xref ref-type="bibr" rid="pcbi.1003915-Chang1">[70]</xref> with a linear kernel was trained using k-fold cross validation (k = 12). The 96 stimuli were randomly partitioned into k = 12 equal size folds. Of the <italic>k</italic> folds, a single fold was retained as the validation data for testing the model categorization performance, and the remaining <italic>k−1</italic> folds were used as training data. The cross-validation process was then repeated <italic>k</italic> times, with each of the <italic>k</italic> folds used exactly once as the validation data. The <italic>k</italic> results from the folds were then averaged.</p>
<p>For each of the categorization tasks the SVM was trained in the following way:</p>
<list list-type="alpha-lower"><list-item>
<p>For the animate vs. inanimate categorization task, we had 96 stimuli. We left out 8 stimuli (4 animates and 4 inanimates) that were used as the validation data, and the SVM was trained using the remaining stimuli.</p>
</list-item><list-item>
<p>For the face vs. body categorization task, we had 48 stimuli. We left out 4 stimuli (2 faces and 2 bodies) that were used as the validation data, and the SVM was trained using the remaining stimuli.</p>
</list-item><list-item>
<p>For the artificial vs. natural inanimate categorization task, again we had 48 stimuli. We left out 4 stimuli (2 artificial and 2 natural inanimates) that were used as the validation data, and the SVM was trained using the remaining stimuli.</p>
</list-item></list>
<p>To see if a model categorization performance significantly differs from chance, we did a permutation test by retraining the models after category-orthogonalized permutation of labels.</p>
</sec><sec id="s3f">
<title>Representational similarity analysis (RSA)</title>
<p>RSA enables us to relate representations obtained from different modalities (e.g. computational models and fMRI patterns) by comparing the dissimilarity patterns of the representations. In this framework representational dissimilarity matrices (RDMs) are used for making the link between different modalities. RDM is a square symmetric matrix in which the diagonal entries reflect comparisons between identical stimuli and are 0, by definition. Each off-diagonal value indicates the dissimilarity between the activity patterns associated with two different stimuli. RDM summarizes the information carried by a given representation from an area in the brain or a computational model.</p>
<p>We had 96 stimuli, of which half were animates and the other half were inanimates. To calculate the RDM for a brain region or a computational model, a 96×96 matrix was made in which each cell was filled with the dissimilarity value between the response patterns elicited by two stimuli. For each pair of stimuli, the dissimilarity measure was 1 minus the Pearson correlation between the response patterns elicited by those stimuli in a brain region or a computational model.</p>
</sec><sec id="s3g">
<title>Kendall <italic>τ<sub>A</sub></italic> (tau-a) correlation and noise ceiling</title>
<p>To judge the ability of a model RDM in explaining a brain RDM, we used Kendall's rank correlation coefficient <italic>τ</italic><sub>A</sub> (which is the proportion of pairs of values that are consistently ordered in both variables). When comparing models that predict tied ranks (e.g. category model RDMs) to models that make more detailed predictions (e.g. brain RDMs, object-vision model RDMs) Kendall's <italic>τ</italic><sub>A</sub> correlation is recommended. In these occasions τA correlation is more likely than the Pearson and Spearman correlation coefficients to prefer the true model over a simplified model that predicts tied ranks for a subset of pairs of dissimilarities. For more information in this regard please refer to the RSA Toolbox paper <xref ref-type="bibr" rid="pcbi.1003915-Nili1">[30]</xref>. This is the first toolbox to implement RSA. It is a modular and work-flow based toolbox that supports an analysis approach that is simultaneously data- and hypothesis-driven. There are a set of “Recipe” functions in the toolbox that allow automatic ROI analysis as well as whole-brain searchlight analysis. Tools for visualization and inference enable the user to relate sets of models to sets of brain regions and to statistically test and compare the models using nonparametric inference methods.</p>
<p><xref ref-type="fig" rid="pcbi-1003915-g002">Figure 2</xref> shows <italic>τ</italic><sub>A</sub> correlation of the hIT/mIT RDM with model RDMs. To estimate significance, randomization and bootstrap tests were used. Randomization tests permute the stimulus labels whereas bootstrap tests bootstrap resample the conditions set.</p>
<p>The noise in the brain activity data has imposed limitations on the amount of dissimilarity variance that a model RDM can explain. Therefore an estimation of noise-ceiling was needed to indicate how much variance of a brain RDM –given the noise level– was expected to be explained by an ideal model RDM (i.e. a model RDM that is able to perfectly capture the true dissimilarity structure of the brain RDM).</p>
<p>The noise-ceiling in <xref ref-type="fig" rid="pcbi-1003915-g002">Figure 2A</xref> is shown by a gray horizontal bar. The upper and lower edges of this bar correspond to upper- and lower-bound estimates on the group-average correlation with the RDM predicted by the unknown true model. There is a hard upper limit to the average correlation with the single-subject reference-RDM estimates that any RDM can achieve for a given data set. Intuitively, the RDM maximizing the group-average correlation lies at the center of the cloud of single-subject RDM estimates. To find an upper bound, we averaged the rank-transformed single-subject RDMs and used an iterative procedure to find the RDM that has the maximum average Kendall's <italic>τ</italic><sub>A</sub> correlation to the single-subject RDMs. This average RDM can be thought of as an estimate of the true model's RDM. This estimate is overfitted to the single-subject RDMs. Its average correlation with the latter therefore overestimates the true model's average correlation, thus providing an upper bound. To estimate a lower bound, we employed a leave-one-subject-out approach. We computed each single-subject RDM's correlation with the average of the other subjects' RDMs. This prevents overfitting and underestimates the true model's average correlation because the amount of data is limited, thus providing a lower bound on the ceiling. For more information about the noise ceiling please refer to the toolbox paper <xref ref-type="bibr" rid="pcbi.1003915-Nili1">[30]</xref>. We did not estimate a noise ceiling for the cell recording data, because our procedure requires several individuals to be measured and we only had data for two monkeys.</p>
</sec><sec id="s3h">
<title>Equating the noise level in the models and the human IT</title>
<p>To compare the categoricality in the models with the categoricality in human IT, we added Gaussian noise to the models to equate the level of noise in the models with that of the fMRI data. To this end, we averaged the pairwise correlation between the IT RDMs of the four human subjects; let's denote the obtained value with ‘<italic>q</italic>’. Then to add the same amount of noise to the models, we iteratively and increasingly added noise to the model outputs until they reach the same level of noise as in human IT. The procedure for each model was that, we made new instantiations of that model by adding random Gaussian noise to the model output. We did this four times for each model, therefore having four noisy instantiation for each model. Then we made four model RDMs for each of the noisy model features, and calculated the mean of their pairwise correlation, which we denote by ‘<italic>q<sub>m</sub></italic>’. If the obtained mean is equal to the mean of the pairwise correlation between the four hIT RDMs, denoted by ‘<italic>q</italic>’, (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003915.e003" xlink:type="simple"/></inline-formula>) we stop the iteration, otherwise the procedure is repeated and in each iteration the added noise to the model output is updated. At the end, when the stopping criterion is satisfied, the four model RDMs are averaged and used as the noise-equated model RDM.</p>
</sec><sec id="s3i">
<title>Stimuli and response measurements</title>
<p>We used the experimental stimuli from Kriegeskorte et al. <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>. The stimuli were 96 images which half were animates and the other half were inanimates. The animate cluster consisted of faces and bodies, and the inanimate cluster consisted of natural and artificial inanimates.</p>
<p>For cell recording data, we had 92 stimuli. To make 92×92 RDMs comparable with 96×96 RDMs, we made a 96×96 RDM from 92×92 RDM by filling the gaps with NaN.</p>
<p>The fMRI and cell recording data, which we used here, have been previously described and analyzed to address different questions. See <xref ref-type="bibr" rid="pcbi.1003915-Kiani1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte3">[18]</xref> for further experimental details.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<p>Computer vision has made great strides in recent years. Early attempts to achieve vision by fitting generative graphics models to images faltered because of the exponential complexity of the search space. However, computer vision made progress in practical applications using hand-engineered feedforward features in combination with machine learning classifiers. In recent years, the advent of efficient training algorithms for deep neural networks <xref ref-type="bibr" rid="pcbi.1003915-LeCun1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Hinton1">[71]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Bengio1">[72]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky1">[41]</xref> has made it possible to learn from image data not just the final classification step, but also the internal representations. This approach has yielded unprecedented object-recognition performance, reaching levels comparable to humans on certain tasks (e.g. <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Zeiler1">[73]</xref>).</p>
<p>These new deep vision models share certain features, some of which parallel the primate visual system. First, they are feedforward hierarchical models: They are composed of a series of stages of representation, where each stage is computed from the output of the previous stage. Moderate modifications of this scheme with bypass connections are also sometimes used. Second, each stage is composed of features, which are linear filters of the previous stage followed by a static nonlinearity. The nonlinearity is key to the representational power of these networks because a sequence of linear transforms would reduce to a single linear transformation. Third, they are convolutional <xref ref-type="bibr" rid="pcbi.1003915-LeCun1">[40]</xref>, computing each linear feature of the input at all visual-field locations. This architectural constraint reduces the effective number of parameters and automatically confers translation invariance. Fourth, they compute visually local features with receptive field sizes increasing from stage to stage, thus gradually transforming a space-based image-like representation into space-insensitive shape-based and semantic representation. Fifth, they are deep, typically including four or more layers of representation. Even shallow neural networks can approximate any nonlinear mapping from input to output. However, deep networks can find concise representations (requiring fewer units) of complex functions. This is essential to make them realistic in terms of both physical implementation and learnability <xref ref-type="bibr" rid="pcbi.1003915-Bengio1">[72]</xref>. Sixth, they are trained with many category-labeled example images, typically more than a million (e.g. <xref ref-type="bibr" rid="pcbi.1003915-Krizhevsky1">[41]</xref>).</p>
<p>A few studies have begun to compare recognition performance and internal representations between these models and primate IT. These investigations have so far given largely convergent results. First, models that perform better at object recognition tend to have representations more similar to IT <xref ref-type="bibr" rid="pcbi.1003915-KhalighRazavi2">[74]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Cadieu1">[76]</xref>. Second, the new deep supervised models perform at unprecedented levels at predicting the IT representation <xref ref-type="bibr" rid="pcbi.1003915-Yamins1">[75]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Yamins2">[77]</xref>.</p>
<p>Our exploration here placed deep supervised models in the context of a wide range of computer-vision features, revealing the extent to which each of these computational mechanisms can explain the IT representational geometry in human and monkey. In addition, we analyzed the degree to which each of the models emphasizes various categorical divisions. Our results, spanning the gamut from unsupervised to strongly supervised models, suggest that strong supervision with many category-labeled images is essential for building features that explain the IT representational geometry. The not-strongly-supervised models were significantly less categorical than IT and this was part of the reason why they failed to explain the IT representational geometry. In addition, IT appears to have a particular categorical geometry. This is consistent with the idea that IT is visuo-semantic, representing visual features including shape, but also imposing categorical divisions (or emphasizing semantic dimensions) that are relevant to the organism's survival and reproduction.</p>
<p>We find strong similarities between the representational geometries of a deep supervised model and IT (see also <xref ref-type="bibr" rid="pcbi.1003915-Cadieu1">[76]</xref>). This is important because it suggests that deep supervised models capture something essential about the IT representation. However, the fact that our IT-geometry-supervised deep representation fully explains our IT data should not be overinterpreted.</p>
<p>First, these models operate in a feedforward fashion, and do not capture the recurrent dynamics in the visual hierarchy. This component of visual processing might be sufficient for “core object recognition” <xref ref-type="bibr" rid="pcbi.1003915-DiCarlo1">[78]</xref>, i.e. rapid recognition at a glance. However, vision provides us with a much more complex appreciation of our surroundings and supports a wide array of tasks. Biological vision involves recurrent processing as well as active exploration of the scene with attentional shifts and eye movements. In the present experiments, stimuli were presented for 105 ms (monkeys) and 300 ms (humans) and eye movements and object-related attentional processes were minimized by using fixation tasks. The experiments were, thus, designed to focus on automatic, task-independent processing. However, recurrent processing is nevertheless likely to have contributed to the emergence of the IT representation. Indeed, recent human magnetoencephalography studies using the same stimulus set <xref ref-type="bibr" rid="pcbi.1003915-Carlson1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Cichy1">[12]</xref> suggest that the major categorical divisions take slightly longer to emerge than a purely feedforward account would predict. This evidence is not unequivocally localized to IT and thus should be interpreted with caution. However, the categorical clustering achieved in a purely feedforward fashion in the deep supervised model considered here might be achieved with some degree of involvement of recurrent computations in the brain.</p>
<p>Second, the IT-geometry-supervised model needed to be explicitly trained to emphasize the same categorical divisions as IT. The analysis of our human and monkey data in <xref ref-type="bibr" rid="pcbi.1003915-Yamins2">[77]</xref> similarly found that a hierarchical feedforward model optimized for invariant object recognition could account for the IT representational geometry only when linear readout features emphasizing the appropriate categorical divisions were fitted to the data. On the one hand, our study suggests that visual similarity (as operationalized by the wide range of unsupervised visual features we investigated) cannot explain the categorical clustering. On the other hand, it begs the question why IT emphasizes the particular divisions between faces and bodies and between animates and inanimates, while deemphasizing other divisions (such as the one between human and animal faces).</p>
<p>Third, our study is limited by the image set. All objects were centered on fixation and presented in isolation on a gray background at the same retinal size. The stimulus set, thus, was not challenging in terms of position, size, and clutter invariance. However, the IT representation has been shown to be less sensitive to changes of position, size, and context than earlier stages of processing <xref ref-type="bibr" rid="pcbi.1003915-Rust1">[79]</xref>. Cadieu et al. (2014) <xref ref-type="bibr" rid="pcbi.1003915-Cadieu1">[76]</xref> used an image set with substantial variations of position, size, and clutter to compare the representation in the same deep supervised model to monkey-IT data and found the categorical clustering to be robust to these variations. Although our image set did not vary position, size, and clutter, note that it covered a broad range of categories and within each category, there was substantial variation among the exemplars in terms of both their intrinsic properties and accidental properties of their appearance, including pose and lighting. The wide exemplar variations within broad categories like animates might present an even more difficult challenge than varying position and size. The human face photos were mostly frontal and therefore visually similar (as reflected in the clustering of human faces in many of the unsupervised feature models). However, the variation among the animal faces and among the exemplars within the animate and inanimate categories was substantial. Taken together, current evidence suggests that the categorical clustering we observed is not an artefact of the stimulus set.</p>
<p>Finally, our data set was affected by noise and intersubject variation. The human fMRI data, for which we were able to compute the noise ceiling, was from 8 sessions (2 sessions in each of four subjects <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>). The fact that the IT-geometry-supervised deep model fully explained the representational geometry of IT does not mean that its representation is identical to IT, but just that given noise and intersubject variability it is not significantly different. Future studies should use more comprehensive data sets to reveal remaining representational discrepancies between IT and deep supervised models.</p>
<sec id="s4a">
<title>What does it mean for a representation to be “categorical” or “semantic”?</title>
<p>The IT representation has been described as categorical by some authors <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref> and as a visual shape space by others <xref ref-type="bibr" rid="pcbi.1003915-Baldassi1">[23]</xref>. How should a “categorical” representation be defined in this context?</p>
<p>One meaning of categoricality refers to the degree to which categorical divisions are <italic>explicit</italic> in the representation. The images themselves (and their retinal representations) clearly contain category information. However, this information is not explicit. Instead it requires a highly nonlinear readout mechanism commonly referred to as object recognition. An explicit representation is sometimes defined <xref ref-type="bibr" rid="pcbi.1003915-DiCarlo1">[78]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte5">[80]</xref> as one that enables linear readout of the category dichotomy. Since linear readout is a trivial one-step operation in a biological neuronal network, this definition of “explicit” is arguably only slightly broader than requiring single-cell step-like responses encoding the category dichotomy. Linear discriminability does not require that the categories form separate clusters in the representational space. A bimodal distribution in representational space, with two clusters corresponding to the categories and divided by a margin or region of lower density, could be considered to be an even more explicitly categorical representation than one that merely enabled linear readout.</p>
<p>Defining categoricality as the degree to which category information is explicit (as all the above definitions do) may be useful in some contexts. However, it misses a crucial point. Depending on the nature of the images and categories, “explicit” category representations could be observed in: (1) pixel images or color histograms, (2) simple computational features (e.g. Gabor filters or gist features), (3) more complex unsupervised features (e.g. HMAX features). If the features happen to be sufficiently correlated with a categorical division, these “visual” representations would be considered explicitly categorical by the above definitions. This illustrates the difficulty of drawing a clear line between visual and categorical (or semantic) representations.</p>
<p>We would rather not refer to the representation as “categorical” when the categories are already separated in the distribution of the sensory input patterns. We therefore suggest a criterion distinct from category explicitness as the defining property of a categorical representation. A representation is “categorical” when it affords <italic>better</italic> category discriminability than any feature set that can be learned without category supervision, i.e. when it is <italic>designed</italic> to emphasize categorical divisions. A categorical representation in this sense can be interpreted as serving the function to emphasize behaviorally relevant categorical divisions or semantic dimensions.</p>
<p>A category is a discrete semantic variable. A semantic representation could also include continuous variables that describe visual objects. Categorical clusters in the representational space do not require discrete categorical variables. A sufficient prevalence of continuous semantic variables that are correlated with a given categorical division could also produce categorical clusters. Future studies should investigate in greater detail whether the semantic component of the IT representation is better accounted for by categorical or continuous semantic dimensions.</p>
</sec><sec id="s4b">
<title>The IT representation appears to be both visual and semantic</title>
<p>Several studies suggested that the IT representation is not purely visual but also semantic <xref ref-type="bibr" rid="pcbi.1003915-Kriegeskorte1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Connolly1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Huth1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Carlson2">[81]</xref>. Our study provides additional support for this claim by showing that IT exhibits significantly stronger category clustering than a wide range of unsupervised models. It is impossible to prove that no visual feature model built without category-label supervision can explain the IT representation. However, our current interpretation is that IT reflects knowledge of category boundaries or semantic dimensions, and is thus not purely visual.</p>
<p>This finding may appear to contradict a previous study suggesting that the IT representation is better accounted for by visual shape than by semantic category <xref ref-type="bibr" rid="pcbi.1003915-Baldassi1">[23]</xref>. Note, however, that the representation of visual shape in IT is uncontroversial. A better account on the basis of visual shape does not preclude an additional semantic component. There is clearly a continuum between visual and semantic, between the representation of the appearance and the representation of the behavioral significance of an object. Our working hypothesis is that the function of the primate ventral stream is to achieve this transformation. Intermediate-level features detecting parts of objects (e.g. eyes, noses, ears) might provide a stepping stone toward semantics and could lead to clustering of faces and animates <xref ref-type="bibr" rid="pcbi.1003915-Devereux1">[82]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Clarke1">[83]</xref>.</p>
<p>Recognition requires abstracting from several sources of within-category variation among object images. One source of variation lies in the accidental properties <xref ref-type="bibr" rid="pcbi.1003915-Biederman1">[84]</xref> of the appearance of the object, such as its pose, distance, and lighting. Another source of within-category variation are the substantial differences between exemplars. In our study, the winning model was supervised with category-labeled images, learning to abstract from both of these sources of variation. It would be interesting to investigate whether training a representation to abstract from accidental properties only with exemplar-label supervision (where multiple images of the same particular object have the same exemplar label) can also produce a representation similar to IT. To our knowledge, however, the previous studies <xref ref-type="bibr" rid="pcbi.1003915-Yamins1">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1003915-Cadieu1">[76]</xref> that investigated accidental property variation in greater detail also required category-label supervision to derive representational geometries resembling that of IT.</p>
</sec><sec id="s4c">
<title>How do biological brains acquire categorical divisions?</title>
<p>In this study, we were looking to discover a model of the mechanism of biological object vision. We did not attempt to model the developmental process that builds that mechanism. Creating a viable model of IT appeared to require supervised learning. How might biological development implement this process? Biologically plausible implementations of backpropagation and related rules for supervised learning have been proposed (e.g. <xref ref-type="bibr" rid="pcbi.1003915-Stork1">[85]</xref>). However, it is unclear what supervision signal such a process would use. What is the equivalent of the category labels in the biological development of the IT representation? One possibility is that the perceptual and behavioral context provides the equivalent of the supervision signal in natural development.</p>
<p>For example, visual images appearing in the same temporal context will often represent the same object in different retinal positions, poses, distances, and sizes. It has been argued that invariance to accidental properties can be learned from temporal proximity in natural experience <xref ref-type="bibr" rid="pcbi.1003915-Fldik1">[86]</xref>–<xref ref-type="bibr" rid="pcbi.1003915-Li2">[88]</xref>. Different visual images in the same temporal context will also tend to represent the same scene. A biological learning mechanism that associates visual inputs that tend to co-occur with similar representational patterns would learn features that are more stable across time, abstracting from rapidly changing aspects of visual appearance. Moreover, objects present in a given scene might tend to be semantically related. Such a mechanism might therefore even learn semantic features.</p>
<p>Another way that context might provide a stepping stone toward a semantic representation is through perceptual channels beyond the current retinal image. Natural perception provides a rich multimodal and dynamic stream of information. Distinct visual patterns associated with similar context percepts might come to be represented together in the representational space. For example, visual motion is associated with animacy <xref ref-type="bibr" rid="pcbi.1003915-Schultz1">[89]</xref>, so dissimilar shapes associated with the same visual motion patterns might come to be co-located in the representational space.</p>
<p>The argument from context can be extended to other sensory modalities (e.g. the same sound associated with two distinct visual stimuli), and to behavioral and social context, which might contain signals correlated with the categories of the objects present in the scene <xref ref-type="bibr" rid="pcbi.1003915-Riesenhuber2">[90]</xref>. Visually dissimilar stimuli may be associated with the same linguistic utterances of contemporaries, or with the same physical actions <xref ref-type="bibr" rid="pcbi.1003915-Mahon1">[91]</xref> or emotional states. Finally, the cognitive context, including conscious inferences based on our perception of the current scene and behavioral goals, might influence the development of the IT representation through feedback signals from frontal regions that provide an endogenous context to natural visual experience.</p>
<p>An unsupervised learning process that receives such context signals alongside the visual input would be expected to cluster percepts that are similar in this more complex multimodal input space. The resulting representational clusters might then persist when the context is removed from the input and only static visual shapes are presented, as in our experiments. The argument from context illustrates how the distinction between supervised and unsupervised learning, which is clearly defined in computer science, is blurred for biological brains. Unsupervised learning from a richly contextualized sensory input might achieve a result similar to that of supervised learning.</p>
</sec><sec id="s4d">
<title>Explaining the IT representation requires considering what it is for</title>
<p>The ultimate purpose of vision is not to provide a veridical representation of our visual environment, but to support successful behavior. An explanation of the IT representation, then, requires consideration of behavioral affordances. It appears plausible that any primate faced with an unknown object might want to determine whether it is animate with high priority. Similarly, faces are important to recognize because they confer a host of information that renders animates somewhat more predictable. In computational modelling, such behavioral affordances can be brought in by optimizing the representations for particular categorization tasks, using supervised training. Such task-specific performance optimization appears essential to explaining IT. Models with higher recognition accuracy better explained not only the categorical clusters, but also the within-category representational geometries observed in IT.</p>
<p>Our results suggest that the IT representation is visuo-semantic. Explaining IT requires consideration of the perceptual and cognitive context and of behavioral affordances. Through phylo- and ontogenesis, IT appears to have learned to emphasize certain behaviorally important divisions that transcend visual appearance and relate to the meaning of objects in the context of the organism's survival and reproduction.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003915.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>The not-strongly-supervised models best explaining EVC (A), FFA (B), LOC (C), and PPA (D).</bold> This figure shows the most correlated model RDMs (from left to right and top to bottom) with the EVC (A), FFA (B), LOC (C) and PPA (D) RDMs. Biologically motivated models are set in black font, and computer-vision models are set in gray font. Models with the subscript ‘UT’ are unsupervised trained models; and others without a subscript are untrained models. The number below each RDM is the Kendall τ<sub>A</sub> correlation coefficient between the model RDM and the respective brain RDM. All correlations are statistically significant, except those that are shown by ‘ns’. Correlation p-values are reported in <xref ref-type="table" rid="pcbi-1003915-t001">Table 1</xref>.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Kendall's τ<sub>A</sub> RDM correlation of the not-strongly-supervised models with EVC (A), FFA (B), LOC (C), and PPA (D).</bold> The bars shows the Kendall's <italic>τ</italic><sub>A</sub> RDM correlation between the not-strongly-supervised model RDMs and EVC (A), FFA (B), LOC (C) and PPA (D). The error bars are standard errors of the mean estimated by bootstrap resampling. Asterisks across the x-axis show the p-values obtained by a random permutation test based on 10,000 randomizations of the condition labels (ns: not significant, p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). These p-values assess the relatedness of different model RDMs with a brain RDM. The grey horizontal rectangle shows the noise ceiling.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Kendall's τ<sub>A</sub> RDM correlation of the deep convolutional network with EVC (A), FFA (B), LOC (C), and PPA (D).</bold> The bars show the Kendall-τ<sub>A</sub> RDM correlations between the layers of the deep supervised convolutional network and EVC (A), FFA (B), LOC (C) and PPA (D). The error bars are standard errors of the mean estimated by bootstrap resampling. Asterisks across the x-axis show the p-values obtained by a random permutation test based on 10,000 randomizations of the condition labels (ns: not significant, p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). The grey horizontal rectangles show the noise ceiling in each of the brain ROIs. The upper and lower edges of the gray horizontal bar are upper and lower bound estimates of the maximum correlation any model can achieve given the noise.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s004" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Different combinations of the not-strongly-supervised models.</bold> Each of the first four RDMs (A, B, C, D) was calculated by combining internal representation of object-vision models for all images and then measuring the pairwise dissimilarity between the combined feature vectors. E and F are categorical model RDMs; F shows animate-inanimate category structure, and E comes with extra information about the within-animate category structure (i.e. face clusters). Underneath each RDM, the Kendall-τ<sub>A</sub> correlations of that RDM with hIT and mIT RDMs are stated. The statistical significance of correlations are shown by asterisks (p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). To estimate significance, randomization test was used.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s005" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>Ten category RDMs used as linear predictors in the RDM model.</bold> These ten category models and a confound mean (all-1) RDM were linearly combined to explain each of the brain and model RDMs (<xref ref-type="fig" rid="pcbi-1003915-g003">Figures 3</xref>, <xref ref-type="fig" rid="pcbi-1003915-g004">4</xref>).</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s006" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s006" position="float" xlink:type="simple"><label>Figure S6</label><caption>
<p><bold>Clustering strength for different categories in IT and not-strongly-supervised models.</bold> We measured the strength of clustering for each of the categories (animate, inanimate, face, human face, non-human face, body, human body, non-human body, natural inanimates, and artificial inanimates), by least-squares fitting of a set of category cluster RDMs (shown in <xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>) to each brain and computational-model RDM. Bars in this figure show the fitted coefficients (clustering strengths). The higher the bar, the more tightly clustered are the objects in that category. Error bars show 95% confidence interval of the coefficient estimates. Significance is shown by red (legend) corrected for 30 * 10 multiple comparisons. Standard errors and p values are based on bootstrapping of the stimulus set.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s007" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s007" position="float" xlink:type="simple"><label>Figure S7</label><caption>
<p><bold>Category-clustering strengths of not-strongly-supervised models relative to hIT.</bold> For each of the categories (animate, inanimate, face, human face, non-human face, body, human body, non-human body, natural inanimates, and artificial inanimates) the difference in clustering strength between the models and hIT was measured. Bars show the difference in clustering strength between the models and hIT. Model clustering strengths that were significantly lower/higher than the hIT clustering strength are shown by blue/red bars (legend). Error bars show 95% confidence interval of the difference in clustering strength estimates between the models and hIT. P values are based on bootstrapping of the stimulus set.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s008" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s008" position="float" xlink:type="simple"><label>Figure S8</label><caption>
<p><bold>Category-clustering strengths of not-strongly-supervised models relative to mIT.</bold> For each of the categories (animate, inanimate, face, human face, non-human face, body, human body, non-human body, natural inanimates, and artificial inanimates) the difference in clustering strength between the models and mIT was measured. Bars show the difference in clustering strength between the models and mIT. Model clustering strengths that were significantly lower/higher than the mIT clustering strength are shown by blue/red bars (legend). Error bars show 95% confidence interval of the difference in clustering strength estimates between the models and mIT. P values are based on bootstrapping of the stimulus set.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s009" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s009" position="float" xlink:type="simple"><label>Figure S9</label><caption>
<p><bold>Categoricality in noise-less models compared with the categoricality in IT.</bold> Bars show categoricality (measured by the category clustering index, CCI) for each of the not-strongly-supervised models.The category clustering index (CCI) for each model and brain RDM is defined as the proportion of RDM variance explained by the category cluster model (<xref ref-type="supplementary-material" rid="pcbi.1003915.s005">Figure S5</xref>), i.e. the squared correlation between the fitted category-cluster model and the RDM it is fitted to. Error bars and shaded regions indicate 95%-confidence intervals. Significant CCIs are indicated by stars underneath the bars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). Significant differences between the CCI of each model and the hIT/mIT CCI are indicated by blue/gray vertical arrows (p&lt;0.05, Bonferroni-adjusted for 28 tests). The corresponding inferential comparisons for mIT are indicated by gray vertical arrows. The categoricality in hIT is significantly higher than in any of the 28 not-strongly-supervised models. This analysis is based on the noise-less model representations.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s010" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s010" position="float" xlink:type="simple"><label>Figure S10</label><caption>
<p><bold>Categoricality in the noise-less representations of the deep convolutional network compared with hIT and mIT.</bold> Bars show categoricality (measured by the category clustering index, CCI) for each layer of the deep convolutional network and for the IT-geometry-supervised layer. For conventions and for definition of the CCI, see <xref ref-type="supplementary-material" rid="pcbi.1003915.s009">Figure S9</xref>. Error bars and shaded regions indicate 95%-confidence intervals. Significant CCIs are indicated by stars underneath the bars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001). Significant differences between the CCI of each model and the hIT/mIT CCI are indicated by blue/gray vertical arrows (p&lt;0.05, Bonferroni-adjusted for 9 tests). The corresponding inferential comparisons for mIT are indicated by gray vertical arrows. Categoricality is significantly greater in hIT and mIT than in any of the internal layers of the deep convolutional network. However, the IT-geometry-supervised layer (remixed and reweighted) achieves a categoricality similar to IT. This analysis is based on the noise-less model representations.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s011" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s011" position="float" xlink:type="simple"><label>Figure S11</label><caption>
<p><bold>Categorization accuracy of all models for natural/artificial (A) and face/body (B).</bold> Each dark blue bar shows the categorization accuracy of a linear SVM applied to one of the computational model representations. Categorization accuracy for each model was estimated by 12-fold crossvalidation on the 96 stimuli. To assess whether categorization accuracy was above chance level, we performed a permutation test, in which we retrained the SVMs on (category-orthogonalized) 10,000 random dichotomies among the stimuli. Light blue bars show the average model categorization accuracy for random label permutations. Categorization performance was significantly greater than chance for most models (ns: not significant, * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001, **** p&lt;0.0001).</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s012" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s012" position="float" xlink:type="simple"><label>Figure S12</label><caption>
<p><bold>Kendall's </bold><bold><italic>τ</italic></bold><bold><sub>A</sub> RDM correlation of the not-strongly-supervised models with the hIT animate (A) and inanimate (B) sub-clusters.</bold> The bars show the Kendall-τ<sub>A</sub> RDM correlations of the not-strongly-supervised models with the hIT RDM for animate images (A), and inanimate images (B). The error bars are standard deviations of the mean estimated by bootstrap resampling. Asterisks across the x-axis show the p-values obtained by a random permutation test based on 10,000 randomizations of the condition labels (ns: not significant, p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). The p-values assess the relatedness of different model RDMs with a brain RDM. The grey horizontal rectangles show the noise ceiling. Models with the subscript ‘UT’ are unsupervised trained models, models with the subscript ‘ST’ are supervised trained models, and others without a subscript are untrained models.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s013" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s013" position="float" xlink:type="simple"><label>Figure S13</label><caption>
<p><bold>Kendall's </bold><bold><italic>τ</italic></bold><bold><sub>A</sub> RDM correlation of the not-strongly-supervised models with the mIT animate (A) and inanimate (B) sub-clusters.</bold> The bars show the Kendall-τ<sub>A</sub> RDM correlations of the not-strongly-supervised models with the mIT RDM for animate images (A), and inanimate images (B). The error bars are standard deviations of the mean estimated by bootstrap resampling. Asterisks across the x-axis show the p-values obtained by a random permutation test based on 10,000 randomizations of the condition labels (ns: not significant, p&lt;0.05: *, p&lt;0.01: **, p&lt;0.001: ***, p&lt;0.0001: ****). The p-values assess the relatedness of different model RDMs with a brain RDM. Models with the subscript ‘UT’ are unsupervised trained models, models with the subscript ‘ST’ are supervised trained models, and others without a subscript are untrained models.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003915.s014" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1003915.s014" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p><bold>Models better explain the representation of the animate objects than the inanimate objects in IT.</bold></p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank all those who kindly shared their model implementation with us. In particular Simon Stringer, Bedeho Mender, Benjamin Evans, Masoud Ghodrati, Karim Rajaei, Pavel Sountsov, and John Lisman who kindly helped us to set up their code.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003915-Desimone1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Albright</surname><given-names>TD</given-names></name>, <name name-style="western"><surname>Gross</surname><given-names>CG</given-names></name>, <name name-style="western"><surname>Bruce</surname><given-names>C</given-names></name> (<year>1984</year>) <article-title>Stimulus-selective properties of inferior temporal neurons in the macaque</article-title>. <source>J Neurosci</source> <volume>4</volume>: <fpage>2051</fpage>–<lpage>2062</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Gross1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname><given-names>CG</given-names></name> (<year>1994</year>) <article-title>How Inferior Temporal Cortex Became a Visual Area</article-title>. <source>Cereb Cortex</source> <volume>4</volume>: <fpage>455</fpage>–<lpage>469</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/4.5.455" xlink:type="simple">10.1093/cercor/4.5.455</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Tanaka1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name> (<year>1996</year>) <article-title>Inferotemporal Cortex and Object Vision</article-title>. <source>Annu Rev Neurosci</source> <volume>19</volume>: <fpage>109</fpage>–<lpage>139</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.ne.19.030196.000545" xlink:type="simple">10.1146/annurev.ne.19.030196.000545</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Hung1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hung</surname><given-names>CP</given-names></name>, <name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2005</year>) <article-title>Fast Readout of Object Identity from Macaque Inferior Temporal Cortex</article-title>. <source>Science</source> <volume>310</volume>: <fpage>863</fpage>–<lpage>866</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1117593" xlink:type="simple">10.1126/science.1117593</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Zoccolan1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kouh</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2007</year>) <article-title>Trade-Off between Object Selectivity and Tolerance in Monkey Inferotemporal Cortex</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>12292</fpage>–<lpage>12307</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1897-07.2007" xlink:type="simple">10.1523/JNEUROSCI.1897-07.2007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Kiani1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Esteky</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Mirpour</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Object Category Structure in Response Patterns of Neuronal Population in Monkey Inferior Temporal Cortex</article-title>. <source>J Neurophysiol</source> <volume>97</volume>: <fpage>4296</fpage>–<lpage>4309</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00024.2007" xlink:type="simple">10.1152/jn.00024.2007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Kriegeskorte1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title>. <source>Neuron</source> <volume>60</volume>: <fpage>1126</fpage>–<lpage>1141</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.10.043" xlink:type="simple">10.1016/j.neuron.2008.10.043</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Sato1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Uchida</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Tanifuji</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Cortical Columnar Organization Is Reconsidered in Inferior Temporal Cortex</article-title>. <source>Cereb Cortex</source> <volume>19</volume>: <fpage>1870</fpage>–<lpage>1888</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhn218" xlink:type="simple">10.1093/cercor/bhn218</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Bell1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname><given-names>AH</given-names></name>, <name name-style="western"><surname>Hadj-Bouziane</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Frihauf</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Tootell</surname><given-names>RBH</given-names></name>, <name name-style="western"><surname>Ungerleider</surname><given-names>LG</given-names></name> (<year>2009</year>) <article-title>Object Representations in the Temporal Cortex of Monkeys and Humans as Revealed by Functional Magnetic Resonance Imaging</article-title>. <source>J Neurophysiol</source> <volume>101</volume>: <fpage>688</fpage>–<lpage>700</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.90657.2008" xlink:type="simple">10.1152/jn.90657.2008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Mur1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bodurka</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name> (<year>2013</year>) <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>. <source>Front Psychol</source> <volume>4</volume>: <fpage>128</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2013.00128" xlink:type="simple">10.3389/fpsyg.2013.00128</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Carlson1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlson</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tovar</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Alink</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name> (<year>2013</year>) <article-title>Representational dynamics of object vision: The first 1000 ms</article-title>. <source>J Vis</source> <volume>13</volume>: <fpage>1</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/13.10.1" xlink:type="simple">10.1167/13.10.1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Cichy1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2014</year>) <article-title>Resolving human object recognition in space and time</article-title>. <source>Nat Neurosci</source> <volume>17</volume>: <fpage>455</fpage>–<lpage>462</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3635" xlink:type="simple">10.1038/nn.3635</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Majaj1"><label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Majaj N, Hong H, Solomon E, DiCarlo J (2012) A unified neuronal population code fully explains human object recognition Cosyne 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Connolly1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Connolly</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Guntupalli</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Gors</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hanke</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Halchenko</surname><given-names>YO</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>The Representation of Biological Classes in the Human Brain</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>2608</fpage>–<lpage>2618</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5547-11.2012" xlink:type="simple">10.1523/JNEUROSCI.5547-11.2012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Naselaris1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Stansbury</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>n.d.</year>) <article-title>Cortical representation of animate and inanimate objects in complex natural scenes</article-title>. <source>Journal of Physiology-Paris</source> Available: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S092842571200006X" xlink:type="simple">http://www.sciencedirect.com/science/article/pii/S092842571200006X</ext-link>. Accessed 20 September 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Mur2"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Bodurka</surname><given-names>J</given-names></name>, <name name-style="western"><surname>De Weerd</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>PA</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Categorical, Yet Graded – Single-Image Activation Profiles of Human Category-Selective Cortical Regions</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>8649</fpage>–<lpage>8662</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2334-11.2012" xlink:type="simple">10.1523/JNEUROSCI.2334-11.2012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Kriegeskorte2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name> (<year>2009</year>) <article-title>Relating population-code representations between man, monkey, and computational models</article-title>. <source>Front Neurosci</source> <volume>3</volume>: <fpage>363</fpage>–<lpage>73</lpage> Available: <ext-link ext-link-type="uri" xlink:href="http://www.frontiersin.org/neuroscience/10.3389/neuro.01.035.2009/pdf/full" xlink:type="simple">http://www.frontiersin.org/neuroscience/10.3389/neuro.01.035.2009/pdf/full</ext-link>. Accessed 18 January 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Kriegeskorte3"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source> <volume>2</volume>: <fpage>4</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.06.004.2008" xlink:type="simple">10.3389/neuro.06.004.2008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Leeds1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leeds</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Seibert</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Pyles</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Tarr</surname><given-names>MJ</given-names></name> (<year>2013</year>) <article-title>Comparing visual representations across human fMRI and computational vision</article-title>. <source>J Vis</source> <volume>13</volume>: <fpage>25</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/13.13.25" xlink:type="simple">10.1167/13.13.25</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Serre1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2007</year>) <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>104</volume>: <fpage>6424</fpage>–<lpage>6429</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0700622104" xlink:type="simple">10.1073/pnas.0700622104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Riesenhuber1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>nature neuroscience</source> <volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Huth1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huth</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Nishimoto</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vu</surname><given-names>AT</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2012</year>) <article-title>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</article-title>. <source>Neuron</source> <volume>76</volume>: <fpage>1210</fpage>–<lpage>1224</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.10.014" xlink:type="simple">10.1016/j.neuron.2012.10.014</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Baldassi1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baldassi</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Alemi-Neissi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Pagan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Zecchina</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Shape Similarity, Better than Semantic Membership, Accounts for the Structure of Visual Object Representations in a Population of Monkey Inferotemporal Neurons</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>: <fpage>e1003167</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003167" xlink:type="simple">10.1371/journal.pcbi.1003167</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-KhalighRazavi1"><label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Khaligh-Razavi S-M (2014) What you need to know about the state-of-the-art computational models of object-vision: A tour through the models. arXiv:14072776 [cs, q-bio]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1407.2776" xlink:type="simple">http://arxiv.org/abs/1407.2776</ext-link>. Accessed 11 July 2014.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Pillow1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source> <volume>454</volume>: <fpage>995</fpage>–<lpage>999</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Mitchell1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>Shinkareva</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Carlson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>K-M</given-names></name>, <name name-style="western"><surname>Malave</surname><given-names>VL</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Predicting Human Brain Activity Associated with the Meanings of Nouns</article-title>. <source>Science</source> <volume>320</volume>: <fpage>1191</fpage>–<lpage>1195</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1152876" xlink:type="simple">10.1126/science.1152876</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Kay1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kay</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>Naselaris</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2008</year>) <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source> <volume>452</volume>: <fpage>352</fpage>–<lpage>355</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature06713" xlink:type="simple">10.1038/nature06713</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Dumoulin1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dumoulin</surname><given-names>SO</given-names></name>, <name name-style="western"><surname>Wandell</surname><given-names>BA</given-names></name> (<year>2008</year>) <article-title>Population receptive field estimates in human visual cortex</article-title>. <source>NeuroImage</source> <volume>39</volume>: <fpage>647</fpage>–<lpage>660</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2007.09.034" xlink:type="simple">10.1016/j.neuroimage.2007.09.034</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Kriegeskorte4"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Kievit</surname><given-names>RA</given-names></name> (<year>2013</year>) <article-title>Representational geometry: integrating cognition, computation, and the brain</article-title>. <source>Trends in Cognitive Sciences</source> <volume>17</volume>: <fpage>401</fpage>–<lpage>412</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2013.06.007" xlink:type="simple">10.1016/j.tics.2013.06.007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Nili1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nili</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Wingfield</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Walther</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Su</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname><given-names>W</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>A Toolbox for Representational Similarity Analysis</article-title>. <source>PLoS Comput Biol</source> <volume>10</volume>: <fpage>e1003553</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003553" xlink:type="simple">10.1371/journal.pcbi.1003553</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Ganguli1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguli</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2012</year>) <article-title>Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis</article-title>. <source>Annu Rev Neurosci</source> <volume>35</volume>: <fpage>485</fpage>–<lpage>508</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-062111-150410" xlink:type="simple">10.1146/annurev-neuro-062111-150410</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Johnson1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname><given-names>WB</given-names></name>, <name name-style="western"><surname>Lindenstrauss</surname><given-names>J</given-names></name> (<year>1984</year>) <article-title>Extensions of Lipschitz mappings into a Hilbert space</article-title>. <source>Contemporary mathematics</source> <volume>26</volume>: <fpage>1</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Mutch1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mutch</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lowe</surname><given-names>DG</given-names></name> (<year>2008</year>) <article-title>Object Class Recognition and Localization Using Sparse Features with Limited Receptive Fields</article-title>. <source>International Journal of Computer Vision</source> <volume>80</volume>: <fpage>45</fpage>–<lpage>57</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11263-007-0118-0" xlink:type="simple">10.1007/s11263-007-0118-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Wallis1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>1997</year>) <article-title>A model of invariant object recognition in the visual system</article-title>. <source>Prog Neurobiol</source> <volume>51</volume>: <fpage>167</fpage>–<lpage>194</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Ghodrati1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghodrati</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rajaei</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Pooyan</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>How Can Selection of Biologically Inspired Features Improve the Performance of a Robust Object Recognition Model?</article-title> <source>PLoS ONE</source> <volume>7</volume>: <fpage>e32357</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0032357" xlink:type="simple">10.1371/journal.pone.0032357</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Rajaei1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rajaei</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name>, <name name-style="western"><surname>Ghodrati</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Shiri Ahmad Abadi</surname><given-names>ME</given-names></name> (<year>2012</year>) <article-title>A Stable Biologically Motivated Learning Mechanism for Visual Feature Extraction to Handle Facial Categorization</article-title>. <source>PLoS ONE</source> <volume>7</volume>: <fpage>e38478</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0038478" xlink:type="simple">10.1371/journal.pone.0038478</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Ghodrati2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghodrati</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Farzmahdi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rajaei</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name> (<year>2014</year>) <article-title>Feedforward Object-Vision Models Only Tolerate Small Image Variations Compared to Human</article-title>. <source>Frontiers in Computational Neuroscience</source> <volume>8</volume> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2014.00074" xlink:type="simple">10.3389/fncom.2014.00074</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Sountsov1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sountsov</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Santucci</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Lisman</surname><given-names>JE</given-names></name> (<year>2011</year>) <article-title>A biologically plausible transform for visual recognition that is invariant to translation, scale, and rotation</article-title>. <source>Frontiers in computational neuroscience</source> <volume>5</volume>: <fpage>53</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Jarrett1"><label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Jarrett K, Kavukcuoglu K, Ranzato MA, LeCun Y (2009) What is the best multi-stage architecture for object recognition? Computer Vision, 2009 IEEE 12th International Conference on. pp. 2146–2153.</mixed-citation>
</ref>
<ref id="pcbi.1003915-LeCun1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name> (<year>1995</year>) <article-title>Convolutional networks for images, speech, and time series</article-title>. <source>The handbook of brain theory and neural networks</source> <volume>3361</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Krizhevsky1"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet Classification with Deep Convolutional Neural Networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ, editors. Advances in Neural Information Processing Systems 25. Curran Associates, Inc. pp. 1097–1105.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Oliva1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</article-title>. <source>International Journal of Computer Vision</source> <volume>42</volume>: <fpage>145</fpage>–<lpage>175</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Lowe1"><label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Lowe DG (1999) Object recognition from local scale-invariant features. iccv. p. 1150.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Lazebnik1"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Lazebnik S, Schmid C, Ponce J (2006) Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. Vol. 2. pp. 2169–2178. doi:10.1109/CVPR.2006.68.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Bosch1"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Bosch A, Zisserman A, Munoz X (2007) Representing shape with a spatial pyramid kernel. Proceedings of the 6th ACM international conference on Image and video retrieval. CIVR '07. New York, NY, USA: ACM. pp. 401–408. Available: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/1282280.1282340" xlink:type="simple">http://doi.acm.org/10.1145/1282280.1282340</ext-link>. Accessed 6 April 2012.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Shechtman1"><label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Shechtman E, Irani M (2007) Matching Local Self-Similarities across Images and Videos. IEEE Conference on Computer Vision and Pattern Recognition, 2007. CVPR '07. pp. 1–8. doi:10.1109/CVPR.2007.383198.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Deselaers1"><label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Deselaers T, Ferrari V (2010) Global and efficient self-similarity for object classification and detection. Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. pp. 1633–1640. doi:10.1109/CVPR.2010.5539775.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Ojala1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ojala</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pietikäinen</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mäenpää</surname><given-names>T</given-names></name> (<year>2001</year>) <article-title>A generalized local binary pattern operator for multiresolution gray scale and rotation invariant texture classification</article-title>. <source>Advances in Pattern Recognition—ICAPR</source> <volume>2001</volume>: <fpage>399</fpage>–<lpage>408</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Deng1"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L-J, Li K, et al. (2009) ImageNet: A large-scale hierarchical image database. IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009. pp. 248–255. doi:10.1109/CVPR.2009.5206848.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Efron1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Efron</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name> (<year>1986</year>) <article-title>Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy</article-title>. <source>Statist Sci</source> <volume>1</volume>: <fpage>54</fpage>–<lpage>75</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/ss/1177013815" xlink:type="simple">10.1214/ss/1177013815</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Lawson1"><label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Lawson CL, Hanson RJ (1974) Solving least squares problems. Englewood Cliffs, NJ: Prentice-hall. Vol. 161.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Krizhevsky2"><label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A., Sutskever I. and Hinton, G E. (2012) ImageNet Classification with Deep Convolutional Neural Networks. NIPS. Lake Tahoe, Nevada.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Konkle1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Konkle</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>A Real-World Size Organization of Object Responses in Occipitotemporal Cortex</article-title>. <source>Neuron</source> <volume>74</volume>: <fpage>1114</fpage>–<lpage>1124</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.04.036" xlink:type="simple">10.1016/j.neuron.2012.04.036</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Donahue1"><label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Donahue J, Jia Y, Vinyals O, Hoffman J, Zhang N, et al. (2013) DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. arXiv:13101531 [cs]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1310.1531" xlink:type="simple">http://arxiv.org/abs/1310.1531</ext-link>. Accessed 7 May 2014.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Oliva2"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Oliva A, Torralba A (2006) Building the gist of a scene: the role of global image features in recognition. Progress in Brain Research. p. 2006.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Belongie1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belongie</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Malik</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Puzicha</surname><given-names>J</given-names></name> (<year>2002</year>) <article-title>Shape matching and object recognition using shape contexts</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <fpage>509</fpage>–<lpage>522</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Berg1"><label>57</label>
<mixed-citation publication-type="other" xlink:type="simple">Berg AC, Berg TL, Malik J (2005) Shape matching and object recognition using low distortion correspondences. Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Vol. 1. pp. 26–33.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Zhang1"><label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhang H, Berg AC, Maire M, Malik J (2006) SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition. Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. Vol. 2. pp. 2126–2136. doi:10.1109/CVPR.2006.301.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Vedaldi1"><label>59</label>
<mixed-citation publication-type="other" xlink:type="simple">Vedaldi A, Gulshan V, Varma M, Zisserman A (2009) Multiple kernels for object detection. Computer Vision, 2009 IEEE 12th International Conference on. pp. 606–613. doi:10.1109/ICCV.2009.5459183.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Lowe2"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lowe</surname><given-names>DG</given-names></name> (<year>2004</year>) <article-title>Distinctive Image Features from Scale-Invariant Keypoints</article-title>. <source>Int J Comput Vision</source> <volume>60</volume>: <fpage>91</fpage>–<lpage>110</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94" xlink:type="simple">10.1023/B:VISI.0000029664.99615.94</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Tromans1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tromans</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stringer</surname><given-names>SM</given-names></name> (<year>2011</year>) <article-title>A Computational Model of the Development of Separate Representations of Facial Identity and Expression in the Primate Visual System</article-title>. <source>PLoS ONE</source> <volume>6</volume>: <fpage>e25616</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0025616" xlink:type="simple">10.1371/journal.pone.0025616</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Stringer1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stringer</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Tromans</surname><given-names>JM</given-names></name> (<year>2007</year>) <article-title>Invariant object recognition with trace learning and multiple stimuli present during training</article-title>. <source>Network</source> <volume>18</volume>: <fpage>161</fpage>–<lpage>187</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/09548980701556055" xlink:type="simple">10.1080/09548980701556055</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Chatfield1"><label>63</label>
<mixed-citation publication-type="other" xlink:type="simple">Chatfield K, Philbin J, Zisserman A (2009) Efficient retrieval of deformable shape classes using local self-similarities. 2009 IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops). pp. 264–271. doi:10.1109/ICCVW.2009.5457691.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Ojala2"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ojala</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pietikainen</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Maenpaa</surname><given-names>T</given-names></name> (<year>2002</year>) <article-title>Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source> <volume>24</volume>: <fpage>971</fpage>–<lpage>987</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Pietikinen1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>PietikÃ¤inen</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Local Binary Patterns</article-title>. <source>Scholarpedia</source> <volume>5</volume>: <fpage>9775</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.4249/scholarpedia.9775" xlink:type="simple">10.4249/scholarpedia.9775</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-HUBEL1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>HUBEL</surname><given-names>D</given-names></name>, <name name-style="western"><surname>WIESEL</surname><given-names>T</given-names></name> (<year>1962</year>) <article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title>. <source>The Journal of physiology</source> <volume>160</volume>: <fpage>106</fpage>–<lpage>154</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Hubel1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1968</year>) <article-title>Receptive fields and functional architecture of monkey striate cortex</article-title>. <source>The Journal of Physiology</source> <volume>195</volume>: <fpage>215</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Zabbah1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zabbah</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Rajaei</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Mirzaei</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name> (<year>2014</year>) <article-title>The impact of the lateral geniculate nucleus and corticogeniculate interactions on efficient coding and higher-order visual object processing</article-title>. <source>Vision Research</source> <volume>101</volume>: <fpage>82</fpage>–<lpage>93</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2014.05.006" xlink:type="simple">10.1016/j.visres.2014.05.006</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Grossberg1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1988</year>) <article-title>Adaptive pattern classification and universal recoding</article-title>. <source>I.: parallel development and coding of neural feature detectors</source> <fpage>243</fpage>–<lpage>258</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Chang1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname><given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>C-J</given-names></name> (<year>2011</year>) <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Trans Intell Syst Technol</source> <volume>2</volume>: <fpage>27:1</fpage>–<lpage>27:27</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/1961189.1961199" xlink:type="simple">10.1145/1961189.1961199</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Hinton1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Osindero</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Teh</surname><given-names>Y-W</given-names></name> (<year>2006</year>) <article-title>A fast learning algorithm for deep belief nets</article-title>. <source>Neural computation</source> <volume>18</volume>: <fpage>1527</fpage>–<lpage>1554</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Bengio1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Learning Deep Architectures for AI</article-title>. <source>Found Trends Mach Learn</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>127</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1561/2200000006" xlink:type="simple">10.1561/2200000006</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Zeiler1"><label>73</label>
<mixed-citation publication-type="other" xlink:type="simple">Zeiler MD, Fergus R (2013) Visualizing and Understanding Convolutional Networks. arXiv:13112901 [cs]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1311.2901" xlink:type="simple">http://arxiv.org/abs/1311.2901</ext-link>. Accessed 26 March 2014.</mixed-citation>
</ref>
<ref id="pcbi.1003915-KhalighRazavi2"><label>74</label>
<mixed-citation publication-type="other" xlink:type="simple">Khaligh-Razavi S-M, Kriegeskorte N (2013) Object-vision models that better explain IT also categorize better, but all models fail at both. Cosyne Abstracts, Salt Lake City USA.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Yamins1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname><given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname><given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>111</volume>: <fpage>8619</fpage>–<lpage>8624</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Cadieu1"><label>76</label>
<mixed-citation publication-type="other" xlink:type="simple">Cadieu CF, Hong H, Yamins DLK, Pinto N, Ardila D, et al. (2014) Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition. arXiv:14063284 [cs, q-bio]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1406.3284" xlink:type="simple">http://arxiv.org/abs/1406.3284</ext-link>. Accessed 17 July 2014.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Yamins2"><label>77</label>
<mixed-citation publication-type="other" xlink:type="simple">Yamins DL, Hong H, Cadieu C, DiCarlo JJ (2013) Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream. In: Burges CJC, Bottou L, Welling M, Ghahramani Z, Weinberger KQ, editors. Advances in Neural Information Processing Systems 26. Curran Associates, Inc. pp. 3093–3101.</mixed-citation>
</ref>
<ref id="pcbi.1003915-DiCarlo1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name> (<year>2007</year>) <article-title>Untangling invariant object recognition</article-title>. <source>Trends in Cognitive Sciences</source> <volume>11</volume>: <fpage>333</fpage>–<lpage>341</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Rust1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Selectivity and Tolerance (“Invariance”) Both Increase as Visual Information Propagates from Cortical Area V4 to IT</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>12978</fpage>–<lpage>12995</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0179-10.2010" xlink:type="simple">10.1523/JNEUROSCI.0179-10.2010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Kriegeskorte5"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name> (<year>2011</year>) <article-title>Pattern-information analysis: from stimulus decoding to computational-model testing</article-title>. <source>Neuroimage</source> <volume>56</volume>: <fpage>411</fpage>–<lpage>421</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.01.061" xlink:type="simple">10.1016/j.neuroimage.2011.01.061</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Carlson2"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlson</surname><given-names>TA</given-names></name>, <name name-style="western"><surname>Simmons</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Slevc</surname><given-names>LR</given-names></name> (<year>2013</year>) <article-title>The Emergence of Semantic Meaning in the Ventral Temporal Pathway</article-title>. <source>Journal of Cognitive Neuroscience</source> <fpage>1</fpage>–<lpage>12</lpage> <comment>doi:__<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocna00458" xlink:type="simple">10.1162/jocn_a_00458</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Devereux1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Devereux</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Clarke</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Marouchos</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Tyler</surname><given-names>LK</given-names></name> (<year>2013</year>) <article-title>Representational Similarity Analysis Reveals Commonalities and Differences in the Semantic Processing of Words and Objects</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>18906</fpage>–<lpage>18916</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3809-13.2013" xlink:type="simple">10.1523/JNEUROSCI.3809-13.2013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Clarke1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Tyler</surname><given-names>LK</given-names></name> (<year>2014</year>) <article-title>Object-specific semantic coding in human perirhinal cortex</article-title>. <source>J Neurosci</source> <volume>34</volume> (<issue>14</issue>) <fpage>4766</fpage>–<lpage>75</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Biederman1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name> (<year>1987</year>) <article-title>Recognition-by-components: A theory of human image understanding</article-title>. <source>Psychological Review</source> <volume>94</volume>: <fpage>115</fpage>–<lpage>147</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Stork1"><label>85</label>
<mixed-citation publication-type="other" xlink:type="simple">Stork DG (1989) Is backpropagation biologically plausible?, International Joint Conference on Neural Networks, 1989. IJCNN. pp. 241–246 vol. 2. doi:10.1109/IJCNN.1989.118705.</mixed-citation>
</ref>
<ref id="pcbi.1003915-Fldik1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name> (<year>1991</year>) <article-title>Learning Invariance from Transformation Sequences</article-title>. <source>Neural Computation</source> <volume>3</volume>: <fpage>194</fpage>–<lpage>200</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1991.3.2.194" xlink:type="simple">10.1162/neco.1991.3.2.194</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Li1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Unsupervised Natural Visual Experience Rapidly Reshapes Size-Invariant Object Representation in Inferior Temporal Cortex</article-title>. <source>Neuron</source> <volume>67</volume>: <fpage>1062</fpage>–<lpage>1075</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.08.029" xlink:type="simple">10.1016/j.neuron.2010.08.029</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Li2"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>N</given-names></name>, <name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name> (<year>2012</year>) <article-title>Neuronal Learning of Invariant Object Representation in the Ventral Visual Stream Is Not Dependent on Reward</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>6611</fpage>–<lpage>6620</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3786-11.2012" xlink:type="simple">10.1523/JNEUROSCI.3786-11.2012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Schultz1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>O'Doherty</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name> (<year>2005</year>) <article-title>Activation in Posterior Superior Temporal Sulcus Parallels Parameter Inducing the Percept of Animacy</article-title>. <source>Neuron</source> <volume>45</volume>: <fpage>625</fpage>–<lpage>635</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2004.12.052" xlink:type="simple">10.1016/j.neuron.2004.12.052</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Riesenhuber2"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Appearance Isn't Everything: News on Object Representation in Cortex</article-title>. <source>Neuron</source> <volume>55</volume>: <fpage>341</fpage>–<lpage>344</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.07.017" xlink:type="simple">10.1016/j.neuron.2007.07.017</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003915-Mahon1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mahon</surname><given-names>BZ</given-names></name>, <name name-style="western"><surname>Milleville</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Negri</surname><given-names>GAL</given-names></name>, <name name-style="western"><surname>Rumiati</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Caramazza</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Action-Related Properties Shape Object Representations in the Ventral Stream</article-title>. <source>Neuron</source> <volume>55</volume>: <fpage>507</fpage>–<lpage>520</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.07.011" xlink:type="simple">10.1016/j.neuron.2007.07.011</ext-link></comment></mixed-citation>
</ref>
</ref-list></back>
</article>