<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00896</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002294</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Circuit models</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons</article-title><alt-title alt-title-type="running-head">Sampling in Graphical Models with Spiking Neurons</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Pecevski</surname>
            <given-names>Dejan</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Buesing</surname>
            <given-names>Lars</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="fn" rid="fn1">
            <sup>¤</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Maass</surname>
            <given-names>Wolfgang</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Institute for Theoretical Computer Science, Graz University of Technology, Graz, Austria</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">dejan@igi.tugraz.at</email></corresp>
        <fn fn-type="current-aff" id="fn1">
          <label>¤</label>
          <p>Current address: Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: DP WM. Performed the experiments: DP. Analyzed the data: DP WM. Wrote the paper: DP WM. Contributed theory: DP LB WM.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>12</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>15</day>
        <month>12</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>12</issue><elocation-id>e1002294</elocation-id><history>
        <date date-type="received">
          <day>19</day>
          <month>6</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>20</day>
          <month>10</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Pecevski et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>An important open problem of computational neuroscience is the generic organization of computations in networks of neurons in the brain. We show here through rigorous theoretical analysis that inherent stochastic features of spiking neurons, in combination with simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of spiking neurons to carry out probabilistic inference through sampling in general graphical models. In particular, it enables them to carry out probabilistic inference in Bayesian networks with converging arrows (“explaining away”) and with undirected loops, that occur in many real-world tasks. Ubiquitous stochastic features of networks of spiking neurons, such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization. We demonstrate through computer simulations that this approach can be scaled up to neural emulations of probabilistic inference in fairly large graphical models, yielding some of the most complex computations that have been carried out so far in networks of spiking neurons.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Experimental data from neuroscience have provided substantial knowledge about the intricate structure of cortical microcircuits, but their functional role, i.e. the computational calculus that they employ in order to interpret ambiguous stimuli, produce predictions, and derive movement plans has remained largely unknown. Earlier assumptions that these circuits implement a logic-like calculus have run into problems, because logical inference has turned out to be inadequate to solve inference problems in the real world which often exhibits substantial degrees of uncertainty. In this article we propose an alternative theoretical framework for examining the functional role of precisely structured motifs of cortical microcircuits and dendritic computations in complex neurons, based on probabilistic inference through sampling. We show that these structural details endow cortical columns and areas with the capability to represent complex knowledge about their environment in the form of higher order dependencies among salient variables. We show that it also enables them to use this knowledge for probabilistic inference that is capable to deal with uncertainty in stored knowledge and current observations. We demonstrate in computer simulations that the precisely structured neuronal microcircuits enable networks of spiking neurons to solve through their inherent stochastic dynamics a variety of complex probabilistic inference tasks.</p>
      </abstract><funding-group><funding-statement>This paper was written under partial support by the European Union project FP7-243914 (BRAIN-I-NETS), project 269921 (BrainScaleS), project FP7-248311 (AMARSI) and project FP7-506778 (PASCAL2). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="25"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>We show in this article that noisy networks of spiking neurons are in principle able to carry out a quite demanding class of computations: probabilistic inference in general graphical models. More precisely, they are able to carry out probabilistic inference for arbitrary probability distributions over discrete random variables (RVs) through sampling. Spikes are viewed here as signals which inform other neurons that a certain RV has been assigned a particular value for a certain time period during the sampling process. This approach had been introduced under the name “neural sampling” in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>. This article extends the results of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, where the validity of this neural sampling process had been established for the special case of distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e001" xlink:type="simple"/></inline-formula> with at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e002" xlink:type="simple"/></inline-formula> order dependencies between RVs, to distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e003" xlink:type="simple"/></inline-formula> with dependencies of arbitrary order. Such higher order dependencies, which may cause for example the explaining away effect <xref ref-type="bibr" rid="pcbi.1002294-Pearl1">[2]</xref>, have been shown to arise in various computational tasks related to perception and reasoning. Our approach provides an alternative to other proposed neural emulations of probabilistic inference in graphical models, that rely on arithmetical methods such as belief propagation. The two approaches make completely different demands on the underlying neural circuits: the belief propagation approach emulates a deterministic arithmetical computation of probabilities, and is therefore optimally supported by noise-free deterministic networks of neurons. In contrast, our sampling based approach shows how an internal model of an arbitrary target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e004" xlink:type="simple"/></inline-formula> can be implemented by a network of stochastically firing neurons (such internal model for a distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e005" xlink:type="simple"/></inline-formula>, that reflects the statistics of natural stimuli, has been found to emerge in primary visual cortex <xref ref-type="bibr" rid="pcbi.1002294-Berkes1">[3]</xref>). This approach requires the presence of stochasticity (noise), and is inherently compatible with experimentally found phenomena such as the ubiquitous trial-to-trial variability of responses of biological networks of neurons.</p>
      <p>Given a network of spiking neurons that implements an internal model for a distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e006" xlink:type="simple"/></inline-formula>, probabilistic inference for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e007" xlink:type="simple"/></inline-formula>, for example the computation of marginal probabilities for specific RVs, can be reduced to counting the number of spikes of specific neurons for a behaviorally relevant time span of a few hundred ms, similarly as in previously proposed mechanisms for evidence accumulation in neural systems <xref ref-type="bibr" rid="pcbi.1002294-Gold1">[4]</xref>. Nevertheless, in this neural emulation of probabilistic inference through sampling, every single spike conveys information, as well as the relative timing among spikes of different neurons. The reason is that for many of the neurons in the model (the so-called principal neurons) each spike represents a tentative value for a specific RV, whose consistency with tentative values of other RVs, and with the available evidence (e.g., an external stimulus), is explored during the sampling process. In contrast, currently known neural emulations of belief propagation in general graphical models are based on firing rate coding.</p>
      <p>The underlying mathematical theory of our proposed new method provides a rigorous proof that the spiking activity in a network of neurons can in principle provide an internal model for an arbitrary distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e008" xlink:type="simple"/></inline-formula>. It builds on the general theory of Markov chains and their stationary distribution (see e.g. <xref ref-type="bibr" rid="pcbi.1002294-Grimmett1">[5]</xref>), the general theory of MCMC (Markov chain Monte Carlo) sampling (see e.g. <xref ref-type="bibr" rid="pcbi.1002294-Neal1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Andrieu1">[7]</xref>), and the theory of sampling in stochastic networks of spiking neurons - modelled by a non-reversible Markov chain <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>. It requires further theoretical analysis for elucidating under what conditions higher order factors of p can be emulated in networks of spiking neurons, which is provided in the Methods section of this article. Whereas the underlying mathematical theory only guarantees convergence of the spiking activity to the target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e009" xlink:type="simple"/></inline-formula>, it does not provide tight bounds for the convergence speed to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e010" xlink:type="simple"/></inline-formula> (the so-called burn–in time in MCMC sampling). Hence we complement our theoretical analysis by computer simulations for three Bayesian networks of increasing size and complexity. We also address in these simulations the question to what extent the speed or precision of the probabilistic inference degrades when one moves from a spiking neuron model that is optimal from the perspective of the underlying theory to a biologically more realistic neuron model. The results show, that in all cases quite good probabilistic inference results can be achieved within a time span of a few hundreds ms. In the remainder of this section we sketch the conceptual and scientific background for our approach. An additional discussion of related work can be found in the discussion section.</p>
      <p>Probabilistic inference in Bayesian networks <xref ref-type="bibr" rid="pcbi.1002294-Pearl1">[2]</xref> and other graphical models <xref ref-type="bibr" rid="pcbi.1002294-Bishop1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Koller1">[9]</xref> is an abstract description of a large class of computational tasks, that subsumes in particular many types of computational tasks that the brain has to solve: The formation of coherent interpretations of incomplete and ambiguous sensory stimuli, integration of previously acquired knowledge with new information, movement planning, reasoning and decision making in the presence of uncertainty <xref ref-type="bibr" rid="pcbi.1002294-Rao1">[10]</xref>–<xref ref-type="bibr" rid="pcbi.1002294-Tenenbaum1">[13]</xref>. The computational tasks become special cases of probabilistic inference if one assumes that the previously acquired knowledge (facts, rules, constraints, successful responses) is encoded in a joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e011" xlink:type="simple"/></inline-formula> over numerous RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e012" xlink:type="simple"/></inline-formula>, that represent features of sensory stimuli, aspects of internal models for the environment, environmental and behavioral context, values of carrying out particular actions in particular situations <xref ref-type="bibr" rid="pcbi.1002294-Toussaint1">[14]</xref>, goals, etc. If the values of some of these RVs assume concrete values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e013" xlink:type="simple"/></inline-formula> (e.g. because of observations, or because a particular goal has been set), the distribution of the remaining variables changes in general (to the conditional distribution given the values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e014" xlink:type="simple"/></inline-formula>). A typical computation that needs to be carried out for probabilistic inference for some joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e015" xlink:type="simple"/></inline-formula> involves in addition marginalization, and requires for example the evaluation of an expression of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e016" xlink:type="simple"/><label>(1)</label></disp-formula>where concrete values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e017" xlink:type="simple"/></inline-formula> (the “evidence”or “observations” have been inserted for the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e018" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e019" xlink:type="simple"/></inline-formula>. These variables are then often called observable variables, and the others latent variables. Note that the term “evidence” is somewhat misleading, since the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e020" xlink:type="simple"/></inline-formula> represents some arbitrary input to a probabilistic inference computation, without any connotation that it represents correct observations or memories. The computation of the resulting marginal distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e021" xlink:type="simple"/></inline-formula> requires a summation over all possible values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e022" xlink:type="simple"/></inline-formula> for the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e023" xlink:type="simple"/></inline-formula> that are currently not of interest for this probabilistic inference. This computation is in general quite complex (in fact, it is NP-complete <xref ref-type="bibr" rid="pcbi.1002294-Koller1">[9]</xref>) because in the worst case exponentially in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e024" xlink:type="simple"/></inline-formula> many terms need to be evaluated and summed up.</p>
      <p>There exist two completely different approaches for solving probabilistic inference tasks of type (1), to which we will refer in the following as the arithmetical and the sampling approach. In the arithmetical approach one exploits particular features of a graphical model, that captures conditional independence properties of the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e025" xlink:type="simple"/></inline-formula>, for organizing the order of summation steps and multiplication steps for the arithmetical calculation of the r.h.s. of (1) in an efficient manner. Belief propagation and message passing algorithms are special cases of this arithmetical approach. All previously proposed neural emulations of probabilistic inference in general graphical models have pursued this arithmetical approach. In the sampling approach, which we pursue in this article, one constructs a method for drawing samples from the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e026" xlink:type="simple"/></inline-formula> (with fixed values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e027" xlink:type="simple"/></inline-formula> for some of the RVs, see (1)). One can then approximate the l.h.s. of (1), i.e., the desired value of the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e028" xlink:type="simple"/></inline-formula>, by counting how often each possible value for the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e029" xlink:type="simple"/></inline-formula> occurs among the samples. More precisely, we identify conditions under which each current firing state (which records which neuron has fired within some time window) of a network of stochastically firing neurons can be viewed as a sample from a probability distribution that converges to the target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e030" xlink:type="simple"/></inline-formula>. For this purpose the temporal dynamics of the network is interpreted as a (non-reversible) Markov chain. We show that a suitable network architecture and parameter choice of the network of spiking neurons can make sure that this Markov chain has the target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e031" xlink:type="simple"/></inline-formula> as its stationary distribution, and therefore produces after some “burn–in time”samples (i.e., firing states) from a distribution that converges to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e032" xlink:type="simple"/></inline-formula>. This general strategy for sampling is commonly referred to as Markov chain Monte Carlo (MCMC) sampling <xref ref-type="bibr" rid="pcbi.1002294-Neal1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Andrieu1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Koller1">[9]</xref>.</p>
      <p>Before the first use of this strategy in networks of spiking neurons in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, MCMC sampling had already been studied in the context of artificial neural networks, so-called Boltzmann machines <xref ref-type="bibr" rid="pcbi.1002294-Ackley1">[15]</xref>. A Boltzmann machine consists of stochastic binary neurons in discrete time, where the output of each neuron has the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e033" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e034" xlink:type="simple"/></inline-formula> at each discrete time step. The probability of each value depends on the output values of neurons at the preceding discrete time step. For a Boltzmann machine a standard way of sampling is Gibbs sampling. The Markov chain that describes Gibbs sampling is reversible, i.e., stochastic transitions between states do not have a preferred direction in time. This sampling method works well in artificial neural networks, where the effect of each neural activity lasts for exactly one discrete time step. But it is in conflict with basic features of networks of spiking neurons, where each action potential (spike) of a neuron triggers inherent temporal processes in the neuron itself (e.g. refractory processes), and postsynaptic potentials of specific durations in other neurons to which it is synaptically connected. These inherent temporal processes of specific durations are non-reversible, and are therefore inconsistent with the mathematical model (Gibbs sampling) that underlies probabilistic inference in Boltzmann machines. <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> proposed a somewhat different mathematical model (sampling in non-reversible Markov chains) as an alternative framework for sampling, that is compatible with these basic features of the dynamics of networks of spiking neurons.</p>
      <p>We consider in this article two types of models for spiking neurons (see <xref ref-type="sec" rid="s4">Methods</xref> for details):</p>
      <list list-type="bullet">
        <list-item>
          <p>stochastic leaky integrate –and –fire neurons with absolute and relative refractory periods, formalized in the spike–response framework of <xref ref-type="bibr" rid="pcbi.1002294-Gerstner1">[16]</xref> (as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>), and</p>
        </list-item>
        <list-item>
          <p>simplified stochastic multi–ompartment neuron models with dendritic spikes.</p>
        </list-item>
      </list>
      <p> A key step for interpreting the firing activity of networks of neurons as sampling from a probability distribution (as proposed in <xref ref-type="bibr" rid="pcbi.1002294-Berkes1">[3]</xref>) in a rigorous manner is to define a formal relationship between spikes and samples. As in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> we relate the firing activity in a network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e035" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e036" xlink:type="simple"/></inline-formula> spiking neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e037" xlink:type="simple"/></inline-formula> to sampling from a distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e038" xlink:type="simple"/></inline-formula> over binary variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e039" xlink:type="simple"/></inline-formula> by setting<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e040" xlink:type="simple"/><label>(2)</label></disp-formula>(we restrict our attention here to binary RVs; multinomial RVs could in principle be represented by WTA circuits –see Discussion). The constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e041" xlink:type="simple"/></inline-formula> models the average length of the effect of a spike on the firing probability of other neurons or of the same neuron, and can be set for example to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e042" xlink:type="simple"/></inline-formula>.</p>
      <p>However with this definition of its internal state (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e043" xlink:type="simple"/></inline-formula>) the dynamics of the neural network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e044" xlink:type="simple"/></inline-formula> can not be modelled by a Markov chain, since knowledge of this current state does not suffice for determining the distribution of states at future time points, say at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e045" xlink:type="simple"/></inline-formula>. This distribution requires knowledge about when exactly a neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e046" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e047" xlink:type="simple"/></inline-formula> had fired. Therefore auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e048" xlink:type="simple"/></inline-formula> with multinomial or analog values were introduced in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, that keep track of when exactly in the preceding time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e049" xlink:type="simple"/></inline-formula> a neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e050" xlink:type="simple"/></inline-formula> had fired, and thereby restore the Markov property for a Markov chain that is defined over an enlarged state set consisting of all possible values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e052" xlink:type="simple"/></inline-formula>. However the introduction of these hidden variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e053" xlink:type="simple"/></inline-formula>, that keep track of inherent temporal processes in the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e054" xlink:type="simple"/></inline-formula> of spiking neurons, comes at the price that the resulting Markov chain is no longer reversible (because these temporal processes are not reversible). But it was shown in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> that one can prove nevertheless for any distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e055" xlink:type="simple"/></inline-formula> for which the so-called neural computability condition (NCC), see below, can be satisfied by a network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e056" xlink:type="simple"/></inline-formula> of spiking neurons, that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e057" xlink:type="simple"/></inline-formula> defines a non-reversible Markov chain whose stationary distribution is an expanded distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e058" xlink:type="simple"/></inline-formula>, whose marginal distribution over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e059" xlink:type="simple"/></inline-formula> (which results when one ignores the values of the hidden variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e060" xlink:type="simple"/></inline-formula>) is the desired distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e061" xlink:type="simple"/></inline-formula>. Hence a network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e062" xlink:type="simple"/></inline-formula> of spiking neurons can sample from any distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e063" xlink:type="simple"/></inline-formula> for which the NCC can be satisfied. This implies that any neural system that contains such network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e064" xlink:type="simple"/></inline-formula> can carry out the probabilistic inference task (1): The evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e065" xlink:type="simple"/></inline-formula> could be implemented through external inputs that force neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e066" xlink:type="simple"/></inline-formula> to fire at a high rate if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e067" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e068" xlink:type="simple"/></inline-formula>, and not to fire if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e069" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e070" xlink:type="simple"/></inline-formula>. In order to estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e071" xlink:type="simple"/></inline-formula>, it suffices that some readout neuron estimates (after some initial transient phase) the resulting firing rate of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e072" xlink:type="simple"/></inline-formula> that represents RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e073" xlink:type="simple"/></inline-formula>.</p>
      <p>In contrast to most of the other neural implementations of probabilistic inference (with some exceptions, see for example <xref ref-type="bibr" rid="pcbi.1002294-Deneve1">[17]</xref> and <xref ref-type="bibr" rid="pcbi.1002294-Boerlin1">[18]</xref>) where information is encoded in the firing rate of the neurons, in this approach the spike times, rather than the firing rate, of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e074" xlink:type="simple"/></inline-formula> carry relevant information as they define the value of the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e075" xlink:type="simple"/></inline-formula> at a particular moment in time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e076" xlink:type="simple"/></inline-formula> according to (2). In this spike-time based coding scheme, the relative timing of spikes (which neuron fires simultaneously with whom) receives a direct functional interpretation since it determines the correlation between the corresponding RVs.</p>
      <p>The NCC requires that for each RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e077" xlink:type="simple"/></inline-formula> the firing probability density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e078" xlink:type="simple"/></inline-formula> of its corresponding neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e079" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e080" xlink:type="simple"/></inline-formula> satisfies, if the neuron is not in a refractory period,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e081" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e082" xlink:type="simple"/></inline-formula> denotes the current value of all other RVs, i.e., all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e083" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e084" xlink:type="simple"/></inline-formula>. We use in this article the same model for a stochastic neuron as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> (continuous time case), which can be matched quite well to biological data according to <xref ref-type="bibr" rid="pcbi.1002294-Jolivet1">[19]</xref>. In the simpler version of this neuron model one assumes that it has an absolute refractory period of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e085" xlink:type="simple"/></inline-formula>, and that the instantaneous firing probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e086" xlink:type="simple"/></inline-formula> satisfies outside of its refractory period <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e087" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e088" xlink:type="simple"/></inline-formula> is its membrane potential (see <xref ref-type="sec" rid="s4">Methods</xref> for an account of the more complex neuron model with a relative refractory period from <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, that we have also tested in our simulations). The NCC from (3) can then be reformulated as a condition on the membrane potential of the neuron<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e089" xlink:type="simple"/><label>(4)</label></disp-formula></p>
      <p>Let us consider a Boltzmann distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e090" xlink:type="simple"/></inline-formula> of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e091" xlink:type="simple"/><label>(5)</label></disp-formula>with symmetric weights (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e092" xlink:type="simple"/></inline-formula>) that vanish on the diagonal (i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e093" xlink:type="simple"/></inline-formula>). In this case the NCC can be satisfied by a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e094" xlink:type="simple"/></inline-formula> that is <italic>linear</italic> in the postsynaptic potentials that neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e095" xlink:type="simple"/></inline-formula> receives from the neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e096" xlink:type="simple"/></inline-formula> that represent other RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e097" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e098" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e099" xlink:type="simple"/></inline-formula> is the bias of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e100" xlink:type="simple"/></inline-formula> (which regulates its excitability), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e101" xlink:type="simple"/></inline-formula> is the strength of the synaptic connection from neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e102" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e103" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e104" xlink:type="simple"/></inline-formula> approximates the time course of the postsynaptic potential caused by a firing of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e105" xlink:type="simple"/></inline-formula> at some time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e106" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e107" xlink:type="simple"/></inline-formula> assumes value 1 during the time interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e108" xlink:type="simple"/></inline-formula>, otherwise it has value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e109" xlink:type="simple"/></inline-formula>).</p>
      <p>However, it is well known that probabilistic inference for distributions of the form (5) is too weak to model various important computational tasks that the brain is obviously able to solve, at least without auxiliary variables. While (5) only allows pairwise interactions between RVs, numerous real world probabilistic inference tasks require inference for distributions with higher order terms. For example, it has been shown that human visual perception involves “explaining away”, a well known effect in probabilistic inference, where a change in the probability of one competing hypothesis for explaining some observation affects the probability of another competing hypothesis <xref ref-type="bibr" rid="pcbi.1002294-Kersten1">[20]</xref>. Such effects can usually only be captured with terms of order at least 3, since 3 RVs (for 2 hypotheses and 1 observation) may interact in complex ways. A well known example from visual perception is shown in <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>, for a probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e110" xlink:type="simple"/></inline-formula> over 4 RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e111" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e112" xlink:type="simple"/></inline-formula> is defined by the perceived relative reflectance of two abutting 2D areas, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e113" xlink:type="simple"/></inline-formula> by the perceived 3D shape of the observed object, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e114" xlink:type="simple"/></inline-formula> by the observed shading of the object, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e115" xlink:type="simple"/></inline-formula> by the contour of the 2D image. The difference in shading of the two abutting surfaces in <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1A</xref> could be explained either by a difference in reflectance of the two surfaces, or by an underlying curved 3D shape. The two different contours (RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e116" xlink:type="simple"/></inline-formula>) in the upper and lower part of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1A</xref> influence the likelihood of a curved 3D shape (RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e117" xlink:type="simple"/></inline-formula>). In particular, a perceived curved 3D shape “explains away” the difference in shading, thereby making a uniform reflectance more likely. The results of <xref ref-type="bibr" rid="pcbi.1002294-Knill1">[21]</xref> and numerous related results suggest that the brain is able to carry out probabilistic inference for more complex distributions than the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e118" xlink:type="simple"/></inline-formula> order Boltzmann distribution (5).</p>
      <fig id="pcbi-1002294-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>The visual perception experiment of <xref ref-type="bibr" rid="pcbi.1002294-Knill1">[<bold>21</bold>]</xref> that demonstrates “explaining away” and its corresponding Bayesian network model.</title>
          <p><bold>A</bold>) Two visual stimuli, each exhibiting the same luminance profile in the horizontal direction, differ only with regard to their contours, which suggest different 3D shapes (flat versus cylindrical). This in turn influences our perception of the reflectance of the two halves of each stimulus (a step in the reflectance at the middle line, versus uniform reflectance): the cylindrical 3D shape “explains away”the reflectance step. <bold>B</bold>) The Bayesian network that models this effect represents the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e119" xlink:type="simple"/></inline-formula>. The relative reflectance (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e120" xlink:type="simple"/></inline-formula>) of the two halves is either different (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e121" xlink:type="simple"/></inline-formula> = 1) or the same (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e122" xlink:type="simple"/></inline-formula> = 0). The perceived 3D shape can be cylindrical (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e123" xlink:type="simple"/></inline-formula> = 1) or flat (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e124" xlink:type="simple"/></inline-formula> = 0). The relative reflectance and the 3D shape are direct causes of the shading (luminance change) of the surfaces (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e125" xlink:type="simple"/></inline-formula>), which can have the profile like in panel A (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e126" xlink:type="simple"/></inline-formula> = 1) or a different one (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e127" xlink:type="simple"/></inline-formula> = 0). The 3D shape of the surfaces causes different perceived contours, flat (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e128" xlink:type="simple"/></inline-formula> = 0) or cylindrical (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e129" xlink:type="simple"/></inline-formula> = 1). The observed variables (evidence) are the contour (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e130" xlink:type="simple"/></inline-formula>) and the shading (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e131" xlink:type="simple"/></inline-formula>). Subjects infer the marginal posterior probability distributions of the relative reflectance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e132" xlink:type="simple"/></inline-formula> and the 3D shape <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e133" xlink:type="simple"/></inline-formula> based on the evidence. <bold>C</bold>) The RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e134" xlink:type="simple"/></inline-formula> are represented in our neural implementations by principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e135" xlink:type="simple"/></inline-formula>. Each spike of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e136" xlink:type="simple"/></inline-formula> sets the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e137" xlink:type="simple"/></inline-formula> to 1 for a time period of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e138" xlink:type="simple"/></inline-formula>. <bold>D</bold>) The structure of a network of spiking neurons that performs probabilistic inference for the Bayesian network of panel B through sampling from conditionals of the underlying distribution. Each principal neuron employs preprocessing to satisfy the NCC, either by dendritic processing or by a preprocessing circuit.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g001" xlink:type="simple"/>
      </fig>
      <p>We show in this article that the neural sampling method of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> can be extended to any probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e139" xlink:type="simple"/></inline-formula> over binary RVs, in particular to distributions with higher order dependencies among RVs, by using auxiliary spiking neurons in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e140" xlink:type="simple"/></inline-formula> that do not directly represent RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e141" xlink:type="simple"/></inline-formula>, or by using nonlinear computational processes in multi-compartment neuron models. As one can expect, the number of required auxiliary neurons or dendritic branches increases with the complexity of the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e142" xlink:type="simple"/></inline-formula> for which the resulting network of spiking neurons has to carry out probabilistic inference. Various types of graphical models <xref ref-type="bibr" rid="pcbi.1002294-Koller1">[9]</xref> have emerged as convenient frameworks for characterizing the complexity of distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e143" xlink:type="simple"/></inline-formula> from the perspective of probabilistic inference for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e144" xlink:type="simple"/></inline-formula>.</p>
      <p>We will focus in this article on Bayesian networks, a common type of graphical model for probability distributions. But our results can also be applied for other types of graphical models. A Bayesian network is a directed graph (without directed cycles), whose nodes represent RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e145" xlink:type="simple"/></inline-formula>. Its graph structure indicates that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e146" xlink:type="simple"/></inline-formula> admits a factorization of the form<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e147" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e148" xlink:type="simple"/></inline-formula> is the set of all (direct) parents of the node indexed by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e149" xlink:type="simple"/></inline-formula>. For example, the Bayesian network in <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref> implies that the factorization <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e150" xlink:type="simple"/></inline-formula> is possible.</p>
      <p>We show that the complexity of the resulting network of spiking neurons for carrying out probabilistic inference for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e151" xlink:type="simple"/></inline-formula> can be bounded in terms of the graph complexity of the Bayesian network that gives rise to the factorization (7). More precisely, we present three different approaches for constructing such networks of spiking neurons:</p>
      <list list-type="bullet">
        <list-item>
          <p>through a reduction of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e152" xlink:type="simple"/></inline-formula> to a Boltzmann distribution (5) with auxiliary RVs</p>
        </list-item>
        <list-item>
          <p>through a Markov blanket expansion of the r.h.s. of the NCC (4)</p>
        </list-item>
        <list-item>
          <p>through a factorized expansion of the r.h.s. of the NCC (4)</p>
        </list-item>
      </list>
      <p>We will show that there exist two different neural implementation options for each of the last two approaches, using either specific network motifs or dendritic processing for nonlinear computation steps. This yields altogether 5 different options for emulating probabilistic inference in Bayesian networks through sampling via the inherent stochastic dynamics of networks of spiking neurons. We will exhibit characteristic differences in the complexity and performance of the resulting networks, and relate these to the complexity of the underlying Bayesian network. All 5 of these neural implementation options can readily be applied to Bayesian networks where several arcs converge to a node (giving rise to the “explaining away” effect), and to Bayesian networks with undirected cycles (“loops”). All methods for probabilistic inference from general graphical models that we propose in this article are from the mathematical perspective special cases of MCMC sampling. However in view of the fact that they expand the neural sampling approach of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, we will refer to them more specifically as neural sampling.</p>
      <p>We show through computer simulations for three different Bayesian networks of different sizes and complexities that neural sampling can be carried quite fast with the help of the second and third approach, providing good inference results within a behaviorally relevant time span of a few hundred ms. One of these Bayesian networks addresses the previously described classical “explaining away” effect in visual perception from <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>. The other two Bayesian networks not only contain numerous “explaining away” effects, but also undirected cycles. Altogether, our computer simulations and our theoretical analyses demonstrate that networks of spiking neurons can emulate probabilistic inference for general Bayesian networks. Hence we propose to view probabilistic inference in graphical models as a generic computational paradigm, that can help us to understand the computational organization of networks of neurons in the brain, and in particular the computational role of precisely structured cortical microcircuit motifs.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>We present several ways how probabilistic inference for a given joint distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e153" xlink:type="simple"/></inline-formula>, that is not required to have the form of a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e154" xlink:type="simple"/></inline-formula> order Boltzmann distribution (5), can be carried out through sampling from the inherent dynamics of a recurrent network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e155" xlink:type="simple"/></inline-formula> of stochastically spiking neurons. All these approaches are based on the idea that such network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e156" xlink:type="simple"/></inline-formula> of spiking neurons can be viewed –for a suitable choice of its architecture and parameters –as an internal or “physical model” for the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e157" xlink:type="simple"/></inline-formula>, in the sense that its distribution of network states converges to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e158" xlink:type="simple"/></inline-formula>, from any initial state. Then probabilistic inference for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e159" xlink:type="simple"/></inline-formula> can be easily carried out by any readout neuron that observes the resulting network states, or the spikes from one or several neurons in the network. This holds not only for sampling from the prior distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e160" xlink:type="simple"/></inline-formula>, but also for sampling from the posterior after some evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e161" xlink:type="simple"/></inline-formula> has become available (see (1)). The link between network states of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e162" xlink:type="simple"/></inline-formula> and the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e163" xlink:type="simple"/></inline-formula> is provided by assuming that there exists for each RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e164" xlink:type="simple"/></inline-formula> a neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e165" xlink:type="simple"/></inline-formula> such that each time when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e166" xlink:type="simple"/></inline-formula> fires, it sets the associated binary RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e167" xlink:type="simple"/></inline-formula> to 1 for a time period of some length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e168" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1C</xref>). We refer to neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e169" xlink:type="simple"/></inline-formula> that represent in this way a RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e170" xlink:type="simple"/></inline-formula> as principal neurons. All other neurons are referred to as auxiliary neurons.</p>
      <p>The mathematical basis for analyzing the distribution of network states, and relating it to a given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e171" xlink:type="simple"/></inline-formula>, is provided by the theory of Markov chains. More precisely, it was shown in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> that by introducing for each principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e172" xlink:type="simple"/></inline-formula> an additional hidden analog RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e173" xlink:type="simple"/></inline-formula>, that keeps track of time within the time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e174" xlink:type="simple"/></inline-formula> after a spike of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e175" xlink:type="simple"/></inline-formula>, one can model the dynamics of the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e176" xlink:type="simple"/></inline-formula> by a non-reversible Markov chain. This Markov chain is non-reversible, in contrast to Gibbs sampling or other Markov chains that are usually considered in Machine Learning and in the theory of Boltzmann machines, because this facilitates the modelling of the temporal dynamics of spiking neurons, in particular refractory processes within a spiking neuron after a spike and temporally extended effects of its spike on the membrane potential of other neurons to which it is synaptically connected (postsynaptic potentials). The underlying mathematical theory guarantees that nevertheless the distribution of network states of this Markov chain converges (for the “original” RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e177" xlink:type="simple"/></inline-formula>) to the given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e178" xlink:type="simple"/></inline-formula>, provided that the NCC (4) is met. This theoretical result reduces our goal, to demonstrate ways how a network of spiking neurons can carry out probabilistic inference in general graphical models, to the analysis of possibilities for satisfying the NCC (4) in networks of spiking neurons. The networks of spiking neurons that we construct and analyze build primarily on the model for neural sampling in continuous time from <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, since this continuous time version is the more satisfactory model from the biological perspective. But all our results also hold for the mathematically simpler version with discrete time.</p>
      <p>We exhibit both methods for satisfying the NCC with the help of auxiliary neurons in networks of point neurons, and in networks of multi-compartment neuron models (where no auxiliary neurons are required). All neuron models that we consider are stochastic, where the probability density function for the firing of a neuron at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e179" xlink:type="simple"/></inline-formula> (provided it is currently not in a refractory state) is proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e180" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e181" xlink:type="simple"/></inline-formula> is its current membrane potential at the soma. We assume (as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>) that in a point neuron model the membrane potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e182" xlink:type="simple"/></inline-formula> can be written as a linear combination of postsynaptic potentials. Thus if the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e183" xlink:type="simple"/></inline-formula> is modelled as a point neuron, we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e184" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e185" xlink:type="simple"/></inline-formula> is the bias of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e186" xlink:type="simple"/></inline-formula> (which regulates its excitability), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e187" xlink:type="simple"/></inline-formula> is the strength of the synaptic connection from neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e188" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e189" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e190" xlink:type="simple"/></inline-formula> approximates the time course of the postsynaptic potential in neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e191" xlink:type="simple"/></inline-formula> caused by a firing of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e192" xlink:type="simple"/></inline-formula>. The ideal neuron model from the perspective of the theory of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> has an absolute refractory period of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e193" xlink:type="simple"/></inline-formula>, which is also the assumed length of a postsynaptic potential (EPSP or IPSP). But it was shown there through computer simulations that neural sampling can be carried out also with stochastically firing neurons that have a relative refractory period, i.e. the neuron can fire with some probability with an interspike interval of less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e194" xlink:type="simple"/></inline-formula>. In particular, it was shown there in simulations that the resulting neural network samples from a slight variation of the target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e195" xlink:type="simple"/></inline-formula>, that is in most cases practically indistinguishable.</p>
      <p>Before we describe two different theoretical approaches for satisfying the NCC, we first consider an even simpler method for extending the neural sampling approach from <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> to arbitrary distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e196" xlink:type="simple"/></inline-formula>: through a reduction to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e197" xlink:type="simple"/></inline-formula> order Boltzmann distributions (5) with auxiliary RVs.</p>
      <sec id="s2a">
        <title>Second Order Boltzmann Distributions with Auxiliary Random Variables (Implementation 1)</title>
        <p>It is well known <xref ref-type="bibr" rid="pcbi.1002294-Ackley1">[15]</xref> that any probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e198" xlink:type="simple"/></inline-formula>, with arbitrarily large factors in a factorization such as (7), can be represented as marginal distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e199" xlink:type="simple"/><label>(9)</label></disp-formula>of an extended distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e200" xlink:type="simple"/></inline-formula> with auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e201" xlink:type="simple"/></inline-formula>, that can be factorized into factors of degrees at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e202" xlink:type="simple"/></inline-formula>. This can be seen as follows. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e203" xlink:type="simple"/></inline-formula> be an arbitrary probability distribution over binary variables with higher order factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e204" xlink:type="simple"/></inline-formula>). Thus<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e205" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e206" xlink:type="simple"/></inline-formula> is a vector composed of the RVs that the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e207" xlink:type="simple"/></inline-formula> depends on and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e208" xlink:type="simple"/></inline-formula> is a normalization constant. We additionally assume that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e209" xlink:type="simple"/></inline-formula> is non-zero for each value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e210" xlink:type="simple"/></inline-formula>. The simple idea is to introduce for each possible assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e211" xlink:type="simple"/></inline-formula> to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e212" xlink:type="simple"/></inline-formula> in a higher order factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e213" xlink:type="simple"/></inline-formula> a new RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e214" xlink:type="simple"/></inline-formula>, that has value 1 only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e215" xlink:type="simple"/></inline-formula> is the current assignment of values to the RVs in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e216" xlink:type="simple"/></inline-formula>. We will illustrate this idea through the concrete example of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>. Since there is only one factor that contains more than 2 RVs in the probability distribution of this example (see caption of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>), the conditional probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e217" xlink:type="simple"/></inline-formula>, there will be 8 auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e218" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e219" xlink:type="simple"/></inline-formula>, …, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e220" xlink:type="simple"/></inline-formula> for this factor, one for each of the 8 possible assignments to the 3 RVs in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e221" xlink:type="simple"/></inline-formula>. Let us consider a particular auxiliary RV, e.g. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e222" xlink:type="simple"/></inline-formula>. It assumes value 1 only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e223" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e224" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e225" xlink:type="simple"/></inline-formula>. This constraint for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e226" xlink:type="simple"/></inline-formula> can be enforced through second order factors between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e227" xlink:type="simple"/></inline-formula> and each of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e228" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e229" xlink:type="simple"/></inline-formula>. For example, the second order factor that relates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e230" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e231" xlink:type="simple"/></inline-formula> has a value of 0 if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e232" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e233" xlink:type="simple"/></inline-formula> (i.e., if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e234" xlink:type="simple"/></inline-formula> is not compatible with the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e235" xlink:type="simple"/></inline-formula>), and value 1 otherwise. The individual values of the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e236" xlink:type="simple"/></inline-formula> for different assignments to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e237" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e238" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e239" xlink:type="simple"/></inline-formula> are introduced in the extended distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e240" xlink:type="simple"/></inline-formula> through first order factors, one for each auxiliary RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e241" xlink:type="simple"/></inline-formula>. Specifically, the first order factor that depends on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e242" xlink:type="simple"/></inline-formula> has value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e243" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e244" xlink:type="simple"/></inline-formula> is a constant that rescales the values of the factors such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e245" xlink:type="simple"/></inline-formula> for all assignments to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e246" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e247" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e248" xlink:type="simple"/></inline-formula>) if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e249" xlink:type="simple"/></inline-formula>, and value 1 otherwise. Further details of the construction method for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e250" xlink:type="simple"/></inline-formula> are given in the Methods section, together with a proof of (9).</p>
        <p>The resulting extended probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e251" xlink:type="simple"/></inline-formula> has the property that, in spite of deterministic dependencies between the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e252" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e253" xlink:type="simple"/></inline-formula>, the state set of the resulting Markov chain realized through a network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e254" xlink:type="simple"/></inline-formula> of spiking neurons according to <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> (that consists of all non-forbidden value assignments to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e255" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e256" xlink:type="simple"/></inline-formula>) is connected. In the previous example a non-forbidden value assignment is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e257" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e258" xlink:type="simple"/></inline-formula>. But <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e259" xlink:type="simple"/></inline-formula> is also a non-forbidden value assignment. Such non-forbidden value assignments to the auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e260" xlink:type="simple"/></inline-formula> corresponding to one higher order factor, where all of them assume value of 0 regardless of the values of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e261" xlink:type="simple"/></inline-formula> RVs provide transition points for paths of probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e262" xlink:type="simple"/></inline-formula> that connect any two non-forbidden value assignments (without requiring that 2 or more RVs switch their values simultaneously). The resulting connectivity of all non-forbidden states (see <xref ref-type="sec" rid="s4">Methods</xref> for a proof) implies that this Markov chain has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e263" xlink:type="simple"/></inline-formula> as its unique stationary distribution. The given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e264" xlink:type="simple"/></inline-formula> arises as marginal distribution of this stationary distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e265" xlink:type="simple"/></inline-formula> , hence one can use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e266" xlink:type="simple"/></inline-formula> to sample from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e267" xlink:type="simple"/></inline-formula> (just ignore the firing activity of neurons that correspond to auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e268" xlink:type="simple"/></inline-formula>).</p>
        <p>Since the number of RVs in the extended probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e269" xlink:type="simple"/></inline-formula> can be much larger than the number of RVs in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e270" xlink:type="simple"/></inline-formula>, the corresponding spiking neural network samples from a much larger probability space. This, as well as the presence of deterministic relations between the auxiliary and the main RVs in the expanded probability distribution, slow down the convergence of the resulting Markov chain to its stationary distribution. We show however in the following, that there are several alternatives for sampling from an arbitrary distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e271" xlink:type="simple"/></inline-formula> through a network of spiking neurons. These alternative methods do not introduce auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e272" xlink:type="simple"/></inline-formula>, but rather aim at directly satisfying the NCC (4) in a network of spiking neurons. Note that the principal neurons in the neural network that implements neural sampling through introduction of auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e273" xlink:type="simple"/></inline-formula> also satisfy the NCC, but in the extended probability distribution with second order relations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e274" xlink:type="simple"/></inline-formula>, whereas in the neural implementations introduced in the following the principal neurons satisfy the NCC in the original distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e275" xlink:type="simple"/></inline-formula>. In Computer Simulation I we have compared the convergence speed of the methods that satisfy the NCC with that of the previously described method via auxiliary RVs. It turns out that the alternative strategy provides an about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e276" xlink:type="simple"/></inline-formula> fold speed-up for the Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref>.</p>
      </sec>
      <sec id="s2b">
        <title>Using the Markov Blanket Expansion of the Log-odd Ratio</title>
        <p>Assume that the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e277" xlink:type="simple"/></inline-formula> for which we want to carry out probabilistic inference is given by some arbitrary Bayesian network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e278" xlink:type="simple"/></inline-formula>. There are two different options for satisfying the NCC for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e279" xlink:type="simple"/></inline-formula>, which differ in the way by which the term on the r.h.s. of the NCC (4) is expanded. The option that we will analyze first uses from the structure of the Bayesian network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e280" xlink:type="simple"/></inline-formula> only the information about which RVs are in the Markov blanket of each RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e281" xlink:type="simple"/></inline-formula>. The Markov blanket <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e282" xlink:type="simple"/></inline-formula> of the corresponding node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e283" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e284" xlink:type="simple"/></inline-formula> (which consists of the parents, children and co-parents of this node) has the property that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e285" xlink:type="simple"/></inline-formula> is independent from all other RVs once any assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e286" xlink:type="simple"/></inline-formula> of values to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e287" xlink:type="simple"/></inline-formula> in the Markov blanket has been fixed. Hence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e288" xlink:type="simple"/></inline-formula> = <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e289" xlink:type="simple"/></inline-formula>, and the term on the r.h.s. of the NCC (4) can be expanded as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e290" xlink:type="simple"/><label>(11)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e291" xlink:type="simple"/><label>(12)</label></disp-formula>The sum indexed by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e292" xlink:type="simple"/></inline-formula> runs over the set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e293" xlink:type="simple"/></inline-formula> of all possible assignments of values to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e294" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e295" xlink:type="simple"/></inline-formula> denotes a predicate which has value 1 if the condition in the brackets is true, and to 0 otherwise. Hence, for satisfying the NCC it suffices if there are auxiliary neurons, or dendritic branches, for each of these <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e296" xlink:type="simple"/></inline-formula>, that become active if and only if the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e297" xlink:type="simple"/></inline-formula> currently assume the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e298" xlink:type="simple"/></inline-formula>. The current values of the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e299" xlink:type="simple"/></inline-formula> are encoded in the firing activity of their corresponding principal neurons. The corresponding term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e300" xlink:type="simple"/></inline-formula> can be implemented with the help of the bias <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e301" xlink:type="simple"/></inline-formula> (see (8)) of the auxiliary neuron that corresponds to the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e302" xlink:type="simple"/></inline-formula>, resulting in a value of its membrane potential equal to the r.h.s. of the NCC (4). We will discuss this implementation option below as Implementation 2. In the subsequently discussed implementation option (Implementation 3) all principal neurons will be multi-compartment neurons, and no auxiliary neurons are needed. In this case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e303" xlink:type="simple"/></inline-formula> scales the amplitude of the signal from a specific dendritic branch to the soma of the multi-compartment principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e304" xlink:type="simple"/></inline-formula>.</p>
        <sec id="s2b1">
          <title>Implementation with auxiliary neurons (Implementation 2)</title>
          <p>We illustrate the implementation of the Markov blanket expansion approach through auxiliary neurons for the concrete example of the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e305" xlink:type="simple"/></inline-formula> in the Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref> (see <xref ref-type="sec" rid="s4">Methods</xref> for a discussion of the general case). Its Markov blanket <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e306" xlink:type="simple"/></inline-formula> consists here of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e307" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e308" xlink:type="simple"/></inline-formula>. Hence the resulting neural circuit (see <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref>) for satisfying the NCC for the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e309" xlink:type="simple"/></inline-formula> uses 4 auxiliary neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e310" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e311" xlink:type="simple"/></inline-formula>, one for each of the 4 possible assignments <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e312" xlink:type="simple"/></inline-formula> of values to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e313" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e314" xlink:type="simple"/></inline-formula>. Each firing of one of these auxiliary neurons should cause an immediately subsequent firing of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e315" xlink:type="simple"/></inline-formula>. Lateral inhibition among these auxiliary neurons can make sure that after a firing of an auxiliary neuron no other auxiliary neuron fires during the subsequent time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e316" xlink:type="simple"/></inline-formula>, thereby implementing the required absolute refractory period of the theoretical model from <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>. The presynaptic principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e317" xlink:type="simple"/></inline-formula>) is connected to the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e318" xlink:type="simple"/></inline-formula> directly if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e319" xlink:type="simple"/></inline-formula> assumes that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e320" xlink:type="simple"/></inline-formula> has value 1, otherwise via an inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e321" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref>). In case of a synaptic connection via an inhibitory interneuron, a firing of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e322" xlink:type="simple"/></inline-formula> prevents a firing of this auxiliary neuron during the subsequent time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e323" xlink:type="simple"/></inline-formula>. The direct excitatory synaptic connections from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e324" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e325" xlink:type="simple"/></inline-formula> raise the membrane potential of that auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e326" xlink:type="simple"/></inline-formula>, for which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e327" xlink:type="simple"/></inline-formula> agrees with the current values of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e328" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e329" xlink:type="simple"/></inline-formula>, so that it reaches the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e330" xlink:type="simple"/></inline-formula>, and fires with a probability equal to the r.h.s. of the NCC (4) during the time interval within which the value assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e331" xlink:type="simple"/></inline-formula> remains valid. The other 3 auxiliary neurons are during this period either inhibited by the inhibitory interneurons, or do not receive enough excitatory input from the direct connections to reach a significant firing probability. Hence, the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e332" xlink:type="simple"/></inline-formula> will always be driven to fire just by a single auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e333" xlink:type="simple"/></inline-formula> corresponding to the current value of the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e334" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e335" xlink:type="simple"/></inline-formula>, and will fire immediately after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e336" xlink:type="simple"/></inline-formula> fires.</p>
          <fig id="pcbi-1002294-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Implementation 2 for the explaining away motif of the Bayesian network from <xref ref-type="fig" rid="pcbi-1002294-g001"><bold>Fig. 1B</bold></xref>.</title>
              <p>Implementation 2 is the neural implementation with auxiliary neurons, that uses the Markov blanket expansion of the log-odd ratio. There are 4 auxiliary neurons, one for each possible value assignment to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e337" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e338" xlink:type="simple"/></inline-formula> in the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e339" xlink:type="simple"/></inline-formula>. The principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e340" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e341" xlink:type="simple"/></inline-formula>) connects to the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e342" xlink:type="simple"/></inline-formula> directly if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e343" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e344" xlink:type="simple"/></inline-formula>) has value 1 in the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e345" xlink:type="simple"/></inline-formula>, or via an inhibitory inter-neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e346" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e347" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e348" xlink:type="simple"/></inline-formula>) has value 0 in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e349" xlink:type="simple"/></inline-formula>. The auxiliary neurons connect with a strong excitatory connection to the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e350" xlink:type="simple"/></inline-formula>, and drive it to fire whenever any one of them fires. The larger gray circle represents the lateral inhibition between the auxiliary neurons.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g002" xlink:type="simple"/>
          </fig>
          <p>As <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e351" xlink:type="simple"/></inline-formula> has a firing probability that satisfies the r.h.s. of the NCC (4) temporally during the time interval while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e352" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e353" xlink:type="simple"/></inline-formula> are consistent with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e354" xlink:type="simple"/></inline-formula>, the firing of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e355" xlink:type="simple"/></inline-formula> satisfies the r.h.s. of the NCC (4) at any moment in time.</p>
        </sec>
        <sec id="s2b2">
          <title>Computer Simulation I: Comparison of two methods for emulating “explaining away” in networks of spiking neurons</title>
          <p>In our preceding theoretical analysis we have exhibited two completely different methods for emulating in networks of spiking neurons probabilistic inference in general graphical models through sampling: either by a reduction to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e356" xlink:type="simple"/></inline-formula> order Boltzmann distributions (5) through the introduction of auxiliary RVs (Implementation 1), or by satisfying the NCC (3) via the Markov blanket expansion. We have tested the accuracy and convergence speed of both methods for the Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref>, and the results are shown in <xref ref-type="fig" rid="pcbi-1002294-g003">Fig. 3</xref>. The approach via the NCC converges substantially faster.</p>
          <fig id="pcbi-1002294-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Results of Computer Simulation I.</title>
              <p>Performance comparison between an ideal version of Implementation 1 (use of auxiliary RVs, results shown in <italic>green</italic>) and an ideal version of implementations that satisfy the NCC (results shown in <italic>blue</italic>) for probabilistic inference in the Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref> (“explaining away”. Evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e357" xlink:type="simple"/></inline-formula> (see (1)) is entered for the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e358" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e359" xlink:type="simple"/></inline-formula>, and the marginal probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e360" xlink:type="simple"/></inline-formula> is estimated. <bold>A</bold>) Target values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e361" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e362" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e363" xlink:type="simple"/></inline-formula> are shown in black, results from sampling for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e364" xlink:type="simple"/></inline-formula> from a network of spiking neurons are shown in green and blue. Panels <bold>C</bold>) and <bold>D</bold>) show the temporal evolution of the Kullback-Leibler divergence between the resulting estimates through neural sampling <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e365" xlink:type="simple"/></inline-formula> and the correct posterior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e366" xlink:type="simple"/></inline-formula>, averaged over 10 trials for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e367" xlink:type="simple"/></inline-formula> in C) and for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e368" xlink:type="simple"/></inline-formula> in D). The green and blue areas around the green and blue curves represent the unbiased value of the standard deviation. The estimated marginal posterior is calculated for each time point from the samples (number of spikes) from the beginning of the simulation (or from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e369" xlink:type="simple"/></inline-formula> for the second inference query with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e370" xlink:type="simple"/></inline-formula>). Panels A, C, D show that both approaches yield correct probabilistic inference through neural sampling, but the approach via satisfying the NCC converges about 10 times faster. <bold>B</bold>) The firing rates of principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e371" xlink:type="simple"/></inline-formula> (solid line) and of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e372" xlink:type="simple"/></inline-formula> (dashed line) in the approach via satisfying the NCC, estimated with a sliding window (alpha kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e373" xlink:type="simple"/></inline-formula>). In this experiment the evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e374" xlink:type="simple"/></inline-formula> was switched after 3 s (red vertical line) from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e375" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e376" xlink:type="simple"/></inline-formula>. The “explaining away”effect is clearly visible from the complementary evolution of the firing rates of the neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e377" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e378" xlink:type="simple"/></inline-formula>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g003" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2b3">
          <title>Implementation with dendritic computation (Implementation 3)</title>
          <p>We now show that the Markov blanket expansion approach can also be implemented through dendritic branches of multi-compartment neuron models (see <xref ref-type="sec" rid="s4">Methods</xref>) for the principal neurons, without using auxiliary neurons (except for inhibitory interneurons). We will illustrate the idea through the same Bayesian network example as discussed in Implementation 2, and refer to Methods for a discussion of the case of arbitrary Bayesian networks. <xref ref-type="fig" rid="pcbi-1002294-g004">Fig. 4</xref> shows the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e379" xlink:type="simple"/></inline-formula> in the spiking neural network for the Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref>. It has 4 dendritic branches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e380" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e381" xlink:type="simple"/></inline-formula>, each of them corresponding to one assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e382" xlink:type="simple"/></inline-formula> of values to the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e383" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e384" xlink:type="simple"/></inline-formula> in the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e385" xlink:type="simple"/></inline-formula>. The input connections from the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e386" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e387" xlink:type="simple"/></inline-formula> to the dendritic branches of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e388" xlink:type="simple"/></inline-formula> follow the same pattern as the connections from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e389" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e390" xlink:type="simple"/></inline-formula> to the auxiliary neurons in Implementation 2. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e391" xlink:type="simple"/></inline-formula> be an assignment that corresponds to the current values of the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e392" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e393" xlink:type="simple"/></inline-formula>. The efficacies of the synapses at the dendritic branches and their thresholds for initiating a dendritic spike are chosen such that the total synaptic input to the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e394" xlink:type="simple"/></inline-formula> is then strong enough to cause a dendritic spike in the branch, that contributes to the membrane potential at the soma a component whose amplitude is equal to the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e395" xlink:type="simple"/></inline-formula> in (11). This amplitude could for example be controlled by the branch strength of this dendritic branch (see <xref ref-type="bibr" rid="pcbi.1002294-Losonczy1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Legenstein1">[23]</xref>). The parameters can be chosen so that all other dendritic branches do not receive enough synaptic input to reach the local threshold for initiating a dendritic spike, and therefore do not affect the membrane potential at the soma. Hence, the membrane potential at the soma of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e396" xlink:type="simple"/></inline-formula> will be equal to the contribution from the currently active dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e397" xlink:type="simple"/></inline-formula>, implementing thereby the r.h.s of (11).</p>
          <fig id="pcbi-1002294-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Implementation 3 for the same explaining away motif as in <xref ref-type="fig" rid="pcbi-1002294-g002"><bold>Fig. 2</bold></xref>.</title>
              <p>Implementation 3 is the neural implementation with dendritic computation that uses the Markov blanket expansion of the log-odd ratio. The principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e398" xlink:type="simple"/></inline-formula> has 4 dendritic branches, one for each possible assignment of values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e399" xlink:type="simple"/></inline-formula> to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e400" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e401" xlink:type="simple"/></inline-formula> in the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e402" xlink:type="simple"/></inline-formula>. The dendritic branches of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e403" xlink:type="simple"/></inline-formula> receive synaptic inputs from the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e404" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e405" xlink:type="simple"/></inline-formula> either directly, or via an interneuron (analogously as in <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref>). It is required that at any moment in time exactly one of the dendritic branches (that one, whose index <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e406" xlink:type="simple"/></inline-formula> agrees with the current firing states of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e407" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e408" xlink:type="simple"/></inline-formula>) generates dendritic spikes, whose amplitude at the soma determines the current firing probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e409" xlink:type="simple"/></inline-formula>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g004" xlink:type="simple"/>
          </fig>
          <p>Since the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e410" xlink:type="simple"/></inline-formula> in (11) can have both positive and negative values and the amplitude of the dendritic spikes and the excitatory synaptic efficacy are positive quantities, in this, and the following neural implementations we always add a positive constant to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e411" xlink:type="simple"/></inline-formula> to shift it into the positive range. We subtract the same constant value from the steady state of the membrane potential.</p>
        </sec>
      </sec>
      <sec id="s2c">
        <title>Using the Factorized Expansion of the Log-odd Ratio</title>
        <p>The second strategy to expand the log-odd ratio on the r.h.s. of the NCC (4) uses the factorized form (10) of the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e412" xlink:type="simple"/></inline-formula>. This form allows us to rewrite the log-odd ratio in (4) as a sum of log terms, one for each factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e413" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e414" xlink:type="simple"/></inline-formula>, that contains the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e415" xlink:type="simple"/></inline-formula> (we write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e416" xlink:type="simple"/></inline-formula> for this set of factors). One can write each of these terms as a sum over all possible assignments <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e417" xlink:type="simple"/></inline-formula> of values of the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e418" xlink:type="simple"/></inline-formula> the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e419" xlink:type="simple"/></inline-formula> depends on (except <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e420" xlink:type="simple"/></inline-formula>). This yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e421" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e422" xlink:type="simple"/></inline-formula> is a vector composed of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e423" xlink:type="simple"/></inline-formula> that the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e424" xlink:type="simple"/></inline-formula> depends on –without <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e425" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e426" xlink:type="simple"/></inline-formula> is the current value of this vector at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e427" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e428" xlink:type="simple"/></inline-formula> denotes the set of all possible assignments to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e429" xlink:type="simple"/></inline-formula>. The parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e430" xlink:type="simple"/></inline-formula> are set to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e431" xlink:type="simple"/><label>(14)</label></disp-formula>The factorized expansion in (13) is similar to (11), but with the difference that we have another sum running over all factors that depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e432" xlink:type="simple"/></inline-formula>. Consequently, in the resulting Implementation 4 with auxiliary neurons and dendritic branches there will be several groups of auxiliary neurons that connect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e433" xlink:type="simple"/></inline-formula>, where each group implements the expansion of one factor in (13). The alternative model that only uses dendritic computation (Implementation 5) will have groups of dendritic branches corresponding to the different factors. The number of auxiliary neurons that connect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e434" xlink:type="simple"/></inline-formula> in Implementation 4 (and the corresponding number of dendritic branches in Implementation 5) is equal to the sum of the exponents of the sizes of factors that depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e435" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e436" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e437" xlink:type="simple"/></inline-formula> denotes the number of RVs in the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e438" xlink:type="simple"/></inline-formula>. This number is never larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e439" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e440" xlink:type="simple"/></inline-formula> is the size of the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e441" xlink:type="simple"/></inline-formula>), which gives the corresponding number of auxiliary neurons or dendritic branches that are required in the Implementation 2 and 3. These two numbers can considerably differ in graphical models where the RVs participate in many factors, but the size of the factors is small. Therefore one advantage of this approach is that it requires in general fewer resources. On the other hand, it introduces a more complex connectivity between the auxiliary neurons and the principal neuron (compare <xref ref-type="fig" rid="pcbi-1002294-g005">Fig. 5</xref> with <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref>).</p>
        <fig id="pcbi-1002294-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Implementation 4 for the same explaining away motif as in <xref ref-type="fig" rid="pcbi-1002294-g002"><bold>Fig. 2</bold></xref> and <xref ref-type="fig" rid="pcbi-1002294-g004"><bold>4</bold></xref>.</title>
            <p>Implementation 4 is the neural implementation with auxiliary neurons and dendritic branches, that uses the factorized expansion of the log-odd ratio. As in <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref> there is one auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e442" xlink:type="simple"/></inline-formula> for each possible value assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e443" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e444" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e445" xlink:type="simple"/></inline-formula>. The connections from the neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e446" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e447" xlink:type="simple"/></inline-formula> (that carry the current values of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e448" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e449" xlink:type="simple"/></inline-formula>) to the auxiliary neurons are the same as in <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref>, and when these RVs change their value, the auxiliary neuron that corresponds to the new value fires. Each auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e450" xlink:type="simple"/></inline-formula> connects to the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e451" xlink:type="simple"/></inline-formula> at a separate dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e452" xlink:type="simple"/></inline-formula>, and there is an inhibitory neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e453" xlink:type="simple"/></inline-formula> connecting to the same branch. The rest of the auxiliary neurons connect to the inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e454" xlink:type="simple"/></inline-formula>. The function of the inhibitory neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e455" xlink:type="simple"/></inline-formula> is to shunt the active EPSP caused by a recent spike from the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e456" xlink:type="simple"/></inline-formula> when the value of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e457" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e458" xlink:type="simple"/></inline-formula> changes from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e459" xlink:type="simple"/></inline-formula> to another value.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g005" xlink:type="simple"/>
        </fig>
        <sec id="s2c1">
          <title>Implementation with auxiliary neurons and dendritic branches (Implementation 4)</title>
          <p>A salient difference to the Markov blanket expansion and Implementation 2 arises from the fact that the r.h.s. of the factor expansion (13) contains an additional summation over all factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e460" xlink:type="simple"/></inline-formula> that contain the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e461" xlink:type="simple"/></inline-formula>. This entails that the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e462" xlink:type="simple"/></inline-formula> has to sum up inputs from several groups of auxiliary neurons, one for each factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e463" xlink:type="simple"/></inline-formula>. Hence in contrast to Implementation 2, where the principal neuron fired whenever one of the associated auxiliary neurons fired, we now aim at satisfying the NCC by making sure that the membrane potential of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e464" xlink:type="simple"/></inline-formula> approximates at any moment in time the r.h.s. of the NCC (4). One can achieve this by making sure that each auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e465" xlink:type="simple"/></inline-formula> fires immediately when the presynaptic principal neurons assume state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e466" xlink:type="simple"/></inline-formula> and by having a synaptic connection between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e467" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e468" xlink:type="simple"/></inline-formula> with a synaptic efficacy equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e469" xlink:type="simple"/></inline-formula> from (13). Some imprecision of the sampling may arise when the value of variables in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e470" xlink:type="simple"/></inline-formula> changes, while EPSPs caused by an earlier value of these variables have not yet vanished at the soma of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e471" xlink:type="simple"/></inline-formula>. This problem can be solved if the firing of the auxiliary neuron caused by the new value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e472" xlink:type="simple"/></inline-formula> shunts such EPSP, that had been caused by the preceding value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e473" xlink:type="simple"/></inline-formula>, directly in the corresponding dendrite. This shunting inhibition should have minimal effect on the membrane potential at the soma of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e474" xlink:type="simple"/></inline-formula>. Therefore excitatory synaptic inputs from different auxiliary neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e475" xlink:type="simple"/></inline-formula> (that cause a depolarization by an amount <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e476" xlink:type="simple"/></inline-formula> at the soma) should arrive on different dendritic branches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e477" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e478" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1002294-g005">Fig. 5</xref>), that also have connections from associated inhibitory neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e479" xlink:type="simple"/></inline-formula>.</p>
          <p><xref ref-type="fig" rid="pcbi-1002294-g005">Fig. 5</xref> shows the resulting implementation for the same explaining away motif of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref> as the preceding <xref ref-type="fig" rid="pcbi-1002294-g002">figures 2</xref> and <xref ref-type="fig" rid="pcbi-1002294-g004">4</xref>. Note that the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e480" xlink:type="simple"/></inline-formula> occurs there only in a single factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e481" xlink:type="simple"/></inline-formula>, such that the previously mentioned summation of EPSPs from auxiliary neurons that arise from different factors cannot be demonstrated in this example.</p>
        </sec>
        <sec id="s2c2">
          <title>Implementation with dendritic computation (Implementation 5)</title>
          <p>The last neural implementation that we consider is an adaptation of Implementation 3 (the implementation with dendritic computation, that uses the Markov blanket expansion of the log-odd ratio) to the factorized expansion of the log-odd ratio. In this case each principal neuron, instead of having all its dendritic branches corresponding to different value assignments to the RVs of the Markov blanket, has several groups of dendritic branches, where each group corresponds to the linear expansion of one factor in the log-odd ratio in (13). <xref ref-type="fig" rid="pcbi-1002294-g006">Fig. 6</xref> shows the complete spiking neural network that samples from the Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref>. The principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e482" xlink:type="simple"/></inline-formula> has the same structure and connectivity as in Implementation 3 (see <xref ref-type="fig" rid="pcbi-1002294-g004">Fig. 4</xref>), since the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e483" xlink:type="simple"/></inline-formula> participates in only one factor, and the set of variables other than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e484" xlink:type="simple"/></inline-formula> in this factor constitute the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e485" xlink:type="simple"/></inline-formula>. The same is true for the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e486" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e487" xlink:type="simple"/></inline-formula>. As the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e488" xlink:type="simple"/></inline-formula> occurs in two factors, the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e489" xlink:type="simple"/></inline-formula> has two groups of dendritic branches, 4 for the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e490" xlink:type="simple"/></inline-formula> with synaptic input from the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e491" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e492" xlink:type="simple"/></inline-formula>, and 2 for the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e493" xlink:type="simple"/></inline-formula> with synaptic inputs from the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e494" xlink:type="simple"/></inline-formula>. Note for comparison, that this neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e495" xlink:type="simple"/></inline-formula> needs to have 8 dendritic branches in Implementation 3, one for each assignment of values to the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e496" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e497" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e498" xlink:type="simple"/></inline-formula> in the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e499" xlink:type="simple"/></inline-formula>.</p>
          <fig id="pcbi-1002294-g006" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g006</object-id>
            <label>Figure 6</label>
            <caption>
              <title>Implementation 5 for the Bayesian network shown in <xref ref-type="fig" rid="pcbi-1002294-g001"><bold>Fig. 1B</bold></xref>.</title>
              <p>Implementation 5 is the implementation with dendritic computation that is based on the factorized expansion of the log-odd ratio. RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e500" xlink:type="simple"/></inline-formula> occurs in two factors, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e501" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e502" xlink:type="simple"/></inline-formula>, and therefore <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e503" xlink:type="simple"/></inline-formula> receives synaptic inputs from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e504" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e505" xlink:type="simple"/></inline-formula> on separate groups of dendritic branches. Altogether the synaptic connections of this network of spiking neurons implement the graph structure of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1D</xref>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g006" xlink:type="simple"/>
          </fig>
          <p>The number of dendritic branches of a principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e506" xlink:type="simple"/></inline-formula> in this implementation is the same as the number of auxiliary neurons for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e507" xlink:type="simple"/></inline-formula> in Implementation 4, and is never larger than the number of dendritic branches of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e508" xlink:type="simple"/></inline-formula> in Implementation 3. Although this implementation is more efficient with respect to the required number of dendritic branches, when considering the possible application of STDP for learning in Implementation 3, it has the advantage that it could learn an approximate generative model of the probability distribution of the inputs without knowing apriori the factorization of the probability distribution.</p>
          <p>The amplitude of the dendritic spikes from the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e509" xlink:type="simple"/></inline-formula> of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e510" xlink:type="simple"/></inline-formula> should be equal to the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e511" xlink:type="simple"/></inline-formula> from (13). The index <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e512" xlink:type="simple"/></inline-formula> identifies the two factors that depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e513" xlink:type="simple"/></inline-formula>. The membrane voltage at the soma of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e514" xlink:type="simple"/></inline-formula> is then equal to the sum of the contributions from the dendritic spikes of the active dendritic branches. At time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e515" xlink:type="simple"/></inline-formula> there is exactly one active branch in each of the two groups of dendritic branches. The sum of the contributions from the two active dendritic branches results in a membrane voltage at the soma of the principal neuron that corresponds to the r.h.s of the (13). In the <xref ref-type="sec" rid="s4">Methods</xref> section we provide a general and detailed explanation of this approach.</p>
        </sec>
      </sec>
      <sec id="s2d">
        <title>Probabilistic Inference through Neural Sampling in Larger and More Complex Bayesian Networks</title>
        <p>We have tested the viability of the previously described approach for neural sampling by satisfying the NCC also on two larger and more complex Bayesian networks: the well-known ASIA-network <xref ref-type="bibr" rid="pcbi.1002294-Lauritzen1">[24]</xref>, and an even larger randomly generated Bayesian network. The primary question is in both cases, whether the convergence speed of neural sampling is in a range where a reasonable approximation to probabilistic inference can be provided within the typical range of biological reaction times of a few 100 ms. In addition, we examine for the ASIA-network the question to what extent more complex and biologically more realistic shapes of EPSPs affect the performance. For the larger random Bayesian network we examine what difference in performance is caused by neuron models with absolute versus relative refractory periods.</p>
        <sec id="s2d1">
          <title>Computer Simulation II: ASIA Bayesian network</title>
          <p>The ASIA-network is an example for a larger class of Bayesian networks that are of special interest from the perspective of Cognitive Science <xref ref-type="bibr" rid="pcbi.1002294-Mansinghka1">[25]</xref>. Networks of this type, that consist of 3 types of RVs (context information, true causes, observable symptoms) with directed edges only from one class to the next, capture the causal structure behind numerous domains of human reasoning. The ASIA-network (see <xref ref-type="fig" rid="pcbi-1002294-g007">Fig. 7A</xref>) encodes knowledge about direct influences between environmental factors, 3 specific diseases, and observable symptoms. A concrete distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e516" xlink:type="simple"/></inline-formula> that is compatible with this Bayesian network was specified through conditional probabilities for each node as in <xref ref-type="bibr" rid="pcbi.1002294-Lauritzen1">[24]</xref> (with one small change to avoid deterministic relationship among RVs, see <xref ref-type="sec" rid="s4">Methods</xref>). The binary RVs of the network encode whether a person had a recent visit to Asia (A), whether the person smokes (S), the presence of diseases tuberculosis (T), lung cancer (C), and bronchitis (B), the presence of the symptom dyspnoea (D), and the result of a chest x-ray test (X). This network not only contains multiple “explaining away” effects (i.e., nodes with more than one parent), but also a loop (i.e., undirected cycle) between the RVs S, B, D, C. Hence no probabilistic inference approach based on belief propagation executed directly on this ASIA Bayesian network is guaranteed to work.</p>
          <fig id="pcbi-1002294-g007" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g007</object-id>
            <label>Figure 7</label>
            <caption>
              <title>Results of Computer Simulation II.</title>
              <p>Probabilistic inference in the ASIA network with networks of spiking neurons that use different shapes of EPSPs. The simulated neural networks correspond to Implementation 2. The evidence is changed at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e517" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e518" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e519" xlink:type="simple"/></inline-formula> (by clamping the x-ray test RV to 1). The probabilistic inference query is to estimate marginal posterior probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e520" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e521" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e522" xlink:type="simple"/></inline-formula>. <bold>A</bold>) The ASIA Bayesian network. <bold>B</bold>) The three different shapes of EPSPs, an alpha shape (green curve), a smooth plateau shape (blue curve) and the optimal rectangular shape (red curve). <bold>C</bold>) and <bold>D</bold>) Estimated marginal probabilities for each of the diseases, calculated from the samples generated during the first 800 ms of the simulation with alpha shaped (green bars), plateau shaped (blue bars) and rectangular (red bars) EPSPs, compared with the corresponding correct marginal posterior probabilities (black bars), for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e523" xlink:type="simple"/></inline-formula> in C) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e524" xlink:type="simple"/></inline-formula> in D). The results are averaged over 20 simulations with different random initial conditions. The error bars show the unbiased estimate of the standard deviation. <bold>E</bold>) and <bold>F</bold>) The sum of the Kullback-Leibler divergences between the correct and the estimated marginal posterior probability for each of the diseases using alpha shaped (green curve), plateau shaped (blue curve) and rectangular (red curve) EPSPs, for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e525" xlink:type="simple"/></inline-formula> in E) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e526" xlink:type="simple"/></inline-formula> in F). The results are averaged over 20 simulation trials, and the light green and light blue areas show the unbiased estimate of the standard deviation for the green and blue curves respectively (the standard deviation for the red curve is not shown). The estimated marginal posteriors are calculated at each time point from the gathered samples from the beginning of the simulation (or from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e527" xlink:type="simple"/></inline-formula> for the second inference query with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e528" xlink:type="simple"/></inline-formula>).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g007" xlink:type="simple"/>
          </fig>
          <p>A typical example for probabilistic inference in this network arises when one enters as evidence the facts that the patient visited Asia (A = 1) and has Dyspnoea (D = 1), and asks what is the likelihood of each of the RVs T, C, B that represent the diseases, and how the result of a positive x-ray test would affects these likelihoods.</p>
          <p>We tested this probabilistic inference in a network of spiking neurons according to Implementation 2 with three different shapes of the EPSPs: an alpha EPSP, a plateau EPSP and the optimal rectangular EPSP (See <xref ref-type="fig" rid="pcbi-1002294-g007">Fig. 7B</xref>). These shapes match qualitatively the shapes of EPSPs recorded in the soma of pyramidal neurons for synaptic inputs that arrive on dendritic branches (see <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref> in <xref ref-type="bibr" rid="pcbi.1002294-Williams1">[26]</xref>). The neurons in the spiking neural network had an absolute refractory period. <xref ref-type="fig" rid="pcbi-1002294-g007">Fig. 7C, D</xref> show that the network provides for all three shapes of the EPSPs within 800 ms of simulated biological time quite accurate answers to the tested probabilistic inference query. <xref ref-type="fig" rid="pcbi-1002294-g007">Fig. 7E, F</xref> show that also with smoother shapes of the EPSPs the networks arrive at good heuristic answers within several hundreds of milliseconds. The Kullback-Leibler divergence converges in this case to a small non-zero value, indicating an error caused by the non-ideal sampling process.</p>
          <p><xref ref-type="fig" rid="pcbi-1002294-g008">Fig. 8</xref> shows the spiking activity of the neural network with alpha shaped EPSPs in one of the simulation trials. During the first 3 seconds of the simulation the network alternated between two different modes of spiking activity, that correspond to two different modes of the posterior probability distribution. There are time periods when the principal neuron for the RV X (positive X-ray), T (tuberculosis) and C (lung c.) had a higher firing rate, with time periods in between where they were silent. After <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e529" xlink:type="simple"/></inline-formula>, when the evidence that the x-ray test is positive was introduced, the activity of the network remained in the first mode.</p>
          <fig id="pcbi-1002294-g008" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g008</object-id>
            <label>Figure 8</label>
            <caption>
              <title>Spike raster of the spiking activity in one of the simulation trials described in <xref ref-type="fig" rid="pcbi-1002294-g007"><bold>Fig. 7</bold></xref>.</title>
              <p>The spiking activity is from a simulation trial with the network of spiking neurons with alpha shaped EPSPs. The evidence was switched after 3 s (red vertical line) from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e530" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e531" xlink:type="simple"/></inline-formula> (by clamping the RV X to 1). In each block of rows the lowest spike train shows the activity of a principal neuron (see left hand side for the label of the associated RV), and the spike trains above show the firing activity of the associated auxiliary neurons. After <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e532" xlink:type="simple"/></inline-formula> the activity of the neurons for the x-ray test RV is not shown, since during this period the RV is clamped and the firing rate of its principal neuron is induced externally.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g008" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2d2">
          <title>Computer Simulation III: Randomly generated Bayesian network</title>
          <p>In order to test the performance of neural sampling for an “arbitrary” less structured, and larger graphical model, we generated a random Bayesian network according to the method proposed in <xref ref-type="bibr" rid="pcbi.1002294-Ide1">[27]</xref> (the details of the generation algorithm are given in the Methods section). We added an additional constraint, that the maximum in-degree of the nodes should be not larger than 8. A resulting randomly generated network is shown in <xref ref-type="fig" rid="pcbi-1002294-g009">Fig. 9</xref>. It contains nodes with up to 8 parents, and it also contains numerous loops. For the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e533" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e534" xlink:type="simple"/></inline-formula> we fixed a randomly chosen assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e535" xlink:type="simple"/></inline-formula>. Neural sampling was tested for an ideal neural network that satisfies the NCC with a variety of random initial states, using spiking neurons with an absolute, and alternatively also with a relative refractory period.</p>
          <fig id="pcbi-1002294-g009" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g009</object-id>
            <label>Figure 9</label>
            <caption>
              <title>The randomly generated Bayesian network used in Computer Simulation III.</title>
              <p>It contains 20 nodes. Each node has up to 8 parents. We consider the generic but more difficult instance for probabilistic inference where evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e536" xlink:type="simple"/></inline-formula> is entered for nodes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e537" xlink:type="simple"/></inline-formula> in the lower part of the directed graph. The conditional probability tables were also randomly generated for all RVs.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g009" xlink:type="simple"/>
          </fig>
          <p><xref ref-type="fig" rid="pcbi-1002294-g010">Fig. 10A</xref> shows that in most of our 10 simulations (with different randomly chosen initial states and different random noise throughout the simulation) the sum of Kullback-Leibler divergences for the 12 RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e538" xlink:type="simple"/></inline-formula> becomes quite small within a second. Only in a few trials several seconds were needed for that. <xref ref-type="fig" rid="pcbi-1002294-g010">Fig. 10C and 10D</xref> show the spiking activity of the neural network from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e539" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e540" xlink:type="simple"/></inline-formula> in one of the 10 trials. It is interesting to observe that the network went through a number of network states, each of them characterized by a high firing rate of a particular subset of the neurons.</p>
          <fig id="pcbi-1002294-g010" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.g010</object-id>
            <label>Figure 10</label>
            <caption>
              <title>Results of Computer Simulation III.</title>
              <p>Neural emulation of probabilistic inference through neural sampling in the fairly large and complex randomly chosen Bayesian network shown in <xref ref-type="fig" rid="pcbi-1002294-g009">Fig. 9</xref>. <bold>A</bold>) The sum of the Kullback-Leibler divergences between the correct and the estimated marginal posterior probability for each of the unobserved random variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e541" xlink:type="simple"/></inline-formula>, calculated from the generated samples (spikes) from the beginning of the simulation up to the current time indicated on the x-axis, for simulations with a neuron model with relative refractory period. Separate curves with different colors are shown for each of the 10 trials with different initial conditions (randomly chosen). The bold black curve corresponds to the simulation for which the spiking activity is shown in C) and D). <bold>B</bold>) As in A) but the mean over the 10 trials is shown, for simulations with a neuron model with relative refractory period (solid curve) and absolute refractory period (dashed curve.). The gray area around the solid curve shows the unbiased estimate of the standard deviation calculated over the 10 trials. <bold>C</bold>) and <bold>D</bold>) The spiking activity of the 12 principal neurons during the simulation from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e542" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e543" xlink:type="simple"/></inline-formula>, for one of the 10 simulations (neurons with relative refractory period). The neural network enters and remains in different network states (indicated by different colors), corresponding to different modes of the posterior probability distribution.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.g010" xlink:type="simple"/>
          </fig>
          <p>Similarly spontaneous switchings between internal network states have been reported in numerous biological experiments (see e.g. <xref ref-type="bibr" rid="pcbi.1002294-Abeles1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Miller1">[29]</xref>), but their functional role has remained unknown. In the context of Computer Simulation III these switchings between network states arise because this is the only way how this network of spiking neurons can sample from a multi-modal target distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e544" xlink:type="simple"/></inline-formula>.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have shown through rigorous theoretical arguments and computer simulations that networks of spiking neurons are in principle able to emulate probabilistic inference in general graphical models. The latter has emerged as a quite suitable mathematical framework for describing those computational tasks that artificial and biological intelligent agents need to solve. Hence the results of this article provide a link between this abstract description level of computational theory and models for networks of neurons in the brain. In particular, they provide a principled framework for investigating how nonlinear computational operations in network motifs of cortical microcircuits and in the dendritic trees of neurons contribute to brain computations on a larger scale. Altogether we view our approach as a contribution to the solution of a fundamental open problem that has been raised in Cognitive Science:</p>
      <p>“What approximate algorithms does the mind use, how do they relate to engineering approximations in probabilistic AI, and how are they implemented in neural circuits? Much recent work points to Monte Carlo or stochastic sampling–based approximations as a unifying framework for understanding how Bayesian inference may work practically across all these levels, in minds, brains, and machines ” <xref ref-type="bibr" rid="pcbi.1002294-Tenenbaum1">[13]</xref>.</p>
      <p>We have presented three different theoretical approaches for extending the results of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, such that they yield explanations how probabilistic inference in general graphical models could be carried out through the inherent dynamics of recurrent networks of stochastically firing neurons (neural sampling). The first and simplest one was based on the fact that any distribution can be represented as marginal distribution of a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e545" xlink:type="simple"/></inline-formula> order Boltzmann distribution (5) with auxiliary RVs. However, as we have demonstrated in <xref ref-type="fig" rid="pcbi-1002294-g003">Fig. 3</xref>, this approach yields rather slow convergence of the distribution of network states to the target distribution. This is a natural consequence of the deterministic definition of new RVs in terms of the original RVs, which reduces the conductance <xref ref-type="bibr" rid="pcbi.1002294-Koller1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Levin1">[30]</xref> (i.e., the probability to get from one set of network states to another set of network states) of the Markov chain that is defined by the network dynamics. Further research is needed to clarify whether this deficiency can be overcome through other methods for introducing auxiliary RVs.</p>
      <p>We have furthermore presented two approaches for satisfying the NCC (3) of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, which is a sufficient condition for sampling from a given distribution. These two closely related approaches rely on different ways of expanding the term on the r.h.s. of the NCC (4). The first approach can be used if the underlying graphical model implies that the Markov blankets of all RVs are relatively small. The second approach yields efficient neural emulations under a milder constraint: if each factor in a factorization of the target distribution is rather small (and if there are not too many factors). Each of these two approaches provides the theoretical basis for two different methods for satisfying the NCC in a network of spiking neurons: either through nonlinear computation in network motifs with auxiliary spiking neurons (that do not directly represent a RV of the target distribution), or through dendritic computation in multi-compartment neuron models. This yields altogether four different options for satisfying the NCC in a network of spiking neurons. These four options are demonstrated in <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref>, <xref ref-type="fig" rid="pcbi-1002294-g004">4</xref>–<xref ref-type="fig" rid="pcbi-1002294-g005"/><xref ref-type="fig" rid="pcbi-1002294-g006">6</xref> for a characteristic explaining away motif in the simple Bayesian network of <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref>, that had previously been introduced to model inference in biological visual processing <xref ref-type="bibr" rid="pcbi.1002294-Knill1">[21]</xref>. The second approach for satisfying the NCC never requires more auxiliary neurons or dendritic branches than the first approach.</p>
      <p>Each of these four options for satisfying the NCC would be optimally supported by somewhat different features of the interaction of excitation and inhibition in canonical cortical microcircuit motifs, and by somewhat different features of dendritic computation. Sufficiently precise and general experimental data are not yet available for many of these features, and we hope that the computational consequences of these features that we have exhibited in this article will promote further experimental work on these open questions. In particular, the neural circuit of <xref ref-type="fig" rid="pcbi-1002294-g005">Fig. 5</xref> uses an implementation strategy that requires for many graphical models (those where Markov blankets are substantially larger than individual factors) fewer auxiliary neurons. But it requires temporally precise local inhibition in dendritic branches that has negligible effects on the membrane potential at the soma, or in other dendritic branches that are used for this computation. Some experimental results in this direction are reported in <xref ref-type="bibr" rid="pcbi.1002294-Williams2">[31]</xref>, where it was shown (see e.g. their <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>) that IPSPs from apical dendrites of layer 5 pyramidal neurons are drastically attenuated at the soma. The options that rely on dendritic computation (<xref ref-type="fig" rid="pcbi-1002294-g004">Fig. 4</xref> and <xref ref-type="fig" rid="pcbi-1002294-g006">6</xref>) would be optimally supported if EPSPs from dendritic branches that are not amplified by dendritic spikes have hardly any effect on the membrane potential at the soma. Some experimental results which support this assumption for distal dendritic branches of layer 5 pyramidal neurons had been reported in <xref ref-type="bibr" rid="pcbi.1002294-Williams1">[26]</xref>, see e.g. their <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>. With regard to details of dendritic spikes, these would optimally support the ideal theoretical models with dendritic computation if they would have a rather short duration at the soma, in order to avoid that they still affect the firing probability of the neuron when the state (i.e., firing or non-firing within the preceding time interval of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e546" xlink:type="simple"/></inline-formula>) of presynaptic neurons has changed. In addition, the ideal impact of a dendritic spike on the membrane potential at the soma would approximate a step function (rather than a function with a pronounced peak at the beginning).</p>
      <p>Another desired property of the dendritic spikes in context of our neural implementations is that their propagation from the dendritic branch to the soma should be very fast, i.e. with short delays that are much smaller than the duration of the EPSPs. This is in accordance with the results reported in <xref ref-type="bibr" rid="pcbi.1002294-Ariav1">[32]</xref> where they found (see their <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref>) that the fast active propagation of the dendritic spike towards the soma reduces the rise time of the voltage at the soma to less than a millisecond, in comparison to the 3 ms rise time during the propagation of the individual EPSPs when there is no dendritic spike. Further, in <xref ref-type="bibr" rid="pcbi.1002294-Losonczy1">[22]</xref> it is shown that the latency of an action potential evoked by a strong dendritic spike, calculated with respect to the time of the activation of the synaptic input at the dendritic branch, is slightly below 2 ms, supporting the assumption of fast propagation of the dendritic spike to the soma.</p>
      <p>We have focused in this article on the description of ideal neural emulations of probabilistic inference in general graphical models. These ideal neural implementations use a complete representation of the conditional odd-ratios, i.e. have a separate auxiliary neuron or dendritic branch for each possible assignment of values to the RVs in the Markov blanket in implementations 2 and 3, or in the factor in implementations 4 and 5. Hence, the required number of neurons (or dendritic branches) scales exponentially with the sizes of the Markov blankets and the factors in the probability distribution, and it would quickly become unfeasible to represent probability distributions with larger Markov blankets or factors. One possible way to overcome this limitation is to consider an approximate implementation of the NCC with fewer auxiliary neurons or dendritic branches. In fact, such an approximate implementation of the NCC could be learned. Our results provide the basis for investigating in subsequent work how approximations to these ideal neural emulations could emerge through synaptic plasticity and other adaptive processes in neurons. First explorations of these questions suggest that in particular approximations to Implementations 1,2 and 4 could emerge through STDP in a ubiquitous network motif of cortical microcircuits <xref ref-type="bibr" rid="pcbi.1002294-Douglas1">[33]</xref>: Winner-Take-All circuits formed by populations of pyramidal neurons with lateral inhibition. This learning-based approach relies on the observation that STDP enables pyramidal neurons in the presence of lateral inhibition to specialize each on a particular pattern of presynaptic firing activity, and to fire after learning only when this presynaptic firing pattern appears <xref ref-type="bibr" rid="pcbi.1002294-Nessler1">[34]</xref>. These neurons would then assume the role of the auxiliary neurons, both in the first option with auxiliary RVs, and in the options shown in <xref ref-type="fig" rid="pcbi-1002294-g002">Fig. 2</xref> and <xref ref-type="fig" rid="pcbi-1002294-g005">5</xref>. Furthermore, the results of <xref ref-type="bibr" rid="pcbi.1002294-Legenstein1">[23]</xref> suggest that STDP in combination with branch strength potentiation enables individual dendritic branches to specialize on particular patterns of presynaptic inputs, similarly as in the theoretically optimal constructions of <xref ref-type="fig" rid="pcbi-1002294-g004">Fig. 4</xref> and <xref ref-type="fig" rid="pcbi-1002294-g006">6</xref>. One difference between the theoretically optimal neural emulations and learning based approximations is that auxiliary neurons or dendritic branches learn to represent only the most frequently occurring patterns of presynaptic firing activity, rather than creating a complete catalogue of all theoretically possible presynaptic firing patterns. This has the advantage that fewer auxiliary neurons and dendritic branches are needed in these biologically more realistic learning-based approximations.</p>
      <p>Other ongoing research explores neural emulations of probabilistic inference for non-binary RVs. In this case a stochastic principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e547" xlink:type="simple"/></inline-formula> that represents a binary RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e548" xlink:type="simple"/></inline-formula> is replaced by a Winner-Take-All circuit, that encodes the value of a multinomial or analog RV through population coding, see <xref ref-type="bibr" rid="pcbi.1002294-Nessler1">[34]</xref>.</p>
      <sec id="s3a">
        <title>Related Work</title>
        <p>There are a number of studies proposing neural network architectures that implement probabilistic inference <xref ref-type="bibr" rid="pcbi.1002294-Ackley1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Deneve1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Boerlin1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Hinton1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1002294-Shi1">[48]</xref>. Most of these models propose neural emulations of the belief propagation algorithm, where the activity of neurons or populations of neurons encodes intermediate values (called messages or beliefs) needed in the arithmetical calculation of the posterior probability distribution. With some exceptions <xref ref-type="bibr" rid="pcbi.1002294-Deneve1">[17]</xref>, most of the approaches assume rate-based coding of information and use rate-based neuron models or mean-field approximations.</p>
        <p>In particular, in <xref ref-type="bibr" rid="pcbi.1002294-Litvak1">[37]</xref> a spiking neural network model was developed that performs the max-product message passing algorithm, a variant of belief propagation, where the necessary maximization and product operations were implemented by specialized neural circuits. Another spiking neural implementation of the sum-product belief propagation algorithm was proposed in <xref ref-type="bibr" rid="pcbi.1002294-Steimer1">[36]</xref>, where the calculation and passing of the messages was achieved in a recurrent network of interconnected liquid state machines <xref ref-type="bibr" rid="pcbi.1002294-Maass1">[49]</xref>. In these studies, that implemented probabilistic inference with spiking neurons through emulation of the belief propagation algorithm on tree factor graphs, the beliefs or the messages during the calculation of the posterior distributions were encoded in an average firing rate of a population of neurons. Regarding the complexity of these neural models, as the number of required computational operations in belief propagation is exponential in the size of the largest factor in the probability distribution, in the neural implementations this translates to a number of neurons in the network that scales exponentially with the size of the largest factor. This complexity corresponds to the required number of neurons (or dendritic branches) in implementations 1, 3 and 5 in our approach, whereas implementations 2 and 4 require a larger number of neurons that scales exponentially with the size of the largest Markov blanket in the distribution. Additionally, note that the time of convergence to the correct posterior differs in both approaches: in the belief propagation based models it scales in the worst case linearly with the number of RVs in the probability distribution, whereas in our approach it can vary depending on the probability distribution.</p>
        <p>Although the belief propagation algorithm can be applied to graphical models with undirected loops (a variant called loopy belief propagation), it is not always guaranteed to work, which limits the applicability of the neural implementations based on this algorithm. The computation and the passing of messages in belief propagation uses, however, equivalent computations as the junction tree algorithm <xref ref-type="bibr" rid="pcbi.1002294-Lauritzen1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Shafer1">[50]</xref>, a message passing algorithm that operates on a junction tree, a tree structure derived from the graphical model. The junction tree algorithm performs exact probabilistic inference in general graphical models, including those that have loops. Hence, the neural implementations of belief propagation could in principle be adapted to work on junction trees as well. This however comes at a computational cost manifested in a larger required size of the neural network, since the number of required operations for the junction tree algorithm scales exponentially with the width of the junction tree, and the width of the junction tree can be larger than the size of the largest factor for graphical models that have loops (see <xref ref-type="bibr" rid="pcbi.1002294-Koller1">[9]</xref>, chap. 10 for a discussion). The analysis of the complexity and performance of resulting emulations in networks of spiking neurons is an interesting topic for future research.</p>
        <p>Another interesting approach, that adopts an alternative spike-time based coding scheme, was described in <xref ref-type="bibr" rid="pcbi.1002294-Deneve1">[17]</xref>. In this study a spiking neuron model estimates the log-odd ratio of a hidden binary state in a hidden Markov model, and it outputs a spike only when it receives new evidence from the inputs that causes a shift in the estimated log-odd ratio that exceeds a certain threshold, that is, only when new information about a change in the log-odd ratio is presented that cannot be predicted by the preceding spikes of the neuron. However, this study considers only a very restricted class of graphical models: Bayesian networks that are trees (where for example no explaining away can occur). The ideas in <xref ref-type="bibr" rid="pcbi.1002294-Deneve1">[17]</xref> have been extended in <xref ref-type="bibr" rid="pcbi.1002294-Boerlin1">[18]</xref>, where the neural model is capable of integration of evidence from multiple simultaneous cues (the underlying graphical model is a hidden Markov model with multiple observations). It uses a population code for encoding the log-posterior estimation of the time varying hidden stimulus, which is modeled as a continuous RV instead of the binary hidden state used in <xref ref-type="bibr" rid="pcbi.1002294-Deneve1">[17]</xref>. In these studies, as in ours, spikes times carry relevant information, although there the spikes are generated deterministically and signal a prediction error used to update and correct the estimated log-posterior, whereas in our approach the spikes are generated by a stochastic neuron model and define the current values of the RVs during the sampling.</p>
        <p>The idea that nonlinear dendritic mechanisms could account for the nonlinear processing that is required in neural models that perform probabilistic inference has been proposed previously in <xref ref-type="bibr" rid="pcbi.1002294-Rao3">[39]</xref> and <xref ref-type="bibr" rid="pcbi.1002294-Siegelmann1">[41]</xref>, albeit for the belief propagation algorithm. In <xref ref-type="bibr" rid="pcbi.1002294-Rao3">[39]</xref> the authors introduce a neural model that implements probabilistic inference in hidden Markov models via the belief propagation algorithm, and suggest that the nonlinear functions that arise in the model can be mapped to the nonlinear dendritic filtering. In <xref ref-type="bibr" rid="pcbi.1002294-Siegelmann1">[41]</xref> another rate-based neural model that implements the loopy belief propagation algorithm in general graphical models was described, where the required multiplication operations in the algorithm were proposed to be implemented by the nonlinear processing in individual dendritic trees.</p>
        <p>While there exist several different spiking neural network models in the literature that perform probabilistic inference based on the belief propagation algorithm, there is a lack of spiking neural network models that implement probabilistic inference through Markov chain Monte Carlo (MCMC sampling). To the best of our knowledge, the neural implementations proposed in this article are the only spiking neural networks for probabilistic inference via MCMC in general graphical models. In <xref ref-type="bibr" rid="pcbi.1002294-Hinton1">[35]</xref> a non-spiking neural network composed of stochastic binary neurons was introduced called Boltzmann machine, that performs probabilistic inference via Gibbs sampling. The neural network in <xref ref-type="bibr" rid="pcbi.1002294-Hinton1">[35]</xref> performs inference via sampling in probability distributions that have only pairwise couplings between the RVs. An extension was proposed in <xref ref-type="bibr" rid="pcbi.1002294-Sejnowski1">[51]</xref>, that can perform Gibbs sampling in probability distributions with higher order dependencies between the variables, which corresponds to the class of probability distributions that we consider in this article. A spiking neural network model based on the results in <xref ref-type="bibr" rid="pcbi.1002294-Hinton1">[35]</xref> had been proposed in <xref ref-type="bibr" rid="pcbi.1002294-Hinton2">[52]</xref>, for a restricted class of probability distributions that only have second order factors, and which satisfy some additional constraints on the conditional independencies between the variables. To the best of our knowledge, this approach had not been extended to more general probability distributions.</p>
        <p>A recent study <xref ref-type="bibr" rid="pcbi.1002294-Tkaik1">[53]</xref> showed that as the noise in the neurons increases and their reliability drops, the optimal couplings between the neurons that maximize the information that the network conveys about the inputs become larger in magnitude, creating a redundant code that reduces the impact of noise. Effectively, the network learns the input distribution in its couplings, and uses this knowledge to compensate for errors due to the unreliable neurons. These findings are consistent with our models, and although we did not consider learning in this article, we expect that the introduction of learning mechanisms that optimize a mutual information measure in our neural implementations would yield optimal couplings that obey the same principles as the ones reported in <xref ref-type="bibr" rid="pcbi.1002294-Tkaik1">[53]</xref>. While stochasticity in the neurons represents a crucial property that neural implementations of probabilistic inference through sampling rely on, this study elucidates an important additional effect it has in learning paradigms that use optimality principles like information maximization: it induces redundant representation of information in a population of neurons.</p>
        <p>The existing gap between abstract computational models of information processing in the brain that use MCMC algorithms for probabilistic inference on one hand, and neuroscientific data about neural structures and neural processes on the other hand, has been pointed out and emphasized by several studies <xref ref-type="bibr" rid="pcbi.1002294-Fiser1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Tenenbaum1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Hoyer1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Gershman1">[55]</xref>. The results in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> and in this article propose neural circuit models that aim to bridge this gap, and thereby suggest new means for analyzing data from spike recordings in experimental neuroscience, and for evaluating the more abstract computational models in light of these data. For instance, perceptual multistability in ambiguous visual stimuli and several of its related phenomena were explained through abstract computational models that employ sequential sampling with the Metropolis MCMC algorithm <xref ref-type="bibr" rid="pcbi.1002294-Gershman1">[55]</xref>. In our simulations (see <xref ref-type="fig" rid="pcbi-1002294-g010">Fig. 10</xref>) we showed that a spiking neural network can exhibit multistability, where the state changes from one mode of the posterior distribution to another, even though the Markov chain defined by the neural network does not satisfy the detailed balance property (i.e. it is not a reversible Markov chain) like the Metropolis algorithm.</p>
      </sec>
      <sec id="s3b">
        <title>Experimentally Testable Predictions of our Models</title>
        <p>Our models postulate that knowledge is encoded in the brain in the form of probability distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e549" xlink:type="simple"/></inline-formula>, that are not required to be of the restricted form of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e550" xlink:type="simple"/></inline-formula> order Boltzmann distributions (5). Furthermore they postulate that these distributions are encoded through synaptic weights and neuronal excitabilities, and possibly also through the strength of dendritic branches. Finally, our approach postulates that these learnt and stored probability distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e551" xlink:type="simple"/></inline-formula> are activated through the inherent stochastic dynamics of networks of spiking neurons, using nonlinear features of network motifs and neurons to represent higher order dependencies between RVs. It also predicts that (in contrast to the model of <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>) synaptic connections between neurons are in general not symmetric, because this enables the network to encode higher order factors of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e552" xlink:type="simple"/></inline-formula>.</p>
        <p>The postulate that knowledge is stored in the brain in the form of probability distributions, sampled from by the stochastic dynamics of neural circuits, is consistent with the ubiquitous trial-to-trial variability found in experimental data <xref ref-type="bibr" rid="pcbi.1002294-Dean1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Tolhurst1">[57]</xref>. It has been partially confirmed through more detailed analyses, which show that spontaneous brain activity shows many characteristic features of brain responses to natural external stimuli (<xref ref-type="bibr" rid="pcbi.1002294-Berkes1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Kenet1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Raichle1">[59]</xref>). Further analysis of spontaneous activity is needed in order to verify this prediction. Beyond this prediction regarding spontaneous activity, our approach proposes that fluctuating neuronal responses to external stimuli (or internal goals) represent samples from a conditional marginal distribution, that results from entering evidence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e553" xlink:type="simple"/></inline-formula> for a subset of RVs of the stored distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e554" xlink:type="simple"/></inline-formula> (see (1)). A verification of this prediction requires an analysis of the distributions of network responses –rather than just averaging –for repeated presentations of the same sensory stimulus or task. Similar analyses of human responses to repeated questions have already been carried out in cognitive science <xref ref-type="bibr" rid="pcbi.1002294-Griffiths1">[60]</xref>–<xref ref-type="bibr" rid="pcbi.1002294-Denison1">[62]</xref>, and have been interpreted as evidence that humans respond to queries by sampling from internally stored probability distributions.</p>
        <p>Our resulting model for neural emulations of probabilistic inference predicts, that even strong firing of a single neuron (provided it represents a RV whose value has a strong impact on many other RVs) may drastically change the activity pattern of many other neurons (see the change of network activity after 3 s in <xref ref-type="fig" rid="pcbi-1002294-g008">Fig. 8</xref>, which results from a change in value of the RV that represents “x-ray”). One experimental result of this type had been reported in <xref ref-type="bibr" rid="pcbi.1002294-Li1">[63]</xref>. <xref ref-type="fig" rid="pcbi-1002294-g008">Fig. 8</xref> also suggests that different neurons may have drastically different firing rates, where a few neurons fire a lot, and many others fire rarely. This is a consequence both of different marginal probabilities for different RVs, but also of the quite different computational role and dynamics of neurons that represent RVs (“principal neurons”), and auxiliary neurons that support the realization of the NCC, and which are only activated by a very specific activation patterns of other presynaptic neurons. Such strong differences in the firing activity of neurons has already been found in some experimental studies, see <xref ref-type="bibr" rid="pcbi.1002294-Koulakov1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Yassin1">[65]</xref>. In addition, <xref ref-type="fig" rid="pcbi-1002294-g010">Fig. 10</xref> predicts that recordings from multiple neurons can typically be partitioned into time intervals, where a different firing pattern dominates during each time interval, see <xref ref-type="bibr" rid="pcbi.1002294-Abeles1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002294-Miller1">[29]</xref> for some related experimental data.</p>
        <p>Apart from these more detailed predictions, a central prediction of our model is, that a subset of cortical neurons (the “principal neurons”) represent through their firing activity the current value of different salient RVs. This could be tested, for example, through simultaneous recordings from large numbers of neurons during experiments, where the values of several RVs that are relevant for the subject, and that could potentially be stored in the cortical area from which one records, are changed in a systematic manner.</p>
        <p>It might potentially be more difficult to test, which of the concrete implementations of computational preprocessing for satisfying the NCC that we have proposed, are implemented in some neural tissue. Both the underlying theoretical framework and our computer simulations (see <xref ref-type="fig" rid="pcbi-1002294-g008">Fig. 8</xref>) predict that the auxiliary neurons involved in these local computations are rarely active. More specifically, the model predicts that they only become active when some specific set of presynaptic neurons (whose firing state represents the current value of the RVs in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e555" xlink:type="simple"/></inline-formula>) assumes a specific pattern of firing and non-firing. Implementation 3 and 5 make corresponding predictions for the activity of different dendritic branches of pyramidal neurons, that could potentially be tested through <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e556" xlink:type="simple"/></inline-formula>-imaging.</p>
      </sec>
      <sec id="s3c">
        <title>Conclusion</title>
        <p>We have proposed a new modelling framework for brain computations, based on probabilistic inference through sampling. We have shown through computer simulations, that stochastic networks of spiking neurons can carry out demanding computational tasks within this modelling framework. This framework predicts specific functional roles for nonlinear computations in network motifs and dendritic computation: they support representation of higher order dependencies between salient random variables. On the micro level this framework proposes that local computational operations of neurons superficially resemble logical operations like AND and OR, but that these atomic computational operations are embedded into a stochastic network dynamics. Our framework proposes that the functional role of this stochastic network dynamics can be understood from the perspective of probabilistic inference through sampling from complex learnt probability distributions, that represent the knowledge base of the brain.</p>
      </sec>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Markov Chains</title>
        <p>A Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e557" xlink:type="simple"/></inline-formula> in discrete time is defined by a set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e558" xlink:type="simple"/></inline-formula> of states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e559" xlink:type="simple"/></inline-formula> (we consider for discrete time only the case where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e560" xlink:type="simple"/></inline-formula> has a finite size, denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e561" xlink:type="simple"/></inline-formula>) together with a transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e562" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e563" xlink:type="simple"/></inline-formula> is a conditional probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e564" xlink:type="simple"/></inline-formula> for the next state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e565" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e566" xlink:type="simple"/></inline-formula>, given its preceding state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e567" xlink:type="simple"/></inline-formula>. The Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e568" xlink:type="simple"/></inline-formula> is started in some initial state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e569" xlink:type="simple"/></inline-formula>, and moves through a trajectory of states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e570" xlink:type="simple"/></inline-formula> via iterated application of the stochastic transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e571" xlink:type="simple"/></inline-formula> (more precisely, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e572" xlink:type="simple"/></inline-formula> is the state at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e573" xlink:type="simple"/></inline-formula>, then the next state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e574" xlink:type="simple"/></inline-formula> is drawn from the conditional probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e575" xlink:type="simple"/></inline-formula>. A powerful theorem from probability theory (see e.g. p. 232 in <xref ref-type="bibr" rid="pcbi.1002294-Grimmett1">[5]</xref>) states that if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e576" xlink:type="simple"/></inline-formula> is irreducible (i.e., any state in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e577" xlink:type="simple"/></inline-formula> can be reached from any other state in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e578" xlink:type="simple"/></inline-formula> in finitely many steps with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e579" xlink:type="simple"/></inline-formula>) and aperiodic (i.e., its state transitions cannot be trapped in deterministic cycles), then the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e580" xlink:type="simple"/></inline-formula> was the initial state) converges for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e581" xlink:type="simple"/></inline-formula> to a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e582" xlink:type="simple"/></inline-formula> that does not depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e583" xlink:type="simple"/></inline-formula>. This state distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e584" xlink:type="simple"/></inline-formula> is called the stationary distribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e585" xlink:type="simple"/></inline-formula>. The irreducibility of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e586" xlink:type="simple"/></inline-formula> implies that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e587" xlink:type="simple"/></inline-formula> is the only distribution over the states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e588" xlink:type="simple"/></inline-formula> that is invariant under the transition operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e589" xlink:type="simple"/></inline-formula>, i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e590" xlink:type="simple"/><label>(15)</label></disp-formula>Thus, in order to generate samples from a given distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e591" xlink:type="simple"/></inline-formula>, it suffices to construct an irreducible and aperiodic Markov chain <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e592" xlink:type="simple"/></inline-formula> that leaves <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e593" xlink:type="simple"/></inline-formula> invariant, i.e., satisfies (15). This Markov chain can then be used to carry out probabilistic inference of posterior distributions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e594" xlink:type="simple"/></inline-formula> given an evidence for some of the variables in the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e595" xlink:type="simple"/></inline-formula>. Analogous results hold for Markov chains in continuous time <xref ref-type="bibr" rid="pcbi.1002294-Grimmett1">[5]</xref>, on which we will focus in this article.</p>
      </sec>
      <sec id="s4b">
        <title>Neuron Models</title>
        <p>We use two types of neurons, a stochastic point neuron model as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, and a multi-compartment neuron model.</p>
        <sec id="s4b1">
          <title>Point neuron model</title>
          <p>We use the same point neuron model as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, i.e. stochastic neurons that are formalized in terms of the spike response model <xref ref-type="bibr" rid="pcbi.1002294-Gerstner1">[16]</xref>. In <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> rigorous proofs of the validity of neural sampling were only given for spiking neurons with an absolute refractory period of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e596" xlink:type="simple"/></inline-formula> (the length of a PSP). The same holds for our results. But it was already shown in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> that practically also a variation of the neurons model with a relative refractory period can be used. In this variation of the model one can have a quite arbitrary refractory mechanism modeled with a refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e597" xlink:type="simple"/></inline-formula>, that represents the readiness of the neuron to fire within the refractory period. The firing probability of the neuron model is then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e598" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e599" xlink:type="simple"/></inline-formula> is the time of the last firing of the neuron before time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e600" xlink:type="simple"/></inline-formula>. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e601" xlink:type="simple"/></inline-formula> function usually has value 0 for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e602" xlink:type="simple"/></inline-formula>, meaning that the neuron cannot fire a second spike immediately after it has fired, and its value rises until <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e603" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e604" xlink:type="simple"/></inline-formula>, indicating that after time interval of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e605" xlink:type="simple"/></inline-formula> the neuron fully recovers from its refractory period (this is a slight variation of the definition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e606" xlink:type="simple"/></inline-formula> in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>).</p>
          <p>For a given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e607" xlink:type="simple"/></inline-formula> function that models the refractory mechanism, the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e608" xlink:type="simple"/></inline-formula> in (16) can be obtained as a solution from the equation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e609" xlink:type="simple"/><label>(17)</label></disp-formula>It can be shown that for any continuous function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e610" xlink:type="simple"/></inline-formula> there is a unique continuous function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e611" xlink:type="simple"/></inline-formula> that satisfies this equation (see <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>). The multiplicative refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e612" xlink:type="simple"/></inline-formula> together with a modified firing probability function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e613" xlink:type="simple"/></inline-formula> were derived in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> to ensure that each neuron performs correct local computations and generates correct samples from the desired probability distribution if one assumes that the other neurons do not change their state. This does not guarantee in the general case that the global computation of the network when all neurons operate simultaneously generates correct samples. Nevertheless, as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>, we observed no significant deviations from the correct posteriors in our simulations.</p>
        </sec>
        <sec id="s4b2">
          <title>Multi-compartment neuron model</title>
          <p>For the neural implementations with dendritic computation (Implementations 3 and 5) we used a multi-compartment neuron model which is a modified version of the neuron model introduced in <xref ref-type="bibr" rid="pcbi.1002294-Legenstein1">[23]</xref>. It extends the stochastic point neuron model described above (with separate compartments that represent the dendritic branches) in order to capture the nonlinear effects in the integration of synaptic inputs at the dendritic branches of CA1 pyramidal neurons reported in <xref ref-type="bibr" rid="pcbi.1002294-Losonczy1">[22]</xref> for radial oblique dendrites.</p>
          <p>The local membrane voltage <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e614" xlink:type="simple"/></inline-formula> of the branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e615" xlink:type="simple"/></inline-formula> has a passive component <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e616" xlink:type="simple"/></inline-formula> equal to the summation of the PSPs elicited by the spikes at the local synaptic inputs<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e617" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e618" xlink:type="simple"/></inline-formula> is the synaptic efficacy of input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e619" xlink:type="simple"/></inline-formula> to branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e620" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e621" xlink:type="simple"/></inline-formula> is the postsynaptic potential elicited in the branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e622" xlink:type="simple"/></inline-formula> by the spikes from input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e623" xlink:type="simple"/></inline-formula>. We model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e624" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e625" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e626" xlink:type="simple"/></inline-formula> is the time of the last spike before <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e627" xlink:type="simple"/></inline-formula> that arrived at input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e628" xlink:type="simple"/></inline-formula>. If a synchronous synaptic input from many synapses at one branch exceeds a certain threshold, the membrane voltage at the branch exhibits a sudden jump due to regenerative integration processes resulting in a dendritic spike <xref ref-type="bibr" rid="pcbi.1002294-Losonczy1">[22]</xref>. This nonlinearity is modeled by a second active component <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e629" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e630" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e631" xlink:type="simple"/></inline-formula> denotes the Heaviside step function, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e632" xlink:type="simple"/></inline-formula> is the threshold of branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e633" xlink:type="simple"/></inline-formula>. The branch potential <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e634" xlink:type="simple"/></inline-formula> is equal to the sum of the passive component and the active component caused by the dendritic spike<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e635" xlink:type="simple"/><label>(21)</label></disp-formula></p>
          <p>The passive and active components contribute with a different weighting factor to the membrane potential at the soma. The passive component is conducted passively with a weighting factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e636" xlink:type="simple"/></inline-formula> that models the attenuation of the passive signal. We assume in the neural implementations that the attenuation of the passive signal is strong, i.e. that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e637" xlink:type="simple"/></inline-formula>. The dendritic spike is scaled by the branch strength <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e638" xlink:type="simple"/></inline-formula>. The membrane potential at the soma of the neuron is a sum of the active and passive contributions from all branches<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e639" xlink:type="simple"/><label>(22)</label></disp-formula>The firing probability in this neuron model and its refractory mechanism are the same as for the point neuron model described above. It also can have an arbitrary refractory mechanism defined with the “eadiness to fire”multiplicative function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e640" xlink:type="simple"/></inline-formula> and a modified firing probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e641" xlink:type="simple"/></inline-formula>.</p>
        </sec>
      </sec>
      <sec id="s4c">
        <title>Details to Second Order Boltzmann Distributions with Auxiliary Variables (Implementation 1)</title>
        <p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e642" xlink:type="simple"/></inline-formula> be a probability distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e643" xlink:type="simple"/><label>(23)</label></disp-formula>that contains higher order factors, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e644" xlink:type="simple"/></inline-formula> is a vector of binary RVs. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e645" xlink:type="simple"/></inline-formula> are the factors that depend on one or two RVs, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e646" xlink:type="simple"/></inline-formula> are the higher order factors that depend on more than 2 RVs. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e647" xlink:type="simple"/></inline-formula> is the vector of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e648" xlink:type="simple"/></inline-formula> in the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e649" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e650" xlink:type="simple"/></inline-formula> is the vector of RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e651" xlink:type="simple"/></inline-formula> that the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e652" xlink:type="simple"/></inline-formula> depends on, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e653" xlink:type="simple"/></inline-formula> is the normalization constant. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e654" xlink:type="simple"/></inline-formula> is the number of first and second order factors, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e655" xlink:type="simple"/></inline-formula> is the total number of factors of order 3 or higher. To simplify the notation, in the following we set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e656" xlink:type="simple"/></inline-formula>, since this set of factors in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e657" xlink:type="simple"/></inline-formula> will not be changed in the extended probability distribution.</p>
        <p>Auxiliary RVs are introduced for each of the higher order factors. Specifically, the higher order relation of factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e658" xlink:type="simple"/></inline-formula> is represented by a set of auxiliary binary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e659" xlink:type="simple"/></inline-formula>, where we have a RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e660" xlink:type="simple"/></inline-formula> for each possible assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e661" xlink:type="simple"/></inline-formula> to the RVs in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e662" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e663" xlink:type="simple"/></inline-formula> is the domain of values of the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e664" xlink:type="simple"/></inline-formula>). With the additional sets of RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e665" xlink:type="simple"/></inline-formula> we define a probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e666" xlink:type="simple"/></inline-formula> as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e667" xlink:type="simple"/><label>(24)</label></disp-formula>We denote the ordered set of indices of the RVs that compose the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e668" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e669" xlink:type="simple"/></inline-formula>, i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e670" xlink:type="simple"/><label>(25)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e671" xlink:type="simple"/></inline-formula> denotes the number of indices in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e672" xlink:type="simple"/></inline-formula>.</p>
        <p>The second order factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e673" xlink:type="simple"/></inline-formula> are defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e674" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e675" xlink:type="simple"/></inline-formula> denotes the component of the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e676" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e677" xlink:type="simple"/></inline-formula> that corresponds to the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e678" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e679" xlink:type="simple"/></inline-formula> is the Kronecker-delta function. The factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e680" xlink:type="simple"/></inline-formula> represent a constraint that if the auxiliary RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e681" xlink:type="simple"/></inline-formula> has value 1, then the values of the RVs in the corresponding factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e682" xlink:type="simple"/></inline-formula> must be equal to the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e683" xlink:type="simple"/></inline-formula> that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e684" xlink:type="simple"/></inline-formula> corresponds to. If all components of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e685" xlink:type="simple"/></inline-formula> are zero, then there is not any constraint on the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e686" xlink:type="simple"/></inline-formula> variables. This implies another property: at most one of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e687" xlink:type="simple"/></inline-formula> in the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e688" xlink:type="simple"/></inline-formula>, the one that corresponds to the state of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e689" xlink:type="simple"/></inline-formula>, can have value 1. Hence, the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e690" xlink:type="simple"/></inline-formula> can have two different states. Either all its RVs are zero, or exactly one component <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e691" xlink:type="simple"/></inline-formula> is equal to 1, in which case one has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e692" xlink:type="simple"/></inline-formula>. The probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e693" xlink:type="simple"/></inline-formula> for values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e694" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e695" xlink:type="simple"/></inline-formula> that do not satisfy these constraints is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e696" xlink:type="simple"/></inline-formula>.</p>
        <p>The values of the factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e697" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e698" xlink:type="simple"/></inline-formula> for various assignments of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e699" xlink:type="simple"/></inline-formula> are represented in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e700" xlink:type="simple"/></inline-formula> by first order factors that depend on a single one of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e701" xlink:type="simple"/></inline-formula>. For each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e702" xlink:type="simple"/></inline-formula> we have a new factor with value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e703" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e704" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e705" xlink:type="simple"/></inline-formula> otherwise. We assume that the original factors are first rescaled, such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e706" xlink:type="simple"/></inline-formula> for all values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e707" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e708" xlink:type="simple"/></inline-formula>. We had to modify the values of the new factors by subtracting 1 from the original value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e709" xlink:type="simple"/></inline-formula>, because we introduced an additional zero state for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e710" xlink:type="simple"/></inline-formula> that is consistent with any of the possible assignments of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e711" xlink:type="simple"/></inline-formula>.</p>
        <p>The resulting probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e712" xlink:type="simple"/></inline-formula> consists of first and second order factors.</p>
        <sec id="s4c1">
          <title>Proposition</title>
          <p><italic>The distribution </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e713" xlink:type="simple"/></inline-formula><italic> defined in</italic> (24) <italic>has </italic><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e714" xlink:type="simple"/></inline-formula><italic> as a marginal distribution, i.e. satisfies</italic> (9).</p>
          <p><italic>Proof.</italic> If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e715" xlink:type="simple"/></inline-formula>, then for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e716" xlink:type="simple"/></inline-formula> either <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e717" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e718" xlink:type="simple"/></inline-formula> denotes the zero vector), or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e719" xlink:type="simple"/></inline-formula> has one component <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e720" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e721" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e722" xlink:type="simple"/></inline-formula>. The latter value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e723" xlink:type="simple"/></inline-formula> we denote as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e724" xlink:type="simple"/></inline-formula>. For all other values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e725" xlink:type="simple"/></inline-formula> we have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e726" xlink:type="simple"/></inline-formula>. Hence<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e727" xlink:type="simple"/><label>(27)</label></disp-formula>Further, if we substitute the definition of the factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e728" xlink:type="simple"/></inline-formula> in (24), for pairs of vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e729" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e730" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e731" xlink:type="simple"/></inline-formula> (i.e. when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e732" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e733" xlink:type="simple"/></inline-formula>) we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e734" xlink:type="simple"/><label>(28)</label></disp-formula>Hence we can rewrite (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e735" xlink:type="simple"/></inline-formula>) as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e736" xlink:type="simple"/><label>(29)</label></disp-formula>yielding a proof of (9).</p>
          <p>The resulting spiking neural network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e737" xlink:type="simple"/></inline-formula> consists of principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e738" xlink:type="simple"/></inline-formula>, one for each of the original RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e739" xlink:type="simple"/></inline-formula>, and one principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e740" xlink:type="simple"/></inline-formula> for each of the auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e741" xlink:type="simple"/></inline-formula>. If we assume that the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e742" xlink:type="simple"/></inline-formula> depends on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e743" xlink:type="simple"/></inline-formula>, then the deterministic constraint that governs the relation between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e744" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e745" xlink:type="simple"/></inline-formula> is implemented by very strong excitatory connections <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e746" xlink:type="simple"/></inline-formula> (ideally equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e747" xlink:type="simple"/></inline-formula>) between the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e748" xlink:type="simple"/></inline-formula> and all principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e749" xlink:type="simple"/></inline-formula> for which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e750" xlink:type="simple"/></inline-formula> is 1 in the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e751" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e752" xlink:type="simple"/></inline-formula>. If for the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e753" xlink:type="simple"/></inline-formula> in the corresponding assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e754" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e755" xlink:type="simple"/></inline-formula> the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e756" xlink:type="simple"/></inline-formula> is 0, then there are strong inhibitory connections <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e757" xlink:type="simple"/></inline-formula> (ideally equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e758" xlink:type="simple"/></inline-formula>) through an inhibitory interneuron between neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e759" xlink:type="simple"/></inline-formula> and neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e760" xlink:type="simple"/></inline-formula>. Additionally, each of the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e761" xlink:type="simple"/></inline-formula> has a bias<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e762" xlink:type="simple"/><label>(30)</label></disp-formula>where the function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e763" xlink:type="simple"/></inline-formula> denotes the number of coordinates of the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e764" xlink:type="simple"/></inline-formula> that have value 1. The biases of the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e765" xlink:type="simple"/></inline-formula> and the efficacies of the direct synaptic connections between the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e766" xlink:type="simple"/></inline-formula> that correspond to the second order factors in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e767" xlink:type="simple"/></inline-formula> are determined in the same way as for the spiking neural network structure in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> and depend only on the first and second order factors of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e768" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s4c2">
          <title>Proposition</title>
          <p>
            <italic>The Markov chain represented by the spiking neural network that performs neural sampling in the Boltzmann distribution </italic>
            <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e769" xlink:type="simple"/></inline-formula>
            <italic> is irreducible.</italic>
          </p>
          <p><italic>Proof.</italic> We designate a state of the neural network with the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e770" xlink:type="simple"/></inline-formula>. Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e771" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e772" xlink:type="simple"/></inline-formula> is the refractory variable of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e773" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e774" xlink:type="simple"/></inline-formula> is a vector of all refractory variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e775" xlink:type="simple"/></inline-formula> for the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e776" xlink:type="simple"/></inline-formula> that correspond to the auxiliary RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e777" xlink:type="simple"/></inline-formula>. The latter are defined as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref>. At each spike of a neuron its refractory variable is set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e778" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e779" xlink:type="simple"/></inline-formula> in neural sampling in discrete time is an integer number, that denotes the duration of the PSP in terms of discrete time steps). It decreases by 1 at each subsequent time step, until it reaches 0. We denote the transition operators for the refractory variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e780" xlink:type="simple"/></inline-formula> changing from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e781" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e782" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e783" xlink:type="simple"/></inline-formula>, and changing from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e784" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e785" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e786" xlink:type="simple"/></inline-formula>. For the refractory variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e787" xlink:type="simple"/></inline-formula> the transition operators are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e788" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e789" xlink:type="simple"/></inline-formula>. In the proof we consider the ideal case where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e790" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e791" xlink:type="simple"/></inline-formula>, which can result in infinitely large membrane potentials equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e792" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e793" xlink:type="simple"/></inline-formula>. These values of the membrane potentials forbid the neuron to change the value of its RV, because if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e794" xlink:type="simple"/></inline-formula> then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e795" xlink:type="simple"/></inline-formula>, and if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e796" xlink:type="simple"/></inline-formula> then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e797" xlink:type="simple"/></inline-formula> (see <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> for details), and the neuron is locked to one value of the RV. In all other cases, when the value of the membrane potential remains finite, we have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e798" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e799" xlink:type="simple"/></inline-formula>. In this case the principal neuron can reach any value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e800" xlink:type="simple"/></inline-formula> from any other value in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e801" xlink:type="simple"/></inline-formula> time steps. The same holds for the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e802" xlink:type="simple"/></inline-formula>.</p>
          <p>If we consider now an initial arbitrary non-forbidden state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e803" xlink:type="simple"/></inline-formula>, then each refractory variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e804" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e805" xlink:type="simple"/></inline-formula> is equal to 0, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e806" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e807" xlink:type="simple"/></inline-formula> can be either non-zero or 0. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e808" xlink:type="simple"/></inline-formula> is non-zero then, since the membrane potential of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e809" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e810" xlink:type="simple"/></inline-formula>, which is finite, there is a non-vanishing probability for the network state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e811" xlink:type="simple"/></inline-formula> to change to another state in which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e812" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e813" xlink:type="simple"/></inline-formula> time steps. Therefore we can conclude, that from the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e814" xlink:type="simple"/></inline-formula> we can reach the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e815" xlink:type="simple"/></inline-formula> that has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e816" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e817" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e818" xlink:type="simple"/></inline-formula> time steps with a non-vanishing probability. In this new state all principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e819" xlink:type="simple"/></inline-formula> are allowed to change the value of their RV, because their membrane potentials have finite values determined by the sum of their biases and the efficacies of the synaptic connections from the second order factors. Hence each non-zero <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e820" xlink:type="simple"/></inline-formula> can change its value to 0 in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e821" xlink:type="simple"/></inline-formula> time steps. From this it follows that from any non-forbidden state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e822" xlink:type="simple"/></inline-formula> we can reach the zero state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e823" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e824" xlink:type="simple"/></inline-formula> time steps with non-vanishing probability.</p>
          <p>We proceed in a similar manner to prove that from the zero state we can reach any other non-forbidden state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e825" xlink:type="simple"/></inline-formula>. First we observe that from the zero state the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e826" xlink:type="simple"/></inline-formula> can change their states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e827" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e828" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e829" xlink:type="simple"/></inline-formula> time steps, since they all have finite membrane potentials, i.e. we can reach the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e830" xlink:type="simple"/></inline-formula>. From the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e831" xlink:type="simple"/></inline-formula> there is non-vanishing probability that the Markov chain goes in the next <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e832" xlink:type="simple"/></inline-formula> time steps through a sequence of subsequent states that all have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e833" xlink:type="simple"/></inline-formula>. If the Markov chain follows such a sequence, then the state after exactly <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e834" xlink:type="simple"/></inline-formula> time steps has also <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e835" xlink:type="simple"/></inline-formula>. Additionally, if the Markov chain goes through such a sequence of states, at each of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e836" xlink:type="simple"/></inline-formula> time steps after the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e837" xlink:type="simple"/></inline-formula> the principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e838" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e839" xlink:type="simple"/></inline-formula> will have finite membrane potentials equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e840" xlink:type="simple"/></inline-formula>. Therefore, there is non-vanishing probability that they change their states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e841" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e842" xlink:type="simple"/></inline-formula> in exactly <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e843" xlink:type="simple"/></inline-formula> steps. Hence, we have shown that we can reach the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e844" xlink:type="simple"/></inline-formula> from the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e845" xlink:type="simple"/></inline-formula> in exactly <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e846" xlink:type="simple"/></inline-formula> number of states. This concludes the proof that we can reach any non-forbidden state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e847" xlink:type="simple"/></inline-formula> from any other other non-forbidden state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e848" xlink:type="simple"/></inline-formula> in at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e849" xlink:type="simple"/></inline-formula> steps with non-vanishing probability, i.e. the Markov chain is irreducible.</p>
        </sec>
      </sec>
      <sec id="s4d">
        <title>Details to Implementation 2</title>
        <p>In this neural implementation each principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e850" xlink:type="simple"/></inline-formula> has a dedicated preprocessing layer of auxiliary neurons with lateral inhibition. All neurons in the network are stochastic point neuron models.</p>
        <p>The auxiliary neurons for the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e851" xlink:type="simple"/></inline-formula> receive as inputs the outputs of the principal neurons corresponding to all RVs in the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e852" xlink:type="simple"/></inline-formula>. The number of auxiliary excitatory neurons that connect to the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e853" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e854" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e855" xlink:type="simple"/></inline-formula> is the number of elements of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e856" xlink:type="simple"/></inline-formula>), and we index these neurons with all possible assignments of values to the RVs in the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e857" xlink:type="simple"/></inline-formula>. Thus, for each state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e858" xlink:type="simple"/></inline-formula> of values at the inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e859" xlink:type="simple"/></inline-formula> we have a corresponding auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e860" xlink:type="simple"/></inline-formula>. The realization of the NCC is achieved by a specific connectivity between the inputs and the auxiliary neurons and appropriate values for the intrinsic excitabilities of the auxiliary neurons, such that at each moment in time only the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e861" xlink:type="simple"/></inline-formula> corresponding to the current state of the inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e862" xlink:type="simple"/></inline-formula>, if it is not inhibited by the lateral inhibition due to a recent spike from another auxiliary neuron, fires with a probability density as demanded by the NCC (3):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e863" xlink:type="simple"/><label>(31)</label></disp-formula>During the time when the state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e864" xlink:type="simple"/></inline-formula> of the inputs is active, the other auxiliary neurons are either strongly inhibited, or do not receive enough excitatory input to reach a significant firing probability.</p>
        <p>The inputs connect to the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e865" xlink:type="simple"/></inline-formula> either with a direct strong excitatory connection, or through an inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e866" xlink:type="simple"/></inline-formula> that connects to the auxiliary neuron. The inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e867" xlink:type="simple"/></inline-formula> fires whenever any of the principal neurons of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e868" xlink:type="simple"/></inline-formula> that connect to it fires. The auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e869" xlink:type="simple"/></inline-formula> receives synaptic connections according to the following rule: if the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e870" xlink:type="simple"/></inline-formula> assigns a value of 1 to the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e871" xlink:type="simple"/></inline-formula> in the Markov blanket <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e872" xlink:type="simple"/></inline-formula>, then the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e873" xlink:type="simple"/></inline-formula> connects to the neuron with a strong excitatory synaptic efficacy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e874" xlink:type="simple"/></inline-formula>, whereas if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e875" xlink:type="simple"/></inline-formula> assigns a value of 0 to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e876" xlink:type="simple"/></inline-formula> then the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e877" xlink:type="simple"/></inline-formula> connects to the inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e878" xlink:type="simple"/></inline-formula>. Thus, whenever <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e879" xlink:type="simple"/></inline-formula> fires, the inhibitory interneuron fires and prevents the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e880" xlink:type="simple"/></inline-formula> to fire for a time period <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e881" xlink:type="simple"/></inline-formula>. We will assume that the synaptic efficacy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e882" xlink:type="simple"/></inline-formula> is much larger than the log-odd ratio value of the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e883" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e884" xlink:type="simple"/></inline-formula> according to the r.h.s. of (3). We set the bias of the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e885" xlink:type="simple"/></inline-formula> equal to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e886" xlink:type="simple"/><label>(32)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e887" xlink:type="simple"/></inline-formula> gives the number of components of the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e888" xlink:type="simple"/></inline-formula> that are 1.</p>
        <p>If the value of the inputs at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e889" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e890" xlink:type="simple"/></inline-formula>, and none of the neurons fired in the time interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e891" xlink:type="simple"/></inline-formula>, then for an auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e892" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e893" xlink:type="simple"/></inline-formula> there are two possibilities. Either there exists a component of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e894" xlink:type="simple"/></inline-formula> that is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e895" xlink:type="simple"/></inline-formula> and its corresponding input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e896" xlink:type="simple"/></inline-formula>, in which case the principal neuron of the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e897" xlink:type="simple"/></inline-formula> connects to the inhibitory interneuron<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e898" xlink:type="simple"/></inline-formula> and inhibits <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e899" xlink:type="simple"/></inline-formula>. Or one has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e900" xlink:type="simple"/></inline-formula> in which case the number of active inputs that connect to neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e901" xlink:type="simple"/></inline-formula> do not provide enough excitatory input to reach the high threshold for firing. In this case the firing probability of the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e902" xlink:type="simple"/></inline-formula> is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e903" xlink:type="simple"/><label>(33)</label></disp-formula>and because of the strong synaptic efficacies of the excitatory connections equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e904" xlink:type="simple"/></inline-formula>, which are by definition much larger than the log-odd ratio of the RV <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e905" xlink:type="simple"/></inline-formula>, it is approximately equal to 0. Hence, only the neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e906" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e907" xlink:type="simple"/></inline-formula> has a non-vanishing firing probability equal to (31).</p>
        <p>The lateral inhibition between the auxiliary neurons is implemented through a common inhibitory circuit to which they all connect. The role of the lateral inhibition is to enforce the necessary refractory period of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e908" xlink:type="simple"/></inline-formula> after any of the auxiliary neurons fires. When an auxiliary neuron fires, the inhibitory circuit is active during the duration of the EPSP (equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e909" xlink:type="simple"/></inline-formula>), and strongly inhibits the other neurons, preventing them from firing. The auxiliary neurons connect to the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e910" xlink:type="simple"/></inline-formula> with an excitatory connection strong enough to drive it to fire a spike whenever any one of them fires. During the time when the state of the input variables satisfies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e911" xlink:type="simple"/></inline-formula>, the firing probability of the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e912" xlink:type="simple"/></inline-formula> satisfies the NCC (3). This implies that the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e913" xlink:type="simple"/></inline-formula> satisfies the NCC as well.</p>
        <p>Introducing an evidence of a known value of a RV in this model is achieved by driving the principal neuron with an external excitatory input to fire a spike train with a high firing rate when the observed value of the RV is 1, or by inhibiting the principal neuron with an external inhibitory input so that it remains silent when the observed value of the RV is 0.</p>
      </sec>
      <sec id="s4e">
        <title>Details to Implementation 3</title>
        <p>We assume that the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e914" xlink:type="simple"/></inline-formula> has a separate dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e915" xlink:type="simple"/></inline-formula> for each possible assignment of values to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e916" xlink:type="simple"/></inline-formula>, and that the principal neurons corresponding to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e917" xlink:type="simple"/></inline-formula> in the Markov blanket <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e918" xlink:type="simple"/></inline-formula> connect to these dendritic branches.</p>
        <p>It is well known that synchronous activation of several synapses at one branch, if it exceeds a certain threshold, causes the membrane voltage at the branch to exhibit a sudden jump resulting from a dendritic spike. Furthermore the amplitude of such dendritic spike is subject to plasticity <xref ref-type="bibr" rid="pcbi.1002294-Losonczy1">[22]</xref>. We use a neuron model according to <xref ref-type="bibr" rid="pcbi.1002294-Legenstein1">[23]</xref>, that is based on these experimental data. The details of this multi-compartment neuron model were presented in the preceding subsection of Methods on Neuron Models. We assume in this model that the contribution of each dendritic branch to the soma membrane voltage is predominantly due to dendritic spikes, and that the passive conductance to the soma can be neglected. Thus, according to (22), the membrane potential at the soma is equal to the sum of the nonlinear active components contributed from each of the branches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e919" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e920" xlink:type="simple"/><label>(34)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e921" xlink:type="simple"/></inline-formula> is the nonlinear contribution from branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e922" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e923" xlink:type="simple"/></inline-formula> is the strength of branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e924" xlink:type="simple"/></inline-formula> (see <xref ref-type="bibr" rid="pcbi.1002294-Losonczy1">[22]</xref> for experimental data on branch strengths). <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e925" xlink:type="simple"/></inline-formula> is the target value of the membrane potential in the absence of any synaptic input. The nonlinear active component (dendritic spike) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e926" xlink:type="simple"/></inline-formula> is assumed to be equal to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e927" xlink:type="simple"/><label>(35)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e928" xlink:type="simple"/></inline-formula> denotes the Heaviside step function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e929" xlink:type="simple"/></inline-formula> is the local activation, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e930" xlink:type="simple"/></inline-formula> is the threshold of branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e931" xlink:type="simple"/></inline-formula>. The amplitude of the total contribution of branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e932" xlink:type="simple"/></inline-formula> to the membrane potential at the soma is then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e933" xlink:type="simple"/></inline-formula>.</p>
        <p>As can be seen in <xref ref-type="fig" rid="pcbi-1002294-g004">Fig. 4</xref>, the connectivity from the inputs to the dendritic branches is analogous as in Implementation 2 with auxiliary neurons: from each principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e934" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e935" xlink:type="simple"/></inline-formula> is in the Markov blanket of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e936" xlink:type="simple"/></inline-formula> there is a direct synaptic connection to the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e937" xlink:type="simple"/></inline-formula> if the assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e938" xlink:type="simple"/></inline-formula> assigns to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e939" xlink:type="simple"/></inline-formula> the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e940" xlink:type="simple"/></inline-formula>, or a connection to the inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e941" xlink:type="simple"/></inline-formula> in case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e942" xlink:type="simple"/></inline-formula> assigns the value 0 to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e943" xlink:type="simple"/></inline-formula>. The inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e944" xlink:type="simple"/></inline-formula> connects to its corresponding branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e945" xlink:type="simple"/></inline-formula>, and fires whenever any of the principal neurons that connect to it fire. The synaptic efficacies of the direct synaptic connections are assumed to satisfy the condition<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e946" xlink:type="simple"/><label>(36)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e947" xlink:type="simple"/></inline-formula> is the set of indices of principal neurons <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e948" xlink:type="simple"/></inline-formula> that directly connect to the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e949" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e950" xlink:type="simple"/></inline-formula> is the efficacy of the synaptic connection to the branch from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e951" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e952" xlink:type="simple"/></inline-formula> is the threshold at the dendritic branch for triggering a dendritic spike. Additionally, each synaptic weight <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e953" xlink:type="simple"/></inline-formula> should also satisfy the condition<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e954" xlink:type="simple"/><label>(37)</label></disp-formula>The same condition applies also for the efficacy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e955" xlink:type="simple"/></inline-formula> of the synaptic connection from inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e956" xlink:type="simple"/></inline-formula> to the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e957" xlink:type="simple"/></inline-formula>.</p>
        <p>These conditions ensure that if the current state of the inputs is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e958" xlink:type="simple"/></inline-formula>, then the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e959" xlink:type="simple"/></inline-formula> will have an active dendritic spike, whereas all other dendritic branches will not receive enough total synaptic input to trigger a dendritic spike. The amplitude of the dendritic spike from branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e960" xlink:type="simple"/></inline-formula> at the soma is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e961" xlink:type="simple"/><label>(38)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e962" xlink:type="simple"/></inline-formula> is a positive constant that is larger than all possible negative values of the log-odd ratio. If the steady value of the membrane potential is equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e963" xlink:type="simple"/></inline-formula>, then we have at each moment a membrane potential that is equal to the sum of the amplitude of the nonlinear contribution of the single active dendritic branch and the steady value of the membrane potential, which yields the expression for the NCC (4).</p>
      </sec>
      <sec id="s4f">
        <title>Details to the Implementation 4</title>
        <p>In this implementation a principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e964" xlink:type="simple"/></inline-formula> has a separate group of auxiliary neurons for each factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e965" xlink:type="simple"/></inline-formula> that depends on the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e966" xlink:type="simple"/></inline-formula>. The group of auxiliary neurons for the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e967" xlink:type="simple"/></inline-formula> receives inputs from the principal neurons that correspond to the set of the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e968" xlink:type="simple"/></inline-formula> that factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e969" xlink:type="simple"/></inline-formula> depends on, but without <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e970" xlink:type="simple"/></inline-formula>. For each possible assignment of values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e971" xlink:type="simple"/></inline-formula> to the inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e972" xlink:type="simple"/></inline-formula>, there is an auxiliary neuron in the group for the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e973" xlink:type="simple"/></inline-formula>, which we will denote with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e974" xlink:type="simple"/></inline-formula>. The neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e975" xlink:type="simple"/></inline-formula> spikes immediately when the state of the inputs switches to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e976" xlink:type="simple"/></inline-formula> from another state, i.e. the spike marks the moment of the state change. This can be achieved by setting the bias of the neuron similarly as in (32) to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e977" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e978" xlink:type="simple"/></inline-formula> is the number of components of the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e979" xlink:type="simple"/></inline-formula> that are equal to 1, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e980" xlink:type="simple"/></inline-formula> is the efficacy of the direct synaptic connections from the principal neurons to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e981" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e982" xlink:type="simple"/></inline-formula> is a constant that ensures high firing probability of this neuron when the current value of the inputs is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e983" xlink:type="simple"/></inline-formula>.</p>
        <p>The connectivity from the auxiliary neurons to the principal neuron keeps the soma membrane voltage of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e984" xlink:type="simple"/></inline-formula> equal to the log-odd ratio of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e985" xlink:type="simple"/></inline-formula> ( = r.h.s. of (4)). From each auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e986" xlink:type="simple"/></inline-formula> there is one excitatory connection to the principal neuron, terminating at a separate dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e987" xlink:type="simple"/></inline-formula>. The efficacy of this synaptic connection is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e988" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e989" xlink:type="simple"/></inline-formula> is the parameter from (13), and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e990" xlink:type="simple"/></inline-formula> is a constant that shifts all these synaptic efficacies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e991" xlink:type="simple"/></inline-formula> into the positive range.</p>
        <p>Additionally, there is an inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e992" xlink:type="simple"/></inline-formula> connecting to the same dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e993" xlink:type="simple"/></inline-formula>. The inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e994" xlink:type="simple"/></inline-formula> receives input from all other auxiliary neurons in the same sub-circuit as the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e995" xlink:type="simple"/></inline-formula>, but not from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e996" xlink:type="simple"/></inline-formula>. The purpose of this inhibitory neuron is to shunt the active EPSP when the inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e997" xlink:type="simple"/></inline-formula> change their state from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e998" xlink:type="simple"/></inline-formula> to another state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e999" xlink:type="simple"/></inline-formula>. Namely, at the time moment when the inputs change to state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1000" xlink:type="simple"/></inline-formula>, the corresponding auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1001" xlink:type="simple"/></inline-formula> will fire, and this will cause firing of the inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1002" xlink:type="simple"/></inline-formula>. A spike of the inhibitory interneuron should have just a local effect: to shunt the active EPSP caused by the previous state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1003" xlink:type="simple"/></inline-formula> at the dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1004" xlink:type="simple"/></inline-formula>. If there is not any active EPSP, this spike of the inhibitory interneuron should not affect the membrane potential at the soma of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1005" xlink:type="simple"/></inline-formula>.</p>
        <p>At any time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1006" xlink:type="simple"/></inline-formula>, the group of auxiliary neurons for the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1007" xlink:type="simple"/></inline-formula> contributes one EPSP to the principal neuron, through the synaptic input originating from the auxiliary neuron that corresponds to the current state of the inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1008" xlink:type="simple"/></inline-formula>. The amplitude of the EPSP from the sub-circuit that corresponds to the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1009" xlink:type="simple"/></inline-formula> is equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1010" xlink:type="simple"/></inline-formula>. If we assume that the bias of the soma membrane potential is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1011" xlink:type="simple"/></inline-formula>, then the total membrane potential at the soma of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1012" xlink:type="simple"/></inline-formula> is equal to:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1013" xlink:type="simple"/><label>(39)</label></disp-formula>which is equal to the expression on the r.h.s. of (13) when one assumes that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1014" xlink:type="simple"/></inline-formula>. Hence, the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1015" xlink:type="simple"/></inline-formula> satisfies the NCC.</p>
      </sec>
      <sec id="s4g">
        <title>Details to the Implementation 5</title>
        <p>In this implementation each principal neuron is a multi-compartment neuron of the same type as in Implementation 3, with a separate group of dendritic branches for each factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1016" xlink:type="simple"/></inline-formula> in the probability distribution that depends on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1017" xlink:type="simple"/></inline-formula>. In the group <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1018" xlink:type="simple"/></inline-formula> (corresponding to factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1019" xlink:type="simple"/></inline-formula>) there is a dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1020" xlink:type="simple"/></inline-formula> for each assignment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1021" xlink:type="simple"/></inline-formula> to the variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1022" xlink:type="simple"/></inline-formula> that the factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1023" xlink:type="simple"/></inline-formula> depends on (without <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1024" xlink:type="simple"/></inline-formula>). The dendritic branches in group <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1025" xlink:type="simple"/></inline-formula> receive synaptic inputs from the principal neurons that correspond to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1026" xlink:type="simple"/></inline-formula>. Each dendritic branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1027" xlink:type="simple"/></inline-formula> can contribute a component <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1028" xlink:type="simple"/></inline-formula> to the soma membrane voltage <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1029" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1030" xlink:type="simple"/></inline-formula> is like in Implementation 3 the branch strength of this branch), but only if the local activation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1031" xlink:type="simple"/></inline-formula> in the branch exceeds the threshold for triggering a dendritic spike. The connectivity from the principal neurons corresponding to the RVs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1032" xlink:type="simple"/></inline-formula> to the dendritic branches of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1033" xlink:type="simple"/></inline-formula> in the group <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1034" xlink:type="simple"/></inline-formula> is such so that at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1035" xlink:type="simple"/></inline-formula> only the dendritic branch corresponding to the current state of the inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1036" xlink:type="simple"/></inline-formula> receives total synaptic input that crosses the local threshold for generating a dendritic spike and initiates a dendritic spike. This is realized with the same connectivity pattern from the inputs to the branches as in Implementation 3 depicted in <xref ref-type="fig" rid="pcbi-1002294-g004">Fig. 4</xref>. The amplitude of the dendritic spike of branch <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1037" xlink:type="simple"/></inline-formula> at the soma should be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1038" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1039" xlink:type="simple"/></inline-formula> is the parameter from (13) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1040" xlink:type="simple"/></inline-formula> is chosen as in Implementation 3.</p>
        <p>The membrane voltage at the soma of the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1041" xlink:type="simple"/></inline-formula> is then equal to the sum of the dendritic spikes from the active dendritic branches. At time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1042" xlink:type="simple"/></inline-formula> there is exactly one active branch in each group of dendritic branches, the one which corresponds to the current state of the inputs. If we additionally assume that the bias of neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1043" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1044" xlink:type="simple"/></inline-formula>, then the membrane voltage at the soma has the desired value (39).</p>
      </sec>
      <sec id="s4h">
        <title>Details to Computer Simulations</title>
        <sec id="s4h1">
          <title>Details to Computer Simulation I</title>
          <p>The simulations with the neural network that corresponds to the approach where the firing of the principal neurons satisfies the NCC were performed with the ideal version of the implementations 2–5, which assumes using rectangular PSPs and no delays in the synaptic connections. In the simulation with the neural network that corresponds to Implementation 1, the network was also implemented with the ideal version of neural sampling. In both cases the duration of the rectangular PSPs was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1045" xlink:type="simple"/></inline-formula> and the neurons had absolute refractory period of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1046" xlink:type="simple"/></inline-formula>. The simulations lasted for 6 seconds biological time, where in the first 3 seconds the RV for the contour (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1047" xlink:type="simple"/></inline-formula>) was clamped to 1 and in the second 3 seconds clamped to 0. For each spiking neural network 10 simulation trials were performed, each time with different randomly chosen initial state. The values of the synaptic efficacies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1048" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1049" xlink:type="simple"/></inline-formula> in the simulation of Implementation 1 were set to 10 times the largest value of any of the factors in the probability distribution. This ensures that a neuron with active input from a synapse with efficacy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1050" xlink:type="simple"/></inline-formula> will have a very high membrane potential and will continuously stay active regardless of the state of the other inputs, and accordingly a neuron with active input from a synapse with efficacy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1051" xlink:type="simple"/></inline-formula> will remain silent regardless of the state of the other inputs. The time step in the simulations was set to 1 ms.</p>
          <p>The values for the conditional probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1052" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1053" xlink:type="simple"/></inline-formula> in the Bayesian network from <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1</xref> used in these simulations are given in <xref ref-type="table" rid="pcbi-1002294-t001">Table 1</xref>. The prior probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1054" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1055" xlink:type="simple"/></inline-formula> were both equal to 0.5.</p>
          <table-wrap id="pcbi-1002294-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.t001</object-id><label>Table 1</label><caption>
              <title>Values for the conditional probabilities in the Bayesian network in <xref ref-type="fig" rid="pcbi-1002294-g001">Fig. 1B</xref> used in Computer Simulation I.</title>
            </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002294-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.t001" xlink:type="simple"/><table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1056" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1057" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1058" xlink:type="simple"/></inline-formula>
                  </td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1059" xlink:type="simple"/></inline-formula> = 0</td>
                  <td align="left" colspan="1" rowspan="1">0.15</td>
                  <td align="left" colspan="1" rowspan="1">0.85</td>
                  <td align="left" colspan="1" rowspan="1">0.15</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1060" xlink:type="simple"/></inline-formula> = 1</td>
                  <td align="left" colspan="1" rowspan="1">0.85</td>
                  <td align="left" colspan="1" rowspan="1">0.15</td>
                  <td align="left" colspan="1" rowspan="1">0.85</td>
                </tr>
              </tbody>
            </table></alternatives></table-wrap>
        </sec>
        <sec id="s4h2">
          <title>Details to Computer Simulation II</title>
          <p>The conditional probability tables of the ASIA-network used in the simulations are given in <xref ref-type="table" rid="pcbi-1002294-t002">Tables 2</xref>,<xref ref-type="table" rid="pcbi-1002294-t003">3</xref>,<xref ref-type="table" rid="pcbi-1002294-t004">4</xref> and <xref ref-type="table" rid="pcbi-1002294-t005">5</xref>. We modified the original network from <xref ref-type="bibr" rid="pcbi.1002294-Lauritzen1">[24]</xref> by eliminating the “tuberculosis or cancer?” RV in order to get it in suitable form to be able to perform neural sampling in it. In the original ASIA network the “tuberculosis or cancer?” RV had deterministic links with the RVs “tuberculosis?” and “cancer?” which results in a Markov chain that is not connected. The model captures the following qualitative medical knowledge facts:</p>
          <list list-type="order">
            <list-item>
              <p>Shortness of breath or dyspnoea may be due to tuberculosis, lung cancer or bronchitis, none of them or many of them at the same time.</p>
            </list-item>
            <list-item>
              <p>A recent visit to Asia increases the chance for tuberculosis.</p>
            </list-item>
            <list-item>
              <p>Smoking is a risk factor for both lung cancer and bronchitis.</p>
            </list-item>
            <list-item>
              <p>Tuberculosis and lung cancer significantly increase the chances of a positive chest x-ray test.</p>
            </list-item>
          </list>
          <table-wrap id="pcbi-1002294-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.t002</object-id><label>Table 2</label><caption>
              <title>Values for the probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1061" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1063" xlink:type="simple"/></inline-formula> in the ASIA Bayesian network used in Computer Simulation II.</title>
            </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002294-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.t002" xlink:type="simple"/><table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1064" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1065" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1066" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1067" xlink:type="simple"/></inline-formula>
                  </td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">0.01</td>
                  <td align="left" colspan="1" rowspan="1">0.5</td>
                  <td align="left" colspan="1" rowspan="1">0.01</td>
                  <td align="left" colspan="1" rowspan="1">0.05</td>
                </tr>
              </tbody>
            </table></alternatives></table-wrap>
          <table-wrap id="pcbi-1002294-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.t003</object-id><label>Table 3</label><caption>
              <title>Values for the conditional probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1069" xlink:type="simple"/></inline-formula> in the ASIA Bayesian network used in Computer Simulation II.</title>
            </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002294-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.t003" xlink:type="simple"/><table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1070" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1071" xlink:type="simple"/></inline-formula>
                  </td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">S = 0</td>
                  <td align="left" colspan="1" rowspan="1">0.3</td>
                  <td align="left" colspan="1" rowspan="1">0.01</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">S = 1</td>
                  <td align="left" colspan="1" rowspan="1">0.6</td>
                  <td align="left" colspan="1" rowspan="1">0.10</td>
                </tr>
              </tbody>
            </table></alternatives></table-wrap>
          <table-wrap id="pcbi-1002294-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.t004</object-id><label>Table 4</label><caption>
              <title>Values for the conditional probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1072" xlink:type="simple"/></inline-formula> in the ASIA Bayesian network used in Computer Simulation II.</title>
            </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002294-t004-4" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.t004" xlink:type="simple"/><table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1073" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">C = 0</td>
                  <td align="left" colspan="1" rowspan="1">C = 1</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">T = 0</td>
                  <td align="left" colspan="1" rowspan="1">0.05</td>
                  <td align="left" colspan="1" rowspan="1">0.98</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">T = 1</td>
                  <td align="left" colspan="1" rowspan="1">0.98</td>
                  <td align="left" colspan="1" rowspan="1">0.98</td>
                </tr>
              </tbody>
            </table></alternatives></table-wrap>
          <table-wrap id="pcbi-1002294-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002294.t005</object-id><label>Table 5</label><caption>
              <title>Values for the conditional probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1074" xlink:type="simple"/></inline-formula> in the ASIA Bayesian network used in Computer Simulation II.</title>
            </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002294-t005-5" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.t005" xlink:type="simple"/><table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1">
                    <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1075" xlink:type="simple"/></inline-formula>
                  </td>
                  <td align="left" colspan="1" rowspan="1">T = 0</td>
                  <td align="left" colspan="1" rowspan="1">T = 1</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">C = 0, B = 0</td>
                  <td align="left" colspan="1" rowspan="1">0.1</td>
                  <td align="left" colspan="1" rowspan="1">0.7</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">C = 0, B = 1</td>
                  <td align="left" colspan="1" rowspan="1">0.8</td>
                  <td align="left" colspan="1" rowspan="1">0.9</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">C = 1, B = 0</td>
                  <td align="left" colspan="1" rowspan="1">0.7</td>
                  <td align="left" colspan="1" rowspan="1">0.7</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">C = 1, B = 1</td>
                  <td align="left" colspan="1" rowspan="1">0.9</td>
                  <td align="left" colspan="1" rowspan="1">0.9</td>
                </tr>
              </tbody>
            </table></alternatives></table-wrap>
          <p>We used a point neuron model as in <xref ref-type="bibr" rid="pcbi.1002294-Buesing1">[1]</xref> described in the Introduction section of this article, where the membrane potential of the neuron is a linear sum of the PSPs elicited by the input spikes. We performed all simulations with three different shapes for the EPSPs. The first EPSP was an alpha shaped EPSP curve <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1076" xlink:type="simple"/></inline-formula> defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1077" xlink:type="simple"/><label>(40)</label></disp-formula>where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1078" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1079" xlink:type="simple"/></inline-formula> are the points in time where the alpha kernel <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1080" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1081" xlink:type="simple"/></inline-formula> = 2.3 is a scaling factor and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1082" xlink:type="simple"/></inline-formula> is the time constant of the alpha kernel. The second used EPSP was a plateau shaped curve <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1083" xlink:type="simple"/></inline-formula> defined with the following equation<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1084" xlink:type="simple"/><label>(41)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1085" xlink:type="simple"/></inline-formula>. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1086" xlink:type="simple"/></inline-formula> defines the duration of the rise of the EPSP kernel after an input spike, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1087" xlink:type="simple"/></inline-formula> determines the duration of part of the EPSP curve corresponding to the fall of the EPSP back to the baseline, modeled here with the sine function, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1088" xlink:type="simple"/></inline-formula> is a scaling factor. The third shape of the EPSP that we used was the theoretically optimal rectangular shape with duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1089" xlink:type="simple"/></inline-formula>. In all simulations for each of the three different shapes of EPSPs we used the same duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1090" xlink:type="simple"/></inline-formula> to calculate the generated samples from the spike times according to (2). All neurons had an absolute refractory period of duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1091" xlink:type="simple"/></inline-formula>. The time step in the simulations was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1092" xlink:type="simple"/></inline-formula>.</p>
          <p>The indirect connections going through inhibitory interneurons from the principal neurons to the auxiliary neurons were modeled as direct connections with negative synaptic efficacies and IPSPs that match the shape of the EPSPs described above. All connections in the network had delays equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1093" xlink:type="simple"/></inline-formula>. The excitatory synaptic weight from the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1094" xlink:type="simple"/></inline-formula> to an auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1095" xlink:type="simple"/></inline-formula> was set to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1096" xlink:type="simple"/><label>(42)</label></disp-formula>and the synaptic weight for the inhibitory synaptic connection from the principal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1097" xlink:type="simple"/></inline-formula> to an auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1098" xlink:type="simple"/></inline-formula> (which models the indirect inhibitory connection through the inhibitory interneuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1099" xlink:type="simple"/></inline-formula>) was set to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1100" xlink:type="simple"/><label>(43)</label></disp-formula></p>
          <p>The efficacy of the synaptic connections from the auxiliary neurons to their principal neuron were set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1101" xlink:type="simple"/></inline-formula>. The lateral inhibition was implemented by a single inhibitory neuron that receives excitatory connections from all auxiliary neurons with synaptic efficacy equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1102" xlink:type="simple"/></inline-formula>. The inhibitory neuron connected back to all auxiliary neurons and these synaptic connections had rectangular shaped IPSPs with duration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1103" xlink:type="simple"/></inline-formula>. These rectangular IPSPs approximate the effect that a circuit of fast-spiking bursting inhibitory neurons with short IPSPs would have on the membrane potential of the auxiliary neurons. The efficacy of the synaptic connection from the inhibitory neuron for the lateral inhibition to the auxiliary neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1104" xlink:type="simple"/></inline-formula> was set equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1105" xlink:type="simple"/></inline-formula> in the previous equation. The bias of the principal neurons were set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1106" xlink:type="simple"/></inline-formula> and the biases of the auxiliary neurons were set according to (32). The inhibitory interneuron for the lateral inhibition had bias <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1107" xlink:type="simple"/></inline-formula>.</p>
          <p>The evidence about known RVs in the neural network was introduced by injected constant current in the corresponding principal neurons of amplitude <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1108" xlink:type="simple"/></inline-formula> if the value of the RV is 1 and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1109" xlink:type="simple"/></inline-formula> if the value of the RV is 0. The simulations were performed for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1110" xlink:type="simple"/></inline-formula> biological time. For the separate cases of each EPSP shape the results were averaged over 20 simulation trials with different initial states of the spiking neural network and different random noise throughout the simulation. The initial states were randomly chosen from the prior distribution of the ASIA network which corresponds to a random state in the activity of the spiking network when no evidence is introduced. For control we performed the same simulations with randomly chosen initial states from a uniform distribution, and the results showed slightly slower convergence (data not shown). The initial states were set by injecting constant current pulse in the principal neurons for the unknown RVs at the beginning of the simulation, with amplitude <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1111" xlink:type="simple"/></inline-formula> ( <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1112" xlink:type="simple"/></inline-formula> ) if the value of the RV in the initial state is 1 (0), and duration equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1113" xlink:type="simple"/></inline-formula>.</p>
          <p>The simulations in Computer Simulation II were performed with the PCSIM simulator for neural circuits (web site: <ext-link ext-link-type="uri" xlink:href="http://www.igi.tugraz.at/pcsim" xlink:type="simple">http://www.igi.tugraz.at/pcsim</ext-link>) <xref ref-type="bibr" rid="pcbi.1002294-Pecevski1">[66]</xref>.</p>
        </sec>
        <sec id="s4h3">
          <title>Details to Computer Simulation III</title>
          <p>The simulations were performed with the ideal implementation of the NCC, which corresponds to using rectangular PSPs and zero delays in the synaptic connections in the implementations 2–5. We performed 10 simulations with an implementation that uses the neuron model with relative refractory period and another 10 simulations with an implementation that uses the neuron model with absolute refractory period. The duration of the PSPs was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1114" xlink:type="simple"/></inline-formula> The time step of the simulation was 1 ms.</p>
          <p>The Bayesian network in this simulation was randomly generated with a variation of the Markov chain Monte Carlo sampling algorithm proposed in <xref ref-type="bibr" rid="pcbi.1002294-Ide1">[27]</xref>. Instead of allowing arcs in the Bayesian network in both directions between the nodes and checking at each new iteration whether the generated Bayesian network graph is acyclic like in <xref ref-type="bibr" rid="pcbi.1002294-Ide1">[27]</xref>, we preserved an ordering of the nodes in the graph and allow an edge from the node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1115" xlink:type="simple"/></inline-formula> to the node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1116" xlink:type="simple"/></inline-formula> only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1117" xlink:type="simple"/></inline-formula>. We started with a simple connected graph where each node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1118" xlink:type="simple"/></inline-formula>, except for the first node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1119" xlink:type="simple"/></inline-formula>, has connection from node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1120" xlink:type="simple"/></inline-formula>. We then performed the following MCMC iterations.</p>
          <list list-type="order">
            <list-item>
              <p>Choose randomly a pair of nodes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1121" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1122" xlink:type="simple"/></inline-formula>;</p>
            </list-item>
            <list-item>
              <p>If there is an edge from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1123" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1124" xlink:type="simple"/></inline-formula> then remove the edge if the Bayesian network remains connected, otherwise keep the same Bayesian network from the previous iteration;</p>
            </list-item>
            <list-item>
              <p>If there is not an edge, then create an edge from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1125" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1126" xlink:type="simple"/></inline-formula> if the node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1127" xlink:type="simple"/></inline-formula> has less than 8 parents, otherwise keep the Bayesian network from the previous iteration.</p>
            </list-item>
          </list>
          <p>Similarly to the proofs in <xref ref-type="bibr" rid="pcbi.1002294-Ide1">[27]</xref>, one can prove that the stationary distribution of the above Markov chain is a uniform distribution over all valid Bayesian networks that satisfy the constraint that a node can not have more than 8 parents. To generate the Bayesian network used in the simulations we performed 500000 iterations of the above Markov chain. The conditional probability distributions for the Bayesian network were sampled from Dirichlet distributions with priors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1128" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1129" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1130" xlink:type="simple"/></inline-formula>.</p>
          <p>In the simulations that use a neuron model with a relative refractory mechanism, we used the following form for the refractory function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1131" xlink:type="simple"/></inline-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1132" xlink:type="simple"/><label>(44)</label></disp-formula>The corresponding function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1133" xlink:type="simple"/></inline-formula> for the firing probability was calculated by numerically solving the equation (17) for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002294.e1134" xlink:type="simple"/></inline-formula> defined in (44).</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank Robert Legenstein for helpful comments.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002294-Buesing1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name><name name-style="western"><surname>Bill</surname><given-names>J</given-names></name><name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>             <year>2011</year>             <article-title>Neural dynamics as sampling: A model for stochastic computation in recurrent networks of spiking neurons.</article-title>             <source>PLoS Comput Biol</source>             <volume>7</volume>             <fpage>e1002211</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Pearl1">
        <label>2</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J</given-names></name></person-group>             <year>1988</year>             <source>Probabilistic Reasoning in Intelligent Systems</source>             <publisher-loc>San Francisco, CA</publisher-loc>             <publisher-name>Morgan- Kaufmann</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Berkes1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name><name name-style="western"><surname>Orbán</surname><given-names>G</given-names></name><name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name></person-group>             <year>2011</year>             <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment.</article-title>             <source>Science</source>             <volume>331</volume>             <fpage>83</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Gold1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2007</year>             <article-title>The neural basis of decision making.</article-title>             <source>Annu Rev Neuroscience</source>             <volume>30</volume>             <fpage>535</fpage>             <lpage>574</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Grimmett1">
        <label>5</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Grimmett</surname><given-names>GR</given-names></name><name name-style="western"><surname>Stirzaker</surname><given-names>DR</given-names></name></person-group>             <year>2001</year>             <source>Probability and Random Processes. 3rd edition</source>             <publisher-name>Oxford University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Neal1">
        <label>6</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Neal</surname><given-names>RM</given-names></name></person-group>             <year>1993</year>             <article-title>Probabilistic inference using Markov chain Monte Carlo methods.</article-title>             <comment>Technical report, University of Toronto, Department of Computer Science</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Andrieu1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Andrieu</surname><given-names>C</given-names></name><name name-style="western"><surname>Freitas</surname><given-names>ND</given-names></name><name name-style="western"><surname>Doucet</surname><given-names>A</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name></person-group>             <year>2003</year>             <article-title>An introduction to MCMC for machine learning.</article-title>             <source>Mach Learn</source>             <volume>50</volume>             <fpage>5</fpage>             <lpage>43</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Bishop1">
        <label>8</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>CM</given-names></name></person-group>             <year>2007</year>             <article-title>Pattern Recognition and Machine Learning (Information Science and Statistics).</article-title>             <comment>1st edition, 2006 corr 2nd printing edition. Springer</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Koller1">
        <label>9</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koller</surname><given-names>D</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name></person-group>             <year>2009</year>             <source>Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning)</source>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Rao1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <source>Probabilistic Models of the Brain</source>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Doya1">
        <label>11</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>S</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <year>2007</year>             <source>Bayesian Brain: Probabilistic Approaches to Neural Coding</source>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Fiser1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name><name name-style="western"><surname>Orbán</surname><given-names>G</given-names></name><name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name></person-group>             <year>2010</year>             <article-title>Statistically optimal perception and learning: from behavior to neural representations.</article-title>             <source>Trends Cogn Sci</source>             <volume>14</volume>             <fpage>119</fpage>             <lpage>130</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Tenenbaum1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name><name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Goodman</surname><given-names>ND</given-names></name></person-group>             <year>2011</year>             <article-title>How to grow a mind: Statistics, structure, and abstraction.</article-title>             <source>Science</source>             <volume>331</volume>             <fpage>1279</fpage>             <lpage>1285</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Toussaint1">
        <label>14</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Toussaint</surname><given-names>M</given-names></name><name name-style="western"><surname>Goerick</surname><given-names>C</given-names></name></person-group>             <year>2010</year>             <article-title>A Bayesian view on motor control and planning.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Sigaud</surname><given-names>O</given-names></name><name name-style="western"><surname>Peters</surname><given-names>J</given-names></name></person-group>             <source>From motor to interaction learning in robots</source>             <publisher-name>Studies in Computational Intelligence. Springer</publisher-name>             <fpage>227</fpage>             <lpage>252</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Ackley1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ackley</surname><given-names>DH</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>1985</year>             <article-title>A learning algorithm for Boltzmann machines.</article-title>             <source>Cogn Sci</source>             <volume>9</volume>             <fpage>147</fpage>             <lpage>169</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Gerstner1">
        <label>16</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>Kistler</surname><given-names>WM</given-names></name></person-group>             <year>2002</year>             <source>Spiking Neuron Models</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Deneve1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name></person-group>             <year>2008</year>             <article-title>Bayesian spiking neurons I: Inference.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>91</fpage>             <lpage>117</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Boerlin1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Boerlin</surname><given-names>M</given-names></name><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name></person-group>             <year>2011</year>             <article-title>Spike-based population coding and working memory.</article-title>             <source>PLoS Comput Biol</source>             <volume>7</volume>             <fpage>e1001080</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Jolivet1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jolivet</surname><given-names>R</given-names></name><name name-style="western"><surname>Rauch</surname><given-names>A</given-names></name><name name-style="western"><surname>Lüscher</surname><given-names>H</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name></person-group>             <year>2006</year>             <article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models.</article-title>             <source>J Comput Neurosci</source>             <volume>21</volume>             <fpage>35</fpage>             <lpage>49</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Kersten1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kersten</surname><given-names>D</given-names></name><name name-style="western"><surname>Yuille</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Bayesian models of object perception.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>13</volume>             <fpage>150</fpage>             <lpage>158</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Knill1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Knill</surname><given-names>DC</given-names></name><name name-style="western"><surname>Kersten</surname><given-names>D</given-names></name></person-group>             <year>1991</year>             <article-title>Apparent surface curvature affects lightness perception.</article-title>             <source>Nature</source>             <volume>351</volume>             <fpage>228</fpage>             <lpage>230</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Losonczy1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Losonczy</surname><given-names>A</given-names></name><name name-style="western"><surname>Makara</surname><given-names>JK</given-names></name><name name-style="western"><surname>Magee</surname><given-names>JC</given-names></name></person-group>             <year>2008</year>             <article-title>Compartmentalized dendritic plasticity and input feature storage in neurons.</article-title>             <source>Nature</source>             <volume>452</volume>             <fpage>436</fpage>             <lpage>441</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Legenstein1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>             <year>2011</year>             <article-title>Branch-specific plasticity enables self-organization of nonlinear computation in single neurons.</article-title>             <source>J Neurosci</source>             <volume>31</volume>             <fpage>10787</fpage>             <lpage>10802</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Lauritzen1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lauritzen</surname><given-names>SL</given-names></name><name name-style="western"><surname>Spiegelhalter</surname><given-names>DJ</given-names></name></person-group>             <year>1988</year>             <article-title>Local computations with probabilities on graphical structures and their application to expert systems.</article-title>             <source>J R Stat Soc Ser B Stat Methodol</source>             <volume>50</volume>             <fpage>157</fpage>             <lpage>224</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Mansinghka1">
        <label>25</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mansinghka</surname><given-names>VK</given-names></name><name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group>             <year>2006</year>             <article-title>Structured priors for structure learning.</article-title>             <fpage>324</fpage>             <lpage>331</lpage>             <comment>In: Proceedings of the Proceedings of the Twenty-Second Conference Annual Conference on Uncertainty in Artificial Intelligence; 13–16 July 2006; Cambridge, Massachusetts, United States. UAI-06. AUAI Press</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Williams1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>SR</given-names></name><name name-style="western"><surname>Stuart</surname><given-names>GJ</given-names></name></person-group>             <year>2002</year>             <article-title>Dependence of EPSP efficacy on synapse location in neocortical pyramidal neurons.</article-title>             <source>Science</source>             <volume>295</volume>             <fpage>1907</fpage>             <lpage>1910</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Ide1">
        <label>27</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ide</surname><given-names>J</given-names></name><name name-style="western"><surname>Cozman</surname><given-names>F</given-names></name></person-group>             <year>2002</year>             <article-title>Random generation of Bayesian networks.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Bittencourt</surname><given-names>G</given-names></name><name name-style="western"><surname>Ramalho</surname><given-names>G</given-names></name></person-group>             <source>Advances in Artificial Intelligence</source>             <publisher-loc>Berlin/Heidelberg</publisher-loc>             <publisher-name>Springer. volume 2507</publisher-name>             <fpage>366</fpage>             <lpage>376</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Abeles1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name><name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name><name name-style="western"><surname>Gat</surname><given-names>I</given-names></name><name name-style="western"><surname>Meilijson</surname><given-names>I</given-names></name><name name-style="western"><surname>Seidemann</surname><given-names>E</given-names></name><etal/></person-group>             <year>1995</year>             <article-title>Cortical activity flips among quasi-stationary states.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>92</volume>             <fpage>8616</fpage>             <lpage>8620</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Miller1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>P</given-names></name><name name-style="western"><surname>Katz</surname><given-names>DB</given-names></name></person-group>             <year>2010</year>             <article-title>Stochastic transitions between neural states in taste processing and decision-making.</article-title>             <source>J Neurosci</source>             <volume>30</volume>             <fpage>2559</fpage>             <lpage>2570</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Levin1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Levin</surname><given-names>DA</given-names></name><name name-style="western"><surname>Peres</surname><given-names>Y</given-names></name><name name-style="western"><surname>Wilmer</surname><given-names>EL</given-names></name></person-group>             <year>2008</year>             <article-title>Markov Chains and Mixing Times.</article-title>             <source>American Mathematical Society</source>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Williams2">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>SR</given-names></name><name name-style="western"><surname>Stuart</surname><given-names>GJ</given-names></name></person-group>             <year>2003</year>             <article-title>Voltage- and site-dependent control of the somatic impact of dendritic IPSPs.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>7358</fpage>             <lpage>7367</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Ariav1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ariav</surname><given-names>G</given-names></name><name name-style="western"><surname>Polsky</surname><given-names>A</given-names></name><name name-style="western"><surname>Schiller</surname><given-names>J</given-names></name></person-group>             <year>2003</year>             <article-title>Submillisecond precision of the input-output transformation function mediated by fast sodium dendritic spikes in basal dendrites of CA1 pyramidal neurons.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>7750</fpage>             <lpage>7758</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Douglas1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name></person-group>             <year>2004</year>             <article-title>Neuronal circuits of the neocortex.</article-title>             <source>Annu Rev Neurosci</source>             <volume>27</volume>             <fpage>419</fpage>             <lpage>451</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Nessler1">
        <label>34</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name><name name-style="western"><surname>Pfeiffer</surname><given-names>M</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>             <year>2009</year>             <article-title>STDP enables spiking neurons to detect hidden causes of their inputs.</article-title>             <fpage>1357</fpage>             <lpage>1365</lpage>             <comment>In: Advances in Neural Information Processing Systems 22</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Hinton1">
        <label>35</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>1986</year>             <article-title>Learning and relearning in Boltzmann machines.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Rumelhart</surname><given-names>DE</given-names></name><name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name></person-group>             <source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1</source>             <publisher-name>MIT Press</publisher-name>             <fpage>282</fpage>             <lpage>317</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Steimer1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Steimer</surname><given-names>A</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Belief propagation in networks of spiking neurons.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>2502</fpage>             <lpage>2523</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Litvak1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Litvak</surname><given-names>S</given-names></name><name name-style="western"><surname>Ullman</surname><given-names>S</given-names></name></person-group>             <year>2009</year>             <article-title>Cortical circuitry implementing graphical models.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>3010</fpage>             <lpage>3056</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Rao2">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <year>2004</year>             <article-title>Bayesian computation in recurrent neural circuits.</article-title>             <source>Neural Comput</source>             <volume>16</volume>             <fpage>1</fpage>             <lpage>38</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Rao3">
        <label>39</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <year>2007</year>             <article-title>Neural models of Bayesian belief propagation.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name><name name-style="western"><surname>Ishii</surname><given-names>S</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name></person-group>             <source>Bayesian Brain</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT-Press</publisher-name>             <fpage>239</fpage>             <lpage>267</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Bobrowski1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bobrowski</surname><given-names>O</given-names></name><name name-style="western"><surname>Meir</surname><given-names>R</given-names></name><name name-style="western"><surname>Eldar</surname><given-names>YC</given-names></name></person-group>             <year>2009</year>             <article-title>Bayesian filtering in spiking neural networks: Noise, adaptation, and multisensory integration.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>1277</fpage>             <lpage>1320</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Siegelmann1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Siegelmann</surname><given-names>HT</given-names></name><name name-style="western"><surname>Holzman</surname><given-names>LE</given-names></name></person-group>             <year>2010</year>             <article-title>Neuronal integration of dynamic sources: Bayesian learning and Bayesian inference.</article-title>             <source>Chaos</source>             <volume>20</volume>             <fpage>037112</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Beck1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2007</year>             <article-title>Exact inferences in a neural implementation of a hidden Markov model.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>1344</fpage>             <lpage>1361</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Rao4">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name><name name-style="western"><surname>Ballard</surname><given-names>DH</given-names></name></person-group>             <year>1999</year>             <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.</article-title>             <source>Nat Neurosci</source>             <volume>2</volume>             <fpage>79</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Ma1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2008</year>             <article-title>Spiking networks for Bayesian inference and choice.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>18</volume>             <fpage>217</fpage>             <lpage>222</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Ma2">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2006</year>             <article-title>Bayesian inference with probabilistic population codes.</article-title>             <source>Nat Neurosci</source>             <volume>9</volume>             <fpage>1432</fpage>             <lpage>1438</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Deneve2">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2001</year>             <article-title>Efficient computation and cue integration with noisy population codes.</article-title>             <source>Nat Neurosci</source>             <volume>4</volume>             <fpage>826</fpage>             <lpage>831</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Yu1">
        <label>47</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name></person-group>             <year>2005</year>             <article-title>Inference, attention, and decision in a Bayesian neural architecture.</article-title>             <source>Advances in Neural Information Processing Systems 17</source>             <publisher-name>MIT Press</publisher-name>             <fpage>1577</fpage>             <lpage>1584</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Shi1">
        <label>48</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shi</surname><given-names>L</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name></person-group>             <year>2009</year>             <article-title>Neural implementation of hierarchical Bayesian inference by importance sampling.</article-title>             <source>Advances in Neural Information Processing Systems 22</source>             <publisher-name>MIT Press</publisher-name>             <fpage>1669</fpage>             <lpage>1677</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Maass1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Natschlaeger</surname><given-names>T</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>             <year>2002</year>             <article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations.</article-title>             <source>Neural Comput</source>             <volume>14</volume>             <fpage>2531</fpage>             <lpage>2560</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Shafer1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shafer</surname><given-names>GR</given-names></name><name name-style="western"><surname>Shenoy</surname><given-names>PP</given-names></name></person-group>             <year>1990</year>             <article-title>Probability propagation.</article-title>             <source>Ann Math Artif Intell</source>             <volume>2</volume>             <fpage>327</fpage>             <lpage>351</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Sejnowski1">
        <label>51</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>1987</year>             <article-title>Higher-order Boltzmann machines.</article-title>             <fpage>398</fpage>             <lpage>403</lpage>             <comment>In: AIP Conference Proceedings 151 on Neural Networks for Computing</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Hinton2">
        <label>52</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Brown</surname><given-names>AD</given-names></name></person-group>             <year>2000</year>             <article-title>Spiking Boltzmann machines.</article-title>             <source>Advances in Neural Information Processing Systems 12</source>             <publisher-name>MIT Press</publisher-name>             <fpage>122</fpage>             <lpage>129</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Tkaik1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name><name name-style="western"><surname>Prentice</surname><given-names>JS</given-names></name><name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name><name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name></person-group>             <year>2010</year>             <article-title>Optimal population coding by noisy spiking neurons.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>107</volume>             <fpage>14419</fpage>             <lpage>14424</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Hoyer1">
        <label>54</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hoyer</surname><given-names>PO</given-names></name><name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Interpreting neural response variability as Monte Carlo sampling of the posterior.</article-title>             <source>Advances in Neural Information Processing Systems 15</source>             <publisher-name>MIT Press</publisher-name>             <fpage>277</fpage>             <lpage>284</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Gershman1">
        <label>55</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Vul</surname><given-names>E</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>Perceptual multistability as Markov chain Monte Carlo inference.</article-title>             <source>Advances in Neural Information Processing Systems 22</source>             <publisher-name>MIT Press</publisher-name>             <fpage>611</fpage>             <lpage>619</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Dean1">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dean</surname><given-names>AF</given-names></name></person-group>             <year>1981</year>             <article-title>The variability of discharge of simple cells in the cat striate cortex.</article-title>             <source>Exp Brain Res</source>             <volume>44</volume>             <fpage>437</fpage>             <lpage>440</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Tolhurst1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tolhurst</surname><given-names>D</given-names></name><name name-style="western"><surname>Movshon</surname><given-names>J</given-names></name><name name-style="western"><surname>Dean</surname><given-names>A</given-names></name></person-group>             <year>1983</year>             <article-title>The statistical reliability of signals in single neurons in cat and monkey visual cortex.</article-title>             <source>Vision Res</source>             <volume>23</volume>             <fpage>775</fpage>             <lpage>785</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Kenet1">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name><name name-style="western"><surname>Bibitchkov</surname><given-names>D</given-names></name><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name><name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name><name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Spontaneously emerging cortical representations of visual attributes.</article-title>             <source>Nature</source>             <volume>425</volume>             <fpage>954</fpage>             <lpage>956</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Raichle1">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Raichle</surname><given-names>ME</given-names></name></person-group>             <year>2010</year>             <article-title>Two views of brain function.</article-title>             <source>Trends Cogn Sci</source>             <volume>14</volume>             <fpage>180</fpage>             <lpage>190</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Griffiths1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group>             <year>2006</year>             <article-title>Optimal Predictions in Everyday Cognition.</article-title>             <source>Psychol Sci</source>             <volume>17</volume>             <fpage>767</fpage>             <lpage>773</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Vul1">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vul</surname><given-names>E</given-names></name><name name-style="western"><surname>Pashler</surname><given-names>H</given-names></name></person-group>             <year>2008</year>             <article-title>Measuring the crowd within: Probabilistic representations within individuals.</article-title>             <source>Psychol Sci</source>             <volume>19</volume>             <fpage>645</fpage>             <lpage>647</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Denison1">
        <label>62</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Denison</surname><given-names>S</given-names></name><name name-style="western"><surname>Bonawitz</surname><given-names>E</given-names></name><name name-style="western"><surname>Gopnik</surname><given-names>A</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name></person-group>             <year>2010</year>             <article-title>Preschoolers sample from probability distributions.</article-title>             <fpage>2272</fpage>             <lpage>2277</lpage>             <comment>In: Proc. of the 32nd Annual Conference of the Cognitive Science Society</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Li1">
        <label>63</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>CT</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2009</year>             <article-title>Burst spiking of a single cortical neuron modifies global brain state.</article-title>             <source>Science</source>             <volume>324</volume>             <fpage>643</fpage>             <lpage>646</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Koulakov1">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koulakov</surname><given-names>AA</given-names></name><name name-style="western"><surname>Hromadka</surname><given-names>T</given-names></name><name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name></person-group>             <year>2009</year>             <article-title>Correlated connectivity and the distribution of firing rates in the neocortex.</article-title>             <source>J Neurosci</source>             <volume>29</volume>             <fpage>3685</fpage>             <lpage>3694</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Yassin1">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yassin</surname><given-names>L</given-names></name><name name-style="western"><surname>Benedetti</surname><given-names>BL</given-names></name><name name-style="western"><surname>Jouhanneau</surname><given-names>JS</given-names></name><name name-style="western"><surname>Wen</surname><given-names>JA</given-names></name><name name-style="western"><surname>Poulet</surname><given-names>JFA</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>An embedded subnetwork of highly active neurons in the neocortex.</article-title>             <source>Neuron</source>             <volume>68</volume>             <fpage>1043</fpage>             <lpage>1050</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002294-Pecevski1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name><name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name><name name-style="western"><surname>Schuch</surname><given-names>K</given-names></name></person-group>             <year>2009</year>             <article-title>PCSIM: a parallel simulation environment for neural circuits fully integrated with Python.</article-title>             <source>Front Neuroinform</source>             <volume>3</volume>             <fpage>11</fpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>