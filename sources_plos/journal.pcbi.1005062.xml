<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00511</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005062</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurochemicals</subject><subj-group><subject>Dopaminergics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurochemicals</subject><subj-group><subject>Dopaminergics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neuronal plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Learning Reward Uncertainty in the Basal Ganglia</article-title>
<alt-title alt-title-type="running-head">Learning Reward Uncertainty</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7622-716X</contrib-id>
<name name-style="western">
<surname>Mikhael</surname> <given-names>John G.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Bogacz</surname> <given-names>Rafal</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Experimental Psychology, University of Oxford, Oxford, United Kingdom</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Harvard Medical School, Boston, Massachusetts, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>MRC Brain Network Dynamics Unit, University of Oxford, Oxford, United Kingdom</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Blackwell</surname> <given-names>Kim T.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>George Mason University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"><list-item><p><bold>Wrote the paper:</bold> JGM RB.</p></list-item> <list-item><p>Developed, analysed and simulated the models: JGM RB.</p></list-item></list></p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">rafal.bogacz@ndcn.ox.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>2</day>
<month>9</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>9</issue>
<elocation-id>e1005062</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>3</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>7</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Mikhael, Bogacz</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005062"/>
<abstract>
<p>Learning the reliability of different sources of rewards is critical for making optimal choices. However, despite the existence of detailed theory describing how the expected reward is learned in the basal ganglia, it is not known how reward uncertainty is estimated in these circuits. This paper presents a class of models that encode both the mean reward and the spread of the rewards, the former in the difference between the synaptic weights of D1 and D2 neurons, and the latter in their sum. In the models, the tendency to seek (or avoid) options with variable reward can be controlled by increasing (or decreasing) the tonic level of dopamine. The models are consistent with the physiology of and synaptic plasticity in the basal ganglia, they explain the effects of dopaminergic manipulations on choices involving risks, and they make multiple experimental predictions.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>To maximize their chances for survival, animals need to base their decisions not only on the average consequences of chosen actions, but also on the variability of the rewards resulting from these actions. For example, when an animal’s food reserves are depleted, it should prefer to forage in an area where food is guaranteed over an area where the amount of food is higher on average but variable, thus avoiding the risk of starvation. To implement such policies, animals need to be able to learn about variability of rewards resulting from taking different actions. This paper proposes how such learning may be implemented in a circuit of subcortical nuclei called the basal ganglia. It also suggests how the information about reward uncertainty can be used during decision making, so that animals can make choices that not only maximize expected rewards but also minimize risks.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000265</institution-id>
<institution>Medical Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>MC UU 12024/5</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Bogacz</surname> <given-names>Rafal</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000266</institution-id>
<institution>Engineering and Physical Sciences Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>EP/I032622/1</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Bogacz</surname> <given-names>Rafal</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by Medical Research Council Grant MC UU 12024/5, and Engineering and Physical Research Council Grant EP/I032622/1. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>This is a theoretic study which does not describe any new data.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In situations where actions are associated with rewards, knowledge of the reliability of rewards for alternative choices is critical for selecting the optimal action. Normative models have suggested that optimal foraging requires adaptively switching between risk aversion and risk seeking depending on the circumstance [<xref ref-type="bibr" rid="pcbi.1005062.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref002">2</xref>]. Indeed, experimental data suggest that humans and animals tend to seek or avoid choice options with reward uncertainty in different situations [<xref ref-type="bibr" rid="pcbi.1005062.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref003">3</xref>]. To implement such policies, animals and humans need to have estimates of the reward variability associated with different sources, as well as the ability to control how this variability should influence their choices. In addition, knowledge of the reliability of reward feedback is important for learning about the mean reward, as it sets the optimal learning rate. Indeed, in high uncertainty situations, a single new data point should not influence the animal’s previously held estimate as strongly as it would in situations where the uncertainty associated with the data point is fairly low [<xref ref-type="bibr" rid="pcbi.1005062.ref004">4</xref>]. Furthermore, the estimate of reliability of rewards is helpful in optimizing the exploration-exploitation trade-off [<xref ref-type="bibr" rid="pcbi.1005062.ref005">5</xref>], because when an animal wishes to find which action yields the highest average reward, it takes more samples to get an accurate estimate of the mean reward for actions with more variable rewards. Hence, such actions should be preferably explored.</p>
<p>One of the key regions of the brain underlying action selection is the basal ganglia (BG). The BG is thought to be involved in learning the expected values of rewards that are associated with given actions and in selecting the actions associated with the highest expected values while inhibiting the others. The learning process in BG is facilitated by neurons releasing dopamine (DA), which encode the reward prediction error, defined as the difference between reward obtained and expected [<xref ref-type="bibr" rid="pcbi.1005062.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref007">7</xref>]. This signal allows BG to update its estimates of reward accordingly [<xref ref-type="bibr" rid="pcbi.1005062.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref009">9</xref>].</p>
<p>The pathologies that affect the function of BG influence how it learns or makes decisions in situations involving uncertainty. For instance, a subset of patients with Parkinson’s disease, who suffer from selective death of dopaminergic neurons in the substantia nigra in the midbrain, are impaired in a task involving choices between options with different spreads of their respective reward distributions [<xref ref-type="bibr" rid="pcbi.1005062.ref010">10</xref>]. When they are on medication (DA agonist), these patients exhibit a well-reported phenomenon of obsessive gambling, in which the patients seem to exhibit a change in their subjective values of risk and reward [<xref ref-type="bibr" rid="pcbi.1005062.ref011">11</xref>]. This change can be reversed by taking the patients off medication [<xref ref-type="bibr" rid="pcbi.1005062.ref012">12</xref>]. Additionally, manipulating the levels of dopamine in humans and animals adjusts their decision making under risk [<xref ref-type="bibr" rid="pcbi.1005062.ref013">13</xref>].</p>
<p>These pieces of evidence suggest that uncertainty is encoded in BG (but one has to note that although BG is the main target of dopaminergic projections, DA neurons also innervate cortex, so some of the effects mentioned above may also have cortical contribution). While computational models have been developed to explain how BG can estimate the expected reward [<xref ref-type="bibr" rid="pcbi.1005062.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>], it is still unclear how the reliability of the reward can be estimated in BG, given its anatomical and physiological properties.</p>
<p>Here we show that there exists a class of models consistent with the physiology of BG that can at once learn both the expected reward from a given action and the reliability of the reward, i.e., the spread of its probability distribution. We then show how the models can use learned information about reward uncertainty in decision making, and how the models can account for the effect of dopaminergic medications on decision making in tasks involving risk.</p>
<p>In the next section (“<xref ref-type="sec" rid="sec002">Models</xref>”), we review previously proposed models of reinforcement learning in BG, on which our models are built. The new models that can learn reward uncertainty are presented in Section “Results”. Readers familiar with the actor-critic model [<xref ref-type="bibr" rid="pcbi.1005062.ref016">16</xref>] and Opponent Actor Learning model (OpAL) [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>] can skip directly to “Results”.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Models</title>
<p>The models of reinforcement learning in BG have been developed in two frameworks: a simpler framework considering only an “actor” and a more complex “actor-critic framework.” We review both of these frameworks, as both can be extended to learning reward uncertainty.</p>
<sec id="sec003">
<title>Actor-only framework</title>
<p>This framework assumes that BG estimates average rewards for selecting different actions. Let <inline-formula id="pcbi.1005062.e001"><alternatives><graphic id="pcbi.1005062.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> denote an estimate of expected reward for selecting the action <italic>i</italic> on trial <italic>t</italic>. Let us assume that after selecting the action, a reward <italic>r</italic><sup>(<italic>t</italic>)</sup> is provided, which comes from a distribution with mean <italic>μ</italic><sub><italic>i</italic></sub> and standard deviation <italic>σ</italic><sub><italic>i</italic></sub>.</p>
<p>We start by considering an abstract Rescorla-Wagner rule [<xref ref-type="bibr" rid="pcbi.1005062.ref017">17</xref>] for estimating the expected reward for a given action. According to this rule, after receiving a reward, the expected reward is updated in the following way:
<disp-formula id="pcbi.1005062.e002"><alternatives><graphic id="pcbi.1005062.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula></p>
<p>According to the above equation, the change in the estimate of the expected reward is proportional to the reward prediction error (<inline-formula id="pcbi.1005062.e003"><alternatives><graphic id="pcbi.1005062.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>), scaled by the learning rate constant <italic>α</italic>, where 0 &lt; <italic>α</italic> &lt; 1. It is intuitive to see why this rule works: If <italic>r</italic><sup>(<italic>t</italic>)</sup> is underestimated, our estimate <italic>Q</italic><sub><italic>i</italic></sub> will increase (i.e., <inline-formula id="pcbi.1005062.e004"><alternatives><graphic id="pcbi.1005062.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>&gt;</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>). If <italic>r</italic><sup>(<italic>n</italic>)</sup> is overestimated, <italic>Q</italic><sub><italic>i</italic></sub> will decrease, and if <italic>r</italic><sup>(<italic>t</italic>)</sup> is estimated perfectly (<inline-formula id="pcbi.1005062.e005"><alternatives><graphic id="pcbi.1005062.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>), then <italic>Q</italic><sub><italic>i</italic></sub> will remain the same. In addition, the amount we increment by will be scaled by the magnitude of the prediction error <inline-formula id="pcbi.1005062.e006"><alternatives><graphic id="pcbi.1005062.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, so that we learn more quickly when we have a lot of learning to do than when our estimate is quite close to the true mean already. Also note that having <italic>α</italic> &lt; 1 ensures that the new data point updates our estimate but does not completely replace it (as would be the case if <italic>α</italic> were in fact equal to 1), an implicit acknowledgement of the existence of uncertainty in the reward and noise in the system.</p>
</sec>
<sec id="sec004">
<title>Actor-critic framework</title>
<p>The actor-critic model [<xref ref-type="bibr" rid="pcbi.1005062.ref016">16</xref>] includes two components: an actor that learns tendencies to select particular actions and a critic that learns an overall value of the current context or state. In the actor-critic model, the value <italic>V</italic> of being in this state is learned by the critic according to the standard Rescorla-Wagner rule [<xref ref-type="bibr" rid="pcbi.1005062.ref017">17</xref>] (cf. <xref ref-type="disp-formula" rid="pcbi.1005062.e002">Eq 1</xref>):
<disp-formula id="pcbi.1005062.e007"><alternatives><graphic id="pcbi.1005062.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>Note that <italic>V</italic><sup>(<italic>t</italic>)</sup> is updated regardless of which action <italic>i</italic> is selected, so <italic>V</italic><sup>(<italic>t</italic>)</sup> is not an estimate of expected reward associated with a particular action, but rather an average reward in the current state.</p>
<p>In the standard actor-critic model, after choosing action <italic>i</italic>, the tendency to choose it, which we denote by <italic>Q</italic><sub><italic>i</italic></sub>, is learned by the actor using the following update rule:
<disp-formula id="pcbi.1005062.e008"><alternatives><graphic id="pcbi.1005062.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>According to the above equation, the tendency to choose action <italic>i</italic> is also modified proportionally to the reward prediction error, i.e., it is increased if the action resulted in a higher reward than expected by the critic and decreased if the reward was below expectation.</p>
<p>The actor-critic model naturally maps on the matrix-patch organization of the striatum [<xref ref-type="bibr" rid="pcbi.1005062.ref018">18</xref>]. Such mapping assumes that <italic>V</italic><sup>(<italic>t</italic>)</sup> is encoded in the synapses between cortical neurons selective for the current context and striatal patch neurons, as shown in <xref ref-type="fig" rid="pcbi.1005062.g001">Fig 1</xref>. The patch neurons directly inhibit dopaminergic neurons [<xref ref-type="bibr" rid="pcbi.1005062.ref019">19</xref>], so that if the dopaminergic neurons also receive input encoding reward, then their activity may encode <italic>r</italic><sup>(<italic>t</italic>)</sup> − <italic>V</italic><sup>(<italic>t</italic>)</sup>. The actor part of the model is mapped on matrix neurons [<xref ref-type="bibr" rid="pcbi.1005062.ref018">18</xref>] that send projections to the output nuclei, which in turn project to areas controlling movement, so they can affect which movement is selected. Finally, the dopaminergic neurons modulate plasticity of the synapses of both patch neurons and matrix neurons. It is worth adding that some studies map actor and critic on dorsal and ventral striatum respectively [<xref ref-type="bibr" rid="pcbi.1005062.ref020">20</xref>], but this mapping is related to the matrix-patch mapping, as the patch neurons are more common in ventral than dorsal striatum [<xref ref-type="bibr" rid="pcbi.1005062.ref021">21</xref>].</p>
<fig id="pcbi.1005062.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Simplified anatomy of the basal ganglia.</title>
<p>The arrows and lines ending with circles denote the excitatory and inhibitory connections respectively. The following abbreviations are used: GPe—external globus pallidus, SNr—substantia nigra pars reticulata, GPi—internal globus pallidus, SNc—substantia nigra pars compacta, VTA—ventral tegmental area.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Opponent actor learning model</title>
<p>A recent model called Opponent Actor Learning (OpAL) [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>] takes into account the fact that the matrix neurons can be subdivided into two groups, which express D1 and D2 DA receptors, respectively. These project through different nuclei of BG, as shown in <xref ref-type="fig" rid="pcbi.1005062.g001">Fig 1</xref> [<xref ref-type="bibr" rid="pcbi.1005062.ref022">22</xref>] and have opposite effects on movement initiation [<xref ref-type="bibr" rid="pcbi.1005062.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref024">24</xref>]. In particular, D1 neurons project through the “direct” pathway to the output nuclei, and their activity facilitates movements [<xref ref-type="bibr" rid="pcbi.1005062.ref025">25</xref>] because they inhibit the output nuclei and thus release thalamus from inhibition. By contrast, D2 neurons project through the “indirect” pathway, and their activity inhibits movement [<xref ref-type="bibr" rid="pcbi.1005062.ref025">25</xref>].</p>
<p>The OpAL model describes learning about the tendencies to choose or inhibit actions <italic>i</italic> in a given state, which we will denote by <inline-formula id="pcbi.1005062.e009"><alternatives><graphic id="pcbi.1005062.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> (for Go) and <inline-formula id="pcbi.1005062.e010"><alternatives><graphic id="pcbi.1005062.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> (for NoGo), respectively. The OpAL model proposes that these tendencies are encoded in the strengths of synaptic connections between the cortical neurons associated with that state and the striatal D1 or D2 neurons selective for action <italic>i</italic>, respectively [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>], as illustrated in <xref ref-type="fig" rid="pcbi.1005062.g001">Fig 1</xref>. In the OpAL model, after selecting action <italic>i</italic> the synaptic weights are modified according to:
<disp-formula id="pcbi.1005062.e011"><alternatives><graphic id="pcbi.1005062.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1005062.e012"><alternatives><graphic id="pcbi.1005062.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>Thus if the reward prediction error is positive, the tendency to select the action is increased, while the tendency to inhibit it is weakened, and vice versa. Additionally, in the OpAL model, the reward prediction error is scaled by <inline-formula id="pcbi.1005062.e013"><alternatives><graphic id="pcbi.1005062.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005062.e014"><alternatives><graphic id="pcbi.1005062.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, which prevents <inline-formula id="pcbi.1005062.e015"><alternatives><graphic id="pcbi.1005062.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005062.e016"><alternatives><graphic id="pcbi.1005062.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> from becoming negative. For example, if <inline-formula id="pcbi.1005062.e017"><alternatives><graphic id="pcbi.1005062.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> becomes close to 0, the changes in its value also tend to 0.</p>
<p>The OpAL model additionally proposes how the probabilities of actions depend on the weights in Go and NoGo pathways, through a generalized version of the softmax rule [<xref ref-type="bibr" rid="pcbi.1005062.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref027">27</xref>]:
<disp-formula id="pcbi.1005062.e018"><alternatives><graphic id="pcbi.1005062.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mi>a</mml:mi> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>b</mml:mi> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mtext>exp</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mi>a</mml:mi> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>b</mml:mi> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mstyle></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>In the above equation, normalization by the denominator ensures that the <inline-formula id="pcbi.1005062.e019"><alternatives><graphic id="pcbi.1005062.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mi>P</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> add up to 1 across all possible actions. Parameters <italic>a</italic> and <italic>b</italic> control how deterministic the choice is: when <italic>a</italic> = <italic>b</italic> = 0, all actions have equal probability, while with higher <italic>a</italic> and <italic>b</italic>, the influence of the learned tendencies on choice increases. The relative value of parameters <italic>a</italic> and <italic>b</italic> describes to what extent the neurons in the Go and NoGo pathways contribute to choice (when <italic>a</italic> = <italic>b</italic>, both pathways contribute equally; otherwise, one pathway dominates). The rationale for introducing two parameters <italic>a</italic> and <italic>b</italic> is that the activity levels of the striatal D1 and D2 neurons are modulated in opposite directions by levels of DA; hence, they can differentially contribute to activity in the output nuclei [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>] (see <xref ref-type="fig" rid="pcbi.1005062.g001">Fig 1</xref>).</p>
</sec>
</sec>
<sec id="sec006" sec-type="results">
<title>Results</title>
<p>We first describe the conceptually simpler actor-only model, which will allow for a clearer explanation of the essential mechanisms of learning reward uncertainty. Then, we show how the model can explain the effect of dopaminergic stimulation on choice in tasks involving selection between safe and risky options, Subsequently, we present generalizations of the model, and compare it with the OpAL model.</p>
<sec id="sec007">
<title>Learning reward uncertainty in actor-only framework</title>
<p>In the models including only the actor, learning about the reward distribution of an individual action is independent of learning about the distribution of another. Thus for simplicity of notation, while introducing the model we will consider just a single context and a single action, and denote the corresponding synaptic weights of D1 and D2 neurons on trial <italic>t</italic> by <italic>G</italic><sup>(<italic>t</italic>)</sup> and <italic>N</italic><sup>(<italic>t</italic>)</sup>, respectively. Furthermore, we will denote the mean and standard deviation of reward distribution by <italic>μ</italic><sub><italic>r</italic></sub> and <italic>σ</italic><sub><italic>r</italic></sub>.</p>
<p>The model employing the original Rescorla-Wagner rule (<xref ref-type="disp-formula" rid="pcbi.1005062.e002">Eq 1</xref>) keeps track of an abstract variable <italic>Q</italic><sup>(<italic>t</italic>)</sup> that describes the overall tendency to select action <italic>i</italic>, but in BG this tendency is encoded in the synaptic weights of D1 and D2 neurons, <italic>G</italic><sup>(<italic>t</italic>)</sup> and <italic>N</italic><sup>(<italic>t</italic>)</sup>. So let us relate these variables by:
<disp-formula id="pcbi.1005062.e020"><alternatives><graphic id="pcbi.1005062.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>The update rules for the weights in the Actor learning Uncertainty (AU) model have the following form:
<disp-formula id="pcbi.1005062.e021"><alternatives><graphic id="pcbi.1005062.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mrow><mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>+</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula> <disp-formula id="pcbi.1005062.e022"><alternatives><graphic id="pcbi.1005062.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>In the equations above, the prediction errors are transformed through threshold-linear functions |<italic>x</italic>|<sub>+</sub> and |<italic>x</italic>|<sub>−</sub> which are equal to |<italic>x</italic>| if <italic>x</italic> is positive or negative respectively, and 0 otherwise. In other words, |<italic>x</italic>|<sub>+</sub> = max(<italic>x</italic>, 0), and |<italic>x</italic>|<sub>−</sub> = max(−<italic>x</italic>, 0). Thus if the prediction error is positive, then so is the corresponding term in <xref ref-type="disp-formula" rid="pcbi.1005062.e021">Eq (8)</xref>, and <italic>G</italic> increases, while if the prediction error is negative, then the corresponding term in <xref ref-type="disp-formula" rid="pcbi.1005062.e022">Eq (9)</xref> is positive, and <italic>N</italic> increases. Furthermore, the decay terms (last terms in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e021">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e022">(9)</xref>) are scaled by a separate constant 0 &lt; <italic>β</italic> &lt; 1.</p>
<p>As we will explain below, the AU model encodes the estimate of mean reward <italic>μ</italic><sub><italic>r</italic></sub> in <italic>G</italic><sup>(<italic>t</italic>)</sup> − <italic>N</italic><sup>(<italic>t</italic>)</sup>, while the estimate of reward spread <italic>σ</italic><sub><italic>r</italic></sub> in <italic>G</italic><sup>(<italic>t</italic>)</sup> + <italic>N</italic><sup>(<italic>t</italic>)</sup>. Before giving a proof for this property, let us first provide an intuition. The AU model encodes the mean reward in <italic>G</italic><sup>(<italic>t</italic>)</sup> − <italic>N</italic><sup>(<italic>t</italic>)</sup> due to its similarity with the Rescorla-Wagner rule. In particular, when the reward is higher than expected, <italic>G</italic> tends to increase, while when the reward is lower than expected, <italic>N</italic> tends to increase, so in both cases <italic>G</italic><sup>(<italic>t</italic>)</sup> − <italic>N</italic><sup>(<italic>t</italic>)</sup> tends to move towards the value of the reward.</p>
<p>To gain some intuition for how the model can encode reward uncertainty in <italic>G</italic><sup>(<italic>t</italic>)</sup> + <italic>N</italic><sup>(<italic>t</italic>)</sup>, it is useful to consider the changes in the weights in two different cases: when the rewards are deterministic, i.e., of the same magnitude each time the action is selected, and when they are stochastic. In the case of deterministic rewards, on initial trials, reward prediction error will be positive, hence only <italic>G</italic> will increase but not <italic>N</italic>, as illustrated in the top left panel of <xref ref-type="fig" rid="pcbi.1005062.g002">Fig 2</xref>. By contrast, in the case of stochastic rewards, on some trials the reward prediction error will be negative. Hence, <italic>N</italic> will also increase, as illustrated in the top right panel of <xref ref-type="fig" rid="pcbi.1005062.g002">Fig 2</xref>.</p>
<fig id="pcbi.1005062.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Changes in <italic>G</italic> and <italic>N</italic> for the AU model as a function of trial number.</title>
<p>Different rows correspond to different mean reward <italic>μ</italic><sub><italic>r</italic></sub> (indicated left of each row), and different columns correspond to different standard deviations of reward <italic>σ</italic><sub><italic>r</italic></sub> (indicated above each column). The rewards were sampled from a Gaussian distribution. Here, both <italic>G</italic> and <italic>N</italic> were initialized at 0, and we set <italic>α</italic> = 0.1. We have selected <inline-formula id="pcbi.1005062.e032"><alternatives><graphic id="pcbi.1005062.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi>β</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> in order to make the figure easier to interpret, because then <inline-formula id="pcbi.1005062.e033"><alternatives><graphic id="pcbi.1005062.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mrow><mml:mi>E</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, and hence in the middle row <italic>G</italic> and <italic>N</italic> approach <italic>σ</italic><sub><italic>r</italic></sub>. For each of the panels, the simulation was run 50 times, for 100 trials each.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g002" xlink:type="simple"/>
</fig>
<p>Finally, the decay terms in the above equations serve to ensure the convergence of the synaptic weights, as in their absence, the update rules would only allow <italic>G</italic> and <italic>N</italic> to either increase or stay the same upon every iteration, but never decrease.</p>
<p>Let us now show that the AU model can learn expected reward. By subtracting <xref ref-type="disp-formula" rid="pcbi.1005062.e022">Eq (9)</xref> from <xref ref-type="disp-formula" rid="pcbi.1005062.e021">Eq (8)</xref> we obtain:
<disp-formula id="pcbi.1005062.e023"><alternatives><graphic id="pcbi.1005062.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula></p>
<p>The threshold-linear functions disappear when Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e021">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e022">(9)</xref> are subtracted, because if the prediction error is positive, the corresponding terms in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e021">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e022">(9)</xref> are equal to the prediction error and 0 respectively, so when subtracted give the prediction error. Conversely, if the prediction error is negative, the corresponding terms in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e021">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e022">(9)</xref> are equal to 0 and the negative of the prediction error, so when subtracted they also give the prediction error. Comparing Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e023">(10)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e002">(1)</xref>, we note that this update rule is similar to the standard Rescorla-Wagner rule, with an added decay term.</p>
<p>For a fixed value of <italic>α</italic>, the variable <italic>Q</italic> never converges when <italic>σ</italic><sub><italic>r</italic></sub> &gt; 0, but constantly fluctuates. Nevertheless, it is useful to consider a value around which it fluctuates. After sufficiently long learning, the expected change in <italic>Q</italic> will be zero. In other words, for large enough <italic>t</italic>,
<disp-formula id="pcbi.1005062.e024"><alternatives><graphic id="pcbi.1005062.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula></p>
<p>The value of <italic>Q</italic><sup>(<italic>t</italic>)</sup> at which <xref ref-type="disp-formula" rid="pcbi.1005062.e024">Eq (11)</xref> holds is referred to as the stochastic fixed point, and we will denote it by <inline-formula id="pcbi.1005062.e025"><alternatives><graphic id="pcbi.1005062.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>. By combining <xref ref-type="disp-formula" rid="pcbi.1005062.e023">Eq (10)</xref> with <xref ref-type="disp-formula" rid="pcbi.1005062.e024">Eq (11)</xref>, we obtain:
<disp-formula id="pcbi.1005062.e026"><alternatives><graphic id="pcbi.1005062.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mrow><mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula></p>
<p>Rearranging the terms in the above equation, we see that <italic>Q</italic> at the stochastic fixed point is equal to:
<disp-formula id="pcbi.1005062.e027"><alternatives><graphic id="pcbi.1005062.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mrow><mml:mi>α</mml:mi> <mml:mo>+</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>r</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula></p>
<p>Although in the AU model <italic>Q</italic>* is not equal to the expected reward, it is proportional to it, with a proportionality constant that is equal across all actions. Thus, choosing an action with the highest <italic>Q</italic>* is equivalent to choosing an action with the highest expected reward.</p>
<p>We now show that the AU model learns reward uncertainty. In order to do so, we will analyze how the sum of the synaptic weights evolves. Thus, let us define:
<disp-formula id="pcbi.1005062.e028"><alternatives><graphic id="pcbi.1005062.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula></p>
<p>By adding <xref ref-type="disp-formula" rid="pcbi.1005062.e022">Eq (9)</xref> to <xref ref-type="disp-formula" rid="pcbi.1005062.e021">Eq (8)</xref>:
<disp-formula id="pcbi.1005062.e029"><alternatives><graphic id="pcbi.1005062.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula></p>
<p>From the above equation we see that at the stochastic fixed point:
<disp-formula id="pcbi.1005062.e030"><alternatives><graphic id="pcbi.1005062.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mi>E</mml:mi><mml:mtext> </mml:mtext><mml:mo>[</mml:mo><mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mtext>]</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mi>E</mml:mi><mml:mtext> </mml:mtext><mml:mo>[</mml:mo><mml:mrow><mml:mo>|</mml:mo> <mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow> <mml:mo>|</mml:mo></mml:mrow><mml:mtext>]</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>The above equation implies that when <italic>Q</italic>* = <italic>μ</italic><sub><italic>r</italic></sub>, the sum of <italic>G</italic> and <italic>N</italic> is equal to the deviation of the reward from the mean. In <xref ref-type="supplementary-material" rid="pcbi.1005062.s001">S1 Text</xref> we illustrate that, when <italic>Q</italic>* = <italic>μ</italic><sub><italic>r</italic></sub>, then <italic>S</italic>* is directly proportional to the standard deviation or variance of rewards (depending on the shape of the reward distribution). When <italic>Q</italic>* ≠ <italic>μ</italic><sub><italic>r</italic></sub>, <italic>S</italic>* is not exactly proportional to the deviation of the rewards from the mean. To see more clearly when it approximates the deviation, let us rewrite the above equation as:
<disp-formula id="pcbi.1005062.e031"><alternatives><graphic id="pcbi.1005062.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>α</mml:mi> <mml:mi>β</mml:mi></mml:mfrac> <mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mfrac><mml:mi>α</mml:mi> <mml:mi>β</mml:mi></mml:mfrac> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula></p>
<p>From the equation above, we see that <italic>S</italic>* becomes proportional to the deviation of rewards when the second term inside the expected value is dominated by the first. This can occur in two cases. First, since the magnitude of the first term increases with <italic>σ</italic><sub><italic>r</italic></sub>, while that of the second term is proportional to <italic>μ</italic><sub><italic>r</italic></sub>, then <italic>S</italic> is close to an estimate of the deviation of rewards when <italic>σ</italic><sub><italic>r</italic></sub> is relatively high with respect to <italic>μ</italic><sub><italic>r</italic></sub>.</p>
<p>
<xref ref-type="fig" rid="pcbi.1005062.g002">Fig 2</xref> shows simulations of the model for different reward mean and standard deviations of rewards and illustrates changes in synaptic weights as learning progresses. The simulations shown in different rows correspond to mean reward being positive, equal to 0, and negative, respectively. Note that the difference between <italic>G</italic> and <italic>N</italic> always approaches a value proportional to the expected reward. The simulations shown in different columns correspond to progressively higher standard deviation of reward. When <italic>μ</italic><sub><italic>r</italic></sub> = 0, the value that <italic>G</italic> and <italic>N</italic> approach increases linearly with <italic>σ</italic><sub><italic>r</italic></sub>. By contrast, when <italic>μ</italic><sub><italic>r</italic></sub> is higher, the encoding of reward uncertainty is less precise. For example, in the top row of <xref ref-type="fig" rid="pcbi.1005062.g002">Fig 2</xref> we observe that the values of synaptic weights change very little as <italic>σ</italic><sub><italic>r</italic></sub> increases from 0 to 2. The increase in weights is slightly higher as <italic>σ</italic><sub><italic>r</italic></sub> increases from 2 to 4. Nevertheless, <xref ref-type="fig" rid="pcbi.1005062.g002">Fig 2</xref> shows that increasing reward uncertainty still results in higher values of both <italic>G</italic> and <italic>N</italic>. Note that in each row, the larger the reward uncertainty, the larger <italic>G</italic> and <italic>N</italic>.</p>
<p>Second, the second term in <xref ref-type="disp-formula" rid="pcbi.1005062.e031">Eq (17)</xref> decreases with the ratio of parameters <inline-formula id="pcbi.1005062.e034"><alternatives><graphic id="pcbi.1005062.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mfrac><mml:mi>β</mml:mi> <mml:mi>α</mml:mi></mml:mfrac></mml:math></alternatives></inline-formula>. Thus the lower <italic>β</italic> is relative to <italic>α</italic>, the closer <italic>S</italic>* is to a linear function of the deviation of rewards. This property is illustrated in <xref ref-type="fig" rid="pcbi.1005062.g003">Fig 3</xref>, which plots <italic>S</italic> as a function of the standard deviation of rewards for different values of <italic>β</italic>. It is evident in the figure that, on average, <italic>S</italic> is a monotonic function of <italic>σ</italic><sub><italic>r</italic></sub>. Hence, it is worth noting that although <italic>S</italic> is an estimate of reward uncertainty, it is possible for the neural system to obtain a closer estimate by learning the function mentioned above and thus decode the estimate of reward deviation from <italic>S</italic> (i.e., correct the biases of <italic>S</italic> in estimating <italic>σ</italic><sub><italic>r</italic></sub>). However, this function has a flat region for low <italic>σ</italic><sub><italic>r</italic></sub>, so that the model’s estimate of the reward deviation will not be precise in that range of <italic>σ</italic><sub><italic>r</italic></sub>. For example, one can observe in <xref ref-type="fig" rid="pcbi.1005062.g003">Fig 3</xref> that when <italic>β</italic> = <italic>α</italic> the value of <italic>S</italic> ≈ 0.5 arises for a wide range of <italic>σ</italic><sub><italic>r</italic></sub>, so knowing that <italic>S</italic> = 0.5 we cannot accurately tell the value of <italic>σ</italic><sub><italic>r</italic></sub>. The size of the region where <italic>σ</italic><sub><italic>r</italic></sub> is not well estimated can be reduced by decreasing <italic>β</italic> relative to <italic>α</italic>. Nevertheless, <xref ref-type="fig" rid="pcbi.1005062.g003">Fig 3</xref> illustrates that there is a trade-off: Lower <inline-formula id="pcbi.1005062.e035"><alternatives><graphic id="pcbi.1005062.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mfrac><mml:mi>β</mml:mi> <mml:mi>α</mml:mi></mml:mfrac></mml:math></alternatives></inline-formula> results in a higher magnitude of weights, and thus higher metabolic cost, and lower <italic>β</italic> also slows learning (see [<xref ref-type="bibr" rid="pcbi.1005062.ref028">28</xref>] for details).</p>
<fig id="pcbi.1005062.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparison of the sum of weights in the Go and NoGo pathways in the AU model (vertical axis) with the standard deviation of rewards (horizontal axis) for different values of parameter <italic>β</italic>.</title>
<p>In all simulations in this figure, <italic>μ</italic><sub><italic>r</italic></sub> = 1 (so <italic>σ</italic><sub><italic>r</italic></sub> is equal to the coefficient of variation) and <italic>α</italic> = 0.1. For each value of <italic>σ</italic><sub><italic>r</italic></sub> the model was simulated 10 times for 300 iterations. For each simulation, the sum of <italic>G</italic> and <italic>N</italic> at the end of the simulation is displayed as a point on the figure.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Control over risk seeking via dopamine level</title>
<p>Let us now consider how the mean and spread of a reward distribution, learned by the model described above, can be used by BG in action selection. In the model the tendency to choose or avoid risky options is controlled by the tonic level of DA. Before giving mathematical justification for this property, let us first provide an intuition for it.</p>
<p>
<xref ref-type="fig" rid="pcbi.1005062.g004">Fig 4</xref> illustrates states of a network choosing between two options, one safe and the other risky, represented by neurons shown in blue and orange, respectively. In the figure, the strength of cortico-striatal connections is denoted by the thickness of the arrows. Thus both options are associated with positive mean reward (as the connections <italic>G</italic><sub><italic>i</italic></sub> are thicker than <italic>N</italic><sub><italic>i</italic></sub>), but the orange option has higher estimated spread of rewards (as the orange connections are thicker than the blue ones). DA is known to activate the D1 or Go neurons and inhibit D2 or NoGo neurons, which is represented in <xref ref-type="fig" rid="pcbi.1005062.g004">Fig 4</xref> by green arrows and lines ending with circles. The top panel illustrates a situation when the tonic DA level is high. In this case the NoGo neurons are suppressed (indicated by bleak color) and the choice is driven by the activity of the Go neurons. Thus with high DA, the more risky, orange option is more likely to be chosen, as <italic>G</italic><sub>2</sub> &gt; <italic>G</italic><sub>1</sub>. By contrast, with low levels of DA, the Go neurons are inhibited (bottom panel of <xref ref-type="fig" rid="pcbi.1005062.g004">Fig 4</xref>), and the choice is driven by NoGo neurons. Thus with low DA, the risky option is inhibited (as <italic>N</italic><sub>2</sub> &gt; <italic>N</italic><sub>1</sub>), and the model is more likely to select the safe option.</p>
<fig id="pcbi.1005062.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Effects of dopamine (DA) on action selection between safe and risky options.</title>
<p>Circles denote different neural populations: black circle corresponds to the neural population in cortex selective for the current state, green circle corresponds to dopaminergic neurons, and blue and orange circles correspond to the striatal neurons selective for two different actions. The circles receiving inputs via connections <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> correspond to D1 and D2 neurons. Arrows and lines ending with circles denote connections with excitatory and inhibitory effect respectively. The top panel illustrates a situation of high tonic level of DA, where the D2 neurons are inhibited (indicated by bleak color), while the bottom panel corresponds to low DA, where the D1 neurons are inhibited.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g004" xlink:type="simple"/>
</fig>
<p>The above example illustrates that the model has the tendency to choose more risky options when the level of DA is high, and safer options otherwise. Let us now show this property formally. The choice rule of <xref ref-type="disp-formula" rid="pcbi.1005062.e018">Eq (6)</xref> can be rewritten to make the effect of the mean and deviation of reward visible. To do so, we first write <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> in terms of <italic>Q</italic><sub><italic>i</italic></sub> and <italic>S</italic><sub><italic>i</italic></sub> (defined in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e020">(7)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e028">(14)</xref>):
<disp-formula id="pcbi.1005062.e036"><alternatives><graphic id="pcbi.1005062.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula> <disp-formula id="pcbi.1005062.e037"><alternatives><graphic id="pcbi.1005062.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula></p>
<p>Substituting the above into <xref ref-type="disp-formula" rid="pcbi.1005062.e018">Eq (6)</xref> we obtain:
<disp-formula id="pcbi.1005062.e038"><alternatives><graphic id="pcbi.1005062.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:msub><mml:mi>U</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mtext>exp</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mstyle> <mml:msub><mml:mi>U</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mstyle></mml:mfrac></mml:mstyle> <mml:mo>,</mml:mo> <mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mtext>where</mml:mtext> <mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:msub><mml:mi>U</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>b</mml:mi> <mml:mo>-</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula></p>
<p>In the choice rule above, the probability of choice depends on a utility function <italic>U</italic><sub><italic>i</italic></sub> that is a linear combination of mean reward and the deviation of reward (cf. [<xref ref-type="bibr" rid="pcbi.1005062.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref030">30</xref>]). By increasing <italic>b</italic> relative to <italic>a</italic> in the above choice rule, one can explicitly control how choice probability is affected by the deviation of rewards. In particular, when <italic>b</italic> &gt; <italic>a</italic>, the uncertainty of rewards reduces the probability of selecting the corresponding action, resulting in risk aversion. By contrast, setting <italic>b</italic> &lt; <italic>a</italic> increases the probability of choosing actions with uncertain rewards, resulting in risk seeking.</p>
<p>Recall that parameters <italic>a</italic> and <italic>b</italic> describe in the OpAL model [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>] to what extent D1 and D2 neurons contribute to determining choice. Since high levels of DA activate the direct pathway and suppress the indirect pathway, increasing the tonic level of DA will correspond in the model to increasing <italic>a</italic> and decreasing <italic>b</italic>, which according to the analysis above would result in more risk-seeking behavior. Thus such modulation provides a mean by which an organism can control whether the action selection should be risk-averse or risk-seeking. The above analysis explains why a tendency for gambling in Parkinson’s patients [<xref ref-type="bibr" rid="pcbi.1005062.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref031">31</xref>] may arise from increasing the level of DA by medications or from deep brain stimulation of subthalamic nucleus (which would also weaken the indirect pathway so would correspond to lowering <italic>b</italic>).</p>
<p>The presented model accounts for the effect of pharmacological manipulations affecting dopaminergic receptors on risk aversion in reinforcement learning tasks. In a particularly comprehensive study [<xref ref-type="bibr" rid="pcbi.1005062.ref032">32</xref>], rats were trained to choose between 2 levers: pressing one of them resulted in certain delivery of a single food pellet, while pressing another could result either in delivery of 4 pellets or none. The probability of receiving the large reward after the selection of the risky lever was varied across conditions. After the rats were well-trained in the task, they were injected with different drugs, and changes in the fraction of risky choices made were measured. An overall increased tendency to choose the risky option was observed either after injection of D1 agonist or D2 agonist, as shown in <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref>. Furthermore, the injection of D1 antagonist or D2 antagonist decreased the tendency to choose the more risky option [<xref ref-type="bibr" rid="pcbi.1005062.ref032">32</xref>].</p>
<fig id="pcbi.1005062.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Effect of dopaminergic receptor manipulations on risky choices.</title>
<p>In each panel the percentage of risky choices is plotted against the probability of obtaining the large reward by choosing the risky lever. Open circles show the data from animals in the control state, while filled squares show the data obtained after treatment with a drug. Each panel corresponds to a different drug indicated in the key. The data in each panel were read from one figure in [<xref ref-type="bibr" rid="pcbi.1005062.ref032">32</xref>] and averaged over different drug concentrations. In particular, the data in the four panels come from Figs 3c, 4c, 3a and 4a in [<xref ref-type="bibr" rid="pcbi.1005062.ref032">32</xref>]. Solid and dashed curves show the fractions of risky choices made by the model, simulated for parameters corresponding to control and drug conditions. During each simulation the model made 10,000 choices in each of four probability conditions (thus the standard error of mean fraction of risky choices made by the model was &lt; 1%). This large number of simulated trials allowed the model to produce stable behavior, which was necessary for the search for parameters resulting in a match with animal behavior.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g005" xlink:type="simple"/>
</fig>
<p>The fraction of risky choices made in simulations by the AU model is shown by curves in <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref>. In the simulations, the parameters controlling learning were fixed to standard values (<italic>α</italic> = <italic>β</italic> = 0.1), and only the parameters controlling choice (<italic>a</italic> and <italic>b</italic>) were fit to the data. Parameters <italic>a</italic> and <italic>b</italic> were fit separately to the data in each panel of <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref>, as each panel was obtained from a different group of rats. While fitting the model to the data from D1 receptor manipulations, it was assumed that <italic>a</italic> differed between control and drug conditions, while <italic>b</italic> did not change. Thus three parameters were fit: <italic>a</italic><sub><italic>control</italic></sub>, <italic>a</italic><sub><italic>drug</italic></sub>, and <italic>b</italic>. We did not enforce any relationship between <italic>a</italic><sub><italic>control</italic></sub> and <italic>a</italic><sub><italic>drug</italic></sub>, but as we will explain below, the estimated parameters followed the relationship expected from the known effects of drugs. Analogously, while fitting the model to the data from D2 receptor manipulations, <italic>a</italic>, <italic>b</italic><sub><italic>control</italic></sub>, and <italic>b</italic><sub><italic>drug</italic></sub> were fit. For each panel, the values of the three parameters were found that minimized the sum of squared errors between the fraction of risky choices made by the animals and the model in the 8 conditions (4 probabilities of large rewards on and off the drug). The parameters were found using the simplex algorithm [<xref ref-type="bibr" rid="pcbi.1005062.ref033">33</xref>] implemented in Matlab (function fminsearch). The search was repeated 10 times with different random initial parameter values sampled from the range [0, 3].</p>
<p>The model reproduced the fractions of risky choices made by the animals relatively well. Importantly, the overall direction of changes in risky choices and estimated parameters is consistent with the pattern in the data. In particular, in the top panels of <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref>, the fraction of risky choices is higher in the simulation of the agonist conditions. Furthermore, in the top left panel, estimated parameters satisfied <italic>a</italic><sub><italic>drug</italic></sub> &gt; <italic>a</italic><sub><italic>control</italic></sub> (<italic>a</italic><sub><italic>control</italic></sub> = 1.71, <italic>a</italic><sub><italic>drug</italic></sub> = 3.13, <italic>b</italic> = 0.59), which is consistent with the excitatory effect of DA on D1 receptors, while in the top right panel, the estimated parameters satisfied <italic>b</italic><sub><italic>drug</italic></sub> &lt; <italic>b</italic><sub><italic>control</italic></sub> (<italic>a</italic> = 2.72, <italic>b</italic><sub><italic>control</italic></sub> = 1.86, <italic>b</italic><sub><italic>drug</italic></sub> = 0.39), consistent with the inhibitory effect of DA on D2 receptors. Thus the choice behavior may become more risky due to activation of either D1 or D2 receptors, as activation of either of them decreases <italic>b</italic> − <italic>a</italic>, which reduces risk aversion in <xref ref-type="disp-formula" rid="pcbi.1005062.e038">Eq 20</xref>. Analogously in the bottom panels of <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref>, the fraction of risky choices is lower in the simulated condition with antagonists, and estimated parameters satisfy <italic>a</italic><sub><italic>drug</italic></sub> &lt; <italic>a</italic><sub><italic>control</italic></sub> for the bottom left (<italic>a</italic><sub><italic>control</italic></sub> = 2.67, <italic>a</italic><sub><italic>drug</italic></sub> = 0.86, <italic>b</italic> = 1.04) and <italic>b</italic><sub><italic>drug</italic></sub> &gt; <italic>b</italic><sub><italic>control</italic></sub> for the bottom right panels (<italic>a</italic> = 1.95, <italic>b</italic><sub><italic>control</italic></sub> = 0.04, <italic>b</italic><sub><italic>drug</italic></sub> = 2.16).</p>
<p>It is worth noting in the bottom left panel of <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref> that the model reproduces the cross-over of the two curves. It occurs in the simulations because as <italic>a</italic> is reduced (corresponding to the effect of D1 antagonist), the choice in the model becomes more random (recall from the Models section that <italic>a</italic> and <italic>b</italic> also control how deterministic the choice is), so that the fraction of risky choices is closer to 50%. In this task, choosing the risky lever gave higher expected reward in the 100% and 50% conditions while choosing the safe lever had higher mean reward in the 12.5% condition, and the model simulated with higher <italic>a</italic> in the bottom left panel of <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref> exploited the options with higher expected rewards more.</p>
</sec>
<sec id="sec009">
<title>Relationship of the model to synaptic plasticity in the striatum</title>
<p>The AU model assumes particular rules for updating striatal synaptic weights, and here we consider whether these rules are consistent with the existing data concerning synaptic plasticity in the striatum. For a synaptic plasticity rule to be plausible, the change in a synaptic weight needs to depend only on the information that can be sensed by a synapse, i.e., the activity of pre-synaptic and post-synaptic neurons, the levels of neuromodulators released in the vicinity of the synapse, and the synaptic weight itself. Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e021">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e022">(9)</xref> describe the change in synaptic weights between the neurons encoding current context and those encoding current movement, i.e., they describe changes in synapses between co-active neurons. This change includes two terms, which are the reward prediction error and decay. As mentioned earlier, a plethora of evidence suggests that reward prediction error (<italic>r</italic><sup>(<italic>t</italic>)</sup> − <italic>Q</italic><sup>(<italic>t</italic>)</sup>) is encoded in phasic changes in DA concentration, which is released in striatum.</p>
<p>The proposed weight update rules are consistent with the pattern of synaptic plasticity modulation by DA [<xref ref-type="bibr" rid="pcbi.1005062.ref034">34</xref>]. It has been observed experimentally that the activation of cortical neurons followed by striatal D1 neurons strengthens the synapses of D1 neurons when the DA level is elevated, and weakens these synapses when the DA level is reduced (Figs 3F and 2E in [<xref ref-type="bibr" rid="pcbi.1005062.ref034">34</xref>]). Such changes are consistent with <xref ref-type="disp-formula" rid="pcbi.1005062.e021">Eq (8)</xref>, because for positive prediction error, the prediction error term will dominate, so <italic>G</italic> will increase. By contrast, if the prediction error is negative, |<italic>r</italic><sup>(<italic>t</italic>)</sup> − <italic>Q</italic><sup>(<italic>t</italic>)</sup>|<sub>+</sub> will be equal to 0, and the decay term will dominate, so <italic>G</italic> will decrease. Conversely, the activation of cortical neurons followed by striatal D2 neurons weakens the synapses of D2 neurons when the DA level is elevated, and strengthens the synapses of D2 neurons when DA level is reduced (Figs 1H and 3B in [<xref ref-type="bibr" rid="pcbi.1005062.ref034">34</xref>]). Such changes are consistent with <xref ref-type="disp-formula" rid="pcbi.1005062.e022">Eq (9)</xref> for analogous reasons.</p>
<p>A critical property of the learning rules allowing encoding reward uncertainty in <italic>G</italic><sup>(<italic>t</italic>)</sup> + <italic>N</italic><sup>(<italic>t</italic>)</sup> is the asymmetry in how synaptic weights change for positive and negative reward prediction error. In particular, in the AU model, the change in <italic>G</italic> is only proportional to the reward prediction error if the error is positive, but not if the error is negative (analogous asymmetry holds for <italic>N</italic>). It is easy to check that if such asymmetry were not present (i.e., nonlinear functions of predictions errors were removed from Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e021">(8)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e022">(9)</xref>), then <italic>G</italic><sup>(<italic>t</italic>)</sup> + <italic>N</italic><sup>(<italic>t</italic>)</sup> would no longer encode the spread of reward distribution.</p>
<p>Such asymmetry may arise in striatal synapses from the observed differences in the affinity of DA receptors, such that a higher DA concentration is necessary to activate D1 receptors than D2 receptors [<xref ref-type="bibr" rid="pcbi.1005062.ref035">35</xref>]. <xref ref-type="fig" rid="pcbi.1005062.g006">Fig 6</xref> shows how the probability of D1 and D2 receptor activation depends on DA concentration in a biophysically realistic model of DA release [<xref ref-type="bibr" rid="pcbi.1005062.ref036">36</xref>]. Simulation of that model based on activity of DA neurons in vivo [<xref ref-type="bibr" rid="pcbi.1005062.ref037">37</xref>] suggested that the baseline DA level in striatum is in a sensitive range of both D1 and D2 receptors (as illustrated by the dashed line in <xref ref-type="fig" rid="pcbi.1005062.g006">Fig 6</xref>). Due to the arrangement shown in <xref ref-type="fig" rid="pcbi.1005062.g006">Fig 6</xref>, an increase in DA level has a larger effect on the activation of D1, while a decrease in DA has a larger effect on D2 receptors.</p>
<fig id="pcbi.1005062.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Schematic illustration of the sensitivity of D1 and D2 receptors to the changes in dopamine (DA) level.</title>
<p>Black and grey curves show how the probability of D1 and D2 receptor occupancy depends on DA concentration in a biophysical model of [<xref ref-type="bibr" rid="pcbi.1005062.ref036">36</xref>]. They assumed that receptor occupancy depends on DA concentration <italic>C</italic> as <inline-formula id="pcbi.1005062.e039"><alternatives><graphic id="pcbi.1005062.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mfrac><mml:mi>C</mml:mi> <mml:mrow><mml:mi>E</mml:mi> <mml:msub><mml:mi>C</mml:mi> <mml:mn>50</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>C</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, where <italic>EC</italic><sub>50</sub> is the receptor affinity, which was taken as 1<italic>μM</italic> and 10<italic>nM</italic> for D1 and D2 receptors respectively, based on [<xref ref-type="bibr" rid="pcbi.1005062.ref038">38</xref>]. Dashed line indicates baseline DA concentration <italic>C</italic> = 60<italic>nM</italic> suggested by simulations in [<xref ref-type="bibr" rid="pcbi.1005062.ref037">37</xref>]. Vertical arrows indicate how much binding probability changes due to changes in DA concentration, shown by horizontal arrows.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g006" xlink:type="simple"/>
</fig>
<p>According to <xref ref-type="fig" rid="pcbi.1005062.g006">Fig 6</xref>, the decrease in DA level may still have some small effect on the binding probability of D1 receptors (analogously the increase in DA may have a small effect on D2 receptors). Hence the complete lack of effect of a decrease (increase) in DA level on D1 (D2) neurons’ plasticity may seem inconsistent with the above analysis. Nevertheless below we show that for learning reward uncertainty, it is sufficient that there exist an asymmetry in the dopaminergic effects on the receptors, i.e., that the increase in DA level affect plasticity of D1 neurons more than D2 neurons (and the opposite for a decrease in DA level).</p>
<p>The Equations describing the AU model can be generalized to include more complex functions of reward prediction error:
<disp-formula id="pcbi.1005062.e040"><alternatives><graphic id="pcbi.1005062.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mrow><mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>+</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo></mml:msub></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula> <disp-formula id="pcbi.1005062.e041"><alternatives><graphic id="pcbi.1005062.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mrow><mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>+</mml:mo></mml:msub></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
where <italic>ϵ</italic> is a constant such that <italic>ϵ</italic> &lt; 1. As synaptic weights cannot be negative, whenever <italic>G</italic><sup>(<italic>t</italic>+1)</sup> or <italic>N</italic><sup>(<italic>t</italic>+1)</sup> computed from the above equations is negative, it is set to 0. A potential advantage of using such functions of prediction error is that after each feedback iteration, they drive changes in both <italic>G</italic> and <italic>N</italic>, and thus potentially result in faster learning. When <italic>ϵ</italic> = 0, the above model reduces to the AU model.</p>
<p>We now show that with these functions, the model can still encode expected reward and reward uncertainty. Subtracting the above two equations gives:
<disp-formula id="pcbi.1005062.e042"><alternatives><graphic id="pcbi.1005062.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula></p>
<p>Hence at the stochastic fixed point:
<disp-formula id="pcbi.1005062.e043"><alternatives><graphic id="pcbi.1005062.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>r</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula></p>
<p>Thus the differences in the synaptic weights of D1 and D2 neurons encode scaled relative values of actions, which are also sufficient to choose the action with the highest value. Similarly adding Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e040">(21)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e041">(22)</xref> we obtain:
<disp-formula id="pcbi.1005062.e044"><alternatives><graphic id="pcbi.1005062.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:msup><mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula></p>
<p>Hence at the stochastic fixed point:
<disp-formula id="pcbi.1005062.e045"><alternatives><graphic id="pcbi.1005062.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>β</mml:mi></mml:mfrac> <mml:mi>E</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mfenced close="|" open="|" separators=""><mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula></p>
<p>Using the analysis applied earlier to the AU model, we see that the sum of the weights of D1 and D2 neurons encodes a scaled version of deviation of the reward, under analogous conditions to those for the AU model (i.e., <italic>σ</italic><sub><italic>r</italic></sub> is relatively high with respect to <italic>μ</italic><sub><italic>r</italic></sub>, or <italic>β</italic> is relatively small with respect to <italic>α</italic>(1 + <italic>ϵ</italic>)). However, when <italic>ϵ</italic> &gt; 0, the weights <italic>G</italic><sup>(<italic>t</italic>+1)</sup> or <italic>N</italic><sup>(<italic>t</italic>+1)</sup> computed from Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e040">(21)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e041">(22)</xref> may become negative, but negative synaptic weights are not allowed in the model, so the calculations of the fixed points above are only valid for <italic>ϵ</italic> sufficiently small so that <italic>G</italic><sup>(<italic>t</italic>+1)</sup> and <italic>N</italic><sup>(<italic>t</italic>+1)</sup> are not negative.</p>
<p>To illustrate how this generalized AU model encodes reward uncertainty, the left panel in <xref ref-type="fig" rid="pcbi.1005062.g007">Fig 7</xref> shows the results of simulations in the same setting as in <xref ref-type="fig" rid="pcbi.1005062.g003">Fig 3</xref>, but with a fixed value of <italic>β</italic> = 0.1, for different values of parameter <italic>ϵ</italic>. The figure shows that when <italic>ϵ</italic> = 0.5, the model also encodes reward uncertainty, but the encoding is less accurate than for <italic>ϵ</italic> = 0. In particular, when <italic>S</italic> is equal to a certain value, we can infer <italic>σ</italic><sub><italic>r</italic></sub> more precisely from the left panel in <xref ref-type="fig" rid="pcbi.1005062.g007">Fig 7</xref> for <italic>ϵ</italic> = 0, as the range of <italic>σ</italic><sub><italic>r</italic></sub> resulting in the certain value of <italic>S</italic> is narrower for <italic>ϵ</italic> = 0 (e.g., <italic>S</italic> = 0.75 for <italic>σ</italic> ∈ [0.6, 1]) than for <italic>ϵ</italic> = 0.5 (e.g., <italic>S</italic> = 0.75 for <italic>σ</italic> ∈ [1, 2]).</p>
<fig id="pcbi.1005062.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparison of the sum of weights in the Go and NoGo pathways (vertical axis) with the standard deviation of rewards (horizontal axis) in the original (black dots) and generalized (grey dots) versions of the AU (left panel) and ACU (right panel) models.</title>
<p>In all simulations in this figure, <italic>μ</italic><sub><italic>r</italic></sub> = 1 and <italic>α</italic> = 0.1. The rewards were sampled from a Gaussian distribution. For each value of <italic>σ</italic><sub><italic>r</italic></sub> the model was simulated 10 times for 300 trials. For each simulation, the sum of <italic>G</italic> and <italic>N</italic> at the end of the simulation is displayed as a point on the figure. At the first trial of each simulation, the weights were initialized to <italic>G</italic> = <italic>N</italic> = 0.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Learning reward uncertainty in actor-critic framework</title>
<p>In this section we show that the actor-critic model after small extension can learn both the mean and spread of rewards associated with actions. The model uses the same rule for the update of the critic (<xref ref-type="disp-formula" rid="pcbi.1005062.e007">Eq (2)</xref>), and the plasticity of synapses of D1 and D2 neurons is described by equations similar to those for the AU model, but in which the prediction error is based on the reward estimated by the critic:
<disp-formula id="pcbi.1005062.e046"><alternatives><graphic id="pcbi.1005062.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e046" xlink:type="simple"/><mml:math display="block" id="M46"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>+</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula> <disp-formula id="pcbi.1005062.e047"><alternatives><graphic id="pcbi.1005062.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula></p>
<p>For simplicity, in the above equations we set the decay constant <italic>β</italic> = <italic>α</italic>, which will also allow relating the model to advantage learning [<xref ref-type="bibr" rid="pcbi.1005062.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref040">40</xref>]. We will refer to a model with the actor described by the above equations, with the critic by the standard Rescorla-Wagner rule of <xref ref-type="disp-formula" rid="pcbi.1005062.e007">Eq (2)</xref>, and with the OpAL choice rule of <xref ref-type="disp-formula" rid="pcbi.1005062.e018">Eq (6)</xref>, as the Actor-Critic learning Uncertainty (ACU). We now show that the ACU model estimates both mean and spread of rewards associated with action <italic>i</italic>, which we denote by <italic>μ</italic><sub><italic>i</italic></sub> and <italic>σ</italic><sub><italic>i</italic></sub>, respectively.</p>
<p>To see that the mean rewards are encoded in the difference between <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub>, we subtract the above equations, and using <xref ref-type="disp-formula" rid="pcbi.1005062.e020">Eq (7)</xref>, we obtain:
<disp-formula id="pcbi.1005062.e048"><alternatives><graphic id="pcbi.1005062.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula></p>
<p>This update rule differs from that of the original actor-critic model of <xref ref-type="disp-formula" rid="pcbi.1005062.e008">Eq (3)</xref> in that it includes a decay term, and the rule is known as advantage learning [<xref ref-type="bibr" rid="pcbi.1005062.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref040">40</xref>] (for reasons that will become apparent below). Let us now find the value the vicinity of which <italic>Q</italic><sub><italic>i</italic></sub> approaches, by noting that at the stochastic fixed point the following condition must hold:
<disp-formula id="pcbi.1005062.e049"><alternatives><graphic id="pcbi.1005062.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e049" xlink:type="simple"/><mml:math display="block" id="M49"><mml:mrow><mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:mi>α</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula></p>
<p>Rearranging the terms in the above equation, we see:
<disp-formula id="pcbi.1005062.e050"><alternatives><graphic id="pcbi.1005062.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></alternatives> <label>(31)</label></disp-formula></p>
<p>Namely, <italic>Q</italic><sub><italic>i</italic></sub> at the stochastic fixed point is equal to the expected reward for action <italic>i</italic> relative to the overall average reward in the current state (this quantity has been termed the advantage of action <italic>i</italic>). Note that knowing the relative values of the actions available in a given state is sufficient for selecting the action with the highest value. The value of the state <italic>V</italic>* is equal to the average value of all actions weighted by how frequently they are selected:
<disp-formula id="pcbi.1005062.e051"><alternatives><graphic id="pcbi.1005062.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mrow><mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msubsup><mml:mi>P</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(32)</label></disp-formula></p>
<p>In this model, the sum of <inline-formula id="pcbi.1005062.e052"><alternatives><graphic id="pcbi.1005062.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005062.e053"><alternatives><graphic id="pcbi.1005062.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:msubsup><mml:mi>N</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> also approximates reward uncertainty. Adding Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e046">(27)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e047">(28)</xref> we obtain:
<disp-formula id="pcbi.1005062.e054"><alternatives><graphic id="pcbi.1005062.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mfenced close="|" open="|" separators=""><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(33)</label></disp-formula></p>
<p>At the stochastic fixed point, the expected change in the sum of weights should be equal to 0, hence:
<disp-formula id="pcbi.1005062.e055"><alternatives><graphic id="pcbi.1005062.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mrow><mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:mi>α</mml:mi> <mml:mfenced close="|" open="|" separators=""><mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>α</mml:mi> <mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:mfenced> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(34)</label></disp-formula></p>
<p>Rearranging terms, we see that the sum of weights <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> at the fixed point is:
<disp-formula id="pcbi.1005062.e056"><alternatives><graphic id="pcbi.1005062.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:mfenced close="|" open="|" separators=""><mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(35)</label></disp-formula></p>
<p>The above equation implies that when <italic>V</italic>* = <italic>μ</italic><sub><italic>i</italic></sub>, the sum of <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> is equal to the deviation of the reward from the mean. We now consider three situations when <italic>V</italic>* is close to <italic>μ</italic><sub><italic>i</italic></sub>.</p>
<p>First, when only one action is available, and chosen on all trials, then <italic>V</italic>* = <italic>μ</italic><sub>1</sub>, and hence <inline-formula id="pcbi.1005062.e057"><alternatives><graphic id="pcbi.1005062.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi> <mml:mn>1</mml:mn> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>∼</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. This property is illustrated in the right panel of <xref ref-type="fig" rid="pcbi.1005062.g007">Fig 7</xref>, where black dots show the uncertainty estimated by the ACU model in simulations with a single action. Note that <italic>S</italic> is proportional to reward uncertainty for the entire range of <italic>σ</italic><sub><italic>r</italic></sub>, so with a single action, the ACU model can accurately encode uncertainty for a wider range of <italic>σ</italic><sub><italic>r</italic></sub> than the AU model (cf. black points in left and right panels of <xref ref-type="fig" rid="pcbi.1005062.g007">Fig 7</xref>).</p>
<p>Second, when a few actions <italic>i</italic> ∈ <italic>I</italic> have similar mean rewards, while other actions <italic>j</italic> ∈ <italic>J</italic> give much lower rewards, then <italic>P</italic><sub><italic>j</italic> ∈ <italic>J</italic></sub> are close to 0. In this case, <italic>V</italic>* is equal to a weighted average of <italic>μ</italic><sub><italic>i</italic> ∈ <italic>I</italic></sub>, but since we assumed that all <italic>μ</italic><sub><italic>i</italic> ∈ <italic>I</italic></sub> are similar, then <italic>V</italic>* is close to <italic>μ</italic><sub><italic>i</italic></sub> for <italic>i</italic> ∈ <italic>I</italic>. Hence the ACU model estimates well the spread of reward distribution for actions with the highest mean reward, i.e., those most frequently selected. It may not estimate the spread of other actions, but this does not matter, as these actions are typically not selected anyway.</p>
<p>Different rows in <xref ref-type="fig" rid="pcbi.1005062.g008">Fig 8</xref> show simulations of the ACU model for different reward distributions and illustrate changes in synaptic weights as learning progresses. In the first simulation, the two actions have the same mean reward, and it can be seen in the top row that the value <italic>V</italic> converges to the expected reward. For each action, <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> converge to values equal to each other, because the ACU model encodes in <italic>G</italic><sub><italic>i</italic></sub> − <italic>N</italic><sub><italic>i</italic></sub> the relative value of actions which are equal to 0 here. In the simulation, the second action has uncertainty twice as high as the first one, and indeed one can see in the top row of <xref ref-type="fig" rid="pcbi.1005062.g008">Fig 8</xref> that <italic>G</italic><sub>2</sub> + <italic>N</italic><sub>2</sub> converges to a value twice as high as <italic>G</italic><sub>1</sub> + <italic>N</italic><sub>1</sub>.</p>
<fig id="pcbi.1005062.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Changes in the variables of the ACU model simulated in a two-alternative choice task as a function of trial number.</title>
<p>The rewards were sampled from a Gaussian distribution. Different rows correspond to simulations with different mean rewards <italic>μ</italic><sub><italic>i</italic></sub> (indicated above the panels), and different columns show: synaptic weights describing the tendency to select <italic>G</italic><sub><italic>i</italic></sub> and inhibit <italic>N</italic><sub><italic>i</italic></sub> for the two actions and the value of the state <italic>V</italic>. Standard deviations of reward <italic>σ</italic><sub><italic>i</italic></sub> associated with the two actions are indicated above the corresponding panels. Here, both <italic>G</italic> and <italic>N</italic> were initialized at 0, and we set <italic>α</italic> = 0.1 and the parameters of the choice rule to <italic>a</italic> = <italic>b</italic> = 1. For each of the panels, the simulation was run 50 times, for 100 trials each.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g008" xlink:type="simple"/>
</fig>
<p>In the simulation illustrated in the bottom row of <xref ref-type="fig" rid="pcbi.1005062.g008">Fig 8</xref>, the first action has a smaller expected reward. The model learns to select the second action on a great majority of trials, which results in the expected reward <italic>V</italic> converging towards the mean reward of the second action. The model estimates well the deviation of rewards associated with the second action—note that <italic>G</italic><sub>2</sub> + <italic>N</italic><sub>2</sub> is similar in both rows of <xref ref-type="fig" rid="pcbi.1005062.g008">Fig 8</xref>. Finally, the model does not estimate well the deviation of reward of the first action, but this does not matter, as this action is very rarely selected.</p>
<p>Third, the ACU model can still estimate reward uncertainty for actions with lower mean rewards than other actions available, if the uncertainty is sufficiently large. To understand this property, it is helpful to rewrite <xref ref-type="disp-formula" rid="pcbi.1005062.e056">Eq 35</xref> as:
<disp-formula id="pcbi.1005062.e058"><alternatives><graphic id="pcbi.1005062.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005062.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mfenced close="]" open=" [" separators=""><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(36)</label></disp-formula></p>
<p>When <italic>σ</italic><sub><italic>i</italic></sub> is sufficiently larger than |<italic>μ</italic><sub><italic>i</italic></sub> − <italic>V</italic>*|, the first term in the above equation will dominate over the second, and <italic>S</italic><sub><italic>i</italic></sub> will be more closely proportional to <italic>σ</italic><sub><italic>i</italic></sub>.</p>
<p>In summary, the AU and ACU models differ in the conditions under which their ability to estimate reward uncertainty is limited. The AU model does not precisely estimate the reward uncertainty in situations where the standard deviation of rewards is small relative to their mean. The ACU model has a limited ability to estimate uncertainty only in a subset of these situations, i.e., when the reward uncertainty is small and additionally the mean value of the action is substantially lower than for other actions available in the corresponding state.</p>
<p>Finally, it is worth mentioning that the learning rule of the ACU model can be generalized as described in the previous subsection, such that the weights of the actor are modified according to Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e040">21</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e041">22</xref> but with <italic>Q</italic> replaced by <italic>V</italic>. The grey dots in the right panel of <xref ref-type="fig" rid="pcbi.1005062.g007">Fig 7</xref> show that the uncertainty estimated by such a generalized ACU model is still proportional to the true variability of rewards but is encoded less precisely than in the original ACU model. Furthermore, a simulation of the ACU model analogous to that shown in <xref ref-type="fig" rid="pcbi.1005062.g005">Fig 5</xref> produced qualitatively similar behavior as the AU model; thus, an increased tendency to take risky options with a high level of DA is a general property of a class of models encoding reward uncertainty in <italic>G</italic> + <italic>N</italic>.</p>
</sec>
<sec id="sec011">
<title>Comparison with the OpAL model</title>
<p>We also investigated the behavior of the OpAL model [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>] in the presence of reward uncertainty. <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref> shows simulations of the OpAL model in the same tasks used for the ACU model in <xref ref-type="fig" rid="pcbi.1005062.g008">Fig 8</xref>. Top rows of <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref> show simulations of a task in which the two actions have the same mean reward but differ in reward deviation. In the initial trials, in which the reward prediction error is positive, <italic>G</italic><sub><italic>i</italic></sub> increase exponentially. The exponential increase arises due to the multiplication of prediction error by <italic>G</italic> or <italic>N</italic> in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e011">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e012">5</xref>, which results in a rate of weight changes that is proportional to the weights themselves. Once the reward prediction becomes equal to 0 on average, the weights start to decay towards 0. The weights have a stochastic fixed point at <italic>G</italic><sub><italic>i</italic></sub> = <italic>N</italic><sub><italic>i</italic></sub> = 0 in the OpAL model, because when <italic>G</italic><sub><italic>i</italic></sub> = <italic>N</italic><sub><italic>i</italic></sub> = 0, there are no changes in weights according to Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e011">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e012">5</xref>. In the task simulated in the top panel of <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref>, this fixed point was attractive, and all weights of the actor eventually approached 0. It is interesting that this decay was faster for the option with higher uncertainty, as for this option the larger fluctuations in the reward prediction error drove the weights to the fixed point faster. In the task simulated in the bottom panel of <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref>, this fixed point was attractive only for the action with the higher value, while for the other action, <italic>N</italic><sub><italic>i</italic></sub> increased with time.</p>
<fig id="pcbi.1005062.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Changes in the variables of the OpAL model simulated in a two-alternative choice task as a function of trial number.</title>
<p>The rewards were sampled from a Gaussian distribution. Different rows correspond to simulations with different mean rewards <italic>μ</italic><sub><italic>i</italic></sub> (indicated above the panels), and different columns show: synaptic weights describing the tendency to select <italic>G</italic><sub><italic>i</italic></sub> and inhibit <italic>N</italic><sub><italic>i</italic></sub> for the two actions and the value of the state <italic>V</italic>. Standard deviations of reward <italic>σ</italic><sub><italic>i</italic></sub> associated with the two actions are indicated above the corresponding panels. Here, both <italic>G</italic> and <italic>N</italic> were initialized at 0.1, and we set <italic>α</italic> = 0.1 and the parameters of the choice rule to <italic>a</italic> = <italic>b</italic> = 1. For each of the panels, the simulation was run 50 times, for 300 trials each.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g009" xlink:type="simple"/>
</fig>
<p>It is evident from <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref> that the OpAL model does not encode reward uncertainty in the weights close to convergence, and the dynamics of weight changes is much more volatile than in the ACU model (note that the range of vertical axes in <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref> is two orders of magnitude higher than in <xref ref-type="fig" rid="pcbi.1005062.g008">Fig 8</xref>). Furthermore, when two actions have equal mean reward, as in the top panels of <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref>, after extensive training, all weights <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> converge to 0, so the probability of choosing a more risky option becomes exactly 0.5, according to <xref ref-type="disp-formula" rid="pcbi.1005062.e018">Eq 6</xref>, irrespective of the values of parameters <italic>a</italic> and <italic>b</italic>. Hence in this case, the probability of a risky choice predicted by the OpAL model is not dependent on the level of DA.</p>
<p>The OpAL model is able to capture the effects of dopaminergic medications seen in a series of experiments [<xref ref-type="bibr" rid="pcbi.1005062.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref042">42</xref>], which as we will see below, are challenging for the AU and ACU models. These experiments were designed to test the effects of DA on learning from positive and negative feedback, but in these studies the feedback uncertainty also varied between choice options. During these experiments the participants were presented with Japanese characters, were asked to choose one them, and subsequently received feedback indicating whether their choice was correct. For clarity, let us consider a simplified version of the task. Assume that during the training phase, the participant is presented on each trial with 3 letters which we will refer to as A, B and C. The probability of obtaining “Correct” feedback after selecting each of the 3 options is 0.8, 0.2 and 0.5 respectively. After the training, the participant is presented with a choice between A and C, or with a choice between B and C. The fraction of A vs. C trials in which the participant chooses A has been interpreted as a measure of learning from positive feedback (as stimulus A was associated with the highest probability of “Correct” feedback). Conversely, the fraction of B vs. C trials in which the participant does not choose B has been interpreted as a measure of learning from negative feedback (as stimulus B was associated with the highest probability of “Incorrect” feedback). It has been observed that Parkinson’s patients on dopaminergic medications exhibit higher accuracy in choosing A than in avoiding B, while the opposite pattern is present off medications [<xref ref-type="bibr" rid="pcbi.1005062.ref014">14</xref>]. Furthermore, it has been suggested that this effect is dependent on the medication state during testing rather than during encoding [<xref ref-type="bibr" rid="pcbi.1005062.ref042">42</xref>].</p>
<p>The OpAL model is able to replicate these effects [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>]. While simulating learning in this task, we assumed that the model receives a reward of <italic>r</italic> = 1 when “Correct” feedback is given, and no reward <italic>r</italic> = 0 after “Incorrect” feedback. The top left panel in <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref> shows the weights learned by the OpAL model. As expected, <italic>G</italic><sub><italic>i</italic></sub> increase with the probability of reward, while <italic>N</italic><sub><italic>i</italic></sub> decrease. Importantly, the relationship between weights and reward probability is non-linear. This non-linearity arises from the multiplication of prediction error by <italic>G</italic><sub><italic>i</italic></sub> or <italic>N</italic><sub><italic>i</italic></sub> in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e011">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e012">5</xref>, which as mentioned above, results in an exponential growth of the weights and thus magnification of weights with high values. The bottom right panel in <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref> illustrates how the values of the weights affect behavior during test. In the simulated on medication condition, the choice is primarily affected by weights <italic>G</italic><sub><italic>i</italic></sub> (<xref ref-type="disp-formula" rid="pcbi.1005062.e018">Eq 6</xref>). Thus the accuracies in choosing A and avoiding B depend on |<italic>G</italic><sub><italic>A</italic></sub> − <italic>G</italic><sub><italic>C</italic></sub>| and |<italic>G</italic><sub><italic>B</italic></sub> − <italic>G</italic><sub><italic>C</italic></sub>|, respectively. Since |<italic>G</italic><sub><italic>A</italic></sub> − <italic>G</italic><sub><italic>C</italic></sub>|&gt; |<italic>G</italic><sub><italic>B</italic></sub> − <italic>G</italic><sub><italic>C</italic></sub>| in the top left panel, the probability of choosing A is higher than the probability of avoiding B on medications in the bottom left panel. In the simulated off medication condition, the choice is primarily affected by weights <italic>N</italic><sub><italic>i</italic></sub>, and hence the model is better at avoiding B than choosing A for analogous reasons. The choice pattern in the bottom left panel of <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref> is qualitatively consistent with that observed in experimental studies [<xref ref-type="bibr" rid="pcbi.1005062.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref042">42</xref>].</p>
<fig id="pcbi.1005062.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005062.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Comparison of the behavior of different models (labelled above columns of panels) in the Japanese letter learning task.</title>
<p>The top panels show the weights at the end of the simulation, and the bottom panels the probability of choosing A and avoiding B (computed from <xref ref-type="disp-formula" rid="pcbi.1005062.e018">Eq 6</xref>). At the start of each simulation <italic>V</italic>, <italic>G</italic> and <italic>N</italic> were initialized at 0.1, and we set <italic>α</italic> = <italic>β</italic> = 0.1. The parameters of the choice rule to were set to <italic>a</italic> = <italic>b</italic> = 2 during training, while during test they were set to <italic>a</italic> = 4, <italic>b</italic> = 0 in the simulated on medication condition, and to <italic>a</italic> = 0, <italic>b</italic> = 4 in the simulated off medication condition. For each of the models, 100 simulations were run, with 100 learning trials each, and error bars show standard error across simulations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.g010" xlink:type="simple"/>
</fig>
<p>The top panel in the middle column of <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref> shows the weights learned by the AU model. Here also, <italic>G</italic><sub><italic>i</italic></sub> increase with reward probability, while <italic>N</italic><sub><italic>i</italic></sub> decrease. However, in the AU model the sum of weights <italic>G</italic><sub><italic>i</italic></sub> + <italic>N</italic><sub><italic>i</italic></sub> is highest for option C, which gives reward on 50% of trials and thus has highest reward variance. Consequently, the relationships between weights and reward probability are concave for the AU model, rather than convex as they were for the OpAL model. This results in the opposite effect of DA on choosing A and avoiding B relative to the OpAL model (cf. left and middle panels in the bottom row of <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref>).</p>
<p>The right panels of <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref> illustrate that the behavior of the ACU model is qualitatively similar to that of the AU model. However, the predicted effect of medications on choice probability in ACU is smaller than in AU, because the relationships between weights and reward probability are more linear for ACU. This occurs because ACU estimates the deviation of reward from the mean across all trials (<xref ref-type="disp-formula" rid="pcbi.1005062.e056">Eq 35</xref>) rather than from the mean reward for a given option, as in AU.</p>
<p>The OpAL model also described the dependence of learning rates <italic>α</italic> for <italic>G</italic><sub><italic>i</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub> on the level of DA [<xref ref-type="bibr" rid="pcbi.1005062.ref015">15</xref>]. Simulations of the AU and ACU models indicate that increasing the learning rate for <italic>G</italic><sub><italic>i</italic></sub> (or <italic>N</italic><sub><italic>i</italic></sub>) scales up the learned values of <italic>G</italic><sub><italic>i</italic></sub> (or <italic>N</italic><sub><italic>i</italic></sub>) but does not change the convexity/concavity of the relationship between weights and reward probability, and hence does not change qualitatively the predicted effects of DA during testing on the probability of choosing A and avoiding B.</p>
<p>In summary, the simulations of the AU and ACU models produced qualitatively different patterns of effects of dopaminergic medications on choosing A and avoiding B than observed experimentally [<xref ref-type="bibr" rid="pcbi.1005062.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref042">42</xref>]. A critical feature of the OpAL model that allows it to capture the experimentally observed effects is the multiplication of prediction error by <italic>G</italic> or <italic>N</italic> in Eqs <xref ref-type="disp-formula" rid="pcbi.1005062.e011">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005062.e012">5</xref>, but it is this very property that also caused unrealistically volatile weight changes in simulations of <xref ref-type="fig" rid="pcbi.1005062.g009">Fig 9</xref>.</p>
<p>It is interesting to ask under what assumptions the pattern of weights in the top left panel of <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref> (that allows reproducing the effects of medications on choosing A and avoiding B) could be obtained in a model learning reward uncertainty. In our simulations we assumed that “Correct” and “Incorrect” feedback were mapped on rewards of 1 and 0. However, it is unclear if the brain simply maps abstract feedback on the reward. It is possible that instead the brain infers that option C is unpredictable and does not engage in learning about it, which would result in relatively low <italic>G</italic><sub><italic>C</italic></sub> and <italic>N</italic><sub><italic>C</italic></sub>, as in the top left panel of <xref ref-type="fig" rid="pcbi.1005062.g010">Fig 10</xref>. This interpretation together with the AU (or ACU) model predicts that if an actual (e.g., monetary) reward is given as feedback, the effect of dopaminergic medications on choosing A and avoiding B should reverse (or be very small). This interpretation is consistent with a result of experiments employing a modified version of the Japanese letter task with more salient feedback, i.e., smiling and sad faces, in which no effect of medications was found [<xref ref-type="bibr" rid="pcbi.1005062.ref043">43</xref>]. However, to fully test this interpretation, further studies are needed that could for example use explicit monetary reward.</p>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper we presented a class of models that can learn both the mean reward and reward uncertainty. The models describe how BG can control the influence of risk on choices and choose actions that not only maximize expected rewards but also minimize risks. Below we relate the models to experimental data, state further predictions, and discuss relationships with other computational models.</p>
<sec id="sec013">
<title>Relationship to experimental data</title>
<p>We discuss here the relationships between predictions of the models and experimental data, including behavior and neural activity. Since in this paper we presented several models, it will be important to distinguish in the future which of them provide the best description of learning uncertainty in the basal ganglia. To differentiate between predictions specific to individual models and common to other models, we will use the term “the models” to refer to a class including all models introduced in this paper.</p>
<p>We already demonstrated in the Results section that the models account for the effect of pharmacological manipulations affecting dopaminergic receptors on risk aversion in reinforcement learning tasks in rats. The studies investigating the effect of DA on human decisions involving risks use two types of paradigms: one in which the mean and spread of rewards associated with choice options are explicitly described to the participant before each decision, and one in which they are gradually learned from feedback. Since human behavior is very different in these paradigms [<xref ref-type="bibr" rid="pcbi.1005062.ref044">44</xref>], and the models assume that the mean and deviation of rewards are learned in cortico-striatal synapses, below we only focus on studies involving learning from experience. The most commonly used paradigm in such tasks is the Iowa gambling task in which participants choose between decks of cards differing in reward variance. In agreement with the models, Parkinson’s patients receiving dopaminergic medications choose the risky decks more frequently than healthy controls, but this effect is not present in patients that have not been put on medications yet [<xref ref-type="bibr" rid="pcbi.1005062.ref045">45</xref>], or who stopped receiving medications [<xref ref-type="bibr" rid="pcbi.1005062.ref046">46</xref>].</p>
<p>The models introduced in this paper do not describe behavior in decision tasks in which information about risks associated with different options is explicitly presented before each trial. It is likely that processing information about uncertainty in such tasks involves different neural mechanisms and circuits than those learning about reward uncertainty over many trials.</p>
<p>The models are also consistent with the results of a recent study showing that optogenetic activation of striatal D2 neurons decreases the probability of choosing options with high reward variance [<xref ref-type="bibr" rid="pcbi.1005062.ref047">47</xref>]. Optogenetic activation of D2 neurons corresponds to a scenario illustrated in the bottom panel of <xref ref-type="fig" rid="pcbi.1005062.g004">Fig 4</xref>, where the choice is primarily driven by D2 neurons, and thus the risky option is inhibited.</p>
<p>The AU and ACU models differ in the predicted activity of DA neurons when the reward exactly matches the expected reward in tasks where only one action is available. In the ACU model, DA response is assumed to carry (<italic>r</italic> − <italic>V</italic>) where <italic>V</italic>* = <italic>E</italic>[<italic>r</italic>], so when <italic>r</italic> = <italic>E</italic>[<italic>r</italic>], DA neurons should not change their firing rate. By contrast, in the AU model the DA release is assumed to encode (<italic>r</italic> − <italic>Q</italic>) where <italic>Q</italic>* &lt; <italic>E</italic>[<italic>r</italic>] (see <xref ref-type="disp-formula" rid="pcbi.1005062.e027">Eq 13</xref>), so when <italic>r</italic> = <italic>E</italic>[<italic>r</italic>], DA neurons should increase their firing rate above baseline. Experimentally observed DA responses after expected rewards differed between experimental studies. For example, DA neurons were found to maintain their activity in classical conditioning in some studies [<xref ref-type="bibr" rid="pcbi.1005062.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref007">7</xref>], while an increase was observed in others [<xref ref-type="bibr" rid="pcbi.1005062.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref049">49</xref>]. Thus, more research is necessary to establish factors determining DA response to expected reward.</p>
</sec>
<sec id="sec014">
<title>Experimental predictions</title>
<p>The AU model predicts that learned synaptic weights in BG are insensitive to small standard deviations of reward; thus, it predicts that an individual’s choices are not affected by small enough uncertainty in reinforcement learning tasks. By contrast, the ACU model predicts that biases in estimation of reward uncertainty should only be present for actions with mean rewards much lower than those of other actions.</p>
<p>The models predict that overall activity in striatum should be higher during choice between options with high reward variance than during choice between options with lower reward variance but similar mean, because in the models the spread of rewards is encoded in <italic>G</italic><sub><italic>i</italic></sub> + <italic>N</italic><sub><italic>i</italic></sub>, so higher reward variance should increase the activity of both D1 and D2 striatal neurons. This prediction could be easily tested using functional MRI.</p>
<p>The models predict that synaptic plasticity will depend on the current value of the weight itself (i.e., <italic>G</italic><sub><italic>i</italic></sub> or <italic>N</italic><sub><italic>i</italic></sub>), because the weight update rules include decay terms proportional to the weights themselves. Thus the models predict that the stronger the weight of a synaptic connection, the higher the amplitude of induced long-term depression. Such dependence of plasticity on the value of weights has been observed in neocortex [<xref ref-type="bibr" rid="pcbi.1005062.ref050">50</xref>], and it would be very interesting to see if it is also present in cortico-striatal synapses.</p>
</sec>
<sec id="sec015">
<title>Relationship to other computational models</title>
<p>In addition to the models presented in this paper, reward uncertainty can be learned by a wide family of models in which the decay terms are proportional to the estimated uncertainty, and these models were analyzed in [<xref ref-type="bibr" rid="pcbi.1005062.ref028">28</xref>]. The models in this family can learn reward uncertainty even for small deviations. However, to implement such learning rules, the information about the uncertainty would need to be provided to a synapse, e.g., by a second neuromodulator. The models in this family predict that the release of this neuromodulator would need to be dependent on uncertainty and promote long-term depression of cortico-striatal synapses. Three different neuromodulators have been proposed to encode information about estimated (or expected) reward uncertainty: tonic DA [<xref ref-type="bibr" rid="pcbi.1005062.ref051">51</xref>], acetylcholine [<xref ref-type="bibr" rid="pcbi.1005062.ref052">52</xref>], and serotonin [<xref ref-type="bibr" rid="pcbi.1005062.ref030">30</xref>]. All of these neuromodulators have been shown to affect risky decisions [<xref ref-type="bibr" rid="pcbi.1005062.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1005062.ref055">55</xref>]. However, we have not found support in existing experimental data for predictions of our models employing multiple neuromodulators, hence we did not include them in this paper.</p>
<p>In previous reinforcement learning models that described learning about uncertainty [<xref ref-type="bibr" rid="pcbi.1005062.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref056">56</xref>], the estimate of reward variance was updated on each trial proportionally to “variance prediction error”, which is equal to the difference between the square of reward prediction error and the current estimate of variance. An interesting model describing how such learning could be implemented in BG [<xref ref-type="bibr" rid="pcbi.1005062.ref057">57</xref>] suggested that the variance of rewards is encoded in striatal neurons co-expressing D1 and D2 receptors. This model assumed that such neurons could increase their weights both when the prediction error is highly positive (like D1 neurons) and when it is strongly negative (like D2 neurons). However, the neurons co-expressing D1 and D2 receptors form only a small proportion of striatal neurons [<xref ref-type="bibr" rid="pcbi.1005062.ref058">58</xref>], and the models we propose describe learning of reward deviation in the great majority of striatal projection neurons that express either D1 or D2 receptors.</p>
<p>An interesting reinforcement learning model has also been proposed in which choosing risky options can be avoided without explicitly learning the spread of reward distributions for different options [<xref ref-type="bibr" rid="pcbi.1005062.ref059">59</xref>]. In this model, the weight update rules are modified such that <italic>Q</italic><sub><italic>i</italic></sub> is decreased when action <italic>i</italic> leads to a reward with higher variance. This model is efficient when the desired level of risk aversion is known and fixed before the learning starts, but unlike the models presented in this paper, it does not allow the trained system to be easily switched from risk aversion to more neutral or risk seeking behavior.</p>
<p>Reward uncertainty is also likely to be estimated in the cortex. A particularly interesting model [<xref ref-type="bibr" rid="pcbi.1005062.ref060">60</xref>] describes how the variance of any feature of the stimulus (including reward) can be estimated in a neural circuit with organization similar to that of the neocortex, and it has been shown how this learning about variance can be implemented with local Hebbian plasticity [<xref ref-type="bibr" rid="pcbi.1005062.ref061">61</xref>]. It is highly likely that the mechanisms of learning uncertainty in neocortex and striatum can operate in parallel. Furthermore, these two structures may estimate complementary measures of dispersion: the cortical model [<xref ref-type="bibr" rid="pcbi.1005062.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref061">61</xref>] estimates variance, while the models presented here estimate the mean absolute deviation (which is less affected by outliers).</p>
<p>In this paper we focused on one particular type of uncertainty associated with variable rewards in a stationary environment, which is typically called “expected uncertainty” [<xref ref-type="bibr" rid="pcbi.1005062.ref052">52</xref>]. But there is also another type of uncertainty connected with rapid changes (or volatility) of mean reward, referred to as “unexpected uncertainty” [<xref ref-type="bibr" rid="pcbi.1005062.ref052">52</xref>]. It is likely that there are complementary neural mechanisms which estimate unexpected uncertainty. For example, it has been proposed that striatal cholinergic tonically active interneurons detect changes in reward contingency and increase the learning rate following such changes [<xref ref-type="bibr" rid="pcbi.1005062.ref062">62</xref>]. Areas beyond BG can also be involved in this process, as the activity in other brain regions has been shown to track reward volatility [<xref ref-type="bibr" rid="pcbi.1005062.ref063">63</xref>] and volatility prediction errors [<xref ref-type="bibr" rid="pcbi.1005062.ref064">64</xref>].</p>
<p>Finally, let us discuss the relationship of the ACU model to advantage learning [<xref ref-type="bibr" rid="pcbi.1005062.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref040">40</xref>]. As mentioned in the Results section, the ACU model estimates the mean reward using the advantage learning rule; thus, the ACU model also provides a description of how this abstract rule may be implemented in the BG circuit. The advantage model was originally introduced to reconcile reinforcement learning models with animals’ innate tendency to approach highly rewarding stimuli [<xref ref-type="bibr" rid="pcbi.1005062.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref040">40</xref>]. The central feature of the advantage model (also inherited by the ACU model) is that as learning progresses, the value <italic>V</italic> represented by the critic approaches the value of the best action in the current state, while the advantage <italic>Q</italic><sub><italic>i</italic></sub> of this action approaches 0. This property describes a transition from an instrumental action selection to a stimulus-response habit, as in the trained state the action selection is implemented in the advantage model by the innate tendency to approach high value states [<xref ref-type="bibr" rid="pcbi.1005062.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1005062.ref040">40</xref>].</p>
<p>The ACU model has an analogous property that in the absence of reward uncertainty, <italic>G</italic><sub><italic>i</italic></sub> decreases towards 0 as learning progresses. Selection under such circumstances is primarily driven in the ACU model by D2 neurons, as suboptimal actions have large <italic>N</italic><sub><italic>i</italic></sub>, and thus are inhibited. This agrees with a recent proposal of D2 neurons being critical for choosing among actions [<xref ref-type="bibr" rid="pcbi.1005062.ref065">65</xref>]. It would be possible to also include in the ACU model the tendency to approach high value states, by including additional terms in the softmax choice rule, as in [<xref ref-type="bibr" rid="pcbi.1005062.ref066">66</xref>].</p>
<p>In the advantage and ACU models, the actor encodes the mean reward relative to the overall reward in the current state (<xref ref-type="disp-formula" rid="pcbi.1005062.e050">Eq (31)</xref>). So although the actor has the information necessary to choose which action is best in the current context, it does not know whether it is worth selecting it at all (e.g., whether any <italic>μ</italic><sub><italic>i</italic></sub> &gt; 0). The information on whether it is worth making a movement in the given state (i.e., on the average value of actions chosen by the actor) is encoded in the critic. Thus the models suggest that patch neurons, which the critic has been mapped onto, should also be involved in movement initiation. It is intriguing that patch neurons project to the dopaminergic neurons [<xref ref-type="bibr" rid="pcbi.1005062.ref019">19</xref>], so one could ask whether they may communicate the information on the value of making a movement via dopaminergic neurons. This idea is consistent with DA controlling the vigor of movements [<xref ref-type="bibr" rid="pcbi.1005062.ref067">67</xref>].</p>
</sec>
</sec>
<sec id="sec016">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005062.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005062.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Relationship between mean absolute deviation and standard deviation.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The Authors thank Hiro Nakahara, Paul Dodson, Michael Frank and Mark Walton for discussion.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005062.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Real</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Caraco</surname> <given-names>T</given-names></name>. <article-title>Risk and foraging in stochastic environments</article-title>. <source>Annual Review of Ecology and Systematics</source>. <year>1986</year>;<volume>17</volume>:<fpage>371</fpage>–<lpage>390</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.es.17.110186.002103" xlink:type="simple">10.1146/annurev.es.17.110186.002103</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mallpress</surname> <given-names>DEW</given-names></name>, <name name-style="western"><surname>Fawcett</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Houston</surname> <given-names>AI</given-names></name>, <name name-style="western"><surname>McNamara</surname> <given-names>JM</given-names></name>. <article-title>Risk attitudes in a changing environment: An evolutionary model of the fourfold pattern of risk preferences</article-title>. <source>Psychological Review</source>. <year>2015</year>;<volume>122</volume>:<fpage>364</fpage>–<lpage>375</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0038970" xlink:type="simple">10.1037/a0038970</ext-link></comment> <object-id pub-id-type="pmid">25844877</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tversky</surname> <given-names>A</given-names></name>. <article-title>Prospect theory: An analysis of decision under risk</article-title>. <source>Econometrica: Journal of the Econometric Society</source>. <year>1979</year>;p. <fpage>263</fpage>–<lpage>291</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/1914185" xlink:type="simple">10.2307/1914185</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kalman</surname> <given-names>RE</given-names></name>. <article-title>A New Approach to Linear Filtering and Prediction Problems</article-title>. <source>Journal of Basic Engineering</source>. <year>1960</year>;<volume>82</volume>:<fpage>35</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1115/1.3662552" xlink:type="simple">10.1115/1.3662552</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Audibert</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Munos</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Szepesvari</surname> <given-names>C</given-names></name>. <article-title>Exploration-exploitation tradeoff using variance estimates in multi-armed bandits</article-title>. <source>Theoretical Computer Science</source>. <year>2009</year>;<volume>410</volume>:<fpage>1876</fpage>–<lpage>1902</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tcs.2009.01.016" xlink:type="simple">10.1016/j.tcs.2009.01.016</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Fiorillo</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Adaptive Coding of Reward Value by Dopamine Neurons</article-title>. <source>Science</source>. <year>2005</year>;<volume>307</volume>:<fpage>1642</fpage>–<lpage>1645</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1105370" xlink:type="simple">10.1126/science.1105370</ext-link></comment> <object-id pub-id-type="pmid">15761155</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref008">
<label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>A model of how the basal ganglia generate ans use neural signals that predict reinforcement</chapter-title>. In: <name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Beiser</surname> <given-names>DG</given-names></name>, editors. <source>Models of information processing in the basal ganglia</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source>The Journal of neuroscience</source>. <year>1996</year>;<volume>16</volume>(<issue>5</issue>):<fpage>1936</fpage>–<lpage>1947</lpage>. <object-id pub-id-type="pmid">8774460</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mimura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Oeda</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kawamura</surname> <given-names>M</given-names></name>. <article-title>Impaired decision-making in Parkinson’s disease</article-title>. <source>Parkinsonism and Related Disorders</source>. <year>2006</year>;<volume>12</volume>:<fpage>169</fpage>–<lpage>175</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.parkreldis.2005.12.003" xlink:type="simple">10.1016/j.parkreldis.2005.12.003</ext-link></comment> <object-id pub-id-type="pmid">16563850</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gallagher</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>O’Sullivan</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Evans</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Lees</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Schrag</surname> <given-names>A</given-names></name>. <article-title>Pathological gambling in Parkinson’s disease: Risk factors and differences from dopamine dysregulation. An analysis of published case series</article-title>. <source>Movement Disorders</source>. <year>2007</year>;<volume>22</volume>:<fpage>1757</fpage>–<lpage>1763</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/mds.21611" xlink:type="simple">10.1002/mds.21611</ext-link></comment> <object-id pub-id-type="pmid">17580327</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Clark</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Dagher</surname> <given-names>A</given-names></name>. <article-title>The role of dopamine in risk taking: a specific look at Parkinson’s disease and gambling</article-title>. <source>Frontiers in behavioral neuroscience</source>. <year>2014</year>;<volume>8</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnbeh.2014.00196" xlink:type="simple">10.3389/fnbeh.2014.00196</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Orsini</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Setlow</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Floresco</surname> <given-names>SB</given-names></name>. <article-title>Neural mechanisms regulating different forms of risk-related decision-making: Insights from animal models</article-title>. <source>Neuroscience and Biobehavioral Reviews</source>. <year>2015</year>;<volume>58</volume>:<fpage>147</fpage>–<lpage>167</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neubiorev.2015.04.009" xlink:type="simple">10.1016/j.neubiorev.2015.04.009</ext-link></comment> <object-id pub-id-type="pmid">26072028</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Seeberger</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <article-title>By carrot or by stick: Cognitive reinforcement learning in Parkinsonism</article-title>. <source>Science</source>. <year>2004</year>;<volume>306</volume>:<fpage>1940</fpage>–<lpage>1943</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1102941" xlink:type="simple">10.1126/science.1102941</ext-link></comment> <object-id pub-id-type="pmid">15528409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Collins</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Opponent actor learning (OpAL): Modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title>. <source>Psychological review</source>. <year>2014</year>;<volume>121</volume>(<issue>3</issue>):<fpage>337</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0037015" xlink:type="simple">10.1037/a0037015</ext-link></comment> <object-id pub-id-type="pmid">25090423</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref016">
<label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Reinforcement Learning</source>. <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref017">
<label>17</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>AR</given-names></name>. <chapter-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</chapter-title>. In: <source>Classical conditioning: current research and theory</source>; <year>1972</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Complementary roles of basal ganglia and cerebellum in learning and motor control</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2000</year>;<volume>10</volume>:<fpage>732</fpage>–<lpage>739</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(00)00153-7" xlink:type="simple">10.1016/S0959-4388(00)00153-7</ext-link></comment> <object-id pub-id-type="pmid">11240282</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watabe-Uchida</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ogawa</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Vamanrao</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Uchida</surname> <given-names>N</given-names></name>. <article-title>Whole-Brain Mapping of Direct Inputs to Midbrain Dopamine Neurons</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>:<fpage>858</fpage>–<lpage>873</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.03.017" xlink:type="simple">10.1016/j.neuron.2012.03.017</ext-link></comment> <object-id pub-id-type="pmid">22681690</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>O’Doherty</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Deichmann</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title>. <source>science</source>. <year>2004</year>;<volume>304</volume>(<issue>5669</issue>):<fpage>452</fpage>–<lpage>454</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1094285" xlink:type="simple">10.1126/science.1094285</ext-link></comment> <object-id pub-id-type="pmid">15087550</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gerfen</surname> <given-names>CR</given-names></name>. <article-title>The neostriatal mosaic: Multiple levels of compartmental organization in the basal ganglia</article-title>. <source>Annu Rev Neurosci</source>. <year>1992</year>;<volume>15</volume>:<fpage>285</fpage>–<lpage>320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.ne.15.030192.001441" xlink:type="simple">10.1146/annurev.ne.15.030192.001441</ext-link></comment> <object-id pub-id-type="pmid">1575444</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bevan</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Shink</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bolam</surname> <given-names>JP</given-names></name>. <article-title>Microcircuitry of the direct and indirect pathways of the basal ganglia</article-title>. <source>Microcircuitry of the direct and indirect pathways of the basal ganglia</source>. <year>1998</year>;<volume>86</volume>:<fpage>353</fpage>–<lpage>387</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Albin</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Penney</surname> <given-names>JB</given-names></name>. <article-title>The functional anatomy of basal ganglia disorders</article-title>. <source>Trends in Neurosciences</source>. <year>1989</year>;<volume>12</volume>:<fpage>366</fpage>–<lpage>375</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0166-2236(89)90074-X" xlink:type="simple">10.1016/0166-2236(89)90074-X</ext-link></comment> <object-id pub-id-type="pmid">2479133</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DeLong</surname> <given-names>MR</given-names></name>. <article-title>Primate models of movement disorders of basal ganglia origin</article-title>. <source>Trends in Neurosciences</source>. <year>1990</year>;<volume>13</volume>:<fpage>281</fpage>–<lpage>285</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0166-2236(90)90110-V" xlink:type="simple">10.1016/0166-2236(90)90110-V</ext-link></comment> <object-id pub-id-type="pmid">1695404</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kravitz</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Tye</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Kreitzer</surname> <given-names>AC</given-names></name>. <article-title>Distinct roles for direct and indirect pathway striatal neurons in reinforcement</article-title>. <source>Nature neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>6</issue>):<fpage>816</fpage>–<lpage>818</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3100" xlink:type="simple">10.1038/nn.3100</ext-link></comment> <object-id pub-id-type="pmid">22544310</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref026">
<label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Luce</surname> <given-names>RD</given-names></name>. <source>Individual Choice Behavior a Theoretical Analysis</source>. <publisher-name>John Wiley and sons</publisher-name>; <year>1959</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shepard</surname> <given-names>RN</given-names></name>. <article-title>Stimulus and response generalization: tests of a model relating generalization to distance in psychological space</article-title>. <source>Journal of Experimental Psychology</source>. <year>1958</year>;<volume>55</volume>(<issue>6</issue>):<fpage>509</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0042354" xlink:type="simple">10.1037/h0042354</ext-link></comment> <object-id pub-id-type="pmid">13563763</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Mikhael JG. Learning Reward Uncertainty in the Basal Ganglia. MSc Thesis, University of Oxford; 2015.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bell</surname> <given-names>DE</given-names></name>. <article-title>Risk, return and utility</article-title>. <source>Manage Sci</source>. <year>1995</year>;<volume>41</volume>:<fpage>23</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1287/mnsc.41.1.23" xlink:type="simple">10.1287/mnsc.41.1.23</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balasubramani</surname> <given-names>PP</given-names></name>, <name name-style="western"><surname>Chakravarthy</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Ravindran</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>. <article-title>An extended reinforcement learning model of basal ganglia to understand the contributions of serotonin and dopamine in risk-based decision making, reward prediction, and punishment learning</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>47</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2014.00047" xlink:type="simple">10.3389/fncom.2014.00047</ext-link></comment> <object-id pub-id-type="pmid">24795614</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smeding</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Goudriaan</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Foncke</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Schuurman</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Speelman</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Schmand</surname> <given-names>B</given-names></name>. <article-title>Pathological gambling after bilateral subthalamic nucleus stimulation in Parkinson disease</article-title>. <source>J Neurol Neurosurg Psychiatry</source>. <year>2007</year>;<volume>78</volume>:<fpage>517</fpage>–<lpage>519</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1136/jnnp.2006.102061" xlink:type="simple">10.1136/jnnp.2006.102061</ext-link></comment> <object-id pub-id-type="pmid">17210626</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>St Onge</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Floresco</surname> <given-names>SB</given-names></name>. <article-title>Dopaminergic Modulation of Risk-Based Decision Making</article-title>. <source>Neuropsychopharmacology</source>. <year>2009</year>;<volume>34</volume>:<fpage>681</fpage>–<lpage>697</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/npp.2008.121" xlink:type="simple">10.1038/npp.2008.121</ext-link></comment> <object-id pub-id-type="pmid">18668030</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nedler</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Mead</surname> <given-names>R</given-names></name>. <article-title>A Simplex Method for Function Minimization</article-title>. <source>The Computer Journal</source>. <year>1965</year>;<volume>7</volume>:<fpage>308</fpage>–<lpage>313</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/comjnl/7.4.308" xlink:type="simple">10.1093/comjnl/7.4.308</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Flajolet</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Greengard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Surmeier</surname> <given-names>DJ</given-names></name>. <article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title>. <source>Science</source>. <year>2008</year>;<volume>321</volume>(<issue>5890</issue>):<fpage>848</fpage>–<lpage>851</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1160575" xlink:type="simple">10.1126/science.1160575</ext-link></comment> <object-id pub-id-type="pmid">18687967</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Richfield</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>AB</given-names></name>. <article-title>Anatomical and affinity state comparison between dopamine D1 and D2 receptors in the rat central nervous system</article-title>. <source>Neuroscience</source>. <year>1989</year>;<volume>30</volume>:<fpage>767</fpage>–<lpage>777</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0306-4522(89)90168-1" xlink:type="simple">10.1016/0306-4522(89)90168-1</ext-link></comment> <object-id pub-id-type="pmid">2528080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dreyer</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Herrik</surname> <given-names>KF</given-names></name>, <name name-style="western"><surname>Berg</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Hounsgaard</surname> <given-names>JD</given-names></name>. <article-title>Influence of phasic and tonic dopamine release on receptor activation</article-title>. <source>The Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>(<issue>42</issue>):<fpage>14273</fpage>–<lpage>14283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1894-10.2010" xlink:type="simple">10.1523/JNEUROSCI.1894-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20962248</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dodson</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Dreyer</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Jennings</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Syed</surname> <given-names>ECJ</given-names></name>, <name name-style="western"><surname>Wade-Martins</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Cragg</surname> <given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Representation of spontaneous movement by dopaminergic neurons is cell-type selective and disrupted in parkinsonism</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;p. <fpage>201515941</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rice</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Cragg</surname> <given-names>SJ</given-names></name>. <article-title>Dopamine spillover after quantal release: rethinking dopamine transmission in the nigrostriatal pathway</article-title>. <source>Brain research reviews</source>. <year>2008</year>;<volume>58</volume>(<issue>2</issue>):<fpage>303</fpage>–<lpage>313</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.brainresrev.2008.02.004" xlink:type="simple">10.1016/j.brainresrev.2008.02.004</ext-link></comment> <object-id pub-id-type="pmid">18433875</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref039">
<label>39</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <chapter-title>Motivated reinforcement learning</chapter-title>. In: <source>Advances in neural information processing systems</source>; <year>2002</year>. p. <fpage>11</fpage>–<lpage>18</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Reward, Motivation, and Reinforcement Learning</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>:<fpage>285</fpage>–<lpage>298</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(02)00963-7" xlink:type="simple">10.1016/S0896-6273(02)00963-7</ext-link></comment> <object-id pub-id-type="pmid">12383782</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Haughey</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Curran</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hutchison</surname> <given-names>KE</given-names></name>. <article-title>Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>41</issue>):<fpage>16311</fpage>–<lpage>16316</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0706111104" xlink:type="simple">10.1073/pnas.0706111104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shiner</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wunderlich</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hill</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bhatia</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Dopamine and performance in a reinforcement learning task: evidence from Parkinson’s disease</article-title>. <source>Brain</source>. <year>2012</year>;<volume>135</volume>(<issue>6</issue>):<fpage>1871</fpage>–<lpage>1883</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/aws083" xlink:type="simple">10.1093/brain/aws083</ext-link></comment> <object-id pub-id-type="pmid">22508958</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Grogan JP. Roles of Dopamine in Human Memory. PhD Thesis, University of Bristol; 2015.</mixed-citation>
</ref>
<ref id="pcbi.1005062.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hertwig</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Erev</surname> <given-names>I</given-names></name>. <article-title>The description-experience gap in risky choice</article-title>. <source>Trends in Cognitive Sciences</source>. <day>20</day>;<volume>13</volume>:<fpage>517</fpage>–<lpage>523</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2009.09.004" xlink:type="simple">10.1016/j.tics.2009.09.004</ext-link></comment> <object-id pub-id-type="pmid">19836292</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Poletti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cavedini</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bonuccelli</surname> <given-names>U</given-names></name>. <article-title>Iowa Gambling Task in Parkinson’s Disease</article-title>. <source>Journal of Clinical and Experimental Neuropsychology</source>. <year>2011</year>;<volume>33</volume>:<fpage>395</fpage>–<lpage>409</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/13803395.2010.524150" xlink:type="simple">10.1080/13803395.2010.524150</ext-link></comment> <object-id pub-id-type="pmid">21140314</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Castrioto</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Funkiewiez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Debû</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Cools</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lhommée</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ardouin</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Iowa gambling task impairment in Parkinson’s disease can be normalised by reduction of dopaminergic medication after subthalamic stimulation</article-title>. <source>Journal of Neurology Neurosurgery and Psychiatry</source>. <year>2015</year>;<volume>86</volume>:<fpage>186</fpage>–<lpage>190</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1136/jnnp-2013-307146" xlink:type="simple">10.1136/jnnp-2013-307146</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zalocusky</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Ramakrishnan</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lerner</surname> <given-names>TN</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Knutson</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>. <article-title>Nucleus accumbens D2R cells signal prior outcomes and control risky decision-making</article-title>. <source>Nature</source>. <year>2016</year>;<volume>531</volume>(<issue>7596</issue>):<fpage>642</fpage>–<lpage>646</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature17400" xlink:type="simple">10.1038/nature17400</ext-link></comment> <object-id pub-id-type="pmid">27007845</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Morris</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Arkadir</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nevet</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>. <article-title>Coincident but Distinct Messages of Midbrain Dopamine and Striatal Tonically Active Neurons</article-title>. <source>Neuron</source>. <year>2004</year>;<volume>43</volume>:<fpage>133</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2004.06.012" xlink:type="simple">10.1016/j.neuron.2004.06.012</ext-link></comment> <object-id pub-id-type="pmid">15233923</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cohen</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Haesler</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vong</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lowell</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Uchida</surname> <given-names>N</given-names></name>. <article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title>. <source>Nature</source>. <year>2012</year>;<volume>482</volume>(<issue>7383</issue>):<fpage>85</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature10754" xlink:type="simple">10.1038/nature10754</ext-link></comment> <object-id pub-id-type="pmid">22258508</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Lonjers</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chistiakova</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Volgushev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bazhenov</surname> <given-names>M</given-names></name>. <article-title>Heterosynaptic Plasticity Prevents Runaway Synaptic Dynamics</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>:<fpage>15915</fpage>–<lpage>15929</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5088-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5088-12.2013</ext-link></comment> <object-id pub-id-type="pmid">24089497</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fiorillo</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Discrete coding of reward probability and uncertainty by dopamine neurons</article-title>. <source>Science</source>. <year>2003</year>;<volume>299</volume>(<issue>5614</issue>):<fpage>1898</fpage>–<lpage>1902</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1077349" xlink:type="simple">10.1126/science.1077349</ext-link></comment> <object-id pub-id-type="pmid">12649484</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yu</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty, neuromodulation, and attention</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>(<issue>4</issue>):<fpage>681</fpage>–<lpage>692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.04.026" xlink:type="simple">10.1016/j.neuron.2005.04.026</ext-link></comment> <object-id pub-id-type="pmid">15944135</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Voon</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Brezing</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Symmonds</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ekanayake</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Fernandez</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Dopamine agonists and risk: impulse control disorders in Parkinson’s; disease</article-title>. <source>Brain</source>. <year>2011</year>;<volume>134</volume>(<issue>5</issue>):<fpage>1438</fpage>–<lpage>1446</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/awr080" xlink:type="simple">10.1093/brain/awr080</ext-link></comment> <object-id pub-id-type="pmid">21596771</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ida</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Goto</surname> <given-names>R</given-names></name>. <article-title>Simultaneous measurement of time and risk preferences: stated preference discrete choice modeling analysis depending on smoking behavior</article-title>. <source>International Economic Review</source>. <year>2009</year>;<volume>50</volume>:<fpage>1169</fpage>–<lpage>1182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1468-2354.2009.00564.x" xlink:type="simple">10.1111/j.1468-2354.2009.00564.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Long</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Kuhn</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Platt</surname> <given-names>ML</given-names></name>. <article-title>Serotonin shapes risky decision making in monkeys</article-title>. <source>Social Cognitive and Affective Neuroscience</source>. <year>2009</year>;<volume>4</volume>:<fpage>346</fpage>–<lpage>356</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/scan/nsp020" xlink:type="simple">10.1093/scan/nsp020</ext-link></comment> <object-id pub-id-type="pmid">19553236</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Larsen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Leslie</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>. <article-title>Posterior Weighted Reinforcement Learning with State Uncertainty</article-title>. <source>Neural Computation</source>. <year>2010</year>;<volume>22</volume>:<fpage>1149</fpage>–<lpage>1179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2010.01-09-948" xlink:type="simple">10.1162/neco.2010.01-09-948</ext-link></comment> <object-id pub-id-type="pmid">20100078</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balasubramani</surname> <given-names>PP</given-names></name>, <name name-style="western"><surname>Chakravarthy</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Ravindran</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>. <article-title>A network model of basal ganglia for understanding the roles of dopamine and serotonin in reward-punishment-risk based decision making</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2015</year>;<volume>9</volume>:<fpage>76</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2015.00076" xlink:type="simple">10.3389/fncom.2015.00076</ext-link></comment> <object-id pub-id-type="pmid">26136679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hasbi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>O’Dowd</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>George</surname> <given-names>SR</given-names></name>. <article-title>Dopamine D1–D2 receptor heteromer signaling pathway in the brain: emerging physiological relevance</article-title>. <source>Molecular Brain</source>. <year>2011</year>;<volume>4</volume>:<fpage>26</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1756-6606-4-26" xlink:type="simple">10.1186/1756-6606-4-26</ext-link></comment> <object-id pub-id-type="pmid">21663703</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mihatsch</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Neuneier</surname> <given-names>R</given-names></name>. <article-title>Risk-sensitive reinforcement learning</article-title>. <source>Machine learning</source>. <year>2002</year>;<volume>49</volume>(<issue>2–3</issue>):<fpage>267</fpage>–<lpage>290</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1017940631555" xlink:type="simple">10.1023/A:1017940631555</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>A theory of cortical responses</article-title>. <source>Philosophical Transactions of the Royal Society B</source>. <year>2005</year>;<volume>360</volume>:<fpage>815</fpage>–<lpage>836</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2005.1622" xlink:type="simple">10.1098/rstb.2005.1622</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>. <article-title>A tutorial on the free-energy framework for modelling perception and learning</article-title>. <source>Journal of Mathematical Psychology</source>. <year>2016</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jmp.2015.11.003" xlink:type="simple">10.1016/j.jmp.2015.11.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Franklin</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>A cholinergic feedback circuit to regulate striatal population uncertainty and optimize reinforcement learning</article-title>. <source>eLife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e12029</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.12029" xlink:type="simple">10.7554/eLife.12029</ext-link></comment> <object-id pub-id-type="pmid">26705698</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MF</given-names></name>. <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nature neuroscience</source>. <year>2007</year>;<volume>10</volume>(<issue>9</issue>):<fpage>1214</fpage>–<lpage>1221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1954" xlink:type="simple">10.1038/nn1954</ext-link></comment> <object-id pub-id-type="pmid">17676057</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Iglesias</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mathys</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brodersen</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Kasper</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Piccirelli</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>den Ouden</surname> <given-names>HE</given-names></name>, <etal>et al</etal>. <article-title>Hierarchical prediction errors in midbrain and basal forebrain during sensory learning</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>(<issue>2</issue>):<fpage>519</fpage>–<lpage>530</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.09.009" xlink:type="simple">10.1016/j.neuron.2013.09.009</ext-link></comment> <object-id pub-id-type="pmid">24139048</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keeler</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Pretsell</surname> <given-names>DO</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>. <article-title>Functional implications of dopamine D1 vs. D2 receptors: A ‘prepare and select’ model of the striatal direct vs. indirect pathways</article-title>. <source>Neuroscience</source>. <year>2014</year>;<volume>282</volume>:<fpage>156</fpage>–<lpage>175</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroscience.2014.07.021" xlink:type="simple">10.1016/j.neuroscience.2014.07.021</ext-link></comment> <object-id pub-id-type="pmid">25062777</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Cools</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gölzer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Friedel</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Heinz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <etal>et al</etal>. <article-title>Disentangling the roles of approach, activation and valence in instrumental and pavlovian responding</article-title>. <source>PLoS Computational Biology</source>. <year>2011</year>;<volume>7</volume>(<issue>4</issue>):<fpage>e1002028</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002028" xlink:type="simple">10.1371/journal.pcbi.1002028</ext-link></comment> <object-id pub-id-type="pmid">21556131</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005062.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Tonic dopamine: opportunity costs and the control of response vigor</article-title>. <source>Psychopharmacology</source>. <year>2007</year>;<volume>191</volume>(<issue>3</issue>):<fpage>507</fpage>–<lpage>520</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00213-006-0502-4" xlink:type="simple">10.1007/s00213-006-0502-4</ext-link></comment> <object-id pub-id-type="pmid">17031711</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>