<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006043</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00914</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Control engineering</subject><subj-group><subject>Control theory</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Control theory</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Control theory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Control engineering</subject><subj-group><subject>Control systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Control systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Control systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Rational metareasoning and the plasticity of cognitive control</article-title>
<alt-title alt-title-type="running-head">Rational metareasoning and the plasticity of cognitive control</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2746-6110</contrib-id>
<name name-style="western">
<surname>Lieder</surname>
<given-names>Falk</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Shenhav</surname>
<given-names>Amitai</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Musslick</surname>
<given-names>Sebastian</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Griffiths</surname>
<given-names>Thomas L.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Helen Wills Neuroscience Institute, University of California, Berkeley, Berkeley, California, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Brown Institute for Brain Science, Department of Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, Rhode Island, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Institute for Cognitive and Brain Sciences, Department of Psychology, University of California, Berkeley, Berkeley, California, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">falk.lieder@berkeley.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>25</day>
<month>4</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>4</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>4</issue>
<elocation-id>e1006043</elocation-id>
<history>
<date date-type="received">
<day>7</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Lieder et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006043"/>
<abstract>
<p>The human brain has the impressive capacity to adapt how it processes information to high-level goals. While it is known that these cognitive control skills are malleable and can be improved through training, the underlying plasticity mechanisms are not well understood. Here, we develop and evaluate a model of how people learn when to exert cognitive control, which controlled process to use, and how much effort to exert. We derive this model from a general theory according to which the function of cognitive control is to select and configure neural pathways so as to make optimal use of finite time and limited computational resources. The central idea of our Learned Value of Control model is that people use reinforcement learning to predict the value of candidate control signals of different types and intensities based on stimulus features. This model correctly predicts the learning and transfer effects underlying the adaptive control-demanding behavior observed in an experiment on visual attention and four experiments on interference control in Stroop and Flanker paradigms. Moreover, our model explained these findings significantly better than an associative learning model and a Win-Stay Lose-Shift model. Our findings elucidate how learning and experience might shape people’s ability and propensity to adaptively control their minds and behavior. We conclude by predicting under which circumstances these learning mechanisms might lead to self-control failure.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>The human brain has the impressive ability to adapt how it processes information to high level goals. While it is known that these cognitive control skills are malleable and can be improved through training, the underlying plasticity mechanisms are not well understood. Here, we derive a computational model of how people learn when to exert cognitive control, which controlled process to use, and how much effort to exert from a formal theory of the function of cognitive control. Across five experiments, we find that our model correctly predicts that people learn to adaptively regulate their attention and decision-making and how these learning effects transfer to novel situations. Our findings elucidate how learning and experience might shape people’s ability and propensity to adaptively control their minds and behavior. We conclude by predicting under which circumstances these learning mechanisms might lead to self-control failure.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000006</institution-id>
<institution>Office of Naval Research</institution>
</institution-wrap>
</funding-source>
<award-id>MURI N00014-13-1-0431</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Griffiths</surname>
<given-names>Thomas L.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by the Office of Naval Research through Grant No. MURI N00014-13-1-0431 to TLG. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="2"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-05-07</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The human brain has the impressive ability to adapt how it processes information and responds to stimuli in the service of high level goals, such as writing an article [<xref ref-type="bibr" rid="pcbi.1006043.ref001">1</xref>]. The mechanisms underlying this behavioral flexibility range from seemingly simple processes, such as inhibiting the impulse to browse your Facebook feed, to very complex processes such as orchestrating your thoughts to reach a solid conclusion. Our capacity for <italic>cognitive control</italic> enables us to override automatic processes when they are inappropriate for the current situation or misaligned with our current goals. One of the paradigms used to study cognitive control is the Stroop task, where participants are instructed to name the hue of a color word (e.g., respond “green” when seeing the stimulus <inline-formula id="pcbi.1006043.e032"><alternatives><graphic id="pcbi.1006043.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mtext mathcolor="#008000">RED</mml:mtext><mml:mo mathcolor="#008000">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> while inhibiting their automatic tendency to read the word (“red”) [<xref ref-type="bibr" rid="pcbi.1006043.ref002">2</xref>]. Similarly, in the Eriksen flanker task, participants are asked to report the identity of a target stimulus surrounded by multiple distractors while overcoming their automatic tendency to respond instead to the distractors. Individual differences in the capacity for cognitive control are highly predictive of academic achievement, interpersonal success, and many other important life outcomes [<xref ref-type="bibr" rid="pcbi.1006043.ref003">3</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref004">4</xref>].</p>
<p>While exerting cognitive control improves people’s performance in these tasks, it is also effortful and appears to be intrinsically costly [<xref ref-type="bibr" rid="pcbi.1006043.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref006">6</xref>]. The Expected Value of Control (EVC) theory maintains that the brain therefore specifies how much control to exert according to a rational cost-benefit analysis, weighing these effort costs against attendant rewards for achieving one’s goals [<xref ref-type="bibr" rid="pcbi.1006043.ref007">7</xref>]. In broad accord with the predictions of the EVC theory, previous research has found that control specification is context-sensitive [<xref ref-type="bibr" rid="pcbi.1006043.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref009">9</xref>] and modulated by reward across multiple domains [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>], such as attention, response inhibition, interference control, and task switching. While previous theories account for that fact that people’s performance in these task is sensitive to reward [<xref ref-type="bibr" rid="pcbi.1006043.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref014">14</xref>], it remains unclear <italic>how</italic> these dependencies arise from people’s experience. Recently, it has been proposed that the underlying mechanism is associative learning [<xref ref-type="bibr" rid="pcbi.1006043.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref016">16</xref>]. Indeed, a number of studies have demonstrated that cognitive control specification is plastic: whether people exert cognitive control in a given situation, which controlled processes they employ, and how much control they allocate to them is learned from experience. For instance, it has been demonstrated that participants in visual search tasks gradually learn to allocate their attention to locations whose features predict the appearance of a target [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>], and a recent study found that learning continuously adjusts how much cognitive control people exert in a Stroop task with changing difficulty [<xref ref-type="bibr" rid="pcbi.1006043.ref018">18</xref>]. Furthermore, it has been shown that people learn to exert more cognitive control after their performance on a control-demanding task was rewarded [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>] and learn to exert more control in response to potentially control-demanding stimuli that are associated with reward than to those that are not [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>].</p>
<p>These studies provide evidence that people can use information from their environment (e.g., stimulus features) to learn when to exert cognitive control and how to exert control, and it has recently been suggested that this can be thought of in terms of associative learning [<xref ref-type="bibr" rid="pcbi.1006043.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref016">16</xref>]. Other studies suggested that cognitive control can be improved through training [<xref ref-type="bibr" rid="pcbi.1006043.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref021">21</xref>]. However, achieving transfer remains challenging [<xref ref-type="bibr" rid="pcbi.1006043.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref025">25</xref>], the underlying learning mechanisms are poorly understood, and there is currently no theory that could be used to determine which training regimens will be most effective and which real-life situations the training will transfer to. Developing precise computational models of the plasticity of cognitive control may be a promising way to address these problems and to enable more effective training programs for remediating executive dysfunctions and enabling people to pursue their goals more effectively.</p>
<p>In this article, we extend the EVC theory to develop a theoretical framework for modeling the function and plasticity of cognitive control specification. This extension incorporates recent theoretical advances inspired by the rational metareasoning framework developed in the artificial intelligence literature [<xref ref-type="bibr" rid="pcbi.1006043.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref027">27</xref>]. We leverage the resulting framework to derive the Learned Value of Control (LVOC) model which can learn to efficiently select control signals based on features of the task environment. The LVOC model can be used to simulate cognitive control (e.g., responding to a goal-relevant target that competes with distractors) and, more importantly, how it is shaped by learning. According to the LVOC model, people learn the value of different cognitive control signals (e.g., how much to attend one stimulus or another). A key strength of this model is that it is very general and can be applied to phenomena ranging from simple learning effects in the Stroop task to the acquisition of complex strategies for reasoning and problem-solving. In order to demonstrate the validity and generality of this model, we show that it can capture the empirical findings of five cognitive control experiments on the plasticity of visual attention [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>], the interacting effects of reward and task difficulty on the plasticity of interference control [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>], and the transfer of such learning to novel stimuli [<xref ref-type="bibr" rid="pcbi.1006043.ref008">8</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref009">9</xref>]. Moreover, the LVOC model outperforms alternate models of such learning processes that rely only on associative learning or a basic win-lose-stay-shift strategy. Our findings shed light on how learning and experience might shape people’s ability and propensity to adaptively control their minds and behavior, and the LVOC model predicts under which circumstances these mechanisms might lead to self-control failure.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Models</title>
<sec id="sec003">
<title>Formalizing the function of cognitive control</title>
<p>At an abstract level, all cognitive control processes serve the same function: to adapt neural information processing to achieve a goal [<xref ref-type="bibr" rid="pcbi.1006043.ref028">28</xref>]. At this abstract level, neural information processing can be characterized by the computations being performed, and the extent to which the brain achieves its goals can be quantified by the expected utility of the resulting actions. From this perspective, an important function of cognitive control is to select computations so as to maximize the agent’s reward rate (i.e., reward per unit time). This problem is formally equivalent to the <italic>rational metareasoning</italic> [<xref ref-type="bibr" rid="pcbi.1006043.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref029">29</xref>] problem studied in computer science: selecting computations so as to make optimal use of the controlled system’s limited computational resources (i.e., to achieve the highest possible sum of rewards with a limited amount of computation).</p>
<p>Thus, rational metareasoning suggests that the specification of cognitive control is a metacognitive decision problem. In reinforcement learning [<xref ref-type="bibr" rid="pcbi.1006043.ref030">30</xref>], decision problems are typically defined by a set of possible actions, the set of possible states, an initial state, the conditional probabilities of transitioning from one state to another depending on the action taken by the agent, and a reward function. Together these five components define a Markov decision process (MDP [<xref ref-type="bibr" rid="pcbi.1006043.ref030">30</xref>]). In a typical application of this framework the agent is an animal, robot, or computer program, actions are behaviors (e.g., pressing a lever), the state characterizes the external environment ℰ (e.g., the rat’s location in the maze), and the rewards are obtained from the environment (e.g., pressing a lever dispenses cheese). In general, the agent cannot observe the state of the environment directly; for instance, the rat running through a maze does not have direct access to its location but has to infer this from sensory observations. The decision problems posed by an environment that is only partially observable can be modelled as a partially observable MDP (POMDP [<xref ref-type="bibr" rid="pcbi.1006043.ref031">31</xref>]). For each POMDP there is an equivalent MDP whose state encodes what the agent knows about the environment and is thus fully observable; this is known as the belief-MDP [<xref ref-type="bibr" rid="pcbi.1006043.ref031">31</xref>].</p>
<p>Critically, the belief-MDP formalism can also be applied to the choice of internal computations [<xref ref-type="bibr" rid="pcbi.1006043.ref027">27</xref>]–such as allocating attention [<xref ref-type="bibr" rid="pcbi.1006043.ref032">32</xref>] or gating information into working memory [<xref ref-type="bibr" rid="pcbi.1006043.ref033">33</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref034">34</xref>]–rather than only physical actions. In the rational metareasoning framework, the agent is the cognitive control system whose actions are control signals that specify which computations the controlled systems should perform. The internal state of the controlled systems is only partially observable. We can formally define the problem of optimal cognitive control specification as maximizing reward the in the meta-level MDP
<disp-formula id="pcbi.1006043.e001">
<alternatives>
<graphic id="pcbi.1006043.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi mathvariant="normal">M</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <inline-formula id="pcbi.1006043.e002"><alternatives><graphic id="pcbi.1006043.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> is the set of possible information states, comprising beliefs about the external environment (e.g., the choices afforded by the current situation) and beliefs about the agent’s internal state (e.g., the decision system’s estimates of the choices’ utilities), <italic>s</italic><sub>0</sub> denotes the initial information state, <inline-formula id="pcbi.1006043.e003"><alternatives><graphic id="pcbi.1006043.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi mathvariant="script">C</mml:mi></mml:math></alternatives></inline-formula> is the set of possible control signals that may be discrete (e.g., “Simulate action 1.”) or continuous (e.g., “Increase the decision threshold by 0.175.” or “Suppress the activity of the word-reading pathway by 75%.”), <italic>T</italic> is a transition model, and <italic>r</italic> is the reward function that cognitive control seeks to maximize. The transition model specifies the conditional probability of transitioning from belief state <italic>s</italic> to belief state <italic>s</italic>′ if the control signal is <italic>c</italic> by <italic>T</italic>(<italic>s</italic>, <italic>c</italic>, <italic>s</italic>′). The meta-level reward function <italic>r</italic> combines the utility of outcome <italic>X</italic> (of actions resulting from control signal <italic>c</italic> in belief state <italic>s</italic>) with the computational cost associated with exerting cognitive control:
<disp-formula id="pcbi.1006043.e004">
<alternatives>
<graphic id="pcbi.1006043.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>X</italic> is the outcome of the resulting action, <italic>u</italic> is utility function of the brain’s reward system, and cost(<italic>s</italic>, <italic>c</italic>) is the cost of implementing the controlled process.</p>
<p>Within this framework, we can define a cognitive control strategy <inline-formula id="pcbi.1006043.e005"><alternatives><graphic id="pcbi.1006043.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mi>π</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">C</mml:mi></mml:math></alternatives></inline-formula> as a mapping from belief states <inline-formula id="pcbi.1006043.e006"><alternatives><graphic id="pcbi.1006043.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> to control signals <inline-formula id="pcbi.1006043.e007"><alternatives><graphic id="pcbi.1006043.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">C</mml:mi></mml:math></alternatives></inline-formula>. The optimal cognitive control strategy <italic>π</italic><sup>⋆</sup> is the one that always chooses the computation with the highest expected value of computation (EVOC):
<disp-formula id="pcbi.1006043.e008">
<alternatives>
<graphic id="pcbi.1006043.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mi>s</mml:mi><mml:mo>↦</mml:mo><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mspace width="4pt"/><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>The EVOC is the expected sum of computational costs and benefits of performing the computation specified by the control signal <italic>c</italic> and continuing optimally from there on:
<disp-formula id="pcbi.1006043.e009">
<alternatives>
<graphic id="pcbi.1006043.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>]</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where <inline-formula id="pcbi.1006043.e010"><alternatives><graphic id="pcbi.1006043.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is known as the Q-function of the optimal control strategy <italic>π</italic><sup>⋆</sup>, and <inline-formula id="pcbi.1006043.e011"><alternatives><graphic id="pcbi.1006043.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> is the expected sum of meta-level rewards of starting <italic>π</italic><sup>⋆</sup> in state <italic>S</italic><sub><italic>t</italic>+1</sub>.</p>
<p>In summary, cognitive control specification selects the sequence of cognitive control signals that maximizes the expected sum of rewards of the resulting actions minus the cost of the controlled process. The optimal solution to this problem is given by the optimal control policy <italic>π</italic><sup>⋆</sup>.</p>
<p>So far, we have assumed that the cognitive control system chooses one control signal at a time, but <bold><italic>c</italic></bold> could also be a vector comprising multiple control signals (e.g., one that increases the rate at which evidence is accumulated towards the correct decision via an attentional mechanism and a second one that adjusts the decision threshold). Furthermore, overriding a habit by a well-reasoned decision also requires executing a coordinated sequence of cognitive operations for planning and reasoning. Instead of specifying each of these operations by a separate control signal, the cognitive control system might sometimes use a single control signal to instruct the decision system to execute an entire planning strategy. The rational metareasoning framework allows us to model cognitive strategies as options [<xref ref-type="bibr" rid="pcbi.1006043.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref038">38</xref>]. An option is a policy combined with an initiation set and a termination condition [<xref ref-type="bibr" rid="pcbi.1006043.ref038">38</xref>]. Options can be treated as if they were elementary computations and elementary computations can be interpreted as options that terminate after the first step. With this extension, the optimal solution to the cognitive control specification problem becomes
<disp-formula id="pcbi.1006043.e012">
<alternatives>
<graphic id="pcbi.1006043.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>arg</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">O</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
where the set of options <inline-formula id="pcbi.1006043.e013"><alternatives><graphic id="pcbi.1006043.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mi mathvariant="script">O</mml:mi></mml:math></alternatives></inline-formula> may include control strategies and elementary control signals.</p>
<p>Critically, this rational metareasoning perspective on cognitive control covers not only simple phenomena, such as inhibiting a pre-potent automatic response in the Stroop task, but also more complex ones, such as sequencing one’s thoughts so as to follow a good decision strategy, and very complex phenomena such as reasoning about how to best solve a complex problem.</p>
</sec>
<sec id="sec004">
<title>The LVOC model of the plasticity of cognitive control specification</title>
<p>The computations required to determine the expected value of control may themselves be costly and time consuming. Yet, in some situations cognitive control has to be engaged very rapidly, because maladaptive reflexes, impulses, and habitual responses have to be inhibited before the triggered response has been executed. In such situations, there is simply not enough time to compute the expected value of control on the fly. Fortunately, this may not be necessary because an approximation to the EVOC can be learned from experience. We therefore hypothesize that the cognitive control system learns to predict the context-dependent value of alternative control signals. By understanding how this learning occurs, we might be able to explain the experience-dependent changes in how people use their capacity for cognitive control, which we will refer to as the plasticity of cognitive control specification. In addition to these systematic, experience-driven changes cognitive control is also intrinsically variable. To model the plasticity and the variability of cognitive control, this section develops a model that combines a novel feature-based learning mechanism with a new control specification mechanism that explores promising control signals probabilistically to accelerate learning which of them is most effective.</p>
<p>The previous section characterized the problem of cognitive control specification as a sequential meta-decision problem. This makes reinforcement learning algorithms [<xref ref-type="bibr" rid="pcbi.1006043.ref039">39</xref>] a natural starting point for exploring how the cognitive control systems learns the EVOC from experience. Approximate Q-learning appears particularly suitable because the optimal control strategy can be expressed in terms of the optimal Q-function (Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e008">3</xref>–<xref ref-type="disp-formula" rid="pcbi.1006043.e012">5</xref>). From this perspective, the plasticity mechanisms of cognitive control specification serve to learn an approximation to the value <italic>Q</italic><sub><italic>t</italic></sub>(<italic>s</italic>, <italic>c</italic>) of selecting control signal <italic>c</italic> in state <italic>s</italic> based on one’s experience with selecting control signals <bold><italic>c</italic></bold> = (<italic>c</italic><sub>1</sub>,⋯,<italic>c</italic><sub><italic>t</italic></sub>) in states <bold><italic>s</italic></bold> = (<italic>s</italic><sub>1</sub>,⋯,<italic>s</italic><sub><italic>t</italic></sub>) and receiving the meta-level rewards <bold><italic>r</italic></bold> = (<italic>r</italic><sub>1</sub>,⋯,<italic>r</italic><sub><italic>t</italic></sub>). Learning an approximate Q-function <italic>Q</italic><sub><italic>t</italic></sub> from this information could enable the cognitive control system to efficiently select a control strategy by comparing learned values rather than reasoning about their effects.</p>
<p>Learning the optimal meta-level state-value function <italic>Q</italic><sup>⋆</sup> can be challenging because the value of each control signal may depend on the outcomes of the control signals selected afterwards. Furthermore, the state space of the meta-level MDP has a very high dimensionality as it comprises all possible states that the controlled system could be in. To overcome these challenges, a neural system like the brain might learn a linear approximation to the meta-level state value function instead of estimating each of its entries separately. Concretely, the cognitive control system might learn to predict the value of selecting a control strategy (e.g., focusing on the presenting speaker instead of attending to an incoming phone call) by a weighted sum of features of the internal state and the current context (e.g. being in a conference room). For instance, the value Q<sup>⋆</sup>(<italic>s</italic>, <italic>c</italic>) of choosing control signal <italic>c</italic> in the internal state <italic>s</italic> can be predicted from the features <italic>f</italic><sub><italic>k</italic></sub>(<italic>s</italic>), the implied control signal intensities <bold><italic>c</italic></bold>, their interactions with the features, that is <italic>f</italic><sub><italic>k</italic></sub>(<italic>s</italic>) <italic>c</italic><sub><italic>i</italic></sub>, and their costs. Concretely, the EVOC of selecting control signal <italic>c</italic> in state <italic>s</italic> is approximated by the <italic>Learned Value of Control</italic> (LVOC),
<disp-formula id="pcbi.1006043.e014">
<alternatives>
<graphic id="pcbi.1006043.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>LVOC</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>w</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext>cost</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where the weight vector <bold><italic>w</italic></bold> includes the offset <italic>w</italic><sub>0</sub>, the weights <inline-formula id="pcbi.1006043.e015"><alternatives><graphic id="pcbi.1006043.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> of the states’ features, the weights <bold><italic>w</italic></bold><sup>(<italic>c</italic>)</sup> of the control signal intensities, the weights <inline-formula id="pcbi.1006043.e016"><alternatives><graphic id="pcbi.1006043.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> of their interaction terms, the weight <italic>w</italic><sup>(<italic>T</italic>)</sup> of the response time <italic>T</italic>, and cost(<italic>c</italic>) is the intrinsic cost of control which scales with the amount of cognitive control applied to the task.</p>
<p>The optimal way to update the weights based on experience in a stationary environment is given by Bayes rule. Our model therefore maintains and continues to update an approximation to the posterior distribution
<disp-formula id="pcbi.1006043.e017">
<alternatives>
<graphic id="pcbi.1006043.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>∝</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
on the weight vector <bold><italic>w</italic></bold> given its experience <italic>e</italic><sub>1,⋯,<italic>t</italic></sub> up until the present time <italic>t</italic>, where each experience <italic>e</italic><sub><italic>i</italic></sub> = (<italic>s</italic><sub><italic>i</italic></sub>, <italic>c</italic><sub><italic>i</italic></sub>, <italic>r</italic><sub><italic>i</italic></sub>, <italic>T</italic><sub><italic>i</italic></sub>, <italic>s</italic><sub><italic>i</italic>+1</sub>) comprises the state, the selected control signal, the reward, the response time, and the next state. In simple settings where a single control signal determines a single reward our model’s learning mechanism is equivalent to Bayesian linear regression [<xref ref-type="bibr" rid="pcbi.1006043.ref040">40</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref041">41</xref>]. In more complex settings involving a series of control signals or delayed rewards the learning rule approximates the Bayesian update by substituting the delayed costs and benefits of control by the model’s predictions. For more details, see <xref ref-type="supplementary-material" rid="pcbi.1006043.s001">S1 Text</xref>.</p>
<p>If the value of control is initially unknown, the optimal way to select control signals is to balance exploiting previous experience to maximize the expected immediate performance with exploring alternative control allocations that might prove even more effective. Our model solves this dilemma by an exploration strategy similar to Thompson sampling: It draws <italic>k</italic> samples from the posterior distribution on the weights and averages them, that is
<disp-formula id="pcbi.1006043.e018">
<alternatives>
<graphic id="pcbi.1006043.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>According to the LVOC model the brain then selects a control signal by maximizing the EVOC predicted by the average weight <inline-formula id="pcbi.1006043.e019"><alternatives><graphic id="pcbi.1006043.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, that is
<disp-formula id="pcbi.1006043.e020">
<alternatives>
<graphic id="pcbi.1006043.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mtext>arg</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mspace width="4pt"/><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mspace width="4pt"/><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>Together, Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e014">6</xref>–<xref ref-type="disp-formula" rid="pcbi.1006043.e020">9</xref> define the LVOC model of the plasticity of cognitive control. The LVOC model extends the EVC theory [<xref ref-type="bibr" rid="pcbi.1006043.ref007">7</xref>] which defines optimal control signals in terms of the EVOC (<xref ref-type="disp-formula" rid="pcbi.1006043.e008">Eq 3</xref>), by proposing two mechanisms through which the brain might be able to approximate this normative ideal: learning a feature-based, probabilistic model of the EVOC (Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e014">6</xref> and <xref ref-type="disp-formula" rid="pcbi.1006043.e017">7</xref>) and selecting control signals by sampling from this model (Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e018">8</xref> and <xref ref-type="disp-formula" rid="pcbi.1006043.e020">9</xref>). This model is very general and can be applied to model cognitive control of many different processes (e.g., which location to saccade to vs. how strongly to inhibit the word-reading pathway) and different components of the same process (e.g., rate of evidence accumulation towards the correct decision vs. the decision threshold). The LVOC model’s core assumptions are that the brain learns to predict the EVOC of alternative control specifications from features of the situation and the control signals, and that the brain then probabilistically selects the control specification with the highest predicted value of control. Both of these components could be implemented by many different mechanisms. For instance, instead of implementing the proposed approximation to Bayesian regression, the brain might learn to predict the EVOC through the reward-modulated associative plasticity mechanism outlined in the SI. We are therefore not committed to the specific instantiation we used (Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e017">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1006043.e020">9</xref>) for the purpose of the simulations reported below.</p>
<p>The LVOC model instantiates the very general theory that the brain learns how to process information via metacognitive reinforcement learning. This includes not only the plasticity of cognitive control but also how people might discover cognitive strategies for reasoning and decision-making and how they learn to regulate their mental activities during problem solving. As a proof of concept, the following sections validate the LVOC model against five experiments on the plasticity of attention and interference control.</p>
</sec>
<sec id="sec005">
<title>Alternative models: Associative learning and Win-Stay Lose-Shift</title>
<p>In principle, the control-demanding behavior considered in this paper could result from simpler mechanisms than the ones proposed here. In this section, we consider two simple models that we use as alternatives to compare against the more complex LVOC model. The first model relies on the assumption that the plasticity of cognitive control can be understood in terms of associative learning [<xref ref-type="bibr" rid="pcbi.1006043.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref016">16</xref>]. We therefore evaluate our model against an associative learning model based on the Rescorla-Wagner learning rule [<xref ref-type="bibr" rid="pcbi.1006043.ref042">42</xref>]. This model forms stimulus-control associations based on the resulting reward. The association <italic>A</italic><sub><italic>s</italic>,<italic>c</italic></sub> between a stimulus <italic>s</italic> and a control signal <italic>c</italic> is strengthened when it is accompanied by (intrinsic or extrinsic) reward and weakened otherwise. Concretely, the association strengths involving the chosen response were updated according to the Rescorla-Wagner rule, that is
<disp-formula id="pcbi.1006043.e021">
<alternatives>
<graphic id="pcbi.1006043.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <italic>α</italic> is the learning rate, <italic>R</italic> is the reward and the indicator variable <italic>I</italic><sub><italic>s</italic></sub> is 1 when the stimulus <italic>s</italic> was present and 0 else. Given the learned associations, the control signal is chosen probabilistically according to the exponentiated Luce’s choice rule, that is each control signal <italic>c</italic> is selected with probability
<disp-formula id="pcbi.1006043.e022">
<alternatives>
<graphic id="pcbi.1006043.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>The second alternative model is based on previous research suggesting that people sequentially adjust their strategy through a simple Win-Stay Lose-Shift mechanism [<xref ref-type="bibr" rid="pcbi.1006043.ref043">43</xref>]. On the first trial, this mechanism chooses a strategy at random, and on each subsequent trial it either repeats the previous strategy when it was successful or switches to a different strategy when the current strategy failed. Here, we apply this idea to model how the brain learns which control signal to select. Concretely, our WSLS model repeats the previous control signal (e.g., “Attend to green.”) when it leads to a positive outcome (Win-Stay) and randomly selects a different control signal (e.g., “Attend to red.”) otherwise (Lose-Shift).</p>
<p>In contrast to the LVOC mode, the two alternative models assume that control signals are discrete rather than continuous. In the context of visual attention, they choose their control signal <italic>c</italic> from the set {1,2,3,⋯,12} of possible locations to attend, and in the context of inhibitory control they decide to either inhibit the process completely or not at all (<italic>c</italic> ∈ {0,1}).</p>
</sec>
<sec id="sec006">
<title>Simulations of learning and transfer effects in cognitive control paradigms</title>
<p>To evaluate the proposed models, we used them to simulate the plasticity of attentional control in a visual search task [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] as well as learning and transfer effects in Stroop and Flanker paradigms [<xref ref-type="bibr" rid="pcbi.1006043.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>]. <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref> summarizes the simulated phenomena and how the LVOC model explains each at a conceptual level.</p>
<table-wrap id="pcbi.1006043.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006043.t001</object-id>
<label>Table 1</label> <caption><title>The core assumption of the LVOC model explains the learning effects observed in five different cognitive control experiments.</title></caption>
<alternatives>
<graphic id="pcbi.1006043.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">Phenomenon</th>
<th align="center">Explanation of the LVOC model</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Lin et al. (2016), Exp. 1</td>
<td align="left">In the training block, participants learn to find the target increasingly faster when it always appears in a location with a certain color. In the test block, participants are significantly slower on trials that violate this regularity.</td>
<td align="left">People learn to predict the value of attending to different locations from their color.</td>
</tr>
<tr>
<td align="left">Krebs et al. (2010), Exp. 1</td>
<td align="left">People come to name the color of incongruent words faster and more accurately for colors for which performance is rewarded.</td>
<td align="left">People learn to predict the value of increasing control intensity from the color of the word.</td>
</tr>
<tr>
<td align="left">Braem et al. (2012), Exp. 1</td>
<td align="left">On a congruent Flanker trial, people are faster when the previous trial was rewarded and congruent than when it was unrewarded and congruent, but the opposite holds when the previous trial was incongruent. These effects are amplified in people with high reward sensitivity.</td>
<td align="left">People learn to exert more control on incongruent trials. Thus, rewarded incongruent trials tend to reinforce higher control signals while rewarded congruent trials tend to reinforce low control signals. Thus, people increase control after the former and lower control after the latter.</td>
</tr>
<tr>
<td align="left">Bugg et al. (2008), Exp. 2</td>
<td align="left">People become faster and more accurate at naming the color of an incongruently colored word when it is usually incongruent than when it is usually congruent.</td>
<td align="left">People learn that exerting more control is more valuable when the color or word is predictive of incongruence.</td>
</tr>
<tr>
<td align="left">Bugg et al. (2011), Exp. 2</td>
<td align="left">People are faster at naming animals in novel, incongruently labelled images when that species was mostly incongruently labelled in the training phase than when it was mostly congruently labelled.</td>
<td align="left">People learn that exerting more control is more valuable when the semantic category of the picture is predictive of incongruence.</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<sec id="sec007">
<title>Learning to control visual attention</title>
<p>Previous research has shown that how people allocate their attention is shaped by learning [<xref ref-type="bibr" rid="pcbi.1006043.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>]. For instance, Lin and colleagues [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] had participants perform a visual search task in which they gradually learned to allocate their attention to locations whose color predicted the appearance of the target (<xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1a</xref>). In this task, participants viewed an array of four rotated letters (one T and three L’s), each encompassed by a different colored circle. They were instructed to report the orientation of the T. The circles appeared before the letters allowing participants to allocate their attention by saccading to a promising location before the letters appeared. In the training phase, the target always appeared within the green circle, but in the test phase it was equally likely to appear in any of the four circles.</p>
<fig id="pcbi.1006043.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006043.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Learning to control the allocation of attention.</title>
<p>a) Visual search task used by Lin et al. (2016). b) Human data from Experiment 1 of Lin et al. (2016). c) Predictions of the LVOC model. d) Fit of Win-Stay Lose-Shift model. e) Fit of Rescorla-Wagner model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.g001" xlink:type="simple"/>
</fig>
<p>Visual search entails sequentially allocating cognitive control to different locations based on their visual features. Since attention can be understood as an instance of cognitive control, this problem is naturally modeled as a meta-level MDP. We therefore applied our LVOC theory to predict the dynamics and consequences of learning which locations to attend to based on their features (the colored circles) in this paradigm. Since the stimuli were presented along a circle, approximating locations people might naturally attend around a clock, we assumed that the control signal <italic>c</italic> ∈ {1,2,3,…,12} specifies which of 12 locations to attend, the state <italic>s</italic><sub><italic>t</italic></sub> encodes which of the 12 locations were highlighted by a colored circle (see <xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1a</xref>), the circles’ colors, the unknown position of the target, and the list of locations that have already been inspected on the current trial. Since the set of possible control signals is small, our simulation assumes that the brain always finds the control signal that maximizes the predicted EVOC (<xref ref-type="disp-formula" rid="pcbi.1006043.e020">Eq 9</xref>).</p>
<p>The features <bold><italic>f</italic></bold>(<italic>s</italic>, <italic>c</italic>) encode only observable aspects of the state <italic>s</italic> that are relevant to the value of the control signal <italic>c</italic>. Concretely, our simulations assumed that the features encode whether the attended location was highlighted by a colored circle, the color of that circle (one binary indicator variable for each possible color), its position (by one binary indicator variable for each of the four possible locations), and whether or not it has been attended before. To capture people’s prior knowledge that attending a location a second time is unlikely to provide new information, we set the prior on the weight of the last feature to −1; this captures the well-known inhibition of return mechanism in visual attention [<xref ref-type="bibr" rid="pcbi.1006043.ref044">44</xref>]. For all other features the mean of the prior on the weights was 0. Based on the results reported by [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>], we modeled reaction times as the sum of a non-decision time of 319ms and a decision-time of 98ms per attended location. Our simulation assumed that people incur a fixed cost (<italic>r</italic>(<italic>s</italic>, <italic>c</italic>) = cost(c) = −1 for all <italic>c</italic> ∈ {1,2,3,4}) every time they deploy their attention to a location. For simplicity, we assume that in this simple task people always search until they find the target and that when they attend to a location they always recognize the presence/absence of the target and respond accordingly. Hence, the intuition that people should try to find the target with as few saccades as possible follows directly from the objective of maximizing the sum of meta-level rewards. Applied to this visual search task, the LVOC model offers a mechanism for how people learn where to allocate their attention based on environmental cues in order to find the target as quickly as possible.</p>
<p>Our associative learning model assumed that finding the target yields an intrinsic reward of +1 and no reward or cost otherwise. The responses <italic>C</italic> were saccades to one of the 12 locations. The stimuli S comprised indicator variables for each of the four colors, the absence of a circle, and whether the location had been inspected before, and one feature that was always 1. To capture the inhibition of return, the reward associations with the stimulus-feature indicating that a location had been attended previously were initialized to −1. All other association strengths were initialized to 0 and the learning rate of the Rescorla-Wagner model was fit to the data from [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] using maximum-likelihood estimation.</p>
<p>In this task, the WSLS model also produces a sequence of saccades <italic>c</italic><sub>1</sub>, <italic>c</italic><sub>2</sub>, ⋯ ∈ <italic>C</italic> by repeatedly shifting its attention to a different location until the target is found.</p>
</sec>
<sec id="sec008">
<title>Learning and transfer effects in inhibitory control</title>
<p>In Stroop and Flanker paradigms, the cognitive control strategy <italic>o</italic> is defined by a single control signal <italic>c</italic> ∈ [0,1] which serves to bias processing away from an automatic mechanism. Following the classic model by Cohen and colleagues [<xref ref-type="bibr" rid="pcbi.1006043.ref045">45</xref>], we assume that control signals determine the relative contribution of the automatic versus the controlled process to the drift rate <italic>d</italic> with which evidence is accumulated towards the controlled response [<xref ref-type="bibr" rid="pcbi.1006043.ref046">46</xref>]:
<disp-formula id="pcbi.1006043.e023">
<alternatives>
<graphic id="pcbi.1006043.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e023" xlink:type="simple"/>
<mml:math display="block" id="M23">
<mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where <italic>d</italic><sub>controlled</sub> and <italic>d</italic><sub>automatic</sub> are the drift rates of the controlled and the automatic process respectively, and <italic>C</italic> = 1 when the trial is congruent or −1 when the trial is incongruent. The drift rates, in turn, affect the response and response time according to a drift-diffusion model [<xref ref-type="bibr" rid="pcbi.1006043.ref046">46</xref>]. When the decision variable exceeds the threshold +<italic>θ</italic>, then the response agrees with the controlled process (equivalent to the correct response for these tasks). When the decision variable falls below −<italic>θ</italic>, the response is incorrect. To capture sources of error outside of the evidence accumulation process (e.g., motor execution errors), our simulation assumes that people accidentally give the opposite of their intended response on a small fraction of trials (<italic>p</italic><sub>flip</sub>&lt;0.05).</p>
<p>We model the selection of continuous control signals as a gradient ascent on the EVOC predicted by Thompson sampling (Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e018">8</xref> and <xref ref-type="disp-formula" rid="pcbi.1006043.e020">9</xref>). Concretely, continuous control signals are selecting by repeatedly applying the update rule
<disp-formula id="pcbi.1006043.e024">
<alternatives>
<graphic id="pcbi.1006043.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:mi mathvariant="normal">c</mml:mi><mml:mo>←</mml:mo><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mspace width="4pt"/><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>;</mml:mo><mml:mspace width="4pt"/><mml:mover accent="true"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
until the change in the Euclidean norm of the control signal intensity vector is less than <italic>δ</italic><sub><italic>c</italic></sub>. This mechanism starts from the control signal deployed on the previous trial and thereby captures the inertia of control specification [<xref ref-type="bibr" rid="pcbi.1006043.ref047">47</xref>]. Furthermore, it predicts that control intensities are adjusted gradually and continually, thereby allowing control to be exerted while the optimal control signal is still being determined. This feature of our model makes the intuitive prediction that time pressure might reduce the magnitude of control adjustment [cf. <xref ref-type="bibr" rid="pcbi.1006043.ref048">48</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref049">49</xref>].</p>
<p>We model the cost associated with a continuous control signal <italic>c</italic> as the sum of the control cost required to exert that amount of control (cost(<italic>c</italic>)) and the opportunity cost of executing the controlled process (<italic>ω</italic>⋅<italic>t</italic>), that is
<disp-formula id="pcbi.1006043.e025">
<alternatives>
<graphic id="pcbi.1006043.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>ω</mml:mi><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
where <italic>ω</italic> is the opportunity cost per unit time, <italic>t</italic> is the duration of the controlled process, and cost(<italic>c</italic>) is the intrinsic cost of exerting the control signal <italic>c</italic>. While the first term captures that goal-directed control processes, such as planning, can take significantly longer than automatic processes, such as habits, the second term captures that due to interference between overlapping pathways the cost of a control signal increases with its intensity [<xref ref-type="bibr" rid="pcbi.1006043.ref013">13</xref>] even when control intensity accelerates the decision process [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref050">50</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref051">51</xref>]. In many real-world scenarios and some experiments, the opportunity cost is time-varying. This can be incorporated into our model by adding a learning mechanism that estimates <italic>ω</italic> from experience [<xref ref-type="bibr" rid="pcbi.1006043.ref052">52</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref053">53</xref>]. Following [<xref ref-type="bibr" rid="pcbi.1006043.ref046">46</xref>], we model the intrinsic cost of control as the implementation cost
<disp-formula id="pcbi.1006043.e026">
<alternatives>
<graphic id="pcbi.1006043.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>exp</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo>||</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>||</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
where <italic>c</italic> is the control signal, <italic>a</italic><sub><italic>i</italic></sub> specifies how rapidly control cost increases with control intensity, and <italic>b</italic><sub><italic>i</italic></sub> determines the lowest possible cost. The monotonic increase of control cost with control signal intensity expressed by this equation models the fact that the more intensely you focus on one process, say color-naming, the less you are able to do other valuable things, such as verbal reasoning. This cognitive opportunity cost of control is a consequence of overlap between neural pathways serving different functions [<xref ref-type="bibr" rid="pcbi.1006043.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref054">54</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref055">55</xref>]. We do not assume any reconfiguration costs [<xref ref-type="bibr" rid="pcbi.1006043.ref046">46</xref>] but our framework can be easily extended to include them.</p>
<p>In all of our simulations, the number of samples drawn from the posterior distribution on the weights was <italic>k</italic> = 2. For simplicity, we modeled control allocation in each trial of the Stroop and Flanker tasks simulated below as an independent, non-sequential, metacognitive control problem. The opportunity cost of time (<italic>ω</italic> in <xref ref-type="disp-formula" rid="pcbi.1006043.e025">Eq 14</xref>) was set to $8/h [cf. <xref ref-type="bibr" rid="pcbi.1006043.ref051">51</xref>]. Model parameters were fitted by maximum likelihood estimation using Bayesian optimization [<xref ref-type="bibr" rid="pcbi.1006043.ref056">56</xref>]. The drift rates of the controlled and the automatic process (<xref ref-type="disp-formula" rid="pcbi.1006043.e023">Eq 12</xref>) were determined from people’s response times on neutral trials. The model’s prior precision on the weights was set to assign 95% confidence to the EVOC of a stimulus lying between the equivalent of ±5 cents per second.</p>
<p>In the color-word Stroop task by Krebs et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>] the participant’s task was to name the font color of a series of color words which were either congruent or incongruent with the word itself (<xref ref-type="fig" rid="pcbi.1006043.g002">Fig 2a</xref>). For two of the four colors, giving the correct response yielded a monetary reward whereas responses to other two colors were never rewarded. Our simulation of this experiment assumed that people represent each stimulus by a list of binary features that encode the presence of each possible color and each possible word independently but do not encode their combinations. To capture the contribution of the experiment’s financial incentives for correct responses, we assumed that the utility <italic>u</italic>(<italic>X</italic>) in <xref ref-type="disp-formula" rid="pcbi.1006043.e004">Eq 2</xref> is the sum of the financial reward and the intrinsic utility of getting it right [<xref ref-type="bibr" rid="pcbi.1006043.ref057">57</xref>], that is
<disp-formula id="pcbi.1006043.e027">
<alternatives>
<graphic id="pcbi.1006043.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
<disp-formula id="pcbi.1006043.e028">
<alternatives>
<graphic id="pcbi.1006043.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e028" xlink:type="simple"/>
<mml:math display="block" id="M28">
<mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula>
where the monetary reward <italic>r</italic><sub>external</sub> was 10 cents on rewarded trials and zero otherwise. The non-decision time was set to 300ms. The implementation cost parameters (<italic>a</italic><sub><italic>i</italic></sub> and <italic>b</italic><sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1006043.e026">Eq 15</xref>), the probability of accidental response flips (<italic>p</italic><sub>flip</sub>), the intrinsic reward <italic>r</italic><sub>intrinsic</sub> of responding correctly (Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e027">16</xref> and <xref ref-type="disp-formula" rid="pcbi.1006043.e028">17</xref>), and the noise parameter <italic>σ</italic> of the drift diffusion model (<xref ref-type="disp-formula" rid="pcbi.1006043.e023">Eq 12</xref>) were fit to the empirical data shown in <xref ref-type="fig" rid="pcbi.1006043.g002">Fig 2b and 2c</xref>. To enable a fair comparison, we gave the associative learning model and the Win-Stay Lose-Shift model degrees of freedom similar to those of the LVOC model by adding parameters for the intrinsic reward of being correct, the probability of response error, and the noise of the drift-diffusion process. In addition, the Rescorla-Wagner model was equipped with a learning rate parameter. Each model was fitted using maximum-likelihood estimation using the Bayes adaptive direct search algorithm [<xref ref-type="bibr" rid="pcbi.1006043.ref058">58</xref>].</p>
<fig id="pcbi.1006043.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006043.g002</object-id>
<label>Fig 2</label>
<caption>
<title>LVOC model captures that in the paradigm by Krebs et al.</title>
<p>(a) People learn to exert more cognitive control on stimuli whose features predict that performance will be rewarded which manifests in faster responses (b) and fewer errors (c).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.g002" xlink:type="simple"/>
</fig>
<p>In the Flanker task by Braem et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>], participants were instructed to name the color of a central square (the <italic>target</italic>) flanked by two other squares (<italic>distractors</italic>) whose color was either the same as the color of the target (congruent trials) or different from it (incongruent trials) (<xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3a</xref>). On a random 25% of the trials, responding correctly was rewarded and on the other 75% of the trials it was not. Our simulation assumed that people predict the EVOC from two features that encode the presence of conflict and congruency respectively: The conflict feature was +1 when the flankers and the target differed in color and zero otherwise. Conversely, the value of the congruency feature was +1 when the flankers had the same color as the target and zero else. To capture that people exert more cognitive control when they detect conflict [<xref ref-type="bibr" rid="pcbi.1006043.ref059">59</xref>], the prior mean on these weights was +1 for the interaction between control signal intensity and incongruence and −1 for the interaction between control signal intensity and congruence. Providing our model with these features instantiates our assumption that in the Flanker task perception is easy but response inhibition can be challenging. In other words, our model assumes that errors in the Flanker arise from the failure to translate three correct percepts into one correct response by inhibiting the automatic responses to the other two. Furthermore, the incongruency feature can also be interpreted as a proxy for the resulting response conflict that is widely assumed to drive the within-trial adjustment of control signals in the Flanker task [<xref ref-type="bibr" rid="pcbi.1006043.ref059">59</xref>].</p>
<fig id="pcbi.1006043.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006043.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Metacognitive reinforcement learning captures the effect of reward on learning from experienced conflict observed by Braem et al.</title>
<p>(2012). a) Illustration of the Flanker task by Braem et al. (2012). b) Human data by Braem et al. (2012). c) Fit of LVOC model. d) Fit of Rescorla-Wagner model. e) Fit of Win-Stay Lose-Shift model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.g003" xlink:type="simple"/>
</fig>
<p>Our simulation assumed that people only learn on trials with feedback. The effect of control was modelled as inhibiting the interference from the flankers according to
<disp-formula id="pcbi.1006043.e029">
<alternatives>
<graphic id="pcbi.1006043.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e029" xlink:type="simple"/>
<mml:math display="block" id="M29">
<mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>C</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
where <italic>C</italic> = 1 if the distractors are congruent and <italic>C</italic> = −1 when they are incongruent. The drift rates for accumulating information from the target (<italic>d</italic><sub>target</sub>) and the distractors (<italic>d</italic><sub>flankers</sub>) were assumed to be identical. Their value was fit to the response time for color naming on rewarded neutral trials reported in [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>], and the non-decision time was 300ms. The perceived reward value of the positive feedback was determined by distributing the prize for high performance (EUR 10) over the 168 rewarded trials of the experiment (<italic>z</italic> = 7.5 US cents per correct response). Braem et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>] found that the effect of reward increased with people’s reward sensitivity. To capture individual differences in reward sensitivity, we modelled people’s subjective utility by
<disp-formula id="pcbi.1006043.e030">
<alternatives>
<graphic id="pcbi.1006043.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e030" xlink:type="simple"/>
<mml:math display="block" id="M30">
<mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula>
<disp-formula id="pcbi.1006043.e031">
<alternatives>
<graphic id="pcbi.1006043.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006043.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(20)</label>
</disp-formula>
where <italic>z</italic> ≥ 0 is the payoff and <italic>α</italic> ∈ [0,1] is the reward sensitivity. The reward sensitivity was set to 1, and the intrinsic reward of being correct (<italic>r</italic><sub><italic>i</italic>ntrinsic</sub>), the standard deviation of the noise (<italic>σ</italic>), the threshold of the drift-diffusion model (<italic>θ</italic>), the implementation cost parameters (<italic>a</italic><sub><italic>i</italic></sub>, <italic>b</italic><sub><italic>i</italic></sub>) were fit to the effects of reward on the reaction times on congruent trials (<xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3c</xref>), the average reaction time, and the effect of reward sensitivity on conflict adaptation reported by [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>]. The probability of accidentally giving the opposite of the intended response was set to zero.</p>
<p>To enable a fair comparison between LVOC model and the two simpler models, we equipped the associative learning model and the Win-Stay Lose-Shift model with the same assumptions and degrees of freedom as the LVOC model. Equivalently to the LVOC model of this task, they included a bias against exerting control was instantiated by an association of -1 between either stimulus feature and control exertion. The effect of control was modeled using the same drift-diffusion model with same set of free parameters, and like the LVOC model they also included a free parameter for the intrinsic reward of being correct and the probability of response error. Furthermore, these models included a free parameter for the cost of exerting control that is equivalent to two parameters of the LVOC model’s implementation and reconfiguration cost parameters, because their control signal was either 1 or 0. Furthermore, the Rescorla-Wagner model included an additional parameter for its learning rate, giving it the same number of parameters as the LVOC model.</p>
<p>Experiment 2 by Bugg et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref008">8</xref>] asked participants to name the color of Stroop stimuli like those used by Krebs et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>]. Critically, some of the color words were printed in color that appeared on congruent trials 80% of the time whereas other color words were printed in a color that appeared on incongruent trials 80% of the time (<xref ref-type="fig" rid="pcbi.1006043.g004">Fig 4a</xref>). Each word was written either in cursive or standard font. We modeled the stimuli by four binary features indicating the presence of each of the four possible words (1 if the feature is present and 0 otherwise), and a fifth feature indicating the font type (0 for regular and 1 for cursive). The non-decision time was set to 400ms. Since there were no external rewards for good performance, the utility of correct/incorrect responses was ± <italic>r</italic><sub>intrinsic</sub>. The implementation cost parameters (<italic>a</italic><sub><italic>i</italic></sub> and <italic>b</italic><sub><italic>i</italic></sub>), the probability of accidental response flips (<italic>p</italic><sub>flip</sub>), the intrinsic reward of being correct (<italic>r</italic><sub>intrinsic</sub>), and the standard deviation of the noise (<italic>σ</italic>) were fit to the empirical data shown in <xref ref-type="fig" rid="pcbi.1006043.g004">Fig 4b and 4c</xref>. Given these parameters, the drift rate for color naming and reading where determined to match the reaction times on unrewarded neutral trials reported in [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>].</p>
<fig id="pcbi.1006043.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006043.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The LVOC model captures the finding that people learn to adjust their control intensity based on features that predict incongruence.</title>
<p>a) Color-Word Stroop paradigm by Bugg et al. (2008). b-c) LVOC model captures that people learn to exploit features that predict incongruency to respond faster and more accurately on incongruent trial. d) Picture-Word Stroop paradigm by Bugg, Jacoby, and Chanani (2011). e-f) Just as human participants, the LVOC model responds more quickly and accurately to novel exemplars from animal categories that it previously learned to associate with more frequent incongruent trials.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.g004" xlink:type="simple"/>
</fig>
<p>Finally, Bugg et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref009">9</xref>] presented their participants with pictures of animals overlaid by animal names <xref ref-type="fig" rid="pcbi.1006043.g004">Fig 4d</xref>). The participants’ task was to name the animal shown in the picture. Critically, for some animals, the picture and the word were usually congruent whereas for other animals the picture and the word were usually incongruent. The training phase was followed by a test phase that used novel pictures of the same animal species. We modelled this picture-word Stroop by representing each stimulus by a vector of binary indicator variables. Concretely, our representation assumed one binary indicator variable for each word (i.e., BIRD, DOG, CAT, FISH) and one indicator variable for each image category (i.e., bird, dog, cat, fish). The non-decision time was set to 400ms. The implementation cost parameters (<italic>a</italic><sub><italic>i</italic></sub> and <italic>b</italic><sub><italic>i</italic></sub>), the intrinsic reward of being correct (<italic>r</italic><sub>intrinsic</sub>), the standard deviation of the noise (<italic>σ</italic>), and the probability of accidental response flips (<italic>p</italic><sub>flip</sub>) were fit to the empirical data shown in <xref ref-type="fig" rid="pcbi.1006043.g004">Fig 4e and 4f</xref>. Given these parameters, the drift rate for word reading was fit as above and the drift rate for picture naming was fit to a response time of 750ms.</p>
</sec>
</sec>
</sec>
<sec id="sec009" sec-type="results">
<title>Results</title>
<p>We found that our model correctly predicted the learning effects observed in five different cognitive control experiments by virtue of its fundamental assumption that people reinforcement-learn to predict the value of potential control signals and control signal intensities from situational features (see <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>). The following sections describe these findings in detail.</p>
<sec id="sec010">
<title>Plasticity of attentional control in visual search</title>
<p>Lin et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] had participants perform a visual search task for which the target of attention could either be predicted (training and predictable test trials) or not (unpredictable test trials) (<xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1a</xref>). For this task, given its core reinforcement learning assumption (<xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>), the LVOC model predicts that 1) people should learn to attend to the circle with the predictive color and thus become faster at finding the target over the course of training, 2) continue to use the learned attentional control strategy in the test block and hence be significantly slower when the target appears in a circle of a different color during the test block, and 3) gradually unlearn their attentional bias during the test block (<xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1c</xref>). As shown <xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1b</xref>, all three predictions were confirmed by Lin and colleagues [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>].</p>
<p>We compared the performance of LVOC to two plausible alternative models of these control adjustments: a Win-Stay Lose-Shift model and a simple associative learning model based on the Rescorla-Wagner learning rule. We found that the Win-Stay Lose-Shift model failed to capture that people’s performance improved gradually during training, and it also failed to capture the difference between people’s response times to predicted versus unpredicted target locations in the test block (see <xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1d</xref>). As <xref ref-type="fig" rid="pcbi.1006043.g001">Fig 1e</xref> shows, the fit of the associative learning model (estimated learning rate: 0.0927) captures that after learning to exploit the predictive regularity in the training block participants were significantly slower in the test block. However, this simple model predicted significantly less learning induced improvement and significantly slower reaction times than was evident from the data by [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>]. A quantitative model comparisons using the Bayesian Information Criterion [<xref ref-type="bibr" rid="pcbi.1006043.ref060">60</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref061">61</xref>] provided very strong evidence that the LVOC model explains the data by [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] better than the Rescorla-Wagner model or the Win-Stay Lose-Shift model (BIC<sub>LVOC</sub> = 1817.8, BIC<sub>RW</sub> = 9763.2, BIC<sub>WSLS</sub> = 3449.9). This reflects that our model was able to accurately predict the data from [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] without any free parameters being fitted to those data. In conclusion, findings suggest that the LVOC model correctly predicted essential learning effects observed by [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] and explains these data significantly better than a simple associative learning model and a Win-Stay Lose-Shift model.</p>
<p>To more accurately capture both the slow improvement in the training block and the rapid unlearning in the test block simultaneously, the LVOC model could be extended by including a mechanism that discounts what has been learned or increases the learning rate when a change is detected [<xref ref-type="bibr" rid="pcbi.1006043.ref062">62</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref063">63</xref>]. Next, we evaluate the LVOC model against empirical data on the plasticity of inhibitory control.</p>
</sec>
<sec id="sec011">
<title>Plasticity of Inhibitory control</title>
<p>We found that our model can capture reward-driven learning effects in Stroop and Flanker tasks, as well as how people learn to adjust their control allocation based on features that predict incongruence and the transfer of these learning effects to novel stimuli. In each case, the LVOC model captured the empirical phenomenon more accurately than either a simple Win-Stay Lose-Shift model or a simple associative learning model. The following two sections present these results in turn.</p>
</sec>
<sec id="sec012">
<title>Reward-driven plasticity in interference control</title>
<sec id="sec013">
<title>People learn to allocate more control on rewarded trials</title>
<p>To determine whether our integrated theory captures the reward-modulated plasticity of cognitive control specification, we used the LVOC model to simulate two sets of experiments that examined the influences of reward on cognitive control. Krebs et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref011">11</xref>] found that participants performing a color-word Stroop task learned to respond faster and more accurately to incongruently colored color words when their color predicted that performance would be rewarded than when the color predicted that performance would not be rewarded. We found that our model can capture these effects with reasonable parameter values (see <xref ref-type="table" rid="pcbi.1006043.t002">Table 2</xref>). <xref ref-type="fig" rid="pcbi.1006043.g002">Fig 2</xref> shows that our model captures Krebs et al.’s finding that people learn to exert more control on trials with rewarded colors than on trials with unrewarded colors even though they were interspersed within the same block. Concretely, our model captured that people become faster (691 ± 8ms vs. 541 ± 7ms; <italic>t</italic>(959998) = −14.6, <italic>p</italic> &lt; 10<sup>−15</sup>) and more accurate (11.8 ± 0.03% errors vs. 4.9 ± 0.02% errors; <italic>t</italic>(959998) = −164.7, <italic>p</italic> &lt; 10<sup>−15</sup>) when the color of the word is associated with reward. Critically, the qualitative effects observed in this experiment follow logically from the core assumption of the LVOC model (see <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>).</p>
<table-wrap id="pcbi.1006043.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006043.t002</object-id>
<label>Table 2</label> <caption><title>Model parameters used in the simulations of empirical findings.</title></caption>
<alternatives>
<graphic id="pcbi.1006043.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center"/>
<th align="center"><italic>a</italic><sub><italic>i</italic></sub></th>
<th align="center"><italic>b</italic><sub><italic>i</italic></sub></th>
<th align="center"><italic>θ</italic></th>
<th align="center"><italic>σ</italic></th>
<th align="center"><italic>r</italic><sub>intrinsic</sub></th>
<th align="center"><italic>p</italic><sub>flip</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Krebs, et al. (2010)</td>
<td align="center">1.60</td>
<td align="center">−0.01</td>
<td align="center">3</td>
<td align="center">0.05</td>
<td align="char" char=".">1.60¢</td>
<td align="char" char=".">3.5%</td>
</tr>
<tr>
<td align="center">Braem et al. (2012)</td>
<td align="center">4.17</td>
<td align="center">−2</td>
<td align="center">2.75</td>
<td align="center">5</td>
<td align="char" char=".">4.17¢</td>
<td align="char" char=".">0.8%</td>
</tr>
<tr>
<td align="center">Bugg et al. (2008)</td>
<td align="center">1.95</td>
<td align="center">−2.1</td>
<td align="center">2.65</td>
<td align="center">3.01</td>
<td align="char" char=".">3.89¢</td>
<td align="char" char=".">0.4%</td>
</tr>
<tr>
<td align="center">Bugg et al. (2011)</td>
<td align="center">5</td>
<td align="center">−2</td>
<td align="center">2.75</td>
<td align="center">3</td>
<td align="char" char=".">18.00¢</td>
<td align="char" char=".">0.8%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We compared the LVOC model’s performance to that of an associative learning model with equivalent parameters (see <xref ref-type="sec" rid="sec002">Models</xref>); the maximum likelihood estimates of these parameters were <italic>α</italic> = 0.0447 for the learning rate, <italic>r</italic><sub>intrinsic</sub> = 0.1811 for the intrinsic reward, <italic>σ</italic><sub><italic>ε</italic></sub> = 0.1525 for the noise of the drift-diffusion process, and <italic>p</italic><sub>error</sub> = 0.1799. While the Rescorla-Wagner model was able to qualitatively capture the effect of potential reward on reaction time and error rate, its quantitative fit was far worse than the fit of the LVOC model (see <xref ref-type="fig" rid="pcbi.1006043.g002">Fig 2b and 2c</xref>); thus, a quantitative model comparison controlling for the number of parameters provided very strong evidence for the LVOC model over the Rescorla-Wagner model (BIC<sub>LVOC</sub> = 45.3 vs. BIC<sub>RW</sub> = 1333.9). We also fitted the Win-Stay Lose-Shift model and its parameter estimates were <italic>r</italic><sub>intrinsic</sub> = 0 for the intrinsic reward, <italic>σ</italic><sub><italic>ε</italic></sub> = 0 for the noise of the drift-diffusion process, and <italic>p</italic><sub>error</sub> = 0.07). We found that the WSLS model was unable to capture the effect of reward on response times and error rates (see <xref ref-type="fig" rid="pcbi.1006043.g002">Fig 2b and 2c</xref>) because its control signals are uninformed by the stimulus presented on the current trial. Consequently, a formal model comparison provided strong evidence for the LVOC model over the Win-Stay Lose-Shift model (BIC<sub>LVOC</sub> = 45.3 vs. BIC<sub>WSLS</sub> = 2454.8).</p>
</sec>
<sec id="sec014">
<title>Reward accelerates trial-by-trial learning of how to allocate control</title>
<p>Braem et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>] found that participants in their Flanker task allocated more cognitive control after rewarded incongruent trials than after rewarded congruent trials or unrewarded trials. As <xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3b</xref> shows, the LVOC model can capture this reward-induced conflict-adaptation effect with a plausible set of parameters (see <xref ref-type="table" rid="pcbi.1006043.t002">Table 2</xref>). Our model correctly predicted that people’s responses on congruent trials are faster when they are preceded by rewarded congruent trials than when they are preceded by rewarded incongruent trials. The predicted difference (7 ms) was smaller than the empirically observed difference (27 ms) but it was statistically significant (<italic>t</italic>(99) = 37.99, <italic>p</italic> &lt; 10<sup>−15</sup>). According to our model, people learn to exert more control on incongruent trials than on congruent trials. Furthermore, being rewarded for exerting a low level of control reduces the control intensity on the subsequent trial, whereas being rewarded for exerting a high level of control increases the control intensity on the subsequent trial. Thus, our model predicts that control intensity should increase after rewarded incongruent trials but decrease after rewarded congruent trials. On congruent trials, more control leads to slower responses because it inhibits the facilitating signal from the flankers (<xref ref-type="disp-formula" rid="pcbi.1006043.e029">Eq 18</xref>). This suggests that our model’s metacognitive reinforcement learning mechanism correctly predicts the findings of Braem et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>] (see <xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3</xref> and <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>).</p>
<p>The LVOC model’s learning increases with the magnitude of the reward. Consequently, the LVOC model predicts that the effect shown in <xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3b</xref> should increase with people’s reward sensitivity. Concretely, as we increased the reward sensitivity parameter <italic>α</italic> from 0 to 1, the predicted reward-driven effect of conflict monotonically increased from 0.6ms to 8.0ms (<italic>t</italic>(102) = 4.77, <italic>p</italic> &lt; 0.0001). Consistent with this prediction, Braem and colleagues [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>] found that the magnitude of reward-driven conflict adaptation effect increased with people’s reward sensitivity, suggesting that the reward experienced for exerting cognitive control was the driving force of their adjustments. The significant positive correlation between people’s reward sensitivity and the magnitude of their conflict adaptation effect reported by [<xref ref-type="bibr" rid="pcbi.1006043.ref010">10</xref>] confirms our model’s prediction. Our model captures all of these effects because it learns to predict the expected rewards and costs of exerting control from features of the situation and probabilistically chooses the control signal that achieves the best cost-benefit tradeoff.</p>
<p>We compared the LVOC’s fit to these behaviors with the associative learning and Win-Stay Lose-Shift models. Even though the associative learning model had the same number of parameters as the LVOC model, its fit was substantially worse than the fit of the LVOC model (mean squared errors: MSE<sub>RW</sub> = 3.33 vs. MSE<sub>LVOC</sub> = 1.35), and its best fit (<xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3d</xref>) failed to capture the qualitative effect shown in <xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3c</xref>. Finally, we evaluated a Win-Stay Lose-Shift model. This model was equipped with the same set of parameters as our Rescorla-Wagner model except for the learning rate parameter. We found that the Win-Stay Lose-Shift model was unable to capture the data by Braem et al. <xref ref-type="fig" rid="pcbi.1006043.g003">Fig 3e</xref>) because it stays with the controlled process forever once it has been rewarded for using it (MSE<sub>WSLS</sub> = 19.4). Taken together with the previous results, this suggests that the simple mechanisms assumed by the associative learning model and the Win-Stay Lose-Shift model are insufficient to explain the complexity of cognitive control plasticity, but the LVOC model can capture it.</p>
</sec>
</sec>
<sec id="sec015">
<title>Transfer of learning effects in interference control</title>
<p>The expected value of computation depends not only on the rewards for correct performance but also on the difficulty of the task. In easy situations, such as the congruent trials of the Stroop task, the automatic response can be as accurate, faster, and less costly than the controlled response. In cases like this, the expected value of exerting control is less than the EVOC of exerting no control. By contrast, in more challenging situations, such as incongruent Stroop trials, the controlled process is more accurate and therefore has a positive EVOC as long as accurate performance is sufficiently important. Therefore, on incongruent trials the expected value of control is larger than the EVOC of exerting no control. Our model thus learns to exert control on incongruent trials but not on congruent trials. Our model achieves this by learning to predict the EVOC from features of the stimuli. This predicts that people should learn to exert more control when they encounter a stimulus feature (such as a color or word) that is predictive of incongruence than when they encounter a feature that is predictive of congruence (see <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>).</p>
<p>Consistent with our model’s predictions, Bugg and colleagues [<xref ref-type="bibr" rid="pcbi.1006043.ref008">8</xref>] found that people learn to exert more control in response to stimulus features that predict incongruence than stimulus features that predict congruence. Their participants performed a color-word Stroop task with four colors and their names printed either in cursive or regular font. Our model captured the effects of congruency-predictive features on control allocation with a plausible set of parameters (see <xref ref-type="table" rid="pcbi.1006043.t002">Table 2</xref>). As shown in <xref ref-type="fig" rid="pcbi.1006043.g004">Fig 4a and 4b</xref>, the LVOC model predicted that responses should be faster (655 ± 9 ms vs. 722 ± 11 ms; <italic>t</italic>(49) = 5.39, <italic>p</italic> &lt; 0.0001) and more accurate (2.85 ± 0.2% errors vs. 4.3 ± 0.3% errors; <italic>t</italic>(49) = 5.01, <italic>p</italic> &lt; 0.0001) on incongruent trials if the word was predictive of incongruence than when it was not. To their surprise, Bugg and colleagues observed that adding an additional feature (font) that conveyed the same information about congruence as the color, did not enhance learning. This is exactly what our model predicted because the presence of a second predictive feature reduces the evidence for the predictive power of the first one and vice versa–this is directly analogous to a phenomenon from the Pavlovian literature known as blocking, whereby an animal fails to learn an association between a stimulus and an outcome that is already perfectly predicted by a second stimulus [<xref ref-type="bibr" rid="pcbi.1006043.ref064">64</xref>].</p>
<p>Since our model learns about the predictive relationship between features and the EVOC, it predicts that all learning effects should transfer to novel stimuli that share the features that were predictive of the expected value of control in the training trials (see <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>). A separate study by Bugg and colleagues [<xref ref-type="bibr" rid="pcbi.1006043.ref009">9</xref>] confirmed this prediction. They trained participants in a picture-word Stroop task to associate particular images of certain categories (e.g., cats and dogs) with incongruence and associated particular images of other categories (e.g., fish and birds) with congruence. As expected, participants learned to exert more control when viewing the stimuli associated with incongruence. More importantly, these participants also exerted more control when tested on <italic>novel</italic> instances of the category associated with incongruence (e.g., cats) than on novel instances of the category associated with congruence (e.g., fish). This finding provides strong evidence for the feature-based learning mechanism that is at the core of our model of the plasticity of cognitive control and is entirely accounted for by our model. As shown in <xref ref-type="fig" rid="pcbi.1006043.g004">Fig 4e and 4f</xref>, our model correctly predicted the positive and the negative transfer effects reported by [<xref ref-type="bibr" rid="pcbi.1006043.ref009">9</xref>] with reasonable parameters (see <xref ref-type="table" rid="pcbi.1006043.t002">Table 2</xref>): The model’s responses were faster (709 ± 3 ms vs. 685 ± 2 ms; <italic>t</italic>(99) = −8.13, <italic>p</italic> &lt; 0.0001) and more accurate (4.8 ± 0.3% errors vs. 3.2 ± 0.1% errors; <italic>t</italic>(99) = −5.06, <italic>p</italic> &lt; 0.0001) on incongruent trials if the word was predictive of incongruence than when it was not (positive transfer). Conversely, on congruent trials, the predicted responses were slightly slower when the features wrongly predicted incongruence (527 ± 0.2ms vs. 530 ± 0.1ms, <italic>t</italic>(99) = 9.28, <italic>p</italic> &lt; 0.0001; negative transfer).</p>
</sec>
</sec>
<sec id="sec016" sec-type="conclusions">
<title>Discussion</title>
<p>Building on previous work modeling the specification of cognitive control in terms of meta-decision making [<xref ref-type="bibr" rid="pcbi.1006043.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref014">14</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref033">33</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref065">65</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref066">66</xref>] and reinforcement learning [<xref ref-type="bibr" rid="pcbi.1006043.ref033">33</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref067">67</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref069">69</xref>], we have illustrated that at least some of the functions of cognitive control can be characterized using the formal framework of rational metareasoning [<xref ref-type="bibr" rid="pcbi.1006043.ref026">26</xref>] and meta-level Markov decision processes [<xref ref-type="bibr" rid="pcbi.1006043.ref027">27</xref>]. Concretely, modeling the function of cognitive control as a meta-level MDP allowed us to derive the first formal computational model of how people learn to specify continuous control signals and how these learning effects transfer to novel situations. This model provides a unifying explanation for how people learn where to attend, the interacting effects of reward and incongruence on interference control, and their transfer to novel stimuli.</p>
<p>Our simulations of learning in Stroop and Flanker paradigms illustrate that the LVOC model can account for people’s ability to learn when and how intensely to engage controlled processing and inhibit automatic processing. We further found that the LVOC model correctly predicted the learning curve in the visual attention experiment by Lin et al. [<xref ref-type="bibr" rid="pcbi.1006043.ref017">17</xref>] without any free parameters. Critically, all of our model’s qualitative predictions follow directly from our theory’s core assumption that people reinforcement-learn to predict the value of alternative control signals and control signal intensities from stimulus features (see <xref ref-type="table" rid="pcbi.1006043.t001">Table 1</xref>). None of our model’s auxiliary assumptions about the cost of control, the reward for being correct, the drift-diffusion model, the details of learning and control signal selection, and the corresponding parameters summarized in <xref ref-type="table" rid="pcbi.1006043.t002">Table 2</xref> are necessary to derive these qualitative predictions; instead they only serve to increase the quantitative accuracy of those predictions.</p>
<p>While the LVOC model is more complex than basic associative learning and the Win-Stay Lose-Shift mechanism, neither of these simpler models was able to capture human learning in the simulated visual search, Stroop, and Flanker paradigms. This suggests that the complexity of the LVOC model may be currently warranted to capture how people learn when to exert how much cognitive control. Furthermore, the LVOC model’s sophistication may be necessary to explain more complex phenomena such as how people learn to orchestrate their thoughts to solve complex problems and acquire sophisticated cognitive strategies. Recent work has indeed shown that the learning mechanism instantiated by the LVOC model can also capture aspects of how people learn how to plan [<xref ref-type="bibr" rid="pcbi.1006043.ref070">70</xref>] and to flexibly and adaptively choose between alternative cognitive strategies [<xref ref-type="bibr" rid="pcbi.1006043.ref053">53</xref>]. Testing whether people learn to select sequences of control signals in the way predicted by our model is an interesting direction for future research.</p>
<sec id="sec017">
<title>The LVOC model integrates control specification and strategy selection learning</title>
<p>The model developed in this article builds on two previous theories: the EVC theory, which offered a normative account of control specification [<xref ref-type="bibr" rid="pcbi.1006043.ref007">7</xref>], and the rational metareasoning theory of strategy selection [<xref ref-type="bibr" rid="pcbi.1006043.ref053">53</xref>], which suggested that people acquire the capacity to select heuristics adaptively by learning a predictive model of the execution time and accuracy of those heuristics. The LVOC model synergistically integrates these two theories: it augments the EVC theory with the metacognitive learning and prediction mechanisms identified by [<xref ref-type="bibr" rid="pcbi.1006043.ref053">53</xref>], and it augments rational metareasoning models of strategy selection with the capacity to specify continuous control signals that gradually adjust parameters of the controlled process (see <xref ref-type="supplementary-material" rid="pcbi.1006043.s002">S2 Text</xref>).</p>
</sec>
<sec id="sec018">
<title>Empirical predictions</title>
<p>All else being equal, the proposed learning rules (see <xref ref-type="disp-formula" rid="pcbi.1006043.e017">Eq 7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006043.s001">S1 Text</xref> Equations 1–7, and <xref ref-type="supplementary-material" rid="pcbi.1006043.s003">S3 Text</xref> Equations 13–14) predict that people’s propensity to exert cognitive control should increase when the controlled process was less costly (e.g., faster) or generated more reward than expected [<xref ref-type="bibr" rid="pcbi.1006043.ref019">19</xref>]. The experience that less controlled (more automatic) processing was more costly or less rewarding than expected should also increase our propensity to exert cognitive control [<xref ref-type="bibr" rid="pcbi.1006043.ref071">71</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref073">73</xref>]. Conversely, if a controlled process performed worse than expected or if an automatic process performed better than expected, people’s propensity to exert cognitive control should decrease [<xref ref-type="bibr" rid="pcbi.1006043.ref074">74</xref>].</p>
<p>At a more detailed level, our theory predicts that the influence of environmental features on control allocation generalizes across contexts, to the extent that their features are similar. Thus, adding or removing features to the internal predictive model of the EVOC should have a profound effect on the degree to which observed performance of the controlled process in Context A changes people’s propensity to select it in Context B, and vice versa. This mechanism can account for empirical evidence that suggests a role for feature-binding in mechanisms of task switching [<xref ref-type="bibr" rid="pcbi.1006043.ref075">75</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref078">78</xref>]. These studies suggest that participants associate the task that they perform on a stimulus with the features of that stimulus. Once they are asked to engage in a new task on that stimulus, the old (associated) task interferes, leading to switch costs.</p>
<p>Furthermore, our theory predicts that increasing the rewards and punishments for the outcomes of the controlled or automatic processes should increase the speed with which people’s control allocation adapts to new task requirements, because the resulting weight updates will be larger; this becomes especially apparent when the updates are rewritten in terms of prediction errors (see <xref ref-type="supplementary-material" rid="pcbi.1006043.s003">S3 Text</xref>, Eqs <xref ref-type="disp-formula" rid="pcbi.1006043.e001">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1006043.e004">2</xref>). Finally, when the assumptions of the internal model are met and its features distinguish between the situations in which each controlled process performs best, then control signal selection should become increasingly more adaptive over time [<xref ref-type="bibr" rid="pcbi.1006043.ref079">79</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref080">80</xref>]. But in situations where the internal model’s assumptions are violated, for instance because the value of control is not additive and linear in the features, then the control system’s plasticity mechanisms may become maladaptive.</p>
<p>This prediction has been confirmed in a recent experiment with a novel color-word Stroop paradigm comprising two association phases and a test phase [<xref ref-type="bibr" rid="pcbi.1006043.ref081">81</xref>]. In the first association phase, participants learned that color naming was rewarded for certain colors whereas word reading was rewarded for the other colors. In the second association phase, participants learned that color-naming was rewarded for certain words whereas word-reading was rewarded for other words. Critically, in the test phase, naming the color was rewarded if either the word or the color had been associated with color naming (SINGLE trials); but when both the color and the word were associated with color naming then participants had to instead read the word (BOTH trials). This non-linear relationship between stimulus-features and control demands caused mal-transfer from SINGLE trials to BOTH trials that significantly interfered with participants’ performance (resulting in participants incorrectly engaging in color-naming, the more control-demanding task which in that context was <italic>also</italic> less rewarding). The LVOC model may thus be able to explain the puzzling phenomenon that people sometimes overexert cognitive control even when it hurts their performance. For instance, if your past experience has taught you to choose your words very carefully on a certain topic then receiving an email on that topic might compel you to mentally compose a perfect response even when you would be better off thinking about how to open the talk you have to deliver in 5 minutes.</p>
<p>According to the LVOC model, control allocation is a process of continuing gradual adjustment (<xref ref-type="disp-formula" rid="pcbi.1006043.e024">Eq 13</xref>). This means that the control intensity for a new situation starts out with the control intensity from the previous situation and is then gradually adjusted towards its optimal value—just like in anchoring-and-adjustment [<xref ref-type="bibr" rid="pcbi.1006043.ref048">48</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref082">82</xref>]. This might provide a mechanism for commonly observed phenomena associated with task set inertia and switch costs [<xref ref-type="bibr" rid="pcbi.1006043.ref047">47</xref>]. Since control adjustment takes time, this mechanism predicts that increased time pressure could potentially lead to decreased control adjustment, thereby biasing people’s control allocation to its value on the previous trial and thus decreasing their cognitive flexibility. Finally, thinking about the neural implementation of the LVOC model leads to additional neural predictions as detailed in the <xref ref-type="supplementary-material" rid="pcbi.1006043.s003">S3 Text</xref>.</p>
</sec>
<sec id="sec019">
<title>Avenues for future research</title>
<p>We view rational metareasoning as a general theoretical framework for modeling the allocation and plasticity of cognitive control. As such, it could be used to develop unifying models of different manifestations of cognitive control, such as attention, response inhibition, and cognitive flexibility. Furthermore, rational metareasoning can also be used to connect existing models of cognitive control [<xref ref-type="bibr" rid="pcbi.1006043.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref007">7</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1006043.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref065">65</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref066">66</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref079">79</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref080">80</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref083">83</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref084">84</xref>]. Interpreting previously proposed mechanisms of control allocation as approximations to rational metareasoning and considering how else rational metareasoning could be approximated might facilitate the systematic evaluation of alternative representations and computational mechanisms and inspire new models. While our computational explorations have focused on which control signal the cognitive control system should select, future work might also shed light on how the cognitive control system monitors the state of the controlled system by viewing the problem solved by the cognitive control system as a partially observable MDP. Concretely, the function of cognitive monitoring could be formulated as a meta-level MDP whose computational actions include sensing operations that update the cognitive control system’s beliefs about the state of the monitored system.</p>
<p>Future work should further evaluate the proposed computational mechanism and its neural implementation by performing quantitative model comparisons against simpler models across a wider range of cognitive control phenomena. This line of work should also evaluate the performance of the proposed metacognitive learning mechanism and evaluate it against alternative mechanisms (e.g., temporal difference learning mechanisms with eligibility traces [<xref ref-type="bibr" rid="pcbi.1006043.ref030">30</xref>]).</p>
<p>Another interesting direction will be to use the learning models to investigate the plasticity of people’s cognitive control skills. We are optimistic that this line of work will lead to better quantitative models of control plasticity that can be used to develop interventions to improve people’s executive functions via a combination of cognitive training and augmenting environments where people’s automatic responses are maladaptive with cues that prime them to employ an appropriate control signal. In addition, future work may also explore model-based metacognitive reinforcement learning [<xref ref-type="bibr" rid="pcbi.1006043.ref085">85</xref>] as a model of the plasticity of cognitive control specification. Model-based hierarchical reinforcement learning approaches [<xref ref-type="bibr" rid="pcbi.1006043.ref037">37</xref>], such as option models [<xref ref-type="bibr" rid="pcbi.1006043.ref038">38</xref>], could be used to integrate the learning mechanisms for the value of individual control signals with the strategy selection model to provide an account of how the brain discovers control strategies. This might explain how people learn to adaptively coordinate their thoughts and actions to pursue increasingly more challenging goals over increasingly longer periods of time.</p>
<p>Finally, the rational metareasoning framework can also be used to model how people reason about the costs and benefits of exerting mental effort and to delineate self-control failure from rational resource-preservation through a normative account of effort avoidance [<xref ref-type="bibr" rid="pcbi.1006043.ref013">13</xref>,<xref ref-type="bibr" rid="pcbi.1006043.ref086">86</xref>].</p>
</sec>
<sec id="sec020">
<title>Conclusion</title>
<p>Our simulation results suggested that the LVOC model provides a promising step towards a mathematical theory of cognitive plasticity that can serve as a scientific foundation for designing cognitive training programs to improve people’s executive functions. This illustrates the utility of formalizing the function of cognitive control in terms of rational metareasoning. Rational metareasoning provides a unifying framework for modeling executive functions, and thus opens up exciting avenues for future research. We are optimistic that the connection between executive functions and metareasoning will channel a flow of useful models and productive ideas from artificial intelligence and machine learning into the neuroscience and psychology of cognitive control.</p>
</sec>
</sec>
<sec id="sec021">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006043.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Mathematical details of the LVC model’s learning mechanism.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006043.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.s002" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Rational metareasoning unifies the EVC theory with the rational metareasoning theory of strategy selection.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006043.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006043.s003" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Speculations about how the learning mechanism postulated by the LVC model might be implemented in the brain.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors thank Matthew Botvinick, Colin Hoy, and S.J. Katarina Slama for comments on an earlier version of the manuscript and Jonathan D. Cohen for useful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006043.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diamond</surname> <given-names>A</given-names></name>. <article-title>Executive Functions</article-title>. <source>Annu Rev Psychol</source>. <year>2013</year>;<volume>64</volume>: <fpage>135</fpage>–<lpage>168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-psych-113011-143750" xlink:type="simple">10.1146/annurev-psych-113011-143750</ext-link></comment> <object-id pub-id-type="pmid">23020641</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stroop</surname> <given-names>JR</given-names></name>. <article-title>Studies of interference in serial verbal reactions</article-title>. <source>J Exp Psychol. Psychological Review Company</source>; <year>1935</year>;<volume>18</volume>: <fpage>643</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tangney</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Baumeister</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Boone</surname> <given-names>AL</given-names></name>. <article-title>High self-control predicts good adjustment, less pathology, better grades, and interpersonal success</article-title>. <source>J Pers. Wiley Online Library</source>; <year>2004</year>;<volume>72</volume>: <fpage>271</fpage>–<lpage>324</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moffitt</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Arseneault</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Belsky</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dickson</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hancox</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Harrington</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>A gradient of childhood self-control predicts health, wealth, and public safety</article-title>. <source>Proc Natl Acad Sci. National Acad Sciences</source>; <year>2011</year>;<volume>108</volume>: <fpage>2693</fpage>–<lpage>2698</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>The intrinsic cost of cognitive control</article-title>. <source>Behav Brain Sci</source>. <year>2013</year>;<volume>36</volume>: <fpage>697</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X1300109X" xlink:type="simple">10.1017/S0140525X1300109X</ext-link></comment> <object-id pub-id-type="pmid">24304795</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>A labor/leisure tradeoff in cognitive control</article-title>. <source>J Exp Psychol Gen</source>. <year>2014</year>;<volume>143</volume>: <fpage>131</fpage>–<lpage>141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0031048" xlink:type="simple">10.1037/a0031048</ext-link></comment> <object-id pub-id-type="pmid">23230991</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shenhav</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>J</given-names></name>. <article-title>The Expected Value of Control: An Integrative Theory of Anterior Cingulate Cortex Function</article-title>. <source>Neuron. Cell Press</source>; <year>2013</year>;<volume>79</volume>: <fpage>217</fpage>–<lpage>240</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.07.007" xlink:type="simple">10.1016/j.neuron.2013.07.007</ext-link></comment> <object-id pub-id-type="pmid">23889930</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bugg</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Jacoby</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Toth</surname> <given-names>JP</given-names></name>. <article-title>Multiple levels of control in the Stroop task</article-title>. <source>Mem &amp; Cogn. Springer</source>; <year>2008</year>;<volume>36</volume>: <fpage>1484</fpage>–<lpage>1494</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bugg</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Jacoby</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Chanani</surname> <given-names>S</given-names></name>. <article-title>Why it is too early to lose control in accounts of item-specific proportion congruency effects</article-title>. <source>J Exp Psychol Hum Percept Perform. American Psychological Association</source>; <year>2011</year>;<volume>37</volume>: <fpage>844</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Braem</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Roggeman</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Notebaert</surname> <given-names>W</given-names></name>. <article-title>Reward modulates adaptations to conflict</article-title>. <source>Cognition. Elsevier</source>; <year>2012</year>;<volume>125</volume>: <fpage>324</fpage>–<lpage>332</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krebs</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Boehler</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Woldorff</surname> <given-names>MG</given-names></name>. <article-title>The influence of reward associations on conflict processing in the Stroop task</article-title>. <source>Cognition. Elsevier</source>; <year>2010</year>;<volume>117</volume>: <fpage>341</fpage>–<lpage>347</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Shenhav</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Cognitive Control as Cost-Benefit Decision Making</article-title>. <source>Wiley Handb Cogn Control. John Wiley &amp; Sons, Ltd</source>; <year>2017</year>; <fpage>167</fpage>–<lpage>189</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shenhav</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Musslick</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lieder</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>, <etal>et al</etal>. <article-title>Toward a rational and mechanistic account of mental effort</article-title>. <source>Annu Rev Neurosci</source>. <year>2017</year>;</mixed-citation></ref>
<ref id="pcbi.1006043.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boureau</surname> <given-names>Y-L</given-names></name>, <name name-style="western"><surname>Sokol-Hessner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Deciding How To Decide: Self-Control and Meta-Decision Making</article-title>. <source>Trends Cogn Sci</source>. <year>2015</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2015.08.013" xlink:type="simple">10.1016/j.tics.2015.08.013</ext-link></comment> <object-id pub-id-type="pmid">26483151</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abrahamse</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Braem</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Notebaert</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>. <article-title>Grounding cognitive control in associative learning</article-title>. <source>Psychol Bull. American Psychological Association</source>; <year>2016</year>;<volume>142</volume>: <fpage>693</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Egner</surname> <given-names>T</given-names></name>. <article-title>Creatures of habit (and control): a multi-level learning perspective on the modulation of congruency effects</article-title>. <source>Front Psychol. Frontiers Media SA</source>; <year>2014</year>;<volume>5</volume>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lin</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Z-L</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>S</given-names></name>. <article-title>Decomposing experience-driven attention: Opposite attentional effects of previously predictive cues</article-title>. <source>Attention, Perception, &amp; Psychophys. Springer</source>; <year>2016</year>;<volume>78</volume>: <fpage>2185</fpage>–<lpage>2198</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muhle-Karbe</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Egner</surname> <given-names>T</given-names></name>. <article-title>Causal Evidence for Learning-Dependent Frontal Lobe Contributions to Cognitive Control</article-title>. <source>J Neurosci. Soc Neuroscience</source>; <year>2018</year>;<volume>38</volume>: <fpage>962</fpage>–<lpage>973</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anguera</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Boccanfuso</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rintoul</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Al-Hashimi</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Faraji</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Janowich</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Video game training enhances cognitive control in older adults</article-title>. <source>Nature. Nature Publishing Group</source>; <year>2013</year>;<volume>501</volume>: <fpage>97</fpage>–<lpage>101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12486" xlink:type="simple">10.1038/nature12486</ext-link></comment> <object-id pub-id-type="pmid">24005416</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karbach</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kray</surname> <given-names>J</given-names></name>. <article-title>How useful is executive control training? Age differences in near and far transfer of task-switching training</article-title>. <source>Dev Sci. Wiley Online Library</source>; <year>2009</year>;<volume>12</volume>: <fpage>978</fpage>–<lpage>990</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref021"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Heller SB, Shah AK, Guryan J, Ludwig J, Mullainathan S, Pollack HA. Thinking, fast and slow? Some field experiments to reduce crime and dropout in Chicago. 2015.</mixed-citation></ref>
<ref id="pcbi.1006043.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Melby-Lervåg</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hulme</surname> <given-names>C</given-names></name>. <article-title>Is working memory training effective? A meta-analytic review</article-title>. <source>Dev Psychol. US: American Psychological Association</source>; <year>2013</year>;<volume>49</volume>: <fpage>270</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shipstead</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Redick</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Engle</surname> <given-names>RW</given-names></name>. <article-title>Is working memory training effective?</article-title> <source>Psychol Bull. American Psychological Association</source>; <year>2012</year>;<volume>138</volume>: <fpage>628</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Functional integration and inference in the brain</article-title>. <source>Prog Neurobiol</source>. <year>2002</year>;<volume>68</volume>: <fpage>113</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0301-0082(02)00076-X" xlink:type="simple">10.1016/S0301-0082(02)00076-X</ext-link></comment> <object-id pub-id-type="pmid">12450490</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Owen</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Hampshire</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Grahn</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Stenton</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dajani</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Burns</surname> <given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>Putting brain training to the test</article-title>. <source>Nature. Nature Research</source>; <year>2010</year>;<volume>465</volume>: <fpage>775</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wefald</surname> <given-names>E</given-names></name>. <article-title>Principles of metareasoning</article-title>. <source>Artif Intell</source>. <year>1991</year>;<volume>49</volume>: <fpage>361</fpage>–<lpage>395</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0004-3702(91)90015-c" xlink:type="simple">10.1016/0004-3702(91)90015-c</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006043.ref027"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Hay N, Russell S, Tolpin D, Shimony S. Selecting Computations: Theory and Applications. In: de Freitas N, Murphy K, editors. Uncertainty in Artificial Intelligence: Proceedings of the Twenty-Eighth Conference. P.O. Box 866 Corvallis, Oregon 97339 USA OR—Uncertainty in Artificial Intelligence: AUAI Press; 2012.</mixed-citation></ref>
<ref id="pcbi.1006043.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>An integrative theory of prefrontal cortex function</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>: <fpage>167</fpage>–<lpage>202</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.24.1.167" xlink:type="simple">10.1146/annurev.neuro.24.1.167</ext-link></comment> <object-id pub-id-type="pmid">11283309</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Horvitz</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</article-title>. <source>Science (80-). American Association for the Advancement of Science</source>; <year>2015</year>;<volume>349</volume>: <fpage>273</fpage>–<lpage>278</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref030"><label>30</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Reinforcement learning: An introduction</source>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT press</publisher-name>; <year>1998</year>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaelbling</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Littman</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Cassandra</surname> <given-names>AR</given-names></name>. <article-title>Planning and acting in partially observable stochastic domains</article-title>. <source>Artif Intell</source>. <year>1998</year>;<volume>101</volume>: <fpage>99</fpage>–<lpage>134</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0004-3702(98)00023-X" xlink:type="simple">https://doi.org/10.1016/S0004-3702(98)00023-X</ext-link></mixed-citation></ref>
<ref id="pcbi.1006043.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gottlieb</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Balan</surname> <given-names>P</given-names></name>. <article-title>Attention as a decision in information space</article-title>. <source>Trends Cogn Sci</source>. <year>2010</year>;<volume>14</volume>: <fpage>240</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2010.03.001" xlink:type="simple">10.1016/j.tics.2010.03.001</ext-link></comment> <object-id pub-id-type="pmid">20399701</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>How to set the switches on this thing</article-title>. <source>Curr Opin Neurobiol</source>. <year>2012</year>;<volume>22</volume>: <fpage>1068</fpage>–<lpage>1074</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.05.011" xlink:type="simple">10.1016/j.conb.2012.05.011</ext-link></comment> <object-id pub-id-type="pmid">22704797</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref034"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Todd MT, Niv Y, Cohen JD. Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement. In: Koller D, Schuurmans D, Bengio Y, Bottou L, editors. Advances in Neural Information Processing Systems 21. Curran Associates, Inc.; 2009. pp. 1689–1696.</mixed-citation></ref>
<ref id="pcbi.1006043.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holroyd</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>Yeung</surname> <given-names>N</given-names></name>. <article-title>Motivation of extended behaviors by anterior cingulate cortex</article-title>. <source>Trends Cogn Sci. Elsevier</source>; <year>2012</year>;<volume>16</volume>: <fpage>122</fpage>–<lpage>128</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holroyd</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>McClure</surname> <given-names>SM</given-names></name>. <article-title>Hierarchical control over effortful behavior by rodent medial frontal cortex: A computational model</article-title>. <source>Psychol Rev. American Psychological Association</source>; <year>2015</year>;<volume>122</volume>: <fpage>54</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Weinstein</surname> <given-names>A</given-names></name>. <article-title>Model-based hierarchical reinforcement learning and human action control</article-title>. <source>Philos Trans R Soc B Biol Sci. The Royal Society</source>; <year>2014</year>;<volume>369</volume>: <fpage>20130480</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Precup</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>. <article-title>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</article-title>. <source>Artif Intell. Essex, UK: Elsevier Science Publishers Ltd.</source>; <year>1999</year>;<volume>112</volume>: <fpage>181</fpage>–<lpage>211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/s0004-3702(99)00052-1" xlink:type="simple">10.1016/s0004-3702(99)00052-1</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006043.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watkins</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Q-learning</article-title>. <source>Mach Learn. Kluwer Academic Publishers</source>; <year>1992</year>;<volume>8</volume>: <fpage>279</fpage>–<lpage>292</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/bf00992698" xlink:type="simple">10.1007/bf00992698</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006043.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindley</surname> <given-names>DV</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>AFM</given-names></name>. <article-title>Bayes Estimates for the Linear Model</article-title>. <source>J R Stat Soc Ser B. Blackwell Publishing for the Royal Statistical Society</source>; <year>1972</year>;<volume>34</volume>: <fpage>1</fpage>–<lpage>41</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref041"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Kunz S. The Bayesian Linear model with Unknown Variance. 2009.</mixed-citation></ref>
<ref id="pcbi.1006043.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>AR</given-names></name>, others. <article-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</article-title>. <source>Class Cond II Curr Res theory. New-York</source>; <year>1972</year>;<volume>2</volume>: <fpage>64</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Restle</surname> <given-names>F</given-names></name>. <article-title>The selection of strategies in cue learning</article-title>. <source>Psychol Rev. American Psychological Association</source>; <year>1962</year>;<volume>69</volume>: <fpage>329</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klein</surname> <given-names>RM</given-names></name>. <article-title>Inhibition of return</article-title>. <source>Trends Cogn Sci. Elsevier</source>; <year>2000</year>;<volume>4</volume>: <fpage>138</fpage>–<lpage>147</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Dunbar</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>. <article-title>On the control of automatic processes: a parallel distributed processing account of the Stroop effect</article-title>. <source>Psychol Rev. American Psychological Association</source>; <year>1990</year>;<volume>97</volume>: <fpage>332</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref046"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Musslick S, Shenhav A, Botvinick MM, Cohen JD. A computational model of control allocation based on the Expected Value of Control. The 2nd Multidisciplinary Conference on Reinforcement Learning and Decision Making. 2015.</mixed-citation></ref>
<ref id="pcbi.1006043.ref047"><label>47</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Allport</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Styles</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Hsieh</surname> <given-names>S</given-names></name>. <chapter-title>Shifting intentional set: Exploring the dynamic control of tasks</chapter-title>. In: <name name-style="western"><surname>Umilta</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Moscovitch</surname> <given-names>M</given-names></name>, editors. <source>Attention and performance XV</source>. <year>1994</year>. pp. <fpage>421</fpage>–<lpage>452</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lieder</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>ND</given-names></name>. <article-title>Empirical Evidence for Resource-Rational Anchoring and Adjustment</article-title>. <source>Psychon Bull &amp; Rev. Springer</source>; <year>2017</year>;</mixed-citation></ref>
<ref id="pcbi.1006043.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lieder</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>ND</given-names></name>. <article-title>The anchoring bias reflects rational use of cognitive resources</article-title>. <source>Psychon Bull &amp; Rev. Springer</source>; <year>2017</year>;</mixed-citation></ref>
<ref id="pcbi.1006043.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Padmala</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pessoa</surname> <given-names>L</given-names></name>. <article-title>Reward Reduces Conflict by Enhancing Attentional Control and Biasing Visual Cortical Processing</article-title>. <source>J Cogn Neurosci. MIT Press</source>; <year>2011</year>;<volume>23</volume>: <fpage>3419</fpage>–<lpage>3432</lpage>. <object-id pub-id-type="pmid">21452938</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manohar</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Chong</surname> <given-names>TT-J</given-names></name>, <name name-style="western"><surname>Apps</surname> <given-names>MAJ</given-names></name>, <name name-style="western"><surname>Batla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stamelou</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jarman</surname> <given-names>PR</given-names></name>, <etal>et al</etal>. <article-title>Reward pays the cost of noise reduction in motor and cognitive control</article-title>. <source>Curr Biol. Elsevier</source>; <year>2015</year>;<volume>25</volume>: <fpage>1707</fpage>–<lpage>1716</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Tonic dopamine: opportunity costs and the control of response vigor</article-title>. <source>Psychopharmacology (Berl). Interdisciplinary Center for Neural Computation, The Hebrew University of Jerusalem, Jerusalem, 91904, Israel.: Springer-Verlag</source>; <year>2007</year>;<volume>191</volume>: <fpage>507</fpage>–<lpage>520</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00213-006-0502-4" xlink:type="simple">10.1007/s00213-006-0502-4</ext-link></comment> <object-id pub-id-type="pmid">17031711</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lieder</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>. <article-title>Strategy selection as rational metareasoning</article-title>. <source>Psychol Rev. American Psychological Association</source>; <year>2017</year>;<volume>124</volume>: <fpage>762</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feng</surname> <given-names>SF</given-names></name>, <name name-style="western"><surname>Schwemmer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>Multitasking versus multiplexing: Toward a normative account of limitations in the simultaneous execution of control-demanding behaviors</article-title>. <source>Cogn Affect &amp; Behav Neurosci. Springer</source>; <year>2014</year>;<volume>14</volume>: <fpage>129</fpage>–<lpage>146</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref055"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">Musslick S, Dey B, Ozcimder K, Patwary MMA, Willke TL, Cohen JD. Controlled vs. Automatic Processing: A Graph-Theoretic Approach to the Analysis of Serial vs. Parallel Processing in Neural Network Architectures. Proceedings of the 38th Annual Conference of the Cognitive Science Society. 2016. pp. 1547–1552.</mixed-citation></ref>
<ref id="pcbi.1006043.ref056"><label>56</label><mixed-citation publication-type="other" xlink:type="simple">Kawaguchi K, Kaelbling LP, Lozano-Pérez T. Bayesian Optimization with Exponential Convergence. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R, editors. Advances in Neural Information Processing Systems 28. Curran Associates, Inc.; 2015. pp. 2809–2817.</mixed-citation></ref>
<ref id="pcbi.1006043.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balci</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Simen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Niyogi</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Saxe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hughes</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Acquisition of decision making criteria: reward rate ultimately beats accuracy</article-title>. <source>Attention, Perception, &amp; Psychophys. Springer</source>; <year>2011</year>;<volume>73</volume>: <fpage>640</fpage>–<lpage>657</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref058"><label>58</label><mixed-citation publication-type="other" xlink:type="simple">Acerbi L, Ma WJ. Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search. In: Guyon I, Luxburg U V, Bengio S, Wallach H, Fergus R, Vishwanathan S, et al., editors. Advances in Neural Information Processing Systems 30. Curran Associates, Inc.; 2017. pp. 1834–1844. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/6780-practical-bayesian-optimization-for-model-fitting-with-bayesian-adaptive-direct-search.pdf" xlink:type="simple">http://papers.nips.cc/paper/6780-practical-bayesian-optimization-for-model-fitting-with-bayesian-adaptive-direct-search.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1006043.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Braver</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Barch</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Carter</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>Conflict monitoring and cognitive control</article-title>. <source>Psychol Rev. Department of Psychology, Carnegie Mellon University, Pennsylvania, USA</source>. <email xlink:type="simple">mmb@cnbc.cmu.edu</email>; <year>2001</year>;<volume>108</volume>: <fpage>624</fpage>–<lpage>652</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kass</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>A</given-names></name>. <article-title>Bayes Factors</article-title>. <source>J Am Stat Assoc. American Statistical Association</source>; <year>1995</year>;<volume>90</volume>: <fpage>773</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2291091" xlink:type="simple">10.2307/2291091</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006043.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name>, others. <article-title>Estimating the dimension of a model</article-title>. <source>Ann Stat. Institute of Mathematical Statistics</source>; <year>1978</year>;<volume>6</volume>: <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Rumsey</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Parikh</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Heasly</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title>. <source>Nat Neurosci. Nature Research</source>; <year>2012</year>;<volume>15</volume>: <fpage>1040</fpage>–<lpage>1046</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGuire</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Kable</surname> <given-names>JW</given-names></name>. <article-title>Functionally dissociable influences on learning rate in a dynamic environment</article-title>. <source>Neuron. Elsevier</source>; <year>2014</year>;<volume>84</volume>: <fpage>870</fpage>–<lpage>881</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kamin</surname> <given-names>LJ</given-names></name>. <article-title>Predictability, surprise, attention, and conditioning</article-title>. <source>Punishm aversive Behav</source>. <year>1969</year>; <fpage>279</fpage>–<lpage>296</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>. <article-title>Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes</article-title>. <source>PLoS Comput Biol. Public Library of Science</source>; <year>2011</year>;<volume>7</volume>: <fpage>e1002055</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002055" xlink:type="simple">10.1371/journal.pcbi.1002055</ext-link></comment> <object-id pub-id-type="pmid">21637741</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci. Nature Publishing Group</source>; <year>2005</year>;<volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1560" xlink:type="simple">10.1038/nn1560</ext-link></comment> <object-id pub-id-type="pmid">16286932</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kool</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Cushman</surname> <given-names>FA</given-names></name>. <article-title>Cost-benefit arbitration between multiple reinforcement-learning systems</article-title>. <source>Psychol Sci</source>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</article-title>. <source>Neural Comput. MIT Press</source>; <year>2006</year>;<volume>18</volume>: <fpage>283</fpage>–<lpage>328</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verguts</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Vassena</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Silvetti</surname> <given-names>M</given-names></name>. <article-title>Adaptive effort investment in cognitive and physical tasks: A neurocomputational model</article-title>. <source>Front Behav Neurosci. Frontiers Media SA</source>; <year>2015</year>;<volume>9</volume>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref070"><label>70</label><mixed-citation publication-type="other" xlink:type="simple">Krueger PM, Lieder F, Griffiths TL. Enhancing Metacognitive Reinforcement learning using reward structures and feedback. Proceedings of the 39th Annual Conference of the Cognitive Science Society. 2017.</mixed-citation></ref>
<ref id="pcbi.1006043.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Danielmeier</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ullsperger</surname> <given-names>M</given-names></name>. <article-title>Post-error adjustments</article-title>. <source>Front Psychol</source>. <year>2011</year>;<volume>2</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2011.00233" xlink:type="simple">10.3389/fpsyg.2011.00233</ext-link></comment> <object-id pub-id-type="pmid">21954390</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref072"><label>72</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Laming</surname> <given-names>DRJ</given-names></name>. <source>Information theory of choice-reaction times</source>. <publisher-loc>Oxford, England</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <year>1968</year>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gratton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Coles</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Donchin</surname> <given-names>E</given-names></name>. <article-title>Optimizing the use of information: strategic control of activation of responses</article-title>. <source>J Exp Psychol Gen</source>. <year>1992</year>;<volume>121</volume>: <fpage>480</fpage>–<lpage>506</lpage>. <object-id pub-id-type="pmid">1431740</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Carr</surname> <given-names>TH</given-names></name>. <article-title>Automaticity in skill acquisition: Mechanisms for reducing interference in concurrent performance</article-title>. <source>J Exp Psychol Hum Percept Perform. American Psychological Association</source>; <year>1989</year>;<volume>15</volume>: <fpage>686</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Waszak</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Hommel</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Allport</surname> <given-names>A</given-names></name>. <article-title>Task-switching and long-term priming: Role of episodic stimulus—task bindings in task-shift costs</article-title>. <source>Cogn Psychol. Elsevier</source>; <year>2003</year>;<volume>46</volume>: <fpage>361</fpage>–<lpage>413</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Waszak</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Hommel</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Allport</surname> <given-names>A</given-names></name>. <article-title>Semantic generalization of stimulus-task bindings</article-title>. <source>Psychon Bull &amp; Rev. Springer</source>; <year>2004</year>;<volume>11</volume>: <fpage>1027</fpage>–<lpage>1033</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Waszak</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Hommel</surname> <given-names>B</given-names></name>. <article-title>The costs and benefits of cross-task priming</article-title>. <source>Mem &amp; Cogn. Springer</source>; <year>2007</year>;<volume>35</volume>: <fpage>1175</fpage>–<lpage>1186</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mayr</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Bryck</surname> <given-names>RL</given-names></name>. <article-title>Outsourcing control to the environment: effects of stimulus/response locations on task selection</article-title>. <source>Psychol Res. Springer</source>; <year>2007</year>;<volume>71</volume>: <fpage>107</fpage>–<lpage>116</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006043.ref079"><label>79</label><mixed-citation publication-type="other" xlink:type="simple">Lieder F, Griffiths TL. When to use which heuristic: A rational solution to the strategy selection problem. In: Noelle DC, Dale R, Warlaumont AS, Yoshimi J, Matlock T, Jennings CD, et al., editors. Proceedings of the 37th Annual Conference of the cognitive science society. Austin, TX: Cognitive Science Society; 2015.</mixed-citation></ref>
<ref id="pcbi.1006043.ref080"><label>80</label><mixed-citation publication-type="other" xlink:type="simple">Lieder F, Plunkett D, Hamrick JB, Russell SJ, Hay NJ, Griffiths TL. Algorithm selection by rational metareasoning as a model of human strategy selection. In: Ghahramani Z, Welling M, Weinberger KQ, Cortes C, Lawrence ND, editors. Advances in Neural Information Processing Systems 27. Curran Associates, Inc.; 2014.</mixed-citation></ref>
<ref id="pcbi.1006043.ref081"><label>81</label><mixed-citation publication-type="other" xlink:type="simple">Bustamante L, Lieder F, Musslick S, Shenhav A, Cohen JD. Learning to (mis)allocate control: maltransfer can lead to self-control failure. In: Brunskill E, N. Daw, editors. The 3rd Multidisciplinary Conference on Reinforcement Learning and Decision Making. Ann Arbor, Michigan; 2017.</mixed-citation></ref>
<ref id="pcbi.1006043.ref082"><label>82</label><mixed-citation publication-type="other" xlink:type="simple">Lieder F, Griffiths TL, Huys QJ, Goodman ND. Reinterpreting anchoring-and-adjustment as rational use of cognitive resources.</mixed-citation></ref>
<ref id="pcbi.1006043.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Braver</surname> <given-names>T</given-names></name>. <article-title>The variable nature of cognitive control: a dual mechanisms framework</article-title>. <source>Trends Cogn Sci</source>. <year>2012</year>;<volume>16</volume>: <fpage>106</fpage>–<lpage>113</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2011.12.010" xlink:type="simple">10.1016/j.tics.2011.12.010</ext-link></comment> <object-id pub-id-type="pmid">22245618</object-id></mixed-citation></ref>
<ref id="pcbi.1006043.ref084"><label>84</label><mixed-citation publication-type="other" xlink:type="simple">Suchow JW. Measuring, monitoring, and maintaining memories in a partially observable mind. Harvard University. 2014.</mixed-citation></ref>
<ref id="pcbi.1006043.ref085"><label>85</label><mixed-citation publication-type="other" xlink:type="simple">Sutton RS. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. Proceedings of the seventh international conference on machine learning. 1990. pp. 216–224.</mixed-citation></ref>
<ref id="pcbi.1006043.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Inzlicht</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schmeichel</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Macrae</surname> <given-names>CN</given-names></name>. <article-title>Why self-control seems (but may not be) limited</article-title>. <source>Trends Cogn Sci. Elsevier Science</source>; <year>2014</year>;<volume>18</volume>: <fpage>127</fpage>–<lpage>133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.12.009" xlink:type="simple">10.1016/j.tics.2013.12.009</ext-link></comment> <object-id pub-id-type="pmid">24439530</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>