<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005119</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00189</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Audio signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Phonetics</subject><subj-group><subject>Vowels</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal filtering</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Equipment</subject><subj-group><subject>Audio equipment</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Digestive system</subject><subj-group><subject>Mouth</subject><subj-group><subject>Tongue</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Digestive system</subject><subj-group><subject>Mouth</subject><subj-group><subject>Tongue</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Real-Time Control of an Articulatory-Based Speech Synthesizer for Brain Computer Interfaces</article-title>
<alt-title alt-title-type="running-head">Real-Time Articulatory-Based Speech Synthesis for BCIs</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bocquelet</surname>
<given-names>Florent</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Hueber</surname>
<given-names>Thomas</given-names>
</name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Girin</surname>
<given-names>Laurent</given-names>
</name>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Savariaux</surname>
<given-names>Christophe</given-names>
</name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Yvert</surname>
<given-names>Blaise</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>INSERM, BrainTech Laboratory U1205, Grenoble, France</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Univ. Grenoble Alpes, BrainTech Laboratory U1205, Grenoble, France</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>CNRS, GIPSA-Lab, Saint-Martin-d'Hères, France</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Univ. Grenoble Alpes, GIPSA-Lab, Saint-Martin-d'Hères, France</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>INRIA Grenoble Rhône-Alpes, Montbonnot, France</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Mindlin</surname>
<given-names>Gabriel</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Buenos Aires, ARGENTINA</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item><p><bold>Conceived and designed the experiments:</bold> FB TH LG BY.</p></list-item> <list-item><p><bold>Performed the experiments:</bold> FB TH CS BY.</p></list-item> <list-item><p><bold>Analyzed the data:</bold> FB TH BY.</p></list-item> <list-item><p><bold>Contributed reagents/materials/analysis tools:</bold> FB.</p></list-item> <list-item><p><bold>Wrote the paper:</bold> FB TH LG BY.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">blaise.yvert@inserm.fr</email> (BY)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>23</day>
<month>11</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<month>11</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>11</issue>
<elocation-id>e1005119</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>2</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>8</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Bocquelet et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005119"/>
<abstract>
<p>Restoring natural speech in paralyzed and aphasic people could be achieved using a Brain-Computer Interface (BCI) controlling a speech synthesizer in real-time. To reach this goal, a prerequisite is to develop a speech synthesizer producing intelligible speech in real-time with a reasonable number of control parameters. We present here an articulatory-based speech synthesizer that can be controlled in real-time for future BCI applications. This synthesizer converts movements of the main speech articulators (tongue, jaw, velum, and lips) into intelligible speech. The articulatory-to-acoustic mapping is performed using a deep neural network (DNN) trained on electromagnetic articulography (EMA) data recorded on a reference speaker synchronously with the produced speech signal. This DNN is then used in both offline and online modes to map the position of sensors glued on different speech articulators into acoustic parameters that are further converted into an audio signal using a vocoder. In offline mode, highly intelligible speech could be obtained as assessed by perceptual evaluation performed by 12 listeners. Then, to anticipate future BCI applications, we further assessed the real-time control of the synthesizer by both the reference speaker and new speakers, in a closed-loop paradigm using EMA data recorded in real time. A short calibration period was used to compensate for differences in sensor positions and articulatory differences between new speakers and the reference speaker. We found that real-time synthesis of vowels and consonants was possible with good intelligibility. In conclusion, these results open to future speech BCI applications using such articulatory-based speech synthesizer.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Speech Brain-Computer-Interfaces may restore communication for people with severe paralysis by decoding cortical speech activity to control in real time a speech synthesizer. Recent data indicate that the speech motor cortex houses a detailed topographical representation of the different speech articulators of the vocal tract. Its activity could thus be used in a speech BCI paradigm to control an articulatory-based speech synthesizer. To date, there has been no report of an articulatory-based speech synthesizer producing fully intelligible speech, as well as no evidence that such device could be controlled in real time by a naive subject. Here, we present an articulatory-based speech synthesizer and show that it could be controlled in real time by several subjects articulating silently to produce intelligible speech.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002915</institution-id>
<institution>Fondation pour la Recherche Médicale</institution>
</institution-wrap>
</funding-source>
<award-id>DBS20140930785</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>YVERT</surname>
<given-names>Blaise</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by the Fondation pour la Recherche Médicale and by the French Research Agency (ANR) under the Brainspeak project (<ext-link ext-link-type="uri" xlink:href="http://www.frm.org" xlink:type="simple">www.frm.org</ext-link>) under grant No DBS20140930785. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="11"/>
<table-count count="0"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The BY2014 articulatory-acoustic dataset is available for download at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/154083" xlink:type="simple">https://zenodo.org/record/154083</ext-link> (doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.154083" xlink:type="simple">10.5281/zenodo.154083</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In the past decades, brain-computer interfaces (BCIs) have been developed in order to restore capabilities of people with severe paralysis, such as tetraplegia or locked-in syndrome. The movements of different effectors, like a computer mouse or a robotic arm, were successfully controlled in several BCI studies, with increasing precision [<xref ref-type="bibr" rid="pcbi.1005119.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005119.ref009">9</xref>]. In the case of severe paralysis including aphasia (e.g., locked-in syndrome), ways of communicating can be provided by BCI approaches, mostly through a letter selection or typing process [<xref ref-type="bibr" rid="pcbi.1005119.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref011">11</xref>]. However, speech remains our most natural and efficient way of communication. BCI approaches could thus also be applied to control a parametric speech synthesizer in real-time in order to restore communication by decoding neural activity from speech processing brain areas [<xref ref-type="bibr" rid="pcbi.1005119.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref013">13</xref>]. Such perspective is indeed supported by an increasing number of studies reporting encouraging performances in decoding speech utterances, including phones, words or even full sentences, from brain activity [<xref ref-type="bibr" rid="pcbi.1005119.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005119.ref021">21</xref>].</p>
<p>Several brain areas are involved in speech processing, forming a wide cortical network, classically modeled by a ventral and a dorsal stream [<xref ref-type="bibr" rid="pcbi.1005119.ref022">22</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref023">23</xref>]. Because this speech network is widely distributed, a choice needs to be made on the cortical areas where to extract and decode signals for a speech BCI. One possibility is to probe signals from auditory areas, which encode the spectro-temporal representation of speech. However, auditory areas are widely involved in the sensory perception and integration of all sounds a person is exposed to, including speech and non-speech environmental sounds. For this reason, probing neural activity in areas more specifically dedicated to speech production might be more relevant for conversational speech production with BCI. It should be noted that although aphasia caused by strokes very often affect the articulatory speech motor cortex or other cortical areas necessary for speech production, this is not the case for other types of aphasia, such as in locked-in patients or patients with amyotrophic lateral sclerosis, for whom cortical speech activity can be intact or largely conserved and thus exploitable in a BCI perspective. In such case, and if successful, continuous speech restoration would be more beneficial than an indirect communication scheme such as a letter selection process.</p>
<p>Previous anatomical and functional data indicate that the speech sensorimotor cortex exhibits a somatotopic organization mapping the different articulators involved in speech production [<xref ref-type="bibr" rid="pcbi.1005119.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1005119.ref028">28</xref>], the detailed dynamics of which can be highlighted using high-density neural recording [<xref ref-type="bibr" rid="pcbi.1005119.ref021">21</xref>]. Interestingly, it was further showed recently that during speech production, the activity of the speech sensorimotor cortex is rather tuned to the articulatory properties of the produced sounds than to their acoustic properties [<xref ref-type="bibr" rid="pcbi.1005119.ref029">29</xref>]. While neural data could be decoded directly into acoustic parameters, these data thus support our hypothesis that a relevant strategy could be to consider a more “indirect” approach accounting for the articulatory activity of the vocal tract under control of the speech sensorimotor cortex to produce sounds. In such approach, cortical signals will be probed and decoded to control in real time a parametric articulatory-based speech synthesizer having enough degrees of freedom to ensure continuous intelligible speech production. Interestingly, these articulatory features are generally considered lower dimensional and varying more slowly in time than acoustic features, thus possibly easier to predict from cortical signals.</p>
<p>Articulatory-based speech synthesizers are able to generate an intelligible speech audio signal from the position of the main speech articulators: tongue, lips, velum, jaw, and larynx [<xref ref-type="bibr" rid="pcbi.1005119.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1005119.ref036">36</xref>]. Articulatory-based synthesizers are mainly divided into two categories: physical or geometrical approaches (such as in [<xref ref-type="bibr" rid="pcbi.1005119.ref036">36</xref>–<xref ref-type="bibr" rid="pcbi.1005119.ref039">39</xref>]), which aim to model the geometry of the vocal tract and its physical properties, and machine-learning approaches (such as [<xref ref-type="bibr" rid="pcbi.1005119.ref034">34</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref035">35</xref>]), which use large databases to automatically learn the mathematical relationship between articulatory and acoustic data. Here we made the choice to build a synthesizer using a machine-learning approach. Indeed, to synthesize speech, geometrical and physical models must first solve the difficult inverse problem of recovering articulatory parameters from the acoustic of each calibration sequence. By contrast, with machine-learning approaches, a large articulatory-acoustic database must be recorded, thus avoiding this issue for all the sentences of this database. For BCI applications, one will require a parallel dataset of brain signals and control parameters for the synthesizer: here we can use known data from the articulatory-acoustic database. Moreover, geometrical and physical models need high computation power while, once trained, a machine-learning model is very fast to apply for real-time synthesis. Finally, we showed in a previous study that such machine-learning approach is robust to noisy input parameters [<xref ref-type="bibr" rid="pcbi.1005119.ref035">35</xref>], which is a non-negligible asset for BCI applications, when the decoding of neural data results in non-perfect signals.</p>
<p>Interestingly, articulatory-based synthesizers can be controlled with about 10 continuous parameters [<xref ref-type="bibr" rid="pcbi.1005119.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref040">40</xref>], which is of the order of the number of degrees of freedom controlled simultaneously in recent complex motor BCI paradigms in monkeys [<xref ref-type="bibr" rid="pcbi.1005119.ref041">41</xref>] and human participants [<xref ref-type="bibr" rid="pcbi.1005119.ref009">9</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref042">42</xref>].</p>
<p>However, it remains unknown whether a given articulatory-based speech synthesizer built from articulatory-acoustic data obtained in one particular reference speaker can be controlled in real time by any other speaker to produce intelligible speech. In this context, we present here an articulatory-based speech synthesizer producing intelligible speech that can be controlled in real time for future BCI applications. This synthesizer is based on a machine-learning approach in which the articulatory data recorded by electro-magnetic articulography (EMA) is converted into acoustic speech signals using deep neural networks (DNNs). We show that intelligible speech could be obtained in a closed-loop paradigm by different subjects controlling this synthesizer in real time from EMA recordings while articulating silently, i.e. without vocalizing. Such a silent speech condition is as close as possible to a speech BCI paradigm where the synthetic voice replaces the actual subject voice. These results thus pave the way toward a future use of such articulatory-based speech synthesizer controlled by neural activity in a speech BCI paradigm.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec003">
<title>Ethics Statement</title>
<p>All subjects gave their informed consent to participate in the study, which was approved by the local ethical committee of Grenoble for non-interventional research (CERNI) under approval No. 2016-01-05-82.</p>
</sec>
<sec id="sec004">
<title>Subjects</title>
<p>Four French native speakers (1 female, 3 males) participated in the study. One male subject was the reference speaker from whom data the synthesizer was built, and all four subjects then controlled in real time the synthesizer.</p>
</sec>
<sec id="sec005">
<title>Construction of an articulatory-based speech synthesizer</title>
<p>In a first step, we designed an intelligible articulatory-based speech synthesizer converting the trajectories of the main speech articulators (tongue, lips, jaw, and velum) into speech (see <xref ref-type="fig" rid="pcbi.1005119.g001">Fig 1</xref>). For this purpose, we first built a large articulatory-acoustic database, in which articulatory data from a native French male speaker was recorded synchronously with the produced audio speech signal. Then computational models based on DNNs were trained on these data to transform articulatory signals into acoustic speech signals (i.e. articulatory-to-acoustic mapping). When considering articulatory synthesis using physical or geometrical models, the articulatory data obtained by EMA can be mapped to the geometrical parameters of the model [<xref ref-type="bibr" rid="pcbi.1005119.ref039">39</xref>]. Here we consider a machine-learning approach in which the articulatory data obtained by EMA is directly mapped to the acoustic parameters of a vocoder.</p>
<fig id="pcbi.1005119.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Articulatory-based speech synthesizer.</title>
<p>Using a DNN, articulatory features of the reference speaker are mapped to acoustic features, which are then converted into an audible signal using the MLSA filter and an excitation signal.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g001" xlink:type="simple"/>
</fig>
<sec id="sec006">
<title>Articulatory data acquisition and parameterization</title>
<p>The articulatory data was recorded using the electromagnetic articulography (EMA) NDI Wave system (NDI, Ontario, Canada), which allows three-dimensional tracking of the position of small coils with a precision of less than a millimeter. Nine such 3D coils were glued on the tongue tip, dorsum, and back, as well as on the upper lip, the lower lip, the left and right lip corners, the jaw and the soft palate (<xref ref-type="fig" rid="pcbi.1005119.g002">Fig 2A</xref>). This configuration was chosen for being similar to the ones used in the main publicly available databases, such as MOCHA (<ext-link ext-link-type="uri" xlink:href="http://www.cstr.ed.ac.uk/research/projects/artic/mocha.html" xlink:type="simple">http://www.cstr.ed.ac.uk/research/projects/artic/mocha.html</ext-link>) and mngu0 [<xref ref-type="bibr" rid="pcbi.1005119.ref043">43</xref>], and in other studies in articulatory-based synthesis [<xref ref-type="bibr" rid="pcbi.1005119.ref034">34</xref>] or articulatory-to-acoustic inversion [<xref ref-type="bibr" rid="pcbi.1005119.ref044">44</xref>]. This configuration allows to capture well the movements of the main articulators while avoiding to perturb the speaker too much: 3 coils on the tongue give information on back, dorsum and apex while 4 coils on lips give information on protrusion and rounding, and we considered that one sensor was enough for the jaw since it is a rigid articulator, and one for the soft palate since it has mostly one degree of freedom. An additional 6D reference coil (which position and orientation can be measured) was used to account for head movements and was glued behind the right ear of the subject. To avoid coil detachment due to salivation, two precautions were taken to glue the sensors. First, the tongue and soft palate sensors were glued onto small pieces of silk in order to increase contact surface, and second, the tongue, soft palate and jaw surfaces were carefully dried using cottons soaked with 55% green Chartreuse liquor. The recorded sequences of articulatory coordinates were down-sampled from 400 Hz to 100 Hz.</p>
<fig id="pcbi.1005119.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Articulatory and acoustic data.</title>
<p><bold>A</bold>–Positioning of the sensors on the lip corners (1 &amp; 3), upper lip (2), lower lip (4), tongue tip (5), tongue dorsum (6), tongue back (7) and velum (8). The jaw sensor was glued at the base of the incisive (not visible in this image). B–Articulatory signals and corresponding audio signal for the sentence “Annie s’ennuie loin de mes parents” (“Annie gets bored away from my parents”). For each sensor, the horizontal caudo-rostral X and below the vertical ventro-dorsal Y coordinates projected in the midsagittal plane are plotted. Dashed lines show the phone segmentation obtained by forced-alignment. C–Acoustic features (25 mel-cepstrum coefficients—MEL) and corresponding segmented audio signal for the same sentence as in B.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g002" xlink:type="simple"/>
</fig>
<p>Finally, in order to assess the effect of the number of articulatory parameters on the intelligibility of synthesized signals, four different parameterizations of the articulatory data were used: using the raw 3D data of the 9 sensors (27 parameters), projecting the data in the midsagittal plane and removing the lip-corner sensors (14 parameters, see <xref ref-type="fig" rid="pcbi.1005119.g002">Fig 2B</xref>), and reducing the original data to either 10 or 7 parameters using principal component analysis (PCA).</p>
</sec>
<sec id="sec007">
<title>Acoustic data acquisition, parameterization and synthesis</title>
<p>The speech signal was recorded at 22,050 Hz synchronously with the articulatory data. Its spectral content was parameterized by 25 mel-cepstrum (MEL) coefficients (<xref ref-type="fig" rid="pcbi.1005119.g002">Fig 2C</xref>) computed every 10 ms (hence a 100 Hz sampling matching the articulatory data acquisition frequency) from a 23-ms (512 samples) sliding window using the Speech Processing ToolKit (SPTK, <ext-link ext-link-type="uri" xlink:href="http://sp-tk.sourceforge.net/" xlink:type="simple">http://sp-tk.sourceforge.net/</ext-link>) <italic>mcep</italic> tools [<xref ref-type="bibr" rid="pcbi.1005119.ref045">45</xref>]. These 25 coefficients efficiently represent the spectral envelope of speech and can be converted back into audible sounds by building a so-called Mel Log Spectrum Approximation (MLSA) filter [<xref ref-type="bibr" rid="pcbi.1005119.ref046">46</xref>]. This approach is based on the source-filter model of speech production, which models the speech signal as a convolution of a sound source (e.g., the glottal activity) with a linear acoustic filter representing the vocal tract,. In the present MLSA model, a set of M mel-cepstrum coefficients <italic>c</italic><sub><italic>α</italic></sub>(<italic>m</italic>) represent the vocal tract filter <italic>H</italic>(<italic>z</italic>) for each audio signal window, as follows:
<disp-formula id="pcbi.1005119.e001">
<alternatives>
<graphic id="pcbi.1005119.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mspace width="0.15em"/><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1005119.e002">
<alternatives>
<graphic id="pcbi.1005119.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>The coefficient α is chosen so that the mel-scale becomes a good approximation of the human sensitivity to the loudness of speech (here α = 0.455). The mel-cepstral coefficients <italic>c</italic><sub><italic>α</italic></sub>(<italic>m</italic>) are approximated using the Newton-Raphson method for numerically solving equations, and linearly combined to obtain the MLSA filter coefficients. This filter is then excited with a source signal representing the glottal activity (i.e. vibration of the vocal folds). Such excitation signal is generally designed by extracting the pitch from the original audio signal, and then generating white noise for non-voiced segments, and a train of pulses for voiced-segments, which period varies according to the pitch. However, since there is no such glottal activity in silent speech, we as well designed an artificial template-based excitation signal using the glottal activity from a single vowel /a/. While such glottal activity could be recorded using an electroglottograph as in [<xref ref-type="bibr" rid="pcbi.1005119.ref047">47</xref>], here we estimated it using inverse filtering [<xref ref-type="bibr" rid="pcbi.1005119.ref048">48</xref>] (using the SPTK <italic>mlsadf</italic> tool). Using such an excitation signal results in an unnatural (‘‘robotic”) speech sound in which all the phonemes are voiced, and have the same pitch. Transcription of the audio signals was first done manually in naturally written text, then translated into phone sequences using <italic>LLiaPhon phonetizer</italic> (<ext-link ext-link-type="uri" xlink:href="https://gna.org/projects/lliaphon" xlink:type="simple">https://gna.org/projects/lliaphon</ext-link>) [<xref ref-type="bibr" rid="pcbi.1005119.ref049">49</xref>], and finally manually corrected. Phone sequences were then automatically aligned on audio files using a standard speech recognition system (based on a set of tied-state context-dependent phonetic hidden Markov models (HMM) trained using the HTK toolkit <ext-link ext-link-type="uri" xlink:href="http://htk.eng.cam.ac.uk/" xlink:type="simple">http://htk.eng.cam.ac.uk/</ext-link>)) and a forced-alignment procedure.</p>
</sec>
<sec id="sec008">
<title>Reference articulatory-acoustic database (the BY2014 corpus)</title>
<p>For this specific study, we recorded a large articulatory-acoustic database, named BY2014, containing more than 45 minutes of speech after removing the periods of silence. This database was composed of 712 items of variable length, ranging from isolated vowels, vowel-consonant-vowel sequences (VCVs), phonetically balanced sentences, and other sentences extracted from French newspaper “Le Monde”. This resulted in 18,828 phones in total. The distribution of the 34 different phonetic classes used to describe French language in this corpus is shown in <xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3A</xref>. Phone frequency ranged from 1,420 occurrences for the phone /a/ to 27 occurrences for /ɲ/. The distribution of the articulatory data points in the midsagittal plane is represented in <xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3B</xref>. The velum was the articulator with the smallest movement amplitude (less than 1cm), followed by the jaw and the upper lip (about 1cm), the lower lip (about 2cm), and finally the tongue had the highest amplitude of movement (about 3cm for each sensor). In the following, this dataset is referred to as the <italic>reference data</italic>, and the subject from whom this data was recorded is referred to as the <italic>reference speaker</italic>. This dataset is available for download at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/154083" xlink:type="simple">https://zenodo.org/record/154083</ext-link> (doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.154083" xlink:type="simple">10.5281/zenodo.154083</ext-link>).</p>
<fig id="pcbi.1005119.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Articulatory-acoustic database description.</title>
<p>A–Occurrence histogram of all phones of the articulatory-acoustic database. Each bar shows the number of occurrence of a specific phone in the whole corpus. B–Spatial distribution of all articulatory data points of the database (silences excluded) in the midsagittal plane. The positions of the 7 different sensors are plotted with different colors. The labeled positions correspond to the mean position for the 7 main French vowels.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Articulatory-to-acoustic mapping using Deep Neural Networks (DNN)</title>
<p>The articulatory-to-acoustic mapping was performed using a deep neural network (DNN) trained on the reference data (see <xref ref-type="fig" rid="pcbi.1005119.g001">Fig 1</xref>). A DNN is an artificial neural network with more than one or two hidden layers, which can be used to address complex (and highly non-linear) classification and regression problems. More details can be found in [<xref ref-type="bibr" rid="pcbi.1005119.ref050">50</xref>]. The choice for using a DNN-based mapping was motivated by previous results [<xref ref-type="bibr" rid="pcbi.1005119.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref051">51</xref>] showing that such mapping was more robust to noisy input than a state-of-the-art mapping based on Gaussian Mixture Model (GMM) such as the one proposed by Toda et al. [<xref ref-type="bibr" rid="pcbi.1005119.ref034">34</xref>], and thus likely a better candidate for future BCI applications where input articulatory control signals will be inferred from noisy cortical signals.</p>
<p>Here, we considered a fully-connected feed-forward DNN with 3 hidden layers containing 200 leaky rectified linear units (LReLU), which are known to improve convergence of the training procedure and to lead to better solutions than conventional sigmoid units [<xref ref-type="bibr" rid="pcbi.1005119.ref052">52</xref>]. Each unit of a layer was connected to all units of the next layer, and there was no connection between units belonging to the same layer. Both input (articulatory) and output (acoustic) data were z-scored (subtraction of the mean and then division by the standard deviation) before being fed to the network, and data frames corresponding to silence periods were removed. To take into account the dynamic properties of speech, we concatenated each articulatory frame with its 4 preceding frames (50-ms time window context compatible with a real-time implementation). The DNN thus mapped the articulatory input features to 25 output mel-cepstrum coefficients, which were then converted into an audible speech signal using the MLSA filter and the excitation signal.</p>
<p>DNN training is usually a complex task since large initial weights typically lead to poor local minima, while small initial weights lead to small gradients making the training infeasible with many hidden layers [<xref ref-type="bibr" rid="pcbi.1005119.ref053">53</xref>]. Here, we trained our network using the classical back-propagation algorithm [<xref ref-type="bibr" rid="pcbi.1005119.ref054">54</xref>]. However, we chose to add the different layers successively. During the first step, the network was only composed of the input layer, the first hidden layer, and the output layer. This initial network was randomly initialized then fine-tuned using back-propagation. Then a new hidden layer was added and the output layer was replaced with a new one so that the new network was now composed by the input layer, the previously trained hidden layer, the new hidden layer, and the new output layer. The weights from the input layer to the first hidden layer were those obtained at the previous step and the other weights were randomly initialized. Back-propagation was then applied to this new network for fine-tuning. This process was repeated until all the hidden layers were added. At each step, the weights of a given layer were randomly initialized using a Gaussian distribution with a zero mean and a 1/N standard deviation, where N was the number of units of the previous layer. The error criterion was the mean squared error (MSE) between predicted and expected values:
<disp-formula id="pcbi.1005119.e003">
<alternatives>
<graphic id="pcbi.1005119.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <italic>o</italic><sub><italic>i</italic></sub> is the i-th output of the network, D = 25 is the number of MEL outputs, and <italic>m</italic><sub>i</sub> is the i-th z-scored MEL coefficient computed on the original audio data. The minimization of this error was done with the Polack-Ribière conjugate gradient method using 3 quadratic/cubic line searches [<xref ref-type="bibr" rid="pcbi.1005119.ref055">55</xref>], on successive batches: at each epoch, the training data samples were randomly shuffled and then divided into 100 blocks. Dividing into batches allowed more efficient computation than when using single samples [<xref ref-type="bibr" rid="pcbi.1005119.ref052">52</xref>]. The training was made by randomly selecting 90% of the articulatory-acoustic database items, while half of the remaining 10% were used for early stopping of the training (the training was stopped when the error on this validation set did not improve over 20 epochs) and the other half for testing. The whole training was done using custom-made optimization tools written in C++, the conjugate gradient code being adapted from the Matlab implementation of the DRToolBox (<ext-link ext-link-type="uri" xlink:href="http://lvdmaaten.github.io/drtoolbox/" xlink:type="simple">http://lvdmaaten.github.io/drtoolbox/</ext-link>). Four different DNNs were trained, one for each of the four different parametrizations of the articulatory data (with 27, 14, 10 and 7 articulatory parameters).</p>
</sec>
</sec>
<sec id="sec010">
<title>Real-time control of the articulatory-based speech synthesizer</title>
<p>In a second step, four speakers controlled the synthesizer in real time. As built, the synthesizer could only be used on the reference data and could not be directly controlled by another speaker or even by the same speaker in a different session. Indeed, from one session to another, sensors might not be placed at the exact same positions with the exact same orientation, or the number of sensors could change, or the speaker could be a new subject with a different vocal tract geometry and different ways of articulating the same sounds. In order to take into account these differences, it was necessary to calibrate a mapping from the articulatory space of each new speaker (or the same reference speaker in a new session) to the articulatory space of the reference speaker, that is, an articulatory-to-articulatory mapping (<xref ref-type="fig" rid="pcbi.1005119.g004">Fig 4A and 4B</xref>, left blue part). To achieve this calibration, we acquired articulatory data from the new speaker that corresponded to known reference articulatory data. This calibration model was then applied in real time to incoming articulatory trajectories of each silent speaker to produce continuous input to the speech synthesizer. Since the subjects were in silent speech and thus no glottal activity was available, we chose to perform the synthesis using the fixed-pitch template-based excitation, and in order to reduce the number of control parameters, we chose the synthesis model using 14 articulatory parameters since results showed that it was able to produce fully intelligible speech (see first part of the Results section). <xref ref-type="fig" rid="pcbi.1005119.g004">Fig 4C</xref> summarizes the whole experimental protocol, which is detailed in the following sections.</p>
<fig id="pcbi.1005119.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Real-time closed loop synthesis.</title>
<p>A) Real-time closed-loop experiment. Articulatory data from a silent speaker are recorded and converted into articulatory input parameters for the articulatory-based speech synthesizer. The speaker receives the auditory feedback of the produced speech through earphones. B) Processing chain for real-time closed-loop articulatory synthesis, where the articulatory-to-articulatory (left part) and articulatory-to-acoustic mappings (right part) are cascaded. Items that depend on the reference speaker are in orange, while those that depend on the new speaker are in blue. The articulatory features of the new speaker are linearly mapped to articulatory features of the reference speaker, which are then mapped to acoustic features using a DNN, which in turn are eventually converted into an audible signal using the MLSA filter and the template-based excitation signal. C) Experimental protocol. First, sensors are glued on the speaker’s articulators, then articulatory data for the calibration is recorded in order to compute the articulatory-to-articulatory mapping, and finally the speaker articulates a set of test items during the closed-loop real-time control of the synthesizer.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g004" xlink:type="simple"/>
</fig>
<sec id="sec011">
<title>Subjects and experimental design</title>
<p>Four subjects (1 female, 3 males) controlled the synthesizer in real time. The reference speaker was one of them (Speaker 1), i.e. he was also used as a test subject, but with data from a different session than the reference data session. The articulatory data was recorded using the NDI Wave system in similar conditions as for acquiring the reference data (recording at 400Hz and down-sampling to 100Hz), except that only 6 sensors were used to record the articulatory movements of the lower and upper lips, the tongue tip, dorsum and back, and the jaw. This was due to the fact that the NDI Wave system was limited to 6 sensors when retrieving the articulatory data in real-time (while it was possible to use more sensors in offline mode). The soft palate sensor was one of the discarded sensor because most subjects (3 out of 4) were very uncomfortable with keeping the soft palate sensor for a long duration. The corner lip sensors were also discarded because they were the least informative for the synthesis. While the coils were positioned at the same anatomical locations, no particular attention was given to place them at very precise locations.</p>
</sec>
<sec id="sec012">
<title>Articulatory-to-articulatory mapping</title>
<p>In order to estimate the articulatory-to-articulatory mapping, it was necessary to obtain articulatory data from the new speakers in synchrony with articulatory data from the reference speaker when articulating the same sounds. The new speakers were thus asked to silently repeat a subset of 50 short sentences (about 4 words each), extracted from the reference articulatory-acoustic database, in synchrony with the corresponding audio presented through earphones. Each sentence was first displayed on the screen during one second, then after a visual countdown, it was played three times at a fixed pace, so that the speaker could adapt to the reference speaker rate and way of speaking. Only the last repetition was considered in order to obtain the best temporal synchronization. Subjects were asked to repeat the sentences silently, and not loudly, because of significant differences that may exist between silent and vocalized speech, and because subsequent real-time closed-loop control of the synthesizer would then be achieved while subjects were silently speaking. For each silent speaker, the articulatory-to-articulatory mapping was performed using a linear model mapping the articulatory data of the speaker to those of the reference speaker. For this purpose, the frames corresponding to silence periods were discarded. In order to counterbalance speaker and system latencies, a global delay between new and reference articulatory data was estimated for each speaker by trying different delays. A different linear model between the new speaker’s articulatory data and the reference data was computed for each candidate delay. Each linear model was then applied to the new speaker’s articulatory data, and the mean-squared error (MSE) between predicted and actual reference articulatory data was computed. The delay which led to the smallest MSE was considered to be due to system latency and was corrected before the training of the final model by simply shifting and cutting the original data. Out of the 50 calibration sentences, 40 were used for the training of the articulatory-to-articulatory mapping while the remaining 10 sentences were kept for testing (randomly chosen, but identical across all speakers). By contrast with the articulatory-to-acoustic mapping, the articulatory-to-articulatory mapping was done frame-by-frame, without concatenating any past frame.</p>
</sec>
<sec id="sec013">
<title>Real-time control</title>
<p>As shown in <xref ref-type="fig" rid="pcbi.1005119.g004">Fig 4B</xref>, the real-time control of the articulatory-based speech synthesizer was achieved by cascading the linear model used for articulatory-to-articulatory mapping and the DNN used for articulatory-to-acoustic mapping (using 14 articulatory parameters). The input articulatory data capture and processing (especially the re-referencing with regards to the reference sensor), the linear and DNN mappings and the MLSA filter were all implemented within the Max/MSP environment (Cycling’74, Walnut CA, USA, <ext-link ext-link-type="uri" xlink:href="https://cycling74.com/products/max/" xlink:type="simple">https://cycling74.com/products/max/</ext-link>) dedicated to real-time audio processing. Special attention was given to audio settings in order to minimize the audio chain latency and obtain a delay inferior to 30ms. Since the subjects were in silent speech condition, and thus no glottal activity was present, we used the template-based excitation signal for the MLSA filter. During this closed-loop situation, each speaker was asked to silently articulate a set of test items while given the synthesized auditory feedback through amagnetic Nicolet TIP-300 insert earphones (Nicolet Biomedical, Madison, USA) ensuring no interference with the magnetic field of the NDI Wave system. The auditory feedback was recorded for further intelligibility evaluation. Subjects were allowed to adapt to the closed-loop situation for at least 20 minutes. Then, they were asked to pronounce a set of test items, which were not part of the datasets used to train the articulatory-to-acoustic and the articulatory-to-articulatory mappings. The test set consisted of the 7 isolated vowels /a/, /e/, /i/, /o/, /u/, /œ/, and /y/, and 21 vowel-consonant-vowel (VCV) pseudo-words made by the 7 consonants /b/, /d/, /g/, /l/, /v/, /z/ and /ʒ/, in /a/, /i/ and /u/ context (e.g., ‘aba’ or ‘ili’). We chose not to include nasal vowels (e.g., /<inline-formula id="pcbi.1005119.e004"><alternatives><graphic id="pcbi.1005119.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mover accent="true"><mml:mi>ɑ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/, which corresponds to a nasalized /a/) since no sensor was placed on the soft palate. Likewise, we did not include nasal consonants (e.g., /m/ or /n/) and most unvoiced consonants (e.g., /p/ which roughly corresponds to an unvoiced /b/). Each item of the test set was repeated three times in a row. The whole set of items was repeated three times by each speaker, each repetition being separated by about 10 minutes of free control of the synthesizer.</p>
</sec>
</sec>
<sec id="sec014">
<title>Evaluation protocol</title>
<p>The quality of open and closed-loop speech synthesis was assessed in two ways. We first carried out a qualitative evaluation, in which the acoustic signals obtained by offline synthesis from the reference database corpus (reference offline synthesis) or during closed-loop experiments (closed-loop synthesis) were compared with the original signals processed through analysis-synthesis. Analysis-synthesis was performed by converting the audio signals into mel-cepstrum (MEL) coefficients, which were then directly converted back into audio signals using the MLSA filter and template-based excitation. Such signal is referred here to as the ‘anasynth signal’. This conversion is not lossless, though it represents what would be the best achievable quality for the synthetic speech signal in the present context.</p>
<p>Then we carried out a quantitative evaluation of our system through an intelligibility test. Twelve subjects participated to this test. All participants were French native speakers with no hearing impairment.</p>
<p>For the offline reference synthesis, the evaluated stimuli consisted of the 10 vowels /a/, /i/, /u/, /o/, /œ/, /e/, /y/, /ã/, /<inline-formula id="pcbi.1005119.e005"><alternatives><graphic id="pcbi.1005119.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mover accent="true"><mml:mi>ɛ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/, and /<inline-formula id="pcbi.1005119.e006"><alternatives><graphic id="pcbi.1005119.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mi>ɔ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/, and the 48 VCVs made of /p/, /t/, /k/, /f/, /s/, /ʃ/, /b/, /d/, /g/, /v/, /z/, /ʒ/, /m/, /n/, /r/, and /l/, in /a/, /i/ and /u/ contexts (i.e., ‘apa’, ‘iti’, ‘uku’, and so on). Each stimuli was synthesized and thus evaluated in 5 different conditions: 4 times using a pulse train excitation generated using the original pitch for each different number of articulatory parameters (27, 14, 10 and 7), and one time using the artificial template-based excitation signal (corresponding to a constantly voiced sound) with all 27 articulatory parameters. In the following, these 5 conditions are respectively denoted as <italic>Pitch_27</italic>, <italic>Pitch_14</italic>, <italic>Pitch_10</italic>, <italic>Pitch_7</italic> and <italic>FixedPitch_27</italic>. This allowed us to evaluate both the influence of the number of articulatory parameters on the intelligibility, and the effect of using or not using glottal activity information (here, the pitch). Indeed, while in the real-time closed-loop experiment presented here no glottal activity is recorded, this glottal activity could be obtained by decoding the neural activity in future BCI application. An additional evaluation was performed for the two conditions <italic>Pitch_27</italic> and <italic>Pitch_14</italic>, which consisted in directly transcribing 30 sentences (see <xref ref-type="supplementary-material" rid="pcbi.1005119.s001">S1 Appendix</xref> for the list of these sentences). For each listener, half of the sentences were randomly picked from the first condition and the other half from the other, ensuring that each listener never evaluated the same sentence twice, and that all sentences were evaluated in both conditions.</p>
<p>For the real-time closed-loop synthesis, the evaluated stimuli consisted of the 7 vowels /a/, /e/, /i/, /o/, /u/, /œ/, and /y/, and the 21 VCVs made of /b/, /d/, /g/, /l/, /v/, /z/ and /ʒ/, in /a/, /i/ and /u/ contexts. Each listener evaluated 3 repetitions (randomly picked for each listener) of each of these 28 items for each of the 4 new speakers. Remind that for the real-time closed-loop synthesis, the stimuli were generated using only the fixed-pitch template-based excitation.</p>
<p>In total, each listener had thus to identify 626 sounds (10 vowels + 48 VCVs for each of the 5 different offline synthesis conditions, 7 vowels + 21 VCVs, three times for each of the 4 speakers) and transcribe 30 sentences. The sounds were all normalized using automatic gain control, and played in random order at the same sound level through Beyerdynamic DT-770 Pro 80 Ohms headphones, while the listener was seated in a quiet environment. No performance feedback was provided during the test.</p>
<p>For the VCVs and vowels evaluation, participants were instructed to select in a list what they thought was the corresponding vowel in the case of an isolated vowel, or the middle consonant in the case of a VCV sequence. Graphical user interface buttons were randomly shuffled for each subject in order to avoid systematic default choice (e.g., always choosing the left button when unable to identify a sound). The subjects were told that some of the sounds were difficult to identify, and thus to choose the closest sound among the offered possibilities. The recognition accuracy was defined as <italic>Acc = R/N</italic> with <italic>R</italic> the number of correct answers for the <italic>N</italic> presented sounds of the test. Since each item had exactly the same number of repetitions, the chance level was estimated by <italic>Acc</italic><sub><italic>Chance</italic></sub> <italic>= 1/C</italic>, with <italic>C</italic> the number of different item categories. For the offline reference synthesis, the chance level was thus <italic>1/10 = 10%</italic> for vowels, and <italic>1/16≈6%</italic> for VCVs, while for the real-time closed-loop synthesis, the chance level was <italic>1/7≈14%</italic> in both cases.</p>
<p>For the sentences, the subjects were asked to transcribe directly the sentences they were listening to. Results were evaluated using the word accuracy <italic>WAcc = (N—S—D—I)/N</italic> (with <italic>N</italic> the total number of words, <italic>S</italic> the number of word substitutions, <italic>D</italic> the number of deletions and <italic>I</italic> the number of insertions), which is a commonly used metric in the field of automatic speech recognition.</p>
</sec>
<sec id="sec015">
<title>Statistical analysis</title>
<sec id="sec016">
<title>Analysis of the offline reference synthesis</title>
<p>Several listeners had to identify the same synthesized items, resulting in a binary answer (wrong or right), for each item and each listener. Statistical analysis of these results was thus performed using mixed logistic regression. For the VCVs and vowels, the following model was used: <italic>Result ~ (Segment + Condition)^2 + (1 | Listener)</italic>, where <italic>Result</italic> is the binary answer (equals 0 if the item was wrongly identified, otherwise 1), <italic>Segment</italic> has two levels corresponding to the type of item (vowel or VCV), <italic>Condition</italic> has five levels corresponding to the five different conditions (<italic>Pitch_27</italic>, <italic>Pitch_14</italic>, <italic>Pitch_10</italic>, <italic>Pitch_7</italic> and <italic>FixedPitch_27</italic>), and <italic>Listener</italic> has 12 levels corresponding to each listener that participated in the listening test. Multiple comparisons were made using contrasts according to [<xref ref-type="bibr" rid="pcbi.1005119.ref056">56</xref>]. For the sentences, a paired Student test was performed to compare the results. All the tests were made using the R software, and packages <italic>lme4</italic>, <italic>multcomp</italic> and <italic>lsmeans</italic>.</p>
</sec>
<sec id="sec017">
<title>Analysis of the articulatory-to-articulatory mapping</title>
<p>For the articulatory-to-articulatory mapping, mean distance between predicted articulatory trajectories and reference articulatory trajectories was computed for each item of the test corpus and each speaker. A two-factor ANOVA with repeated measures was performed using the following model: <italic>Distance ~ Sensor*RefSpeaker + Error (Item / (Sensor*RefSpeaker))</italic>, where <italic>Distance</italic> is the mean distance between predicted and reference trajectories, <italic>RefSpeaker</italic> has two levels indicating if it was the reference speaker (Speaker 1) or another speaker (Speaker 2, 3 or 4), <italic>Item</italic> corresponds to the identifier of the tested item, and <italic>Sensor</italic> has 7 levels corresponding to the different EMA sensor positions to be predicted (upper lip, lower lip, jaw, tongue tip, tongue dorsum, tongue back and velum). Multiple comparisons were made using contrasts according to [<xref ref-type="bibr" rid="pcbi.1005119.ref056">56</xref>]. All the tests were made using the R software, and packages <italic>lme4</italic>, and <italic>multcomp</italic>.</p>
</sec>
<sec id="sec018">
<title>Analysis of the real-time closed-loop synthesis</title>
<p>As for offline reference synthesis, statistical analysis of the real-time closed-loop synthesis results was performed using mixed logistic regression. The following model was used: <italic>Result ~ (Segment + RefSpeaker) ^ 2 + (1 | Listener)</italic>, where <italic>Result</italic> is the binary answer (equals 0 if the item was wrongly identified, otherwise 1), <italic>Segment</italic> has two levels corresponding to the type of item (vowel or VCV), <italic>RefSpeaker</italic> has two levels indicating if it was the reference speaker (Speaker 1) or another speaker (Speaker 2, 3 or 4), and <italic>Listener</italic> has 12 levels corresponding to each listener that participated in the listening test. Multiple comparisons were made using contrasts according to [<xref ref-type="bibr" rid="pcbi.1005119.ref056">56</xref>]. All the tests were made using the R software, and packages <italic>lme4</italic>, <italic>multcomp</italic> and <italic>lsmeans</italic>.</p>
</sec>
</sec>
</sec>
<sec id="sec019" sec-type="results">
<title>Results</title>
<sec id="sec020">
<title>Intelligibility of the articulatory-based speech synthesizer (offline reference synthesis)</title>
<p>First, we evaluated the proposed DNN-based articulatory synthesizer described in the Methods section. <xref ref-type="fig" rid="pcbi.1005119.g005">Fig 5</xref> shows the spectrogram of the original sound for an example sentence (the sentence was <italic>“Le fermier est parti pour la foire”</italic>, meaning <italic>“The farmer went to the fair”</italic>), together with the 5 different synthesis. Note that there is speech present in the synthesized sample before the actual beginning of the reference sentence, since no assumption can be made on the presence of the air flow when considering only articulatory data. The corresponding synthesized sounds are provided in <xref ref-type="supplementary-material" rid="pcbi.1005119.s002">S1</xref>–<xref ref-type="supplementary-material" rid="pcbi.1005119.s007">S6</xref> Audio Files, further illustrating the good intelligibility of the synthesized sounds when using at least 10 articulatory parameters. Note however that, in the following, the quality of the articulatory-to-acoustic mapping was evaluated subjectively by naive listeners mainly on isolated vowels and VCVs in order to avoid the influence of the linguistic context that tends to over-estimate evaluation results.</p>
<fig id="pcbi.1005119.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Offline reference synthesis example.</title>
<p>Comparison of the spectrograms of the original audio, and the corresponding audio signal produced by the 5 different offline articulatory synthesis for the sentence “Le fermier est parti pour la foire” (“The farmer went to the fair”). Dashed lines show the phonetic segmentation obtained by forced-alignment.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g005" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6</xref> summarizes the result of the subjective listening test. The recognition accuracy was better for vowels than for consonants for <italic>FixedPitch_27</italic>, <italic>Pitch_27</italic> and <italic>Pitch_7</italic> (<italic>P</italic> &lt; 0.01), while this difference was only a trend for <italic>Pitch_14</italic> (<italic>P</italic> = 0.0983) and <italic>Pitch_10</italic> (<italic>P</italic> &gt; 0.99).</p>
<fig id="pcbi.1005119.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Subjective evaluation of the intelligibility of the speech synthesizer (offline reference synthesis).</title>
<p><bold>A</bold>–Recognition accuracy for vowels and consonants for each of the 5 synthesis conditions. The dashed lines show the chance level for vowels (blue) and VCVs (orange). B–Recognition accuracy of the VCVs regarding the vocalic context, for the 5 synthesis conditions. The dashed line shows the chance level. C–Recognition accuracy of the consonant of the VCVs, for the 5 synthesis conditions. Dashed line shows the chance level. See text for statistical comparison results.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g006" xlink:type="simple"/>
</fig>
<p>For vowels, the recognition accuracy was far above chance (chance level = 10%) for all conditions (<italic>P</italic> &lt; 0.01, <xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6A</xref>) and decreasing when decreasing the number of articulatory parameters, ranging from 89% for <italic>Pitch_27</italic> to 61% for <italic>Pitch_7</italic>. Taking <italic>Pitch_27</italic> as reference, this decrease was not found significant for <italic>Pitch_14</italic> (<italic>P</italic> = 0.7116), and significant for <italic>Pitch_10</italic> and <italic>Pitch_7</italic> (<italic>P</italic> &lt; 0.01 in both cases). No statistically significant difference was observed when not using the glottal activity versus when using the glottal activity (<italic>FixedPitch_27</italic> = 87%, <italic>Pitch_27</italic> = 89%, <italic>P</italic> &gt; 0.99).</p>
<p>For the consonants, the recognition accuracy was also far above chance (chance level = 6.25%) for all conditions (<italic>P</italic> &lt; 0.01, <xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6A</xref>). A decrease in recognition accuracy was also observed when decreasing the number of articulatory parameters, ranging from 70% for <italic>Pitch_27</italic> to 42% for <italic>Pitch_7</italic>. However, taking <italic>Pitch_27</italic> as reference, this decrease was not significant for <italic>Pitch 14</italic> (<italic>P</italic> &gt; 0.99) and <italic>Pitch 10</italic> (<italic>P</italic> = 0.6328), and only significant for <italic>Pitch_7</italic> (<italic>P</italic> &lt; 0.01). A significant difference was observed when not using the glottal activity (<italic>FixedPitch_27</italic> vs <italic>Pitch_27</italic>, <italic>P</italic> &lt; 0.01). The differences in recognition accuracy for each condition were studied regarding the vowel of the VCV (<xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6B</xref>) and the consonant (<xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6C</xref>). Overall the intelligibility was higher when the consonant was in /a/ context (/a/ being the most represented phone in the corpus, see <xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3A</xref>) than when in /i/ and /u/ context (<italic>P</italic> &lt; 0.01), and no significant difference was observed between /i/ and /u/ contexts (<italic>P</italic> &gt; 0.99): for instance, for <italic>Pitch_27</italic>, accuracy decreased from 80% for /a/ context, to 63% and 67% for /i/ and /u/ contexts respectively. Regarding consonants (<xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6C</xref>), no clear differences were observed between the three synthesis <italic>Pitch_27</italic>, <italic>Pitch_14</italic> and <italic>Pitch_10</italic> except for /p/, /l/, /d/, /g/ and /ʒ/. Clear differences between these three conditions and <italic>Pitch_7</italic> were observed for consonants /p/, /f/, /b/, /v/, /ʒ/, /m/, /n/, /r/ and /l/. Clear differences were also observed between <italic>FixedPitch_27</italic> and <italic>Pitch_27</italic> for the unvoiced consonants /p/, /t/, /k/, /f/, /s/, and /ʃ/. Conversely, no significant differences between <italic>FixedPitch_27</italic> and <italic>Pitch_27</italic> were found for all the voiced consonants, which includes all the consonants chosen for the real-time closed loop synthesis that does not use the glottal activity (i.e. it is similar to <italic>FixedPitch_27</italic>). All conditions taken together, best results (at least one condition above 90%) were achieved for the fricative consonants /f/, /s/, /ʃ/, /z/, and /ʒ/, the nasal consonants /m/ and /n/, and /l/. Worst results (all conditions below 50%) were achieved for the plosive consonants /p/, /t/, /k/, /b/ and /d/. Note that there is no clear correlation with the number of occurrences of each phone in the training corpus, since for instance the corpus contained few instances of /ʃ/, and a large number of /t/ (<xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3A</xref>).</p>
<p>Analysis of the confusion matrices can enlighten the sources of synthesis errors (<xref ref-type="fig" rid="pcbi.1005119.g007">Fig 7</xref>). Each row <italic>i</italic> of a confusion matrix <italic>M</italic> corresponds to the ground truth phone <italic>p</italic><sub><italic>i</italic></sub>, while column <italic>j</italic> corresponds to the phone <italic>p</italic><sub><italic>j</italic></sub> recognized by the listeners, so that a diagonal value <italic>M</italic><sub><italic>i</italic>,<italic>i</italic></sub> corresponds to the proportion of occurrences of the phone <italic>p</italic><sub><italic>i</italic></sub> that were correctly recognized, and a value <italic>M</italic><sub><italic>i</italic>,<italic>j</italic></sub> outside the diagonal corresponds to the proportion of occurrences of the phone <italic>p</italic><sub><italic>i</italic></sub> that were recognized as the phone <italic>p</italic><sub><italic>j</italic></sub>. The order of the rows and columns of the confusion matrices were automatically sorted in order to emphasize the main confusions by forming high value blocks near the diagonal.</p>
<fig id="pcbi.1005119.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Confusion matrices of the subjective evaluation of the intelligibility of the speech synthesizer (offline reference synthesis).</title>
<p>Confusion matrices for vowels (left) and consonants (right), for each of the three conditions FixedPitch_27, Pitch_27 and Pitch_14. In the matrices, rows correspond to ground truth while columns correspond to user answer. The last column indicates the amount of errors made on each phone. Cells are colored by their values, while text color is for readability only.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g007" xlink:type="simple"/>
</fig>
<p>The confusion matrices of the perceptual listening test for the condition <italic>Pitch_27</italic> (<xref ref-type="fig" rid="pcbi.1005119.g007">Fig 7</xref>, middle row) reflect the global good quality of this synthesis (indicated by the fact that they are near-diagonal matrices). For vowels, six out of the ten vowels were always correctly recognized (/o/, /u/, /a/, /œ/, /y/ and /e/). Main errors come from confusions between /<inline-formula id="pcbi.1005119.e007"><alternatives><graphic id="pcbi.1005119.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mover accent="true"><mml:mi>ɛ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/ and /a/ (67% of / <inline-formula id="pcbi.1005119.e008"><alternatives><graphic id="pcbi.1005119.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mover accent="true"><mml:mi>ɛ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> / were recognized as /a/), and other errors come from confusions between /<inline-formula id="pcbi.1005119.e009"><alternatives><graphic id="pcbi.1005119.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mover accent="true"><mml:mi>ɑ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/ and /a/ (17% of /<inline-formula id="pcbi.1005119.e010"><alternatives><graphic id="pcbi.1005119.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mover accent="true"><mml:mi>ɑ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/ were recognized as /a/), and between /<inline-formula id="pcbi.1005119.e011"><alternatives><graphic id="pcbi.1005119.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mover accent="true"><mml:mi>ɔ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/ and /œ/ (17% of /<inline-formula id="pcbi.1005119.e012"><alternatives><graphic id="pcbi.1005119.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mover accent="true"><mml:mi>ɔ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/ were recognized as /œ/. For consonants, main confusions came from /b/ being recognized as /v/ (75%), /d/ being recognized as /z/ (58%), /p/ being recognized as /f/ (56%) and /d/ being recognized as /z/ (58%). Other more minor errors come from /g/ being recognized as /v/ (11%), and /k/ being recognized as /r/ (19%) and /t/ (19%).</p>
<p>By comparing confusion matrices of <italic>Pitch_27</italic> with those of <italic>FixedPitch_27</italic>, we can observe that not using the glottal activity resulted in increased confusions mainly for the vowel /<inline-formula id="pcbi.1005119.e013"><alternatives><graphic id="pcbi.1005119.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005119.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mover accent="true"><mml:mi>ɑ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>/ (accuracy going from 83% for <italic>Pitch_27</italic> to 58% for <italic>FixedPitch_27</italic>) while no clear difference can be observed for the other vowels. Note that between the two conditions <italic>Pitch_27</italic> and <italic>FixedPitch_27</italic>, the articulatory-to-acoustic model remains the same, the only change being the excitation signal that is used for the final synthesis with the MLSA filter. Importantly, for the consonants, not using the glottal activity resulted in a drastic decrease in the recognition accuracy of all the unvoiced consonants /p/, /t/, /k/, /f/, /s/ and /ʃ/, while all the voiced consonants remained recognized with similar accuracy. Indeed, /p/ was mainly recognized as /v/ (72%), /t/ as /z/ (58%), /f/ as /v/ (64%), /s/ as /z/ (86%), and /ʃ/ as /ʒ/ (86%). Note that /v/ is the voiced counterpart of /f/, /z/ of /s/ and /ʒ/ of /ʃ/. Hence, the use of the template-based excitation naturally leads to a predictable shift of the unvoiced consonants to their more or less corresponding (in terms of place of articulation) voiced counterparts.</p>
<p>By comparing the confusion matrices of <italic>Pitch_27</italic> with those of <italic>Pitch_14</italic>, we can observe that there is no clear pattern of increased confusions. This confirms the results previously obtained from <xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6</xref>, where no significant differences between <italic>Pitch_27</italic> and <italic>Pitch_14</italic> were found for both vowels and consonants.</p>
<p>Finally, the results of the subjective evaluation on sentences are presented in <xref ref-type="fig" rid="pcbi.1005119.g008">Fig 8</xref>. While the recognition accuracy for <italic>Pitch_27</italic> and <italic>Pitch_14</italic> was below 90% for vowels and below 70% for consonants, the word recognition accuracy for the sentences is above 90% for both conditions (96% for <italic>Pitch_27</italic> and 92% for <italic>Pitch_14</italic>). Note that the difference in recognition accuracy for <italic>Pitch_27</italic> and for <italic>Pitch_14</italic> is here significant (<italic>P</italic> = 0.015).</p>
<fig id="pcbi.1005119.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Subjective evaluation of the intelligibility of the speech synthesizer on sentences (offline reference synthesis).</title>
<p>Word recognition accuracy for the sentences, for both conditions Pitch_27 and Pitch_14.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec021">
<title>Control of the articulatory-based speech synthesizer (real-time closed-loop synthesis)</title>
<sec id="sec022">
<title>Accuracy of the articulatory-to-articulatory mapping</title>
<p><xref ref-type="fig" rid="pcbi.1005119.g009">Fig 9A</xref> shows an example of articulatory data recorded from a new speaker (for instance Speaker 2), with the corresponding reference audio signal that the speaker was presented and asked to silently repeat synchronously (in this example, the sentence was <italic>“Deux jolis boubous”</italic>, meaning <italic>“two beautiful booboos”</italic>, which was not part of the training set). <xref ref-type="fig" rid="pcbi.1005119.g009">Fig 9B</xref> shows the transformation of these signals after their articulatory-to-articulatory mapping onto the reference speaker’s articulatory space. One can clearly see that articulatory movements of the new speaker were originally quite different than those of the reference speaker; and that they became similar once the articulatory-to-articulatory mapping was performed.</p>
<fig id="pcbi.1005119.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Articulatory-to-articulatory mapping.</title>
<p>A–Articulatory data recorded on a new speaker (Speaker 2) and corresponding reference audio signal for the sentence “Deux jolis boubous” (“Two beautiful booboos”). For each sensor, the X (rostro-caudal), Y (ventro-dorsal) and Z (left-right) coordinates are plotted. Dashed lines show the phonetic segmentation of the reference audio, which the new speaker was ask to silently repeat in synchrony. B–Reference articulatory data (dashed line), and articulatory data of Speaker 2 after articulatory-to-articulatory linear mapping (predicted, plain line) for the same sentence as in A. Note that X,Y,Z data were mapped onto X,Y positions on the midsagittal plane. C–Mean Euclidean distance between reference and predicted sensor position in the reference midsagittal plane for each speaker and each sensor, averaged over the duration of all speech sounds of the calibration corpus. Error bars show the standard deviations, and “All” refer to mean distance error when pooling all the sensors together.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g009" xlink:type="simple"/>
</fig>
<p>We further quantified the quality of the articulatory-to-articulatory mapping. Since articulatory data consist of geometrical coordinates, the mean Euclidean distance between predicted and true positions could be estimated for each sensor and for each speaker (<xref ref-type="fig" rid="pcbi.1005119.g009">Fig 9C</xref>). The average error across all sensors and speakers was 2.5 mm ± 1.5 mm. Errors were significantly higher for tongue sensors than for non-tongue sensors (<italic>P</italic> &lt; 0.005 for 22 out of 24 pairwise comparisons corrected for multiple comparisons–see <xref ref-type="sec" rid="sec002">Methods</xref>), and lower for the velum sensor than for the non-velum sensors (<italic>P</italic> &lt; 0.001 for 10 out of 12 pairwise comparisons corrected for multiple comparisons–see <xref ref-type="sec" rid="sec002">Methods</xref>). This is consistent with the fact that the tongue and velum are the articulators for which movement amplitudes were the highest and lowest, respectively (see <xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3B</xref>). Mean distances for the reference speaker (Speaker 1) were systematically lower than for other speakers for all sensors except the velum. These differences were statistically significant for the tongue tip (<italic>P</italic> = 0.00229) and the tongue dorsum (<italic>P</italic> = 0.03051).</p>
</sec>
<sec id="sec023">
<title>Intelligibility of the real-time closed-loop speech synthesis</title>
<p>During real-time control, the speakers were asked to reproduce a specific set of test sounds (see <xref ref-type="sec" rid="sec002">Materials and Methods</xref>). The remaining time of the experiment was kept for other tasks, including spontaneous conversations. <xref ref-type="supplementary-material" rid="pcbi.1005119.s008">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005119.s009">S2</xref> Video Files illustrate the closed-loop experiment, with Speaker 1 (reference speaker) and Speaker 2 (new speaker), respectively. <xref ref-type="fig" rid="pcbi.1005119.g010">Fig 10</xref> shows examples of spectrograms of vowels and VCVs obtained during a session of real-time control (Speaker 2, first occurrence of each sound), compared with the corresponding spectrograms of anasynth and reference offline synthesis sounds. In general, we found that the spectrograms for the three conditions presented very similar characteristics, although some differences did exist in their fine structure, especially for consonants. For instance, the real-time examples of the plosive consonants /b/, /d/ and /g/ showed more energy smearing from vocalic to consonant segments as compared to the anasynth and offline synthesized versions. Also, the real-time example of /ʒ/ had characteristics closer to the anasynth version of /l/ than to the anasynth version of /ʒ/ (<xref ref-type="fig" rid="pcbi.1005119.g010">Fig 10B</xref>).</p>
<fig id="pcbi.1005119.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Real-time closed loop synthesis examples.</title>
<p>Examples of audio spectrograms for anasynth, reference offline synthesis and real-time closed-loop (Speaker 2), A) for the vowels /a/, /e/, /i/, /o/, /u/, /œ/ and /y/, and B) for the consonants /b/, /d/, /g/, /l/, /v/, /z/ and /ʒ/ in /a/ context. The thick black line under the spectrograms corresponds to 100 ms.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g010" xlink:type="simple"/>
</fig>
<p>The test sounds produced in the closed-loop experiment were recorded and then their intelligibility was evaluated in the same way as for the offline synthesis intelligibility evaluation, i.e. a subjective intelligibility test performed by 12 listeners. <xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11</xref> summarizes the result of this listening test. The speech sounds produced by all 4 speakers obtained high vowel accuracy (93% for Speaker 1, 76% for Speaker 2, 85% for Speaker 3, and 88% for Speaker 4, leading to a mean accuracy score of 86%), and reasonable consonant accuracy (52% for Speaker 1, 49% for Speaker 2, 48% for Speaker 3, and 48% for Speaker 4, leading to a mean accuracy score of 49%). These scores were far above chance level (chance = 14%, <italic>P</italic> &lt; 0.001) for both vowels and consonants. For all speakers, the 48–52% VCVs accuracy obtained during real-time control is to be compared to the 61% score obtained for the same VCVs in the offline reference synthesis. The difference is significant (<italic>P</italic> = 0.020 for reference speaker and <italic>P</italic> &lt; 0.001 for other speakers, compare <xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11A</xref> and <xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6A</xref>) but the decrease is quite limited when considering that the speaker is no longer the reference speaker and that the synthesis is performed in an online closed-loop condition. The same observation applies to the vowel identification results: The 76–93% vowel accuracy for the closed-loop online synthesis is also found significantly lower than the 99% accuracy score obtained for the same vowels in the offline synthesis (<italic>P</italic> &lt; 0.001 for reference and other speakers), but the decrease is relatively limited. The recognition accuracy for vowels was significantly higher for the reference speaker (<italic>P</italic> = 0.002) but no significant difference between the reference speaker and the other speakers was found for the VCVs (<italic>P</italic> = 0.262), even if the reference speaker obtained the highest average accuracy value for VCVs.</p>
<fig id="pcbi.1005119.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005119.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Results of the subjective listening test for real-time articulatory synthesis.</title>
<p>A–Recognition accuracy for vowels and consonants, for each subject. The grey dashed line shows the chance level, while the blue and orange dashed lines show the corresponding recognition accuracy for the offline articulatory synthesis, for vowels and consonants respectively (on the same subsets of phones). B–Recognition accuracy for the VCVs regarding the vowel context, for each subject. C–Recognition accuracy for the VCVs, by consonant and for each subject. D–Confusion matrices for vowels (left) and consonants from VCVs in /a/ context (right). Rows correspond to ground truth while columns correspond to user answer. The last column indicates the amount of errors made on each phone. Cells are colored by their values, while text color is for readability only.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.g011" xlink:type="simple"/>
</fig>
<p>Regarding the vocalic context (<xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11B</xref>), VCVs in /a/ context had better recognition accuracy than those in /i/ context (<italic>P</italic> &lt; 0.001) and /u/ (<italic>P</italic> &lt; 0.001) for all subjects, which is consistent with results from the offline reference synthesis. VCVs in /u/ context were found to have a better recognition accuracy than those in /i/ context (<italic>P</italic> = 0.009). Regarding the VCVs (<xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11C</xref>), the recognition accuracy varied largely across consonants, ranging from an average of 21% for /b/ to 85% for /ʒ/. It was generally lower for the plosive consonants /b/, /d/ and /g/, which is consistent with results from the offline reference synthesis, while the accuracy on the remaining consonants was different for each subject. For instance, Subjects 1, 2 and 4 had good accuracy on /v/ while Subject 4 had a much lower accuracy. Similar result can be observed for /z/ and /ʒ/ for different subjects.</p>
<p>Confusion matrices for both vowels and consonants are shown in <xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11D</xref>. These confusion matrices present features that are similar to the confusion matrices obtained for offline articulatory synthesis (<xref ref-type="fig" rid="pcbi.1005119.g007">Fig 7</xref>), and reflect well the results quality. All vowels show a recognition accuracy above 80%, and the highest accuracy was obtained for /y/, with 90%. The majority of the confusions are between /e/ and /i/ (17% of /e/ were recognized as /i/, and 16% of /i/ as /e/). Secondary confusions are between /o/ and /u/ (11% of /u/ were recognized as /o/, and 8% of /o/ as /u/), between /y/ and /œ/ (10% of /y/ were recognized as /œ/), and between /a/ and /œ/ (9% of /a/ were recognized as /œ/). The confusion matrix for consonant roughly corresponds to the confusion matrix obtained for offline articulatory synthesis, with emphasized confusions. Thus, the main confusions occurred again for plosive consonants /b/ (57% of /b/ were recognized as /v/) and /d/ (54% of /d/ were recognized as /z/), while quite few errors were made on /ʒ/ (85% of accuracy). Some errors were also made on /g/ but with less systematic confusion (26% with /v/, 13% with /ʒ/, and 10% with /z/). However, new confusions appeared that explain the significant drop in consonants accuracy with respect to offline articulatory synthesis: between /ʒ/ and /l/ (10% of /ʒ/ were recognized as /l/), and between /z/ and /l/ (19% of /z/ were recognized as /l/).</p>
</sec>
</sec>
</sec>
<sec id="sec024" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper we first presented an articulatory-based speech synthesizer built from articulatory-acoustic data from one reference speaker using deep neural networks, and then showed that this synthesizer could be controlled in real-time closed-loop situation by several speakers using motion capture data (electromagnetic articulography) as input parameters. Experiments included the same reference speaker in a different session, as well as other speakers. All speakers were silently articulating and were given the synthesized acoustic feedback through headphones. A calibration method was used to take into account articulatory differences across speakers (and across sessions for the reference speaker), such as sensor positioning and ways of articulating the different sounds. Subjective listening tests were conducted to assess the quality of the synthesizer and in particular its performance during real-time closed-loop control by new speakers.</p>
<p>We first assessed the intelligibility of the synthesizer itself. Several versions of the synthesizer were built to assess the effect of the number of articulatory parameters (27, 14, 10 and 7), and the effect of using or not using glottal activity (by comparing synthesis using a constant artificial pitch, and the original pitch). The phone recognition accuracy for offline reference synthesis was far above chance level for all five tested parameterizations, and fully intelligible speech sentences could be produced (see <xref ref-type="fig" rid="pcbi.1005119.g008">Fig 8</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005119.s002">S1</xref>–<xref ref-type="supplementary-material" rid="pcbi.1005119.s006">S5</xref> Audio Files). Most errors on vowels were made between vowels that had close articulatory positions (e.g. /i/ and /e/, see <xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3B</xref>). Regarding consonants, most errors were made on the plosive consonants, and main confusions were observed within pairs of consonants corresponding to relatively similar articulatory movements in terms of place of articulation: for instance, /b/ is a labial consonant and /v/ is a labio-dental, and /d/ is a dental or an alveolar and /z/ is an alveolar. For /b/-/v/ confusion, this could be explained by a positioning of the EMA coils too far from the lip edges, resulting in a tracking of the lips by the EMA system that did not allow to capture sharp differences between /b/ and /v/ lip movements. A similar interpretation can be given for /d/-/z/ confusions, since in practice the coil had to be attached more than 5 mm back from the tongue tip (see <xref ref-type="fig" rid="pcbi.1005119.g002">Fig 2A</xref>). Moreover, results showed that the accuracy on the VCVs was correlated to the vocalic context, with consonant in /a/ context having a better recognition accuracy. This could be explained by the fact that the phone /a/ is more largely present in the training corpus than the phones /i/ and /u/ (see <xref ref-type="fig" rid="pcbi.1005119.g003">Fig 3A</xref>). However, this is not consistent with the fact that some phones that are less represented in the corpus, like /ʃ/, have high recognition accuracy, while other phone that are largely represented, like /d/, have low recognition accuracy. Another possible explanation is that /a/ is the most opened vowel and thus VCVs in /a/ context are performed by movements of higher amplitude, which could be more discriminant. By removing the glottal activity information (here by using a constant pitch), we found that the recognition accuracy significantly decreased for all unvoiced consonants, while remaining roughly the same for all voiced consonants and vowels (see <xref ref-type="fig" rid="pcbi.1005119.g006">Fig 6</xref> and top and middle rows of <xref ref-type="fig" rid="pcbi.1005119.g007">Fig 7</xref>). The unvoiced consonants were thus confused with their voiced counterpart (e.g. /ʃ/ with /ʒ/), or with the voiced counterpart of the consonant they were already confused with (for instance, /p/ was originally confused with /s/ in the <italic>Pitch_27</italic> condition and was then confused with /ʒ/ when using a constant pitch, in the <italic>FixedPitch_27</italic> condition). This supports the choice we made to keep only 7 consonants for the real-time closed-loop synthesis since no glottal activity was available in silent speech condition. It should be noted that the quality of the synthesis could still be improved by estimating an optimal MLSA excitation source using inverse glottal filtering and that it could be envisioned that the parameters of such supplementary signal be predicted from brain signals in a BCI situation.</p>
<p>Regarding the number of articulatory parameters, results showed that using 14 articulatory parameters yields intelligibility scores that are close to the best scores achieved with 27 parameters. This supports the choice that was made to use a synthesizer with 14 articulatory parameters for the real-time closed-loop synthesis. Interestingly, using 10 parameters did not significantly impact the intelligibility of consonants, but started to affect that of vowels, although the accuracy remained at the high level of 67%. Decreasing further the number of parameters down to 7, significantly impacted the intelligibility of both vowels and consonants. Finally, although the accuracy on consonants was inferior to 70% for 27 and 14 articulatory parameters, this was enough to produce very intelligible sentences, with word recognition accuracy superior to 90% (see <xref ref-type="fig" rid="pcbi.1005119.g009">Fig 9</xref>). This can be explained by the fact that most confusions were made with similar consonants, thus ensuring a good intelligibility when constrained with closed vocabulary and syntactic rules. Thus, overall, the number of parameters required to achieve a sufficient intelligibility is of the order of the 10 degrees of freedoms that could be controlled successfully in recent state of the art BCI experiments (Wodlinger et al. 2015). It should be noted that the reduction in the number of parameters was done here in a drastic way either by dropping parameters or by PCA, while more efficient dimensionality reduction techniques could be envisioned such as autoencoders that we previously started to investigate [<xref ref-type="bibr" rid="pcbi.1005119.ref035">35</xref>].</p>
<p>Next, we assessed the intelligibility of the real-time closed-loop synthesis. In this case, the phone recognition accuracy was again far above chance level, both for vowels and consonants (<xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11</xref>). Interestingly, this good intelligibility was obtained despite significant trajectory errors made on input control parameters obtained by the articulatory-to-articulatory mapping (about 2.5 mm on average, see <xref ref-type="fig" rid="pcbi.1005119.g009">Fig 9B</xref>). This confirms our previous results indicating that DNN-based articulatory synthesis is robust to fluctuations of the input parameters [<xref ref-type="bibr" rid="pcbi.1005119.ref035">35</xref>]. As expected, the closed-loop synthesis intelligibility was lower than for the reference offline synthesis. However, it was relatively limited. Confusions were similarly distributed in both cases, indicating that using the synthesizer in a closed-loop paradigm mainly emphasized the already existing confusions. The fact that most errors were consistent between offline and closed-loop synthesis suggests that real-time closed-loop articulatory synthesis could still benefit from improving the articulatory-to-acoustic mapping. This could be achieved by efficiently detecting specific constrictions from the articulatory data in order to improve the synthesis of plosive consonants, which are the major source of errors. The presence of additional minor confusions suggests that other aspects might also be improved, such as the articulatory-to-articulatory mapping with a better calibration approach. Indeed, to remain in a situation as close as possible to future BCI paradigms with aphasic participants, the articulatory-to-articulatory calibration step was performed under a silent speech condition. This was also consistent with the fact that the closed-loop condition was also performed in a silent speech condition so that the speaker received only the synthesized feedback, not superimposed on his/her own produced speech. Thus the articulatory-to-articulatory mapping converted articulatory trajectories recorded under a silent speech condition (for each speaker) into articulatory trajectories recorded under overt speech condition (of the reference speaker). Previous studies have shown that articulatory movements differ between silent and overt speech, and especially that silent speakers tend to hypo-articulate [<xref ref-type="bibr" rid="pcbi.1005119.ref057">57</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref058">58</xref>]. Such phenomenon may thus leads to smaller discrimination of articulatory trajectories during silent speech.</p>
<p>Improving the articulatory-to-articulatory and the articulatory-to-acoustic mappings might however not be the sole possibility to improve the intelligibility of closed-loop speech synthesis. Indeed, while results from the evaluation of the articulatory-to-articulatory mapping showed that for most sensors the mean prediction error was lower for Speaker 1 (the reference speaker), the results obtained during the real-time experiment showed that other speakers could achieve a control of the articulatory synthesizer similar to Speaker 1, in particular for consonants (see <xref ref-type="fig" rid="pcbi.1005119.g011">Fig 11A</xref>). For example, episodes of spontaneous conversation could be achieved not only with Speaker 1 but also with Speaker 2 (see <xref ref-type="supplementary-material" rid="pcbi.1005119.s008">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005119.s009">S2</xref> Video Files). This suggests that other factors come into play for the control of the synthesizer. One possibility is that subjects may adapt differently to the articulatory-to-articulatory mapping errors and find behavioral strategies to compensate for these errors. Here, each subject had about 20 minutes of free closed-loop control of the synthesizer between the two productions of test items, but we could not see any significant improvement over this short period of time. Finding behavioral strategies might thus need a more significant amount of training time. Finally, and according to the results from the offline reference synthesis, all unvoiced consonants were excluded since no glottal activity can be recorded in silent speech condition. In a BCI application for speech rehabilitation, such glottal activity could be predicted from the neural activity, thus allowing the synthesis of all the French phones.</p>
<p>To our knowledge these results are the first indication that an intelligible articulatory-based speech synthesizer can be controlled in real-time by different speakers to produce not only vowels, but also intelligible consonants and some sentences (some spontaneous conversations, while not reported here, could be achieved with 2 of the 4 subjects using only the synthesized audio i.e. the interlocutor could not see the subject articulating). These results thus go beyond previous preliminary achievements of speech synthesis from EMA data where discrete sentences could be successfully classified in a closed vocabulary context with training and testing performed in the same subjects [<xref ref-type="bibr" rid="pcbi.1005119.ref059">59</xref>]. Indeed, here the speech synthesis was performed in real time on a frame-by-frame basis to provide an online audio feedback delivered in real time with a very short time delay (less than 30 ms). Moreover, we showed here that a synthesizer built from a reference speaker data in an overt speech condition could be controlled to produce free speech in real time in a silent speech condition by other speakers with a different vocal tract anatomy and a different articulatory strategy using a simple linear calibration stage. This result is of particular interest for the emerging research field on ‘silent speech interfaces’, which are lightweight devices able to capture silent articulation using non-invasive sensors and convert it into audible speech [<xref ref-type="bibr" rid="pcbi.1005119.ref060">60</xref>–<xref ref-type="bibr" rid="pcbi.1005119.ref063">63</xref>]. Indeed, although the presented EMA-based interface is not strictly a silent-speech interface, the present results indicate that it is possible to synthesize intelligible speech in real time from articulatory data acquired in silent speech condition. Further studies could extend these results using less invasive techniques to obtain articulatory signals, such as EMG [<xref ref-type="bibr" rid="pcbi.1005119.ref061">61</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref062">62</xref>] and/or ultrasound signals [<xref ref-type="bibr" rid="pcbi.1005119.ref063">63</xref>,<xref ref-type="bibr" rid="pcbi.1005119.ref064">64</xref>].</p>
<p>Finally, this study is also a first step toward future speech BCI applications. Here we indeed showed that closed-loop speech synthesis was possible by subjects that had different speech production constrains (e.g., different anatomy of the vocal tract, different manner of articulation) than those of the reference speaker from whom the speech synthesizer was built. This means that differences in anatomical constrains could be compensated by the articulatory-to-articulatory mapping. In the context of a speech BCI paradigm, a similar situation will be encountered, where the synthesizer will be built from a subject different that the BCI participants. In this case, the question will be whether differences in neuronal constrains between individuals can also be compensated by a proper neural signal decoding strategy. Here, the DNN-based mapping approach was robust to trajectory errors of several millimeters that were present in the input signals of the synthesizer resulting from imperfections in the articulatory-to-articulatory mapping. This is encouraging given that decoding neural signal into input signals of the synthesizer will also be imperfect, and suggests that an articulatory-based speech synthesizer such as the one developed and tested here is a good candidate for being used in a speech BCI paradigm. The choice we made here to envision articulatory parameters as an intermediate representation for decoding speech from neural activity recorded from the speech motor cortex. This hypothesis will need to be tested in future BCI experiment and compared to a direct decoding of cortical activity into acoustic speech parameters.</p>
</sec>
<sec id="sec025">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005119.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s001" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>List of the sentences used for the subjective evaluation of the synthesizer.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s002" xlink:type="simple">
<label>S1 Audio File</label>
<caption>
<title>Example of original audio from the reference speaker.</title>
<p>The corresponding sentence is <italic>“Le fermier est parti pour la foire”</italic> (<italic>“The farmer went to the fair”</italic>).</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s003" xlink:type="simple">
<label>S2 Audio File</label>
<caption>
<title>Example of a sentence synthesized from the reference speaker data (reference offline synthesis) for the <italic>FixedPitch_27</italic> condition (fixed-pitch template-based excitation signal and 27 articulatory parameters).</title>
<p>The corresponding sentence is <italic>“Le fermier est parti pour la foire”</italic> (<italic>“The farmer went to the fair”</italic>).</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s004" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s004" xlink:type="simple">
<label>S3 Audio File</label>
<caption>
<title>Example of a sentence synthesized from the reference speaker data (reference offline synthesis) for the <italic>Pitch_27</italic> condition (excitation signal generated using the pitch extracted from the original audio and 27 articulatory parameters).</title>
<p>The corresponding sentence is <italic>“Le fermier est parti pour la foire”</italic> (<italic>“The farmer went to the fair”</italic>).</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s005" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s005" xlink:type="simple">
<label>S4 Audio File</label>
<caption>
<title>Example of a sentence synthesized from the reference speaker data (reference offline synthesis) for the <italic>Pitch_14</italic> condition (excitation signal generated using the pitch extracted from the original audio and 14 articulatory parameters).</title>
<p>The corresponding sentence is <italic>“Le fermier est parti pour la foire”</italic> (<italic>“The farmer went to the fair”</italic>).</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s006" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s006" xlink:type="simple">
<label>S5 Audio File</label>
<caption>
<title>Example of a sentence synthesized from the reference speaker data (reference offline synthesis) for the <italic>Pitch_10</italic> condition (excitation signal generated using the pitch extracted from the original audio and 10 articulatory parameters).</title>
<p>The corresponding sentence is <italic>“Le fermier est parti pour la foire”</italic> (<italic>“The farmer went to the fair”</italic>).</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s007" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s007" xlink:type="simple">
<label>S6 Audio File</label>
<caption>
<title>Example of a sentence synthesized from the reference speaker data (reference offline synthesis) for the <italic>Pitch_7</italic> condition (excitation signal generated using the pitch extracted from the original audio and 7 articulatory parameters).</title>
<p>The corresponding sentence is <italic>“Le fermier est parti pour la foire”</italic> (<italic>“The farmer went to the fair”</italic>).</p>
<p>(WAV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s008" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s008" xlink:type="simple">
<label>S1 Video File</label>
<caption>
<title>Example of spontaneous conversation during the real-time closed-loop control of the synthesizer by the reference speaker (Speaker 1).</title>
<p>The corresponding sentence is <italic>“Je ne t’entends pas”</italic> (<italic>“I cannot hear you”</italic>).</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005119.s009" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005119.s009" xlink:type="simple">
<label>S2 Video File</label>
<caption>
<title>Example of spontaneous conversation during the real-time closed-loop control of the synthesizer by a new speaker (Speaker 2).</title>
<p>The corresponding sentence is <italic>“Je vais être papa</italic>. <italic>C’est une bonne occasion de vous l’annoncer</italic>. <italic>Je suis très content</italic>.<italic>”</italic> (<italic>“I am going to be a father</italic>. <italic>It is a good opportunity to tell you this</italic>. <italic>I am very happy</italic>.<italic>”</italic>).</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors wish to thank Silvain Gerber for his help in the statistical analysis of the results.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005119.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Birbaumer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ghanayim</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hinterberger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Iversen</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kotchoubey</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kubler</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>A spelling device for the paralysed</article-title>. <source>Nature</source>. <year>1999</year>;<volume>398</volume>: <fpage>297</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/18581" xlink:type="simple">10.1038/18581</ext-link></comment> <object-id pub-id-type="pmid">10192330</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chapin</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Moxon</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Markowitz</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name>. <article-title>Real-time control of a robot arm using simultaneously recorded neurons in the motor cortex</article-title>. <source>Nat Neurosci</source>. <year>1999</year>;<volume>2</volume>: <fpage>664</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/10223" xlink:type="simple">10.1038/10223</ext-link></comment> <object-id pub-id-type="pmid">10404201</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wessberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stambaugh</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Kralik</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Laubach</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chapin</surname> <given-names>JK</given-names></name>, <etal>et al</etal>. <article-title>Real-time prediction of hand trajectory by ensembles of cortical neurons in primates</article-title>. <source>Nature</source>. <year>2000</year>;<volume>408</volume>: <fpage>361</fpage>–<lpage>365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35042582" xlink:type="simple">10.1038/35042582</ext-link></comment> <object-id pub-id-type="pmid">11099043</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serruya</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Hatsopoulos</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fellows</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>. <article-title>Brain-machine interface: Instant neural control of a movement signal</article-title>. <source>Nature</source>. <year>2002</year>;<volume>416</volume>: <fpage>141</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/416141a" xlink:type="simple">10.1038/416141a</ext-link></comment> <object-id pub-id-type="pmid">11894084</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochberg</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Serruya</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Friehs</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Mukand</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Saleh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Caplan</surname> <given-names>AH</given-names></name>, <etal>et al</etal>. <article-title>Neuronal ensemble control of prosthetic devices by a human with tetraplegia</article-title>. <source>Nature</source>. <year>2006</year>;<volume>442</volume>: <fpage>164</fpage>–<lpage>171</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04970" xlink:type="simple">10.1038/nature04970</ext-link></comment> <object-id pub-id-type="pmid">16838014</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Velliste</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Spalding</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Whitford</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>. <article-title>Cortical control of a prosthetic arm for self-feeding</article-title>. <source>Nature</source>. Nature Publishing Group; <year>2008</year>;<volume>453</volume>: <fpage>1098</fpage>–<lpage>1101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature06996" xlink:type="simple">10.1038/nature06996</ext-link></comment> <object-id pub-id-type="pmid">18509337</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochberg</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bacher</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jarosiewicz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Masse</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Simeral</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vogel</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Reach and grasp by people with tetraplegia using a neurally controlled robotic arm</article-title>. <source>Nature</source>. Nature Publishing Group; <year>2012</year>;<volume>485</volume>: <fpage>372</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11076" xlink:type="simple">10.1038/nature11076</ext-link></comment> <object-id pub-id-type="pmid">22596161</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilja</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Chestek</surname> <given-names>C a</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Fan</surname> <given-names>JM</given-names></name>, <etal>et al</etal>. <article-title>A high-performance neural prosthesis enabled by control algorithm design</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>1752</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3265" xlink:type="simple">10.1038/nn.3265</ext-link></comment> <object-id pub-id-type="pmid">23160043</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collinger</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Wodlinger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Downey</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Weber</surname> <given-names>DJ</given-names></name>, <etal>et al</etal>. <article-title>High-performance neuroprosthetic control by an individual with tetraplegia</article-title>. <source>Lancet</source>. Elsevier Ltd; <year>2013</year>;<volume>381</volume>: <fpage>557</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0140-6736(12)61816-9" xlink:type="simple">10.1016/S0140-6736(12)61816-9</ext-link></comment> <object-id pub-id-type="pmid">23253623</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donchin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Spencer</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Wijesinghe</surname> <given-names>R</given-names></name>. <article-title>The mental prosthesis: Assessing the speed of a P300-based brain- computer interface</article-title>. <source>IEEE Trans Rehabil Eng</source>. <year>2000</year>;<volume>8</volume>: <fpage>174</fpage>–<lpage>179</lpage>. <object-id pub-id-type="pmid">10896179</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brumberg</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>. <article-title>Development of speech prostheses: current status and recent advances</article-title>. <source>Expert Rev Med Devices</source>. <year>2010</year>;<volume>7</volume>: <fpage>667</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1586/erd.10.34" xlink:type="simple">10.1586/erd.10.34</ext-link></comment> <object-id pub-id-type="pmid">20822389</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>, <name name-style="western"><surname>Brumberg</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Nieto-Castanon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tourville</surname> <given-names>J a</given-names></name>, <name name-style="western"><surname>Panko</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A wireless brain-machine interface for real-time speech synthesis</article-title>. <source>PLoS One</source>. <year>2009</year>;<volume>4</volume>: <fpage>e8218</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0008218" xlink:type="simple">10.1371/journal.pone.0008218</ext-link></comment> <object-id pub-id-type="pmid">20011034</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref013"><label>13</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Pasley</surname> <given-names>BN</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>RT</given-names></name>. <chapter-title>Decoding speech for understanding and treating aphasia</chapter-title>. <edition>1st ed.</edition> <source>Progress in brain research</source>. <publisher-name>Elsevier B.V.</publisher-name>; <year>2013</year>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chan</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Dykstra</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Jayaram</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Leonard</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Travis</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Gygi</surname> <given-names>B</given-names></name>, <etal>et al</etal>. <article-title>Speech-Specific Tuning of Neurons in Human Superior Temporal Gyrus</article-title>. <source>Cereb cortex</source>. <year>2013</year>;<volume>10</volume>: <fpage>2679</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kellis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Thomson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>House</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Greger</surname> <given-names>B</given-names></name>. <article-title>Decoding spoken words using local field potentials recorded from the cortical surface</article-title>. <source>J Neural Eng</source>. <year>2010</year>;<volume>7</volume>: <fpage>056007</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1741-2560/7/5/056007" xlink:type="simple">10.1088/1741-2560/7/5/056007</ext-link></comment> <object-id pub-id-type="pmid">20811093</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pei</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Barbour</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Leuthardt</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>G</given-names></name>. <article-title>Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans</article-title>. <source>J Neural Eng</source>. <year>2011</year>;<volume>8</volume>: <fpage>046028</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1741-2560/8/4/046028" xlink:type="simple">10.1088/1741-2560/8/4/046028</ext-link></comment> <object-id pub-id-type="pmid">21750369</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tankus</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Shoham</surname> <given-names>S</given-names></name>. <article-title>Structured neuronal encoding and decoding of human speech features</article-title>. <source>Nat Commun</source>. Nature Publishing Group; <year>2012</year>;<volume>3</volume>: <fpage>1015</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ncomms1995" xlink:type="simple">10.1038/ncomms1995</ext-link></comment> <object-id pub-id-type="pmid">22910361</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martin</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Brunner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Holdgraf</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Heinze</surname> <given-names>H-J</given-names></name>, <name name-style="western"><surname>Crone</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Rieger</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Decoding spectrotemporal features of overt and covert speech from the human cortex</article-title>. <source>Front Neuroeng</source>. <year>2014</year>;<volume>7</volume>: <fpage>14</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fneng.2014.00014" xlink:type="simple">10.3389/fneng.2014.00014</ext-link></comment> <object-id pub-id-type="pmid">24904404</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mugler</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Patton</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Flint</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>Z a</given-names></name>, <name name-style="western"><surname>Schuele</surname> <given-names>SU</given-names></name>, <name name-style="western"><surname>Rosenow</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Direct classification of all American English phonemes using signals from functional speech motor cortex</article-title>. <source>J Neural Eng</source>. <year>2014</year>;<volume>11</volume>: <fpage>035015</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1741-2560/11/3/035015" xlink:type="simple">10.1088/1741-2560/11/3/035015</ext-link></comment> <object-id pub-id-type="pmid">24836588</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herff</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Heger</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>de Pesters</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Telaar</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brunner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schalk</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Brain-to-text: decoding spoken phrases from phone representations in the brain</article-title>. <source>Front Neurosci</source>. <year>2015</year>;<volume>9</volume>: <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bouchard</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Mesgarani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>Functional organization of human sensorimotor cortex for speech articulation</article-title>. <source>Nature</source>. Nature Publishing Group; <year>2013</year>;<volume>495</volume>: <fpage>327</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11911" xlink:type="simple">10.1038/nature11911</ext-link></comment> <object-id pub-id-type="pmid">23426266</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickok</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>The cortical organization of speech processing</article-title>. <source>Nat Rev Neurosci</source>. <year>2007</year>;<volume>8</volume>: <fpage>393</fpage>–<lpage>402</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2113" xlink:type="simple">10.1038/nrn2113</ext-link></comment> <object-id pub-id-type="pmid">17431404</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hickok</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Houde</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rong</surname> <given-names>F</given-names></name>. <article-title>Sensorimotor integration in speech processing: computational basis and neural organization</article-title>. <source>Neuron</source>. Elsevier Inc.; <year>2011</year>;<volume>69</volume>: <fpage>407</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.01.019" xlink:type="simple">10.1016/j.neuron.2011.01.019</ext-link></comment> <object-id pub-id-type="pmid">21315253</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penfield</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Boldrey</surname> <given-names>E</given-names></name>. <article-title>Somatic motor and sensory representation in the cerebral cortex of man as studied by electrical stimulation</article-title>. <source>Brain</source>. <year>1937</year>;<volume>60</volume>: <fpage>389</fpage>–<lpage>443</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pulvermüller</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Huss</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kherif</surname> <given-names>F</given-names></name>, <article-title>Moscoso del Prado Martin F, Hauk O, Shtyrov Y. Motor cortex maps articulatory features of speech sounds</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2006</year>;<volume>103</volume>: <fpage>7865</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guenther</surname> <given-names>FH</given-names></name>. <article-title>Cortical interactions underlying the production of speech sounds</article-title>. <source>J Commun Disord</source>. <year>2006</year>;<volume>39</volume>: <fpage>350</fpage>–<lpage>365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jcomdis.2006.06.013" xlink:type="simple">10.1016/j.jcomdis.2006.06.013</ext-link></comment> <object-id pub-id-type="pmid">16887139</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grabski</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Lamalle</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vilain</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>J-L</given-names></name>, <name name-style="western"><surname>Vallée</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Tropres</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Functional MRI assessment of orofacial articulators: neural correlates of lip, jaw, larynx, and tongue movements</article-title>. <source>Hum Brain Mapp</source>. <year>2012</year>;<volume>33</volume>: <fpage>2306</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hbm.21363" xlink:type="simple">10.1002/hbm.21363</ext-link></comment> <object-id pub-id-type="pmid">21826760</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tate</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Herbet</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Moritz-Gasser</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tate</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Duffau</surname> <given-names>H</given-names></name>. <article-title>Probabilistic map of critical functional regions of the human cerebral cortex: Broca’s area revisited</article-title>. <source>Brain</source>. <year>2014</year>;<volume>137</volume>: <fpage>2773</fpage>–<lpage>2782</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/awu168" xlink:type="simple">10.1093/brain/awu168</ext-link></comment> <object-id pub-id-type="pmid">24970097</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cheung</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Hamiton</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>EF</given-names></name>. <article-title>The auditory representation of speech sounds in human motor cortex</article-title>. <source>Elife</source>. <year>2016</year>;<volume>5</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maeda</surname> <given-names>S</given-names></name>. <article-title>A digital simulation method of the vocal-tract system</article-title>. <source>Speech Commun</source>. <year>1982</year>; <fpage>199</fpage>–<lpage>229</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sondhi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schroeter</surname> <given-names>J</given-names></name>. <article-title>A hybrid time-frequency domain articulatory speech synthesizer</article-title>. <source>IEEE Trans Acoust</source>. <year>1987</year>;<volume>35</volume>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kello</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>. <article-title>A neural network model of the articulatory-acoustic forward mapping trained on recordings of articulatory parameters</article-title>. <source>J Acoust Soc Am</source>. <year>2004</year>;<volume>116</volume>: <fpage>2354</fpage>. <object-id pub-id-type="pmid">15532666</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Birkholz</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Jackel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kroger</surname> <given-names>KJ</given-names></name>. <article-title>Construction And Control Of A Three-Dimensional Vocal Tract Model</article-title>. <source>2006 IEEE Int Conf Acoust Speech Signal Process Proc</source>. <year>2006</year>;<volume>1</volume>: <fpage>873</fpage>–<lpage>876</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toda</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Black</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Tokuda</surname> <given-names>K</given-names></name>. <article-title>Statistical mapping between articulatory movements and acoustic spectrum using a Gaussian mixture model</article-title>. <source>Speech Commun</source>. <year>2008</year>;<volume>50</volume>: <fpage>215</fpage>–<lpage>227</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref035"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Bocquelet F, Hueber T, Girin L, Badin P, Yvert B. Robust Articulatory Speech Synthesis using Deep Neural Networks for BCI Applications. Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). 2014. pp. 2288–2292.</mixed-citation></ref>
<ref id="pcbi.1005119.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Assaneo</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Trevisan</surname> <given-names>M a</given-names></name>., <name name-style="western"><surname>Mindlin</surname> <given-names>GB</given-names></name>. <article-title>Discrete motor coordinates for vowel production</article-title>. <source>PLoS One</source>. <year>2013</year>;<volume>8</volume>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Preuß</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Neuschaefer-Rube</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Birkholz</surname> <given-names>P</given-names></name>. <article-title>Real-time control of a 2D animation model of the vocal tract using optopalatography</article-title>. <source>Proc Annu Conf Int Speech Commun Assoc INTERSPEECH</source>. <year>2013</year>; <fpage>997</fpage>–<lpage>1001</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Story</surname> <given-names>BH</given-names></name>. <article-title>Phrase-level speech simulation with an airway modulation model of speech production</article-title>. <source>Comput Speech Lang</source>. Elsevier Ltd; <year>2013</year>;<volume>27</volume>: <fpage>989</fpage>–<lpage>1010</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csl.2012.10.005" xlink:type="simple">10.1016/j.csl.2012.10.005</ext-link></comment> <object-id pub-id-type="pmid">23503742</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toutios</surname> <given-names>a</given-names></name>, <name name-style="western"><surname>Ouni</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Laprie</surname> <given-names>Y</given-names></name>. <article-title>Estimating the control parameters of an articulatory model from electromagnetic articulograph data</article-title>. <source>J Acoust Soc Am</source>. <year>2011</year>;<volume>129</volume>: <fpage>3245</fpage>–<lpage>3257</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.3569714" xlink:type="simple">10.1121/1.3569714</ext-link></comment> <object-id pub-id-type="pmid">21568426</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beautemps</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Badin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bailly</surname> <given-names>G</given-names></name>. <article-title>Linear degrees of freedom in speech production: Analysis of cineradio- and labio-film data and articulatory-acoustic modeling</article-title>. <source>J Acoust Soc Am</source>. <year>2001</year>;<volume>109</volume>: <fpage>2165</fpage>–<lpage>2180</lpage>. <object-id pub-id-type="pmid">11386568</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ifft</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Shokur</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Lebedev</surname> <given-names>M a</given-names></name>, <article-title>International LS. Brain-Machine Interface Enables Bimanual Arm Movements in Monkeys</article-title>. <source>Sci Transl Med</source>. <year>2013</year>;<volume>5</volume>. e</mixed-citation></ref>
<ref id="pcbi.1005119.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wodlinger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Downey</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Schwartz a</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Boninger</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Collinger</surname> <given-names>JL</given-names></name>. <article-title>Ten-dimensional anthropomorphic arm control in a human brain-machine interface: difficulties, solutions, and limitations</article-title>. <source>J Neural Eng</source>. IOP Publishing; <year>2015</year>;<volume>12</volume>: <fpage>016011</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1741-2560/12/1/016011" xlink:type="simple">10.1088/1741-2560/12/1/016011</ext-link></comment> <object-id pub-id-type="pmid">25514320</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref043"><label>43</label><mixed-citation publication-type="other" xlink:type="simple">Richmond K, Hoole P, King S, Forum I. Announcing the Electromagnetic Articulography (Day 1) Subset of the mngu0 Articulatory Corpus. Proc Interspeech.: 1–4.</mixed-citation></ref>
<ref id="pcbi.1005119.ref044"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Uría B. A Deep Belief Network for the Acoustic-Articulatory Inversion Mapping Problem [Internet]. 2011.</mixed-citation></ref>
<ref id="pcbi.1005119.ref045"><label>45</label><mixed-citation publication-type="other" xlink:type="simple">Tokuda K, Oura K, Tamamori A, Sako S, Zen H, Nose T, et al. Speech Signal Processing Toolkit (SPTK). In: <ext-link ext-link-type="uri" xlink:href="http://sp-tk.sourceforge.net/" xlink:type="simple">http://sp-tk.sourceforge.net/</ext-link>. 2014.</mixed-citation></ref>
<ref id="pcbi.1005119.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Imai</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sumita</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Furuichi</surname> <given-names>C</given-names></name><article-title>. Mel Log Spectrum Approximation (MLSA) Filter for Speech Synthesis</article-title>. <source>Electron Commun Japan</source>. <year>1983</year>;<volume>66-A</volume>: <fpage>10</fpage>–<lpage>18</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref047"><label>47</label><mixed-citation publication-type="other" xlink:type="simple">Grimaldi M, Fivela BG. New technologies for simultaneous acquisition of speech articulatory data: 3D articulograph, ultrasound and electroglottograph. Proc …. 2008;</mixed-citation></ref>
<ref id="pcbi.1005119.ref048"><label>48</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Markel</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Gray</surname> <given-names>AH</given-names></name>. <source>Linear Prediction of Speech</source> [Internet]. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>1976</year>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bechet</surname> <given-names>F</given-names></name>. <article-title>LIA_PHON—Un systeme complet de phonetisation de textes</article-title>. <source>Trait Autom des Langues</source>. <year>2001</year>;<volume>42</volume>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larochelle</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Lamblin</surname> <given-names>P</given-names></name>. <article-title>Exploring Strategies for Training Deep Neural Networks</article-title>. <source>J Mach Learn Res</source>. <year>2009</year>;<volume>1</volume>: <fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aryal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gutierrez-Osuna</surname> <given-names>R</given-names></name>. <article-title>Data driven articulatory synthesis with deep neural networks</article-title>. <source>Comput Speech Lang</source>. Elsevier Ltd; <year>2015</year>; <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>A Practical Guide to Training Restricted Boltzmann Machines</article-title>. <source>Comput Sci</source>. <year>2010</year>;<volume>7700</volume>: <fpage>599</fpage>–<lpage>619</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname> <given-names>RR</given-names></name>. <article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source>. <year>2006</year>;<volume>313</volume>: <fpage>504</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1127647" xlink:type="simple">10.1126/science.1127647</ext-link></comment> <object-id pub-id-type="pmid">16873662</object-id></mixed-citation></ref>
<ref id="pcbi.1005119.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name>. <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source>. <year>1986</year>;<volume>323</volume>: <fpage>533</fpage>–<lpage>536</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref055"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">Rasmussen CE. Function minimization using conjugate gradients. 1996;0: 1–7.</mixed-citation></ref>
<ref id="pcbi.1005119.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hothorn</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bretz</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Westfall</surname> <given-names>P</given-names></name>. <article-title>Simultaneous Inference in General Parametric Models</article-title>. <source>Biometrical J</source>. <year>2008</year>;<volume>50</volume>: <fpage>346</fpage>–<lpage>363</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref057"><label>57</label><mixed-citation publication-type="other" xlink:type="simple">Hueber T, Badin P, Savariaux C. Differences in articulatory strategies between silent, whispered and normal speech? a pilot study using electromagnetic articulography. Proc ISSP, to …. 2010; 0–1.</mixed-citation></ref>
<ref id="pcbi.1005119.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Janke</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wand</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>T</given-names></name>. <article-title>Impact of lack of acoustic feedback in EMG-based silent speech recognition</article-title>. <source>Proc Interspeech</source>. <year>2010</year>; <fpage>2686</fpage>–<lpage>2689</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref059"><label>59</label><mixed-citation publication-type="other" xlink:type="simple">Wang J, Samal A, Green JR. Across-speaker Articulatory Normalization for Speaker-independent Silent Speech Recognition Callier Center for Communication Disorders Department of Computer Science &amp; Engineering Department of Communication Sciences &amp; Disorders. 2014; 1179–1183.</mixed-citation></ref>
<ref id="pcbi.1005119.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Denby</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Honda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hueber</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Brumberg</surname> <given-names>JS</given-names></name>. <article-title>Silent speech interfaces</article-title>. <source>Speech Commun</source>. Elsevier B.V.; <year>2010</year>;<volume>52</volume>: <fpage>270</fpage>–<lpage>287</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref061"><label>61</label><mixed-citation publication-type="other" xlink:type="simple">Wand M, Schulte C, Janke M, Schultz T. Array-based Electromyographic Silent Speech Interface. 6th International Conference on Bio-inspired Systems and Signal Processing. 2013.</mixed-citation></ref>
<ref id="pcbi.1005119.ref062"><label>62</label><mixed-citation publication-type="other" xlink:type="simple">Cler MJ, Nieto-Castanon a, Guenther FH, Stepp CE. Surface electromyographic control of speech synthesis. Eng Med Biol Soc (EMBC), 2014 36th Annu Int Conf IEEE. 2014; 5848–5851.</mixed-citation></ref>
<ref id="pcbi.1005119.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hueber</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bailly</surname> <given-names>G</given-names></name>. <article-title>Statistical conversion of silent articulation into audible speech using full-covariance HMM</article-title>. <source>Comput Speech Lang</source>. <year>2016</year>;<volume>36</volume>: <fpage>274</fpage>–<lpage>293</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005119.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hueber</surname> <given-names>T.</given-names></name>, <name name-style="western"><surname>Benaroya</surname> <given-names>E.L.</given-names></name>, <name name-style="western"><surname>Chollet</surname> <given-names>G.</given-names></name>, <name name-style="western"><surname>Denby</surname> <given-names>B.</given-names></name>, <name name-style="western"><surname>Dreyfus</surname> <given-names>G.</given-names></name>, <name name-style="western"><surname>Stone</surname> <given-names>M.</given-names></name>, (<year>2010</year>) "<article-title>Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips</article-title>", <source>Speech Communication</source>, <volume>52</volume>(<issue>4</issue>), pp. <fpage>288</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>