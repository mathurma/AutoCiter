<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006613</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01838</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Visual object recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Materials science</subject><subj-group><subject>Materials</subject><subj-group><subject>Amorphous solids</subject><subj-group><subject>Glass</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Fish</subject><subj-group><subject>Chondrichthyes</subject><subj-group><subject>Elasmobranchii</subject><subj-group><subject>Sharks</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep convolutional networks do not classify based on global object shape</article-title>
<alt-title alt-title-type="running-head">Deep networks do not classify based on global object shape</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0673-2486</contrib-id>
<name name-style="western">
<surname>Baker</surname>
<given-names>Nicholas</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lu</surname>
<given-names>Hongjing</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9594-2660</contrib-id>
<name name-style="western">
<surname>Erlikhman</surname>
<given-names>Gennady</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kellman</surname>
<given-names>Philip J.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, University of California, Los Angeles, Los Angeles, California, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>University of Nevada, Reno, Nevada, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname>
<given-names>Wolfgang</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Technische Universitat Chemnitz, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label>
<p>Current address: Department of Psychology, University of California, Los Angeles, Los Angeles, California, United States of America</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">nbaker9@ucla.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>7</day>
<month>12</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>12</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>12</issue>
<elocation-id>e1006613</elocation-id>
<history>
<date date-type="received">
<day>3</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Baker et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006613"/>
<abstract>
<p>Deep convolutional networks (DCNNs) are achieving previously unseen performance in object classification, raising questions about whether DCNNs operate similarly to human vision. In biological vision, shape is arguably the most important cue for recognition. We tested the role of shape information in DCNNs trained to recognize objects. In Experiment 1, we presented a trained DCNN with object silhouettes that preserved overall shape but were filled with surface texture taken from other objects. Shape cues appeared to play some role in the classification of artifacts, but little or none for animals. In Experiments 2–4, DCNNs showed no ability to classify glass figurines or outlines but correctly classified some silhouettes. Aspects of these results led us to hypothesize that DCNNs do not distinguish object’s bounding contours from other edges, and that DCNNs access some local shape features, but not global shape. In Experiment 5, we tested this hypothesis with displays that preserved local features but disrupted global shape, and vice versa. With disrupted global shape, which reduced human accuracy to 28%, DCNNs gave the same classification labels as with ordinary shapes. Conversely, local contour changes eliminated accurate DCNN classification but caused no difficulty for human observers. These results provide evidence that DCNNs have access to some local shape information in the form of local edge relations, but they have no access to global object shapes.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>“Deep learning” systems–specifically, deep convolutional neural networks (DCNNs)–have recently achieved near human levels of performance in object recognition tasks. It has been suggested that the processing in these systems may model or explain object perception abilities in biological vision. For humans, shape is the most important cue for recognizing objects. We tested whether deep convolutional neural networks trained to recognize objects make use of object shape. Our findings indicate that other cues, such as surface texture, play a larger role in deep network classification than in human recognition. Most crucially, we show that deep learning systems have no sensitivity to the overall shape of an object. Whereas deep learning systems can access some local shape features, such as local orientation relations, they are not sensitive to the arrangement of these edge features or global shape in general, and they do not appear to distinguish bounding contours of objects from other edge information. These findings show a crucial divergence between artificial visual systems and biological visual processes.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>DGE-1829071</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0673-2486</contrib-id>
<name name-style="western">
<surname>Baker</surname>
<given-names>Nicholas</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was funded by the National Science Foundation Research Traineeship for ModEling and uNdersTanding human behaviOR (MENTOR) DGE-1829071 (<ext-link ext-link-type="uri" xlink:href="http://www.math.ucla.edu/~bertozzi/NRT/index.html" xlink:type="simple">http://www.math.ucla.edu/~bertozzi/NRT/index.html</ext-link>) to NB and the Advancing Theory and Application in Perceptual and Adaptive Learning to Improve Community College Mathematics NSF Grant ECR-1644916 (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1644916&amp;HistoricalAwards=false" xlink:type="simple">https://www.nsf.gov/awardsearch/showAward?AWD_ID=1644916&amp;HistoricalAwards=false</ext-link>) to PJK. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="28"/>
<table-count count="2"/>
<page-count count="43"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-12-26</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the manuscript and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Machine vision is one of the most challenging problems in artificial intelligence. Task-general image understanding is so difficult that it constitutes an “AI complete” problem [<xref ref-type="bibr" rid="pcbi.1006613.ref001">1</xref>], that is, a problem of sufficient difficulty and generality that it requires intelligence on a par with humans. If solved, it would be considered equivalent to the first successful completion of a Turing test [<xref ref-type="bibr" rid="pcbi.1006613.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1006613.ref003">3</xref>]. While the general problem of image understanding is still far outside the capabilities of modern artificial systems, algorithms are beginning to reach near human capabilities on certain specialized tasks. In particular, deep convolutional neural network (DCNN) algorithms are achieving previously unseen performance on object recognition tasks.</p>
<p>Since their first entrance [<xref ref-type="bibr" rid="pcbi.1006613.ref004">4</xref>] in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), deep convolutional networks have substantially outperformed other state of the art recognition algorithms (e.g., [<xref ref-type="bibr" rid="pcbi.1006613.ref005">5</xref>]), to the point of the practical extinction of the latter. Modern ILSVRC competitions (including 1.2 million images associated with 1000 categories) almost exclusively feature deep convolutional networks, and their error rates have continuously fallen with more powerful hardware and more sophisticated engineering. The current winner has a top-five error rate of less than 3% on the image classification task, meaning that it fails to include the correct category out of 1000 object categories in its top five most likely choices less than 3% of the time, which is even lower than human performance on the same task (~5.1%).</p>
<p>The impressive performance of DCNNs on natural image recognition tasks and certain apparent similarities between human physiology and the architecture of these networks suggest the natural question of whether these systems explain the capabilities of human perception and acquire similar representations to those used by the human visual system. As DCNNs approach human performance in object recognition tasks, we may ask whether or in what ways their architecture and processing mirror that of human vision. In this paper, we take up this question with special focus on object shape. In a series of five experiments, we probe the capabilities of DCNNs and humans to cope with object classifications with the goal of finding out whether trained networks overtly or implicitly encode object shape and use it to perform classifications.</p>
<p>To anticipate our results: Deep learning networks lack shape representations and processing capabilities that form the primary bases of human object classification. Deep learning networks do have access to some relations of local orientations that may be considered local shape constituents, but they do not appear to form global shape representations. We also show that deep learning networks make no special use of the bounding contours of objects, which most reliably define shape in human and biological vision.</p>
<sec id="sec002" sec-type="intro">
<title>Background</title>
<p>DCNNs have attracted considerable attention, and several different approaches have been used to compare their performance to human object processing. Some of the similarities begin with the basic architecture. Deep convolutional neural networks perform a series of nonlinear transformations on input data such as an image in the case of object recognition. The final transformation outputs a vector of category probability values, one for each object category. Critically, early layers of these networks are not fully connected as in classical neural networks. Instead, they have convolutional windows that preserve spatial information in the image [<xref ref-type="bibr" rid="pcbi.1006613.ref006">6</xref>]. In modern DCNNs, early layers tend to operate on very local regions of the image, while deeper into the network, each node receives input from filters over a larger area of the image, allowing the network to access relations between more distant regions [<xref ref-type="bibr" rid="pcbi.1006613.ref004">4</xref>]. This network architecture has some obvious similarities with biological vision. Convolutional layers are analogous to receptive fields in visual cortex, which likewise consider more disparate regions together at higher levels of extrastriate cortex [<xref ref-type="bibr" rid="pcbi.1006613.ref007">7</xref>].</p>
<p>Do the similarities between DCNNs and biological vision go deeper than this basic architectural feature? One way to evaluate this is by comparing physiological activity of neural units in biological systems with the activity of certain nodes in an artificial network. Pospisil, Pasupathy, and Bair presented AlexNet, a groundbreaking DCNN, with shape stimuli to which V4 cells are optimally tuned [<xref ref-type="bibr" rid="pcbi.1006613.ref008">8</xref>] and found that there is some resemblance between node responses in intermediate layers of AlexNet and cell responses in V4, although the network response was quite sparse compared to biological systems, with many units responding to none or very few of the shape stimuli [<xref ref-type="bibr" rid="pcbi.1006613.ref009">9</xref>]. Other studies have looked at the correlation between a network’s classification accuracy and the similarity between network representations and representations in the inferotemporal gyrus (IT). Randomly varying parameters across several networks, they found that the activity of nodes in networks that perform better on the object classification task give better predictions about the activity of clusters of neurons in IT when primates are presented with the same image [<xref ref-type="bibr" rid="pcbi.1006613.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1006613.ref011">11</xref>].</p>
<p>Comparisons have also been made between a network’s performance and human behavior in similar tasks. In a sense, all performance measures on image classification are a comparison to human vision, as accuracy is being measured based on labels assigned by humans [<xref ref-type="bibr" rid="pcbi.1006613.ref012">12</xref>]. However, to evaluate similarities and differences between DCNNs and human vision, it can be instructive to examine network performance on tasks for which they were not explicitly trained. Several experiments have found similarities between convolutional networks and humans in such tasks. One study used features from a convolutional network to predict the memorability of certain object segments. Features extracted from the DCNN were predictive of objects’ memorability for human subjects, suggesting that humans and networks might be attending to similar features when viewing an object [<xref ref-type="bibr" rid="pcbi.1006613.ref013">13</xref>]. In another study, Peterson, Abbott, and Griffiths found a strong correlation between similarity judgments made by DCNNs with human similarity ratings [<xref ref-type="bibr" rid="pcbi.1006613.ref014">14</xref>].</p>
<p>Although it is interesting to observe similar performance level for object recognition between DCNNs and biological vision, it is unclear whether the systems process information for object recognition in a similar manner. The present paper focuses on the perception of shapes, the most important cue for object recognition [<xref ref-type="bibr" rid="pcbi.1006613.ref015">15</xref>]. Objects can be recognized accurately despite impoverishments across every other visual dimension provided that global shape information is preserved [<xref ref-type="bibr" rid="pcbi.1006613.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref017">17</xref>]. For example, consider the image pair presented in <xref ref-type="fig" rid="pcbi.1006613.g001">Fig 1</xref>. In <xref ref-type="fig" rid="pcbi.1006613.g001">Fig 1A</xref>, the information available for recognition has been significantly reduced across several feature dimensions. The object has no texture or background context, and the information along its contour has been simplified. Still, it is far more easily recognized as a bear than the object in <xref ref-type="fig" rid="pcbi.1006613.g001">Fig 1B</xref>, where cues like texture and context are preserved, but object shape is interrupted. Similarly, an ordinary line drawing, or even a few well-chosen lines as in a Picasso sketch, readily allows object recognition via shape perception processes in humans.</p>
<fig id="pcbi.1006613.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Demonstration of the importance of global shape in object recognition.</title>
<p>(a) Silhouette of a bear; (b) Scrambled natural image of a bear (See text). Image URLs are in <xref ref-type="supplementary-material" rid="pcbi.1006613.s002">S2 File</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g001" xlink:type="simple"/>
</fig>
<p>If deep networks are to be taken as models of human perception, we would expect object shape to be a critical component of their classification decisions. Currently, it is unclear to what extent shape representations play a role in object recognition in DCNNs. Kubilius, Bracci, and Op de Beeck conducted several intriguing experiments that suggest deep networks do have shape representations that are reasonably similar to human shape representations [<xref ref-type="bibr" rid="pcbi.1006613.ref018">18</xref>]. Networks were able to classify object silhouettes with ~40% accuracy and had some sensitivity to non-accidental features of an object, which are thought to be important for recognition in human observers [<xref ref-type="bibr" rid="pcbi.1006613.ref016">16</xref>]. They also compared the impact of shape cues on recognition performance for the networks with different architectures (e.g., different number of layers) and found some evidence that deeper networks did better on tasks where object shape was important to performance.</p>
<p>On the other hand, some research on DCNNs is difficult to reconcile with the claim that they utilize global shape of objects in detecting and recognizing objects. Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus found that perturbation of a small subset of image pixels could result in consistent misclassification of the image, across multiple DCNNs, despite the changes being undetectable to human observers [<xref ref-type="bibr" rid="pcbi.1006613.ref019">19</xref>]. In the perturbed images, global shape of the object is unchanged, so the change in classification revealed that the global shape information of objects is likely not used for recognizing objects in the DCNNs. Another study used evolutionary algorithms to develop images that networks classified as certain objects with a high degree of confidence, despite a total absence of object shape in the images [<xref ref-type="bibr" rid="pcbi.1006613.ref020">20</xref>]. Zhu, Xie, and Yuille tested DCNN classification accuracy for images in which the object to be classified was removed [<xref ref-type="bibr" rid="pcbi.1006613.ref021">21</xref>]. They found that despite the object not actually being present in the image, networks performed reliably better than chance on the classification task, based purely on contextual information. These examples suggest that shape might be neither sufficient nor necessary for recognition in DCNNs.</p>
<p>More systematic tests are needed to understand whether or not, or in what ways, DCNNs process object shape. The fact that a network’s responses are sensitive to variables other than shape (such as texture or color) may reflect a valid use of information in its training history. The supervised learning method with which DCNNs are trained is agnostic about what information to consider when classifying an image. In natural images, texture and shape information are often highly correlated, so we can learn little about what cue is most relevant to the network’s classification without disentangling them. Reduction of performance by disruption of other variables may indicate that shape representations do not predominate, but such outcomes do not necessarily imply that shape information is not captured or potentially usable within the network. We term this “the latency problem”. To show that shape information is not implicitly captured or usable in a system, more systematic tests are required than simply showing that variables other than shape can be decisive in classification performance. Shape information may nevertheless be something DCNNs can in principle capture, but it may be latent in the system, overshadowed by other informational variables relevant for classification. Conversely, tests that have suggested that DCNN’s do use shape information have not distinguished local shape features from more global shape characteristics, nor have they disentangled responses to local orientation relations in visible contours from the more shape-defining contributions of the bounding contours of objects.</p>
<p>Questions about shape processing in DCNNs can be considered at three different levels: First, do deep networks trained to recognize objects use shape features in their classification decision? Second, can deep networks be trained to use shape information in their classification decisions? Third, can formal analysis of deep networks’ computational processes tell us what kinds of shape features they can and cannot extract from an image? In the current study, we focus on the first question, aiming to understand the capabilities that deep networks automatically learn through training on natural images. We consider the study of trained networks to be particularly important for two reasons. First, the attention garnered by DCNNs relates to the success of trained networks in object classification tasks. Both for theoretical understanding of these achievements and for practical applications, understanding how DCNNs achieve their high classification is important. Second, comparisons between biological vision systems and DCNNs require understanding how trained networks are operating, and require an understanding of the role of shape processing in particular. A sizable literature has recently emerged finding similarity between DCNNs trained on object recognition tasks and human perception [<xref ref-type="bibr" rid="pcbi.1006613.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1006613.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1006613.ref026">26</xref>]. These efforts highlight the importance of understanding the functioning of trained DCNNs that are successful in object recognition.</p>
<p>In the experiments reported here, we presented DCNNs trained for object recognition (and, where appropriate, human observers) with stimuli intended to reveal information about the usability of global shape and bounding contours. The series of experiments was designed to provide multiple sources of information with regard to the latency problem in characterizing shape capabilities, in the process clarifying the roles of texture information and object shape in deep convolutional networks’ classification decisions. To carry out these studies, we tested two commonly used deep convolutional networks: AlexNet [<xref ref-type="bibr" rid="pcbi.1006613.ref004">4</xref>] and VGG-19 [<xref ref-type="bibr" rid="pcbi.1006613.ref027">27</xref>]. AlexNet has eight layers and is the deep network that started the DCNN revolution for object recognition. VGG-19 is deeper, 19 layers, and approaches the state of the art in object classification. Our approach was to use a variety of systematically modified stimuli to reveal the contribution of shape information to network responses. In Experiment 1, we examined the relative importance of overall shape and texture information by using objects in which the overall shape was preserved, but different texture, from another object, was superimposed on the object’s silhouette. In Experiments 2–4, we tested the networks on images with impoverished or altered texture and context information by using glass figurines, object outlines, and silhouettes. Following up the results and hypotheses emerging from these experiments, we tested the networks on images with manipulations that altered shape at a global level, while largely preserving local shape features, and vice versa (Experiment 5).</p>
</sec>
</sec>
<sec id="sec003" sec-type="results">
<title>Results</title>
<sec id="sec004">
<title>Experiment 1</title>
<p>As noted earlier, shape is of great importance in human perception of objects, and shape information predominates in human object recognition. Prior tests of DCNNs have yielded some evidence that they classify by means of shape, whereas other work revealed examples in which images with textural similarity, but no shape in common with an object, were classified as that object with a high degree of confidence [<xref ref-type="bibr" rid="pcbi.1006613.ref020">20</xref>]. In Experiment 1, we directly compared convolutional networks’ use of shape and texture information in their classification decision. Using object silhouettes with no surface information, we overlaid a texture from a different object on top of the black figural region. We then compared the networks’ preference for both the object whose shape is displayed and the object whose texture is displayed.</p>
<sec id="sec005">
<title>Experiment 1 Method</title>
<p><italic>Test stimuli</italic>. Forty images of object silhouettes and 40 natural images were obtained from internet sources (for URLs of original images, see <xref ref-type="supplementary-material" rid="pcbi.1006613.s002">S2 File</xref>). Half of the object silhouettes were animals, and half manmade artifacts. In the ImageNet database, about 40% of categories are animals, and 50% are manmade objects (the last 10% are other inanimate objects like food). For each silhouette, a texture from one of the natural images was overlaid on the object. All shapes and textures were taken from the 1000 object categories on which the network was trained. See <xref ref-type="fig" rid="pcbi.1006613.g002">Fig 2</xref> for examples.</p>
<fig id="pcbi.1006613.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Sample stimuli used in Experiment 1.</title>
<p>The bounding shape of an object was combined with the texture of a different object to generate each image. <bold>a)</bold> Shape: Teapot | Texture: Golf ball; <bold>b)</bold> Shape: Vase | Texture: Gong; <bold>c)</bold> Shape: Airplane | Texture: Otter; <bold>d)</bold> Shape: Obelisk | Texture: Lobster; <bold>e)</bold> Shape: Cannon | Texture: Pineapple; <bold>f)</bold> Shape: Ram | Texture: Bison; <bold>g)</bold> Shape: Camel | Texture: Zebra; <bold>h)</bold> Shape: Orca | Texture: Kimono; <bold>i)</bold> Shape: Otter | Texture: Speedometer; <bold>j)</bold> Shape: Elephant | Texture: Sock. The full image set is displayed in Figs <xref ref-type="fig" rid="pcbi.1006613.g003">3</xref>–<xref ref-type="fig" rid="pcbi.1006613.g006">6</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g002" xlink:type="simple"/>
</fig>
<p><bold><italic>Network</italic>.</bold> Tests were conducted on a pre-trained VGG-19 network.</p>
</sec>
<sec id="sec006">
<title>Experiment 1 results</title>
<p>For each of the displayed images, the network assigned a probability value to each of the 1000 object categories it had been trained to classify. The objects that received the five highest probability assignments for each image are shown, broken into four parts for size considerations, in Figs <xref ref-type="fig" rid="pcbi.1006613.g003">3</xref>–<xref ref-type="fig" rid="pcbi.1006613.g006">6</xref>, along with the probability assigned to the correct shape and texture label. Based on shape, the network chose as its highest probability classification the correct answer for 5 of the 40 objects. Based on texture, the network chose as its highest probability classification the correct answer for 4 of the 40 objects. In terms of including the correct answer in its top 5 possibilities, the network classified 8 of 40 objects within its top 5 choices by shape and 7 of 40 objects within its top 5 choices by texture. Overall, the assigned probability was lower than is typical for natural images for both the correct texture-object label and the correct shape-object label. For photographs of objects that include texture, shape and context, 90% or more of the total probability across 1000 object categories will ordinarily be assigned to the correct object label. In this simulation, there were a few shape-based classifications that were near natural image performance, such as the abacus and the trombone, but on average shape-based classifications were nearer to 10%. Likewise, for textures, a few of objects were assigned probabilities that were 20% or greater but average performance was quite poor. Human observers, for whom shape is predominant in object recognition, readily produce correct shape labels for all of these objects, as confirmed by pilot studies. By contrast, across the whole display set used here, the object whose shape was depicted in the displays was selected by the network, on average, as its 209<sup>th</sup> ranked choice.</p>
<fig id="pcbi.1006613.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Network classifications for the stimuli presented in Experiment 1 Part 1.</title>
<p>The left most column shows the image presented. The second column in each row names the object from which the shape was sampled. The third column names the object from which the texture silhouette was obtained. Probabilities assigned to the object name in columns 2 and 3 are shown as percents below the object label. The remaining five columns show the probabilities (as percents) produced by the network for its top five classifications, ordered left to right in terms of probability. Correct shape classifications in the top five are shaded in blue and correct texture classifications are shaded in orange.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g003" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Network classifications for the stimuli presented in Experiment 1 Part 2.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g004" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Network classifications for the stimuli presented in Experiment 1 Part 3.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g005" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Network classifications for the stimuli presented in Experiment 1 Part 4.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g006" xlink:type="simple"/>
</fig>
<p>Although there were indications of some use of shape information by the network for only about 20% of the displays tested in Experiment 1, an interesting pattern can be seen in the data. Use of shape information appeared to play some role in DCNN classification of artifacts but almost none for animals. The network had the object shape in its top-five classification selections for seven of the 20 artifacts, and only one of the 20 animals. The average probability assigned to the image’s shape label was 10 times higher for artifacts than for animals (17.90% vs. 1.75%). Texture appears to be about equally considered for both kinds of stimuli. There are three classifications in the top 5 choices for the texture-object in the 20 artifact images, and four in the 20 animal images; the mean probability assigned to the texture object is about equal for both kinds of images (3.22% vs. 3.73%).</p>
<p>The data from Experiment 1 were also analyzed by directly comparing the probability value associated with the object whose shape was described by the silhouette and the object whose texture was overlaid atop the shape. Overall, texture was preferred more often than shape (23 vs. 17). However, there was a large difference between network behavior in manmade objects versus animals. The network assigned higher probability to the shape-label in 14 of the 20 manmade objects, but only three of the 20 animal images (see Figs <xref ref-type="fig" rid="pcbi.1006613.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006613.g008">8</xref>).</p>
<fig id="pcbi.1006613.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparison of probabilities assigned to image shapes and textures for animals.</title>
<p>On the x-axis, the shape and texture of each object are given as shape-texture. Filled black bars display the probability given by the network to the correct shape. Outlined bars display the probability given by the network for the correct texture.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g007" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Comparison of probabilities assigned to image shapes and textures for artifacts.</title>
<p>On the x-axis, the shape and texture of each object are given as shape-texture. Filled black bars display the probability given by the network to the correct shape. Outlined bars display the probability given by the network for the correct texture.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g008" xlink:type="simple"/>
</fig>
<p>Finally, we measured the contribution of shape and texture to network classification by looking at the rank order of the correct shape-object and the correct texture-object for each display used. For the correct shape response, the mean rank among network outputs was 86.70 for artifacts, and 330.50 for animals. For the correct texture response, the mean rank was 249.95 for artifacts and 65.30 for animals.</p>
</sec>
<sec id="sec007">
<title>Experiment 1 Discussion</title>
<p>Experiment 1 suggested a major difference between human observers and DCNNs. Whereas human observers readily classify objects by shape, even in the face of uncharacteristic texture or context information, VGG-19 showed no evidence that shape information plays a primary role in DCNN classification. The correct label based on shape was chosen as the first choice classification by the network for only 5 of the 40 objects sampled, and the correct shape turned up on average as the 209<sup>th</sup> ranked choice among network outputs in object classification.</p>
<p>Despite the lack of a clear, general accessibility of object shape in DCNN classifications, there was some evidence suggesting use of shape information in some cases. These cases were almost entirely confined to the 20 artifacts tested, in which 5 of 20 objects selected as first-choice classifications matched on shape, with 7 out of 20 objects placing the correct shape in the top 5 choices. In contrast, no first-choice classifications were correct for animal shapes, and only one of 20 animal displays showed any shape match among the top five classifications.</p>
<p>Although the network appears to utilize shape for classifying artifacts but not animals, there are still inconsistent examples in the test. Some objects, even from the artifact stimuli, do not show any evidence that shape is involved in network classification. For example, the airplane with otter texture was assigned essentially zero (.000002) probability to the airplane label (where .001 would be the value obtained by randomly guessing), instead classifying the image as “hatchet”, “nail”, or “hook” as its top three choices, none of which shares any shape similarity with an airplane. While this is a particularly glaring failure on the network’s part, over half of the artifacts were misclassified in all of the network’s top-five selections and the average rank order of the shape label of artifacts was 86.70. Many implausible shape misclassifications are assigned higher probability than the correct shape label. We mention this not to diminish the network’s success for some shapes on a task for which it was not explicitly trained, but to indicate that the results of Exp. 1 pose some differences in misclassification errors between the network and the human visual system. Humans have considerably less difficulty recognizing any of the objects by shape and would never consider some of the objects to which the network assigns high probability (e.g., “parachute” for hammer, or “electric guitar” for apron) as likely candidates.</p>
<p>Understanding why the network makes these kinds of misclassifications could be an important step to reveal the differences between network and human classification capabilities. It is possible that these erroneous responses are simply some intermediate landing point between shape and texture evidence, but classifications like “hatchet” for the airplane-otter suggest little consideration for the object’s texture in the network’s perceptual decision. It is also important to note that we tested objects separated from backgrounds and contexts. Typical tests of DCNNs include contextual information, which has been shown to be so important that networks perform reliably better than chance in classifying objects with images in which the object to be classified has been removed [<xref ref-type="bibr" rid="pcbi.1006613.ref021">21</xref>].</p>
<p>What could give rise to the differences we observed between artifacts and animals in networks that are not explicitly trained to recognize differences between superordinate categories like animacy? One possibility is that the network might learn to down-weight the contribution of texture cues in recognition of many artifact categories during training, since there is a more diverse range of textures and color associated with artifacts such as guitar and hammer. A sofa, for example, can be upholstered with any number of patterns, while a leopard’s fur will tend to be more consistent across exemplars. Another possibility is that DCNNs attribute less importance to shape cues in natural objects due to the large variability in the bounding contours of some natural objects. Animals are non-rigid, and their bounding contours vary considerably from image to image depending on pose, so it might be maladaptive to learn shape features for natural objects during training. Yet another possibility is that DCNNs really do not encode global shape but can nevertheless make use of some local shape features, which tend to be highly diagnostic for some artifacts, but provide little discriminability between different kinds of animals. Later experiments shed light on these possibilities. After considering additional results, we return to these issues in Experiment 4 and the General Discussion.</p>
</sec>
</sec>
<sec id="sec008">
<title>Experiment 2</title>
<p>Experiment 1 showed that in displays that preserved overall object shape but altered their texture, shape was a poor predictor of network classifications. There was some indication, however, of sensitivity to shape information in certain cases. The network made several accurate classifications of objects with non-canonical surface texture. This success appeared to be largely confined to artifacts, although even artifact classifications included many implausible top selections. On the other hand, shape information appeared to be largely irrelevant for classification responses generated for animal displays. In Experiments 2–4, we developed more detailed tests to examine whether networks could classify objects only based on shape with changed or absent surface texture and context information.</p>
<p>It is a remarkable fact, one attesting to the primacy of shape processing in human perception, that human observers readily recognize shapes in arbitrary materials (and construct and display them, etc.). In Experiment 2, we presented two deep networks with glass figurines. All figurines were pictures of real glass objects. Since glass figurines lack the natural surface colors and textures of the objects represented, we expected that accurate classification would be difficult without a representation of the object’s bounding shape. We expected that if the networks did have access to object shape, they might be able to accurately classify the glass figurines even in the absence of other, usually accompanying, cues for recognition. In other words, DCNN classifications that would resemble even a child’s intuitive response the first time they see a glass elephant would furnish evidence that shape information plays a role in DCNN classification.</p>
<sec id="sec009">
<title>Experiment 2 Method</title>
<p><italic>Images</italic>. Twenty images of glass figurines were found from the internet (see <xref ref-type="supplementary-material" rid="pcbi.1006613.s002">S2 File</xref> for URLs). Half of the figurines were of animals and half were of manmade objects. The images had some texture information, but the overall texture of each object was very different from a canonical instance of the represented object. The background in all but one of the 20 images was non-descriptive; either a homogeneous field or a color gradient. The one exception is the schooner figurine (see below), which is photographed on a table. See <xref ref-type="fig" rid="pcbi.1006613.g009">Fig 9</xref> for examples.</p>
<fig id="pcbi.1006613.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Sample stimuli used in Experiment 2.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g009" xlink:type="simple"/>
</fig>
<p><italic>Network</italic>. classification was tested on two networks: AlexNet, with seven layers, and VGG-19.</p>
</sec>
<sec id="sec010">
<title>Experiment 2 results</title>
<p>We assessed the networks as correct if they generated the names that a human observer would give to each image. (Human classification was verified in pilot work with human observers.) Neither network produced as its top choice the correct label for any of the 20 objects. Figs <xref ref-type="fig" rid="pcbi.1006613.g010">10</xref>–<xref ref-type="fig" rid="pcbi.1006613.g011">11</xref> show the top five classification choices for the 20 images shown for VGG-19 (the better-performing network; see below). Percentages in parentheses represent the probability assigned to each label by the network. In the absence of any evidence, the baseline probability of an object was 0.1%, as there were 1000 object categories.</p>
<fig id="pcbi.1006613.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g010</object-id>
<label>Fig 10</label>
<caption>
<title>VGG-19 classifications for glass figurines Part 1.</title>
<p>The leftmost column shows the image presented to the VGG-19 DCNN. The second column shows the correct object label and the probability generated by the network for that label. The other five columns show probabilities for the network’s top five classifications, ordered left to right from highest to lowest. Correct classifications are shaded in blue.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g010" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g011</object-id>
<label>Fig 11</label>
<caption>
<title>VGG-19 classifications for glass figurines Part 2.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g011" xlink:type="simple"/>
</fig>
<p>Most of the top-choice responses seem bizarre for human perceivers, such as "web site" for goose, "oxygen mask" for otter, "can opener" for polar bear, and "chain" for fox. Although the stimuli here are (intentionally) different from what the networks were trained on (because they are glass figurines), the results clearly indicate that shape, if accessible at all by DCNNs, does not play the defining role in object recognition that it does for human perceivers. Comparing the networks' responses to chance level performance (0.1%), AlexNet assigned a probability below chance to the correct shape for 18 of the 20 test objects, and VGG-19 assigned a probability below chance to the correct shape for 15 of the 20 objects. Analysis of the rank order of correct labels revealed that the correct shape choice averaged, across the display set, a mean rank of 162.60.</p>
<p>The criterion of finding the correct answer among the top five most probable responses is often used to assess performance of DCNNs. Using this criterion, VGG-19 correctly classified two of the 20 images (the shark figurine and the grand piano figurine), while AlexNet misclassified all 20 images. VGG-19 assigned a 20.58% probability to the correct grand piano response, second only to its 53.07% probability assigned to "radio telescope." For the shark, VGG-19 selected the correct label as its 4th choice, assigning a 2.58% probability. For many of the remaining images, the objects were misidentified as glass-made or metal-made kitchen objects, such as “water jug”, or “can-opener”.</p>
<p>As another way of measuring the network’s sensitivity to object shape, we compared the probability the network gave to the object in the image with the probability it gave to the nine other objects that were used in the experiment. For example, we compared the probability that the network gave to “goose” when it was shown a goose figurine to the average of the probabilities the network gave to the other nine animal labels (“otter”, “peacock”, and so on) for the same figurine. For this analysis, we kept glass animals and glass objects separate to ensure that higher probabilities could not be accounted for by low-level contour features like the presence or absence of a straight edge. For both animate and inanimate objects, probabilities were not higher for the correct shape-label than for the average of the other nine incorrect shape labels more often than would be expected by chance. Five of the 10 inanimate objects, and seven of the 10 animate objects were given a lower probability than the average of the other nine in their class.</p>
</sec>
<sec id="sec011">
<title>Experiment 2 Discussion</title>
<p>The networks showed little capability of classifying glass figurines. Although glass figurines of animals remove the natural surface texture, they also introduce surface properties of their own. As in Experiment 1, surface features appear to play a major role in the network’s classification decisions. For example, the peacock figurine has “water jug” and “pitcher” in its top-five objects, despite having no shape similarity to either. “Goblet”, “vase”, “cup”, and “hourglass” are also common misclassifications made by both AlexNet and VGG-19. By contrast, the networks appear to make few misclassifications based on similarity between the shape of two objects. Aside from the piano and great white shark, only the tiger had a misclassification (“Egyptian cat”) that would be consistent with use of some sort of information about object shape. It appears that shape does not play an independent, predominant role in recognition, as it does in humans [<xref ref-type="bibr" rid="pcbi.1006613.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1006613.ref030">30</xref>].</p>
<p>Do the network’s few successes point to a broader trend that the network is utilizing both shape and texture in its classification decisions? It is difficult to determine what is different about these three images than the other 20 images that the network fails to classify. One thing to keep in mind is that there is a chance that a plausible shape classification appears in the network’s top-five selections without the network having any sensitivity to the shape of the image. The tiger seems like a likely candidate for this possibility. While Egyptian cats and tigers have some shape features in common, Egyptian cats and elephants have very few, but the network names “Egyptian cat” as its top selection for both the elephant and the tiger glass figurines. In fact, it assigns higher probability that the elephant is an Egyptian cat than that the tiger is an Egyptian cat. It is possible that some surface feature, or conjunction of surface features and local edge properties, is driving classification in both cases.</p>
<p>This explanation is less satisfactory for the grand piano and great white shark, whose shape label is actually in the top-five selections and does not appear in any of the other images’ top selections. In the case of the piano, one possibility we considered is that the texture of the keys drove classification, but a further test showed that the network performs well even after the keys have been occluded or blurred out. We tested the network on five additional glass grand piano images (see <xref ref-type="fig" rid="pcbi.1006613.g012">Fig 12</xref>), and it was unable to correctly classify any of them in its top-five selections. It is unclear what information is present in the image where the network does well that is absent in the other five, but it is likely a local shape feature, as global shape is very similar across the six images. Likewise, it is likely that local contour features, not global shape, are driving the network’s accurate classification of the great white shark. We discuss this hypothesis in greater detail and revisit these positive examples in Experiment 4, after we have considered more data regarding the network’s sensitivity to local and global shape information.</p>
<fig id="pcbi.1006613.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Five additional glass Pianos.</title>
<p>VGG-19 incorrectly classified each of these five images despite correctly classifying the glass piano shown in <xref ref-type="fig" rid="pcbi.1006613.g011">Fig 11</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g012" xlink:type="simple"/>
</fig>
<p>The results of Experiment 2 clearly showed an absence of shape sensitivity for glass figurines. Human classification of such objects is affirmed by the fact that we make, display, and recognize such objects routinely. Although in natural scenes, an elephant is never made of glass, is never 4" high, and only rarely appears on anyone's desk or coffee table, human use of object shape makes recognition of a glass elephant on a desk effortless and routine. Not only is this predominance of shape not seen in DCNN performance, there is little to suggest that shape, independent of other information, is accessible at all in classification of these objects.</p>
<p>In this experiment, texture or surface quality information provided a stronger influence than object shape on object recognition by both AlexNet and VGG-19. We might suspect that the strength of surface texture cues pulled the networks’ top-five classifications towards texture-object labels. If object shape played any role at all, however, we would expect that the correct object label would be assigned a probability that is at least greater than chance. In most cases, the correct shape label was assigned a value less than chance, and objects of similar composition but different shapes tended to receive probabilities as high as the correct shape.</p>
</sec>
</sec>
<sec id="sec012">
<title>Experiment 3</title>
<p>A remarkable fact about human object perception is that we readily extract shape from outline drawings. This ability clearly depends on shape, as outlines omit surface information completely. Object outlines have the same texture within the bounding contour as outside it, and there is no variation in texture between the outlines of two different objects. We tested outlines in Experiment 3 to extend the earlier results and specifically to remove competing texture or surface information as much as possible. If, for example, the pictures of glass figurines somehow distracted deep networks from utilizing some encoded shape information due to competing surface texture, we expected that the problem would be substantially mitigated by using outlines. On the other hand, if deep networks cannot access shape from outline drawings of objects, we expected poor performance.</p>
<sec id="sec013">
<title>Experiment 2 Method</title>
<p><italic>Images</italic>. Forty images of object outlines were selected from the internet, half of which were manmade artifacts, and half of which were animals. In all images, the only contrast difference was at the object contour. Images were uniformly white at all other locations in the image. There was a degree of abstraction in the object contours, as the outlines were not boundaries of real natural objects. All were readily recognized and correctly named by human observers. <xref ref-type="fig" rid="pcbi.1006613.g013">Fig 13</xref> shows examples of stimuli used in Experiment 3.</p>
<fig id="pcbi.1006613.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Sample outline stimuli used in Experiment 3.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g013" xlink:type="simple"/>
</fig>
<p><bold>Networks:</bold> Tests were conducted using both AlexNet and VGG-19.</p>
</sec>
<sec id="sec014">
<title>Experiment 2 results</title>
<p>None of the correct shape labels for the 40 objects were chosen as the first-choice classification by either VGG-19 or AlexNet. Two of the 40 objects were named among the top-five choices for VGG-19, and only one was given as a top-five response by AlexNet. Figs <xref ref-type="fig" rid="pcbi.1006613.g014">14</xref>–<xref ref-type="fig" rid="pcbi.1006613.g017">17</xref> show the full set of responses. Twenty-eight of the probabilities assigned to the object label given by humans were below the chance rate of 0.1% in VGG-19, and 35 out of 40 were below 0.1% in AlexNet. On average, the correct classification by shape was the network’s 328<sup>th</sup> most preferred choice. As in Experiment 2, we compared the probability assigned to the correct shape label to the shape label of 19 other animate or inanimate objects. For animals, the probability was lower for the shape-object than for the mean of 19 other animal images in eight of the 20 trials. For artifacts, it was lower in 10 of the 20 images. These results do not differ from the chance likelihood that the target shape will be higher than the mean of the other 19 in half the trials. Of the objects that were correctly classified, all were artifacts and none were animals.</p>
<fig id="pcbi.1006613.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g014</object-id>
<label>Fig 14</label>
<caption>
<title>VGG-19 classifications for object outlines Part 1.</title>
<p>The leftmost column is the image presented to the DCNN. The second column from the left is the correct object label and the classification probability produced for that label. The other five columns show probabilities for the VGG-19’s top five classifications, ordered left to right in terms of the probability given by the network. Correct classifications are shaded in blue.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g014" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g015" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g015</object-id>
<label>Fig 15</label>
<caption>
<title>VGG-19 classifications for object outlines Part 2.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g015" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g016" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g016</object-id>
<label>Fig 16</label>
<caption>
<title>VGG-19 classifications for object outlines Part 3.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g016" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g017" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g017</object-id>
<label>Fig 17</label>
<caption>
<title>VGG-19 classifications for object outlines Part 4.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g017" xlink:type="simple"/>
</fig>
<p>For some reason, "corkscrew" was the VGG-19's first or second choice for 10 of the 40 objects, and it appeared among the top 5 choices for 15 objects. More understandable, perhaps, is the finding that "envelope" was the first or second choice for 14 of the objects, and this response appeared in the top 5 for 19 objects. These responses are most likely due to the local similarities between training photographs of a white envelope, possibly including black letters, and the tested outline images, which are also white with thin black lines. There is no evidence that the shape of the outline images is being extracted and classified using shape features acquired from training images with similar figure boundaries.</p>
</sec>
<sec id="sec015">
<title>Experiment 2 Discussion</title>
<p>Deep convolutional networks showed little capability to recognize objects with shapes defined by outline contours. Whereas the glass figurines had textures that were compatible with a different set of objects from their correct shape-object label, the texture in object outlines is not differentially diagnostic of any object classification, as it is identical to the surrounding background and identical throughout the object set. We hypothesized that if global shape information is extracted by the neural networks but was not used for glass figurines due to the strength of texture cues, object shape might play a larger role for outline images. The data do not support this hypothesis; classification performance was just as poor for object outlines as it was for pictures of glass objects.</p>
<p>The failure to classify objects correctly based on outline shape marks a clear divergence from human perception. All of the displays used in this experiment were consistently and accurately classified by human observers. We verified this with behavioral tests in which the outlines were shown together, as well as interleaved with photographs to confirm that humans have no trouble classifying object outlines, even in unexpected contexts. However, the results do not by themselves exclude some possibilities for use of shape information in DCNNs. That humans readily see shape in outline drawings is a remarkable fact in itself, and one that is not completely understood [<xref ref-type="bibr" rid="pcbi.1006613.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref032">32</xref>]. It would certainly be possible to envision a shape processing system that balked at outlines and used only surface edges. As introductory drawing teachers often stress, outlines are ecologically anomalous. Within biological vision, some perceptual processes seem to treat outlines and surface edges differently, as in perceptual completion [<xref ref-type="bibr" rid="pcbi.1006613.ref033">33</xref>]. Humans can see shape given by ordinary surface edges or outlines, but if deep networks cannot utilize the latter, it does not necessarily imply that surfaces edges do not play some role in classification in conjunction with texture features. Humans' fluent use of outlines to see object shape indicates the strong role of shape representation in human object recognition, and human interpretation of forms in outlines probably connects naturally to some stage of perceptual representation [<xref ref-type="bibr" rid="pcbi.1006613.ref034">34</xref>]. It appears that DCNNs differ from human processors in that they have little or no linkage between shape properties embodied in outlines and the classification labels in the output layer. DCNNs’ failure to use outlines does not, however, rule out the possibility that these systems may utilize some shape information in more natural cases.</p>
<p>These issues may relate to an important factor not yet mentioned. Figure-ground assignment, or equivalently, assignment of border ownership at occluding edges, is a well-known feature of human perceptual organization [<xref ref-type="bibr" rid="pcbi.1006613.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref036">36</xref>]. It appears that outlines, especially closed outlines, are interpreted in human vision as owning their borders. (The enclosed area is taken to be the bounded object.) DCNNs do not have an obvious way of representing figure vs. ground or border ownership. These seem to be more explicitly representational aspects of human perceptual processing. At least some of the problem with outlines may involve figure-ground issues. Misclassifications of objects as “hook”, “safety pin”, and “syringe”, which all have empty interior regions, suggest that DCNN's might be interpreting the actual outline as the object, rather than seeing an object as occupying the region within the outline.</p>
<p>On the other hand, results for a few of the images presented in Exp. 3, the beer bottle and the coffee mug, as well as a few objects like the tandem bicycle and trombone which are not correctly classified but are assigned probabilities significantly greater than chance, suggest that deep networks do not exclusively treat the black outlines as bodies of the objects themselves. If the networks were only treating the black outlines as the figure, these objects should have below-chance probability, as they do not have thin, black forms. Of course, we do not mean to imply that a DCNN employs any consistent approach or strategy; any good predictor from any of the many filters in the network, may influence the outcomes toward a correct classification. Perhaps these images have certain local features that can be extracted and further facilitate the recognition. DCNNs may not capture global shape, but may pick up some relatively local shape features, a possibility we discuss further in connection with later results.</p>
</sec>
</sec>
<sec id="sec016">
<title>Experiment 4</title>
<p>Experiments 2 and 3 found little evidence that deep convolutional networks access global shape in object recognition tasks. These results may seem surprising, as some recent reports have suggested that DCNNs do possess some shape classification abilities. Kubilius et al. [<xref ref-type="bibr" rid="pcbi.1006613.ref018">18</xref>] found that removing surface features from the Snodgrass and Vanderwart dataset of colored-in line drawings [<xref ref-type="bibr" rid="pcbi.1006613.ref037">37</xref>] did not totally destroy networks’ classification performance. With the regular line-drawing images, classification performance was 80–90%. Removal of color information reduced performance to around 70%, and removal of all inner surface gradients (black silhouettes) brought performance down to about 40%. It is arguable whether 40% classification accuracy represents a success or failure in shape-based classification for DCNNs. On the one hand, this marks a divergence from human performance, which is largely unaffected by the removal of color and inner surface gradient information from most objects. On the other hand, it seems almost impossible that the network would reach even 40% accuracy without some information about object shape. In contrast, our findings about recognition for glass objects and line drawings provided almost no evidence that shape representations are used for classification in DCNNs. In Experiment 4, we tried to replicate Kubilius et al.’s findings as a first step in clarifying this apparent discrepancy.</p>
<sec id="sec017">
<title>Experiment 3 Method</title>
<p><italic>Images</italic>. The same 40 object silhouettes found on the internet and used in Experiment 1 were used in Experiment 4, this time without any texture substitution. All images consisted of a single black figure on a white background. Half of the images were artifacts, and half were animals. The black figures were silhouettes of object drawings, rather than being taken from photographs of real objects with their textures removed, so some contour information that would typically be in a natural instance of the object was abstracted in the silhouette images. These abstractions have no effect on the objects’ recognizability to a human observer. See <xref ref-type="fig" rid="pcbi.1006613.g018">Fig 18</xref> for examples. We also tested the network on the same 40 silhouettes with white figures on a black background and red figures on a white background to measure the influence of surface color on network classification performance.</p>
<fig id="pcbi.1006613.g018" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g018</object-id>
<label>Fig 18</label>
<caption>
<title>Sample stimuli used in Experiment 4.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g018" xlink:type="simple"/>
</fig>
<p><italic>Networks</italic>. As in Experiments 2 and 3, we tested AlexNet and VGG-19 on the silhouette images.</p>
</sec>
<sec id="sec018">
<title>Experiment 3 results</title>
<p>For the 40 black silhouettes on a white background, VGG-19 and AlexNet correctly classified 20 and 15 of the 40 presented images (in their top-five classifications), respectively. Figs <xref ref-type="fig" rid="pcbi.1006613.g019">19</xref>–<xref ref-type="fig" rid="pcbi.1006613.g022">22</xref> shows the results for VGG-19. Performance was worse for images with white figures on black grounds, where the network classified seven of the 40 images correctly, and for images with red figures on white grounds, where the network classified nine of the 40 images correctly.</p>
<fig id="pcbi.1006613.g019" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g019</object-id>
<label>Fig 19</label>
<caption>
<title>VGG-19 classifications for black object silhouettes Part 1.</title>
<p>The leftmost column shows the image presented to VGG-19. The second column from the left shows the correct object label and the classification probability produced for that label. The other five columns show probabilities for the network’s top five classifications, ordered left to right in terms of the probability given by the network. Correct classifications are shaded in blue.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g019" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g020" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g020</object-id>
<label>Fig 20</label>
<caption>
<title>VGG-19 classifications for black object silhouettes Part 2.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g020" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g021" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g021</object-id>
<label>Fig 21</label>
<caption>
<title>VGG-19 classifications for black object silhouettes Part 3.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g021" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006613.g022" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g022</object-id>
<label>Fig 22</label>
<caption>
<title>VGG-19 classifications for black object silhouettes Part 4.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g022" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec019">
<title>Experiment 3 Discussion</title>
<p>The results from Experiment 4 are largely consistent with the findings reported by Kubilius et al. In the absence of other information, deep convolutional networks could classify object silhouettes about 50% of the time. In the better-performing VGG-19, there were 12 out of 40 correct top choices, and another eight correct choices in the network’s top-five selections. Classification accuracy was once again higher for artifacts (13/20 correct) than for animals (7/20 correct).</p>
<p>Performance was notably worse for white-on-black and red-on-white figures than for black-on-white figures. One reason for this might be that there are more canonically black objects in the training set than white or red. For example, the cannon is correctly classified when presented as a black figure, but incorrectly classified when presented as a red figure. Instead, “fire engine” appears in the network’s top-five choices, a selection obviously driven by surface properties. Another reason networks might be better at classifying black figures is that they more closely resemble photographic images that were used in network training. Objects will appear very dark or even black if they are between the camera and a bright light, as in actual silhouettes. Possibly, exposure to training examples like these makes the network more likely to accept dark figures as instances of an object, even one that is not canonically dark. The differences in network performance across these three testing sets points to the strong influence of surface information in classification. For humans, a homogenous surface texture would likely not be considered at all in recognition, as the visual system would recognize that there is not enough surface information present to be diagnostic. The network makes no such evaluation and remains highly sensitive to such cues.</p>
<p>Regarding shape, this experiment showed a clear contribution of contour properties in classification of object silhouettes. Within a given display set, all of the test displays shared the same coloration; therefore, all differences in classification responses from the DCNNs involved contour information. Performance at this level demands explanations that go beyond a simple conclusion that DCNNs do or do not process object shape. If DCNNs had access to global shape information, we might have expected their performance to be similar to humans, readily producing accurate classifications for all displays. Indeed, in about 50% of cases, responses seemed to be correct or close in shape to the target object. Still, the results fall far below 100%, even when scored correct if a network generated a correct classification among its top five outputs. The results also contain some rather conspicuous failures to process overall shape. For the porcupine display, for example, VGG-19 gave, as its top choice, "bald eagle". For "lion", AlexNet's top choice was "goose", with high confidence, more than twice the probability of any other response. These results contain important information regarding processing of overall shape. For humans, at least, the lion display is both recognizable as having the shape of a lion and is clearly not shaped at all like a goose. It could be pointed out that failure to give a certain label may merely indicate that the particular shape captured in silhouette may have had a particular vantage point that was uncharacteristic of examples in the training set. The implications for global shape processing here, however, hinge less on the selection of the correct name than on the incorrect answers furnished. “Bald eagle”, as well as runners up “vulture”, “ostrich”, and “buckeye” are not a close shape match for any porcupine.</p>
<p>These results suggest that overall shape is elusive in DCNN responses, but also that something relating to shape allows success in some of the cases tested. Perhaps a deeper analysis of what is meant by shape is needed to understand both the successes and failures of DCNNs. We consider this in successive steps below.</p>
<p>First, why is classification so much better for object silhouettes than for glass figures and shape outlines? We have already commented that the ability to use outlines as depicting shape, although significant in human perception, is not a necessary condition for a DCNN to be shape processor. To confirm that the difference between network performance on silhouettes and outlines was not item specific, we sampled the outline of the 40 silhouettes and tested the network on outline images of the stimuli in Experiment 4. The results closely matched those reported in Experiment 3—the network only classified three of the 40 images in its top-five selections.</p>
<p>What about the better performance of black silhouettes over glass objects? One reason may be that silhouettes successfully reduce distracting texture information, i.e., texture information that would tend to promote a classification other than the correct shape-based response. Glass objects may have contained more misleading surface information than black silhouettes. There are more objects with glass or other transparent or reflective surfaces in the 1000 categories on which the networks were trained than there are uniformly black objects. As mentioned, silhouettes also have the advantage of potentially looking similar to some photographs of the objects during network training if the photographs were taken at sunset or with a bright light behind the object. So, although we still see some misclassifications based on object color, such as “mortarboard” and “academic gown”, the network appears to be overall more robust to black surface textures.</p>
<p>Another important factor contributing to the differences in the previous three experiments is the involvement of figure-ground segmentation. Silhouettes are likely easier to classify than glass objects because the surface information provided contains <italic>no internal contours</italic>. Although figure-ground segmentation is an important part of human shape processing, it is likely that DCNNs produce object classifications with natural images without performing any explicit figure-ground segmentation. In human perception, bounding contours are defining of shape, and shape descriptions are conferred based on the bounding contours of segmented objects [<xref ref-type="bibr" rid="pcbi.1006613.ref038">38</xref>]. Without any figure-ground segmentation mechanism, all contours in training examples probably have equal status. Nothing designates a bounding contour relevant to overall shape, as opposed to contour information that may be part of surface texture, or noise, etc.</p>
<p>With displays stripped of all contour information except for bounding contours, the networks do better. This suggests that the networks must have used some information about the forms of objects to achieve the performance observed in Experiment 4, even though performance with silhouettes still falls far short of classification that humans readily do with shape. By comparison with the earlier experiments, it also suggests that important (bounding) contour information is more influential when no other contours are present.</p>
<p>What use is made of contour information? An important insight may be provided by the results for the black bear silhouette. The silhouette is substantially simplified such that key points of concavity along a bear outline are connected, mostly by straight lines. This is similar to a classic demonstration in object recognition, Attneave’s cat [<xref ref-type="bibr" rid="pcbi.1006613.ref039">39</xref>]. Attneave observed that humans can robustly recognize objects whose contour has been changed significantly at a local level, provided that at the global level the spatial relationships between important points along the contour are preserved. Deep networks do not appear to have the same capabilities. We suspect that they are doing essentially the reverse of humans with regard to global and local aspects of shape. We refer to this as the <italic>local contour feature hypothesis</italic>. The top five responses given by the bear silhouette appears to support this conjecture. The silhouette is confused not with other quadrupedal mammals, but with manufactured objects like “warplane”, “stretcher” and “studio couch”, suggesting that the network is using shape information, but at a local, not global level. Locally, there are no parts of a bear’s contour that are straight edges, so the network does not consider “black bear” a probable response, whereas an object like a warplane, though its global shape differs completely from the presented stimulus, has a similar set of local contour segments—straight edges and a few shorter rounded edges. Likewise, the network misclassifies the electric guitar as “shovel”, “hatchet”, or “assault rifle”. No human observer would make such errors, but it is easy to see how local curvature information might produce such responses if global form is disregarded.</p>
<p>The local contour feature hypothesis can also help clarify some positive results from Experiments 1–3. In Experiment 1, we found that deep networks use shape much more when classifying artifacts than when classifying animals. This was also observed in Experiment 4, where 13 of the 20 correctly identified objects were manmade. Artifacts will tend to be discriminable based on local features like curvature more often than animals because they are functional and have different component parts depending on their intended purpose. The hook of a hammer, for example, is highly diagnostic in discriminating it from other shafted tools, as is the curve of a French horn, or the long, thin tubes of a trombone. On the other hand, the local features of animals tend more often to be very similar to each other, with some exceptions such as the horn of a ram, or the legs of a flamingo. Two animals will tend to be more discriminable from each other based on global features, such as the ratio of neck length to body length, or ear size to head size.</p>
<p>In Experiment 2, the only figurines correctly classified were the piano and the great white shark. The network’s successful classification of the piano could be attributable to recognition of local contour features. In particular, the curvature of the top board of a grand piano is highly regular and could be driving the network towards that classification. On the other hand, the curvature of the top board was visible in two of the five additional grand piano figurines we tested on, and the network failed to classify these. For the great white shark, the local contour feature hypothesis fits well with the network’s pattern of response. “Warplane”, “hammerhead”, and “airliner” were all assigned higher probability than the correct object label for the shark figurine. These objects all have some local shape features in common. The fin and tail of a shark are fairly similar to the wings and tail of a plane, but the two objects are hardly confusable to humans. This is because humans group the shape features into a unified whole, while deep networks appear to be influenced by local feature aspects, but perhaps less so their relations to the whole.</p>
<p>In Experiment 3, the beer bottle and coffee mug were classified as “wine bottle” and “cup”. These were scored as correct, since the global shape of the objects are quite similar. Importantly, though, their local curvatures are also quite similar. In particular, the transition from the body to the neck of the beer bottle, and the handle of the coffee mug could be important local shape cues driving the network’s good performance. This seems all the more likely when one considers that “sunglass” is the network’s top classification for the coffee mug. Sunglasses share little global shape similarity with a coffee mug, but the curvature of a sunglass’s frame is often locally quite similar to a cup handle. Likewise, for the tandem bike and trombone, which were not correctly classified, but were assigned higher than chance probability, local contour features like thin straight lines for the trombone and constant curvature in the wheels for the bike could be driving classification. We directly test this hypothesis in Experiments 5.</p>
</sec>
</sec>
<sec id="sec020">
<title>Experiment 5</title>
<p>In Experiments 2–4, only the silhouette condition provides some supportive evidence that DCNNs use shape. However, further analysis of the networks’ classification of individual items in Experiments 2–4 suggest that some accurate classification decision is likely based on local contour features, not global object shape. We tested this hypothesis in Experiment 5, by comparing the effects of changing local and global features on network classification performance.</p>
</sec>
<sec id="sec021">
<title>Experiment 5a</title>
<p>In Experiment 5a, we explicitly tested the hypothesis that deep networks use local shape features, such as the curvature of contour segments, but not global shape, in their classification decisions. We found new examples of shape silhouettes that could be correctly classified and tested to see if the network can still classify them despite changes to their global contour. Preserving most local curvatures, we scrambled the shapes so that the overall shape was radically changed. If the network was robust to these alterations, that would provide evidence that the network is using local shape primitives as cues, rather than the shape’s global contour. If the network utilizes global shape, these disruptions should impede accurate classification.</p>
<sec id="sec022">
<title>Experiment 5a Method</title>
<p><italic>Images</italic>. Six images were presented, two from the silhouette database that had been correctly classified, plus four new object silhouettes whose correct label appeared in the network’s top-five selections. Parts of the object were rearranged so that the global contour no longer matched the correct object label, but local edges were preserved. <xref ref-type="fig" rid="pcbi.1006613.g023">Fig 23</xref> shows the stimulus objects before and after part-scrambling.</p>
<fig id="pcbi.1006613.g023" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g023</object-id>
<label>Fig 23</label>
<caption>
<title>Stimuli used in Experiment 5a.</title>
<p>Top row: the original silhouette images, all correctly classified by VGG-19 (appearing in top-five). Bottom row: Scrambled images on which the network was tested.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g023" xlink:type="simple"/>
</fig>
<p><italic>Network</italic>. The images were tested with VGG-19. We omitted simulations on AlexNet, since performance was reliably better in VGG-19 in Experiments 2–4.</p>
<p><italic>Tests on human subjects</italic>. In addition to evaluating the DCNN’s performance on the six scrambled object silhouettes, we tested human subjects to see if they could recognize the objects after scrambling.</p>
<p><italic>Participants</italic>. Ten human subjects (two male, eight female, M<sub>age</sub> = 19.5) were recruited from the University of California, Los Angeles and participated in the study for course credit.</p>
<p><italic>Design and procedure</italic>. Subjects completed two experiments. For the first experiment, on each trial, subjects were shown one of 30 silhouettes (24 different, unscrambled objects and the six part-scrambled objects), for one second. After presentation of each silhouette, subjects were asked to write down what they were shown on a piece of paper, after which they were free to continue to the next trial.</p>
<p>The second experiment was identical to the first, except that subjects’ exposure duration to each silhouette image was no longer limited to one second. Subjects could view the silhouette for as much time as they wanted before writing down what they believed the object to be on a piece of paper. Once they had recorded their response, they were free to continue to the next trial. As in the first experiment, the second experiment ended when all 30 silhouettes had been presented.</p>
</sec>
<sec id="sec023">
<title>Experiment 5a results</title>
<p>The top five responses and associated probabilities for part-scrambled objects are shown in <xref ref-type="fig" rid="pcbi.1006613.g024">Fig 24</xref>. Five of the six scrambled objects were “correctly” classified in the network’s top-five selections. The one correct object label that did not fall in the top-five was “shirt”, but “sweatshirt” did, and the misclassifications made by the network—“suit”, “bulletproof vest”, and “sweatshirt”, for example—are clearly influenced by similar local edge features.</p>
<fig id="pcbi.1006613.g024" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g024</object-id>
<label>Fig 24</label>
<caption>
<title>VGG-19 classifications for part-scrambled silhouettes.</title>
<p>The leftmost column shows the image presented to the DCNN. The second column shows the correct object label and the classification probability produced by the network for that label. The other five columns show probabilities for the network’s top five classifications, ordered left to right from highest to lowest. Correct classifications are shaded in blue.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g024" xlink:type="simple"/>
</fig>
<p>We also compared the probability associated with the correct response for the unscrambled image silhouettes with the probabilities for the scrambled silhouettes. Results are shown in <xref ref-type="fig" rid="pcbi.1006613.g025">Fig 25</xref>. On average, the correct response was given a probability 2.26 times higher for unscrambled images than for scrambled images.</p>
<fig id="pcbi.1006613.g025" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g025</object-id>
<label>Fig 25</label>
<caption>
<title>VGG-19 for unscrambled and part-scrambled images.</title>
<p>Bars show probabilities for correct responses for each of the objects. Probability is plotted on a logarithmic scale to make small values visible.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g025" xlink:type="simple"/>
</fig>
<p><italic>Human object recognition</italic>. Among human participants, subjects correctly identified 23.33% of the part-scrambled objects and 96.67% of the unscrambled objects when their viewing time was restricted to one second. When viewing time was unrestricted, subjects correctly identified 36.67% of part-scrambled objects, and 94.8% of unscrambled objects. <xref ref-type="table" rid="pcbi.1006613.t001">Table 1</xref> shows subject accuracy for each of the individual part-scrambled objects.</p>
<table-wrap id="pcbi.1006613.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.t001</object-id>
<label>Table 1</label> <caption><title>Human observers’ performance on individual items for part-scrambled objects.</title></caption>
<alternatives>
<graphic id="pcbi.1006613.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="2">Proportion Correct Classification</th>
</tr>
<tr>
<th align="left">Part-Scrambled<break/>Object</th>
<th align="left">Display Time: 1 sec</th>
<th align="left">Display Time: Unlimited</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="background-color:#BFBFBF">Camel</td>
<td align="right" style="background-color:#BFBFBF">40%</td>
<td align="center" style="background-color:#BFBFBF">60%</td>
</tr>
<tr>
<td align="left">Hammer</td>
<td align="right">10%</td>
<td align="center">0%</td>
</tr>
<tr>
<td align="left" style="background-color:#BFBFBF">Microphone</td>
<td align="right" style="background-color:#BFBFBF">40%</td>
<td align="center" style="background-color:#BFBFBF">30%</td>
</tr>
<tr>
<td align="left">Warplane</td>
<td align="right">20%</td>
<td align="center">20%</td>
</tr>
<tr>
<td align="left" style="background-color:#BFBFBF">Shirt</td>
<td align="right" style="background-color:#BFBFBF">20%</td>
<td align="center" style="background-color:#BFBFBF">30%</td>
</tr>
<tr>
<td align="left">Violin</td>
<td align="right">40%</td>
<td align="center">80%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
<sec id="sec024">
<title>Experiment 5b</title>
<p>Experiment 5a tested deep networks’ ability to classify objects whose global shape was destroyed, with local curvature largely preserved. We hypothesized that if DCNNs did not classify based on global shape, performance would remain good, provided that enough local contour features were preserved in the scrambled objects. In Experiment 5b, we conducted a complementary study to test the hypothesis that if local contour features are disrupted, but global shape is preserved, network classification accuracy will suffer.</p>
<sec id="sec025">
<title>Experiment 5b Method</title>
<p><italic>Images</italic>. We used the same six silhouette images as in Exp. 5a. In their original format, VGG-19 correctly classified all six images. We disrupted the local contour features in each image by adding jagged edges to the bounding contour, creating a sawtooth effect. <xref ref-type="fig" rid="pcbi.1006613.g026">Fig 26</xref> shows the six original images and the sawtooth images.</p>
<fig id="pcbi.1006613.g026" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g026</object-id>
<label>Fig 26</label>
<caption>
<title>Stimuli used in Experiment 5b.</title>
<p>Top row: the original silhouette images, all correctly classified by the network. Bottom row: images with local contour features disrupted.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g026" xlink:type="simple"/>
</fig>
<p><italic>Network</italic>. The perturbed contour images were tested on VGG-19.</p>
<p><italic>Test on human subjects</italic>. As in Experiment 5a, we tested humans’ ability to recognize images with global shape preserved and local contour information disrupted.</p>
<p><italic>Participants</italic>. Ten participants (four male, six female, M<sub>age</sub> = 20.0) were recruited from the University of California, Los Angeles and participated in the study for course credit. None of the subjects who participated in Exp. 5a participated in this experiment.</p>
<p><italic>Design and procedure</italic>. The design for the study was identical to Experiment 5a in all respects except for the six disrupted stimuli that were presented. On those trials, subjects were shown the locally disrupted sawtooth images instead of the globally disrupted scrambled images.</p>
</sec>
<sec id="sec026">
<title>Experiment 5b results</title>
<p>The network’s top-five classification selections for each of the locally disrupted images are shown in <xref ref-type="fig" rid="pcbi.1006613.g027">Fig 27</xref>. None of the images were correctly classified, either as the first choice or anywhere among the network's top 5 choices, and the average rank of the correct label for perturbed contour images was 96.3.</p>
<fig id="pcbi.1006613.g027" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g027</object-id>
<label>Fig 27</label>
<caption>
<title>VGG-19 classifications for serrated edge silhouettes.</title>
<p>The leftmost column shows the image presented to the DCNN. The second column shows the correct object label and the classification probability produced by the network for that label. The other five columns show probabilities for the network’s top five classifications, ordered left to right from highest to lowest. Correct classifications are shaded in blue.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g027" xlink:type="simple"/>
</fig>
<p>The network performed much worse on images where local contour was disrupted but global shape preserved than with either unperturbed or globally scrambled images. The average probability assigned to shape labels was 58.73 times higher for unperturbed local contour images than for perturbed local contour images, and 26.0 times higher in globally scrambled images than for perturbed local contour images. <xref ref-type="fig" rid="pcbi.1006613.g028">Fig 28</xref> shows a comparison between these three conditions.</p>
<fig id="pcbi.1006613.g028" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.g028</object-id>
<label>Fig 28</label>
<caption>
<title>Comparison of VGG-19 performance for locally perturbed contours with unscrambled and part-scrambled images.</title>
<p>Bars show probabilities for correct responses for each of the objects. Probability is plotted on a logarithmic scale to make small values visible.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.g028" xlink:type="simple"/>
</fig>
<p><italic>Human object recognition</italic>. Human participants recognized objects very accurately when local contours were perturbed. When viewing time was limited to one second, subjects’ recognition accuracy on locally disrupted contour silhouettes was 91.67%. With unlimited viewing time, subjects’ accuracy was 96.67%. Subjects’ accuracy on individual items is shown in <xref ref-type="table" rid="pcbi.1006613.t002">Table 2</xref>. These results did not significantly differ from subjects’ performance on unperturbed object either for the brief presentation condition, <italic>t</italic>(18) = 0.98, <italic>p</italic> = .341, or for the unlimited presentation condition, <italic>t</italic>(18) = 0.82, <italic>p</italic> = .425.</p>
<table-wrap id="pcbi.1006613.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006613.t002</object-id>
<label>Table 2</label> <caption><title>Human observers’ performance on individual items for perturbed-contour objects.</title></caption>
<alternatives>
<graphic id="pcbi.1006613.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="2">Proportion Correct Classification</th>
</tr>
<tr>
<th align="left">Locally Perturbed Object</th>
<th align="left">Display Time: 1 sec</th>
<th align="left">Display Time: Unlimited</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="background-color:#BFBFBF">Camel</td>
<td align="center" style="background-color:#BFBFBF">100%</td>
<td align="center" style="background-color:#BFBFBF">100%</td>
</tr>
<tr>
<td align="left">Hammer</td>
<td align="center">100%</td>
<td align="center">100%</td>
</tr>
<tr>
<td align="left" style="background-color:#BFBFBF">Microphone</td>
<td align="center" style="background-color:#BFBFBF">90%</td>
<td align="center" style="background-color:#BFBFBF">100%</td>
</tr>
<tr>
<td align="left">Warplane</td>
<td align="center">90%</td>
<td align="center">90%</td>
</tr>
<tr>
<td align="left" style="background-color:#BFBFBF">Shirt</td>
<td align="center" style="background-color:#BFBFBF">100%</td>
<td align="center" style="background-color:#BFBFBF">100%</td>
</tr>
<tr>
<td align="left">Violin</td>
<td align="center">70%</td>
<td align="center">90%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec027">
<title>Experiment 5 Discussion</title>
<p>The results of Experiments 5a and 5b suggest robust and interesting differences between humans and DCNNs in the use of shape information. Humans mostly failed to correctly recognize the objects presented in Experiment 5a. For one or two of the objects, they saw some recognizable parts (e.g., for the scrambled violin, one subject said “I see some pieces of a broken guitar”). Even where some parts could be recognized, every human observer pointed out that the scrambled objects are not actual versions of the label that would be correct for the unscrambled versions. In contrast, when human subjects were shown objects whose global shape was preserved, but whose local contour features were changed, they performed extremely well on the recognition task, both with short exposures and unlimited viewing time.</p>
<p>The DCNN results showed the reverse pattern of the human results. For VGG-19, classification for objects whose global shape had been destroyed by local scrambling remained very strong. However, in the local contour disruption condition, the network’s classification accuracy fell off dramatically. Despite preservation of global shape that allowed essentially perfect human object classification for both brief exposures and unlimited viewing, VGG-19 did not select any correct label for any object in Exp. 5b among its top five choices. The influence of local contour information is clearly visible in many of the network’s top-five selections from this simulation. Animals like poodles, curly-coated retrievers, and Scotch terriers are often selected despite no similarity in global shape, likely because the local features resemble a curly-furred animal. These results suggest that the network’s processing of shape is restricted to relatively local features along an object’s bounding contour, but not the shape as a whole. They also represent a clear difference between human and deep network recognition processes. Whereas local contour features seem to play a key role in deep network classification, global shape seems to have primacy in human perception.</p>
<p>These results, along with those of the earlier experiments, push us to think about what is really meant by shape. The convolution operations that form the groundwork for DCNNs are certainly capable of responding to local oriented contrast, as such filters have long been used to model orientation sensitivity [<xref ref-type="bibr" rid="pcbi.1006613.ref035">35</xref>]. For a shape feature such as the “claw” part of the hammer image, it would be sufficient for the network to encode a few orientations in proximity and in a certain spatial relation. These are, undoubtedly, aspects of “shape”. And our results suggest that they are accessible within the trained AlexNet and VGG-19 networks. There are other, more global, notions of shape, however. Larger relations of parts, and overall characteristics, such as aspect ratios, may generally be inaccessible. The network was remarkably undeterred by serious scrambling of parts, despite the fact that this scrambling undoubtedly also disrupted some local features of the sort we have noted here.</p>
<p>The issue of local shape features vs. global shape descriptions bears an important relation to questions of subsymbolic and symbolic representations in visual perception [<xref ref-type="bibr" rid="pcbi.1006613.ref036">36</xref>]. A number of phenomena indicate that human vision produces abstract shape representations that capture the gist of objects and support similarity relationships, despite local noise, and variations in local elements [<xref ref-type="bibr" rid="pcbi.1006613.ref040">40</xref>]. Even the perception of a clear object edge or the notion of a continuous contour are abstractions from earlier inputs of local oriented contrast in various locations and different spatial frequencies in the same location (for discussion, see [<xref ref-type="bibr" rid="pcbi.1006613.ref041">41</xref>]). Conversely, measurement of oriented contrast in two nearby locations and use of some conjunction of orientations in nearby positions can be done without conversion to a symbolic representation. These are the beginnings of shape descriptions, or perhaps even local shape descriptions. Deep networks may be accessing shape features that are conjunctions of local orientations and their relations; indeed, such representations may lie somewhere in the transition between subsymbolic and symbolic representations of an object’s shape (see [<xref ref-type="bibr" rid="pcbi.1006613.ref042">42</xref>] for a related proposal regarding the representation of contour shape). Taken together with the results of Experiments 1–4, the findings from Experiment 5 are consistent with the idea that DCNNs access aspects of local edge and curvature information, but not a representation of how local parts spatially relate to each other as a whole.</p>
</sec>
</sec>
</sec>
<sec id="sec028" sec-type="conclusions">
<title>Discussion</title>
<p>The purpose of this work was to determine the extent to which shape information is represented and used for recognition in trained deep convolutional networks. We tested deep networks trained on ImageNet to determine if shape information is relevant at all to DCNN object recognition performance, and if it is, how shape cues are weighted compared to other information, such as texture and context.</p>
<p>In Experiment 1, we tested VGG-19 for sensitivity to shape apart from appropriate texture and surface information, showing the network 40 object silhouettes with a different object’s surface texture overlaid on each shape. Evidence for use of shape information was weak: The correct label based on shape was chosen as the first choice classification for only 5 of the 40 objects sampled, and the average rank among network outputs for the correct shape was 209. Where evidence of shape influences on classification did appear, however, they seemed to depend highly on the kind of object being classified. While classification of artifacts and rigid objects appeared to depend on shape more than on texture cues, the opposite was true for images of animals. Even for artifacts in which the network weights shape more strongly, the absence of typical surface texture greatly reduced the network’s classification accuracy, resulting in many spurious classifications. The importance of texture information in object recognition was also seen in Experiment 2, where glass objects with no shape similarity to the presented stimuli were selected preferentially over the object whose shape matched the glass figurine, and in Experiment 4, where performance changed dramatically depending on whether a silhouette was black, white, or red.These results differ greatly from what is observed in research on human vision. Several studies (e.g., [<xref ref-type="bibr" rid="pcbi.1006613.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1006613.ref033">33</xref>]) have found that texture plays little role in facilitating human object recognition.</p>
<p>In Experiments 2 and 3, we presented two networks with images whose shape matched object categories that the network had been trained to recognize, while differing in terms of context and surface texture. Deep networks were in general unable to classify glass figurines or object outlines in the absence of other cues that are typically present in natural images. Across both datasets, only four of 80 presented images were correctly classified, and analyses of the probabilities assigned to each object label revealed that the network was assigning near-minimum probabilities to the correct objects in all but a few cases.</p>
<p>A possible explanation for the network’s poor performance in Experiment 3 is that the outline images are flat, lacking volumetric cues that might be present in 2D photographs of real objects. If the network depends on volumetric cues to recognize objects, the flatness of the outline images might prevent it from matching these images to its trained categories. This is an intriguing hypothesis, but the network was no better for glass figurines than for object outlines, even though figurines have 3D structures that would be quite similar to those of objects the network is trained to recognize. Moreover, the network did comparatively well in Experiment 4 classifying object silhouettes, which also lack any volumetric properties.</p>
<p>Experiment 4 found some evidence for use of shape information in object recognition by deep networks. Networks were presented with object silhouettes and correctly classified 20 of the 40 images (based on the top-five criterion), as well as assigning non-minimum probabilities to several others. One explanation for the networks’ superior performance for silhouettes is that these displays more fully eliminate competing surface texture information than do objects with glass surfaces, or objects whose texture exactly matched the background. They also remove orientation information that is not part of an object's bounding contour, thus preserving the orientation information most likely to be diagnostic of an object's shape. This selective preservation of orientation is probably especially helpful, as it seems unlikely that DCNNs trained for object classification have any differential representation of a bounding contour vs. any other contour.</p>
<p>Classification of object silhouettes would not be possible without some edge-based recognition capabilities, but it is unlikely that deep networks accessed global form for the task. Instead, the network appears to be to have extracted contour segments and local features based on some relations of proximate contour orientations. Indirect evidence supporting this explanation appeared in some of the network’s misclassifications. For example, a great white shark figurine had warplane and airliner in its top-five labels, likely due to the local similarity between a shark’s fins and a plane’s wings. Likewise, a porcupine was misclassified as a bird, probably because its spines had features in common with feathers, although globally birds and porcupines differ greatly.</p>
<p>Experiment 5 sought direct evidence for the local contour feature hypothesis. In Experiment 5a we used silhouette images, including some of the best performing ones from Experiment 4, for which the networks produced accurate classification outcomes. These images were then scrambled in such a way that curvature of local edge segments was preserved, but the shape as a whole was radically altered. Networks performed nearly as well on these scrambled objects as they did on the original images. On the other hand, in Experiment 5b, when global shape was preserved but local contour features were changed by adding jagged edges, network classification became extremely poor. These results suggest that the features relating to shape that figure in recognition in deep networks relate to curvature or orientation relations of highly local parts of the contour. The network recognizes the object by the mere presence of these features, not by how they might globally relate to each other in space. The convolution operations that occur at the earliest levels of DCNNs are well suited to extract local oriented contrast, and conjunctions of nearby orientations could serve as a serviceable marker for local curvature, as well as local shape features. This is why rearranging existing parts of a silhouette has little effect on network performance (since local feature extraction is the same) while the addition of a serrated border to the silhouette destroys network performance. The opposite is true in human perception.</p>
<p>In human vision, the transition from subsymbolic to symbolic processing depends critically on how encoded features fit together into a unified whole [<xref ref-type="bibr" rid="pcbi.1006613.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006613.ref045">45</xref>]. Identifying the curvature of contour segments may be an important first step in this transition, but it cannot account for the robust capabilities of our visual system to classify objects based on their shape. For example, Attneave [<xref ref-type="bibr" rid="pcbi.1006613.ref039">39</xref>], showed that all local curvature segments in a cat can be straightened without detriment to its recognizability. We actually tested one image, a bear, that had straightened segments, much like Attneave's cat, in Experiments 1 and 4. The probabilities assigned by the network were less than chance with both inconsistent texture, in Exp. 1 (.04%), and in silhouette form, in Exp. 4 (.03%). Consistent with these ideas, the bear image in both cases was unequivocally seen as a bear by human observers. This and many other phenomena of human perception suggest that relationships between contour segments and object parts are central to human object recognition and shape perception, more so than encoding local features.</p>
<p>In the introduction, we mentioned three levels of questions that might be asked regarding DCNNs and the use of shape information. The present results apply to the first level: the important question of understanding the role of shape in DCNNs trained to do object recognition. We believe that our results provide a good picture of the use of shape information by trained DCNNs, specifically suggesting a profound difference between the accessibility of local shape features and more global aspects of shape. The results of this study appear to strongly suggest that such models do not classify based on an object’s global shape cues. These results regarding the shape sensitivity of trained networks are important for comparisons of human perception with DCNNs, which in other respects have been argued to mirror the human visual brain and human behavior in a remarkable number of ways.</p>
<p>We noted earlier a second level of question–whether DCNNs can be trained from scratch to classify based on global shape information. Our present results clearly suggest that standard training on the ImageNet database does not produce such capabilities, but it leaves open the question of whether deep networks are <italic>incapable</italic> of classification based on global shape features. The photographs in ImageNet tend to show a single object in the foreground of the image. It is possible that training conditions in which the object is displayed in a more complete scene would produce better shape sensitivity because the network would learn to extract the spatial relationships between objects. Another way the network might be trained to encode global shape would be to include training examples with nondiagnostic texture and local shape properties. The network might have too many other rich cues for classification to develop global shape sensitivity under standard training conditions, but perhaps could encode global shape if deprived of some of these other information streams. Our suspicion is that even under training conditions specifically selected to develop global shape representations, DCNNs will not be able to classify based on global shape alone, but these hyotheses can only be confirmed through experimentation on untrained neural networks.</p>
<sec id="sec029">
<title>Conclusion</title>
<p>In human vision, abstract representations of shape describe how the various parts of an object spatially relate to one another. They are critical for recognition across a variety of viewing conditions, and are robust to perturbations of local contour features. Deep networks have impressive capabilities for object recognition, but they do not appear to handle the problem of recognition the same way humans do. Unlike humans, surface texture appears to be an equally strong cue for recognition as shape. Moreover, the shape information used by by deep networks is highly limited: DCNNs appear to be capable of encoding local shape features including local edge segments and relations. Sensitivity to how these local features fit together as a whole is lacking; DCNNs trained for object recognition do not appear to represent global shape at all.</p>
</sec>
</sec>
<sec id="sec030" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec031">
<title>Ethics statement</title>
<p>All research on human subjects in this and subsequent experiments was under IRB approval (IRB#11-002079-CR-00001).</p>
</sec>
<sec id="sec032">
<title>Network</title>
<p>Tests were conducted on VGG-19 [<xref ref-type="bibr" rid="pcbi.1006613.ref027">27</xref>] and AlexNet [<xref ref-type="bibr" rid="pcbi.1006613.ref004">4</xref>]. For most experiments, the results of the better-performing VGG-19 are reported. Both networks were trained on the ImageNet database prior to testing. Tests were conducted using the Neural Networks Toolbox from Matlab 2017b.</p>
</sec>
<sec id="sec033" sec-type="materials|methods">
<title>Materials</title>
<p>Testing images are described in the Methods description for individual experiments. In all experiments, the correct classification for the testing images was among the 1000 object categories that the networks had been trained to classify.</p>
</sec>
</sec>
<sec id="sec034">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006613.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Results from outlines sampled from Exp. 4 silhouettes.</title>
<p>The leftmost column shows the image presented to VGG-19. The second column from the left shows the correct object label and the classification probability produced for that label. The other five columns show probabilities for the network’s top five classifications, ordered left to right in terms of the probability given by the network. Correct classifications are shaded in blue.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006613.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006613.s002" xlink:type="simple">
<label>S2 File</label>
<caption>
<title>URLS For test images.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006613.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yampolskiy</surname> <given-names>R</given-names></name>. <article-title>Turing test as a defining feature of AI-completeness</article-title>. <source>Artificial Intelligence, Evolutionary Computing and Metaheuristics</source>. <year>2013</year>:<fpage>3</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turing</surname> <given-names>AM</given-names></name>. <article-title>Computing machinery and intelligence</article-title>. <source>Mind</source>. <year>1950</year> <month>Oct</month> <day>1</day>;<volume>59</volume>(<issue>236</issue>):<fpage>433</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Geman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hallonquist</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Younes</surname> <given-names>L</given-names></name>. <article-title>Visual turing test for computer vision systems</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year> <month>Mar</month> <day>24</day>;<volume>112</volume>(<issue>12</issue>):<fpage>3618</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>In: Advances in Neural Information Processing Systems</source>, <year>2012</year> (pp. <fpage>1097</fpage>–<lpage>1105</lpage>).</mixed-citation></ref>
<ref id="pcbi.1006613.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gunji</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Higuchi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Yasumoto</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Muraoka</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ushiku</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Harada</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kuniyoshi</surname> <given-names>Y</given-names></name>. <source>Scalable multiclass object categorization with Fisher based features</source>. ILSVRC <year>2012</year>, <publisher-name>The Univ. of Tokyo</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Haffner</surname> <given-names>P</given-names></name>. <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source>. <year>1998</year> <month>Nov</month>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>324</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Greenlee</surname> <given-names>MW</given-names></name>. <article-title>Estimating receptive field size from fMRI data in human striate and extrastriate visual cortex</article-title>. <source>Cerebral Cortex</source>. <year>2001</year> <month>Dec</month> <day>1</day>;<volume>11</volume>(<issue>12</issue>):<fpage>1182</fpage>–<lpage>90</lpage>. <object-id pub-id-type="pmid">11709489</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname> <given-names>CE</given-names></name>. <article-title>Shape representation in area V4: position-specific tuning for boundary conformation</article-title>. <source>Journal of Neurophysiology</source>. <year>2001</year> <month>Nov</month> <day>1</day>;<volume>86</volume>(<issue>5</issue>):<fpage>2505</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.2001.86.5.2505" xlink:type="simple">10.1152/jn.2001.86.5.2505</ext-link></comment> <object-id pub-id-type="pmid">11698538</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref009"><label>9</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Pospisil</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bair</surname> <given-names>W</given-names></name>. <chapter-title>Comparing the brainʼs representation of shape to that of a deep convolutional neural network</chapter-title>. In: <source>Proceedings of the 9th EAI International Conference on Bio-inspired Information and Communications Technologies (formerly BIONETICS) on 9th EAI International Conference on Bio-inspired Information and Communications Technologies (formerly BIONETICS) 2016 May 24 (pp. 516–523)</source>. <publisher-name>ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year> <month>Jun</month> <day>10</day>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year> <month>Nov</month> <day>6</day>;<volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003915" xlink:type="simple">10.1371/journal.pcbi.1003915</ext-link></comment> <object-id pub-id-type="pmid">25375136</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref012"><label>12</label><mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L. Imagenet: A large-scale hierarchical image database. In: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on 2009 Jun 20 (pp. 248–255). IEEE.</mixed-citation></ref>
<ref id="pcbi.1006613.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dubey</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Peterson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Ghanem</surname> <given-names>B</given-names></name>. <article-title>What makes an object memorable?</article-title> In: <source>Proceedings of the IEEE International Conference on Computer Vision</source> <year>2015</year> (pp. <fpage>1089</fpage>–<lpage>1097</lpage>).</mixed-citation></ref>
<ref id="pcbi.1006613.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peterson</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>. <source>Adapting deep network features to capture psychological representations. arXiv preprint arXiv:1608.02164</source>. <year>2016</year> <month>Aug</month> <day>6</day>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>SE</given-names></name>. <source>Vision science: Photons to phenomenology</source>. <publisher-name>MIT Press</publisher-name>; <year>1999</year> <month>Apr</month> <day>14</day>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>. <article-title>Recognition-by-components: a theory of human image understanding</article-title>. <source>Psychological Review</source>. <year>1987</year> <month>Apr</month>; <volume>94</volume>(<issue>2</issue>):<fpage>115</fpage>. <object-id pub-id-type="pmid">3575582</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nishihara</surname> <given-names>HK</given-names></name>. <article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source>. <year>1978</year> <month>Feb</month> <day>23</day>; <volume>200</volume>(<issue>1140</issue>):<fpage>269</fpage>–<lpage>94</lpage>. <object-id pub-id-type="pmid">24223</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bracci</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>de Beeck</surname> <given-names>HP</given-names></name>. <article-title>Deep neural networks as a computational model for human shape sensitivity</article-title>. <source>PLoS Computational Biology</source>. <year>2016</year> <month>Apr</month> <day>28</day>;<volume>12</volume>(<issue>4</issue>):<fpage>e1004896</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004896" xlink:type="simple">10.1371/journal.pcbi.1004896</ext-link></comment> <object-id pub-id-type="pmid">27124699</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Szegedy</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zaremba</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bruna</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Erhan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Fergus</surname> <given-names>R</given-names></name>. <source>Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199</source>. <year>2013</year> <month>Dec</month> <day>21</day>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nguyen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yosinski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Clune</surname> <given-names>J</given-names></name>. <article-title>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</article-title>. <source>In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> <year>2015</year> (pp. <fpage>427</fpage>–<lpage>436</lpage>).</mixed-citation></ref>
<ref id="pcbi.1006613.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Xie</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yuille</surname> <given-names>AL</given-names></name>. <source>Object Recognition with and without Objects. arXiv preprint arXiv:1611.06596</source>. <year>2016</year> <month>Nov</month> <day>20</day>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Khosla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Scientific Reports</source>. <year>2016</year> <month>Jun</month> <day>10</day>;<volume>6</volume>:<fpage>27755</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep27755" xlink:type="simple">10.1038/srep27755</ext-link></comment> <object-id pub-id-type="pmid">27282108</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ardila</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Majaj</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year> <month>Dec</month> <day>18</day>;<volume>10</volume>(<issue>12</issue>):<fpage>e1003963</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003963" xlink:type="simple">10.1371/journal.pcbi.1003963</ext-link></comment> <object-id pub-id-type="pmid">25521294</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MA</given-names></name>. <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year> <month>Jul</month> <day>8</day>;<volume>35</volume>(<issue>27</issue>):<fpage>10005</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5023-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5023-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26157000</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kümmerer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Theis</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <source>Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet</source>. arXiv preprint arXiv:1411.1045. <year>2014</year> <month>Nov</month> <day>4</day>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ithapu</surname> <given-names>VK</given-names></name>. <article-title>Decoding the Deep: Exploring class hierarchies of deep representations using multiresolution matrix factorization</article-title>. In: <source>CVPR Workshop on Explainable Computer Vision and Job Candidate Screening Competition</source> <year>2017</year> <month>Jul</month> <day>1</day> (Vol. <volume>2</volume>).</mixed-citation></ref>
<ref id="pcbi.1006613.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simonyan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zisserman</surname> <given-names>A</given-names></name>. <source>Very deep convolutional networks for large-scale image recognition</source>. arXiv preprint arXiv:1409.1556. <year>2014</year> <month>Sep</month> <day>4</day>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ju</surname> <given-names>G</given-names></name>. <article-title>Surface versus edge-based determinants of visual recognition</article-title>. <source>Cognitive Psychology</source>. <year>1988</year> <month>Jan</month> <day>31</day>;<volume>20</volume>(<issue>1</issue>):<fpage>38</fpage>–<lpage>64</lpage>. <object-id pub-id-type="pmid">3338267</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davidoff</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Ostergaard</surname> <given-names>AL</given-names></name>. <article-title>The role of colour in categorial judgements</article-title>. <source>The Quarterly Journal of Experimental Psychology Section A</source>. <year>1988</year> <month>Aug</month>;<volume>40</volume>(<issue>3</issue>):<fpage>533</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Velisavljević</surname> <given-names>L</given-names></name>. <article-title>Cue dynamics underlying rapid detection of animals in natural scenes</article-title>. <source>Journal of Vision</source>. <year>2009</year> <month>Jul</month> <day>1</day>;<volume>9</volume>(<issue>7</issue>):<fpage>7</fpage>–. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/9.7.7" xlink:type="simple">10.1167/9.7.7</ext-link></comment> <object-id pub-id-type="pmid">19761322</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bergevin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Levine</surname> <given-names>MD</given-names></name>. <article-title>Generic object recognition: Building and matching coarse descriptions from line drawings</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>1993</year> <month>Jan</month>;<volume>15</volume>(<issue>1</issue>):<fpage>19</fpage>–<lpage>36</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lloyd-Jones</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Luckhurst</surname> <given-names>L</given-names></name>. <article-title>Outline shape is a mediator of object recognition that is particularly important for living things</article-title>. <source>Memory &amp; Cognition</source>. <year>2002</year> <month>Jun</month> <day>1</day>;<volume>30</volume>(<issue>4</issue>):<fpage>489</fpage>–<lpage>98</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kellman</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Shipley</surname> <given-names>TF</given-names></name>. <article-title>A theory of visual interpolation in object perception</article-title>. <source>Cognitive Psychology</source>. <year>1991</year> <month>Apr</month> <day>30</day>;<volume>23</volume>(<issue>2</issue>):<fpage>141</fpage>–<lpage>221</lpage>. <object-id pub-id-type="pmid">2055000</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Brooks</surname> <given-names>V</given-names></name>. <article-title>Pictorial recognition as an unlearned ability: A study of one child's performance</article-title>. <source>The American Journal of Psychology</source>. <year>1962</year> <month>Dec</month> <day>1</day>;<volume>75</volume>(<issue>4</issue>):<fpage>624</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>. <source>Vision. 1982. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source>. <year>1982</year>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koffka</surname> <given-names>K</given-names></name>. <source>Principles of Gestalt Psychology, International Library of Psychology, Philosophy and Scientific Method</source>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Snodgrass</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Vanderwart</surname> <given-names>M</given-names></name>. <article-title>A standardized set of 260 pictures: norms for name agreement, image agreement, familiarity, and visual complexity</article-title>. <source>Journal of Experimental Psychology: Human Learning and Memory</source>. <year>1980</year> <month>Mar</month>;<volume>6</volume>(<issue>2</issue>):<fpage>174</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref038"><label>38</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kellman</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Garrigan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Erlikhman</surname> <given-names>G</given-names></name>. <chapter-title>Challenges in Understanding Visual Shape Perception and Representation: Bridging Subsymbolic and Symbolic Coding</chapter-title>. <source>In Shape Perception in Human and Computer Vision</source> <year>2013</year> (pp. <fpage>249</fpage>–<lpage>274</lpage>). <publisher-name>Springer</publisher-name>, <publisher-loc>London</publisher-loc>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attneave</surname> <given-names>F</given-names></name>. <article-title>Some informational aspects of visual perception</article-title>. <source>Psychological Review</source>. <year>1954</year> <month>May</month>;<volume>61</volume>(<issue>3</issue>):<fpage>183</fpage>. <object-id pub-id-type="pmid">13167245</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baker</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kellman</surname> <given-names>PJ</given-names></name>. <article-title>Abstract shape representation in human visual perception</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>2018</year> <month>Sep</month>;<volume>147</volume>(<issue>9</issue>):<fpage>1295</fpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kellman</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Massey</surname> <given-names>CM</given-names></name>. <article-title>Perceptual learning, cognition, and expertise</article-title>. <source>The Psychology of Learning and Motivation</source>. <year>2013</year> <month>Jan</month> <day>1</day>;<volume>58</volume>:<fpage>117</fpage>–<lpage>65</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref042"><label>42</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kellman</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Garrigan</surname> <given-names>P</given-names></name>. <chapter-title>Segmentation, grouping, and shape: some Hochbergian questions</chapter-title>. <source>Perception: Essays in Honor of Julian Hochberg</source> Ed. <name name-style="western"><surname>Peterson</surname> <given-names>MA</given-names></name> (<publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>) pp. <year>2006</year>:<fpage>542</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Presnell</surname> <given-names>LM</given-names></name>. <article-title>Color diagnosticity in object recognition</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>. <year>1999</year> <month>Aug</month> <day>1</day>;<volume>61</volume>(<issue>6</issue>):<fpage>1140</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006613.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rossion</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pourtois</surname> <given-names>G</given-names></name>. <article-title>Revisiting Snodgrass and Vanderwart's object pictorial set: The role of surface detail in basic-level object recognition</article-title>. <source>Perception</source>. <year>2004</year> <month>Feb</month>;<volume>33</volume>(<issue>2</issue>):<fpage>217</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p5117" xlink:type="simple">10.1068/p5117</ext-link></comment> <object-id pub-id-type="pmid">15109163</object-id></mixed-citation></ref>
<ref id="pcbi.1006613.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wertheimer</surname> <given-names>M</given-names></name>. <article-title>Laws of organization in perceptual forms</article-title>. <source>A source book of Gestalt Psychology</source>. <year>1923</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>