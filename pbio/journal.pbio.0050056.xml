<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pbio</journal-id><journal-id journal-id-type="allenpress-id">plbi</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="ppub">1544-9173</issn><issn pub-type="epub">1545-7885</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="doi">10.1371/journal.pbio.0050056</article-id><article-id pub-id-type="publisher-id">06-PLBI-RA-1746R2</article-id><article-id pub-id-type="sici">plbi-05-03-09</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Neuroscience</subject>
        </subj-group>
        <subj-group subj-group-type="System Taxonomy">
          <subject>Homo (human)</subject>
        </subj-group>
      </article-categories><title-group><article-title>Auditory Short-Term Memory Behaves Like Visual Short-Term Memory</article-title><alt-title alt-title-type="running-head">Auditory Memory Behaves Like Visual Memory</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Visscher</surname>
            <given-names>Kristina M</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kaplan</surname>
            <given-names>Elina</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kahana</surname>
            <given-names>Michael J</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Sekuler</surname>
            <given-names>Robert</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
				<label>1</label><addr-line> Volen Center for Complex Systems, Brandeis University, Waltham, Massachusetts, United States of America
			</addr-line></aff><aff id="aff2">
				<label>2</label><addr-line> Department of Psychology, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America
			</addr-line></aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Shamma</surname>
            <given-names>Shihab</given-names>
          </name>
          <role>Academic Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University of Maryland, United States of America</aff><author-notes>
        <fn fn-type="con" id="ack1">
          <p> KMV, EK, MJK, and RS conceived and designed the experiments. KMV and EK performed the experiments. KMV analyzed the data and contributed reagents/materials/analysis tools. KMV and RS wrote the paper.</p>
        </fn>
        <corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">visscher@brandeis.edu</email></corresp>
      <fn fn-type="conflict" id="ack3">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="ppub">
        <month>3</month>
        <year>2007</year>
      </pub-date><pub-date pub-type="epub">
        <day>20</day>
        <month>2</month>
        <year>2007</year>
      </pub-date><volume>5</volume><issue>3</issue><elocation-id>e56</elocation-id><history>
        <date date-type="received">
          <day>19</day>
          <month>9</month>
          <year>2006</year>
        </date>
        <date date-type="accepted">
          <day>20</day>
          <month>12</month>
          <year>2006</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2007</copyright-year><copyright-holder>Visscher et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Are the information processing steps that support short-term sensory memory common to all the senses? Systematic, psychophysical comparison requires identical experimental paradigms and comparable stimuli, which can be challenging to obtain across modalities. Participants performed a recognition memory task with auditory and visual stimuli that were comparable in complexity and in their neural representations at early stages of cortical processing. The visual stimuli were static and moving Gaussian-windowed, oriented, sinusoidal gratings (Gabor patches); the auditory stimuli were broadband sounds whose frequency content varied sinusoidally over time (moving ripples). Parallel effects on recognition memory were seen for number of items to be remembered, retention interval, and serial position. Further, regardless of modality, predicting an item's recognizability requires taking account of (1) the probe's similarity to the remembered list items (summed similarity), and (2) the similarity between the items in memory (inter-item homogeneity). A model incorporating both these factors gives a good fit to recognition memory data for auditory as well as visual stimuli. In addition, we present the first demonstration of the orthogonality of summed similarity and inter-item homogeneity effects. These data imply that auditory and visual representations undergo very similar transformations while they are encoded and retrieved from memory.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <sec id="st1">
          <title/>
          <p>Memories are not exact representations of the past. But can we say that all our senses are equally reliable (or unreliable) sources for memory? We performed a series of experiments to test that proposition. Sound and light are processed by different receptors and neural pathways in the brain. Previous comparisons of auditory and visual memory have done little to place on equal footing the stimuli that will be remembered, limiting the ability to truly compare the two processes. However, using current knowledge of how these sensations are represented in the nervous system, we created auditory and visual stimuli of similar complexity and that undergo similar initial processing by the nervous system. We then used these well-matched stimuli to examine memory for studied lists of either auditory or visual items. Using behavioral measures and a computational model for list memory, we show that memory representations are altered similarly for both hearing and vision. We found that auditory and visual memory exhibit striking parallels in terms of how memory is affected by all the parameters we changed in this experiment. These results imply that auditory and visual short-term memory employ similar mechanisms.</p>
        </sec>
      </abstract><abstract abstract-type="toc">
        <p>The mechanisms by which memories are encoded and retrieved share common principles between the visual and auditory systems of humans.</p>
      </abstract><funding-group><funding-statement>Supported by National Insititutes of Health (NIH) grants MH068404 and MH55687, and by NIH post-doctoral training grant T32 NS07292.</funding-statement></funding-group><counts>
        <page-count count="11"/>
      </counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group>
        <custom-meta>
          <meta-name>citation</meta-name>
          <meta-value>Visscher KM, Kaplan E, Kahana MJ, Sekuler R (2007) Auditory short-term memory behaves like visual short-term memory. PLoS Biol 5(3): e56. doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0050056" xlink:type="simple">10.1371/journal.pbio.0050056</ext-link></meta-value>
        </custom-meta>
      </custom-meta-group></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>In the past decade, cognitive science has spawned some powerful computational models for both the large-scale and detailed structure of many fundamental phenomena, including categorization and recognition. These models have enjoyed considerable success, particularly in accounting for recognition of simple visual stimuli, such as sinusoidal gratings and chromatic patches [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>–<xref ref-type="bibr" rid="pbio-0050056-b003">3</xref>], and more complex visual stimuli, such as realistic synthetic human faces [<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>]. By exploiting stimuli whose properties can be easily manipulated, but resist consistent verbal rehearsal strategies [<xref ref-type="bibr" rid="pbio-0050056-b005">5</xref>], researchers can formulate and test detailed predictions about visual recognition memory.</p>
      <p>To date, this effort has focused on vision, raising the possibility that the properties of recognition memory revealed thus far might be modality specific and therefore of limited generality. There are several prerequisites that must be satisfied before another sensory modality can be addressed in a comparable fashion. First, a suitable task must be found; second, a family of stimuli must be identified that can be parametrically varied along dimensions thought to be encoded in memory. In addition, baseline memory performance must be comparable across modalities, and the effect of early perceptual processing on the stimulus representations must be similar. Failure to satisfy any of these prerequisites would undermine inter-modal comparisons of memory.</p>
      <p>We decided to use Sternberg's recognition memory task, which had been used previously with visual stimuli and whose properties were well understood [<xref ref-type="bibr" rid="pbio-0050056-b006">6</xref>]. We then identified a family of auditory stimuli—moving ripple sounds—whose attributes resembled ones that had proven useful in modeling visual recognition memory. These auditory stimuli vary sinusoidally in both time and in frequency content, and are generated by superimposing sets of tones whose intensities are sinusoidally modulated. Until now, these stimuli have been mainly used to characterize the spectro-temporal response fields of neurons in mammalian primary auditory cortex [<xref ref-type="bibr" rid="pbio-0050056-b007">7</xref>–<xref ref-type="bibr" rid="pbio-0050056-b009">9</xref>], but because their spectro-temporal properties resemble those of human speech [<xref ref-type="bibr" rid="pbio-0050056-b007">7</xref>,<xref ref-type="bibr" rid="pbio-0050056-b010">10</xref>], moving ripple stimuli are well suited to probe human speech perception and memory with minimal contamination by semantic properties or by the strong boundaries between existing perceptual categories [<xref ref-type="bibr" rid="pbio-0050056-b011">11</xref>].</p>
      <p>This selection of stimuli was influenced by previous attempts to compare auditory and visual memory. Some of those attempts used auditory and visual stimuli that differed substantially in their early sensory processing, but shared semantic representations [<xref ref-type="bibr" rid="pbio-0050056-b012">12</xref>]. For example, Conrad and Hull's classic study compared memory for a list of digits presented either visually or as spoken items [<xref ref-type="bibr" rid="pbio-0050056-b013">13</xref>]. Initial processing differs tremendously for the two types of inputs, indicating that differences in memory may be due to the divergent initial processing. Further, with stimuli like these, once the items have been encoded into verbal form for storage in memory, shared semantic processes may obscure any fundamental differences in memory for the two modalities. Other experiments use stimuli that arguably are free from semantic influences, yet still fail to equate the early stages of processing required by the stimuli [<xref ref-type="bibr" rid="pbio-0050056-b014">14</xref>].</p>
      <p>We examined short-term memory for auditory and visual stimuli whose early sensory processing is comparable. Finding comparable stimuli across modalities is difficult, as it may initially seem incontrovertible that the brain operates differently upon auditory and visual inputs. Certainly the initial stages of processing by the modalities' respective receptors differ from one another in many ways. However, the transformations performed by the nervous system on the information generated by the auditory and visual receptors appear to be very similar [<xref ref-type="bibr" rid="pbio-0050056-b007">7</xref>,<xref ref-type="bibr" rid="pbio-0050056-b015">15</xref>]. Starting from each modality's sensory receptors and continuing to the modality's respective processing networks within the cerebral cortex, analogs between hearing and vision have been noted by several researchers [<xref ref-type="bibr" rid="pbio-0050056-b007">7</xref>,<xref ref-type="bibr" rid="pbio-0050056-b009">9</xref>,<xref ref-type="bibr" rid="pbio-0050056-b016">16</xref>,<xref ref-type="bibr" rid="pbio-0050056-b017">17</xref>]. To take a few examples, adjacent sensory receptors in the cochlea of the ear detect neighboring frequencies of sound the same way adjacent sensory receptors in the retina of the eye respond to light from neighboring locations in space. This analogy extends to the retinotopic/tonotopic structure and receptive fields of auditory and visual cortex.</p>
      <p>Both moving ripples and Gabor patches vary sinusoidally along the dimensions that primary sensory neurons encode. These stimuli are described in <xref ref-type="fig" rid="pbio-0050056-g001">Figure 1</xref>. Moving further along the processing hierarchy, it appears that primary auditory cortex responds to moving ripple stimuli analogously to the way primary visual cortex responds to Gabor patches: a few neurons respond robustly to the stimulus, but most are relatively quiet [<xref ref-type="bibr" rid="pbio-0050056-b009">9</xref>]. The sets of stimuli, therefore, are very well matched in terms of early sensory processing. In addition, to decrease reliance upon verbal rehearsal, these unfamiliar stimuli can be varied continuously, and do not support readily available verbal or semantic labels [<xref ref-type="bibr" rid="pbio-0050056-b005">5</xref>]. So we should expect results to be minimally influenced by semantic relationships among stimuli.</p>
      <fig id="pbio-0050056-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Auditory, Stationary Visual, and Moving Visual Stimuli</title>
          <p>(A and B) Spectro-temporal plots of two auditory moving ripple stimuli. The horizontal axis shows time in seconds; the vertical axis shows frequency content in Hertz (Hz). Brightness indicates the amplitude at a particular frequency and time; white represents sounds of greater amplitude. Modulations in frequency content are referred to as spectral density, defined by sinusoidal frequency Ω; modulations over time are referred to as the ripple's velocity, defined by sinusoidal frequency <italic>w</italic>. The ripple velocity for (A) is 16 Hz, and for (B), it is 8 Hz.</p>
          <p>(C) Image representing the static grating stimuli used. Horizontal and vertical axes represent actual spatial extent, in degrees visual angle from fixation, of visual stimuli.</p>
          <p>(D) Space–time plot of one moving grating stimulus. The horizontal axis shows time in seconds; the vertical axis shows the horizontal dimension in space <italic>(y)</italic>. Brightness indicates the luminance at that point on the display. Moving visual gratings looked like (C) in which the bars drifted upwards over time.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g001" xlink:type="simple"/>
      </fig>
      <p>Finally, to promote comparability in the difficulty of the memory task with auditory or visual stimuli, we adopted a strategy introduced by Zhou and colleagues [<xref ref-type="bibr" rid="pbio-0050056-b018">18</xref>]. Recognizing that the similarity relationships among visual stimuli strongly influenced recognition memory, those researchers adjusted each participants' memory test stimuli according to that participant's discrimination threshold. Their aim was to minimize individual differences on the memory task. We took the procedure one step further, adjusting stimuli separately within each modality according to each participant's discrimination threshold for that modality. This was meant to equate for both auditory and visual modalities the powerful influence that similarity exerts on memory.</p>
      <p>We present the results of two experiments. Experiment 1 assessed several basic properties of recognition memory for ripple stimuli and memory for Gabor patches; Experiment 2 used ripple stimuli to isolate the effects of summed probe-item similarity and inter-item homogeneity. The design of Experiment 2 was meant to orthogonalize these two potential influences on recognition memory, allowing the effects of summed similarity and inter-item homogeneity to be explored independently. A previously proposed model for visual memory, the Noisy Exemplar Model (NEMo) was fit to the data [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>]. Because so many trials were required for each case, and because the NEMo has been shown previously to fit data for visual stimuli quite well [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>], only auditory stimuli were used in Experiment 2.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Experiment 1: Basic Properties of Short-Term Recognition Memory</title>
        <p>Experiment 1 measured short-term recognition memory for moving ripple stimuli and for both moving as well as stationary Gabor patches. We used a variant of Sternberg's recognition task [<xref ref-type="bibr" rid="pbio-0050056-b006">6</xref>,<xref ref-type="bibr" rid="pbio-0050056-b019">19</xref>]. On each trial, one to four stimuli were sequentially presented, followed after some retention interval by a probe. The participants' task was to identify whether the probe matched any of the items presented in the list, pressing a button to indicate their choice. The use of the Sternberg paradigm for auditory stimuli allows comparisons to the many studies that have used the same paradigm with visual stimuli [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b019">19</xref>,<xref ref-type="bibr" rid="pbio-0050056-b020">20</xref>].</p>
        <p>Both moving and static visual Gabor patches were tested because although moving Gabor patches change in time similarly to the ripple sounds, their stationary counterparts have been extensively studied in psychophysical examinations of memory [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>]. We examined several basic properties of short-term memory for auditory and visual stimuli: the effect of the number of stimuli that must be remembered (list length), the interval over which those stimuli must be remembered (retention interval), and the serial position of the stimulus matching a probe.</p>
        <p>Each participant's data from trials of a given list length and retention interval were averaged to obtain a proportion correct for that combination of conditions. These were compared across participants using standard parametric statistics. Proportion correct measures were used rather than, for example, <italic>d′</italic> measures because in this case, the assumption that variances associated with target (probe matches a list item) and lure (probe does not match a list item) trials are identical is probably not defensible, as the range of summed probe-item similarities for target trials is much smaller than for lures (as by definition, Target trials always include a stimulus that is identical to the probe, with a similarity equal to 1) [<xref ref-type="bibr" rid="pbio-0050056-b021">21</xref>].</p>
        <sec id="s2a1">
          <title>Effects of length of the study list, retention interval, and serial position.</title>
          <p><xref ref-type="fig" rid="pbio-0050056-g002">Figure 2</xref> shows the proportion of correct responses made as a function of the length of the study list. Error bars show the within-participant standard error of the mean, taking out between-participant variability and between–stimulus type variability, and indicate results of the effect of list length by analysis of variance (ANOVA) [<xref ref-type="bibr" rid="pbio-0050056-b022">22</xref>,<xref ref-type="bibr" rid="pbio-0050056-b023">23</xref>]. Note that as the number of elements in the list increases, participants are correct less often. The effect of list length is significant in a 3 × 4 (stimulus types by list lengths) ANOVA (<italic>F</italic><sub>3,39</sub> = 32.5, <italic>p</italic> &lt; 0.0001). In addition, the overall proportion correct is different depending on the stimulus type (<italic>F</italic><sub>2,26</sub> = 29.2, <italic>p</italic> &lt; 0.0001). Participants' proportion correct was overall larger for the ripple sounds than for the grating stimuli in all conditions. This difference indicates that the sound stimuli chosen were more easily discriminable than the visual stimuli. The interaction of the effect of list length with stimulus type was nonsignificant (<italic>F</italic><sub>6,78</sub> = 1.53, <italic>p</italic> = 0.18,).</p>
          <fig id="pbio-0050056-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Effect of Length of the Study List</title>
              <p>Difference from mean proportion correct responses (<italic>P</italic>(correct) diff) is plotted as a function of length of the study list. Statistics show a significant effect of stimulus type and list length, but no interaction between them. The mean proportions correct subtracted for each stimulus type were: sound, 0.72; moving visual, 0.61; and stationary visual, 0.59.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g002" xlink:type="simple"/>
          </fig>
          <p><xref ref-type="fig" rid="pbio-0050056-g003">Figure 3</xref> shows that even as the retention interval goes to 9.7 s, the proportion correct changes less than 10%. This change is nonetheless significant in a 3 × 5 (stimulus types by retention intervals) ANOVA (<italic>F</italic><sub>4,52</sub> = 10.76, <italic>p</italic> &lt; 0.0001). As with all conditions, proportion correct was overall larger for ripple sounds (<italic>F</italic><sub>4,52</sub> = 26.12, <italic>p</italic> &lt; 0.0001). The interaction between stimulus type and retention interval was only marginally significant (<italic>F</italic><sub>8,104</sub> = 2.02, <italic>p</italic> = 0.051). Error bars show the within-participant standard error of the mean, taking out between-participant variability and between–stimulus type variability, indicating results of the effect of retention interval by ANOVA [<xref ref-type="bibr" rid="pbio-0050056-b022">22</xref>,<xref ref-type="bibr" rid="pbio-0050056-b023">23</xref>].</p>
          <fig id="pbio-0050056-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Effect of Retention Interval</title>
              <p>Difference from mean proportion correct responses (<italic>P</italic>(correct) diff) is plotted as a function of the retention interval. Proportion correct decreases modestly with retention intervals up to 9.7 s. The mean proportions correct subtracted for each stimulus type were: sound, 0.77; moving visual, 0.66; and stationary visual, 0.62.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g003" xlink:type="simple"/>
          </fig>
          <p><xref ref-type="fig" rid="pbio-0050056-g004">Figure 4</xref> shows the effect of serial position on recognition rate. For clarity, only the data for the four-stimulus case are shown. Effects were similar for the other list lengths. The most recently presented stimulus is recognized more often when it matches the probe than are earlier stimuli. For lists of four items, a 3 × 4 (stimulus types by serial positions) ANOVA showed no significant interaction between stimulus type and serial position (<italic>F</italic><sub>6,78</sub> = 1.3, <italic>p</italic> = 0.26). However, there was a highly significant effect of serial position (<italic>F</italic><sub>3,39</sub> = 24.3, <italic>p</italic> &lt; 0.0001), and an effect of stimulus type (<italic>F</italic><sub>2,26</sub> = 10.1, <italic>p</italic> &lt;0.001). Error bars show the within-participant standard error of the mean, taking out between-participant variability and between–stimulus type variability, indicating results of the effect of serial position by ANOVA [<xref ref-type="bibr" rid="pbio-0050056-b022">22</xref>,<xref ref-type="bibr" rid="pbio-0050056-b023">23</xref>].</p>
          <fig id="pbio-0050056-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Effect of Serial Position</title>
              <p>Difference from mean proportion correct responses (<italic>P</italic>(correct) diff) is plotted as a function of the serial position of the matching study item. Data are shown for the case in which four stimuli were presented. Statistics show a significant effect of serial position and list length, but no interaction between them. The mean proportions correct subtracted for each stimulus type were: sound, 0.72; moving visual, 0.62; and stationary visual, 0.61.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g004" xlink:type="simple"/>
          </fig>
          <p>There appears to be a slight trend towards a greater recency effect in the case of auditory stimuli than for the visual stimuli, so that later serial positions are remembered more accurately. Although this does not reach significance in the list length 4 or 2 cases, it is marginally significant in the list length 3 case, in which a 3 × 3 (stimulus types by serial positions) ANOVA reveals a slight interaction between stimulus type and serial position (<italic>F</italic><sub>4,52</sub> = 2.7, <italic>p</italic> = 0.04).</p>
        </sec>
      </sec>
      <sec id="s2b">
        <title>Experiment 2: Effect of Inter-Item Homogeneity and Summed Similarity on Memory for Ripple Sounds</title>
        <p>Previous studies have shown that short-term recognition memory for visual stimuli can be understood using the NEMo introduced by Kahana and Sekuler [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>]. Experiment 2 directly tested this model's predictions for memory for moving ripple sounds, and compared these results to previous results obtained with visual stimuli. This experiment was crafted so that the key assumptions of the model, effects of inter-item homogeneity and summed similarity, could be explored in a model-free way, while also allowing data to be fit to NEMo for a more quantitative assessment of these effects. The next section explains the logic of the experimental design.</p>
        <sec id="s2b1">
          <title>NEMo.</title>
          <p>Contemporary, exemplar-based memory models, such as the Generalized Context Model [<xref ref-type="bibr" rid="pbio-0050056-b024">24</xref>], assume that when participants judge whether a probe stimulus replicated one of the preceding study items, their judgments reflect the summed similarity of the probe to each study item in turn, (summed probe-item similarity), rather than the similarity of the probe to its one most-similar study item [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b025">25</xref>]. In addition, recent studies have shown that the similarity between the individual items to be remembered, the inter-item homogeneity, also has an effect on participants' performance. When items in memory are more homogeneous, participants make relatively few false alarms; when items in memory are less homogeneous, rate of false alarms increases [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>–<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>]. This could indicate that the participant adopts a less-strict criterion on trials in which the items are less homogeneous, or it may indicate that the memory representation is less fine-grained on trials in which stimuli are more different from one another. This effect has been found with a range of various visual stimuli, including oriented, compound sinusoidal gratings [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b003">3</xref>], realistic, synthetic human faces [<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>], and color samples [<xref ref-type="bibr" rid="pbio-0050056-b002">2</xref>]. Because all the memory stimuli assayed thus far were visual, it may be that the effect of homogeneity on memory is modality specific, a possibility that we examined in the current studies.</p>
          <p>If the inter-item homogeneity effect held for auditory stimuli, this would support the idea that the mechanism supporting the effect of homogeneity is shared by both auditory and visual memory. Ripple stimuli are useful for examining the effects of homogeneity and summed probe-item similarity due to their many parallels with Gabor patches, and their parametric variability.</p>
        </sec>
        <sec id="s2b2">
          <title>Summed probe-item similarity.</title>
          <p>Several models of visual short-term memory (including NEMo) posit that participants use information about the summed similarity between the probe and all the remembered items, rather than just the item most similar to the probe, to make a judgment about whether the probe was seen before [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b026">26</xref>]. Two pairs of conditions were created (shown in <xref ref-type="fig" rid="pbio-0050056-g005">Figure 5</xref>A) that were similar in all respects, but the summed probe-item similarity varied between the two conditions in the pair.</p>
          <fig id="pbio-0050056-g005" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g005</object-id>
            <label>Figure 5</label>
            <caption>
              <title>Effect of Probe-Item Similarity</title>
              <p>(A) A schematic diagram of four stimulus conditions. These examine effects of summed probe-item similarity on participants' report of having seen a stimulus. Conditions a–d keep inter-item homogeneity constant (pairs a &amp; b and c &amp; d) while changing summed probe-item similarity. These diagrams show the relationships between conditions, rather than the actual values stimuli may take. Throughout, summed probe-item similarity is denoted in green, whereas inter-item similarity is denoted in blue. Conditions in the first row have high summed similarity (indicated by the shorter green solid bars). These are identical to their pairs (b and d, respectively) in the second row in terms of inter-item homogeneity (indicated by the length of the blue dashed bar), and similarity of the probe to the closest item (shorter of the two green solid bars). The second row shows cases of lower summed similarity (longer green solid bars).</p>
              <p>(B) The results of the experiment. For each pair of otherwise matched stimuli, when summed similarity is larger, participants are more likely to indicate that a probe has been seen before (<italic>p</italic> &lt; 0.01). These box plots show the median (thick bar), and boxes include the middle 50% of data. The whiskers include all data points that are not outliers. Outliers are shown as circles, and defined as those points more than 1.5 times the interquartile range from the median. Light and dark green indicate high (conditions a &amp; c) and low (conditions b &amp; d) summed similarity, respectively.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g005" xlink:type="simple"/>
          </fig>
          <p><xref ref-type="fig" rid="pbio-0050056-g005">Figure 5</xref>B shows that greater summed probe-item similarity (left side of each pair) predicts greater probability of a <italic>Yes</italic> response (paired T-test for conditions a and b showed <italic>p</italic> &lt; 0.00001, for c and d, <italic>p</italic> &lt; 0.01). This experiment controlled for the similarity between the probe and the stimulus closest to it, as well as the inter-item homogeneity of the list, therefore indicating that the observed effects are due to summed probe-item similarity rather than other variables.</p>
        </sec>
        <sec id="s2b3">
          <title>Inter-item homogeneity.</title>
          <p>As noted in the Introduction, one goal in performing this experiment was to determine whether and how the homogeneity between items in memory influences participants' subsequent recognition for sounds. With this in mind, stimulus conditions were created that varied inter-item homogeneity while other factors (summed similarity between the probe and each item, and similarity between the probe and the item most similar to it) were held constant. This allowed the effect of inter-item homogeneity to be explored independently.</p>
          <p><xref ref-type="fig" rid="pbio-0050056-g006">Figure 6</xref> shows that as inter-item homogeneity is increased, a probe is less likely to attract a <italic>Yes</italic> response (paired T-test for conditions e and f showed <italic>p</italic> &lt; 0.00001, for c and d, <italic>p</italic> &lt; 0.01). Note that the experiment controlled for similarity between the probe and the list items, as well as the similarity between the probe and the item closest to it.</p>
          <fig id="pbio-0050056-g006" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g006</object-id>
            <label>Figure 6</label>
            <caption>
              <title>Effect of Inter-Item Similarity</title>
              <p>(A) A schematic diagram of four stimulus conditions. These examine effects of inter-item homogeneity on participants' report of having seen a stimulus. Conditions e &amp; f keep summed probe-item similarity constant (indicated by the total length of green bars for each condition) while changing inter-item homogeneity (length of blue dashed bar). Conditions g &amp; h do the same, but for a different summed probe-item similarity. These diagrams show the relationships between conditions, rather than the actual values stimuli may take. As in all figures, inter-item similarity is denoted in shades of blue, whereas summed probe-item similarity is in green.</p>
              <p>(B) The results of the experiment. For each pair of otherwise matched stimuli, when stimulus items are more homogeneous (dark blue), participants are less likely to indicate that a probe has been seen before than if the stimulus items are less homogeneous (light blue) (<italic>p</italic> &lt; 0.01). Box and whisker plot conventions are as described in the caption for <xref ref-type="fig" rid="pbio-0050056-g005">Figure 5</xref>B.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g006" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2b4">
          <title>Perceptual similarity.</title>
          <p>The perceived similarity between ripple sounds is monotonic with their physical difference. <xref ref-type="fig" rid="pbio-0050056-g007">Figure 7</xref> shows data from the cases in which a single list item was presented and followed immediately by a probe. When the probe matched the stimulus, participants were very likely to respond “<italic>Yes,</italic> there was a match.” As the difference between the stimuli increased, the proportion of <italic>Yes</italic> responses decreased monotonically, indicating a reduced likelihood that the probe would be confused with the stimulus that preceded it.</p>
          <fig id="pbio-0050056-g007" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g007</object-id>
            <label>Figure 7</label>
            <caption>
              <title>Proportion <italic>Yes</italic> Responses as a Function of Difference between Probe and To-Be-Remembered Item (in Units of JND)</title>
              <p>Twelve participants' data are shown in thin lines, with the average in thick red. These curves are well described by the exponential function of <xref ref-type="disp-formula" rid="pbio-0050056-e004">Equation 4</xref>. This function relates actual stimulus values to their mental representations. <italic>P</italic>(yes) responses indicate proportion of <italic>Yes</italic> responses.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g007" xlink:type="simple"/>
          </fig>
        </sec>
      </sec>
      <sec id="s2c">
        <title>Computational Models: Context Affects Memory for Sounds</title>
        <p><xref ref-type="fig" rid="pbio-0050056-g005">Figures 5</xref> and <xref ref-type="fig" rid="pbio-0050056-g006">6</xref> show that inter-item homogeneity and summed probe-item similarity both affect memory for complex sounds. This result is analogous to that observed for visual stimuli [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b002">2</xref>,<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>,<xref ref-type="bibr" rid="pbio-0050056-b018">18</xref>]. By fitting the same computational models to these auditory data and visual memory data, we can more sensitively examine whether the cognitive processing undergone by auditory and visual representations are similar.</p>
        <p>The data for Experiment 2 were fit to the models described in the Methods section: a three-parameter model that does not take into account inter-item homogeneity, a four-parameter model that adopts values describing perceptual similarity based on participants' performance when list length is 1 (<xref ref-type="fig" rid="pbio-0050056-g007">Figure 7</xref>), and a five-parameter model including inter-item homogeneity effects, and not assuming that perceptual similarity can be based on participants performance for list length 1.</p>
        <p><xref ref-type="table" rid="pbio-0050056-t001">Table 1</xref> shows the parameter values produced by model fits to the combined data for 12 participants. Fits were made for individual participants as well, and the parameters are similar in the individual participant fits and the fits to the average. The <italic>τ</italic> and <italic>σ</italic> parameters showed most variability across participants. Models in which the τ parameter was estimated from an independent dataset in which study lists comprised just one item are indicated. The value for <italic>A</italic> calculated from data with list length 1 was 0.93 for the data averaged over participants, with individual participant values ranging from 0.88 to 1.02. The value for <italic>τ</italic> calculated from data with list length 1 ranged from 0.43 to 1.41 across participants. When <italic>τ</italic> was allowed to vary in the five-parameter model, the value ranged from 0.97 to 3 (the maximum of the allowed range) across participants. The parameter <italic>τ</italic> and the criterion <italic>C</italic> have somewhat of a reciprocal relationship mathematically, and so their values depend on one another: as <italic>C</italic> decreases, <italic>τ</italic> increases.</p>
        <table-wrap content-type="1col" id="pbio-0050056-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.0050056.t001</object-id><label>Table 1</label><caption>
            <p>Parameter Values for Models with Three, Four, or Five Free Parameters</p>
          </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.t001" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb1col1" align="left" charoff="0" char=""/><col id="tb1col2" align="left" charoff="0" char=""/><col id="tb1col3" align="left" charoff="0" char=""/><col id="tb1col4" align="left" charoff="0" char=""/><col id="tb1col5" align="left" charoff="0" char=""/><col id="tb1col6" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" rowspan="2" valign="top"><hr/>Model</td><td colspan="5" valign="middle"><hr/>Parameter</td></tr><tr><td valign="middle"><hr/><italic>&sigma;</italic></td><td valign="middle"><hr/><italic>&tau;</italic></td><td valign="middle"><hr/><italic>&alpha;</italic></td><td valign="middle"><hr/><italic>&beta;</italic></td><td valign="middle"><hr/><italic>C</italic></td></tr></thead><tbody><tr><td valign="middle">Three parameters</td><td valign="middle">1.11</td><td valign="middle">0.67<sup>a</sup></td><td valign="middle">1</td><td valign="middle">&mdash;</td><td valign="middle">0.57</td></tr><tr><td valign="middle">Four parameters</td><td valign="middle">0.99</td><td valign="middle">0.67<sup>a</sup></td><td valign="middle">1</td><td valign="middle">&minus;1.12</td><td valign="middle">0.43</td></tr><tr><td valign="middle">Five parameters</td><td valign="middle">1.04</td><td valign="middle">1.69</td><td valign="middle">1</td><td valign="middle">&minus;1.10</td><td valign="middle">0.12</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt101"><p><sup>a</sup>Model in which the <italic>&tau;</italic> parameter was estimated from an independent dataset in which study lists comprised just one item.</p></fn></table-wrap-foot> --></table-wrap>
        <p>Interestingly, the <italic>α</italic> parameter was not significantly less than 1, indicating that in this experiment, when participants had to remember only two stimuli, both stimuli were remembered equally well. When participants must maintain more stimuli in memory, however, they are more likely to forget stimuli presented earlier in the list, as is shown in <xref ref-type="fig" rid="pbio-0050056-g004">Figure 4</xref>.</p>
        <p>Note that the <italic>β</italic> parameter remained negative and with a similar value regardless of model. Note that in both the four- and five-parameter models, <italic>β</italic> ∼ −1. This result is similar to that found by Kahana and Sekuler [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>].</p>
        <p>Results indicated that models must incorporate inter-item homogeneity in order to fit the data well. The three-parameter model that did not incorporate inter-item homogeneity (as shown in <xref ref-type="fig" rid="pbio-0050056-g008">Figure 8</xref>A) accounted for only 51% of the variance (<italic>r</italic><sup>2</sup>), and had an Akaike information criterion (AIC) value (see Methods) of 1,010 (higher AIC values indicate worse fit [<xref ref-type="bibr" rid="pbio-0050056-b027">27</xref>]). On the other hand, the four-parameter model accounted for 78% of the variance (<italic>r</italic><sup>2</sup>), and had a considerably lower AIC value of 652. The five-parameter model, allowing <italic>τ</italic> to vary according to the list length 2 data, accounted for 81% of the variance (<italic>r</italic><sup>2</sup>) and had a slightly higher AIC value of 696, indicating that the addition of this extra parameter does not make the model more generalizable.</p>
        <fig id="pbio-0050056-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pbio.0050056.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Model Fits for NEMo</title>
            <p>Model fits for NEMo are much better for versions including inter-item homogeneity (four-parameter model in [B]), than for identical models that do not include inter-item homogeneity (three-parameter model in [A]). Line indicates best fit to the data.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.g008" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>The ripple stimuli used here share many similarities with visual grating stimuli. Grating stimuli have long been a fixture of psychophysical experiments because they can be used to explore some properties of vision that are thought to be fundamental: spatial location, luminance, orientation, and spatial frequency. Similarly, the ripple sounds used in the present study can be used to examine some fundamental properties of hearing: frequency spectrum, sound level, and temporal frequency. The experiments presented here make use of the similarities to explore whether the fundamental information processing steps in vision and hearing are similar.</p>
      <p>Moving ripple stimuli and visual gratings are processed by the nervous system in analogous ways, and therefore represent an important class of stimuli for comparing memory in the visual and auditory domains. Both auditory and visual cortical receptive fields have characteristic center-surround properties [<xref ref-type="bibr" rid="pbio-0050056-b007">7</xref>,<xref ref-type="bibr" rid="pbio-0050056-b009">9</xref>,<xref ref-type="bibr" rid="pbio-0050056-b015">15</xref>]. Further, edge detection in visual cortex appears to have an analog in auditory cortex [<xref ref-type="bibr" rid="pbio-0050056-b028">28</xref>]. Relatedly, both auditory and visual systems appear to exploit “sparse” coding [<xref ref-type="bibr" rid="pbio-0050056-b029">29</xref>]: when presented with stimuli of the appropriate type, individual cells respond very strongly to one example of the stimulus type and less strongly to other examples. In the visual modality, single primary visual cortical cells show large responses and specific tuning for oriented sine-wave gratings, or Gabor patches [<xref ref-type="bibr" rid="pbio-0050056-b009">9</xref>,<xref ref-type="bibr" rid="pbio-0050056-b030">30</xref>]. In the auditory modality, single primary auditory cortical cells show large responses and specific tuning for moving ripple stimuli [<xref ref-type="bibr" rid="pbio-0050056-b007">7</xref>,<xref ref-type="bibr" rid="pbio-0050056-b009">9</xref>,<xref ref-type="bibr" rid="pbio-0050056-b015">15</xref>].</p>
      <p>Thus, early stages of cortical processing seem to treat Gabor patches and moving auditory ripples in an analogous fashion. Although a number of studies have examined recognition memory for Gabor patches [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b003">3</xref>], comparable tests of memory for auditory ripple stimuli have been lacking until now.</p>
      <p>Parametrically manipulable stimuli were used in order to explore how memory alters the representation of stimuli. By using an auditory stimulus set for which early processing is similar to the visual gratings used here and in myriad previous studies (e.g., [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b020">20</xref>,<xref ref-type="bibr" rid="pbio-0050056-b030">30</xref>]), comparisons between memory effects in the two modalities can be made. Our results indicate that these auditory stimuli are processed in a way that is quite analogous to visual gratings. In Experiment 1, we directly tested properties of memory between the two modalities, and found little or no difference depending on stimulus type in how memory is affected by list length, retention interval, or serial position. The overall mean proportions correct were larger for the auditory stimuli than the visual stimuli, but the change with each of these variables was similar regardless of the stimulus type. In Experiment 2, we tested the hypothesis that a quantitative model for visual memory, NEMo, would fit the data for auditory memory better than other models. Indeed, NEMo fit the auditory memory data quite well, as shown in <xref ref-type="fig" rid="pbio-0050056-g008">Figure 8</xref>, and the two major assumptions of the model proved true for auditory stimuli just as they had for visual stimuli: summed probe-item similarity and inter-item homogeneity each contribute to a participant's probability of responding that <italic>Yes,</italic> an item has been seen before.</p>
      <sec id="s3a">
        <title>Direct Comparison between Memory for Auditory and Visual Stimuli</title>
        <p>In our hands, direct comparison between auditory and visual memory revealed the two to be strikingly similar. The list length manipulation effectively changed the memory load participants had to bear, and has been used in experiments on vision [<xref ref-type="bibr" rid="pbio-0050056-b006">6</xref>] and hearing [<xref ref-type="bibr" rid="pbio-0050056-b011">11</xref>]. The current experiment reveals that the effect of load does not depend on the modality of the stimulus by comparing the stimulus types using the same participants and same experimental paradigm.</p>
        <p>The effects of retention interval on recognition memory are also quite similar across stimulus types, as seen in <xref ref-type="fig" rid="pbio-0050056-g003">Figure 3</xref>. Memory for auditory and visual stimuli decreased only modestly with retention interval. This result is consistent with previous studies of visual memory [<xref ref-type="bibr" rid="pbio-0050056-b020">20</xref>].</p>
        <p>The effects of serial position on recognition memory were found to be quite similar across modality, as seen in <xref ref-type="fig" rid="pbio-0050056-g004">Figure 4</xref>. Although this is consistent with some studies [<xref ref-type="bibr" rid="pbio-0050056-b031">31</xref>], there is an apparent contradiction in the literature: some researchers have found serial position curves of different shapes for auditory and visual experiments [<xref ref-type="bibr" rid="pbio-0050056-b032">32</xref>]. Many such experiments rely on auditory stimuli that are phonological in nature, and others use different experimental paradigms or stimulus types for auditory and visual experiments. A study by Ward and colleagues [<xref ref-type="bibr" rid="pbio-0050056-b031">31</xref>] implies that the auditory versus visual difference seen in other studies can be explained by the differing experimental methods used. When experimental methods are held constant, little or no serial position difference was seen between the two modalities, consistent with our data. Although there was no significant interaction of the effect of serial position with stimulus type, there is a trend toward a larger recency effect for the auditory stimuli than for the visual stimuli (<xref ref-type="fig" rid="pbio-0050056-g004">Figure 4</xref>). The origin of this recency effect has been debated [<xref ref-type="bibr" rid="pbio-0050056-b033">33</xref>]. One idea put forward by Baddeley and Hitch [<xref ref-type="bibr" rid="pbio-0050056-b033">33</xref>] implies that the recency effect may be due to implicit learning of the items (similar to priming) followed by explicit retrieval of the residual memory.</p>
        <p>Many studies using various types of stimuli in free-recall tasks have shown a “primacy effect” in which serial position 1 shows a better proportion correct than serial position 2 [<xref ref-type="bibr" rid="pbio-0050056-b034">34</xref>]. No such primacy effect is evident in our data, as can be seen in <xref ref-type="fig" rid="pbio-0050056-g006">Figure 6</xref>. The lack of a prominent primacy effect is consistent with some previous experiments using this paradigm [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b035">35</xref>], whereas other experiments using the same paradigm, but different stimuli, have found modest primacy effects [<xref ref-type="bibr" rid="pbio-0050056-b036">36</xref>]. Previous experiments have shown that these effects are sensitive to the delay between the stimulus items and probe [<xref ref-type="bibr" rid="pbio-0050056-b014">14</xref>,<xref ref-type="bibr" rid="pbio-0050056-b037">37</xref>]. The absence of a primacy effect may be due to specifics of timing, the difficulty of rehearsing these stimuli, or an interaction of the stimuli and recognition memory task employed.</p>
        <sec id="s3a1">
          <title>Differences between means.</title>
          <p>Although the effects of list length, retention interval, and serial position were similar across the stimulus types, there is a striking and statistically significant difference between the mean proportion correct for the auditory and visual stimuli. Differences in mean in experiments like these may result from a difference in the overall difficulty of discriminating any two stimuli presented in the experiment. Although we performed a threshold test to determine each participant's just noticeable difference (JND) thresholds for each stimulus type, it is possible that these estimates erred on the side of being too easy for the auditory stimuli, despite the fact that JNDs were estimated using the same algorithm for all stimulus types.</p>
          <p>Another possibility is that participants became gradually better at the auditory task, but not the visual tasks. Because the threshold tests were performed before the six experimental sessions, this would result in the auditory task becoming easier in later sessions, and a higher mean proportion correct. Analysis of participants' performance across session does not rule out this explanation. <xref ref-type="supplementary-material" rid="pbio-0050056-sd001">Text S1</xref> explains the analysis that compares performance on trials early in the string of sessions to later trials. Participants' proportion correct increased with time in the auditory case, but not in the visual case, suggesting that participants improved on the auditory but not the visual task. Further experiments would be necessary to fully explore this differential learning effect. Although neither of these particular auditory and visual stimuli occur in participants' normal environments, it is possible that through prior exposure to stimuli like our visual stimuli, participants were better able to optimize their performance, but were less familiar with stimuli like the auditory stimuli.</p>
        </sec>
      </sec>
      <sec id="s3b">
        <title>Summed Probe-Item Similarity</title>
        <p><xref ref-type="fig" rid="pbio-0050056-g005">Figure 5</xref> shows that summed probe-item similarity correlates very strongly with whether a probe will be judged as new. Because the similarity of the probe to the closest item is identical in each pair, the data imply that participants use information from all stimuli when making a judgment, not just information about the stimulus closest to the probe [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b025">25</xref>]. This gives credence to an exemplar model of memory, rather than a prototype model [<xref ref-type="bibr" rid="pbio-0050056-b025">25</xref>], and is entirely consistent with the results found in the visual domain [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>,<xref ref-type="bibr" rid="pbio-0050056-b018">18</xref>].</p>
      </sec>
      <sec id="s3c">
        <title>Inter-Item Homogeneity</title>
        <p>These data indicate strongly that inter-item homogeneity plays a role in memory for sounds. When items in a list are more similar to each other, participants are less likely to say that a probe was a member of the list. This result was robust through direct data comparison (<xref ref-type="fig" rid="pbio-0050056-g006">Figure 6</xref>) as well as by model fitting, which gave a more sensitive measure of the effect of inter-item homogeneity. As noted earlier, these results are consistent with experiments that examine memory for visual stimuli, including gratings and faces [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>]. In fact, some older experiments using sound stimuli are consistent with this inter-item homogeneity effect. In one experiment, participants were required to remember a tone stimulus during presentation of distracter tones, and performed much worse when the distracter tones were presented both higher and lower than the remembered stimuli (low homogeneity between the remembered tone and the distracters), as opposed to the case when distracters were presented only higher or only lower than the remembered stimulus (higher overall homogeneity between the remembered tone and distracters) [<xref ref-type="bibr" rid="pbio-0050056-b038">38</xref>]. The similarity across stimulus type implies that the origin of the inter-item homogeneity effect is a process common to both auditory and visual memory.</p>
      </sec>
      <sec id="s3d">
        <title>Similar Patterns Imply Similar Processing</title>
        <p>The strikingly similar patterns of memory observed for the auditory and visual stimuli imply that the information-processing steps involved in memory for each stimulus type are similar. Previous research has shown that sensory-specific cortex is re-activated during memory for a sensation [<xref ref-type="bibr" rid="pbio-0050056-b039">39</xref>,<xref ref-type="bibr" rid="pbio-0050056-b040">40</xref>]. Further, lesions of some auditory-specific cortex results in impairment specifically to auditory memory [<xref ref-type="bibr" rid="pbio-0050056-b041">41</xref>]. The current data imply that the effects of inter-item homogeneity and summed probe-item similarity on memory either arise from non-sensory–specific cortex, or that the mechanisms in each sensory-specific region are very similar.</p>
      </sec>
      <sec id="s3e">
        <title>Conclusion</title>
        <p>The data presented here show that memory for visual and auditory stimuli obey many of the same principles. In both modalities, recognition performance changes in similar ways in response to variation in list length, retention interval, and serial position. Further, memory performance depends not only on the summed similarity between a probe and the remembered items, but also on the similarity of remembered items to one another. Memory performance data for both modalities are fit well by the NEMo. These results imply that auditory and visual short-term memory employ similar mechanisms.</p>
        <p>Previous studies have examined how auditory and visual items are encoded into memory, implicating some structures in both visual and auditory working memory [<xref ref-type="bibr" rid="pbio-0050056-b042">42</xref>,<xref ref-type="bibr" rid="pbio-0050056-b043">43</xref>]. Behaviorally, visual and auditory stimuli can interfere with each other, indicating some shared processing [<xref ref-type="bibr" rid="pbio-0050056-b044">44</xref>]. On the other hand, some memory information is processed in sensory-specific cortex, indicating that the transformations performed on such information may differ between modalities [<xref ref-type="bibr" rid="pbio-0050056-b039">39</xref>,<xref ref-type="bibr" rid="pbio-0050056-b040">40</xref>,<xref ref-type="bibr" rid="pbio-0050056-b045">45</xref>]. Our data imply that, regardless of whether the processing is performed by the same brain area or not, similar processing is performed on auditory and visual stimuli as they are maintained and retrieved from memory.</p>
        <p>For centuries, people have pondered possible parallels between their experiences of light and sound [<xref ref-type="bibr" rid="pbio-0050056-b017">17</xref>]. Belief that the two modalities were parallel probably influenced Sir Isaac Newton's conclusion that the visible spectrum contained seven colors, the same number of tone intervals in a musical octave [<xref ref-type="bibr" rid="pbio-0050056-b046">46</xref>]. (Newton observed: “And possibly colour may be distinguished into its principle degrees, red, orange, yellow, green, blue, indigo and deep violet, on the same ground that sound within an eighth is graduated into tones.” [<xref ref-type="bibr" rid="pbio-0050056-b046">46</xref>]) Today, 300 years after Newton, understanding of the neural signals supporting vision and hearing has advanced sufficiently that we have been able to formulate and test hypotheses about fundamental relationships between the characteristics of short-term memory for each modality.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Materials and Methods</title>
      <sec id="s4a">
        <title>Experiment 1.</title>
        <p>Moving ripple sounds: moving ripple stimuli varied sinusoidally in both time (with a period <italic>w</italic> cycles per second [cps]) and frequency content (with a period <italic>Ω</italic> cycles per octave). The sounds were generated by superimposing sounds at many frequencies whose intensity at any time, and for any frequency <italic>(f),</italic> was defined by
					<disp-formula id="pbio-0050056-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.0050056.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>D</mml:mi><mml:mo>&sdot;</mml:mo><mml:mi>cos</mml:mi><mml:mo></mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mn>2</mml:mn><mml:mi>&pi;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mi>&Omega;</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&psi;</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mrow></mml:math> --></disp-formula>where <italic>g</italic> = log(<italic>f/f</italic><sub>0</sub>), <italic>t</italic> is time, <italic>ψ</italic> is the phase of the ripple, and <italic>D</italic> is modulation depth. (<italic>D</italic><sub>0</sub> represents the baseline intensity, and is set to 1 in the equation to avoid negative intensity values.) <italic>f</italic><sub>0</sub> is the lowest allowed frequency. In these experiments, the parameter space was simplified by allowing only one parameter <italic>(w)</italic> to vary. Other parameters took the following fixed values: Ω = 1, <italic>D</italic><sub>0</sub> = 0.9, <italic>f</italic><sub>0</sub> = 200 Hz, and <italic>ψ</italic> was varied randomly between 0 and π/2 for each stimulus. Frequencies ranged over three octaves above <italic>f</italic><sub>0</sub> <italic>,</italic> that is, from 200 to 1,600 Hz. Choices for these parameters were made so that a range of stimuli with parameters close to these could be discriminated, as suggested by existing psychophysical data [<xref ref-type="bibr" rid="pbio-0050056-b010">10</xref>,<xref ref-type="bibr" rid="pbio-0050056-b047">47</xref>,<xref ref-type="bibr" rid="pbio-0050056-b048">48</xref>], and in pilot experiments of our own.
				</p>
        <p>Each stimulus contained 20 logarithmically spaced frequencies per octave. Levels for each frequency were identical, but psychophysical loudness varied. However, the same group of frequencies was used for every stimulus, so the time-averaged loudness should be nearly identical for each of the stimuli. <xref ref-type="disp-formula" rid="pbio-0050056-e001">Equation 1</xref> describes for each frequency <italic>f,</italic> a sinusoidal modulation of the level around some mean, at a rate of <italic>w</italic> cps. This produces a spectral profile that drifts in time, so that different frequencies are at their peaks at different times. <xref ref-type="fig" rid="pbio-0050056-g001">Figure 1</xref> illustrates the dynamic spectrum of a moving ripple, with modulation in both time (<italic>w</italic>, horizontal axis) and frequency content (Ω, vertical axis). For all stimuli, duration was fixed at 1 s. The level of the stimulus was ramped on and off gradually and linearly over 10 ms at the beginning and end of each stimulus. Frequencies at the spectral edges of the stimulus were treated identically to frequencies in the middle of the frequency range. Two examples of auditory stimuli with different <italic>w</italic> values are given in <xref ref-type="supplementary-material" rid="pbio-0050056-sa001">Audios S1</xref> and <xref ref-type="supplementary-material" rid="pbio-0050056-sa002">S2</xref>, and correspond to the stimuli schematized in <xref ref-type="fig" rid="pbio-0050056-g001">Figure 1</xref>A and <xref ref-type="fig" rid="pbio-0050056-g001">1</xref>B.</p>
        <p>Visual stimuli: visual stimuli were Gabor patches, created and displayed using Matlab and extensions from the Psychtoolbox [<xref ref-type="bibr" rid="pbio-0050056-b049">49</xref>]. The CRT monitor was calibrated using Eye-One Match hardware and software from GretagMacbeth (<ext-link ext-link-type="uri" xlink:href="http://www.gretagmacbeth.com/index.htm" xlink:type="simple">http://www.gretagmacbeth.com/index.htm</ext-link>). The Gabor patches' mean luminance matched that of the background; the peak contrast of a Gabor patch was 0.2. Patches were windowed with a two-dimensional Gaussian envelope with a standard deviation of 1.4 degrees. Before windowing, the visual stimuli were generated according to the following equation:
					<disp-formula id="pbio-0050056-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.0050056.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>D</mml:mi><mml:mo>&sdot;</mml:mo><mml:mi>cos</mml:mi><mml:mo></mml:mo><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mn>2</mml:mn><mml:mi>&pi;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>&Omega;</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&psi;</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mrow></mml:math> --></disp-formula>where <italic>s</italic> represents the luminance of the stimulus at any <italic>y</italic> (vertical) position and time, <italic>t</italic>. Note that these stimuli were aligned horizontally and moved only vertically; the luminance did not change with horizontal position. <italic>ψ</italic> is the phase of the grating, which varied randomly between 0 and π/2 for each stimulus. <italic>D</italic> is modulation depth. (<italic>D</italic><sub>0</sub> is the mean luminance, set to a mid-gray level on the monitor.) In these experiments, the parameter space was simplified by allowing only one parameter to vary at a time. In blocks with moving gratings, the <italic>w</italic><sub>v</sub> parameter varied; in blocks with static gratings, the spatial frequency, Ω<sub>v</sub>, parameter varied. Other parameters took the following fixed values: <italic>D</italic><sub>0</sub> = 0.9 and <italic>f</italic><sub>0</sub> = 200 Hz.
				</p>
        <p>All moving gratings had a spatial frequency, Ω<sub>v</sub><italic>,</italic> of 0.72 cycles per degree, and moved with speeds that ranged upward from 1.5 cps (2.1 degrees per second). For static gratings, stimuli did not move (<italic>w</italic><sub>v</sub> = 0), and had spatial frequencies, Ω<sub>v</sub><italic>,</italic> with a minimum of 0.36 cycles per degree. An example of a moving grating is shown in <xref ref-type="supplementary-material" rid="pbio-0050056-sv001">Video S1</xref>, and an example of a static grating is shown in <xref ref-type="fig" rid="pbio-0050056-g001">Figure 1</xref>C. Parameter values were chosen based on pilot experiments and previous data so that a range of stimuli with parameters near these would be discriminable.</p>
        <p>Stimuli were tailored to each participant in an initial session, JND thresholds to achieve 70% correct were estimated using the QUEST algorithm [<xref ref-type="bibr" rid="pbio-0050056-b050">50</xref>] as implemented in the Psychtoolbox [<xref ref-type="bibr" rid="pbio-0050056-b049">49</xref>]. Participants were presented with two stimuli sequentially and responded indicating which stimulus was “faster” (in the case of moving ripples or moving gratings) or “thinner” (in the case of stationary gratings). Thresholds for each stimulus type were estimated in separate blocks. These JND values were used to create an array of ten stimuli for each participant, in which each stimulus differed from its nearest neighbor by one JND. All stimuli were chosen from this array, and were thus separated from one another by an integer number of JNDs.</p>
        <p>The timing of stimulus presentation during threshold measurements was the same as that used in the later memory tests for a list with a single item. Stimuli were thus individually tailored for each participant, so that the task was of similar difficulty for all participants, and somewhat similar difficulty across modality [<xref ref-type="bibr" rid="pbio-0050056-b018">18</xref>]. The lowest value that each stimulus could take was the same for all participants. Other stimulus values were allowed to vary by participant in order to equate discriminability across participants. In Experiment 1, for the static grating stimulus type, in which the spatial frequency, Ω<sub>v</sub><italic>,</italic> changed, the lowest Ω<sub>v</sub> value was 0.36 cycles per degree. For the moving grating stimulus type, in which temporal frequency, <italic>w</italic><sub>v</sub><italic>,</italic> changed, the lowest <italic>w</italic><sub>v</sub> value was 0.025 cps. For the moving ripple sounds, the lowest possible ripple velocity, <italic>w,</italic> was 6 cps. In Experiment 2, the lowest ripple velocity, <italic>w,</italic> was 7 cps.</p>
        <p>In order to minimize the possibility that participants could memorize all stimuli, a second, “jittered” set of stimuli was created and then used on half the trials chosen randomly. This list of stimuli started at 0.5 JND above the base value, and increased in units of 1 JND to create a second array of ten stimuli. For data analysis, we do not distinguish between trials on which the two arrays were used.</p>
        <p>We experimentally manipulate the physical difference between any two stimuli, here measured in JND. However, the perceptual similarity is traditionally referred to in models that take perception into account. Therefore, when discussing physical stimuli, we refer to their difference (in JND), but later, when discussing fits to models, it is the related perceptual similarity that is relevant.</p>
        <p>Participants: participants for all experiments were between the ages of 18 to 30 y, and were recruited from the Brandeis student population. They participated for payment of $8 per session plus a performance-based bonus. Using a MAICO MA39 audiometer, participants' hearing thresholds were measured at 250, 500, 750, 1,000, 2,000, 3,000, 4,000, and 6,000 Hz. Each participant had normal or better hearing, that is, thresholds under 20 dB<sub>HL</sub> (decibels hearing level) at each frequency.</p>
        <p>Fourteen participants participated in seven total sessions each. In an initial session, hearing was tested and vision was tested to be 20/20 or better (using a Snellen eye chart), participants performed 30 practice trials for each stimulus type, and JND thresholds were measured at a 70% accuracy level for each stimulus type. Each of the subsequent six sessions lasted approximately 1 h, and consisted of 504 trials. A session began with 15 practice trials, whose results were not included in subsequent data analysis. For each participant, successive sessions were separated by at least 3 h, and all sessions were completed within 2.5 wk.</p>
        <p>Apparatus and sound levels: participants listened to ripple sounds through Sennheiser Pro HD 280 headphones. All stimuli were produced by Apple Macintosh iMac computers and Matlab, using extensions from the Psychtoolbox [<xref ref-type="bibr" rid="pbio-0050056-b049">49</xref>]. Sound levels for this system were measured using a Knowles electronic mannequin for acoustic research, in order to define the stimulus intensity at the participant's eardrum. Levels for all stimuli in Experiment 2 were 79 dB<sub>SPL</sub> (decibels sound pressure level), well above our participants' hearing thresholds, and levels for stimuli in Experiment 1 were similar (with the same code and hardware settings, but a different computer).</p>
        <p>This experiment examined and compared some basic characteristics of short-term memory for moving ripple sounds and for Gabor patches. Using Sternberg's recognition memory paradigm, we examined recognition's dependence on the number of items to be remembered, the interval over which the items had to be retained, and the serial position of the to-be-remembered item [<xref ref-type="bibr" rid="pbio-0050056-b006">6</xref>]. The experiment used static visual gratings (in which the spatial frequency of the gratings, Ω<sub>v</sub><italic>,</italic> varied), moving visual gratings (in which the speed of the gratings, <italic>w</italic><sub>v</sub><italic>,</italic> varied), and moving ripple sounds (in which the temporal frequency, <italic>w,</italic> of the ripples varied).</p>
        <p>Stimulus presentation: trials were presented in blocks such that only one stimulus type (moving ripple sounds, static gratings, or moving visual gratings) was presented per block. During presentation of either visual or auditory list stimuli, participants fixated on a “+” in the center of a computer screen. Each stimulus, auditory or visual, lasted for 1 s. After the last item from a list was presented, a short beep sounded, and the “+” was replaced by the text “...”, indicating that the participant should wait for the probe. The text “?” was presented onscreen during presentation of the probe (for sound stimuli only) and after the probe presentation, before the participant made a response. Participants were instructed to be as quick and accurate with responses as possible. Stimuli were presented in blocks of 84 trials of a given stimulus type. Six total blocks were presented per session. The first two trials of each block were not used for analysis to allow for task-switching effects.</p>
        <p>Stimuli for each list were chosen from a set created as described above for each participant based on their own JND threshold. Trials with different list lengths and retention intervals were randomly interleaved. Twenty-four trials of each possible serial position were presented to each participant, for each stimulus type. Effect of retention interval was examined by having participants perform trials in which a single stimulus was followed by a probe, after a retention interval of 0.6, 1.9, 3.2, 4.5, or 9.7 s; 24 trials of each retention interval were performed by each participant. Equal numbers of trials in which the probe matched a list item (target), and trials in which the probe did not match (lure) were performed.</p>
        <p>Trials were self-paced, with each beginning only when participants indicated with a key press that they were ready. Participants were alerted with a high or low tone whether they got the current trial correct or incorrect, and were updated after each trial as to their percent correct. For every percentage point above 70%, participants received an extra $0.25 reward above their base payment of $56.</p>
      </sec>
      <sec id="s4b">
        <title>Experiment 2.</title>
        <p>Participants and stimulus presentation: on each trial, a list of one or two ripple stimuli (<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>) were presented, followed by a probe (<italic>p</italic>). As in Experiment 1, the participants' task was to identify whether the probe stimulus matched any of the items presented in the list, and press a button to indicate a choice. During list presentation, participants fixated on a “+” in the center of a computer screen. This was replaced by a “?” during the presentation of the probe item. Twelve participants participated in each of eight sessions, following an initial session in which hearing was tested, JND thresholds for the <italic>w</italic> parameter (cps) were measured, and 200 practice trials were performed. Sessions were approximately 1 h each, and consisted of 586 trials. At the beginning of every session, each participant completed at least 30 practice trials that were excluded from data analysis. Each session began at least 6 h from the previous session, and all sessions were completed within 3 wk. All other details are as described for auditory stimuli in Experiment 1.</p>
        <p>Summed probe-item similarity: in order to examine the effect of summed probe-item similarity independently of other confounds, such as the similarity of the probe to the closest item or the inter-item homogeneity, stimulus conditions were created that varied summed probe-item similarity while other factors were held constant. Two pairs of conditions were created that were similar in all respects, but the summed probe-item similarity varied between the two conditions in the pair.</p>
        <p><xref ref-type="fig" rid="pbio-0050056-g005">Figure 5</xref>A shows the relationships between stimuli for each condition. All figures indicate relationships between stimuli in terms of their differences in units of JND.</p>
        <p>Pairs of conditions (labeled a &amp; b on one side, and c &amp; d on the other) were created with identical inter-item homogeneities, and identical similarities between the probe and the item closest to it. However, each pair has one low and one high summed probe-item similarity (pair a &amp; b, for example, both have inter-item difference = 2 JND, but summed probe-item differences of 2 and 4 JND units, respectively).</p>
        <p><xref ref-type="fig" rid="pbio-0050056-g005">Figures 5</xref> and <xref ref-type="fig" rid="pbio-0050056-g006">6</xref> indicate only the relationship among the stimuli in units of JND, not their physical values. Part A in these figures illustrates the case when <italic>s</italic><sub>1</sub> &lt; <italic>s</italic><sub>2</sub>, equally often <italic>s</italic><sub>1</sub> &gt; <italic>s</italic><sub>2</sub>. Also in conditions b and d, the probe, <italic>p,</italic> is equally likely to be greater than or less than the stimuli <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub>. The conditions as shown in the figures do not specify exactly the stimulus values for a trial. Eight cases of each condition were chosen randomly from all possible configurations that satisfy the condition, given ten stimuli in the array. This made 64 lure cases. Twenty repetitions of each case were performed by each participant, interleaved among the other trial types. For each lure case, analogous target cases were created where the probe matched one of the stimuli. Each target case matched a different lure case in either inter-item homogeneity (in conditions a–d), or summed probe-item similarity (in conditions e–h, explained below).</p>
        <p>Inter-item homogeneity: stimulus conditions with high and low inter-item homogeneity were created according to <xref ref-type="fig" rid="pbio-0050056-g006">Figure 6</xref>A, which follows the same conventions as <xref ref-type="fig" rid="pbio-0050056-g005">Figure 5</xref>A. Relationships between stimuli for each condition are shown in terms of their physical differences, in units of JND. Two sets of paired high and low homogeneity conditions were created; both members of a pair had the same inter-item homogeneity and similarity between the stimulus and the closest probe.</p>
        <p>Computational modeling of results: fitting computational models to experimental data can help determine what information processing steps are involved in short-term memory. Previous experiments in the visual domain found that a NEMo, including effects of summed probe-item similarity as well as inter-item homogeneity, fit data for short-term visual memory well [<xref ref-type="bibr" rid="pbio-0050056-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0050056-b002">2</xref>,<xref ref-type="bibr" rid="pbio-0050056-b004">4</xref>].</p>
        <p>The NEMo model was applied only to the data from the 128 auditory memory cases whose list length was two items, because only those trials incorporated information about inter-item homogeneity, important to the model. The NEMo assumes that given a list of <italic>L</italic> items and a probe item, <italic>p,</italic> the participant will respond that “<italic>Yes,</italic> the probe is a member of the list” if the quantity:
					<disp-formula id="pbio-0050056-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.0050056.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&eta;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>&epsiv;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>&#x304;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>\overline{ Summed probe-item similarity}</mml:mtext></mml:mrow></mml:munder><mml:mo>&plus;</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>L</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:mrow></mml:mfrac><mml:mi>&beta;</mml:mi><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&equals;</mml:mo><mml:mi>i</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mi>&eta;</mml:mi></mml:mstyle></mml:mrow></mml:mstyle><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>&epsiv;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>&epsiv;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mo stretchy='true'>&#x304;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>\overline{Mean inter-item similarity}</mml:mtext></mml:mrow></mml:munder></mml:mrow></mml:math> --></disp-formula>exceeds a threshold criterion value, <italic>C</italic>.
				</p>
        <p>The first term depends on the summed similarity between the probe and the items on the list. <italic>α</italic> is defined as 1 for the most recent stimulus; its value for a less-recent stimulus determines the degree of forgetting of that stimulus. It should take on values less than 1 if the earlier item is forgotten more readily. <italic>η,</italic> as defined in <xref ref-type="disp-formula" rid="pbio-0050056-e004">Equation 4</xref>, measures the perceptual similarity between any two stimuli, as a function of <italic>τ,</italic> which defines how quickly perceptual similarity drops with physical distance:
					<disp-formula id="pbio-0050056-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.0050056.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>&eta;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>&tau;</mml:mi><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math> --></disp-formula>
				</p>
        <p>The parameter <italic>A</italic> in <xref ref-type="disp-formula" rid="pbio-0050056-e004">Equation 4</xref> defines the maximum similarity between two stimuli. <italic>ε</italic> defines the noise in the memory representation of the stimulus (hence the label “Noisy Exemplar”). The parameter <italic>ε</italic> is a normally distributed random variable with variance <italic>σ</italic><sup>2</sup>. Note that the similarities incorporated in the model depend on the noisy values of the remembered stimuli.</p>
        <p>The second term in <xref ref-type="disp-formula" rid="pbio-0050056-e003">Equation 3</xref> involves the homogeneity of the list, that is, the similarity between the remembered list items. <italic>β</italic> is a parameter determining the direction and amplitude of the effect of list homogeneity. If <italic>β</italic> &lt; 0, as was found in earlier experiments using visual stimuli, a given lure will be more tempting when <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub> are widely separated; conversely, if <italic>β</italic> &gt; 0, a lure will be less tempting when <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub> are widely separated. If <italic>β</italic> = 0, the model does not depend on inter-item homogeneity, and is a close variant of Nosofsky's Generalized Context Model [<xref ref-type="bibr" rid="pbio-0050056-b024">24</xref>]. The parameter <italic>A,</italic> as defined in <xref ref-type="disp-formula" rid="pbio-0050056-e004">Equation 4</xref>, was set to 1. This model allows five parameters, <italic>σ, α, β, C,</italic> and <italic>τ,</italic> to vary.</p>
        <p>Two additional similar models were also examined. A second model assumes that the similarity between items can be predicted from participants' probability of confusing two items in a trial of list length 1. This model adopts values for <italic>τ</italic> and <italic>A</italic> for each participant based on the fit of <xref ref-type="disp-formula" rid="pbio-0050056-e004">Equation 4</xref> to their data with list length 1. This model is identical to that above, but simpler, allowing only four parameters (<italic>σ, α, β,</italic> and <italic>C</italic>) to vary based on the data with list length 2.</p>
        <p>A third model is identical to the second, but in it, <italic>β</italic> is forced to be 0, which means that only three parameters are free to vary: <italic>σ, α,</italic> and <italic>C</italic>. Note that this last model does not take into account any possible influence of inter-item homogeneity. Models are labeled according to the number of parameters varied in each: five, four, and three.</p>
        <p>Model fits: models were fit to participants' accuracy data by means of a genetic algorithm. Such a method was chosen because it is robust to the presence of local minima [<xref ref-type="bibr" rid="pbio-0050056-b051">51</xref>]. The parameter spaces involved in this experiment are relatively complex, so the genetic algorithm approach was particularly attractive. To summarize our implementation of a genetic algorithm, 3,000 “individuals” were generated, each a vector of randomly chosen values for each of model's parameters. The ranges for each parameter were: 0 &lt; <italic>σ</italic> &lt; 5, −3 &lt; <italic>τ</italic> &lt; 3, 0 &lt; <italic>α</italic> &lt;1, −2 &lt; <italic>β</italic> &lt; 2, and 0 &lt; <italic>C</italic> &lt; 2. Three thousand trials were simulated for each individual, each with a randomly chosen value for <italic>ε</italic> given the parameter <italic>σ</italic>. When the value in Expression 3 exceeds <italic>C,</italic> the simulation produced a <italic>Yes</italic> response. The proportion of <italic>Yes</italic> responses for each case was calculated. The fitness of each individual was computed by calculating the log likelihood that the predicted and observed data came from the same distribution. Log likelihood was chosen because it is more robust to non-normal data than is a least-squares error method [<xref ref-type="bibr" rid="pbio-0050056-b027">27</xref>]. The 10% most fit individuals are maintained to the next generation. These act as “parents” to the next generation: the parameters for the 3,000 individuals of the next generation come from combinations of pairs of parents and mutations. This procedure was repeated for 25 generations. Best-fit parameters typically did not change past the 20th generation, indicating stable parameter values had been obtained.</p>
        <p>Model comparison: in order to compare the three models described above, the predicted data and observed data were plotted against each other, and a measurement of the variance accounted for by the model, <italic>r</italic><sup>2</sup><italic>,</italic> was calculated. However, when comparing two models with different complexities, for example, with different numbers of parameters, the important distinction between models is their generalizability to new data, that is, the likelihood that the model will fit another set of similar data. The AIC is a measure of model fitness that takes into account both how well the data fit the model and the number of parameters in the model. See the work of Myung et al. [<xref ref-type="bibr" rid="pbio-0050056-b027">27</xref>] for more information about AIC and calculation techniques. Thus, both the AIC and <italic>r</italic><sup>2</sup> values were used to discriminate between different models.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pbio-0050056-sa001" mimetype="audio/wav" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.sa001" xlink:type="simple">
        <label>Audio S1</label>
        <caption>
          <title>Auditory Ripple Sample (Faster)</title>
          <p>An example of one auditory ripple sound used. This sound is “faster” than the other auditory ripple example (<xref ref-type="supplementary-material" rid="pbio-0050056-sa002">Audio S2</xref>), and corresponds to <xref ref-type="fig" rid="pbio-0050056-g001">Figure 1</xref>A with <italic>w</italic> = 16 Hz.</p>
          <p>(31 KB WAV)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0050056-sa002" mimetype="audio/wav" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.sa002" xlink:type="simple">
        <label>Audio S2</label>
        <caption>
          <title>Auditory Ripple Sample (Slower)</title>
          <p>An example of one auditory ripple sound used. This sound is “slower” than the other auditory ripple example (<xref ref-type="supplementary-material" rid="pbio-0050056-sa001">Audio S1</xref>), and corresponds to <xref ref-type="fig" rid="pbio-0050056-g001">Figure 1</xref>B with <italic>w</italic> = 8 Hz.</p>
          <p>(31 KB WAV)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0050056-sd001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.sd001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <title>Possible Differential Learning Effects for Auditory and Visual Data</title>
          <p>(95 KB PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0050056-sv001" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pbio.0050056.sv001" xlink:type="simple">
        <label>Video S1</label>
        <caption>
          <title>Moving Visual Grating Demo</title>
          <p>An example of the moving visual grating stimulus.</p>
          <p>(2.7 MB MOV)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Ray Shanantu Sandip and Shihab Shamma for help with the code used to generate our stimuli, and Christine Mason for assistance in verifying sound levels. Finally, we thank two reviewers for their constructive and insightful comments.</p>
    </ack>
    
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>AIC</term>
          <def>
            <p>Akaike information criterion</p>
          </def>
        </def-item>
        <def-item>
          <term>ANOVA</term>
          <def>
            <p>analysis of variance</p>
          </def>
        </def-item>
        <def-item>
          <term>cps</term>
          <def>
            <p>cycles per second</p>
          </def>
        </def-item>
        <def-item>
          <term>JND</term>
          <def>
            <p>just noticeable difference</p>
          </def>
        </def-item>
        <def-item>
          <term>NEMo</term>
          <def>
            <p>Noisy Exemplar Model</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    <ref-list>
      <title>References</title>
      <ref id="pbio-0050056-b001">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kahana</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name></person-group>
					<year>2002</year>
					<article-title>Recognizing spatial patterns: A noisy exemplar approach.</article-title>
					<source>Vision Res</source>
					<volume>42</volume>
					<fpage>2177</fpage>
					<lpage>2192</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b002">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nosofsky</surname><given-names>R</given-names></name><name name-style="western"><surname>Kantner</surname><given-names>J</given-names></name></person-group>
					<year>2006</year>
					<article-title>Exemplar similarity, study list homogeneity, and short-term perceptual recognition.</article-title>
					<source>Mem Cognit</source>
					<volume>34</volume>
					<fpage>112</fpage>
					<lpage>124</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b003">
        <label>3</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kahana</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>F</given-names></name><name name-style="western"><surname>Geller</surname><given-names>A</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name></person-group>
					<year>2007</year>
					<article-title>Lure-similarity affects visual episodic recognition: Detailed tests of a noisy exemplar model.</article-title>
					<source>Mem Cognit</source>
					<comment>In press.</comment>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b004">
        <label>4</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Yotsumoto</surname><given-names>Y</given-names></name><name name-style="western"><surname>Kahana</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>H</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name></person-group>
					<year>2007</year>
					<article-title>Recognition memory for realistic synthetic faces.</article-title>
					<source>Mem Cognit</source>
					<comment>In press.</comment>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b005">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hwang</surname><given-names>G</given-names></name><name name-style="western"><surname>Jacobs</surname><given-names>J</given-names></name><name name-style="western"><surname>Geller</surname><given-names>A</given-names></name><name name-style="western"><surname>Danker</surname><given-names>J</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name><etal/></person-group>
					<year>2005</year>
					<article-title>EEG correlates of verbal and nonverbal working memory.</article-title>
					<source>Behav Brain Funct</source>
					<volume>1</volume>
					<fpage>20</fpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b006">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sternberg</surname><given-names>S</given-names></name></person-group>
					<year>1966</year>
					<article-title>High-speed scanning in human memory.</article-title>
					<source>Science</source>
					<volume>153</volume>
					<fpage>652</fpage>
					<lpage>654</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b007">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name></person-group>
					<year>2001</year>
					<article-title>On the role of space and time in auditory processing.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>5</volume>
					<fpage>340</fpage>
					<lpage>348</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b008">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Fritz</surname><given-names>J</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name><name name-style="western"><surname>Klein</surname><given-names>D</given-names></name></person-group>
					<year>2003</year>
					<article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex.</article-title>
					<source>Nat Neurosci</source>
					<volume>6</volume>
					<fpage>1216</fpage>
					<lpage>1223</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b009">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>deCharms</surname><given-names>RC</given-names></name><name name-style="western"><surname>Blake</surname><given-names>DT</given-names></name><name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name></person-group>
					<year>1998</year>
					<article-title>Optimizing sound features for cortical neurons.</article-title>
					<source>Science</source>
					<volume>280</volume>
					<fpage>1439</fpage>
					<lpage>1443</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b010">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name><name name-style="western"><surname>Gao</surname><given-names>Y</given-names></name><name name-style="western"><surname>Guyton</surname><given-names>MC</given-names></name><name name-style="western"><surname>Ru</surname><given-names>P</given-names></name><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name></person-group>
					<year>1999</year>
					<article-title>Spectro-temporal modulation transfer functions and speech intelligibility.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>106</volume>
					<fpage>2719</fpage>
					<lpage>2732</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b011">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Boyer</surname><given-names>RW</given-names></name><name name-style="western"><surname>Charleston</surname><given-names>DE</given-names></name></person-group>
					<year>1985</year>
					<article-title>Auditory memory-search.</article-title>
					<source>Percept Mot Skills</source>
					<volume>60</volume>
					<fpage>927</fpage>
					<lpage>939</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b012">
        <label>12</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Purdy</surname><given-names>JE</given-names></name><name name-style="western"><surname>Markham</surname><given-names>MR</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>BL</given-names></name><name name-style="western"><surname>Gordon</surname><given-names>WC</given-names></name></person-group>
					<year>2001</year>
					<source>Learning and Memory. 2nd edition</source>
					<publisher-loc>Belmont (California)</publisher-loc>
					<publisher-name>Wadsworth/Thomson Learning</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">406</size>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b013">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Conrad</surname><given-names>R</given-names></name><name name-style="western"><surname>Hull</surname><given-names>AJ</given-names></name></person-group>
					<year>1968</year>
					<article-title>Input modality and the serial position curve in short-term memory.</article-title>
					<source>Psychon Sci</source>
					<volume>55</volume>
					<fpage>135</fpage>
					<lpage>136</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b014">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wright</surname><given-names>AA</given-names></name></person-group>
					<year>1998</year>
					<article-title>Auditory and visual serial position functions obey different laws.</article-title>
					<source>Psychon Bull Rev</source>
					<volume>4</volume>
					<fpage>564</fpage>
					<lpage>584</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b015">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name><name name-style="western"><surname>Martin</surname><given-names>K</given-names></name></person-group>
					<year>2004</year>
					<article-title>Neuronal circuits of the neocortex.</article-title>
					<source>Annu Rev Neurosci</source>
					<volume>27</volume>
					<fpage>419</fpage>
					<lpage>451</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b016">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Clément</surname><given-names>S</given-names></name><name name-style="western"><surname>Demany</surname><given-names>L</given-names></name><name name-style="western"><surname>Semal</surname><given-names>C</given-names></name></person-group>
					<year>1999</year>
					<article-title>Memory for pitch versus memory for loudness.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>106</volume>
					<fpage>2805</fpage>
					<lpage>2811</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b017">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kubovy</surname><given-names>M</given-names></name><name name-style="western"><surname>Van Valkenburg</surname><given-names>D</given-names></name></person-group>
					<year>2001</year>
					<article-title>Auditory and visual objects.</article-title>
					<source>Cognition</source>
					<volume>80</volume>
					<fpage>97</fpage>
					<lpage>126</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b018">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>F</given-names></name><name name-style="western"><surname>Kahana</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name></person-group>
					<year>2004</year>
					<article-title>Short-term episodic memory for visual textures: A roving probe gathers some memory.</article-title>
					<source>Psychol Sci</source>
					<volume>15</volume>
					<fpage>112</fpage>
					<lpage>118</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b019">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sternberg</surname><given-names>S</given-names></name></person-group>
					<year>1969</year>
					<article-title>The discovery of processing stages: Extensions of Donders' method.</article-title>
					<source>Acta Psychol (Amst)</source>
					<volume>30</volume>
					<fpage>267</fpage>
					<lpage>315</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b020">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Magnussen</surname><given-names>S</given-names></name><name name-style="western"><surname>Greenlee</surname><given-names>M</given-names></name><name name-style="western"><surname>Asplund</surname><given-names>R</given-names></name><name name-style="western"><surname>Dyrnes</surname><given-names>S</given-names></name></person-group>
					<year>1991</year>
					<article-title>Stimulus-specific mechanisms of visual short-term memory.</article-title>
					<source>Vis Res</source>
					<volume>31</volume>
					<fpage>1213</fpage>
					<lpage>1219</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b021">
        <label>21</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Yotsumoto</surname><given-names>Y</given-names></name><name name-style="western"><surname>Kahana</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name></person-group>
					<year>2005</year>
					<article-title>Vision leaves its fingerprints on memory: Recognition and identification memory for compound gratings [abstract].</article-title>
					<source>J Vis</source>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://journalofvision.org/5/8/419/" xlink:type="simple">http://journalofvision.org/5/8/419/</ext-link>. Accessed 10 January 2007.</comment>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b022">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Loftus</surname><given-names>GR</given-names></name><name name-style="western"><surname>Masson</surname><given-names>ME</given-names></name></person-group>
					<year>1994</year>
					<article-title>Using confidence intervals in within-subject designs.</article-title>
					<source>Psychon Bull Rev</source>
					<volume>1</volume>
					<fpage>476</fpage>
					<lpage>490</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b023">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Couisineau</surname><given-names>D</given-names></name></person-group>
					<year>2005</year>
					<article-title>Confidence intervals in within-subject designs: A simpler solution to Loftus and Masson's method.</article-title>
					<comment>Tutor Quant Methods Psychol</comment>
					<volume>1</volume>
					<fpage>42</fpage>
					<lpage>45</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b024">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nosofsky</surname><given-names>R</given-names></name></person-group>
					<year>1986</year>
					<article-title>Attention, similarity, and the identification-categorization relationship.</article-title>
					<source>J Exp Psychol Gen</source>
					<volume>115</volume>
					<fpage>39</fpage>
					<lpage>61</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b025">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nosofsky</surname><given-names>R</given-names></name></person-group>
					<year>1985</year>
					<article-title>Overall similarity and the identification of separable-dimension stimuli: A choice model analysis.</article-title>
					<source>Percept Psychophys</source>
					<volume>38</volume>
					<fpage>415</fpage>
					<lpage>432</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b026">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nosofsky</surname><given-names>R</given-names></name></person-group>
					<year>1984</year>
					<article-title>Choice, similarity, and the context theory of classification.</article-title>
					<source>J Exp Psychol Learn Mem Cogn</source>
					<volume>10</volume>
					<fpage>104</fpage>
					<lpage>114</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b027">
        <label>27</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Myung</surname><given-names>IJ</given-names></name><name name-style="western"><surname>Pitt</surname><given-names>MA</given-names></name><name name-style="western"><surname>Kim</surname><given-names>W</given-names></name></person-group>
					<year>2005</year>
					<article-title>Model evaluation, testing and selection.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Lamberts</surname><given-names>K</given-names></name><name name-style="western"><surname>Goldstone</surname><given-names>RL</given-names></name></person-group>
					<source>Handbook of Cognition</source>
					<publisher-loc>Thousand Oaks (California)</publisher-loc>
					<publisher-name>Sage Publications</publisher-name>
					<fpage>422</fpage>
					<lpage>436</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b028">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Fishbach</surname><given-names>A</given-names></name><name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name><name name-style="western"><surname>Yeshurun</surname><given-names>Y</given-names></name></person-group>
					<year>2001</year>
					<article-title>Auditory edge detection: A neural model for physiological and psychoacoustical responses to amplitude transients.</article-title>
					<source>J Neurophysiol</source>
					<volume>85</volume>
					<fpage>2303</fpage>
					<lpage>2323</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b029">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name><name name-style="western"><surname>Field</surname><given-names>D</given-names></name></person-group>
					<year>2004</year>
					<article-title>Sparse coding of sensory inputs.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>14</volume>
					<fpage>481</fpage>
					<lpage>487</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b030">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name><name name-style="western"><surname>Wiesel</surname><given-names>T</given-names></name></person-group>
					<year>1959</year>
					<article-title>Receptive fields of single neurones in the cat's striate cortex.</article-title>
					<source>J Physiol</source>
					<volume>148</volume>
					<fpage>574</fpage>
					<lpage>591</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b031">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ward</surname><given-names>G</given-names></name><name name-style="western"><surname>Avons</surname><given-names>S</given-names></name><name name-style="western"><surname>Melling</surname><given-names>L</given-names></name></person-group>
					<year>2005</year>
					<article-title>Serial position curves in short-term memory: Functional equivalence across modalities.</article-title>
					<source>Memory</source>
					<volume>13</volume>
					<fpage>308</fpage>
					<lpage>317</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b032">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Cowan</surname><given-names>N</given-names></name><name name-style="western"><surname>Saults</surname><given-names>J</given-names></name><name name-style="western"><surname>Brown</surname><given-names>G</given-names></name></person-group>
					<year>2004</year>
					<article-title>On the auditory modality superiority effect in serial recall: Separating input and output factors.</article-title>
					<source>J Exp Psychol Learn Mem Cogn</source>
					<volume>30</volume>
					<fpage>639</fpage>
					<lpage>644</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b033">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Baddeley</surname><given-names>A</given-names></name><name name-style="western"><surname>Hitch</surname><given-names>G</given-names></name></person-group>
					<year>1993</year>
					<article-title>The recency effect: Implicit learning with explicit retrieval?</article-title>
					<source>Mem Cognit</source>
					<volume>21</volume>
					<fpage>146</fpage>
					<lpage>155</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b034">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Agam</surname><given-names>Y</given-names></name><name name-style="western"><surname>Bullock</surname><given-names>D</given-names></name><name name-style="western"><surname>Sekuler</surname><given-names>R</given-names></name></person-group>
					<year>2005</year>
					<article-title>Imitating unfamiliar sequences of connected linear motions.</article-title>
					<source>J Neurophysiol</source>
					<volume>94</volume>
					<fpage>2832</fpage>
					<lpage>2843</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b035">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sternberg</surname><given-names>S</given-names></name></person-group>
					<year>1967</year>
					<article-title>Retrieval of contextual information from memory.</article-title>
					<source>Psychon Sci</source>
					<volume>8</volume>
					<fpage>55</fpage>
					<lpage>56</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b036">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Forrin</surname><given-names>B</given-names></name><name name-style="western"><surname>Cunningham</surname><given-names>K</given-names></name></person-group>
					<year>1973</year>
					<article-title>Recognition time and serial position of probed item in short-term memory.</article-title>
					<source>J Exp Psychol</source>
					<volume>99</volume>
					<fpage>272</fpage>
					<lpage>279</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b037">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wright</surname><given-names>AA</given-names></name></person-group>
					<year>1999</year>
					<article-title>Auditory list memory and interference processes in monkeys.</article-title>
					<source>J Exp Psychol Anim Behav Process</source>
					<volume>25</volume>
					<fpage>284</fpage>
					<lpage>296</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b038">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Deutsch</surname><given-names>D</given-names></name></person-group>
					<year>1974</year>
					<article-title>Generality of interference by tonal stimuli in recognition memory for pitch.</article-title>
					<source>Q J Exp Psychol</source>
					<volume>26</volume>
					<fpage>229</fpage>
					<lpage>234</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b039">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wheeler</surname><given-names>ME</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>SE</given-names></name><name name-style="western"><surname>Buckner</surname><given-names>RL</given-names></name></person-group>
					<year>2000</year>
					<article-title>Memory's echo: Vivid remembering reactivates sensory-specific cortex.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>97</volume>
					<fpage>11125</fpage>
					<lpage>11129</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b040">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Pasternak</surname><given-names>T</given-names></name><name name-style="western"><surname>Greenlee</surname><given-names>MW</given-names></name></person-group>
					<year>2005</year>
					<article-title>Working memory in primate sensory systems.</article-title>
					<source>Nat Rev Neurosci</source>
					<volume>6</volume>
					<fpage>97</fpage>
					<lpage>107</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b041">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Colombo</surname><given-names>M</given-names></name><name name-style="western"><surname>D'Amato</surname><given-names>M</given-names></name><name name-style="western"><surname>Rodman</surname><given-names>H</given-names></name><name name-style="western"><surname>Gross</surname><given-names>C</given-names></name></person-group>
					<year>1990</year>
					<article-title>Auditory association cortex lesions impair auditory short-term memory in monkeys.</article-title>
					<source>Science</source>
					<volume>247</volume>
					<fpage>336</fpage>
					<lpage>338</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b042">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gabrieli</surname><given-names>J</given-names></name><name name-style="western"><surname>Poldrack</surname><given-names>R</given-names></name><name name-style="western"><surname>Desmond</surname><given-names>J</given-names></name></person-group>
					<year>1998</year>
					<article-title>The role of left prefrontal cortex in language and memory.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>95</volume>
					<fpage>906</fpage>
					<lpage>913</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b043">
        <label>43</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="editor"><name name-style="western"><surname>Gazzaniga</surname><given-names>MS</given-names></name><name name-style="western"><surname>Ivry</surname><given-names>RB</given-names></name><name name-style="western"><surname>Mangun</surname><given-names>GR</given-names></name></person-group>
					<year>2002</year>
					<source>Cognitive neuroscience: The biology of the mind</source>
					<comment>2nd edition.</comment>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>W.W. Norton and Company</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">681</size>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b044">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Elliott</surname><given-names>E</given-names></name><name name-style="western"><surname>Cowan</surname><given-names>N</given-names></name><name name-style="western"><surname>Valle-Inclan</surname><given-names>F</given-names></name></person-group>
					<year>1998</year>
					<article-title>The nature of cross-modal color-word interference effects.</article-title>
					<source>Percept Psychophys</source>
					<volume>60</volume>
					<fpage>761</fpage>
					<lpage>767</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b045">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Weinberger</surname><given-names>N</given-names></name></person-group>
					<year>2004</year>
					<article-title>Specific long-term memory traces in primary auditory cortex.</article-title>
					<source>Nat Rev Neurosci</source>
					<volume>5</volume>
					<fpage>279</fpage>
					<lpage>290</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b046">
        <label>46</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Newton</surname><given-names>I</given-names></name></person-group>
					<year>1756–1757</year>
					<article-title>An hypothesis explaining the properties of light.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Birch</surname><given-names>T</given-names></name></person-group>
					<source>The history of the Royal Society of London for improving of natural knowledge. Volume 3</source>
					<publisher-loc>London</publisher-loc>
					<comment>Printed for</comment>
					<publisher-name>A. Millar</publisher-name>
					<fpage>247</fpage>
					<lpage>305</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b047">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Supin</surname><given-names>AY</given-names></name><name name-style="western"><surname>Popov</surname><given-names>VV</given-names></name><name name-style="western"><surname>Milekhina</surname><given-names>ON</given-names></name><name name-style="western"><surname>Tarakanov</surname><given-names>MB</given-names></name></person-group>
					<year>1998</year>
					<article-title>Ripple density resolution for various rippled-noise patterns.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>103</volume>
					<fpage>2042</fpage>
					<lpage>2050</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b048">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Supin</surname><given-names>A</given-names></name><name name-style="western"><surname>Popov</surname><given-names>V</given-names></name><name name-style="western"><surname>Milekhina</surname><given-names>O</given-names></name><name name-style="western"><surname>Tarakanov</surname><given-names>M</given-names></name></person-group>
					<year>2003</year>
					<article-title>Rippled-spectrum resolution dependence on level.</article-title>
					<source>Hear Res</source>
					<volume>185</volume>
					<fpage>1</fpage>
					<lpage>12</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b049">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Brainard</surname><given-names>D</given-names></name></person-group>
					<year>1997</year>
					<article-title>The Psychophysics Toolbox.</article-title>
					<source>Spat Vis</source>
					<volume>10</volume>
					<fpage>433</fpage>
					<lpage>436</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b050">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Watson</surname><given-names>AB</given-names></name><name name-style="western"><surname>Pelli</surname><given-names>DG</given-names></name></person-group>
					<year>1983</year>
					<article-title>QUEST: A Bayesian adaptive psychometric method.</article-title>
					<source>Percept Psychophys</source>
					<volume>33</volume>
					<fpage>113</fpage>
					<lpage>120</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0050056-b051">
        <label>51</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Mitchell</surname><given-names>M</given-names></name></person-group>
					<year>1996</year>
					<source>An introduction to genetic algorithms</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">205</size>
				</element-citation>
      </ref>
    </ref-list>
  </back>
</article>