<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-10-00363</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002063</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Biophysics</subject>
            <subj-group>
              <subject>Biophysics simulations</subject>
              <subject>Biophysics theory</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Circuit models</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Circuit models</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Learning and memory</subject>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Theoretical biology</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Computer modeling</subject>
          </subj-group>
          <subj-group>
            <subject>Computerized simulations</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied mathematics</subject>
            <subj-group>
              <subject>Complex systems</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Probability theory</subject>
            <subj-group>
              <subject>Statistical distributions</subject>
              <subj-group>
                <subject>Distribution curves</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical methods</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Physics</subject>
          <subj-group>
            <subject>Biophysics</subject>
            <subj-group>
              <subject>Biophysics simulations</subject>
              <subject>Biophysics theory</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Interdisciplinary physics</subject>
          </subj-group>
          <subj-group>
            <subject>Statistical mechanics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Biophysics</subject>
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
          <subject>Physics</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>Learning, Memory, and the Role of Neural Network Architecture</article-title><alt-title alt-title-type="running-head">Learning and Memory in Neural Networks</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Hermundstad</surname>
            <given-names>Ann M.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Brown</surname>
            <given-names>Kevin S.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Bassett</surname>
            <given-names>Danielle S.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Carlson</surname>
            <given-names>Jean M.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Physics Department, University of California, Santa Barbara, Santa Barbara, California, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">ann@physics.ucsb.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: AMH KSB DSB JMC. Performed the experiments: AMH. Analyzed the data: AMH KSB DSB JMC. Wrote the paper: AMH KSB DSB JMC.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>6</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>30</day>
        <month>6</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>6</issue><elocation-id>e1002063</elocation-id><history>
        <date date-type="received">
          <day>9</day>
          <month>12</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>6</day>
          <month>4</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Hermundstad et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>The performance of information processing systems, from artificial neural networks to natural neuronal ensembles, depends heavily on the underlying system architecture. In this study, we compare the performance of parallel and layered network architectures during sequential tasks that require both acquisition and retention of information, thereby identifying tradeoffs between learning and memory processes. During the task of supervised, sequential function approximation, networks produce and adapt representations of external information. Performance is evaluated by statistically analyzing the error in these representations while varying the initial network state, the structure of the external information, and the time given to learn the information. We link performance to complexity in network architecture by characterizing local error landscape curvature. We find that variations in error landscape structure give rise to tradeoffs in performance; these include the ability of the network to maximize accuracy versus minimize inaccuracy and produce specific versus generalizable representations of information. Parallel networks generate smooth error landscapes with deep, narrow minima, enabling them to find highly specific representations given sufficient time. While accurate, however, these representations are difficult to generalize. In contrast, layered networks generate rough error landscapes with a variety of local minima, allowing them to quickly find coarse representations. Although less accurate, these representations are easily adaptable. The presence of measurable performance tradeoffs in both layered and parallel networks has implications for understanding the behavior of a wide variety of natural and artificial learning systems.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Information processing systems, such as natural biological networks and artificial computational networks, exhibit a strong interdependence between structural organization and functional performance. However, the extent to which variations in structure impact performance is not well understood, particularly in systems whose functionality must be simultaneously flexible and stable. By statistically analyzing the behavior of network systems during flexible learning and stable memory processes, we quantify the impact of structural variations on the ability of the network to learn, modify, and retain representations of information. Across a range of architectures drawn from both natural and artificial systems, we show that these networks face tradeoffs between the ability to learn and retain information, and the observed behavior varies depending on the initial network state and the time given to process information. Furthermore, we analyze the difficulty with which different network architectures produce accurate versus generalizable representations of information, thereby identifying the structural mechanisms that give rise to functional tradeoffs between learning and memory.</p>
      </abstract><funding-group><funding-statement>This work was supported by the David and Lucile Packard Foundation and the Institute for Collaborative Biotechnologies through contract no. W911NF-09-D-0001 from the U.S. Army Research Office. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="12"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Learning, the assimilation of new information, and memory, the retention of old information, are competing processes; the first requires flexibility and the second stability in the presence of external stimuli. Varying structural complexity could uncover tradeoffs between flexibility and stability, particularly when comparing the functional performance of structurally distinct learning systems. We use neural networks as model learning systems to explore these tradeoffs in system architectures inspired by both biology and computer science, considering layered structures like those found in cortical lamina <xref ref-type="bibr" rid="pcbi.1002063-Mountcastle1">[1]</xref> and parallel structures such as those used for clustering <xref ref-type="bibr" rid="pcbi.1002063-Jain1">[2]</xref>, image processing <xref ref-type="bibr" rid="pcbi.1002063-EgmontPetersen1">[3]</xref>, and forecasting <xref ref-type="bibr" rid="pcbi.1002063-Zhang1">[4]</xref>. We find inherent tradeoffs in network performance, most notably between acquisition versus retention of information and between the ability of the network to maximize success versus minimize failure during sequential learning and memory tasks. Identifying tradeoffs in performance that arise from complexity in architecture is crucial for understanding the relationship between structure and function in both natural and artificial learning systems.</p>
      <p>Natural neuronal systems display a complex combination of serial and parallel <xref ref-type="bibr" rid="pcbi.1002063-Chittka1">[5]</xref> structural motifs which enable the performance of disparate functions <xref ref-type="bibr" rid="pcbi.1002063-Honey1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Scholz1">[9]</xref>. For example, layered <xref ref-type="bibr" rid="pcbi.1002063-Mountcastle1">[1]</xref> and hierarchical <xref ref-type="bibr" rid="pcbi.1002063-Bassett1">[10]</xref> architectures theoretically important for sustained limited activity <xref ref-type="bibr" rid="pcbi.1002063-Kaiser1">[11]</xref> have been consistently identified over a range of spatial scales in primate cortical systems <xref ref-type="bibr" rid="pcbi.1002063-Reid1">[12]</xref>. Neurons themselves are organized into layers, or “lamina,” and both intra-laminar <xref ref-type="bibr" rid="pcbi.1002063-Ress1">[13]</xref> and inter-laminar <xref ref-type="bibr" rid="pcbi.1002063-Atencio1">[14]</xref> connectivity differentially impact function. Similarly, information processing systems developed by technological innovation rather than natural evolution have structures designed to match their functionality. For example, the topological complexity of very large integrated circuits scales with the function to be performed <xref ref-type="bibr" rid="pcbi.1002063-Bakoglu1">[15]</xref>. Likewise, the internal structure of artificial neural networks can be carefully constructed <xref ref-type="bibr" rid="pcbi.1002063-Galushkin1">[16]</xref> to enable these systems to learn a variety of complex relationships. While parallel, rather than serial, structures are appealing in artificial neural networks because of their efficiency and speed, variations in structure may provide additional benefits or drawbacks during the performance of sequential tasks.</p>
      <p>The dependence of functional performance on structural architecture can be systematically examined within the framework of neural networks, where the complexity of both the network architecture and the external information can be precisely varied. In this study, we evaluate the representations of information produced by feedforward neural networks during supervised, sequential tasks that require both acquisition and retention of information. Our approach is quite different from studies in which large, dense networks are given an extended period of time to produce highly accurate representations of information (e.g. <xref ref-type="bibr" rid="pcbi.1002063-Fukushima1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Robinson1">[18]</xref>). Instead, we investigate the links between structure and function by performing a statistical analysis of the error in the representations produced by small networks during short training sessions, thereby identifying mechanisms that underlie tradeoffs in performance. Our work therefore has important implications for understanding the behavior of larger, more complicated systems in which statistical studies of performance would be impossible.</p>
      <p>In the remainder of the paper, we discuss the extent to which network architectures differ in their ability to both learn and retain information. We first describe the network model and architectures considered in this study. We then quantify the best, worst, and average performance achieved by each network during sequential tasks that vary in both their duration and complexity. We consider the adaptability of these networks to variable initial states, thereby probing the structure of functional error landscapes. Finally, we explore how landscape variations that arise from structural complexity lead to differences in performance.</p>
    </sec>
    <sec id="s2">
      <title>Models</title>
      <sec id="s2a">
        <title>Sequential Learning Approach</title>
        <p>Our approach differs from traditional machine learning studies in that our goal is not to design the optimal network system for performing a specific task. Rather, we identify tradeoffs in network performance across a range of architectures that share a common algorithmic framework. In this context, the term “architecture” refers specifically to the structural organization of network connections and not, as is found in engineering studies, to the broader set of constraints governing the interactions of network components.</p>
        <p>In evaluating network performance, we use techniques relevant to both artificial and biological systems. Artificial network systems often favor high accuracy and consistency during a single task, regardless of the time required to achieve such a solution. In biological systems, however, speed and generalizability are often more important that absolute accuracy when dynamically adapting to a variety of tasks. To probe features such as network accuracy, consistency, speed, and adaptability, we examine the representations of information produced by neural networks during competing learning and memory tasks.</p>
        <p>We choose to study learning and memory within the biologically-motivated framework of feedforward, backpropagation (FFBP) artificial neural networks that perform the task of supervised, one-dimensional function approximation. The training process, which consists of adjusting internal connection strengths to minimize the network error on a set of external data points, can be mapped to motion within a continuous error landscape. Within this context, “learning” refers to the ability of the network to successfully navigate this landscape and produce an accurate functional representation of a set of data points, while “memory” refers to the ability to store a representation of previously-learned information. Additional details of this framework are described in the following subsection.</p>
        <p>To simultaneously study learning and memory processes, information must be presented to the network sequentially. “Catastrophic forgetting,” in which a network learns new information at the cost of forgetting old information, is a longstanding problem in sequential training of neural networks and has been addressed with several types of rehearsal methods <xref ref-type="bibr" rid="pcbi.1002063-McCloskey1">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Sharkey1">[21]</xref>. Standard rehearsal involves training the network with both the original and new information during sequential training sessions. We use a more biologically motivated approach, the pseudorehearsal method <xref ref-type="bibr" rid="pcbi.1002063-Robins1">[22]</xref>, in which the network trains with a <italic>representation</italic> of the original information. Pseudorehearsal has been shown to prevent catastrophic forgetting in both feedforward and recurrent networks and does not require extensive storage of examples <xref ref-type="bibr" rid="pcbi.1002063-Robins1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Robins2">[23]</xref>.</p>
        <p>In training FFBP networks, local minima and plateaus within the error landscape can prevent the network from finding a global optimum <xref ref-type="bibr" rid="pcbi.1002063-Auer1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Fukumizu1">[25]</xref>. While considered disadvantageous in machine learning studies, the existence of local minima may provide benefits during the training process, particularly in biological systems for which highly accurate global optimums may be unnecessary or undesirable. Additionally, FFBP networks can suffer from overfitting, a problem in which the creation of highly specific representations of information hinders the ability of the network to generalize to new situations <xref ref-type="bibr" rid="pcbi.1002063-Rojas1">[26]</xref>. While also considered disadvantageous, failure to generalize has important biological consequences and has been linked to neurological development disorders such as Autism <xref ref-type="bibr" rid="pcbi.1002063-Cohen1">[27]</xref>. Instead of attempting to eliminate these sensitivities, we seek to understand the architectural basis for differences in landscape features and examine their impact on representational capabilities such as specificity and generalizability.</p>
      </sec>
      <sec id="s2b">
        <title>Neural Network Model</title>
        <p>The construction of our network model is consistent with standard FFBP neural network models <xref ref-type="bibr" rid="pcbi.1002063-Rojas1">[26]</xref>. We consider the five distinct architectures shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>, all of which obey identical training rules. Each network has 12 hidden nodes arranged into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e001" xlink:type="simple"/></inline-formula> layers of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e002" xlink:type="simple"/></inline-formula> nodes per layer. Nodes in adjacent layers are connected via variable, unidirectional weights. The “fan” and “stacked” networks are both fully connected and have the same total number of connections. The connectivities of the “intermediate” networks, which have slightly greater numbers of connections, were chosen in order to roughly maintain the same total number of adjustable parameters per network, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e003" xlink:type="simple"/></inline-formula>, noted in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>.</p>
        <fig id="pcbi-1002063-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002063.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Network architectures and training task.</title>
            <p>(a) Network architectures considered in this study. Indicated below each network are the number of hidden layers <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e004" xlink:type="simple"/></inline-formula> and nodes per layer <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e005" xlink:type="simple"/></inline-formula>, the total number of adjustable parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e006" xlink:type="simple"/></inline-formula>, and the name by which we refer to the network. (b) Illustration of the sequential learning task described in the text applied to the fan network. Each step of the task includes a concise description of the procedure and the choice of network weights and training data.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.g001" xlink:type="simple"/>
        </fig>
        <p>Each node has a sigmoid transfer function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e007" xlink:type="simple"/></inline-formula> with a variable threshold <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e008" xlink:type="simple"/></inline-formula>. The output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e009" xlink:type="simple"/></inline-formula> of each node is a function of the weighted sum of its inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e010" xlink:type="simple"/></inline-formula>, given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e011" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e012" xlink:type="simple"/></inline-formula> gives the weight of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e013" xlink:type="simple"/></inline-formula> input connection. Representing the threshold as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e014" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e015" xlink:type="simple"/></inline-formula> for all nodes, allows us to organize all adjustable parameters into a single, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e016" xlink:type="simple"/></inline-formula>-dimensional weight vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e017" xlink:type="simple"/></inline-formula>.</p>
        <p>During training, each network is presented with a training pattern of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e018" xlink:type="simple"/></inline-formula> pairs of input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e019" xlink:type="simple"/></inline-formula> and target <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e020" xlink:type="simple"/></inline-formula> values, denoted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e021" xlink:type="simple"/></inline-formula>. We restrict the input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e022" xlink:type="simple"/></inline-formula> space to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e023" xlink:type="simple"/></inline-formula>, and the sigmoid transfer function restricts the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e024" xlink:type="simple"/></inline-formula> space to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e025" xlink:type="simple"/></inline-formula>. The set of variable weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e026" xlink:type="simple"/></inline-formula> is iteratively updated via the Polak-Ribiere conjugate gradient descent method with an adaptive step size <xref ref-type="bibr" rid="pcbi.1002063-Fletcher1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Powell1">[30]</xref> in order to minimize the output error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e027" xlink:type="simple"/></inline-formula>. We use online training, for which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e028" xlink:type="simple"/></inline-formula> is the sum of squared errors between the network output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e029" xlink:type="simple"/></inline-formula> and target output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e030" xlink:type="simple"/></inline-formula> calculated after all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e031" xlink:type="simple"/></inline-formula> points are presented to the network:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e032" xlink:type="simple"/><label>(1)</label></disp-formula></p>
      </sec>
      <sec id="s2c">
        <title>Task Implementation</title>
        <p>Each network shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref> is trained over two sequential sessions. In describing parameter choices for each training session, we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e033" xlink:type="simple"/></inline-formula> to denote a continuous uniform probability distribution over the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e034" xlink:type="simple"/></inline-formula>. The steps of the sequential training process are shown schematically in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(b)</xref> and are described below:</p>
      </sec>
      <sec id="s2d">
        <title>First Training Session</title>
        <sec id="s2d1">
          <title>Step 1.1 - Initialize</title>
          <p>Network weights are randomly chosen from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e035" xlink:type="simple"/></inline-formula>. We refer to this state of the network as the “randomly initialized state”.</p>
        </sec>
        <sec id="s2d2">
          <title>Step 1.2 - Train</title>
          <p>The network trains on six “original” points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e036" xlink:type="simple"/></inline-formula> whose values remain fixed for all simulations. The original points are chosen to be evenly spaced in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e037" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e038" xlink:type="simple"/></inline-formula>) and random in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e039" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e040" xlink:type="simple"/></inline-formula>). Similar behavior is observed for different choices, including permutations, of the specific values used here (see <xref ref-type="supplementary-material" rid="pcbi.1002063.s003">Figure S3</xref>). The original points represent the information we wish the network to remember during subsequent training. The network is given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e041" xlink:type="simple"/></inline-formula> iterations to generate a functional representation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e042" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e043" xlink:type="simple"/></inline-formula> (see second panel of <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(b)</xref> and <xref ref-type="fig" rid="pcbi-1002063-g002">Figures 2(a) and 2(b)</xref>), and training ceases if the error plateaus (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e044" xlink:type="simple"/></inline-formula> for 1000 iterations). We refer to this situation as allowing “unlimited” training time because in practice, the network finds a solution before reaching the maximum number of iterations.</p>
          <fig id="pcbi-1002063-g002" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002063.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Network solutions and error distributions.</title>
              <p>Panels (a) and (b) show solutions produced respectively by the fan and stacked networks, indicating for each network the approximation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e045" xlink:type="simple"/></inline-formula> (solid curve) of the original points (point markers) and a subset of approximations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e046" xlink:type="simple"/></inline-formula> (dashed curves) of the new and buffer points. In this realization, the fan network fits the original points with a high order polynomial, while the stacked network produces a largely linear fit. Subsequent approximations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e047" xlink:type="simple"/></inline-formula> retain these features of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e048" xlink:type="simple"/></inline-formula>. Panels (c) and (d) respectively show the CDFs of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e049" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e050" xlink:type="simple"/></inline-formula>, with the average value of each distribution marked by a filled circle. (c) The fan network achieves a lower minimum but higher maximum error on the original points than does the stacked network, resulting in a wider distribution with a higher average error. (d) Both networks produce low minimum errors on the new points, but the fan network again produces higher average and maximum errors than does the stacked network. These results are qualitatively similar given larger networks (<xref ref-type="supplementary-material" rid="pcbi.1002063.s001">Figure S1</xref>) and different sets of original points (<xref ref-type="supplementary-material" rid="pcbi.1002063.s003">Figure S3</xref>).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.g002" xlink:type="simple"/>
          </fig>
        </sec>
      </sec>
      <sec id="s2e">
        <title>Second Training Session</title>
        <sec id="s2e1">
          <title>Step 2.1 - Sample</title>
          <p>The set of weights that produce <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e051" xlink:type="simple"/></inline-formula> forms the starting point for the second training session. We refer to this state of the network as the “sampled state” in order to distinguish it from the randomly initialized state chosen prior to the first training session. In this state, the network randomly samples a pool of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e052" xlink:type="simple"/></inline-formula> buffer points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e053" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e054" xlink:type="simple"/></inline-formula> (see third panel of <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(b)</xref>). This is accomplished by (<italic>i</italic>) randomly choosing input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e055" xlink:type="simple"/></inline-formula> values from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e056" xlink:type="simple"/></inline-formula> and (<italic>ii</italic>) computing the corresponding output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e057" xlink:type="simple"/></inline-formula> values using the set of network weights that produce <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e058" xlink:type="simple"/></inline-formula>. Subsets of buffer points, which lie along the functional representation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e059" xlink:type="simple"/></inline-formula> of the original points, are used in the following step to simulate memory rehearsal.</p>
        </sec>
        <sec id="s2e2">
          <title>Step 2.2 - Re-train</title>
          <p>The network re-trains on six new points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e060" xlink:type="simple"/></inline-formula> and six buffer points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e061" xlink:type="simple"/></inline-formula> (see fourth panel of <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(b)</xref>). New points are chosen by randomly selecting six independent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e063" xlink:type="simple"/></inline-formula> values from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e064" xlink:type="simple"/></inline-formula>. Buffer points are chosen by randomly selecting, with uniform probability, six <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e065" xlink:type="simple"/></inline-formula> pairs from the pool of the buffer points generated in <italic>Step 2.1</italic>. Training on the same number of new and buffer points places equal emphasis on learning and memory rehearsal. Because the new points are randomly chosen and poorly constrained, we repeat the second training session <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e066" xlink:type="simple"/></inline-formula> times to generate a distribution of solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e067" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1002063-g002">Figures 2(a) and 2(b)</xref>). Both the new and buffer points vary from session to session, but the buffer points are always sampled from the same original function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e068" xlink:type="simple"/></inline-formula>. We restrict the training time of each session to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e069" xlink:type="simple"/></inline-formula> iterations, thereby giving the network “limited” time to learn.</p>
        </sec>
        <sec id="s2e3">
          <title>Notation</title>
          <p>We use the super and subscripts “<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e070" xlink:type="simple"/></inline-formula>” and “<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e071" xlink:type="simple"/></inline-formula>” to refer respectively to the “original” and “new” points, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e072" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e073" xlink:type="simple"/></inline-formula>, and functional approximations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e074" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e075" xlink:type="simple"/></inline-formula>. Each function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e076" xlink:type="simple"/></inline-formula> produces a single error value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e077" xlink:type="simple"/></inline-formula> measured with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e078" xlink:type="simple"/></inline-formula>. Each set of functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e079" xlink:type="simple"/></inline-formula> produces two sets of error values, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e080" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e081" xlink:type="simple"/></inline-formula>, measured with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e083" xlink:type="simple"/></inline-formula>, respectively.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Tradeoffs in Learning and Memory Tasks</title>
        <p>We train the five networks shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>, first considering the differences between the boundary fan (parallel) and stacked (layered) networks. Given the large number of adjustable parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e084" xlink:type="simple"/></inline-formula> relative to the small number of training points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e085" xlink:type="simple"/></inline-formula>, we expect all five networks to fit the points with high accuracy. Instead, the networks show significant differences in performance both within individual training sessions and measured statistically over many sessions. These results, discussed in detail below, show the same qualitative features for larger networks (<xref ref-type="supplementary-material" rid="pcbi.1002063.s001">Figures S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002063.s002">S2</xref>) and for different sets of original points (<xref ref-type="supplementary-material" rid="pcbi.1002063.s003">Figures S3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002063.s004">S4</xref>).</p>
        <sec id="s3a1">
          <title>Fan and stacked architectures</title>
          <p>Examples of the solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e086" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e087" xlink:type="simple"/></inline-formula> produced by the fan and stacked networks are shown in <xref ref-type="fig" rid="pcbi-1002063-g002">Figures 2(a) and 2(b)</xref>. Each set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e088" xlink:type="simple"/></inline-formula> is characterized by errors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e089" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e090" xlink:type="simple"/></inline-formula>, which measure the ability of the network to retain and learn information, respectively. The cumulative distribution functions (CDFs) of these errors are shown in <xref ref-type="fig" rid="pcbi-1002063-g002">Figures 2(c) and 2(d)</xref>, where the CDF gives the probability that the network produces an error greater than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e091" xlink:type="simple"/></inline-formula> for any value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e092" xlink:type="simple"/></inline-formula>.</p>
          <p>The fan and stacked networks produce qualitatively different types of solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e093" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e094" xlink:type="simple"/></inline-formula>. While the specific functional form of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e095" xlink:type="simple"/></inline-formula> depends on the randomly initialized network state (see the following section), the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e096" xlink:type="simple"/></inline-formula> solutions shown here have errors that are representative of the average network performance over a range of randomly initialized states. The stacked solution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e097" xlink:type="simple"/></inline-formula> averages over the variation in the original points (<xref ref-type="fig" rid="pcbi-1002063-g002">Figure 2(b)</xref>). In contrast, the fan solution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e098" xlink:type="simple"/></inline-formula> accurately fits all six original points with a high order polynomial (<xref ref-type="fig" rid="pcbi-1002063-g002">Figure 2(a)</xref>). In both networks, subsequent solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e099" xlink:type="simple"/></inline-formula> retain the features of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e100" xlink:type="simple"/></inline-formula>. Because the sigmoid transfer function (see Methods) is identical for all nodes, the differences between the fan and stacked solutions arise solely from variations in network architecture. As the sigmoid function maps an infinite input space to a finite output space bounded between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e101" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e102" xlink:type="simple"/></inline-formula>, successive applications of sigmoids produced by serial (stacked) computations tend to result in linear or step function outputs, while a sum of sigmoids produced by parallel (fan) computations tends to result in highly variable outputs.</p>
          <p>The interference between the two training sessions results in the deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e103" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e104" xlink:type="simple"/></inline-formula>, which tends to increase <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e105" xlink:type="simple"/></inline-formula> relative to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e106" xlink:type="simple"/></inline-formula>. We find that in its best case, the stacked network shows no deviation in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e107" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e108" xlink:type="simple"/></inline-formula>. In contrast, the fan network shows a minimum deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e109" xlink:type="simple"/></inline-formula> and a higher deviation on average compared to the stacked network. This deviation measures the ability of the network to retain the original representation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e110" xlink:type="simple"/></inline-formula>, regardless of how erroneous that representation may be. Although the stacked network generates a higher error representation of the original points during the first training session, it can more accurately retain this representation when presented with new points.</p>
          <p>The minimum and maximum values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e111" xlink:type="simple"/></inline-formula> measure the best success and worst failure of the network in retaining old information while avoiding interference from new information. While the bounded output space limits the maximum error, linear solutions tend to further restrict these bounds. As a result, the stacked network has a lower maximum error at the cost of having a higher minimum error, as shown in <xref ref-type="fig" rid="pcbi-1002063-g002">Figure 2(c)</xref>. In contrast, the fan network can retain the original information more accurately by achieving a lower minimum error, but it can also fail more catastrophically with a higher maximum error.</p>
          <p>Similar features are observed in the distributions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e112" xlink:type="simple"/></inline-formula> shown in <xref ref-type="fig" rid="pcbi-1002063-g002">Figure 2(d)</xref>. The minimum and maximum values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e113" xlink:type="simple"/></inline-formula> measure the best success and worst failure of the network in learning new information while attempting to retain old information. While both networks achieve low minimum error at their best, the fan network produces a much larger maximum error than does the stacked network. In addition to achieving more extreme best and worst cases, the fan network also has higher average error values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e114" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e115" xlink:type="simple"/></inline-formula>.</p>
        </sec>
        <sec id="s3a2">
          <title>Intermediate architectures: Tradeoffs in learning and memory</title>
          <p>We extend this analysis to the intermediate architecftures shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>, organizing the results based on the degree of network serialization <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e116" xlink:type="simple"/></inline-formula> (a purely geometrical factor).</p>
          <p>Tradeoffs in performance are observed across the range of architectures. For example, in <xref ref-type="fig" rid="pcbi-1002063-g003">Figure 3(a)</xref>, we see a tradeoff between the minimum and maximum values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e117" xlink:type="simple"/></inline-formula>. As <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e118" xlink:type="simple"/></inline-formula> increases, the network does not fail as badly in its worst case but also does not succeed as well in its best case. <xref ref-type="fig" rid="pcbi-1002063-g003">Figure 3(b)</xref> shows that increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e119" xlink:type="simple"/></inline-formula> decreases the maximum error in both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e120" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e121" xlink:type="simple"/></inline-formula>, indicating that the stacked architecture is best suited for minimizing failure in both learning and memory. <xref ref-type="fig" rid="pcbi-1002063-g003">Figure 3(c)</xref> shows that increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e122" xlink:type="simple"/></inline-formula> decreases both the average solution variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e123" xlink:type="simple"/></inline-formula> and the average errors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e124" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e125" xlink:type="simple"/></inline-formula>. While we might naively expect that high solution variance (fan) would indicate a flexible network able to accurately fit nonlinear data, we instead find that high variance leads to high average error. In contrast, low variance, linear solutions (stacked) tend to minimize average error.</p>
          <fig id="pcbi-1002063-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002063.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Tradeoffs in network learning and memory.</title>
              <p>Best, worst, and average network performance is measured with respect to solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e126" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e127" xlink:type="simple"/></inline-formula> produced by the five networks shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>. With respect to solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e128" xlink:type="simple"/></inline-formula> produced during the second training session, increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e129" xlink:type="simple"/></inline-formula> (a) decreases the maximum value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e130" xlink:type="simple"/></inline-formula> at the cost of increasing its minimum value, (b) decreases the maximum error in both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e131" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e132" xlink:type="simple"/></inline-formula>, and (c) decreases the average solution variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e133" xlink:type="simple"/></inline-formula> and the average errors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e134" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e135" xlink:type="simple"/></inline-formula>. (d) Increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e136" xlink:type="simple"/></inline-formula> increases <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e137" xlink:type="simple"/></inline-formula> achieved during the first session but decreases <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e138" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e139" xlink:type="simple"/></inline-formula> achieved during the second session. These results are qualitatively similar given larger networks (<xref ref-type="supplementary-material" rid="pcbi.1002063.s002">Figure S2</xref>) and different sets of original points (<xref ref-type="supplementary-material" rid="pcbi.1002063.s004">Figure S4</xref>).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.g003" xlink:type="simple"/>
          </fig>
          <p>Furthermore, we find a tradeoff in performance between the first and second sessions, shown in <xref ref-type="fig" rid="pcbi-1002063-g003">Figure 3(d)</xref>. Increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e140" xlink:type="simple"/></inline-formula> worsens performance during the first session by increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e141" xlink:type="simple"/></inline-formula> but improves average performance during the second session by decreasing both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e142" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e143" xlink:type="simple"/></inline-formula>, suggesting a tradeoff between the accuracy and generalizability of network solutions. The fan network, which produces a very accurate, specific representation of the original points, shows a much higher average error when it tries to generalize this representation. In contrast, the coarser representation produced by the stacked network is better able to incorporate new information.</p>
        </sec>
      </sec>
      <sec id="s3b">
        <title>Adaptation to Variable Learning Conditions</title>
        <p>Both natural and artificial systems can be found in a variety of states when presented with new information. The success in learning this information may depend both on the initial state of the system and on the learning conditions. We explore these possible dependencies by varying both the randomly initialized network state and the training conditions.</p>
        <sec id="s3b1">
          <title>Variable initialized states</title>
          <p>Because the conjugate gradient descent algorithm (see Methods) is deterministic, the randomly initialized state determines <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e144" xlink:type="simple"/></inline-formula>, which then influences subsequent solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e145" xlink:type="simple"/></inline-formula>.</p>
          <p>To study the influence of random initialization on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e146" xlink:type="simple"/></inline-formula>, we train all five networks on the original points with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e147" xlink:type="simple"/></inline-formula> sets of randomly chosen weights, allowing “unlimited” training time. Each network produces a set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e148" xlink:type="simple"/></inline-formula> functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e149" xlink:type="simple"/></inline-formula> with error values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e150" xlink:type="simple"/></inline-formula>.</p>
          <p>The CDF of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e151" xlink:type="simple"/></inline-formula>, shown in <xref ref-type="fig" rid="pcbi-1002063-g004">Figure 4(a)</xref>, reveals that the fan network consistently finds zero error solutions, while all other networks find solutions with a wide range of error values. The networks can collectively produce both zero error and high error solutions and do so with probabilities that respectively decrease and increase as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e152" xlink:type="simple"/></inline-formula> increases. The discontinuities in the stacked error distribution may indicate that the error landscape is composed of localized sets of minima with distinct depths. In comparison, the intermediate distributions show greater continuity in error, suggesting the presence of a larger number of connected minima with variable depths.</p>
          <fig id="pcbi-1002063-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002063.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Network performance under variable learning conditions.</title>
              <p>CDFs of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e153" xlink:type="simple"/></inline-formula> are shown given (a) unlimited and (b) limited training time for the five networks shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>. (a) The fan network consistently finds zero error solutions, while all other networks find solutions with a range of error values. (b) Intermediate networks find lower error solutions than do the fan and stacked networks (upper inset). Increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e154" xlink:type="simple"/></inline-formula> significantly decreases the both the maximum error and the frequency of high error solutions (lower inset). In both (a) and (b), increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e155" xlink:type="simple"/></inline-formula> increases <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e156" xlink:type="simple"/></inline-formula> (filled circles).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.g004" xlink:type="simple"/>
          </fig>
          <p>The distributions are more heavily weighted toward high error as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e157" xlink:type="simple"/></inline-formula> increases, thereby increasing the average error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e158" xlink:type="simple"/></inline-formula>. For a given architecture, the average number of training iterations decreases with increasing solution error, indicating an inherent tradeoff between speed and accuracy. While able to produce solutions with the same degree of accuracy as the fan network, the intermediate and stacked networks can also quickly produce coarse solutions. However, the intermediate networks require fewer iterations than the stacked network to reach solutions of similar error, suggesting that the presence of additional connections may facilitate faster performance.</p>
          <p>If we inspect the solutions produced by each network, we find that low, medium, and high error solutions correspond respectively to fitting all, some, or none of the points with a high order polynomial and fitting the remaining points with a horizontal line. To emphasize differences in network performance, the solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e159" xlink:type="simple"/></inline-formula> used to generate the results shown in <xref ref-type="fig" rid="pcbi-1002063-g002">Figures 2</xref> and <xref ref-type="fig" rid="pcbi-1002063-g003">3</xref> were chosen because their error was representative of the distribution averages shown in <xref ref-type="fig" rid="pcbi-1002063-g004">Figure 4(a)</xref>.</p>
        </sec>
        <sec id="s3b2">
          <title>Temporal constraints</title>
          <p>In natural systems, the time allowed to gather information from the environment is often limited, and a highly specific representation of information may not be desirable or even attainable. To investigate the effect of temporal constraints, we train the five networks on the original points with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e160" xlink:type="simple"/></inline-formula> sets of randomly chosen weights, now terminating training after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e161" xlink:type="simple"/></inline-formula> iterations. The increased number of randomly initialized states allows us to better resolve the edges of the error distributions shown in <xref ref-type="fig" rid="pcbi-1002063-g004">Figure 4(b)</xref>.</p>
          <p>Once training time is limited, all distributions shift toward higher error values, again revealing a tradeoff between speed and accuracy. As before, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e162" xlink:type="simple"/></inline-formula> increases as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e163" xlink:type="simple"/></inline-formula> increases. Discontinuities in the distributions are also removed, indicating that the networks do not have sufficient time to consistently find distinct sets of minima.</p>
          <p>The dynamic range of performance decreases as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e164" xlink:type="simple"/></inline-formula> increases, resulting in significant differences between the edges of each distribution. At the rightmost edge, both the frequency of high error solutions and the maximum error value increase as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e165" xlink:type="simple"/></inline-formula> increases. The stacked network shows an abrupt cutoff near the minimum error achieved by fitting the original points with a horizontal line. All other distributions extend beyond this value. In contrast to the case of unlimited training time, the fan network shows the least consistency in performance and produces several catastrophic errors, thereby revealing the greatest sensitivity to changes in training time. At the leftmost edge of the distributions, the intermediate networks find lower minimum error values than do the fan and stacked networks. This is similar to the behavior observed with unlimited training time, where the intermediate networks found comparable solutions to the fan and stacked extremes in fewer iterations. It may therefore be interesting in the future to verify the dependence of performance on the number of network connections.</p>
        </sec>
      </sec>
      <sec id="s3c">
        <title>Dependence on Error Landscape Structure</title>
        <p>Given unlimited training time, the distributions in <xref ref-type="fig" rid="pcbi-1002063-g004">Figure 4(a)</xref> mark the error of local minima found within the error landscape of each network. Each minimum can be characterized by the degree of local landscape curvature, where directions of high curvature specify combinations of weight adjustments that produce large changes in error. We adopt the terminology used in previous studies and refer to directions with high and low curvature as stiff and sloppy, respectively <xref ref-type="bibr" rid="pcbi.1002063-Brown1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Brown2">[32]</xref>. Stiff and sloppy directions are found by diagonalizing the error Hessian <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e166" xlink:type="simple"/></inline-formula> evaluated at the set of weights that produces the local error minimum. For computational efficiency, we use the approximate Levenberg-Marquardt (LM) Hessian <xref ref-type="bibr" rid="pcbi.1002063-Fletcher2">[33]</xref>, defined as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e167" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e168" xlink:type="simple"/></inline-formula> is the residual of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e169" xlink:type="simple"/></inline-formula> original point.</p>
        <p>The LM Hessian is a good approximation to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e170" xlink:type="simple"/></inline-formula> when the error of local minima, and thus the residual <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e171" xlink:type="simple"/></inline-formula>, is small and the additional Hessian term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e172" xlink:type="simple"/></inline-formula> can be neglected. For a given model and data set, the LM Hessian agrees well with the stiffest eigenvectors of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e173" xlink:type="simple"/></inline-formula> and is equivalent to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e174" xlink:type="simple"/></inline-formula> when the model perfectly fits the data. In addition, it has a known number of exactly zero eigenvalues equal to the difference in the number of model parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e175" xlink:type="simple"/></inline-formula> and the number of data points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e176" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002063-Brown1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Brown2">[32]</xref>.</p>
        <p>We diagonalize the LM Hessian about each of the 500 minima with the error values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e177" xlink:type="simple"/></inline-formula> shown in <xref ref-type="fig" rid="pcbi-1002063-g004">Figure 4(a)</xref>. Each error minimum produces a set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e178" xlink:type="simple"/></inline-formula> eigenvalues <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e179" xlink:type="simple"/></inline-formula> and normalized eigenvectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e180" xlink:type="simple"/></inline-formula>, which give the degrees and directions of stiffness in weight space.</p>
        <p>As an illustrative example of landscape features observed along these relevant directions, <xref ref-type="fig" rid="pcbi-1002063-g005">Figures 5(a) and 5(b)</xref> show the projection of the error landscape onto the two stiffest eigenvector directions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e181" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e182" xlink:type="simple"/></inline-formula> centered on zero error minima produced by the fan and stacked networks, respectively.</p>
        <fig id="pcbi-1002063-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002063.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Network error landscapes.</title>
            <p>Error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e183" xlink:type="simple"/></inline-formula> is projected onto the two stiffest eigenvector directions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e184" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e185" xlink:type="simple"/></inline-formula> about minima produced by the (a) fan and (b) stacked network given unlimited training time. The two minima were chosen for comparison because they have the same number and similar magnitude of nonzero eigenvalues, although similar behavior was observed for alternative minima. The insets show zoomed in views of the contour plots about their central minima. (a) The projection of the fan landscape shows a single deep minimum surrounded by smooth peaks. (b) In contrast, the projection of the stacked landscape shows a long, deep valley of several local putative minima separated by low barriers. The surrounding landscape is much bumpier than that of the fan network.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.g005" xlink:type="simple"/>
        </fig>
        <p>The fan landscape shows a single deep basin surrounded by smoothly varying peaks. In contrast, the stacked landscape is rugged, showing a deep valley with several minima separated by small barriers. While these minima appear to be distinct, they may be connected by higher dimensional pathways that cannot be seen in this reduced space.</p>
        <sec id="s3c1">
          <title>Participation of network connections</title>
          <p>The ability of a network to move along relevant eigenvector directions may depend on the number of weights that must be significantly adjusted, or equivalently the localization of eigenvector components. To quantify the degree of localization of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e186" xlink:type="simple"/></inline-formula> eigenvector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e187" xlink:type="simple"/></inline-formula>, we calculate its participation ratio <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e188" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002063-Mello1">[34]</xref>, where individual eigenvector components <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e189" xlink:type="simple"/></inline-formula> correspond to specific weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e190" xlink:type="simple"/></inline-formula> in the network. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e191" xlink:type="simple"/></inline-formula> is a dimensionless quantity that ranges between a completely delocalized minimum of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e192" xlink:type="simple"/></inline-formula>, for which all components have equal weight <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e193" xlink:type="simple"/></inline-formula>, and a completely localized maximum of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e194" xlink:type="simple"/></inline-formula>, for which a single component carries unit weight.</p>
          <p>For the set of minima with error values <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e195" xlink:type="simple"/></inline-formula>, we quantify <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e196" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e197" xlink:type="simple"/></inline-formula> of the stiffest eigenvectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e198" xlink:type="simple"/></inline-formula>, as combinations of weight changes specified by these eigenvector directions produce the largest changes in error. The covariances <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e199" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e200" xlink:type="simple"/></inline-formula> in these quantities are shown by the ellipses centered about their average values in <xref ref-type="fig" rid="pcbi-1002063-g006">Figures 6(a) and 6(b)</xref>, respectively.</p>
          <fig id="pcbi-1002063-g006" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002063.g006</object-id>
            <label>Figure 6</label>
            <caption>
              <title>Properties of network error landscapes.</title>
              <p>Covariances between (a) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e201" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e202" xlink:type="simple"/></inline-formula> and between (b) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e203" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e204" xlink:type="simple"/></inline-formula> are shown for error landscape minima produced by the five networks shown in <xref ref-type="fig" rid="pcbi-1002063-g001">Figure 1(a)</xref>. For each network, the values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e205" xlink:type="simple"/></inline-formula> are taken from the distributions shown in <xref ref-type="fig" rid="pcbi-1002063-g004">Figure 4(a)</xref>. Covariances, indicated by ellipses, are centered about their average values, indicated by markers. The semimajor axis of each ellipse marks the direction of maximum covariance. Increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e206" xlink:type="simple"/></inline-formula> increases both the average and variance in all three quantities. For a given network, larger values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e207" xlink:type="simple"/></inline-formula> generally correspond to smaller values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e208" xlink:type="simple"/></inline-formula> and larger values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e209" xlink:type="simple"/></inline-formula>.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.g006" xlink:type="simple"/>
          </fig>
          <p><xref ref-type="fig" rid="pcbi-1002063-g006">Figure 6</xref> highlights the variability in basin structure within and between the networks. As <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e210" xlink:type="simple"/></inline-formula> increases, both the average and variance in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e211" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e212" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e213" xlink:type="simple"/></inline-formula> increase. Higher variance leads to lower confidence in predicting the success of the network, but it also suggests that the network has more options when exploring its error landscape.</p>
          <p>The orientations of the covariance ellipses in <xref ref-type="fig" rid="pcbi-1002063-g006">Figures 6(a) and 6(b)</xref> provide information regarding the relationships between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e214" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e215" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e216" xlink:type="simple"/></inline-formula>. The semi-major axis of each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e217" xlink:type="simple"/></inline-formula> ellipse in <xref ref-type="fig" rid="pcbi-1002063-g006">Figure 6(a)</xref> lies along the trend swept out by the average values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e218" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e219" xlink:type="simple"/></inline-formula>, suggesting a general, positive correlation between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e220" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e221" xlink:type="simple"/></inline-formula>. While the average values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e222" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e223" xlink:type="simple"/></inline-formula> would suggest that these quantities are also positively correlated, <xref ref-type="fig" rid="pcbi-1002063-g006">Figure 6(b)</xref> shows that for a given value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e224" xlink:type="simple"/></inline-formula>, larger values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e225" xlink:type="simple"/></inline-formula> correspond to smaller values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e226" xlink:type="simple"/></inline-formula>. These results reveal general characteristics of error landscape structure; higher error minima (larger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e227" xlink:type="simple"/></inline-formula>) tend to be shallower (smaller <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e228" xlink:type="simple"/></inline-formula>) and require the adjustment of fewer weights (larger <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e229" xlink:type="simple"/></inline-formula>).</p>
        </sec>
        <sec id="s3c2">
          <title>Landscape characteristics and successful learning</title>
          <p>Variations in landscape structure provide insight into the way in which each network searches for solutions. In particular, fan solutions are characterized by low error and participation ratio, indicating that the fan network must adjust nearly all of its weights in order to navigate zero error basins. In contrast, stacked solutions span a range of error values. The corresponding basins are characterized by a variety of eigenvalues and participation ratios, indicating that the stacked network can navigate many types of basins by adjusting variable numbers of weights. Larger participation ratios correspond to higher error and lower eigenvalues, suggesting that the stacked network can navigate shallow, high error basins by adjusting only a few of its connections. Narrow, low error basins, found by both the fan and stacked networks, require fine tuning of a larger number of connections.</p>
          <p>In combination, landscape characteristics help explain the results shown in <xref ref-type="fig" rid="pcbi-1002063-g003">Figures 3</xref> and <xref ref-type="fig" rid="pcbi-1002063-g004">4</xref>. Given unlimited training time, landscape variability is disadvantageous and can prevent a network from finding a low error minimum. Once time is limited, landscape variability can be advantageous in preventing failure by providing the network with high error, shallow basins that can be navigated with the adjustment of relatively few connections. If limited training time is coupled with extremely noisy information, landscapes with high error basins can be advantageous by decreasing average error relative to landscapes with no easily reachable basins. Because our sequential sessions combined both limited and unlimited training time and both clean and noisy data, we see an additional tradeoff between the two sessions. Unlimited training time and well constrained data favor the fan over the stacked network in minimizing average error, while limited time and noisy data favor the stacked network over the fan.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>In this study, we investigated the tradeoffs in learning and memory performance that arise from structural complexity. Importantly, none of the architectures considered here simultaneously mastered both learning and memory tasks, which suggests that systems whose function depends on such simultaneous success might require architectures that are complex combinations of both parallel and serial structures. Indeed, this inherent sensitivity of function to underlying architecture may help to explain the high degree of variability evident in architectural motifs of large-scale biological and technical systems. For instance, in natural neuronal networks, cortical connection patterns display a variety of architectural complexities at varying spatial scales. Examples of fan architectures are found in hub-and-spoke motifs, which form an important part of the small-world architecture <xref ref-type="bibr" rid="pcbi.1002063-Bettencourt1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Hagmann1">[37]</xref>, as well as in the decomposition of cortical network architectures into subnetworks or modules which may simultaneously process differential information <xref ref-type="bibr" rid="pcbi.1002063-Bassett1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Kim1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Bassett2">[41]</xref>. Moreover, stacked architectures are evident within cortical lamina <xref ref-type="bibr" rid="pcbi.1002063-Mountcastle1">[1]</xref>, within the hierarchical organization displayed in the sequential ordering of the visual system <xref ref-type="bibr" rid="pcbi.1002063-Felleman1">[42]</xref>, and within the nested modularity of large-scale cortical connectivity <xref ref-type="bibr" rid="pcbi.1002063-Bassett1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Bassett2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Meunier2">[43]</xref>. Similarly, artificial neural networks display complex combinations of fan and stacked motifs including modularity <xref ref-type="bibr" rid="pcbi.1002063-Fu1">[44]</xref>, hierarchy <xref ref-type="bibr" rid="pcbi.1002063-Ersoy1">[45]</xref>, and small-worldness <xref ref-type="bibr" rid="pcbi.1002063-Oshima1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Dominguez1">[47]</xref>.</p>
      <sec id="s4a">
        <title>Parallel versus Layered Architectures</title>
        <p>Given the wealth of structural motifs present in real world systems, it is of interest to first isolate the tradeoffs in performance associated with small parallel and layered network structures which together form the complex architectural landscape of larger systems and thereby constrain their overall performance. Here we found that the deep, narrow basins within the error landscape enabled the fan network to produce very accurate solutions. However, the difficulty of simultaneously adjusting many network connections in order to escape deep basins may have hindered the ability of the fan network to adapt, a result that helps explain the susceptibility of parallel networks to the problems of overfitting and failure to generalize <xref ref-type="bibr" rid="pcbi.1002063-Rojas1">[26]</xref>. In contrast, higher variability in the width and depth of local minima enabled the stacked network to quickly find coarse but generalizable solutions through the adjustment of a smaller fraction of weights. In combination, these results support the hypothesis that the number and width of local landscape minima may increase with increasing number of hidden layers <xref ref-type="bibr" rid="pcbi.1002063-Zhang1">[4]</xref>, and we suggest that this variability helps explain why layered networks may require fewer computational units and may better generalize than parallel networks <xref ref-type="bibr" rid="pcbi.1002063-Bengio1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Bengio2">[50]</xref>. However, the impact of structural variations on functional tradeoffs, for example between specificity and generalizability, extends beyond artificial network studies and is crucial for understanding the interaction of learning processes in large scale models of the brain <xref ref-type="bibr" rid="pcbi.1002063-Atallah1">[51]</xref>. While parallel architectures are often preferred in artificial network studies due to their consistency and accuracy <xref ref-type="bibr" rid="pcbi.1002063-Larochelle1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Bengio2">[50]</xref>, our results highlight the advantages of layered architectures when performance criteria favor generalizability and minimization of failure.</p>
      </sec>
      <sec id="s4b">
        <title>Intermediate Architectures</title>
        <p>Building on the intuition gained from the two benchmark extremes – fan and stacked – we further assessed the characteristics of intermediate networks, which can be used to more directly probe the expected behavior of structurally complex composite systems. In particular, our intermediate structures were composed of several adjacent stacked networks and therefore shared principal features of both parallel and layered systems. Additionally, these networks had slightly larger numbers of connections than the fan and stacked networks.</p>
        <p>Due to these structural differences, the depth of local minima within the intermediate landscapes displayed more variation than fan minima but more continuity than stacked minima. As landscape variability was linked to improved generalization capabilities, a continuous range of basin depths may have enabled the more successful balance between flexible learning and stable memory observed in the intermediate networks. This performance supports the hypothesis that short path lengths (similar to the serialization <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e230" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002063-Bullmore1">[52]</xref>) and low connection densities may facilitate simultaneous performance of information segregation (memory retention) and integration (generalization) within natural neuronal systems <xref ref-type="bibr" rid="pcbi.1002063-Tononi1">[53]</xref>. These competing processes are also maintained in natural neuronal systems and neural circuit models through homeostatic plasticity mechanisms such as synaptic scaling <xref ref-type="bibr" rid="pcbi.1002063-Turrigiano1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Turrigiano2">[55]</xref> and redistribution <xref ref-type="bibr" rid="pcbi.1002063-Markram1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Abbott1">[57]</xref>, in addition to the rehearsal methods employed here <xref ref-type="bibr" rid="pcbi.1002063-McCloskey1">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Robins2">[23]</xref>. Even in the absence of such homeostatic plasticity mechanisms, we found that the architectural combination of parallel and layered connectivity helped foster a balance between learning and memory.</p>
      </sec>
      <sec id="s4c">
        <title>Variable Learning Conditions and Network Efficiency</title>
        <p>We extended our analysis from the case of unlimited training time, which revealed information about error landscape structure, to the biologically-motivated case of limited training time. Comparison of these two cases revealed a tradeoff in performance between training speed and solution accuracy. In the absence of temporal constraints, the production of highly accurate representations required longer training times. Similarly, temporal constraints led to larger solution errors. This tradeoff between speed and accuracy has been observed in cortical networks, where emphasis on performance speed during perceptual learning tasks increased the baseline activity but decreased the transient task-related activity of neurons within the decision-making regions of the human brain <xref ref-type="bibr" rid="pcbi.1002063-Bogacz1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-vanVeen1">[59]</xref>. Here we found that network architecture played a significant role in the manifestation of this tradeoff, and the presence of additional hidden layers helped minimize network susceptibility to changes in training time. In particular, the fan network demonstrated the greatest change in performance under temporal constraints, showing a decrease in consistency coupled with occasional catastrophic error values. In contrast, the intermediate and stacked networks improved consistency and minimized inaccuracy once training time was limited.</p>
        <p>Upon closer inspection, we found that the intermediate networks produced solutions with increased speed given unlimited time and with increased potential for accuracy when time was limited as compared to the fan and stacked extremes. The presence of additional connections may have influenced the number of iterations required to find a solution, or similarly the minimum error found with a fixed number of iterations. While the graph measure of path length is known to influence network efficiency <xref ref-type="bibr" rid="pcbi.1002063-Bullmore1">[52]</xref>, these results imply that the number of networks connections may additionally enable the network to quickly find an accurate solution.</p>
        <p>In addition to static variations in connectivity, dynamic structural changes such as synapse formation <xref ref-type="bibr" rid="pcbi.1002063-Xu1">[60]</xref> can facilitate learning and memory processes. The converse case of network degradation, or disruptions to structural connectivity, is also known to have widespread consequences in functional properties of the brain <xref ref-type="bibr" rid="pcbi.1002063-Alstott1">[61]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Allred1">[63]</xref>. A more detailed study of the relationfship between connection number and robustness could provide additional insight into the effects of synapse formation and degradation on functional performance. Our analysis of error landscape features revealed that different architectures showed variable localization properties in the eigenvectors associated with local error minima, and we therefore expect robustness to depend on both the architecture and the location of growth or damage within the network.</p>
      </sec>
      <sec id="s4d">
        <title>Methodological Considerations</title>
        <p>We found that parallel networks suffered from the creation of excessively detailed representations of information, an “overfitting” problem that is often addressed through the use of cross-validation <xref ref-type="bibr" rid="pcbi.1002063-Cucker1">[64]</xref> and weight regularization <xref ref-type="bibr" rid="pcbi.1002063-Bousquet1">[65]</xref> techniques. As one goal of this study was to uncover the structural basis for differences in representational capabilities, it was crucial to understand network behavior in the absence of task-specific cross-validation schemes. Additionally, as the number of parameters was roughly constant across all network structures (and identical for the fan and stacked networks), we were able to draw comparisons across network architectures in the absence of additional weight regularization constraints.</p>
        <p>While parallel network models have commonly been used in machine learning studies, multi-layer “deep” networks have recently gained interest due to their potential ability to compactly represent (using fewer computational units and parameters) highly variable functions <xref ref-type="bibr" rid="pcbi.1002063-Bengio1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002063-Bengio2">[50]</xref>. The “deep belief” framework has been successful for training large, multi-layered networks, and training methods often couple unsupervised, layer-wise (greedy) training with supervised fine-tuning <xref ref-type="bibr" rid="pcbi.1002063-Hinton1">[66]</xref>. Recent studies of deep belief networks found that classification performance improved with the addition of layers <xref ref-type="bibr" rid="pcbi.1002063-Larochelle1">[48]</xref>. In addition, it was suggested that a reduction in the number of hidden layers would require an exponential increase in the number of hidden units in order to achieve similar network performance <xref ref-type="bibr" rid="pcbi.1002063-Bengio2">[50]</xref>. These results emphasize the capabilities of layered networks and provide an additional framework in which to explore structure-function tradeoffs.</p>
        <p>Although biologically-motivated, the FFBP framework includes several simplifying assumptions that could be modified to include additional, realistic complexity. First, we assumed that only the connection weights, analogous to synaptic strengths, were variable. Real neurons also exhibit changes in intrinsic dynamics <xref ref-type="bibr" rid="pcbi.1002063-Marder1">[67]</xref> that interact with network architecture to constrain functionality in the brain <xref ref-type="bibr" rid="pcbi.1002063-Gaiteri1">[68]</xref>. Accounting for such relationships could be particularly relevant, for example, in the study of neuron response profiles within different cortical layers <xref ref-type="bibr" rid="pcbi.1002063-Ress1">[13]</xref>. Second, we assumed that signals passed between nodes had no temporal structure, analogous to representing steady state neuron firing rates. Temporally varying signals could be included to study the dependence of dynamic properties, such as synchronization <xref ref-type="bibr" rid="pcbi.1002063-Gaiteri1">[68]</xref>–<xref ref-type="bibr" rid="pcbi.1002063-Roelfsema1">[70]</xref> and signal propagation <xref ref-type="bibr" rid="pcbi.1002063-Vogels1">[71]</xref>, on structural organization <xref ref-type="bibr" rid="pcbi.1002063-Rubinov1">[72]</xref>. Lastly, we assumed feedforward connectivity. The addition of recurrent connections could be used to study the relationship between recurrent structure and oscillatory functions such as cortical sleep rhythms <xref ref-type="bibr" rid="pcbi.1002063-SanchezVives1">[73]</xref> and oscillation couplings relevant for associative learning and memory <xref ref-type="bibr" rid="pcbi.1002063-Tort1">[74]</xref>. In each of these directions, we anticipate that underlying structural complexity will continue to impact performance through functional tradeoffs.</p>
      </sec>
      <sec id="s4e">
        <title>Conclusion</title>
        <p>In summary, different network architectures produce error landscapes with distinguishable characteristics, such as the height and width of local minima, which in turn determine performance features such as speed, accuracy, and adaptability. Inherent tradeoffs, observed across a range of architectures, arise as a consequence of the underlying error landscape structure. The presence of local landscape minima enable greater speed, more generalizable solutions, and minimization of catastrophic failure. However, these successes come at the cost of decreased accuracy. Understanding how both the landscape characteristics and the resulting performance features vary across a range of architectures is crucial for both understanding and guiding the design of more complex biological and technical systems.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002063.s001" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p><bold>Network solutions and error distributions produced by larger networks.</bold> Panels (a) and (b) show solutions produced respectively by larger versions of the fan (1×18) and stacked (9×2) networks, indicating for each network the approximation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e231" xlink:type="simple"/></inline-formula> (solid curve) of the original points (point markers) and a subset of approximations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e232" xlink:type="simple"/></inline-formula> (dashed curves) of the new and buffer points. Panels (c) and (d) respectively show the CDFs of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e233" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e234" xlink:type="simple"/></inline-formula>. All results are qualitatively similar to those obtained using smaller networks (<xref ref-type="fig" rid="pcbi-1002063-g002">Figure 2</xref>).</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002063.s002" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p><bold>Tradeoffs in network learning and memory observed in larger networks.</bold> Best, worst, and average network performance is measured with respect to solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e235" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e236" xlink:type="simple"/></inline-formula> produced by networks of size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e237" xlink:type="simple"/></inline-formula>×<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e238" xlink:type="simple"/></inline-formula> = 1×18, 2×9, 3×6, 6×3, 9×2. Panels (a) and (b) show the maximum values in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e239" xlink:type="simple"/></inline-formula> versus (a) the minimum values in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e240" xlink:type="simple"/></inline-formula> and (b) the maximum values in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e241" xlink:type="simple"/></inline-formula>. Panels (c) and (d) show the the average errors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e242" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e243" xlink:type="simple"/></inline-formula> versus (c) the average solution variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e244" xlink:type="simple"/></inline-formula> and (d) the original error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e245" xlink:type="simple"/></inline-formula>. All results are qualitatively similar to those obtained using smaller networks (<xref ref-type="fig" rid="pcbi-1002063-g003">Figure 3</xref>).</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002063.s003" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p><bold>Network solutions and error distributions produced using a permuted training function.</bold> During the first training session, all networks were trained using the same random permutation of the original point values quoted in the main text. Panels (a) and (b) show solutions produced respectively by the fan and stacked networks, indicating for each network the approximation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e246" xlink:type="simple"/></inline-formula> (solid curve) of the permuted set of original points (point markers) and a subset of approximations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e247" xlink:type="simple"/></inline-formula> (dashed curves) of the new and buffer points. Panels (c) and (d) respectively show the CDFs of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e248" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e249" xlink:type="simple"/></inline-formula>. All results show the same qualitative features as those produced using the unpermuted set of original points (<xref ref-type="fig" rid="pcbi-1002063-g002">Figure 2</xref>).</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002063.s004" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002063.s004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p><bold>Tradeoffs in network learning and memory observed with a permuted training function.</bold> Best, worst, and average network performance is measured with respect to solutions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e250" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e251" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e252" xlink:type="simple"/></inline-formula> was generated using a random permutation of the original point values quoted in the main text. Panels (a) and (b) show the maximum values in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e253" xlink:type="simple"/></inline-formula> versus (a) the minimum values in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e254" xlink:type="simple"/></inline-formula> and (b) the maximum values in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e255" xlink:type="simple"/></inline-formula>. Panels (c) and (d) show the the average errors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e256" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e257" xlink:type="simple"/></inline-formula> versus (c) the average solution variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e258" xlink:type="simple"/></inline-formula> and (d) the original error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002063.e259" xlink:type="simple"/></inline-formula>. All results are qualitatively similar to those obtained using the unpermuted set of original points (<xref ref-type="fig" rid="pcbi-1002063-g003">Figure 3</xref>).</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002063-Mountcastle1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mountcastle</surname><given-names>VB</given-names></name></person-group>             <year>1997</year>             <article-title>The columnar organization of the neocortex.</article-title>             <source>Brain</source>             <volume>120</volume>             <fpage>701</fpage>             <lpage>722</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Jain1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jain</surname><given-names>AK</given-names></name><name name-style="western"><surname>Murty</surname><given-names>MN</given-names></name><name name-style="western"><surname>Flynn</surname><given-names>PJ</given-names></name></person-group>             <year>1999</year>             <article-title>Data clustering: a review.</article-title>             <source>ACM Comput Surv</source>             <volume>31</volume>             <fpage>264</fpage>             <lpage>323</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-EgmontPetersen1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Egmont-Petersen</surname><given-names>M</given-names></name><name name-style="western"><surname>de Ridder</surname><given-names>D</given-names></name><name name-style="western"><surname>Handels</surname><given-names>H</given-names></name></person-group>             <year>2002</year>             <article-title>Image processing with neural networks–a review.</article-title>             <source>Pattern Recognit</source>             <volume>35</volume>             <fpage>2279</fpage>             <lpage>2301</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Zhang1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>G</given-names></name><name name-style="western"><surname>Patuwo</surname><given-names>BE</given-names></name><name name-style="western"><surname>Hu</surname><given-names>MY</given-names></name></person-group>             <year>1998</year>             <article-title>Forecasting with artificial neural networks: the state of the art.</article-title>             <source>Int J Forecast</source>             <volume>14</volume>             <fpage>35</fpage>             <lpage>62</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Chittka1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chittka</surname><given-names>L</given-names></name><name name-style="western"><surname>Niven</surname><given-names>JJ</given-names></name></person-group>             <year>2009</year>             <article-title>Are bigger brains better?</article-title>             <source>Current Biology</source>             <volume>19</volume>             <fpage>R99535</fpage>             <lpage>R1008</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Honey1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Honey</surname><given-names>CJ</given-names></name></person-group>             <year>2009</year>             <article-title>Predicting human resting-state functional connectivity from structual connectivity.</article-title>             <source>Proc of the Natl Acad of Sci</source>             <volume>106</volume>             <fpage>2035</fpage>             <lpage>2040</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Kenet1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name><name name-style="western"><surname>Bibitchkov</surname><given-names>D</given-names></name><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name><name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name><name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Spontaneously emerging cortical representations of visual attributes.</article-title>             <source>Nature</source>             <volume>425</volume>             <fpage>954</fpage>             <lpage>956</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-McIntosh1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McIntosh</surname><given-names>AR</given-names></name><name name-style="western"><surname>Rajah</surname><given-names>MN</given-names></name><name name-style="western"><surname>Lobaugh</surname><given-names>NJ</given-names></name></person-group>             <year>2003</year>             <article-title>Functional connectivity of the medial temporal lobe relates to learning and awareness.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>6520</fpage>             <lpage>6528</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Scholz1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Scholz</surname><given-names>J</given-names></name><name name-style="western"><surname>Klein</surname><given-names>MC</given-names></name><name name-style="western"><surname>Behrens</surname><given-names>TEJ</given-names></name><name name-style="western"><surname>Johansen-Berg</surname><given-names>H</given-names></name></person-group>             <year>2009</year>             <article-title>Training induces changes in whitematter architecture.</article-title>             <source>Nat Neurosci</source>             <volume>12</volume>             <fpage>1370</fpage>             <lpage>1371</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bassett1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bassett</surname><given-names>DS</given-names></name><name name-style="western"><surname>Greenfield</surname><given-names>DL</given-names></name><name name-style="western"><surname>Meyer-Lindenberg</surname><given-names>A</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>DR</given-names></name><name name-style="western"><surname>Moore</surname><given-names>SW</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Efficient physical embedding of topologically complex information processing networks in brains and computer circuits.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <fpage>e1000748</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Kaiser1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kaiser</surname><given-names>M</given-names></name><name name-style="western"><surname>Hilgetag</surname><given-names>CC</given-names></name></person-group>             <year>2010</year>             <article-title>Optimal hierarchical modular topologies for producing limited sustained activation of neural networks.</article-title>             <source>Front Neuroinformatics</source>             <volume>4</volume>             <fpage>1</fpage>             <lpage>14</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Reid1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reid</surname><given-names>AT</given-names></name><name name-style="western"><surname>Krumnack</surname><given-names>A</given-names></name><name name-style="western"><surname>Wanke</surname><given-names>E</given-names></name><name name-style="western"><surname>Kotter</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Optimization of cortical hierarchies with continuous scales and ranges.</article-title>             <source>Neuro Image</source>             <volume>47</volume>             <fpage>611</fpage>             <lpage>617</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Ress1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ress</surname><given-names>D</given-names></name><name name-style="western"><surname>Glover</surname><given-names>GH</given-names></name><name name-style="western"><surname>Liu</surname><given-names>J</given-names></name><name name-style="western"><surname>Wandell</surname><given-names>B</given-names></name></person-group>             <year>2007</year>             <article-title>Laminar profiles of functional activity in the human brain.</article-title>             <source>Neuroimage</source>             <volume>34</volume>             <fpage>74</fpage>             <lpage>84</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Atencio1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Atencio</surname><given-names>CA</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name></person-group>             <year>2007</year>             <article-title>Columnar connectivity and laminar processing in cat primary auditory cortex.</article-title>             <source>PLoS ONE</source>             <volume>5</volume>             <fpage>e9521</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bakoglu1">
        <label>15</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bakoglu</surname><given-names>HB</given-names></name></person-group>             <year>1990</year>             <source>Circuits, Interconnections, and Packaging for VLSI</source>             <publisher-loc>Boston</publisher-loc>             <publisher-name>Addison Wesley</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">527</size>           </element-citation>
      </ref>
      <ref id="pcbi.1002063-Galushkin1">
        <label>16</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Galushkin</surname><given-names>AI</given-names></name></person-group>             <year>2007</year>             <source>Neural Networks Theory</source>             <publisher-loc>Secaucus, NJ</publisher-loc>             <publisher-name>Springer-Verlag New York</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">396</size>           </element-citation>
      </ref>
      <ref id="pcbi.1002063-Fukushima1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fukushima</surname><given-names>K</given-names></name></person-group>             <year>1988</year>             <article-title>Neocognitron: a hierarchical neural network capable of visual pattern recognition.</article-title>             <source>Neural Networks</source>             <volume>1</volume>             <fpage>119</fpage>             <lpage>130</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Robinson1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robinson</surname><given-names>AJ</given-names></name></person-group>             <year>1994</year>             <article-title>An application of recurrent nets to phone probability estimation.</article-title>             <source>IEEE Trans Neural Netw</source>             <volume>5</volume>             <fpage>298</fpage>             <lpage>305</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-McCloskey1">
        <label>19</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McCloskey</surname><given-names>M</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>NJ</given-names></name></person-group>             <year>1989</year>             <article-title>Catastrophic interference in connectionist networks: The sequential learning problem.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Bower</surname><given-names>GH</given-names></name></person-group>             <source>The Psychology of Learning and Motivation</source>             <publisher-name>Academic Press, volume 24</publisher-name>             <fpage>109</fpage>             <lpage>159</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Ratcliff1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ratcliff</surname><given-names>R</given-names></name></person-group>             <year>1990</year>             <article-title>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.</article-title>             <source>Psychol Rev</source>             <volume>97</volume>             <fpage>285</fpage>             <lpage>308</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Sharkey1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sharkey</surname><given-names>NE</given-names></name><name name-style="western"><surname>Sharkey</surname><given-names>AJC</given-names></name></person-group>             <year>1995</year>             <article-title>An analysis of catastrophic interference.</article-title>             <source>Conn Sci</source>             <volume>7</volume>             <fpage>301</fpage>             <lpage>329</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Robins1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robins</surname><given-names>A</given-names></name></person-group>             <year>1995</year>             <article-title>Catastrophic forgetting, rehearsal, and pseudorehearsal.</article-title>             <source>Connection Science</source>             <volume>7</volume>             <fpage>123</fpage>             <lpage>146</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Robins2">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Robins</surname><given-names>A</given-names></name><name name-style="western"><surname>McCallum</surname><given-names>S</given-names></name></person-group>             <year>1998</year>             <article-title>Catastrophic forgetting and the pseudorehearsal solution in hopfieldtype networks.</article-title>             <source>Conn Sci</source>             <volume>10</volume>             <fpage>121</fpage>             <lpage>135</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Auer1">
        <label>24</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Auer</surname><given-names>P</given-names></name><name name-style="western"><surname>Herbster</surname><given-names>M</given-names></name><name name-style="western"><surname>Warmuth</surname><given-names>MK</given-names></name></person-group>             <year>1996</year>             <article-title>Exponentially many local minima for single neurons.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Mozer</surname><given-names>M</given-names></name><name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name><name name-style="western"><surname>Perrone</surname><given-names>M</given-names></name></person-group>             <source>Advances in Neural Information Processing Systems</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press, volume 8</publisher-name>             <fpage>315</fpage>             <lpage>322</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Fukumizu1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fukumizu</surname><given-names>K</given-names></name><name name-style="western"><surname>Amari</surname><given-names>S</given-names></name></person-group>             <year>2000</year>             <article-title>Local minima and plateaus in hierarchical structures of multilayer perceptrons.</article-title>             <source>Neural Networks</source>             <volume>13</volume>             <fpage>317</fpage>             <lpage>327</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Rojas1">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rojas</surname><given-names>R</given-names></name></person-group>             <year>1996</year>             <source>Neural Networks: A Systematic Introduction</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer-Verlag</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">502</size>           </element-citation>
      </ref>
      <ref id="pcbi.1002063-Cohen1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cohen</surname><given-names>IL</given-names></name></person-group>             <year>1994</year>             <article-title>An artificial neural network analogue of learning in autism.</article-title>             <source>Biol Psychiatry</source>             <volume>36</volume>             <fpage>5</fpage>             <lpage>20</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Fletcher1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fletcher</surname><given-names>R</given-names></name><name name-style="western"><surname>Reeves</surname><given-names>CM</given-names></name></person-group>             <year>1964</year>             <article-title>Function minimization by conjugate gradients.</article-title>             <source>Comput J</source>             <volume>7</volume>             <fpage>149</fpage>             <lpage>154</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Polak1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Polak</surname><given-names>E</given-names></name><name name-style="western"><surname>Ribiere</surname><given-names>G</given-names></name></person-group>             <year>1969</year>             <article-title>Note sur la convergence de methodes de directions conjugees.</article-title>             <source>Rev Franc Inform Rech Oper</source>             <volume>16</volume>             <fpage>35</fpage>             <lpage>43</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Powell1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Powell</surname><given-names>MJD</given-names></name></person-group>             <year>1986</year>             <article-title>Convergence properties of algorithms for nonlinear optimization.</article-title>             <source>SIAM Rev</source>             <volume>28</volume>             <fpage>487</fpage>             <lpage>500</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Brown1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>KS</given-names></name><name name-style="western"><surname>Sethna</surname><given-names>JP</given-names></name></person-group>             <year>2003</year>             <article-title>Statistical mechanical approaches to models with many poorly known parameters.</article-title>             <source>Phys Rev E</source>             <volume>68</volume>             <fpage>021904</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Brown2">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>KS</given-names></name><name name-style="western"><surname>Hill</surname><given-names>CC</given-names></name><name name-style="western"><surname>Calero</surname><given-names>GA</given-names></name><name name-style="western"><surname>Myers</surname><given-names>CR</given-names></name><name name-style="western"><surname>Lee</surname><given-names>KH</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>The statistical mechanics of complex signaling networks: nerve growth factor signaling.</article-title>             <source>Phys Biol</source>             <volume>1</volume>             <fpage>184</fpage>             <lpage>195</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Fletcher2">
        <label>33</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fletcher</surname><given-names>R</given-names></name></person-group>             <year>1987</year>             <source>Practical Methods of Optimization</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley-Interscience, 2 edition</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">436</size>           </element-citation>
      </ref>
      <ref id="pcbi.1002063-Mello1">
        <label>34</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mello</surname><given-names>PA</given-names></name><name name-style="western"><surname>Kuma</surname><given-names>N</given-names></name></person-group>             <year>2004</year>             <source>Quantum transport in mesoscopic systems: complexity and statistical fluctuations</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Oxford University Press</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">416</size>           </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bettencourt1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bettencourt</surname><given-names>LM</given-names></name><name name-style="western"><surname>Stephens</surname><given-names>GJ</given-names></name><name name-style="western"><surname>Ham</surname><given-names>MI</given-names></name><name name-style="western"><surname>Gross</surname><given-names>GW</given-names></name></person-group>             <year>2007</year>             <article-title>Functional structure of cortical neuronal networks grown in vitro.</article-title>             <source>Phys Rev E</source>             <volume>75</volume>             <fpage>021915</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Achard1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Achard</surname><given-names>S</given-names></name><name name-style="western"><surname>Salvador</surname><given-names>R</given-names></name><name name-style="western"><surname>Whitcher</surname><given-names>B</given-names></name><name name-style="western"><surname>Suckling</surname><given-names>J</given-names></name><name name-style="western"><surname>Bullmore</surname><given-names>E</given-names></name></person-group>             <year>2006</year>             <article-title>A resilient, low-frequency, smallworld human brain functional network with highly connected association cortical hubs.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>63</fpage>             <lpage>72</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Hagmann1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hagmann</surname><given-names>P</given-names></name><name name-style="western"><surname>Cammoun</surname><given-names>L</given-names></name><name name-style="western"><surname>Gigandet</surname><given-names>X</given-names></name><name name-style="western"><surname>Meuli</surname><given-names>R</given-names></name><name name-style="western"><surname>Honey</surname><given-names>CJ</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Mapping the structural core of human cerebral cortex.</article-title>             <source>PLoS Biol</source>             <volume>6</volume>             <fpage>e159</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Kim1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>H</given-names></name></person-group>             <year>2010</year>             <article-title>Dissociating the roles of the default-mode, dorsal, and ventral networks in episodic memory retrieval.</article-title>             <source>Neuroimage</source>             <volume>50</volume>             <fpage>1648</fpage>             <lpage>1657</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Chen1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>ZJ</given-names></name><name name-style="western"><surname>He</surname><given-names>Y</given-names></name><name name-style="western"><surname>Rosa-Neto</surname><given-names>P</given-names></name><name name-style="western"><surname>Germann</surname><given-names>J</given-names></name><name name-style="western"><surname>Evans</surname><given-names>AC</given-names></name></person-group>             <year>2008</year>             <article-title>Revealing modular architecture of human brain structural networks by using cortical thickness from MRI.</article-title>             <source>Cereb Cortex</source>             <volume>18</volume>             <fpage>2374</fpage>             <lpage>2381</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Meunier1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Meunier</surname><given-names>D</given-names></name><name name-style="western"><surname>Achard</surname><given-names>S</given-names></name><name name-style="western"><surname>Morcom</surname><given-names>A</given-names></name><name name-style="western"><surname>Bullmore</surname><given-names>E</given-names></name></person-group>             <year>2009</year>             <article-title>Age-related changes in modular organization of human brain functional networks.</article-title>             <source>Neuroimage</source>             <volume>44</volume>             <fpage>715</fpage>             <lpage>723</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bassett2">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bassett</surname><given-names>DS</given-names></name><name name-style="western"><surname>Brown</surname><given-names>JA</given-names></name><name name-style="western"><surname>Deshpande</surname><given-names>V</given-names></name><name name-style="western"><surname>Carlson</surname><given-names>JM</given-names></name><name name-style="western"><surname>Grafton</surname><given-names>ST</given-names></name></person-group>             <year>2011</year>             <article-title>Conserved and variable architecture of human white matter connectivity.</article-title>             <source>Neuroimage</source>             <volume>54</volume>             <fpage>1262</fpage>             <lpage>1279</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Felleman1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name><name name-style="western"><surname>van Essen</surname><given-names>DC</given-names></name></person-group>             <year>1991</year>             <article-title>Distributed hierarchical processing in the primate cerebral cortex.</article-title>             <source>Cereb Cortex</source>             <volume>1</volume>             <fpage>1</fpage>             <lpage>47</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Meunier2">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Meunier</surname><given-names>D</given-names></name><name name-style="western"><surname>Lambiotte</surname><given-names>R</given-names></name><name name-style="western"><surname>Bullmore</surname><given-names>ET</given-names></name></person-group>             <year>2010</year>             <article-title>Modular and hierarchically modular organization of brain networks.</article-title>             <source>Front Neurosci</source>             <volume>4</volume>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Fu1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>HC</given-names></name><name name-style="western"><surname>Lee</surname><given-names>YP</given-names></name><name name-style="western"><surname>Chiang</surname><given-names>CC</given-names></name><name name-style="western"><surname>Pao</surname><given-names>HT</given-names></name></person-group>             <year>2001</year>             <article-title>Divide-and-conquer learning and modular perceptron networks.</article-title>             <source>IEEE Transactions on Neural Netw</source>             <volume>12</volume>             <fpage>250</fpage>             <lpage>263</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Ersoy1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ersoy</surname><given-names>OK</given-names></name><name name-style="western"><surname>Deng</surname><given-names>SW</given-names></name></person-group>             <year>1995</year>             <article-title>Parallel, self-organizing, hierarchical neural networks with continuous inputs and outputs.</article-title>             <source>IEEE Trans Neural Netw</source>             <volume>6</volume>             <fpage>1037</fpage>             <lpage>1044</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Oshima1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Oshima</surname><given-names>H</given-names></name><name name-style="western"><surname>Odagaki</surname><given-names>T</given-names></name></person-group>             <year>2007</year>             <article-title>Storage capacity and retrieval time of small-world neural networks.</article-title>             <source>Phys Rev E</source>             <volume>76</volume>             <fpage>036114</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Dominguez1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dominguez</surname><given-names>D</given-names></name><name name-style="western"><surname>González</surname><given-names>M</given-names></name><name name-style="western"><surname>Serrano</surname><given-names>E</given-names></name><name name-style="western"><surname>Rodríguez</surname><given-names>FB</given-names></name></person-group>             <year>2009</year>             <article-title>Structured information in small-world neural networks.</article-title>             <source>Phys Rev E</source>             <volume>79</volume>             <fpage>021909</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Larochelle1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Larochelle</surname><given-names>H</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>Louradour</surname><given-names>J</given-names></name><name name-style="western"><surname>Lamblin</surname><given-names>P</given-names></name></person-group>             <year>2009</year>             <article-title>Exploring strategies for training deep neural networks.</article-title>             <source>J Mach Learn Res</source>             <volume>10</volume>             <fpage>1</fpage>             <lpage>40</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bengio1">
        <label>49</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name></person-group>             <year>2007</year>             <article-title>Scaling learning algorithms toward AI.</article-title>             <source>Large Scale KernelMachines</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>321</fpage>             <lpage>360</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bengio2">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name></person-group>             <year>2009</year>             <article-title>Learning deep architectures for AI.</article-title>             <source>Found Trends Mach Learn</source>             <volume>2</volume>             <fpage>1</fpage>             <lpage>127</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Atallah1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Atallah</surname><given-names>HE</given-names></name><name name-style="western"><surname>Frank</surname><given-names>MJ</given-names></name><name name-style="western"><surname>O'Reilly</surname><given-names>RC</given-names></name></person-group>             <year>2004</year>             <article-title>Hippocampus, cortex, and basal ganglia: Insights from computational models of complementary learning systems.</article-title>             <source>Neurobiol Learn Mem</source>             <volume>82</volume>             <fpage>253</fpage>             <lpage>267</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bullmore1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bullmore</surname><given-names>E</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>             <year>2009</year>             <article-title>Complex brain networks: graph theoretical analysis of structural and functional systems.</article-title>             <source>Nat Rev Neurosci</source>             <volume>10</volume>             <fpage>186</fpage>             <lpage>198</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Tononi1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name></person-group>             <year>1994</year>             <article-title>A measure for brain complexity: relating functional segregation and integration in the nervous system.</article-title>             <source>Proc Natl Acad Sci</source>             <volume>91</volume>             <fpage>5033</fpage>             <lpage>5037</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Turrigiano1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name><name name-style="western"><surname>Leslie</surname><given-names>KR</given-names></name><name name-style="western"><surname>Desai</surname><given-names>NS</given-names></name><name name-style="western"><surname>Rutherford</surname><given-names>LC</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>             <year>1998</year>             <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons.</article-title>             <source>Nature</source>             <volume>391</volume>             <fpage>892</fpage>             <lpage>896</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Turrigiano2">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>             <year>2004</year>             <article-title>Homeostatic plasticity in the developing nervous system.</article-title>             <source>Nat Rev Neurosci</source>             <volume>5</volume>             <fpage>97</fpage>             <lpage>107</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Markram1">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name></person-group>             <year>1996</year>             <article-title>Redistribution of synaptic efficacy between neocortical pyramidal neurons.</article-title>             <source>Nature</source>             <volume>382</volume>             <fpage>807</fpage>             <lpage>810</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Abbott1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>             <year>2000</year>             <article-title>Synaptic plasticity: Taming the beast.</article-title>             <source>Nat Neurosci</source>             <volume>3</volume>             <fpage>1178</fpage>             <lpage>1183</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bogacz1">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bogacz</surname><given-names>R</given-names></name><name name-style="western"><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Forstmann</surname><given-names>BU</given-names></name><name name-style="western"><surname>Nieuwenhuis</surname><given-names>S</given-names></name></person-group>             <year>2009</year>             <article-title>The neural basis of the speedaccuracy tradeoff.</article-title>             <source>Trends Neurosci</source>             <volume>33</volume>             <fpage>10</fpage>             <lpage>16</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-vanVeen1">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Veen</surname><given-names>V</given-names></name><name name-style="western"><surname>Krug</surname><given-names>MK</given-names></name><name name-style="western"><surname>Carter</surname><given-names>CS</given-names></name></person-group>             <year>2008</year>             <article-title>The neural and computational basis of controlled speedaccuracy tradeoff during task performance.</article-title>             <source>J Cognitive Neurosci</source>             <volume>20</volume>             <fpage>1952</fpage>             <lpage>1965</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Xu1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>T</given-names></name><name name-style="western"><surname>Yu</surname><given-names>X</given-names></name><name name-style="western"><surname>Perlik</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Tobin</surname><given-names>WF</given-names></name><name name-style="western"><surname>Zweig</surname><given-names>JA</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>Rapid formation and selective stabilization of synapses for enduring motor memories.</article-title>             <source>Nature</source>             <volume>462</volume>             <fpage>915</fpage>             <lpage>919</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Alstott1">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alstott</surname><given-names>J</given-names></name><name name-style="western"><surname>Breakspear</surname><given-names>M</given-names></name><name name-style="western"><surname>Hagmann</surname><given-names>P</given-names></name><name name-style="western"><surname>Cammoun</surname><given-names>L</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>             <year>2009</year>             <article-title>Modeling the impact of lesions in the human brain.</article-title>             <source>PLoS Comp Biol</source>             <volume>5</volume>             <fpage>e1000408</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Honey2">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Honey</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>             <year>2008</year>             <article-title>Dynamical consequences of lesions in cortical networks.</article-title>             <source>Hum Brain Mapp</source>             <volume>29</volume>             <fpage>802</fpage>             <lpage>809</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Allred1">
        <label>63</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Allred</surname><given-names>RP</given-names></name><name name-style="western"><surname>Adkins</surname><given-names>D</given-names></name><name name-style="western"><surname>Woodlee</surname><given-names>MT</given-names></name><name name-style="western"><surname>Husbands</surname><given-names>LC</given-names></name><name name-style="western"><surname>Maldonado</surname><given-names>MA</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>The vermicelli handling test: A simple quantitative measure of dexterous forepaw function in rats.</article-title>             <source>J Neurosci Methods</source>             <volume>170</volume>             <fpage>229</fpage>             <lpage>244</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Cucker1">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cucker</surname><given-names>F</given-names></name><name name-style="western"><surname>Smale</surname><given-names>S</given-names></name></person-group>             <year>2001</year>             <article-title>On the mathematical foundations of learning.</article-title>             <source>Bull Amer Math Soc</source>             <volume>39</volume>             <fpage>1</fpage>             <lpage>49</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bousquet1">
        <label>65</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bousquet</surname><given-names>O</given-names></name><name name-style="western"><surname>Boucheron</surname><given-names>S</given-names></name><name name-style="western"><surname>Lugosi</surname><given-names>G</given-names></name></person-group>             <year>2004</year>             <article-title>Introduction to statistical learning theory.</article-title>             <source>Advanced Lectures on Machine Learning</source>             <publisher-name>Springer Berlin, volume 3176</publisher-name>             <fpage>169</fpage>             <lpage>207</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Hinton1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name><name name-style="western"><surname>Osindero</surname><given-names>S</given-names></name><name name-style="western"><surname>Teh</surname><given-names>YW</given-names></name></person-group>             <year>2006</year>             <article-title>A fast learning algorithm for deep belief nets.</article-title>             <source>Neural Comput</source>             <volume>18</volume>             <fpage>1527</fpage>             <lpage>1554</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Marder1">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Marder</surname><given-names>E</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name><name name-style="western"><surname>Golowasch</surname><given-names>J</given-names></name></person-group>             <year>1996</year>             <article-title>Memory from the dynamics of intrinsic membrane currents.</article-title>             <source>Proc Natl Acad Sci</source>             <volume>93</volume>             <fpage>13481</fpage>             <lpage>13486</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Gaiteri1">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gaiteri</surname><given-names>C</given-names></name><name name-style="western"><surname>Rubin</surname><given-names>JE</given-names></name></person-group>             <year>2011</year>             <article-title>The interaction of intrinsic brain dynamics and network topology in determining network burst synchrony.</article-title>             <source>Front Comput Neurosci</source>             <volume>5</volume>             <fpage>1</fpage>             <lpage>14</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Bush1">
        <label>69</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bush</surname><given-names>P</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name></person-group>             <year>1996</year>             <article-title>Inhibition synchronizes sparsely connected cortical neurons within and between columns in realistic network models.</article-title>             <source>J Comput Neurosci</source>             <volume>3</volume>             <fpage>91</fpage>             <lpage>110</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Roelfsema1">
        <label>70</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roelfsema</surname><given-names>PR</given-names></name><name name-style="western"><surname>Engel</surname><given-names>AK</given-names></name><name name-style="western"><surname>Konig</surname><given-names>P</given-names></name><name name-style="western"><surname>Singer</surname><given-names>W</given-names></name></person-group>             <year>1997</year>             <article-title>Visuomotor integration is associated with zero time-lag synchronization among cortical areas.</article-title>             <source>Nature</source>             <volume>385</volume>             <fpage>157</fpage>             <lpage>161</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Vogels1">
        <label>71</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vogels</surname><given-names>TP</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2005</year>             <article-title>Signal propagation and logic gating in networks of integrate-and-fire neurons.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>10786</fpage>             <lpage>10795</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Rubinov1">
        <label>72</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rubinov</surname><given-names>M</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>van Leeuwen</surname><given-names>C</given-names></name><name name-style="western"><surname>Breakspear</surname><given-names>M</given-names></name></person-group>             <year>2009</year>             <article-title>Symbiotic relationship between brain structure and dynamics.</article-title>             <source>BMC Neuroscience</source>             <volume>10</volume>             <fpage>1</fpage>             <lpage>18</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-SanchezVives1">
        <label>73</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sanchez-Vives</surname><given-names>MV</given-names></name><name name-style="western"><surname>McCormick</surname><given-names>DA</given-names></name></person-group>             <year>2000</year>             <article-title>Cellular and network mechanisms of rythmic recurrent activity in the neocortex.</article-title>             <source>Nat Neurosci</source>             <volume>3</volume>             <fpage>1027</fpage>             <lpage>1034</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002063-Tort1">
        <label>74</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tort</surname><given-names>ABL</given-names></name><name name-style="western"><surname>Komorowski</surname><given-names>RW</given-names></name><name name-style="western"><surname>Manns</surname><given-names>JR</given-names></name><name name-style="western"><surname>Kopell</surname><given-names>NJ</given-names></name><name name-style="western"><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group>             <year>2009</year>             <article-title>Theta-gamma coupling increases during the learning of item-content associations.</article-title>             <source>Proc Natl Acad Sci</source>             <volume>106</volume>             <fpage>20942</fpage>             <lpage>20947</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>